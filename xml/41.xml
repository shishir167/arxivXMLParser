<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T01:08:05Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|40001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1886</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1886</id><created>2013-01-09</created><authors><author><keyname>Luzi</keyname><forenames>Daniela</forenames></author><author><keyname>Pecoraro</keyname><forenames>Fabrizio</forenames></author></authors><title>An Information System to Support and Monitor Clinical Trial Process</title><categories>cs.SE</categories><comments>16 pages, 1 table, 5 figures; International Journal of Computer
  Science &amp; Information Technology (IJCSIT) Vol 4, No 6, December 2012</comments><doi>10.5121/ijcsit.2012.4602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The demand of transparency of clinical research results, the need of
accelerating the process of transferring innovation in the daily medical
practice as well as assuring patient safety and product efficacy make it
necessary to extend the functionality of traditional trial registries. These
new systems should combine different functionalities to track the information
exchange, support collaborative work, manage regulatory documents and monitor
the entire clinical investigation (CIV) lifecycle. This is the approach used to
develop MEDIS, a Medical Device Information System, described in this paper
under the perspective of the business process, and the underlining
architecture. Moreover, MEDIS was designed on the basis of Health Level 7 (HL7)
v.3 standards and methodology to make it interoperable with similar registries,
but also to facilitate information exchange between different health
information systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1887</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1887</id><created>2013-01-09</created><authors><author><keyname>Gualdi</keyname><forenames>Stanislao</forenames></author><author><keyname>Medo</keyname><forenames>Matus</forenames></author><author><keyname>Zhang</keyname><forenames>Yi-Cheng</forenames></author></authors><title>Crowd Avoidance and Diversity in Socio-Economic Systems and
  Recommendation</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 4 figures</comments><journal-ref>EPL 101, 20008, 2013</journal-ref><doi>10.1209/0295-5075/101/20008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems recommend objects regardless of potential adverse effects
of their overcrowding. We address this shortcoming by introducing
crowd-avoiding recommendation where each object can be shared by only a limited
number of users or where object utility diminishes with the number of users
sharing it. We use real data to show that contrary to expectations, the
introduction of these constraints enhances recommendation accuracy and
diversity even in systems where overcrowding is not detrimental. The observed
accuracy improvements are explained in terms of removing potential bias of the
recommendation method. We finally propose a way to model artificial
socio-economic systems with crowd avoidance and obtain first analytical
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1894</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1894</id><created>2013-01-09</created><authors><author><keyname>Nagavi</keyname><forenames>Trisiladevi C.</forenames></author><author><keyname>Bhajantri</keyname><forenames>Nagappa U.</forenames></author></authors><title>An Extensive Analysis of Query by Singing/Humming System Through Query
  Proportion</title><categories>cs.MM cs.IR cs.SD</categories><comments>14 pages,11 figures; The International Journal of Multimedia &amp; Its
  Applications (IJMA) Vol.4, No.6, December 2012. arXiv admin note: text
  overlap with arXiv:1003.4083 by other authors</comments><doi>10.5121/ijma.2012.4606</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Query by Singing/Humming (QBSH) is a Music Information Retrieval (MIR) system
with small audio excerpt as query. The rising availability of digital music
stipulates effective music retrieval methods. Further, MIR systems support
content based searching for music and requires no musical acquaintance. Current
work on QBSH focuses mainly on melody features such as pitch, rhythm, note
etc., size of databases, response time, score matching and search algorithms.
Even though a variety of QBSH techniques are proposed, there is a dearth of
work to analyze QBSH through query excerption. Here, we present an analysis
that works on QBSH through query excerpt. To substantiate a series of
experiments are conducted with the help of Mel-Frequency Cepstral Coefficients
(MFCC), Linear Predictive Coefficients (LPC) and Linear Predictive Cepstral
Coefficients (LPCC) to portray the robustness of the knowledge representation.
Proposed experiments attempt to reveal that retrieval performance as well as
precision diminishes in the snail phase with the growing database size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1897</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1897</id><created>2013-01-09</created><authors><author><keyname>Memarsadeghi</keyname><forenames>Nargess</forenames></author><author><keyname>Moigne</keyname><forenames>Jacqueline Le</forenames></author><author><keyname>Blake</keyname><forenames>Peter N.</forenames></author><author><keyname>Morey</keyname><forenames>Peter A.</forenames></author><author><keyname>Landsman</keyname><forenames>Wayne B.</forenames></author><author><keyname>Chambers</keyname><forenames>Victor J.</forenames></author><author><keyname>Moseley</keyname><forenames>Samuel H.</forenames></author></authors><title>Image Registration for Stability Testing of MEMS</title><categories>cs.CV astro-ph.IM</categories><comments>Proceedings of 2011 IS&amp;T/SPIE Electronic Imaging, Computational
  Imaging IX Conference, San Francisco, CA, January 2011, Vol. 7873, 78730G-1:7</comments><doi>10.1117/12.872076</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Image registration, or alignment of two or more images covering the same
scenes or objects, is of great interest in many disciplines such as remote
sensing, medical imaging, astronomy, and computer vision. In this paper, we
introduce a new application of image registration algorithms. We demonstrate
how through a wavelet based image registration algorithm, engineers can
evaluate stability of Micro-Electro-Mechanical Systems (MEMS). In particular,
we applied image registration algorithms to assess alignment stability of the
MicroShutters Subsystem (MSS) of the Near Infrared Spectrograph (NIRSpec)
instrument of the James Webb Space Telescope (JWST). This work introduces a new
methodology for evaluating stability of MEMS devices to engineers as well as a
new application of image registration algorithms to computer scientists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1907</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1907</id><created>2013-01-09</created><authors><author><keyname>Memarsadeghi</keyname><forenames>Nargess</forenames></author><author><keyname>McFadden</keyname><forenames>Lucy A.</forenames></author><author><keyname>Skillman</keyname><forenames>David</forenames></author><author><keyname>McLean</keyname><forenames>Brian</forenames></author><author><keyname>Mutchler</keyname><forenames>Max</forenames></author><author><keyname>Carsenty</keyname><forenames>Uri</forenames></author><author><keyname>Palmer</keyname><forenames>Eric E.</forenames></author><author><keyname>Group</keyname><forenames>the Dawn Mission's Satellite Working</forenames></author></authors><title>Moon Search Algorithms for NASA's Dawn Mission to Asteroid Vesta</title><categories>astro-ph.IM astro-ph.EP cs.CV</categories><comments>Proceedings of the 2012 IS&amp;T/SPIE Electronic Imaging, Computational
  Imaging X Conference, San Francisco, CA, January 2012, Vol. 8296, pages
  82960H-1:12</comments><doi>10.1117/12.915564</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  A moon or natural satellite is a celestial body that orbits a planetary body
such as a planet, dwarf planet, or an asteroid. Scientists seek understanding
the origin and evolution of our solar system by studying moons of these bodies.
Additionally, searches for satellites of planetary bodies can be important to
protect the safety of a spacecraft as it approaches or orbits a planetary body.
If a satellite of a celestial body is found, the mass of that body can also be
calculated once its orbit is determined. Ensuring the Dawn spacecraft's safety
on its mission to the asteroid (4) Vesta primarily motivated the work of Dawn's
Satellite Working Group (SWG) in summer of 2011. Dawn mission scientists and
engineers utilized various computational tools and techniques for Vesta's
satellite search. The objectives of this paper are to 1) introduce the natural
satellite search problem, 2) present the computational challenges, approaches,
and tools used when addressing this problem, and 3) describe applications of
various image processing and computational algorithms for performing satellite
searches to the electronic imaging and computer science community. Furthermore,
we hope that this communication would enable Dawn mission scientists to improve
their satellite search algorithms and tools and be better prepared for
performing the same investigation in 2015, when the spacecraft is scheduled to
approach and orbit the dwarf planet (1) Ceres.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1912</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1912</id><created>2013-01-09</created><authors><author><keyname>LaBarge</keyname><forenames>Ralph</forenames></author><author><keyname>McGuire</keyname><forenames>Thomas</forenames></author></authors><title>Cloud Penetration Testing</title><categories>cs.CR cs.SE</categories><comments>20 pages, 16 figures; International Journal on Cloud Computing:
  Services and Architecture (IJCCSA),Vol.2, No.6, December 2012</comments><acm-class>D.4.6</acm-class><doi>10.5121/ijccsa.2012.2604</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the results of a series of penetration tests performed on
the OpenStack Essex Cloud Management Software. Several different types of
penetration tests were performed including network protocol and command line
fuzzing, session hijacking and credential theft. Using these techniques
exploitable vulnerabilities were discovered that could enable an attacker to
gain access to restricted information contained on the OpenStack server, or to
gain full administrative privileges on the server. Key recommendations to
address these vulnerabilities are to use a secure protocol, such as HTTPS, for
communications between a cloud user and the OpenStack Horizon Dashboard, to
encrypt all files that store user or administrative login credentials, and to
correct a software bug found in the OpenStack Cinder typedelete command.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1917</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1917</id><created>2013-01-09</created><authors><author><keyname>Wunder</keyname><forenames>Gerhard</forenames></author><author><keyname>Zhou</keyname><forenames>Chan</forenames></author><author><keyname>Kasparick</keyname><forenames>Martin</forenames></author></authors><title>Stability and Cost Optimization in Controlled Random Walks Using
  Scheduling Fields</title><categories>cs.SY cs.IT cs.NI math.IT</categories><comments>submitted to SIAM Journal on Control and Optimization. arXiv admin
  note: text overlap with arXiv:1208.2972, arXiv:0904.2302</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The control of large queueing networks is a notoriously difficult problem.
Recently, an interesting new policy design framework for the control problem
called h-MaxWeight has been proposed: h-MaxWeight is a natural generalization
of the famous MaxWeight policy where instead of the quadratic any other
surrogate value function can be applied. Stability of the policy is then
achieved through a perturbation technique. However, stability crucially depends
on parameter choice which has to be adapted in simulations. In this paper we
use a different technique where the required perturbations can be directly
implemented in the weight domain, which we call a scheduling field then.
Specifically, we derive the theoretical arsenal that guarantees universal
stability while still operating close to the underlying cost criterion.
Simulation examples suggest that the new approach to policy synthesis can even
provide significantly higher gains irrespective of any further assumptions on
the network model or parameter choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1918</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1918</id><created>2013-01-09</created><authors><author><keyname>Trautmann</keyname><forenames>Anna-Lena</forenames></author></authors><title>A lower bound for constant dimension codes from multi-component lifted
  MRD codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we investigate unions of lifted MRD codes of a fixed dimension
and minimum distance and derive an explicit formula for the cardinality of such
codes. This will then imply a lower bound on the cardinality of constant
dimension codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1932</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1932</id><created>2013-01-09</created><authors><author><keyname>Mahesha</keyname><forenames>P.</forenames></author><author><keyname>Vinod</keyname><forenames>D. S.</forenames></author></authors><title>An Approach for Classification of Dysfluent and Fluent Speech Using K-NN
  And SVM</title><categories>cs.SD cs.AI</categories><comments>10 pages,4 figures; International Journal of Computer Science,
  Engineering and Applications (IJCSEA) Vol.2, No.6, December 2012</comments><doi>10.5121/ijcsea.2012.2603</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new approach for classification of dysfluent and fluent
speech using Mel-Frequency Cepstral Coefficient (MFCC). The speech is fluent
when person's speech flows easily and smoothly. Sounds combine into syllable,
syllables mix together into words and words link into sentences with little
effort. When someone's speech is dysfluent, it is irregular and does not flow
effortlessly. Therefore, a dysfluency is a break in the smooth, meaningful flow
of speech. Stuttering is one such disorder in which the fluent flow of speech
is disrupted by occurrences of dysfluencies such as repetitions, prolongations,
interjections and so on. In this work we have considered three types of
dysfluencies such as repetition, prolongation and interjection to characterize
dysfluent speech. After obtaining dysfluent and fluent speech, the speech
signals are analyzed in order to extract MFCC features. The k-Nearest Neighbor
(k-NN) and Support Vector Machine (SVM) classifiers are used to classify the
speech as dysfluent and fluent speech. The 80% of the data is used for training
and 20% for testing. The average accuracy of 86.67% and 93.34% is obtained for
dysfluent and fluent speech respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1936</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1936</id><created>2013-01-09</created><authors><author><keyname>Sani</keyname><forenames>Amir</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Lazaric</keyname><forenames>Alessandro</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Munos</keyname><forenames>R&#xe9;mi</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Risk-Aversion in Multi-armed Bandits</title><categories>cs.LG</categories><comments>(2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic multi-armed bandits solve the Exploration-Exploitation dilemma and
ultimately maximize the expected reward. Nonetheless, in many practical
problems, maximizing the expected reward is not the most desirable objective.
In this paper, we introduce a novel setting based on the principle of
risk-aversion where the objective is to compete against the arm with the best
risk-return trade-off. This setting proves to be intrinsically more difficult
than the standard multi-arm bandit setting due in part to an exploration risk
which introduces a regret associated to the variability of an algorithm. Using
variance as a measure of risk, we introduce two new algorithms, investigate
their theoretical guarantees, and report preliminary empirical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1942</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1942</id><created>2013-01-09</created><updated>2016-01-10</updated><authors><author><keyname>Wang</keyname><forenames>Ziyu</forenames></author><author><keyname>Hutter</keyname><forenames>Frank</forenames></author><author><keyname>Zoghi</keyname><forenames>Masrour</forenames></author><author><keyname>Matheson</keyname><forenames>David</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author></authors><title>Bayesian Optimization in a Billion Dimensions via Random Embeddings</title><categories>stat.ML cs.LG</categories><comments>33 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian optimization techniques have been successfully applied to robotics,
planning, sensor placement, recommendation, advertising, intelligent user
interfaces and automatic algorithm configuration. Despite these successes, the
approach is restricted to problems of moderate dimension, and several workshops
on Bayesian optimization have identified its scaling to high-dimensions as one
of the holy grails of the field. In this paper, we introduce a novel random
embedding idea to attack this problem. The resulting Random EMbedding Bayesian
Optimization (REMBO) algorithm is very simple, has important invariance
properties, and applies to domains with both categorical and continuous
variables. We present a thorough theoretical analysis of REMBO. Empirical
results confirm that REMBO can effectively solve problems with billions of
dimensions, provided the intrinsic dimensionality is low. They also show that
REMBO achieves state-of-the-art performance in optimizing the 47 discrete
parameters of a popular mixed integer linear programming solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1950</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1950</id><created>2013-01-09</created><authors><author><keyname>Patrut</keyname><forenames>Bogdan</forenames></author></authors><title>Syntactic Analysis Based on Morphological Characteristic Features of the
  Romanian Language</title><categories>cs.CL cs.AI</categories><comments>13 pages, 3 figures, DIASEXP, International Journal on Natural
  Language Computing, 2012, Volume 1, Number 4</comments><msc-class>68T50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper refers to the syntactic analysis of phrases in Romanian, as an
important process of natural language processing. We will suggest a real-time
solution, based on the idea of using some words or groups of words that
indicate grammatical category; and some specific endings of some parts of
sentence. Our idea is based on some characteristics of the Romanian language,
where some prepositions, adverbs or some specific endings can provide a lot of
information about the structure of a complex sentence. Such characteristics can
be found in other languages, too, such as French. Using a special grammar, we
developed a system (DIASEXP) that can perform a dialogue in natural language
with assertive and interogative sentences about a &quot;story&quot; (a set of sentences
describing some events from the real life).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1959</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1959</id><created>2013-01-09</created><authors><author><keyname>Davis</keyname><forenames>L. C.</forenames></author></authors><title>The Effects of Powertrain Mechanical Response on the Dynamics and String
  Stability of a Platoon of Adaptive Cruise Control Vehicles</title><categories>nlin.AO cs.SY physics.soc-ph</categories><comments>16 pages, 10 figures</comments><doi>10.1016/j.physa.2013.04.023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamics of a platoon of adaptive cruise control vehicles is analyzed for
a general mechanical response of the vehicle's power-train. Effects of
acceleration-feedback control that were not previously studied are found. For
small acceleration-feedback gain, which produces marginally string-stable
behavior, the reduction of a disturbance (with increasing car number n) is
found to be faster than for the maximum allowable gain. The asymptotic
magnitude of a disturbance is shown to fall off as erf(ct./sq. rt. n) when n
goes to infinity. For gain approaching the lower limit of stability,
oscillations in acceleration associated with a secondary maximum in the
transfer function (as a function of frequency) can occur. A frequency-dependent
gain that reduces the secondary maximum, but does not affect the transfer
function near zero frequency, is proposed. Performance is thereby improved by
elimination of the undesirable oscillations while the rapid disturbance
reduction is retained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1999</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1999</id><created>2013-01-09</created><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Grandoni</keyname><forenames>Fabrizio</forenames></author><author><keyname>Kavitha</keyname><forenames>Telikepalli</forenames></author></authors><title>On Pairwise Spanners</title><categories>cs.DS</categories><comments>Full version of STACS 2013 paper; 13 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected $n$-node unweighted graph $G = (V, E)$, a spanner with
stretch function $f(\cdot)$ is a subgraph $H\subseteq G$ such that, if two
nodes are at distance $d$ in $G$, then they are at distance at most $f(d)$ in
$H$. Spanners are very well studied in the literature. The typical goal is to
construct the sparsest possible spanner for a given stretch function.
  In this paper we study pairwise spanners, where we require to approximate the
$u$-$v$ distance only for pairs $(u,v)$ in a given set $\cP \subseteq V\times
V$. Such $\cP$-spanners were studied before [Coppersmith,Elkin'05] only in the
special case that $f(\cdot)$ is the identity function, i.e. distances between
relevant pairs must be preserved exactly (a.k.a. pairwise preservers).
  Here we present pairwise spanners which are at the same time sparser than the
best known preservers (on the same $\cP$) and of the best known spanners (with
the same $f(\cdot)$). In more detail, for arbitrary $\cP$, we show that there
exists a $\mathcal{P}$-spanner of size $O(n(|\cP|\log n)^{1/4})$ with
$f(d)=d+4\log n$. Alternatively, for any $\eps&gt;0$, there exists a $\cP$-spanner
of size $O(n|\cP|^{1/4}\sqrt{\frac{\log n}{\eps}})$ with $f(d)=(1+\eps)d+4$. We
also consider the relevant special case that there is a critical set of nodes
$S\subseteq V$, and we wish to approximate either the distances within nodes in
$S$ or from nodes in $S$ to any other node. We show that there exists an
$(S\times S)$-spanner of size $O(n\sqrt{|S|})$ with $f(d)=d+2$, and an
$(S\times V)$-spanner of size $O(n\sqrt{|S|\log n})$ with $f(d)=d+2\log n$. All
the mentioned pairwise spanners can be constructed in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2005</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2005</id><created>2013-01-09</created><updated>2015-06-03</updated><authors><author><keyname>Zhang</keyname><forenames>Xiaowang</forenames></author><author><keyname>Wang</keyname><forenames>Kewen</forenames></author><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Ma</keyname><forenames>Yue</forenames></author><author><keyname>Qi</keyname><forenames>Guilin</forenames></author></authors><title>A Distance-based Paraconsistent Semantics for DL-Lite</title><categories>cs.AI</categories><comments>17 pages</comments><msc-class>03Cxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DL-Lite is an important family of description logics. Recently, there is an
increasing interest in handling inconsistency in DL-Lite as the constraint
imposed by a TBox can be easily violated by assertions in ABox in DL-Lite. In
this paper, we present a distance-based paraconsistent semantics based on the
notion of feature in DL-Lite, which provides a novel way to rationally draw
meaningful conclusions even from an inconsistent knowledge base. Finally, we
investigate several important logical properties of this entailment relation
based on the new semantics and show its promising advantages in non-monotonic
reasoning for DL-Lite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2010</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2010</id><created>2013-01-09</created><authors><author><keyname>Valluri</keyname><forenames>Maheswara Rao</forenames><affiliation>School of Mathematical and Computing Sciences, Fiji National University, Derrick Campus, Suva, Fiji</affiliation></author></authors><title>Authentication Schemes Using Polynomials Over Non-Commutative Rings</title><categories>cs.CR math.RA</categories><comments>International Journal on Cryptography and Information Security
  (IJCIS),Vol.2, No.4, December 2012</comments><doi>10.5121/ijcis.2012.2406</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Authentication is a process by which an entity,which could be a person or
intended computer,establishes its identity to another entity.In private and
public computer networks including the Internet,authentication is commonly done
through the use of logon passwords. Knowledge of the password is assumed to
guarantee that the user is authentic.Internet business and many other
transactions require a more stringent authentication process. The aim of this
paper is to propose two authentication schemes based on general non-commutative
rings. The key idea of the schemes is that for a given non-commutative ring;
one can build polynomials on additive structure and takes them as underlying
work structure. By doing so, one can implement authentication schemes, one of
them being zero-knowledge interactive proofs of knowledge, on multiplicative
structure of the ring. The security of the schemes is based on the
intractability of the polynomial symmetrical decomposition problem over the
given non-commutative ring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2012</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2012</id><created>2013-01-09</created><authors><author><keyname>Laxman</keyname><forenames>Srivatsan</forenames></author><author><keyname>Mittal</keyname><forenames>Sushil</forenames></author><author><keyname>Venkatesan</keyname><forenames>Ramarathnam</forenames></author></authors><title>Error Correction in Learning using SVMs</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with learning binary classifiers under adversarial
label-noise. We introduce the problem of error-correction in learning where the
goal is to recover the original clean data from a label-manipulated version of
it, given (i) no constraints on the adversary other than an upper-bound on the
number of errors, and (ii) some regularity properties for the original data. We
present a simple and practical error-correction algorithm called SubSVMs that
learns individual SVMs on several small-size (log-size), class-balanced, random
subsets of the data and then reclassifies the training points using a majority
vote. Our analysis reveals the need for the two main ingredients of SubSVMs,
namely class-balanced sampling and subsampled bagging. Experimental results on
synthetic as well as benchmark UCI data demonstrate the effectiveness of our
approach. In addition to noise-tolerance, log-size subsampled bagging also
yields significant run-time benefits over standard SVMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2013</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2013</id><created>2013-01-09</created><authors><author><keyname>Cui</keyname><forenames>Ke</forenames></author><author><keyname>Wang</keyname><forenames>Jian</forenames></author><author><keyname>Zhang</keyname><forenames>Hong-fei</forenames></author><author><keyname>Luo</keyname><forenames>Chun-li</forenames></author><author><keyname>Jin</keyname><forenames>Ge</forenames></author><author><keyname>Chen</keyname><forenames>Teng-yun</forenames></author></authors><title>A real-time design based on FPGA for Expeditious Error Reconciliation in
  QKD system</title><categories>quant-ph cs.CR</categories><comments>7 pages,10 figures</comments><journal-ref>IEEE transactions on Information Forensics and Security, Vol. 8,
  Issue 1, pp. 184-190 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For high-speed quantum key distribution systems, error reconciliation is
often the bottleneck affecting system performance. By exchanging common
information through a public channel, the identical key can be generated on
both communicating sides. However, the necessity to eliminate disclosed bits
for security reasons lowers the final key rate. To improve this key rate, the
amount of disclosed bits should be minimized. In addition, decreasing the time
spent on error reconciliation also improves the key rate. In this paper we
introduce a practical method for expeditious error reconciliation implemented
in a Field Programmable Gate Array for a discrete variable quantum key
distribution system, and illustrate the superiority of this method to other
similar algorithms running on a PC. Experimental results demonstrate the
rapidity of the proposed protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2015</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2015</id><created>2013-01-09</created><authors><author><keyname>Khashabi</keyname><forenames>Daniel</forenames></author><author><keyname>Ziyadi</keyname><forenames>Mojtaba</forenames></author><author><keyname>Liang</keyname><forenames>Feng</forenames></author></authors><title>Heteroscedastic Relevance Vector Machine</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this work we propose a heteroscedastic generalization to RVM, a fast
Bayesian framework for regression, based on some recent similar works. We use
variational approximation and expectation propagation to tackle the problem.
The work is still under progress and we are examining the results and comparing
with the previous works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2020</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2020</id><created>2013-01-09</created><authors><author><keyname>Perevalov</keyname><forenames>Eugene</forenames></author><author><keyname>Grace</keyname><forenames>David</forenames></author></authors><title>Towards the full information chain theory: expected loss and information
  relevance</title><categories>physics.data-an cs.IT math.IT</categories><comments>33 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When additional information sources are available, an important question for
an agent solving a certain problem is how to optimally use the information the
sources are capable of providing. A framework that relates information accuracy
on the source side to information relevance on the problem side is proposed. An
optimal information acquisition problem is formulated as that of question
selection to maximize the loss reduction for the problem solved by the agent. A
duality relationship between pseudoenergy (accuracy related) quantities on the
source side and loss (relevance related) quantities on the problem side is
observed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2030</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2030</id><created>2013-01-09</created><updated>2013-10-09</updated><authors><author><keyname>Noam</keyname><forenames>Yair</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea J.</forenames></author></authors><title>The One-Bit Null Space Learning Algorithm and its Convergence</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1202.0366</comments><doi>10.1109/TSP.2013.2278155</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new algorithm for MIMO cognitive radio Secondary Users
(SU) to learn the null space of the interference channel to the Primary User
(PU) without burdening the PU with any knowledge or explicit cooperation with
the SU.
  The knowledge of this null space enables the SU to transmit in the same band
simultaneously with the PU by utilizing separate spatial dimensions than the
PU. Specifically, the SU transmits in the null space of the interference
channel to the PU. We present a new algorithm, called the One-Bit Null Space
Learning Algorithm (OBNSLA), in which the SU learns the PU's null space by
observing a binary function that indicates whether the interference it inflicts
on the PU has increased or decreased in comparison to the SU's previous
transmitted signal. This function is obtained by listening to the PU
transmitted signal or control channel and extracting information from it about
whether the PU's Signal to Interference plus Noise power Ratio (SINR) has
increased or decreased.
  In addition to introducing the OBNSLA, this paper provides a thorough
convergence analysis of this algorithm. The OBNSLA is shown to have a linear
convergence rate and an asymptotic quadratic convergence rate. Finally, we
derive bounds on the interference that the SU inflicts on the PU as a function
of a parameter determined by the SU. This lets the SU control the maximum level
of interference, which enables it to protect the PU completely blindly with
minimum complexity. The asymptotic analysis and the derived bounds also apply
to the recently proposed Blind Null Space Learning Algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2032</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2032</id><created>2013-01-10</created><authors><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Paisitkriangkrai</keyname><forenames>Sakrapee</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>Training Effective Node Classifiers for Cascade Classification</title><categories>cs.CV cs.LG stat.ML</categories><comments>Appearing in Int'l J. Computer Vision. This is a substantially
  revised version of http://arxiv.org/abs/1008.3742</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cascade classifiers are widely used in real-time object detection. Different
from conventional classifiers that are designed for a low overall
classification error rate, a classifier in each node of the cascade is required
to achieve an extremely high detection rate and moderate false positive rate.
Although there are a few reported methods addressing this requirement in the
context of object detection, there is no principled feature selection method
that explicitly takes into account this asymmetric node learning objective. We
provide such an algorithm here. We show that a special case of the biased
minimax probability machine has the same formulation as the linear asymmetric
classifier (LAC) of Wu et al (2005). We then design a new boosting algorithm
that directly optimizes the cost function of LAC. The resulting
totally-corrective boosting algorithm is implemented by the column generation
technique in convex optimization. Experimental results on object detection
verify the effectiveness of the proposed boosting algorithm as a node
classifier in cascade object detection, and show performance better than that
of the current state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2041</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2041</id><created>2013-01-10</created><updated>2013-08-26</updated><authors><author><keyname>Chee</keyname><forenames>Yeow Meng</forenames></author><author><keyname>Kiah</keyname><forenames>Han Mao</forenames></author><author><keyname>Purkayastha</keyname><forenames>Punarbasu</forenames></author><author><keyname>Wang</keyname><forenames>Chengmin</forenames></author></authors><title>Importance of Symbol Equity in Coded Modulation for Power Line
  Communications</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of multiple frequency shift keying modulation with permutation codes
addresses the problem of permanent narrowband noise disturbance in a power line
communications system. In this paper, we extend this coded modulation scheme
based on permutation codes to general codes and introduce an additional new
parameter that more precisely captures a code's performance against permanent
narrowband noise. As a result, we define a new class of codes, namely,
equitable symbol weight codes, which are optimal with respect to this measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2046</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2046</id><created>2013-01-10</created><authors><author><keyname>Cetin</keyname><forenames>A. Emre</forenames></author></authors><title>In-situ associative permuting</title><categories>cs.DS</categories><comments>12 pages</comments><msc-class>68P05, 68P10</msc-class><acm-class>E.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The technique of in-situ associative permuting is introduced which is an
association of in-situ permuting and in-situ inverting. It is suitable for
associatively permutable permutations of {1,2,...,n} where the elements that
will be inverted are negative and stored in order relative to each other
according to their absolute values.
  Let K[1...n] be an array of n integer keys each in the range [1,n], and it is
allowed to modify the keys in the range [-n,n]. If the integer keys are
rearranged such that one of each distinct key having the value i is moved to
the i'th position of K, then the resulting arrangement (will be denoted by K^P)
can be transformed in-situ into associatively permutable permutation pi^P using
only logn additional bits. The associatively permutable permutation pi^P not
only stores the ranks of the keys of K^P but also uniquely represents K^P.
Restoring the keys from pi^P is not considered. However, in-situ associative
permuting pi^P in O(n) time using logn additional bits rearranges the elements
of pi^P in order, as well as lets to restore the keys of K^P in O(n) further
time using the inverses of the negative ranks. This means that an array of n
integer keys each in the range [1,n] can be sorted using only logn bits of
additional space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2055</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2055</id><created>2013-01-10</created><updated>2013-12-30</updated><authors><author><keyname>Qi</keyname><forenames>Junjian</forenames></author><author><keyname>Mei</keyname><forenames>Shengwei</forenames></author></authors><title>A Cascading Failure Model by Quantifying Interactions</title><categories>physics.soc-ph cs.SI cs.SY</categories><comments>5 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cascading failures triggered by trivial initial events are encountered in
many complex systems. It is the interaction and coupling between components of
the system that causes cascading failures. We propose a simple model to
simulate cascading failure by using the matrix that determines how components
interact with each other. A careful comparison is made between the original
cascades and the simulated cascades by the proposed model. It is seen that the
model can capture general features of the original cascades, suggesting that
the interaction matrix can well reflect the relationship between components. An
index is also defined to identify important links and the distribution follows
an obvious power law. By eliminating a small number of most important links the
risk of cascading failures can be significantly mitigated, which is
dramatically different from getting rid of the same number of links randomly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2060</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2060</id><created>2013-01-10</created><authors><author><keyname>Schreiber</keyname><forenames>Michael</forenames></author></authors><title>How relevant is the predictive power of the h-index? A case study of the
  time-dependent Hirsch index</title><categories>physics.soc-ph cs.DL</categories><comments>6 pages, 4 figures, accepted for publication in Journal of
  Informetrics</comments><journal-ref>Journal of Informetrics, 7, 325-329 (2013)</journal-ref><doi>10.1016/j.joi.2013.01.001</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The h-index has been shown to have predictive power. Here I report results of
an empirical study showing that the increase of the h-index with time often
depends for a long time on citations to rather old publications. This inert
behavior of the h-index means that it is difficult to use it as a measure for
predicting future scientific output.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2086</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2086</id><created>2013-01-10</created><authors><author><keyname>Gouriten</keyname><forenames>Georges</forenames></author><author><keyname>Senellart</keyname><forenames>Pierre</forenames></author></authors><title>API Blender: A Uniform Interface to Social Platform APIs</title><categories>cs.SE cs.SI</categories><comments>Presented in WWW 2012 Developer Track, April 18-20, 2012, Lyon,
  France and published in WWW no companion proceedings (url:
  http://www2012.org/proceedings/nocompanion/DevTrack_039.pdf)</comments><acm-class>H.3.5; D.3.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  With the growing success of the social Web, most Web developers have to
interact with at least one social Web platform, which implies studying the
related API specifications. These are often only informally described, may
contain errors, lack harmonization, and generally speaking make the developer's
work difficult. Most attempts to solve this problem, proposing formal
description languages for Web service APIs, have had limited success outside of
B2B applications; we believe it is due to their top-down nature. In addition, a
programmer dealing with one or several of these APIs has to deal with a number
of related tasks such as data integration, requests chaining, or policy
management, that are cumbersome to implement. Inspired by the SPORE project, we
present API Blender, an open-source solution to describe, interact with, and
integrate the most common social Web APIs. In this perspective, we first
introduce two new lightweight description formats for requests and services and
demonstrate their relevance with respect to current platform APIs. We present
our Python implementation of API Blender and its features regarding
authentication, policy management and multi-platform data integration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2092</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2092</id><created>2013-01-10</created><authors><author><keyname>Kowalski</keyname><forenames>Jakub</forenames></author><author><keyname>Szyku&#x142;a</keyname><forenames>Marek</forenames></author></authors><title>The \v{C}ern\'{y} conjecture for small automata: experimental report</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a report from a series of experiments involving computation of the
shortest reset words for automata with small number of states. We confirm that
the \v{C}ern\'{y} conjecture is true for all automata with at most 11 states on
2 letters. Also some new interesting results were obtained, including the third
gap in the distribution of the shortest reset words and new slowly
synchronizing classes of automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2102</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2102</id><created>2013-01-10</created><updated>2014-05-13</updated><authors><author><keyname>Soodhalter</keyname><forenames>Kirk M.</forenames></author></authors><title>A block MINRES algorithm based on the banded Lanczos method</title><categories>math.NA cs.MS cs.NA</categories><comments>20 Pages, 8 figures, 1 Algorithm, Revision based on reviewer comments</comments><msc-class>65F10</msc-class><doi>10.1007/s11075-014-9907-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a block minimum residual (MINRES) algorithm for symmetric
indefinite matrices. This version is built upon the band Lanczos method that
generates one basis vector of the block Krylov subspace per iteration rather
than a whole block as in the block Lanczos process. However, we modify the
method such that the most expensive operations are still performed in a block
fashion. The benefit of using the band Lanczos method is that one can detect
breakdowns from scalar values arising in the computation, allowing for a
handling of breakdown which is straightforward to implement.
  We derive a progressive formulation of the MINRES method based on the band
Lanczos process and give some implementation details. Specifically, a simple
reordering of the steps allows us to perform many of the operations at the
block level in order to take advantage of communication efficiencies offered by
the block Lanczos process. This is an important concern in the context of
next-generation super computing applications.
  We also present a technique allowing us to maintain the block size by
replacing dependent Lanczos vectors with pregenerated random vectors whose
orthogonality against all Lanczos vectors is maintained. Numerical results
illustrate the performance on some sample problems. We present experiments that
show how the relationship between right-hand sides can effect the performance
of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2115</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2115</id><created>2013-01-10</created><authors><author><keyname>Muandet</keyname><forenames>Krikamol</forenames></author><author><keyname>Balduzzi</keyname><forenames>David</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Domain Generalization via Invariant Feature Representation</title><categories>stat.ML cs.LG</categories><comments>The 30th International Conference on Machine Learning (ICML 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates domain generalization: How to take knowledge acquired
from an arbitrary number of related domains and apply it to previously unseen
domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based
optimization algorithm that learns an invariant transformation by minimizing
the dissimilarity across domains, whilst preserving the functional relationship
between input and output variables. A learning-theoretic analysis shows that
reducing dissimilarity improves the expected generalization ability of
classifiers on new domains, motivating the proposed algorithm. Experimental
results on synthetic and real-world datasets demonstrate that DICA successfully
learns invariant features and improves classifier performance in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2130</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2130</id><created>2013-01-10</created><updated>2013-10-14</updated><authors><author><keyname>Ravazzi</keyname><forenames>Chiara</forenames></author><author><keyname>Fosson</keyname><forenames>Sophie M.</forenames></author><author><keyname>Magli</keyname><forenames>Enrico</forenames></author></authors><title>Distributed soft thresholding for sparse signal recovery</title><categories>cs.IT cs.DC math.IT math.OC</categories><comments>Revised version. Main improvements: extension of the convergence
  theorem to regular graphs; new numerical results and comparisons with other
  algorithms</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we address the problem of distributed sparse recovery of
signals acquired via compressed measurements in a sensor network. We propose a
new class of distributed algorithms to solve Lasso regression problems, when
the communication to a fusion center is not possible, e.g., due to
communication cost or privacy reasons. More precisely, we introduce a
distributed iterative soft thresholding algorithm (DISTA) that consists of
three steps: an averaging step, a gradient step, and a soft thresholding
operation. We prove the convergence of DISTA in networks represented by regular
graphs, and we compare it with existing methods in terms of performance,
memory, and complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2137</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2137</id><created>2013-01-10</created><authors><author><keyname>Xu</keyname><forenames>Dai</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaowang</forenames></author><author><keyname>Lin</keyname><forenames>Zuoquan</forenames></author></authors><title>A Forgetting-based Approach to Merging Knowledge Bases</title><categories>cs.AI</categories><comments>5 pages</comments><msc-class>03Cxx</msc-class><journal-ref>2010 International Conference on Progress in Informatics and
  Computing, IEEE Computer Society, vol 1, pp. 321-325</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach based on variable forgetting, which is a
useful tool in resolving contradictory by filtering some given variables, to
merging multiple knowledge bases. This paper first builds a relationship
between belief merging and variable forgetting by using dilation. Variable
forgetting is applied to capture belief merging operation. Finally, some new
merging operators are developed by modifying candidate variables to amend the
shortage of traditional merging operators. Different from model selection of
traditional merging operators, as an alternative approach, variable selection
in those new operators could provide intuitive information about an atom
variable among whole knowledge bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2138</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2138</id><created>2013-01-10</created><authors><author><keyname>de Kerret</keyname><forenames>Paul</forenames></author><author><keyname>Yi</keyname><forenames>Xinping</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author></authors><title>On the Degrees of Freedom of the K-User Time Correlated Broadcast
  Channel with Delayed CSIT</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Degrees of Freedom (DoF) of a K-User MISO Broadcast Channel (BC) is
studied when the Transmitter (TX) has access to a delayed channel estimate in
addition to an imperfect estimate of the current channel. The current estimate
could be for example obtained from prediction applied on past estimates, in the
case where feedback delay is within the coherence time. Building on previous
recent works on this setting with two users, the estimation error of the
current channel is characterized by its scaling as P at the exponent \alpha,
where \alpha=1 (resp. \alpha=0) corresponds to an estimate being essentially
perfect (resp. useless) in terms of DoF. In this work, we contribute to the
characterization of the DoF region in such a setting by deriving an outerbound
for the DoF region and by providing an achievable DoF region. The achievable
DoF is obtained by developing a new alignment scheme, called the K\alpha-MAT
scheme, which builds upon both the principle of the MAT alignment scheme from
Maddah-Ali and Tse and Zero-Forcing to achieve a larger DoF when the delayed
CSIT received is correlated with the instantaneous channel state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2146</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2146</id><created>2013-01-10</created><authors><author><keyname>Zhang</keyname><forenames>Xiaowang</forenames></author><author><keyname>Xiao</keyname><forenames>Guohui</forenames></author><author><keyname>Lin</keyname><forenames>Zuoquan</forenames></author></authors><title>A Paraconsistent Tableau Algorithm Based on Sign Transformation in
  Semantic Web</title><categories>cs.AI</categories><comments>11 pages, in Chinese; the 4th Chinese Semantic Web Symposium (CSWS
  2010), Beijing, China</comments><msc-class>03Fxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an open, constantly changing and collaborative environment like the
forthcoming Semantic Web, it is reasonable to expect that knowledge sources
will contain noise and inaccuracies. It is well known, as the logical
foundation of the Semantic Web, description logic is lack of the ability of
tolerating inconsistent or incomplete data. Recently, the ability of
paraconsistent approaches in Semantic Web is weaker in this paper, we present a
tableau algorithm based on sign transformation in Semantic Web which holds the
stronger ability of reasoning. We prove that the tableau algorithm is decidable
which hold the same function of classical tableau algorithm for consistent
knowledge bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2150</identifier>
 <datestamp>2013-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2150</id><created>2013-01-09</created><updated>2013-10-01</updated><authors><author><keyname>Vieland</keyname><forenames>V. J.</forenames></author></authors><title>An Evidential Interpretation of the 1st and 2nd Laws of Thermodynamics</title><categories>physics.data-an cond-mat.stat-mech cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I argue here that both the first and second laws of thermodynamics, generally
understood to be quintessentially physical in nature, can be equally well
described as being about certain types of information without the need to
invoke physical manifestations for information. In particular, I show that the
statistician's familiar likelihood principle is a general conservation
principle on a par with the first law, and that likelihood itself involves a
form of irrecoverable information loss that can be expressed in the form of
(one version of) the second law. Each of these principles involves a particular
type of information, and requires its own form of bookkeeping to properly
account for information accumulation. I illustrate both sets of books with a
simple coin-tossing (binomial) experiment. In thermodynamics, absolute
temperature T is the link that relates energy-based and entropy-based
bookkeeping systems. I consider the information-based analogue of this link,
denoted here as E, and show that E has a meaningful interpretation in its own
right in connection with statistical inference. These results contribute to a
growing body of theory at the intersection of thermodynamics, information
theory and statistical inference, and suggest a novel framework in which E
itself for the first time plays a starring role.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2158</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2158</id><created>2013-01-10</created><authors><author><keyname>Bennett</keyname><forenames>Casey C.</forenames></author><author><keyname>Hauser</keyname><forenames>Kris</forenames></author></authors><title>Artificial Intelligence Framework for Simulating Clinical
  Decision-Making: A Markov Decision Process Approach</title><categories>cs.AI stat.ML</categories><comments>Keywords: Markov Decision Process; Dynamic Decision Network;
  Multi-Agent System; Clinical Artificial Intelligence; Medical Decision
  Making; Chronic Illness; (2013) Artificial Intelligence in Medicine</comments><journal-ref>Artificial Intelligence in Medicine. 57(1): 9-19. (2013)</journal-ref><doi>10.1016/j.artmed.2012.12.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the modern healthcare system, rapidly expanding costs/complexity, the
growing myriad of treatment options, and exploding information streams that
often do not effectively reach the front lines hinder the ability to choose
optimal treatment decisions over time. The goal in this paper is to develop a
general purpose (non-disease-specific) computational/artificial intelligence
(AI) framework to address these challenges. This serves two potential
functions: 1) a simulation environment for exploring various healthcare
policies, payment methodologies, etc., and 2) the basis for clinical artificial
intelligence - an AI that can think like a doctor. This approach combines
Markov decision processes and dynamic decision networks to learn from clinical
data and develop complex plans via simulation of alternative sequential
decision paths while capturing the sometimes conflicting, sometimes synergistic
interactions of various components in the healthcare system. It can operate in
partially observable environments (in the case of missing observations or data)
by maintaining belief states about patient health status and functions as an
online agent that plans and re-plans. This framework was evaluated using real
patient data from an electronic health record. Such an AI framework easily
outperforms the current treatment-as-usual (TAU) case-rate/fee-for-service
models of healthcare (Cost per Unit Change: $189 vs. $497) while obtaining a
30-35% increase in patient outcomes. Tweaking certain model parameters further
enhances this advantage, obtaining roughly 50% more improvement for roughly
half the costs. Given careful design and problem formulation, an AI simulation
framework can approximate optimal decisions even in complex and uncertain
environments. Future work is described that outlines potential lines of
research and integration of machine learning algorithms for personalized
medicine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2165</identifier>
 <datestamp>2014-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2165</id><created>2013-01-10</created><authors><author><keyname>Trautmann</keyname><forenames>Anna-Lena</forenames></author><author><keyname>Silberstein</keyname><forenames>Natalia</forenames></author><author><keyname>Rosenthal</keyname><forenames>Joachim</forenames></author></authors><title>List Decoding of Lifted Gabidulin Codes via the Pl\&quot;ucker Embedding</title><categories>cs.IT math.IT</categories><comments>Submitted to International Workshop on Coding and Cryptography (WCC)
  2013 in Bergen, Norway</comments><journal-ref>Preproceedings of the International Workshop on Coding and
  Cryptography (WCC) 2013, Bergen, Norway, pages 539--549, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Codes in the Grassmannian have recently found an application in random
network coding. All the codewords in such codes are subspaces of $\F_q^n$ with
a given dimension.
  In this paper, we consider the problem of list decoding of a certain family
of codes in the Grassmannian, called lifted Gabidulin codes.
  For this purpose we use the Pl\&quot;ucker embedding of the Grassmannian. We
describe a way of representing a subset of the Pl\&quot;ucker coordinates of lifted
Gabidulin codes as linear block codes. The union of the parity-check equations
of these block codes and the equations which arise from the description of a
ball around a subspace in the Pl\&quot;ucker coordinates describe the list of
codewords with distance less than a given parameter from the received word.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2172</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2172</id><created>2013-01-10</created><authors><author><keyname>Bouaziz</keyname><forenames>Bassem</forenames></author><author><keyname>Mahdi</keyname><forenames>Walid</forenames></author><author><keyname>Zlitni</keyname><forenames>Tarek</forenames></author><author><keyname>Hamadou</keyname><forenames>Abdelmajid ben</forenames></author></authors><title>Content-Based Video Browsing by Text Region Localization and
  Classification</title><categories>cs.MM cs.IR</categories><comments>11 pages, 12 figures, International Journal of Video &amp; Image
  Processing and Network Security IJVIPNS-IJENS Vol:10 No: 01</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The amount of digital video data is increasing over the world. It highlights
the need for efficient algorithms that can index, retrieve and browse this data
by content. This can be achieved by identifying semantic description captured
automatically from video structure. Among these descriptions, text within video
is considered as rich features that enable a good way for video indexing and
browsing. Unlike most video text detection and extraction methods that treat
video sequences as collections of still images, we propose in this paper
spatiotemporal. video-text localization and identification approach which
proceeds in two main steps: text region localization and text region
classification. In the first step we detect the significant appearance of the
new objects in a frame by a split and merge processes applied on binarized edge
frame pair differences. Detected objects are, a priori, considered as text.
They are then filtered according to both local contrast variation and texture
criteria in order to get the effective ones. The resulted text regions are
classified based on a visual grammar descriptor containing a set of semantic
text class regions characterized by visual features. A visual table of content
is then generated based on extracted text regions occurring within video
sequence enriched by a semantic identification. The experimentation performed
on a variety of video sequences shows the efficiency of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2173</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2173</id><created>2013-01-10</created><authors><author><keyname>Bouaziz</keyname><forenames>Baseem</forenames></author><author><keyname>Zlitni</keyname><forenames>Tarek</forenames></author><author><keyname>Mahdi</keyname><forenames>Walid</forenames></author></authors><title>AViTExt: Automatic Video Text Extraction, A new Approach for video
  content indexing Application</title><categories>cs.MM cs.IR</categories><comments>5 pages, 5 figures, 3rd International Conference on Information and
  Communication Technologies: From Theory to Applications(ICTTA 2008)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a spatial temporal video-text detection technique
which proceed in two principal steps:potential text region detection and a
filtering process. In the first step we divide dynamically each pair of
consecutive video frames into sub block in order to detect change. A
significant difference between homologous blocks implies the appearance of an
important object which may be a text region. The temporal redundancy is then
used to filter these regions and forms an effective text region. The
experimentation driven on a variety of video sequences shows the effectiveness
of our approach by obtaining a 89,39% as precision rate and 90,19 as recall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2180</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2180</id><created>2013-01-10</created><updated>2014-08-12</updated><authors><author><keyname>Khisti</keyname><forenames>Ashish</forenames></author><author><keyname>Draper</keyname><forenames>Stark</forenames></author></authors><title>The Streaming-DMT of Fading Channels</title><categories>cs.IT math.IT</categories><comments>To Appear, IEEE Trans. Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the sequential transmission of a stream of messages over a
block-fading multi-input-multi-output (MIMO) channel. A new message arrives at
the beginning of each coherence block, and the decoder is required to output
each message sequentially, after a delay of $T$ coherence blocks. In the
special case when $T=1$, the setup reduces to the quasi-static fading channel.
We establish the optimal diversity-multiplexing tradeoff (DMT) in the high
signal-to-noise-ratio (SNR) regime, and show that it equals $T$ times the DMT
of the quasi-static channel. The converse is based on utilizing the delay
constraint to amplify a local outage event associated with a message, globally
across all the coherence blocks. This approach appears to be new. We propose
two coding schemes that achieve the optimal DMT. The first scheme involves
interleaving of messages, such that each message is transmitted across $T$
consecutive coherence blocks. This scheme requires the knowledge of the delay
constraint at both the encoder and decoder. Our second coding scheme involves a
sequential tree code and is delay-universal i.e., the knowledge of the decoding
delay is not required by the encoder. However, in this scheme we require the
coherence block-length to increase as $\log\mathrm{({SNR})}$, in order to
attain the optimal DMT. Finally, we discuss the case when multiple messages
arrive at uniform intervals {\em within} each coherence period. Through a
simple example we exhibit the sub-optimality of interleaving, and propose
another scheme that achieves the optimal DMT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2181</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2181</id><created>2013-01-10</created><authors><author><keyname>B&#xf6;hm</keyname><forenames>Stanislav</forenames></author><author><keyname>G&#xf6;ller</keyname><forenames>Stefan</forenames></author><author><keyname>Jan&#x10d;ar</keyname><forenames>Petr</forenames></author></authors><title>Equivalence of Deterministic One-Counter Automata is NL-complete</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that language equivalence of deterministic one-counter automata is
NL-complete. This improves the superpolynomial time complexity upper bound
shown by Valiant and Paterson in 1975. Our main contribution is to prove that
two deterministic one-counter automata are inequivalent if and only if they can
be distinguished by a word of length polynomial in the size of the two input
automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2182</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2182</id><created>2013-01-10</created><updated>2014-09-12</updated><authors><author><keyname>Girard</keyname><forenames>Antoine</forenames></author></authors><title>Dynamic Triggering Mechanisms for Event-Triggered Control</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a new class of event triggering mechanisms for
event-triggered control systems. This class is characterized by the
introduction of an internal dynamic variable, which motivates the proposed name
of dynamic event triggering mechanism. The stability of the resulting closed
loop system is proved and the influence of design parameters on the decay rate
of the Lyapunov function is discussed. For linear systems, we establish a lower
bound on the inter-execution time as a function of the parameters. The
influence of these parameters on a quadratic integral performance index is also
studied. Some simulation results are provided for illustration of the
theoretical claims.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2194</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2194</id><created>2013-01-10</created><authors><author><keyname>Hill</keyname><forenames>Steven M.</forenames></author><author><keyname>Mukherjee</keyname><forenames>Sach</forenames></author></authors><title>Network-based clustering with mixtures of L1-penalized Gaussian
  graphical models: an empirical investigation</title><categories>stat.ML cs.LG stat.ME</categories><comments>A version of this work also appears in the first author's PhD Thesis
  (Sparse Graphical Models for Cancer Signalling, University of Warwick, 2012),
  which can be accessed at http://wrap.warwick.ac.uk/id/eprint/49626</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications, multivariate samples may harbor previously unrecognized
heterogeneity at the level of conditional independence or network structure.
For example, in cancer biology, disease subtypes may differ with respect to
subtype-specific interplay between molecular components. Then, both subtype
discovery and estimation of subtype-specific networks present important and
related challenges. To enable such analyses, we put forward a mixture model
whose components are sparse Gaussian graphical models. This brings together
model-based clustering and graphical modeling to permit simultaneous estimation
of cluster assignments and cluster-specific networks. We carry out estimation
within an L1-penalized framework, and investigate several specific penalization
regimes. We present empirical results on simulated data and provide general
recommendations for the formulation and use of mixtures of L1-penalized
Gaussian graphical models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2200</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2200</id><created>2013-01-10</created><authors><author><keyname>Zlitni</keyname><forenames>Tarek</forenames></author><author><keyname>Mahdi</keyname><forenames>Walid</forenames></author></authors><title>A Visual Grammar Approach for TV Program Identification</title><categories>cs.MM cs.IR</categories><comments>8 pages, 6 figures, (IJCNS) International Journal of Computer and
  Network Security, Vol. 2, No. 9, September 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic identification of TV programs within TV streams is an important
task for archive exploitation. This paper proposes a new spatial-temporal
approach to identify programs in TV streams in two main steps: First, a
reference catalogue for video grammars of visual jingles is constructed. We
exploit visual grammars characterizing instances of the same program type in
order to identify the various program types in the TV stream. The role of video
grammar is to represent the visual invariants for each visual jingle using a
set of descriptors appropriate for each TV program. Secondly, programs in TV
streams are identified by examining the similarity of the video signal to the
visual grammars in the catalogue. The main idea of identification process
consists in comparing the visual similarity of the video signal signature in TV
stream to the catalogue elements. After presenting the proposed approach, the
paper overviews the encouraging experimental results on several streams
extracted from different channels and composed of several programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2215</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2215</id><created>2013-01-10</created><authors><author><keyname>Fink</keyname><forenames>Michael</forenames></author><author><keyname>Lierler</keyname><forenames>Yuliya</forenames></author></authors><title>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2012), 5th International Workshop, September 4, 2012, Budapest,
  Hungary</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the papers presented at the fifth workshop on Answer Set
Programming and Other Computing Paradigms (ASPOCP 2012) held on September 4th,
2012 in Budapest, co-located with the 28th International Conference on Logic
Programming (ICLP 2012). It thus continues a series of previous events
co-located with ICLP, aiming at facilitating the discussion about crossing the
boundaries of current ASP techniques in theory, solving, and applications, in
combination with or inspired by other computing paradigms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2218</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2218</id><created>2013-01-10</created><updated>2013-02-19</updated><authors><author><keyname>Liao</keyname><forenames>Chenda</forenames></author><author><keyname>Barooah</keyname><forenames>Prabir</forenames></author></authors><title>Estimation from Relative Measurements in Mobile Networks with Markovian
  Switching Topology: Clock Skew and Offset Estimation for Time Synchronization</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze a distributed algorithm for estimation of scalar parameters
belonging to nodes in a mobile network from noisy relative measurements. The
motivation comes from the problem of clock skew and offset estimation for the
purpose of time synchronization. The time variation of the network was modeled
as a Markov chain. The estimates are shown to be mean square convergent under
fairly weak assumptions on the Markov chain, as long as the union of the graphs
is connected. Expressions for the asymptotic mean and correlation are also
provided. The Markovian switching topology model of mobile networks is
justified for certain node mobility models through empirically estimated
conditional entropy measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2220</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2220</id><created>2013-01-10</created><authors><author><keyname>Kim</keyname><forenames>Yoora</forenames></author><author><keyname>Lee</keyname><forenames>Kyunghan</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author><author><keyname>Rhee</keyname><forenames>Injong</forenames></author></authors><title>Providing Probabilistic Guarantees on the Time of Information Spread in
  Opportunistic Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A variety of mathematical tools have been developed for predicting the
spreading patterns in a number of varied environments including infectious
diseases, computer viruses, and urgent messages broadcast to mobile agent
(e.g., humans, vehicles, and mobile devices). These tools have mainly focused
on estimating the average time for the spread to reach a fraction (e.g.,
$\alpha$) of the agents, i.e., the so-called average completion time
$E(T_{\alpha})$. We claim that providing probabilistic guarantee on the time
for the spread $T_{\alpha}$ rather than only its average gives a much better
understanding of the spread, and hence could be used to design improved methods
to prevent epidemics or devise accelerated methods for distributing data. To
demonstrate the benefits, we introduce a new metric $G_{\alpha, \beta}$ that
denotes the time required to guarantee $\alpha$ completion with probability
$\beta$, and develop a new framework to characterize the distribution of
$T_\alpha$ for various spread parameters such as number of seeds, level of
contact rates, and heterogeneity in contact rates. We apply our technique to an
experimental mobility trace of taxies in Shanghai and show that our framework
enables us to allocate resources (i.e., to control spread parameters) for
acceleration of spread in a far more efficient way than the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2223</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2223</id><created>2013-01-08</created><authors><author><keyname>Conti</keyname><forenames>Edoardo</forenames></author><author><keyname>Cao</keyname><forenames>Steve</forenames></author><author><keyname>Thomas</keyname><forenames>A. J.</forenames></author></authors><title>Disruptions in the U.S. Airport Network</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our project analyzes the United States domestic airport network. We attempt
to determine which airports are most vital in maintaining the underlying
infrastructure for all domestic flights within the United States. To perform
our analysis, we use data from the first quarter of 2010 and use several
methods and algorithms that are frequently used in network science. Using these
statistics, we identified the most important airports in the United States and
investigate the role and significance that these airports play in maintaining
the structure of the entire domestic airport network. Some of these airports
include Denver International and Ted Stevens Anchorage International. We also
identified any structural holes and suggested improvements that can be made to
the network. Finally, through our analysis, we developed a disaster response
algorithm that calculates flight path reroutes in emergency situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2236</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2236</id><created>2013-01-10</created><authors><author><keyname>Khemiri</keyname><forenames>Rym</forenames></author><author><keyname>Bentayeb</keyname><forenames>Fadila</forenames></author></authors><title>User Profile-Driven Data Warehouse Summary for Adaptive OLAP Queries</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data warehousing is an essential element of decision support systems. It aims
at enabling the user knowledge to make better and faster daily business
decisions. To improve this decision support system and to give more and more
relevant information to the user, the need to integrate user's profiles into
the data warehouse process becomes crucial. In this paper, we propose to
exploit users' preferences as a basis for adapting OLAP (On-Line Analytical
Processing) queries to the user. For this, we present a user profile-driven
data warehouse approach that allows dening user's profile composed by his/her
identifier and a set of his/her preferences. Our approach is based on a general
data warehouse architecture and an adaptive OLAP analysis system. Our main idea
consists in creating a data warehouse materialized view for each user with
respect to his/her profile. This task is performed off-line when the user
defines his/her profile for the first time. Then, when a user query is
submitted to the data warehouse, the system deals with his/her data warehouse
materialized view instead of the whole data warehouse. In other words, the data
warehouse view summaries the data warehouse content for the user by taking into
account his/her preferences. Moreover, we are implementing our data warehouse
personalization approach under the SQL Server 2005 DBMS (DataBase Management
System).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2237</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2237</id><created>2013-01-10</created><authors><author><keyname>Xu</keyname><forenames>Ge</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Chen</keyname><forenames>Biao</forenames></author></authors><title>Wyner's Common Information: Generalizations and A New Lossy Source
  Coding Interpretation</title><categories>cs.IT math.IT</categories><comments>31 pages, 5 figures. Submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wyner's common information was originally defined for a pair of dependent
discrete random variables. Its significance is largely reflected in, hence also
confined to, several existing interpretations in various source coding
problems. This paper attempts to both generalize its definition and to expand
its practical significance by providing a new operational interpretation. The
generalization is two-folded: the number of dependent variables can be
arbitrary, so are the alphabet of those random variables. New properties are
determined for the generalized Wyner's common information of N dependent
variables. More importantly, a lossy source coding interpretation of Wyner's
common information is developed using the Gray-Wyner network. In particular, it
is established that the common information equals to the smallest common
message rate when the total rate is arbitrarily close to the rate distortion
function with joint decoding. A surprising observation is that such equality
holds independent of the values of distortion constraints as long as the
distortions are within some distortion region. Examples about the computation
of common information are given, including that of a pair of dependent Gaussian
random variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2247</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2247</id><created>2013-01-10</created><authors><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author><author><keyname>G&#xf3;mez-Garde&#xf1;es</keyname><forenames>Jes&#xfa;s</forenames></author><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Flor&#xed;a</keyname><forenames>Luis M.</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author></authors><title>Evolutionary dynamics of group interactions on structured populations: A
  review</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI nlin.AO q-bio.PE</categories><comments>17 two-column pages, 13 figures; accepted for publication in Journal
  of the Royal Society Interface</comments><journal-ref>J. R. Soc. Interface 10 (2013) 20120997</journal-ref><doi>10.1098/rsif.2012.0997</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactions among living organisms, from bacteria colonies to human
societies, are inherently more complex than interactions among particles and
nonliving matter. Group interactions are a particularly important and
widespread class, representative of which is the public goods game. In
addition, methods of statistical physics have proven valuable for studying
pattern formation, equilibrium selection, and self-organisation in evolutionary
games. Here we review recent advances in the study of evolutionary dynamics of
group interactions on structured populations, including lattices, complex
networks and coevolutionary models. We also compare these results with those
obtained on well-mixed populations. The review particularly highlights that the
study of the dynamics of group interactions, like several other important
equilibrium and non-equilibrium dynamical processes in biological, economical
and social sciences, benefits from the synergy between statistical physics,
network science and evolutionary game theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2252</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2252</id><created>2013-01-10</created><authors><author><keyname>Achan</keyname><forenames>Kannan</forenames></author><author><keyname>Frey</keyname><forenames>Brendan J.</forenames></author><author><keyname>Koetter</keyname><forenames>Ralf</forenames></author></authors><title>A Factorized Variational Technique for Phase Unwrapping in Markov Random
  Fields</title><categories>cs.CV</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-1-6</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some types of medical and topographic imaging device produce images in which
the pixel values are &quot;phase-wrapped&quot;, i.e. measured modulus a known scalar.
Phase unwrapping can be viewed as the problem of inferring the number of shifts
between each and every pair of neighboring pixels, subject to an a priori
preference for smooth surfaces, and subject to a zero curl constraint, which
requires that the shifts must sum to 0 around every loop. We formulate phase
unwrapping as a mean field inference problem in a Markov network, where the
prior favors the zero curl constraint. We compare our mean field technique with
the least squares method on a synthetic 100x100 image, and give results on a
512x512 synthetic aperture radar image from Sandia National Laboratories.&lt;Long
Text&gt;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2253</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2253</id><created>2013-01-10</created><authors><author><keyname>Amir</keyname><forenames>Eyal</forenames></author></authors><title>Efficient Approximation for Triangulation of Minimum Treewidth</title><categories>cs.DS cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-7-15</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present four novel approximation algorithms for finding triangulation of
minimum treewidth. Two of the algorithms improve on the running times of
algorithms by Robertson and Seymour, and Becker and Geiger that approximate the
optimum by factors of 4 and 3 2/3, respectively. A third algorithm is faster
than those but gives an approximation factor of 4 1/2. The last algorithm is
yet faster, producing factor-O(lg/k) approximations in polynomial time. Finding
triangulations of minimum treewidth for graphs is central to many problems in
computer science. Real-world problems in artificial intelligence, VLSI design
and databases are efficiently solvable if we have an efficient approximation
algorithm for them. We report on experimental results confirming the
effectiveness of our algorithms for large graphs associated with real-world
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2254</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2254</id><created>2013-01-10</created><authors><author><keyname>Angelopoulos</keyname><forenames>Nicos</forenames></author><author><keyname>Cussens</keyname><forenames>James</forenames></author></authors><title>Markov Chain Monte Carlo using Tree-Based Priors on Model Structure</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-16-23</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general framework for defining priors on model structure and
sampling from the posterior using the Metropolis-Hastings algorithm. The key
idea is that structure priors are defined via a probability tree and that the
proposal mechanism for the Metropolis-Hastings algorithm operates by traversing
this tree, thereby defining a cheaply computable acceptance probability. We
have applied this approach to Bayesian net structure learning using a number of
priors and tree traversal strategies. Our results show that these must be
chosen appropriately for this approach to be successful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2255</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2255</id><created>2013-01-10</created><authors><author><keyname>Benferhat</keyname><forenames>Salem</forenames></author><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Kaci</keyname><forenames>Souhila</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>Graphical readings of possibilistic logic bases</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-24-31</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Possibility theory offers either a qualitive, or a numerical framework for
representing uncertainty, in terms of dual measures of possibility and
necessity. This leads to the existence of two kinds of possibilistic causal
graphs where the conditioning is either based on the minimum, or the product
operator. Benferhat et al. (1999) have investigated the connections between
min-based graphs and possibilistic logic bases (made of classical formulas
weighted in terms of certainty). This paper deals with a more difficult issue :
the product-based graphical representations of possibilistic bases, which
provides an easy structural reading of possibilistic bases. Moreover, this
paper also provides another reading of possibilistic bases in terms of
comparative preferences of the form &quot;in the context p, q is preferred to not
q&quot;. This enables us to explicit preferences underlying a set of goals with
different levels of priority.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2256</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2256</id><created>2013-01-10</created><authors><author><keyname>Bodlaender</keyname><forenames>Hans L.</forenames></author><author><keyname>Koster</keyname><forenames>Arie M. C. A.</forenames></author><author><keyname>Eijkhof</keyname><forenames>Frank van den</forenames></author><author><keyname>van der Gaag</keyname><forenames>Linda C.</forenames></author></authors><title>Pre-processing for Triangulation of Probabilistic Networks</title><categories>cs.AI cs.DS</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-32-39</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The currently most efficient algorithm for inference with a probabilistic
network builds upon a triangulation of a network's graph. In this paper, we
show that pre-processing can help in finding good triangulations
forprobabilistic networks, that is, triangulations with a minimal maximum
clique size. We provide a set of rules for stepwise reducing a graph, without
losing optimality. This reduction allows us to solve the triangulation problem
on a smaller graph. From the smaller graph's triangulation, a triangulation of
the original graph is obtained by reversing the reduction steps. Our
experimental results show that the graphs of some well-known real-life
probabilistic networks can be triangulated optimally just by preprocessing; for
other networks, huge reductions in their graph's size are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2257</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2257</id><created>2013-01-10</created><authors><author><keyname>Bonet</keyname><forenames>Blai</forenames></author></authors><title>A Calculus for Causal Relevance</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-40-47</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a sound and completecalculus for causal relevance, based
onPearl's functional models semantics.The calculus consists of axioms and
rulesof inference for reasoning about causalrelevance relationships.We extend
the set of known axioms for causalrelevance with three new axioms, andintroduce
two new rules of inference forreasoning about specific subclasses
ofmodels.These subclasses give a more refinedcharacterization of causal models
than the one given in Halpern's axiomatizationof counterfactual
reasoning.Finally, we show how the calculus for causalrelevance can be used in
the task ofidentifying causal structure from non-observational data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2258</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2258</id><created>2013-01-10</created><authors><author><keyname>Bonet</keyname><forenames>Blai</forenames></author></authors><title>Instrumentality Tests Revisited</title><categories>cs.AI stat.ME</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-48-55</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An instrument is a random variable thatallows the identification of
parameters inlinear models when the error terms arenot uncorrelated.It is a
popular method used in economicsand the social sciences that reduces theproblem
of identification to the problemof finding the appropriate instruments.Few
years ago, Pearl introduced a necessarytest for instruments that allows the
researcher to discard those candidatesthat fail the test.In this paper, we make
a detailed study of Pearl's test and the general model forinstruments. The
results of this studyinclude a novel interpretation of Pearl'stest, a general
theory of instrumentaltests, and an affirmative answer to aprevious conjecture.
We also presentnew instrumentality tests for the casesof discrete and
continuous variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2259</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2259</id><created>2013-01-10</created><authors><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author><author><keyname>Bacchus</keyname><forenames>Fahiem</forenames></author><author><keyname>Brafman</keyname><forenames>Ronen I.</forenames></author></authors><title>UCP-Networks: A Directed Graphical Representation of Conditional
  Utilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-56-64</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new directed graphical representation of utility functions,
called UCP-networks, that combines aspects of two existing graphical models:
generalized additive models and CP-networks. The network decomposes a utility
function into a number of additive factors, with the directionality of the arcs
reflecting conditional dependence of preference statements - in the underlying
(qualitative) preference ordering - under a {em ceteris paribus} (all else
being equal) interpretation. This representation is arguably natural in many
settings. Furthermore, the strong CP-semantics ensures that computation of
optimization and dominance queries is very efficient. We also demonstrate the
value of this representation in decision making. Finally, we describe an
interactive elicitation procedure that takes advantage of the linear nature of
the constraints on &quot;`tradeoff weights&quot; imposed by a UCP-network. This procedure
allows the network to be refined until the regret of the decision with minimax
regret (with respect to the incompletely specified utility function) falls
below a specified threshold (e.g., the cost of further questioning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2260</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2260</id><created>2013-01-10</created><authors><author><keyname>Cheng</keyname><forenames>Jian</forenames></author><author><keyname>Druzdzel</keyname><forenames>Marek J.</forenames></author></authors><title>Confidence Inference in Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-75-82</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two sampling algorithms for probabilistic confidence inference in
Bayesian networks. These two algorithms (we call them AIS-BN-mu and
AIS-BN-sigma algorithms) guarantee that estimates of posterior probabilities
are with a given probability within a desired precision bound. Our algorithms
are based on recent advances in sampling algorithms for (1) estimating the mean
of bounded random variables and (2) adaptive importance sampling in Bayesian
networks. In addition to a simple stopping rule for sampling that they provide,
the AIS-BN-mu and AIS-BN-sigma algorithms are capable of guiding the learning
process in the AIS-BN algorithm. An empirical evaluation of the proposed
algorithms shows excellent performance, even for very unlikely evidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2261</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2261</id><created>2013-01-10</created><authors><author><keyname>Chu</keyname><forenames>Tianjiao</forenames></author><author><keyname>Scheines</keyname><forenames>Richard</forenames></author><author><keyname>Spirtes</keyname><forenames>Peter L.</forenames></author></authors><title>Semi-Instrumental Variables: A Test for Instrument Admissibility</title><categories>stat.ME cs.AI stat.AP</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-83-90</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a causal graphical model, an instrument for a variable X and its effect Y
is a random variable that is a cause of X and independent of all the causes of
Y except X. (Pearl (1995), Spirtes et al (2000)). Instrumental variables can be
used to estimate how the distribution of an effect will respond to a
manipulation of its causes, even in the presence of unmeasured common causes
(confounders). In typical instrumental variable estimation, instruments are
chosen based on domain knowledge. There is currently no statistical test for
validating a variable as an instrument. In this paper, we introduce the concept
of semi-instrument, which generalizes the concept of instrument. We show that
in the framework of additive models, under certain conditions, we can test
whether a variable is semi-instrumental. Moreover, adding some distribution
assumptions, we can test whether two semi-instruments are instrumental. We give
algorithms to estimate the p-value that a random variable is semi-instrumental,
and the p-value that two semi-instruments are both instrumental. These
algorithms can be used to test the experts' choice of instruments, or to
identify instruments automatically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2262</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2262</id><created>2013-01-10</created><authors><author><keyname>Cowell</keyname><forenames>Robert G.</forenames></author></authors><title>Conditions Under Which Conditional Independence and Scoring Methods Lead
  to Identical Selection of Bayesian Network Models</title><categories>cs.AI cs.LG stat.ML</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-91-97</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is often stated in papers tackling the task of inferring Bayesian network
structures from data that there are these two distinct approaches: (i) Apply
conditional independence tests when testing for the presence or otherwise of
edges; (ii) Search the model space using a scoring metric. Here I argue that
for complete data and a given node ordering this division is a myth, by showing
that cross entropy methods for checking conditional independence are
mathematically identical to methods based upon discriminating between models by
their overall goodness-of-fit logarithmic scores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2263</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2263</id><created>2013-01-10</created><authors><author><keyname>Danks</keyname><forenames>David</forenames></author><author><keyname>Glymour</keyname><forenames>Clark</forenames></author></authors><title>Linearity Properties of Bayes Nets with Binary Variables</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-98-104</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is &quot;well known&quot; that in linear models: (1) testable constraints on the
marginal distribution of observed variables distinguish certain cases in which
an unobserved cause jointly influences several observed variables; (2) the
technique of &quot;instrumental variables&quot; sometimes permits an estimation of the
influence of one variable on another even when the association between the
variables may be confounded by unobserved common causes; (3) the association
(or conditional probability distribution of one variable given another) of two
variables connected by a path or trek can be computed directly from the
parameter values associated with each edge in the path or trek; (4) the
association of two variables produced by multiple treks can be computed from
the parameters associated with each trek; and (5) the independence of two
variables conditional on a third implies the corresponding independence of the
sums of the variables over all units conditional on the sums over all units of
each of the original conditioning variables.These properties are exploited in
search procedures. It is also known that properties (2)-(5) do not hold for all
Bayes nets with binary variables. We show that (1) holds for all Bayes nets
with binary variables and (5) holds for all singly trek-connected Bayes nets of
that kind. We further show that all five properties hold for Bayes nets with
any DAG and binary variables parameterized with noisy-or and noisy-and gates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2264</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2264</id><created>2013-01-10</created><authors><author><keyname>Davis</keyname><forenames>Gary A.</forenames></author></authors><title>Using Bayesian Networks to Identify the Causal Effect of Speeding in
  Individual Vehicle/Pedestrian Collisions</title><categories>cs.AI stat.AP</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-105-111</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On roads showing significant violations of posted speed limits, one measure
of the safety effect of speeding is the difference between the road's actual
accident count and the count that would have occurred if the posted speed limit
had been strictly obeyed. An estimate of this accident reduction can be had by
computing the probability that speeding was a necessary condition for each of
set of accidents. This is an instance of assessing individual probabilities of
causation, which is generally not possible absent prior knowledge of causal
structure. For traffic accidents such prior knowledge is often available and
this paper illustrates how, for a commonly occurring class of
vehicle/pedestrian accidents, approaches to uncertainty and causal analyses
appearing in the accident reconstruction literature can be unified using
Bayesian networks. Measured skidmarks, pedestrian throw distances, and
pedestrian injury severity are treated as evidence, and using the Gibbs
Sampling routine BUGS, the posterior probability distribution over exogenous
variables, such as the vehicle's initial speed, location, and driver reaction
time, is computed. This posterior distribution is then used to compute the
&quot;probability of necessity&quot; for speeding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2265</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2265</id><created>2013-01-10</created><authors><author><keyname>Dechter</keyname><forenames>Rina</forenames></author><author><keyname>Larkin</keyname><forenames>David Ephraim</forenames></author></authors><title>Hybrid Processing of Beliefs and Constraints</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-112-119</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores algorithms for processing probabilistic and deterministic
information when the former is represented as a belief network and the latter
as a set of boolean clauses. The motivating tasks are 1. evaluating beliefs
networks having a large number of deterministic relationships and2. evaluating
probabilities of complex boolean querie over a belief network. We propose a
parameterized family of variable elimination algorithms that exploit both types
of information, and that allows varying levels of constraint propagation
inferences. The complexity of the scheme is controlled by the induced-width of
the graph {em augmented} by the dependencies introduced by the boolean
constraints. Preliminary empirical evaluation demonstrate the effect of
constraint propagation on probabilistic computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2266</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2266</id><created>2013-01-10</created><authors><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author><author><keyname>Hojen-Sorensen</keyname><forenames>Pedro</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Russell</keyname><forenames>Stuart</forenames></author></authors><title>Variational MCMC</title><categories>cs.LG stat.CO stat.ML</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-120-127</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new class of learning algorithms that combines variational
approximation and Markov chain Monte Carlo (MCMC) simulation. Naive algorithms
that use the variational approximation as proposal distribution can perform
poorly because this approximation tends to underestimate the true variance and
other features of the data. We solve this problem by introducing more
sophisticated MCMC algorithms. One of these algorithms is a mixture of two MCMC
kernels: a random walk Metropolis kernel and a blockMetropolis-Hastings (MH)
kernel with a variational approximation as proposaldistribution. The MH kernel
allows one to locate regions of high probability efficiently. The Metropolis
kernel allows us to explore the vicinity of these regions. This algorithm
outperforms variationalapproximations because it yields slightly better
estimates of the mean and considerably better estimates of higher moments, such
as covariances. It also outperforms standard MCMC algorithms because it locates
theregions of high probability quickly, thus speeding up convergence. We
demonstrate this algorithm on the problem of Bayesian parameter estimation for
logistic (sigmoid) belief networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2267</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2267</id><created>2013-01-10</created><authors><author><keyname>Deshpande</keyname><forenames>Amol</forenames></author><author><keyname>Garofalakis</keyname><forenames>Minos</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Efficient Stepwise Selection in Decomposable Models</title><categories>cs.AI cs.DS</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-128-135</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an efficient way of performing stepwise selection
in the class of decomposable models. The main contribution of the paper is a
simple characterization of the edges that canbe added to a decomposable model
while keeping the resulting model decomposable and an efficient algorithm for
enumerating all such edges for a given model in essentially O(1) time per edge.
We also discuss how backward selection can be performed efficiently using our
data structures.We also analyze the complexity of the complete stepwise
selection procedure, including the complexity of choosing which of the eligible
dges to add to (or delete from) the current model, with the aim ofminimizing
the Kullback-Leibler distance of the resulting model from the saturated model
for the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2268</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2268</id><created>2013-01-10</created><authors><author><keyname>El-Hay</keyname><forenames>Tal</forenames></author><author><keyname>Friedman</keyname><forenames>Nir</forenames></author></authors><title>Incorporating Expressive Graphical Models in Variational Approximations:
  Chain-Graphs and Hidden Variables</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-136-143</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Global variational approximation methods in graphical models allow efficient
approximate inference of complex posterior distributions by using a simpler
model. The choice of the approximating model determines a tradeoff between the
complexity of the approximation procedure and the quality of the approximation.
In this paper, we consider variational approximations based on two classes of
models that are richer than standard Bayesian networks, Markov networks or
mixture models. As such, these classes allow to find better tradeoffs in the
spectrum of approximations. The first class of models are chain graphs, which
capture distributions that are partially directed. The second class of models
are directed graphs (Bayesian networks) with additional latent variables. Both
classes allow representation of multi-variable dependencies that cannot be
easily represented within a Bayesian network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2269</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2269</id><created>2013-01-10</created><authors><author><keyname>Elidan</keyname><forenames>Gal</forenames></author><author><keyname>Friedman</keyname><forenames>Nir</forenames></author></authors><title>Learning the Dimensionality of Hidden Variables</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-144-151</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A serious problem in learning probabilistic models is the presence of hidden
variables. These variables are not observed, yet interact with several of the
observed variables. Detecting hidden variables poses two problems: determining
the relations to other variables in the model and determining the number of
states of the hidden variable. In this paper, we address the latter problem in
the context of Bayesian networks. We describe an approach that utilizes a
score-based agglomerative state-clustering. As we show, this approach allows us
to efficiently evaluate models with a range of cardinalities for the hidden
variable. We show how to extend this procedure to deal with multiple
interacting hidden variables. We demonstrate the effectiveness of this approach
by evaluating it on synthetic and real-life data. We show that our approach
learns models with hidden variables that generalize better and have better
structure than previous approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2270</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2270</id><created>2013-01-10</created><authors><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Mosenzon</keyname><forenames>Ori</forenames></author><author><keyname>Slonim</keyname><forenames>Noam</forenames></author><author><keyname>Tishby</keyname><forenames>Naftali</forenames></author></authors><title>Multivariate Information Bottleneck</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-152-161</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Information bottleneck method is an unsupervised non-parametric data
organization technique. Given a joint distribution P(A,B), this method
constructs a new variable T that extracts partitions, or clusters, over the
values of A that are informative about B. The information bottleneck has
already been applied to document classification, gene expression, neural code,
and spectral analysis. In this paper, we introduce a general principled
framework for multivariate extensions of the information bottleneck method.
This allows us to consider multiple systems of data partitions that are
inter-related. Our approach utilizes Bayesian networks for specifying the
systems of clusters and what information each captures. We show that this
construction provides insight about bottleneck variations and enables us to
characterize solutions of these variations. We also present a general framework
for iterative algorithms for constructing solutions, and apply it to several
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2271</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2271</id><created>2013-01-10</created><authors><author><keyname>Giang</keyname><forenames>Phan H.</forenames></author><author><keyname>Shenoy</keyname><forenames>Prakash P.</forenames></author></authors><title>A Comparison of Axiomatic Approaches to Qualitative Decision Making
  Using Possibility Theory</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-162-170</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze two recent axiomatic approaches proposed by Dubois
et al and by Giang and Shenoy to qualitative decision making where uncertainty
is described by possibility theory. Both axiomtizations are inspired by von
Neumann and Morgenstern's system of axioms for the case of probability theory.
We show that our approach naturally unifies two axiomatic systems that
correspond respectively to pessimistic and optimistic decision criteria
proposed by Dubois et al. The simplifying unification is achieved by (i)
replacing axioms that are supposed to reflect two informational attitudes
(uncertainty aversion and uncertainty attraction) by an axiom that imposes
order on set of standard lotteries and (ii) using a binary utility scale in
which each utility level is represented by a pair of numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2272</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2272</id><created>2013-01-10</created><authors><author><keyname>Gillispie</keyname><forenames>Steven B.</forenames></author><author><keyname>Perlman</keyname><forenames>Michael D.</forenames></author></authors><title>Enumerating Markov Equivalence Classes of Acyclic Digraph Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-171-177</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphical Markov models determined by acyclic digraphs (ADGs), also called
directed acyclic graphs (DAGs), are widely studied in statistics, computer
science (as Bayesian networks), operations research (as influence diagrams),
and many related fields. Because different ADGs may determine the same Markov
equivalence class, it long has been of interest to determine the efficiency
gained in model specification and search by working directly with Markov
equivalence classes of ADGs rather than with ADGs themselves. A computer
program was written to enumerate the equivalence classes of ADG models as
specified by Pearl &amp; Verma's equivalence criterion. The program counted
equivalence classes for models up to and including 10 vertices. The ratio of
number of classes to ADGs appears to approach an asymptote of about 0.267.
Classes were analyzed according to number of edges and class size. By edges,
the distribution of number of classes approaches a Gaussian shape. By class
size, classes of size 1 are most common, with the proportions for larger sizes
initially decreasing but then following a more irregular pattern. The maximum
number of classes generated by any undirected graph was found to increase
approximately factorially. The program also includes a new variation of orderly
algorithm for generating undirected graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2273</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2273</id><created>2013-01-10</created><authors><author><keyname>Guestrin</keyname><forenames>Carlos E.</forenames></author><author><keyname>Ormoneit</keyname><forenames>Dirk</forenames></author></authors><title>Robust Combination of Local Controllers</title><categories>cs.AI cs.SY</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-178-185</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Planning problems are hard, motion planning, for example, isPSPACE-hard. Such
problems are even more difficult in the presence of uncertainty. Although,
Markov Decision Processes (MDPs) provide a formal framework for such problems,
finding solutions to high dimensional continuous MDPs is usually difficult,
especially when the actions and time measurements are continuous. Fortunately,
problem-specific knowledge allows us to design controllers that are good
locally, though having no global guarantees. We propose a method of
nonparametrically combining local controllers to obtain globally good
solutions. We apply this formulation to two types of problems : motion planning
(stochastic shortest path) and discounted MDPs. For motion planning, we argue
that usual MDP optimality criterion (expected cost) may not be practically
relevant. Wepropose an alternative: finding the minimum cost path,subject to
the constraint that the robot must reach the goal withhigh probability. For
this problem, we prove that a polynomial number of samples is sufficient to
obtain a high probability path. For discounted MDPs, we propose a formulation
that explicitly deals with model uncertainty, i.e., the problem introduced when
transition probabilities are not known exactly. We formulate the problem as a
robust linear program which directly incorporates this type of uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2274</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2274</id><created>2013-01-10</created><authors><author><keyname>Ha</keyname><forenames>Vu A.</forenames></author><author><keyname>Haddawy</keyname><forenames>Peter</forenames></author><author><keyname>Miyamoto</keyname><forenames>John</forenames></author></authors><title>Similarity Measures on Preference Structures, Part II: Utility Functions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-186-193</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work cite{Ha98:Towards} we presented a case-based approach to
eliciting and reasoning with preferences. A key issue in this approach is the
definition of similarity between user preferences. We introduced the
probabilistic distance as a measure of similarity on user preferences, and
provided an algorithm to compute the distance between two partially specified
{em value} functions. This is for the case of decision making under {em
certainty}. In this paper we address the more challenging issue of computing
the probabilistic distance in the case of decision making under{em
uncertainty}. We provide an algorithm to compute the probabilistic distance
between two partially specified {em utility} functions. We demonstrate the use
of this algorithm with a medical data set of partially specified patient
preferences,where none of the other existing distancemeasures appear definable.
Using this data set, we also demonstrate that the case-based approach to
preference elicitation isapplicable in domains with uncertainty. Finally, we
provide a comprehensive analytical comparison of the probabilistic distance
with some existing distance measures on preferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2275</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2275</id><created>2013-01-10</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Causes and Explanations: A Structural-Model Approach --- Part 1: Causes</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001), later extended version is
  arXiv:cs/0011012</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-194-202</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new definition of actual causes, using structural equations to
model counterfactuals.We show that the definitions yield a plausible and
elegant account ofcausation that handles well examples which have caused
problems forother definitions and resolves major difficulties in the
traditionalaccount. In a companion paper, we show how the definition of
causality can beused to give an elegant definition of (causal) explanation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2276</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2276</id><created>2013-01-10</created><authors><author><keyname>Hattori</keyname><forenames>Hiromitsu</forenames></author><author><keyname>Yokoo</keyname><forenames>Makoto</forenames></author><author><keyname>Sakurai</keyname><forenames>Yuko</forenames></author><author><keyname>Shintani</keyname><forenames>Toramatsu</forenames></author></authors><title>A Dynamic Programming Model for Determining Bidding Strategies in
  Sequential Auctions: Quasi-linear Utility and Budget Constraints</title><categories>cs.GT</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-211-218</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a new method for finding an optimal biddingstrategy
in sequential auctions, using a dynamic programming technique. Theexisting
method assumes that the utility of a user is represented in anadditive form.
Thus, the remaining endowment of money must be explicitlyrepresented in each
state, and the calculation of the optimal biddingstrategy becomes
time-consuming when the initial endowment of money mbecomes large.In this
paper, we develop a new problem formalization that avoids
explicitlyrepresenting the remaining endowment, by assuming the utility of a
user canbe represented in a quasi-linear form, and representing the payment as
astate-transition cost. Experimental evaluations show that we can obtainmore
than an m-fold speed-up in the computation time. Furthermore, we havedeveloped
a method for obtaining a semi-optimal bidding strategy underbudget constraints,
and have experimentally confirmed the efficacy of thismethod.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2277</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2277</id><created>2013-01-10</created><authors><author><keyname>Hauskrecht</keyname><forenames>Milos</forenames></author><author><keyname>Upfal</keyname><forenames>Eli</forenames></author></authors><title>A Clustering Approach to Solving Large Stochastic Matching Problems</title><categories>cs.AI cs.DS</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-219-226</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we focus on efficient heuristics for solving a class of
stochastic planning problems that arise in a variety of business, investment,
and industrial applications. The problem is best described in terms of future
buy and sell contracts. By buying less reliable, but less expensive, buy
(supply) contracts, a company or a trader can cover a position of more reliable
and more expensive sell contracts. The goal is to maximize the expected net
gain (profit) by constructing a dose to optimum portfolio out of the available
buy and sell contracts. This stochastic planning problem can be formulated as a
two-stage stochastic linear programming problem with recourse. However, this
formalization leads to solutions that are exponential in the number of possible
failure combinations. Thus, this approach is not feasible for large scale
problems. In this work we investigate heuristic approximation techniques
alleviating the efficiency problem. We primarily focus on the clustering
approach and devise heuristics for finding clusterings leading to good
approximations. We illustrate the quality and feasibility of the approach
through experimental data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2278</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2278</id><created>2013-01-10</created><authors><author><keyname>Hinton</keyname><forenames>Geoffrey E.</forenames></author><author><keyname>Teh</keyname><forenames>Yee Whye</forenames></author></authors><title>Discovering Multiple Constraints that are Frequently Approximately
  Satisfied</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-227-234</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some high-dimensional data.sets can be modelled by assuming that there are
many different linear constraints, each of which is Frequently Approximately
Satisfied (FAS) by the data. The probability of a data vector under the model
is then proportional to the product of the probabilities of its constraint
violations. We describe three methods of learning products of constraints using
a heavy-tailed probability distribution for the violations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2279</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2279</id><created>2013-01-10</created><authors><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author><author><keyname>Ruan</keyname><forenames>Yongshao</forenames></author><author><keyname>Gomes</keyname><forenames>Carla P.</forenames></author><author><keyname>Kautz</keyname><forenames>Henry</forenames></author><author><keyname>Selman</keyname><forenames>Bart</forenames></author><author><keyname>Chickering</keyname><forenames>David Maxwell</forenames></author></authors><title>A Bayesian Approach to Tackling Hard Computational Problems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-235-244</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are developing a general framework for using learned Bayesian models for
decision-theoretic control of search and reasoningalgorithms. We illustrate the
approach on the specific task of controlling both general and domain-specific
solvers on a hard class of structured constraint satisfaction problems. A
successful strategyfor reducing the high (and even infinite) variance in
running time typically exhibited by backtracking search algorithms is to cut
off and restart the search if a solution is not found within a certainamount of
time. Previous work on restart strategies have employed fixed cut off values.
We show how to create a dynamic cut off strategy by learning a Bayesian model
that predicts the ultimate length of a trial based on observing the early
behavior of the search algorithm. Furthermore, we describe the general
conditions under which a dynamic restart strategy can outperform the
theoretically optimal fixed strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2280</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2280</id><created>2013-01-10</created><authors><author><keyname>Jarrad</keyname><forenames>Geoff A.</forenames></author></authors><title>Estimating Well-Performing Bayesian Networks using Bernoulli Mixtures</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-245-252</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel method for estimating Bayesian network (BN) parameters from data is
presented which provides improved performance on test data. Previous research
has shown the value of representing conditional probability distributions
(CPDs) via neural networks(Neal 1992), noisy-OR gates (Neal 1992, Diez 1993)and
decision trees (Friedman and Goldszmidt 1996).The Bernoulli mixture network
(BMN) explicitly represents the CPDs of discrete BN nodes as mixtures of local
distributions,each having a different set of parents.This increases the space
of possible structures which can be considered,enabling the CPDs to have
finer-grained dependencies.The resulting estimation procedure induces a
modelthat is better able to emulate the underlying interactions occurring in
the data than conventional conditional Bernoulli network models.The results for
artificially generated data indicate that overfitting is best reduced by
restricting the complexity of candidate mixture substructures local to each
node. Furthermore, mixtures of very simple substructures can perform almost as
well as more complex ones.The BMN is also applied to data collected from an
online adventure game with an application to keyhole plan recognition. The
results show that the BMN-based model brings a dramatic improvement in
performance over a conventional BN model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2281</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2281</id><created>2013-01-10</created><updated>2015-03-07</updated><authors><author><keyname>Kearns</keyname><forenames>Michael</forenames></author><author><keyname>Littman</keyname><forenames>Michael L.</forenames></author><author><keyname>Singh</keyname><forenames>Satinder</forenames></author></authors><title>Graphical Models for Game Theory</title><categories>cs.GT cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-2001-PG-253-260</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we introduce graphical modelsfor multi-player game theory, and
give powerful algorithms for computing their Nash equilibria in certain cases.
An n-player game is given by an undirected graph on n nodes and a set of n
local matrices. The interpretation is that the payoff to player i is determined
entirely by the actions of player i and his neighbors in the graph, and thus
the payoff matrix to player i is indexed only by these players. We thus view
the global n-player game as being composed of interacting local games, each
involving many fewer players. Each player's action may have global impact, but
it occurs through the propagation of local influences.Our main technical result
is an efficient algorithm for computing Nash equilibria when the underlying
graph is a tree (or can be turned into a tree with few node mergings). The
algorithm runs in time polynomial in the size of the representation (the graph
and theassociated local game matrices), and comes in two related but distinct
flavors. The first version involves an approximation step, and computes a
representation of all approximate Nash equilibria (of which there may be an
exponential number in general). The second version allows the exact computation
of Nash equilibria at the expense of weakened complexity bounds. The algorithm
requires only local message-passing between nodes (and thus can be implemented
by the players themselves in a distributed manner). Despite an analogy to
inference in Bayes nets that we develop, the analysis of our algorithm is more
involved than that for the polytree algorithm in, owing partially to the fact
that we must either compute, or select from, an exponential number of potential
solutions. We discuss a number of extensions, such as the computation of
equilibria with desirable global properties (e.g. maximizing global return),
and directions for further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2282</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2282</id><created>2013-01-10</created><authors><author><keyname>Kocka</keyname><forenames>Tomas</forenames></author><author><keyname>Bouckaert</keyname><forenames>Remco R.</forenames></author><author><keyname>Studeny</keyname><forenames>Milan</forenames></author></authors><title>On characterizing Inclusion of Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-261-268</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every directed acyclic graph (DAG) over a finite non-empty set of variables
(= nodes) N induces an independence model over N, which is a list of
conditional independence statements over N.The inclusion problem is how to
characterize (in graphical terms) whether all independence statements in the
model induced by a DAG K are in the model induced by a second DAG L. Meek
(1997) conjectured that this inclusion holds iff there exists a sequence of
DAGs from L to K such that only certain 'legal' arrow reversal and 'legal'
arrow adding operations are performed to get the next DAG in the sequence.In
this paper we give several characterizations of inclusion of DAG models and
verify Meek's conjecture in the case that the DAGs K and L differ in at most
one adjacency. As a warming up a rigorous proof of well-known graphical
characterizations of equivalence of DAGs, which is a highly related problem, is
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2283</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2283</id><created>2013-01-10</created><authors><author><keyname>Kocka</keyname><forenames>Tomas</forenames></author><author><keyname>Castelo</keyname><forenames>Robert</forenames></author></authors><title>Improved learning of Bayesian networks</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-269-276</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The search space of Bayesian Network structures is usually defined as Acyclic
Directed Graphs (DAGs) and the search is done by local transformations of DAGs.
But the space of Bayesian Networks is ordered by DAG Markov model inclusion and
it is natural to consider that a good search policy should take this into
account. First attempt to do this (Chickering 1996) was using equivalence
classes of DAGs instead of DAGs itself. This approach produces better results
but it is significantly slower. We present a compromise between these two
approaches. It uses DAGs to search the space in such a way that the ordering by
inclusion is taken into account. This is achieved by repetitive usage of local
moves within the equivalence class of DAGs. We show that this new approach
produces better results than the original DAGs approach without substantial
change in time complexity. We present empirical results, within the framework
of heuristic search and Markov Chain Monte Carlo, provided through the Alarm
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2284</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2284</id><created>2013-01-10</created><authors><author><keyname>Kontkanen</keyname><forenames>Petri</forenames></author><author><keyname>Myllymaki</keyname><forenames>Petri</forenames></author><author><keyname>Tirri</keyname><forenames>Henry</forenames></author></authors><title>Classifier Learning with Supervised Marginal Likelihood</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-277-284</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been argued that in supervised classification tasks, in practice it
may be more sensible to perform model selection with respect to some more
focused model selection score, like the supervised (conditional) marginal
likelihood, than with respect to the standard marginal likelihood criterion.
However, for most Bayesian network models, computing the supervised marginal
likelihood score takes exponential time with respect to the amount of observed
data. In this paper, we consider diagnostic Bayesian network classifiers where
the significant model parameters represent conditional distributions for the
class variable, given the values of the predictor variables, in which case the
supervised marginal likelihood can be computed in linear time with respect to
the data. As the number of model parameters grows in this case exponentially
with respect to the number of predictors, we focus on simple diagnostic models
where the number of relevant predictors is small, and suggest two approaches
for applying this type of models in classification. The first approach is based
on mixtures of simple diagnostic models, while in the second approach we apply
the small predictor sets of the simple diagnostic models for augmenting the
Naive Bayes classifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2285</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2285</id><created>2013-01-10</created><authors><author><keyname>Lang</keyname><forenames>Jerome</forenames></author><author><keyname>Muller</keyname><forenames>Philippe</forenames></author></authors><title>Plausible reasoning from spatial observations</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-285-292</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article deals with plausible reasoning from incomplete knowledge about
large-scale spatial properties. The availableinformation, consisting of a set
of pointwise observations,is extrapolated to neighbour points. We make use of
belief functions to represent the influence of the knowledge at a given point
to another point; the quantitative strength of this influence decreases when
the distance between both points increases. These influences arethen aggregated
using a variant of Dempster's rule of combination which takes into account the
relative dependence between observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2286</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2286</id><created>2013-01-10</created><authors><author><keyname>Lafferty</keyname><forenames>John</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry A.</forenames></author></authors><title>Iterative Markov Chain Monte Carlo Computation of Reference Priors and
  Minimax Risk</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-293-300</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an iterative Markov chainMonte Carlo algorithm for
computingreference priors and minimax risk forgeneral parametric families.
Ourapproach uses MCMC techniques based onthe Blahut-Arimoto algorithm
forcomputing channel capacity ininformation theory. We give astatistical
analysis of the algorithm,bounding the number of samples requiredfor the
stochastic algorithm to closelyapproximate the deterministic algorithmin each
iteration. Simulations arepresented for several examples fromexponential
families. Although we focuson applications to reference priors andminimax risk,
the methods and analysiswe develop are applicable to a muchbroader class of
optimization problemsand iterative algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2287</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2287</id><created>2013-01-10</created><authors><author><keyname>Laskey</keyname><forenames>Kathryn Blackmond</forenames></author><author><keyname>Mahoney</keyname><forenames>Suzanne M.</forenames></author><author><keyname>Wright</keyname><forenames>Ed</forenames></author></authors><title>Hypothesis Management in Situation-Specific Network Construction</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-301-309</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of knowledge-based model construction in the
presence of uncertainty about the association of domain entities to random
variables. Multi-entity Bayesian networks (MEBNs) are defined as a
representation for knowledge in domains characterized by uncertainty in the
number of relevant entities, their interrelationships, and their association
with observables. An MEBN implicitly specifies a probability distribution in
terms of a hierarchically structured collection of Bayesian network fragments
that together encode a joint probability distribution over arbitrarily many
interrelated hypotheses. Although a finite query-complete model can always be
constructed, association uncertainty typically makes exact model construction
and evaluation intractable. The objective of hypothesis management is to
balance tractability against accuracy. We describe an application to the
problem of using intelligence reports to infer the organization and activities
of groups of military vehicles. Our approach is compared to related work in the
tracking and fusion literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2288</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2288</id><created>2013-01-10</created><authors><author><keyname>Lerner</keyname><forenames>Uri</forenames></author><author><keyname>Parr</keyname><forenames>Ron</forenames></author></authors><title>Inference in Hybrid Networks: Theoretical Limits and Practical
  Algorithms</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-310-318</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important subclass of hybrid Bayesian networks are those that represent
Conditional Linear Gaussian (CLG) distributions --- a distribution with a
multivariate Gaussian component for each instantiation of the discrete
variables. In this paper we explore the problem of inference in CLGs. We show
that inference in CLGs can be significantly harder than inference in Bayes
Nets. In particular, we prove that even if the CLG is restricted to an
extremely simple structure of a polytree in which every continuous node has at
most one discrete ancestor, the inference task is NP-hard.To deal with the
often prohibitive computational cost of the exact inference algorithm for CLGs,
we explore several approximate inference algorithms. These algorithms try to
find a small subset of Gaussians which are a good approximation to the full
mixture distribution. We consider two Monte Carlo approaches and a novel
approach that enumerates mixture components in order of prior probability. We
compare these methods on a variety of problems and show that our novel
algorithm is very promising for large, hybrid diagnosis problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2289</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2289</id><created>2013-01-10</created><authors><author><keyname>Lerner</keyname><forenames>Uri</forenames></author><author><keyname>Segal</keyname><forenames>Eran</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Exact Inference in Networks with Discrete Children of Continuous Parents</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-319-328</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real life domains contain a mixture of discrete and continuous variables
and can be modeled as hybrid Bayesian Networks. Animportant subclass of hybrid
BNs are conditional linear Gaussian (CLG) networks, where the conditional
distribution of the continuous variables given an assignment to the discrete
variables is a multivariate Gaussian. Lauritzen's extension to the clique tree
algorithm can be used for exact inference in CLG networks. However, many
domains also include discrete variables that depend on continuous ones, and CLG
networks do not allow such dependencies to berepresented. No exact inference
algorithm has been proposed for these enhanced CLG networks. In this paper, we
generalize Lauritzen's algorithm, providing the first &quot;exact&quot; inference
algorithm for augmented CLG networks - networks where continuous nodes are
conditional linear Gaussians but that also allow discrete children ofcontinuous
parents. Our algorithm is exact in the sense that it computes the exact
distributions over the discrete nodes, and the exact first and second moments
of the continuous ones, up to the accuracy obtained by numerical integration
used within thealgorithm. When the discrete children are modeled with softmax
CPDs (as is the case in many real world domains) the approximation of the
continuous distributions using the first two moments is particularly accurate.
Our algorithm is simple to implement and often comparable in its complexity to
Lauritzen's algorithm. We show empirically that it achieves substantially
higher accuracy than previous approximate algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2290</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2290</id><created>2013-01-10</created><authors><author><keyname>Lukasiewicz</keyname><forenames>Thomas</forenames></author></authors><title>Probabilistic Logic Programming under Inheritance with Overriding</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-329-336</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present probabilistic logic programming under inheritance with overriding.
This approach is based on new notions of entailment for reasoning with
conditional constraints, which are obtained from the classical notion of
logical entailment by adding the principle of inheritance with overriding. This
is done by using recent approaches to probabilistic default reasoning with
conditional constraints. We analyze the semantic properties of the new
entailment relations. We also present algorithms for probabilistic logic
programming under inheritance with overriding, and program transformations for
an increased efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2291</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2291</id><created>2013-01-10</created><authors><author><keyname>Madsen</keyname><forenames>Anders L.</forenames></author><author><keyname>Nilsson</keyname><forenames>Dennis</forenames></author></authors><title>Solving Influence Diagrams using HUGIN, Shafer-Shenoy and Lazy
  Propagation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-337-345</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we compare three different architectures for the evaluation of
influence diagrams: HUGIN, Shafer-Shenoy, and Lazy Evaluation architecture. The
computational complexity of the architectures are compared on the LImited
Memory Influence Diagram (LIMID): a diagram where only the requiste information
for the computation of the optimal policies are depicted. Because the requsite
information is explicitly represented in the LIMID the evaluation can take
advantage of it, and significant savings in computational can be obtained. In
this paper we show how the obtained savings is considerably increased when the
computations performed on the LIMID is according to the Lazy Evaluation scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2292</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2292</id><created>2013-01-10</created><authors><author><keyname>Margaritis</keyname><forenames>Dimitris</forenames></author><author><keyname>Thrun</keyname><forenames>Sebastian</forenames></author></authors><title>A Bayesian Multiresolution Independence Test for Continuous Variables</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-346-353</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a method ofcomputing the posterior probability
ofconditional independence of two or morecontinuous variables from
data,examined at several resolutions. Ourapproach is motivated by
theobservation that the appearance ofcontinuous data varies widely atvarious
resolutions, producing verydifferent independence estimatesbetween the
variablesinvolved. Therefore, it is difficultto ascertain independence
withoutexamining data at several carefullyselected resolutions. In our paper,
weaccomplish this using the exactcomputation of the posteriorprobability of
independence, calculatedanalytically given a resolution. Ateach examined
resolution, we assume amultinomial distribution with Dirichletpriors for the
discretized tableparameters, and compute the posteriorusing Bayesian
integration. Acrossresolutions, we use a search procedureto approximate the
Bayesian integral ofprobability over an exponential numberof possible
histograms. Our methodgeneralizes to an arbitrary numbervariables in a
straightforward manner.The test is suitable for Bayesiannetwork learning
algorithms that useindependence tests to infer the networkstructure, in domains
that contain anymix of continuous, ordinal andcategorical variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2293</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2293</id><created>2013-01-10</created><authors><author><keyname>Maynard-Reid</keyname><forenames>Pedrito</forenames><suffix>II</suffix></author><author><keyname>Chajewska</keyname><forenames>Urszula</forenames></author></authors><title>Aggregating Learned Probabilistic Beliefs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-354-361</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the task of aggregating beliefs of severalexperts. We assume that
these beliefs are represented as probabilitydistributions. We argue that the
evaluation of any aggregationtechnique depends on the semantic context of this
task. We propose aframework, in which we assume that nature generates samples
from a`true' distribution and different experts form their beliefs based onthe
subsets of the data they have a chance to observe. Naturally, theideal
aggregate distribution would be the one learned from thecombined sample sets.
Such a formulation leads to a natural way tomeasure the accuracy of the
aggregation mechanism.We show that the well-known aggregation operator LinOP is
ideallysuited for that task. We propose a LinOP-based learning
algorithm,inspired by the techniques developed for Bayesian learning,
whichaggregates the experts' distributions represented as Bayesiannetworks. Our
preliminary experiments show that this algorithmperforms well in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2294</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2294</id><created>2013-01-10</created><authors><author><keyname>Minka</keyname><forenames>Thomas P.</forenames></author></authors><title>Expectation Propagation for approximate Bayesian inference</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-362-369</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new deterministic approximation technique in Bayesian
networks. This method, &quot;Expectation Propagation&quot;, unifies two previous
techniques: assumed-density filtering, an extension of the Kalman filter, and
loopy belief propagation, an extension of belief propagation in Bayesian
networks. All three algorithms try to recover an approximate distribution which
is close in KL divergence to the true distribution. Loopy belief propagation,
because it propagates exact belief states, is useful for a limited class of
belief networks, such as those which are purely discrete. Expectation
Propagation approximates the belief states by only retaining certain
expectations, such as mean and variance, and iterates until these expectations
are consistent throughout the network. This makes it applicable to hybrid
networks with discrete and continuous nodes. Expectation Propagation also
extends belief propagation in the opposite direction - it can propagate richer
belief states that incorporate correlations between nodes. Experiments with
Gaussian mixture models show Expectation Propagation to be convincingly better
than methods with similar computational cost: Laplace's method, variational
Bayes, and Monte Carlo. Expectation Propagation also provides an efficient
algorithm for training Bayes point machine classifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2295</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2295</id><created>2013-01-10</created><authors><author><keyname>Morris</keyname><forenames>Quaid</forenames></author></authors><title>Recognition Networks for Approximate Inference in BN20 Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-370-377</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose using recognition networks for approximate inference inBayesian
networks (BNs). A recognition network is a multilayerperception (MLP) trained
to predict posterior marginals given observedevidence in a particular BN. The
input to the MLP is a vector of thestates of the evidential nodes. The activity
of an output unit isinterpreted as a prediction of the posterior marginal of
thecorresponding variable. The MLP is trained using samples generated fromthe
corresponding BN.We evaluate a recognition network that was trained to do
inference ina large Bayesian network, similar in structure and complexity to
theQuick Medical Reference, Decision Theoretic (QMR-DT). Our networkis a
binary, two-layer, noisy-OR network containing over 4000 potentially observable
nodes and over 600 unobservable, hidden nodes. Inreal medical diagnosis, most
observables are unavailable, and there isa complex and unknown bias that
selects which ones are provided. Weincorporate a very basic type of selection
bias in our network: a knownpreference that available observables are positive
rather than negative.Even this simple bias has a significant effect on the
posterior. We compare the performance of our recognition network
tostate-of-the-art approximate inference algorithms on a large set oftest
cases. In order to evaluate the effect of our simplistic modelof the selection
bias, we evaluate algorithms using a variety ofincorrectly modeled observation
biases. Recognition networks performwell using both correct and incorrect
observation biases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2296</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2296</id><created>2013-01-10</created><authors><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author><author><keyname>Weiss</keyname><forenames>Yair</forenames></author></authors><title>The Factored Frontier Algorithm for Approximate Inference in DBNs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-378-385</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Factored Frontier (FF) algorithm is a simple approximate
inferencealgorithm for Dynamic Bayesian Networks (DBNs). It is very similar
tothe fully factorized version of the Boyen-Koller (BK) algorithm, butinstead
of doing an exact update at every step followed bymarginalisation (projection),
it always works with factoreddistributions. Hence it can be applied to models
for which the exactupdate step is intractable. We show that FF is equivalent to
(oneiteration of) loopy belief propagation (LBP) on the original DBN, andthat
BK is equivalent (to one iteration of) LBP on a DBN where wecluster some of the
nodes. We then show empirically that byiterating, LBP can improve on the
accuracy of both FF and BK. Wecompare these algorithms on two real-world DBNs:
the first is a modelof a water treatment plant, and the second is a coupled
HMM, used tomodel freeway traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2297</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2297</id><created>2013-01-10</created><authors><author><keyname>Nicholson</keyname><forenames>Ann</forenames></author><author><keyname>Boneh</keyname><forenames>Tal</forenames></author><author><keyname>Wilkin</keyname><forenames>Tim</forenames></author><author><keyname>Stacey</keyname><forenames>Kaye</forenames></author><author><keyname>Sonenberg</keyname><forenames>Liz</forenames></author><author><keyname>Steinle</keyname><forenames>Vicki</forenames></author></authors><title>A Case Study in Knowledge Discovery and Elicitation in an Intelligent
  Tutoring Application</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-386-394</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most successful Bayesian network (BN) applications to datehave been built
through knowledge elicitation from experts.This is difficult and time
consuming, which has lead to recentinterest in automated methods for learning
BNs from data. We present a case study in the construction of a BN in
anintelligent tutoring application, specifically decimal misconceptions.
Wedescribe the BN construction using expert elicitation and then investigate
how certainexisting automated knowledge discovery methods might support the BN
knowledge engineering process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2298</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2298</id><created>2013-01-10</created><authors><author><keyname>Ormoneit</keyname><forenames>Dirk</forenames></author><author><keyname>Lemieux</keyname><forenames>Christiane</forenames></author><author><keyname>Fleet</keyname><forenames>David J.</forenames></author></authors><title>Lattice Particle Filters</title><categories>cs.AI cs.CV</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-395-402</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A standard approach to approximate inference in state-space models isto apply
a particle filter, e.g., the Condensation Algorithm.However, the performance of
particle filters often varies significantlydue to their stochastic nature.We
present a class of algorithms, called lattice particle filters, thatcircumvent
this difficulty by placing the particles deterministicallyaccording to a
Quasi-Monte Carlo integration rule.We describe a practical realization of this
idea, discuss itstheoretical properties, and its efficiency.Experimental
results with a synthetic 2D tracking problem show that thelattice particle
filter is equivalent to a conventional particle filterthat has between 10 and
60% more particles, depending ontheir &quot;sparsity&quot; in the state-space.We also
present results on inferring 3D human motion frommoving light displays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2299</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2299</id><created>2013-01-10</created><authors><author><keyname>Park</keyname><forenames>James D.</forenames></author><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author></authors><title>Approximating MAP using Local Search</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-403-410</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MAP is the problem of finding a most probable instantiation of a set of
variables in a Bayesian network, given evidence. Unlike computing marginals,
posteriors, and MPE (a special case of MAP), the time and space complexity of
MAP is not only exponential in the network treewidth, but also in a larger
parameter known as the &quot;constrained&quot; treewidth. In practice, this means that
computing MAP can be orders of magnitude more expensive than
computingposteriors or MPE. Thus, practitioners generally avoid MAP
computations, resorting instead to approximating them by the most likely value
for each MAP variableseparately, or by MPE.We present a method for
approximating MAP using local search. This method has space complexity which is
exponential onlyin the treewidth, as is the complexity of each search step. We
investigate the effectiveness of different local searchmethods and several
initialization strategies and compare them to otherapproximation
schemes.Experimental results show that local search provides a much more
accurate approximation of MAP, while requiring few search steps.Practically,
this means that the complexity of local search is often exponential only in
treewidth as opposed to the constrained treewidth, making approximating MAP as
efficient as other computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2300</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2300</id><created>2013-01-10</created><authors><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Direct and Indirect Effects</title><categories>cs.AI stat.ME</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-411-420</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The direct effect of one eventon another can be defined and measured
byholding constant all intermediate variables between the two.Indirect effects
present conceptual andpractical difficulties (in nonlinear models), because
they cannot be isolated by holding certain variablesconstant. This paper shows
a way of defining any path-specific effectthat does not invoke blocking the
remainingpaths.This permits the assessment of a more naturaltype of direct and
indirect effects, one thatis applicable in both linear and nonlinear models.
The paper establishesconditions under which such assessments can be estimated
consistentlyfrom experimental and nonexperimental data,and thus extends
path-analytic techniques tononlinear and nonparametric models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2301</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2301</id><created>2013-01-10</created><authors><author><keyname>Pfeffer</keyname><forenames>Avi</forenames></author></authors><title>Sufficiency, Separability and Temporal Probabilistic Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-421-428</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose we are given the conditional probability of one variable given some
other variables.Normally the full joint distribution over the conditioning
variablesis required to determine the probability of the conditioned
variable.Under what circumstances are the marginal distributions over the
conditioning variables sufficient to determine the probability ofthe
conditioned variable?Sufficiency in this sense is equivalent to additive
separability ofthe conditional probability distribution.Such separability
structure is natural and can be exploited forefficient inference.Separability
has a natural generalization to conditional separability.Separability provides
a precise notion of weaklyinteracting subsystems in temporal probabilistic
models.Given a system that is decomposed into separable subsystems,
exactmarginal probabilities over subsystems at future points in time can
becomputed by propagating marginal subsystem probabilities, rather thancomplete
system joint probabilities.Thus, separability can make exact prediction
tractable.However, observations can break separability,so exact monitoring of
dynamic systems remains hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2302</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2302</id><created>2013-01-10</created><authors><author><keyname>Pless</keyname><forenames>Daniel</forenames></author><author><keyname>Luger</keyname><forenames>George</forenames></author></authors><title>Toward General Analysis of Recursive Probability Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-429-436</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is increasing interest within the research community in the design and
use of recursive probability models. Although there still remains concern about
computational complexity costs and the fact that computing exact solutions can
be intractable for many nonrecursive models and impossible in the general case
for recursive problems, several research groups are actively developing
computational techniques for recursive stochastic languages. We have developed
an extension to the traditional lambda-calculus as a framework for families of
Turing complete stochastic languages. We have also developed a class of exact
inference algorithms based on the traditional reductions of the
lambda-calculus. We further propose that using the deBruijn notation (a
lambda-calculus notation with nameless dummies) supports effective caching in
such systems (caching being an essential component of efficient computation).
Finally, our extension to the lambda-calculus offers a foundation and general
theory for the construction of recursive stochastic modeling languages as well
as promise for effective caching and efficient approximation algorithms for
inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2303</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2303</id><created>2013-01-10</created><authors><author><keyname>Popescul</keyname><forenames>Alexandrin</forenames></author><author><keyname>Ungar</keyname><forenames>Lyle H.</forenames></author><author><keyname>Pennock</keyname><forenames>David M</forenames></author><author><keyname>Lawrence</keyname><forenames>Steve</forenames></author></authors><title>Probabilistic Models for Unified Collaborative and Content-Based
  Recommendation in Sparse-Data Environments</title><categories>cs.IR cs.LG stat.ML</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-437-444</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems leverage product and community information to target
products to consumers. Researchers have developed collaborative recommenders,
content-based recommenders, and (largely ad-hoc) hybrid systems. We propose a
unified probabilistic framework for merging collaborative and content-based
recommendations. We extend Hofmann's [1999] aspect model to incorporate
three-way co-occurrence data among users, items, and item content. The relative
influence of collaboration data versus content data is not imposed as an
exogenous parameter, but rather emerges naturally from the given data sources.
Global probabilistic models coupled with standard Expectation Maximization (EM)
learning algorithms tend to drastically overfit in sparse-data situations, as
is typical in recommendation applications. We show that secondary content
information can often be used to overcome sparsity. Experiments on data from
the ResearchIndex library of Computer Science publications show that
appropriate mixture models incorporating secondary data produce significantly
better quality recommenders than k-nearest neighbors (k-NN). Global
probabilistic models also allow more general inferences than local methods like
k-NN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2304</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2304</id><created>2013-01-10</created><authors><author><keyname>Poupart</keyname><forenames>Pascal</forenames></author><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author></authors><title>Vector-space Analysis of Belief-state Approximation for POMDPs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-445-452</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new approach to value-directed belief state approximation for
POMDPs. The value-directed model allows one to choose approximation methods for
belief state monitoring that have a small impact on decision quality. Using a
vector space analysis of the problem, we devise two new search procedures for
selecting an approximation scheme that have much better computational
properties than existing methods. Though these provide looser error bounds, we
show empirically that they have a similar impact on decision quality in
practice, and run up to two orders of magnitude more quickly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2305</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2305</id><created>2013-01-10</created><authors><author><keyname>Poupart</keyname><forenames>Pascal</forenames></author><author><keyname>Ortiz</keyname><forenames>Luis E.</forenames></author><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author></authors><title>Value-Directed Sampling Methods for POMDPs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-453-461</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of approximate belief-state monitoring using particle
filtering for the purposes of implementing a policy for a partially-observable
Markov decision process (POMDP). While particle filtering has become a
widely-used tool in AI for monitoring dynamical systems, rather scant attention
has been paid to their use in the context of decision making. Assuming the
existence of a value function, we derive error bounds on decision quality
associated with filtering using importance sampling. We also describe an
adaptive procedure that can be used to dynamically determine the number of
samples required to meet specific error bounds. Empirical evidence is offered
supporting this technique as a profitable means of directing sampling effort
where it is needed to distinguish policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2306</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2306</id><created>2013-01-10</created><authors><author><keyname>Raphael</keyname><forenames>Christopher S</forenames></author></authors><title>A Mixed Graphical Model for Rhythmic Parsing</title><categories>cs.AI cs.SD</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-462-471</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method is presented for the rhythmic parsing problem: Given a sequence of
observed musical note onset times, we estimate the corresponding notated rhythm
and tempo process. A graphical model is developed that represents the
simultaneous evolution of tempo and rhythm and relates these hidden quantities
to observations. The rhythm variables are discrete and the tempo and
observation variables are continuous. We show how to compute the globally most
likely configuration of the tempo and rhythm variables given an observation of
note onset times. Preliminary experiments are presented on a small data set. A
generalization to arbitrary conditional Gaussian distributions is outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2307</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2307</id><created>2013-01-10</created><authors><author><keyname>Rohanimanesh</keyname><forenames>Khashayar</forenames></author><author><keyname>Mahadevan</keyname><forenames>Sridhar</forenames></author></authors><title>Decision-Theoretic Planning with Concurrent Temporally Extended Actions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-472-479</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a model for planning under uncertainty with temporallyextended
actions, where multiple actions can be taken concurrently at each decision
epoch. Our model is based on the options framework, and combines it with
factored state space models,where the set of options can be partitioned into
classes that affectdisjoint state variables. We show that the set of
decisionepochs for concurrent options defines a semi-Markov decisionprocess, if
the underlying temporally extended actions being parallelized arerestricted to
Markov options. This property allows us to use SMDPalgorithms for computing the
value function over concurrentoptions. The concurrent options model allows
overlapping execution ofoptions in order to achieve higher performance or in
order to performa complex task. We describe a simple experiment using a
navigationtask which illustrates how concurrent options results in a faster
planwhen compared to the case when only one option is taken at a time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2308</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2308</id><created>2013-01-10</created><authors><author><keyname>Rusmevichientong</keyname><forenames>Paat</forenames></author><author><keyname>van Roy</keyname><forenames>Benjamin</forenames></author></authors><title>A Tractable POMDP for a Class of Sequencing Problems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-480-487</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a partially observable Markov decision problem (POMDP) that
models a class of sequencing problems. Although POMDPs are typically
intractable, our formulation admits tractable solution. Instead of maintaining
a value function over a high-dimensional set of belief states, we reduce the
state space to one of smaller dimension, in which grid-based dynamic
programming techniques are effective. We develop an error bound for the
resulting approximation, and discuss an application of the model to a problem
in targeted advertising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2309</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2309</id><created>2013-01-10</created><authors><author><keyname>Sharma</keyname><forenames>Rita</forenames></author><author><keyname>Poole</keyname><forenames>David L</forenames></author></authors><title>Symmetric Collaborative Filtering Using the Noisy Sensor Model</title><categories>cs.IR cs.LG</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-488-495</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative filtering is the process of making recommendations regarding
the potential preference of a user, for example shopping on the Internet, based
on the preference ratings of the user and a number of other users for various
items. This paper considers collaborative filtering based on
explicitmulti-valued ratings. To evaluate the algorithms, weconsider only {em
pure} collaborative filtering, using ratings exclusively, and no other
information about the people or items.Our approach is to predict a user's
preferences regarding a particularitem by using other people who rated that
item and other items ratedby the user as noisy sensors. The noisy sensor model
uses Bayes' theorem to compute the probability distribution for the
user'srating of a new item. We give two variant models: in one, we learn a{em
classical normal linear regression} model of how users rate items; in
another,we assume different users rate items the same, but the accuracy of
thesensors needs to be learned. We compare these variant models
withstate-of-the-art techniques and show how they are significantly
better,whether a user has rated only two items or many. We reportempirical
results using the EachMovie database
footnote{http://research.compaq.com/SRC/eachmovie/} of movie ratings. Wealso
show that by considering items similarity along with theusers similarity, the
accuracy of the prediction increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2310</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2310</id><created>2013-01-10</created><authors><author><keyname>Shelton</keyname><forenames>Christian R.</forenames></author></authors><title>Policy Improvement for POMDPs Using Normalized Importance Sampling</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-496-503</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method for estimating the expected return of a POMDP from
experience. The method does not assume any knowledge of the POMDP and allows
the experience to be gathered from an arbitrary sequence of policies. The
return is estimated for any new policy of the POMDP. We motivate the estimator
from function-approximation and importance sampling points-of-view and derive
its theoretical properties. Although the estimator is biased, it has low
variance and the bias is often irrelevant when the estimator is used for
pair-wise comparisons. We conclude by extending the estimator to policies with
memory and compare its performance in a greedy search algorithm to REINFORCE
algorithms showing an order of magnitude reduction in the number of trials
required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2311</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2311</id><created>2013-01-10</created><authors><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>Maximum Likelihood Bounded Tree-Width Markov Networks</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-504-511</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chow and Liu (1968) studied the problem of learning a maximumlikelihood
Markov tree. We generalize their work to more complexMarkov networks by
considering the problem of learning a maximumlikelihood Markov network of
bounded complexity. We discuss howtree-width is in many ways the appropriate
measure of complexity andthus analyze the problem of learning a maximum
likelihood Markovnetwork of bounded tree-width.Similar to the work of Chow and
Liu, we are able to formalize thelearning problem as a combinatorial
optimization problem on graphs. Weshow that learning a maximum likelihood
Markov network of boundedtree-width is equivalent to finding a maximum weight
hypertree. Thisequivalence gives rise to global, integer-programming
based,approximation algorithms with provable performance guarantees, for
thelearning problem. This contrasts with heuristic local-searchalgorithms which
were previously suggested (e.g. by Malvestuto 1991).The equivalence also allows
us to study the computational hardness ofthe learning problem. We show that
learning a maximum likelihoodMarkov network of bounded tree-width is NP-hard,
and discuss thehardness of approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2312</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2312</id><created>2013-01-10</created><authors><author><keyname>Tian</keyname><forenames>Jin</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Causal Discovery from Changes</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-512-521</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method of discovering causal structures, based on the
detection of local, spontaneous changes in the underlying data-generating
model. We analyze the classes of structures that are equivalent relative to a
stream of distributions produced by local changes, and devise algorithms that
output graphical representations of these equivalence classes. We present
experimental results, using simulated data, and examine the errors associated
with detection of changes and recovery of structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2313</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2313</id><created>2013-01-10</created><authors><author><keyname>Van Allen</keyname><forenames>Tim</forenames></author><author><keyname>Greiner</keyname><forenames>Russell</forenames></author><author><keyname>Hooper</keyname><forenames>Peter</forenames></author></authors><title>Bayesian Error-Bars for Belief Net Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-522-529</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Bayesian Belief Network (BN) is a model of a joint distribution over a
setof n variables, with a DAG structure to represent the immediate
dependenciesbetween the variables, and a set of parameters (aka CPTables) to
represent thelocal conditional probabilities of a node, given each assignment
to itsparents. In many situations, these parameters are themselves random
variables - this may reflect the uncertainty of the domain expert, or may come
from atraining sample used to estimate the parameter values. The distribution
overthese &quot;CPtable variables&quot; induces a distribution over the response the
BNwill return to any &quot;What is Pr(H | E)?&quot; query. This paper investigates
thevariance of this response, showing first that it is asymptotically
normal,then providing its mean and asymptotical variance. We then present
aneffective general algorithm for computing this variance, which has the
samecomplexity as simply computing the (mean value of) the response itself -
ie,O(n 2^w), where n is the number of variables and w is the effective
treewidth. Finally, we provide empirical evidence that this algorithm,
whichincorporates assumptions and approximations, works effectively in
practice,given only small samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2314</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2314</id><created>2013-01-10</created><authors><author><keyname>van der Gaag</keyname><forenames>Linda C.</forenames></author><author><keyname>Renooij</keyname><forenames>Silja</forenames></author></authors><title>Analysing Sensitivity Data from Probabilistic Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-530-537</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advance of efficient analytical methods for sensitivity analysis
ofprobabilistic networks, the interest in the sensitivities revealed by
real-life networks is rekindled. As the amount of data resulting from a
sensitivity analysis of even a moderately-sized network is alreadyoverwhelming,
methods for extracting relevant information are called for. One such methodis
to study the derivative of the sensitivity functions yielded for a network's
parameters. We further propose to build upon the concept of admissible
deviation, that is, the extent to which a parameter can deviate from the true
value without inducing a change in the most likely outcome. We illustrate these
concepts by means of a sensitivity analysis of a real-life probabilistic
network in oncology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2315</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2315</id><created>2013-01-10</created><authors><author><keyname>Weaver</keyname><forenames>Lex</forenames></author><author><keyname>Tao</keyname><forenames>Nigel</forenames></author></authors><title>The Optimal Reward Baseline for Gradient-Based Reinforcement Learning</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-538-545</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There exist a number of reinforcement learning algorithms which learnby
climbing the gradient of expected reward. Their long-runconvergence has been
proved, even in partially observableenvironments with non-deterministic
actions, and without the need fora system model. However, the variance of the
gradient estimator hasbeen found to be a significant practical problem. Recent
approacheshave discounted future rewards, introducing a bias-variance
trade-offinto the gradient estimate. We incorporate a reward baseline into
thelearning system, and show that it affects variance without
introducingfurther bias. In particular, as we approach the
zero-bias,high-variance parameterization, the optimal (or variance
minimizing)constant reward baseline is equal to the long-term average
expectedreward. Modified policy-gradient algorithms are presented, and anumber
of experiments demonstrate their improvement over previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2316</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2316</id><created>2013-01-10</created><authors><author><keyname>Wegelin</keyname><forenames>Jacob A.</forenames></author><author><keyname>Richardson</keyname><forenames>Thomas S.</forenames></author></authors><title>Cross-covariance modelling via DAGs with hidden variables</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-546-553</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DAG models with hidden variables present many difficulties that are not
present when all nodes are observed. In particular, fully observed DAG models
are identified and correspond to well-defined sets ofdistributions, whereas
this is not true if nodes are unobserved. Inthis paper we characterize exactly
the set of distributions given by a class of one-dimensional Gaussian latent
variable models. These models relate two blocks of observed variables, modeling
only the cross-covariance matrix. We describe the relation of this model to the
singular value decomposition of the cross-covariance matrix. We show that,
although the model is underidentified, useful information may be extracted. We
further consider an alternative parametrization in which one latent variable is
associated with each block. Our analysis leads to some novel covariance
equivalence results for Gaussian hidden variable models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2317</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2317</id><created>2013-01-10</created><authors><author><keyname>Welling</keyname><forenames>Max</forenames></author><author><keyname>Teh</keyname><forenames>Yee Whye</forenames></author></authors><title>Belief Optimization for Binary Networks: A Stable Alternative to Loopy
  Belief Propagation</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-554-561</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel inference algorithm for arbitrary, binary, undirected
graphs. Unlike loopy belief propagation, which iterates fixed point equations,
we directly descend on the Bethe free energy. The algorithm consists of two
phases, first we update the pairwise probabilities, given the marginal
probabilities at each unit,using an analytic expression. Next, we update the
marginal probabilities, given the pairwise probabilities by following the
negative gradient of the Bethe free energy. Both steps are guaranteed to
decrease the Bethe free energy, and since it is lower bounded, the algorithm is
guaranteed to converge to a local minimum. We also show that the Bethe free
energy is equal to the TAP free energy up to second order in the weights. In
experiments we confirm that when belief propagation converges it usually finds
identical solutions as our belief optimization method. However, in cases where
belief propagation fails to converge, belief optimization continues to converge
to reasonable beliefs. The stable nature of belief optimization makes it
ideally suited for learning graphical models from data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2318</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2318</id><created>2013-01-10</created><authors><author><keyname>Young</keyname><forenames>Steve</forenames></author></authors><title>Statistical Modeling in Continuous Speech Recognition (CSR)(Invited
  Talk)</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-562-571</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic continuous speech recognition (CSR) is sufficiently mature that a
variety of real world applications are now possible including large vocabulary
transcription and interactive spoken dialogues. This paper reviews the
evolution of the statistical modelling techniques which underlie current-day
systems, specifically hidden Markov models (HMMs) and N-grams. Starting from a
description of the speech signal and its parameterisation, the various
modelling assumptions and their consequences are discussed. It then describes
various techniques by which the effects of these assumptions can be mitigated.
Despite the progress that has been made, the limitations of current modelling
techniques are still evident. The paper therefore concludes with a brief review
of some of the more fundamental modelling work now in progress.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2319</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2319</id><created>2013-01-10</created><authors><author><keyname>Zhang</keyname><forenames>Bo</forenames></author><author><keyname>Cai</keyname><forenames>Qingsheng</forenames></author><author><keyname>Mao</keyname><forenames>Jianfeng</forenames></author><author><keyname>Guo</keyname><forenames>Baining</forenames></author></authors><title>Planning and Acting under Uncertainty: A New Model for Spoken Dialogue
  Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-572-579</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uncertainty plays a central role in spoken dialogue systems. Some stochastic
models like Markov decision process (MDP) are used to model the dialogue
manager. But the partially observable system state and user intention hinder
the natural representation of the dialogue state. MDP-based system degrades
fast when uncertainty about a user's intention increases. We propose a novel
dialogue model based on the partially observable Markov decision process
(POMDP). We use hidden system states and user intentions as the state set,
parser results and low-level information as the observation set, domain actions
and dialogue repair actions as the action set. Here the low-level information
is extracted from different input modals, including speech, keyboard, mouse,
etc., using Bayesian networks. Because of the limitation of the exact
algorithms, we focus on heuristic approximation algorithms and their
applicability in POMDP for dialogue management. We also propose two methods for
grid point selection in grid-based approximation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2320</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2320</id><created>2013-01-10</created><authors><author><keyname>Zimdars</keyname><forenames>Andrew</forenames></author><author><keyname>Chickering</keyname><forenames>David Maxwell</forenames></author><author><keyname>Meek</keyname><forenames>Christopher</forenames></author></authors><title>Using Temporal Data for Making Recommendations</title><categories>cs.IR cs.AI cs.LG</categories><comments>Appears in Proceedings of the Seventeenth Conference on Uncertainty
  in Artificial Intelligence (UAI2001)</comments><proxy>auai</proxy><report-no>UAI-P-2001-PG-580-588</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We treat collaborative filtering as a univariate time series estimation
problem: given a user's previous votes, predict the next vote. We describe two
families of methods for transforming data to encode time order in ways amenable
to off-the-shelf classification and density estimation tools, and examine the
results of using these approaches on several real-world data sets. The
improvements in predictive accuracy we realize recommend the use of other
predictive algorithms that exploit the temporal order of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2335</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2335</id><created>2013-01-10</created><authors><author><keyname>Abid</keyname><forenames>Ounasser</forenames></author><author><keyname>Ettanfouhi</keyname><forenames>Jaouad</forenames></author><author><keyname>Khadir</keyname><forenames>Omar</forenames></author></authors><title>New digital signature protocol based on elliptic curves</title><categories>cs.CR cs.IT math.IT</categories><journal-ref>International Journal on Cryptography and Information Security
  (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a new digital signature based on elliptic curves is presented.
We established its efficiency and security. The method, derived from a variant
of ElGamal signature scheme, can be seen as a secure alternative protocol if
known systems are completely broken.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2342</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2342</id><created>2013-01-10</created><updated>2013-01-21</updated><authors><author><keyname>Yu</keyname><forenames>Jingjin</forenames></author></authors><title>A Linear Time Algorithm for the Feasibility of Pebble Motion on Graphs</title><categories>cs.DS cs.RO</categories><comments>Added reference to an earlier linear result on pebble motion on
  graphs by Goraly and Hassin</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a connected, undirected, simple graph $G = (V, E)$ and $p \le |V|$
pebbles labeled $1,..., p$, a configuration of these $p$ pebbles is an
injective map assigning the pebbles to vertices of $G$. Let $S$ and $D$ be two
such configurations. From a configuration, pebbles can move on $G$ as follows:
In each step, at most one pebble may move from the vertex it currently occupies
to an adjacent unoccupied vertex, yielding a new configuration. A natural
question in this setting is the following: Is configuration $D$ reachable from
$S$ and if so, how? We show that the feasibility of this problem can be decided
in time $O(|V| + |E|)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2343</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2343</id><created>2013-01-10</created><authors><author><keyname>van Seijen</keyname><forenames>Harm</forenames></author><author><keyname>Sutton</keyname><forenames>Richard S.</forenames></author></authors><title>Planning by Prioritized Sweeping with Small Backups</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient planning plays a crucial role in model-based reinforcement
learning. Traditionally, the main planning operation is a full backup based on
the current estimates of the successor states. Consequently, its computation
time is proportional to the number of successor states. In this paper, we
introduce a new planning backup that uses only the current value of a single
successor state and has a computation time independent of the number of
successor states. This new backup, which we call a small backup, opens the door
to a new class of model-based reinforcement learning methods that exhibit much
finer control over their planning process than traditional methods. We
empirically demonstrate that this increased flexibility allows for more
efficient planning by showing that an implementation of prioritized sweeping
based on small backups achieves a substantial performance improvement over
classical implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2351</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2351</id><created>2013-01-10</created><authors><author><keyname>Washizawa</keyname><forenames>Teruyoshi</forenames></author></authors><title>Application of Hopfield Network to Saccades</title><categories>cs.CV q-bio.NC</categories><comments>6 pages, 6 figures</comments><journal-ref>IEEE Transactions on NEURAL NETWORKS, vol.4, no.6, pp-995-997,
  NOVEMBER 1993</journal-ref><doi>10.1109/72.286896</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human eye movement mechanisms (saccades) are very useful for scene analysis,
including object representation and pattern recognition. In this letter, a
Hopfield neural network to emulate saccades is proposed. The network uses an
energy function that includes location and identification tasks. Computer
simulation shows that the network performs those tasks cooperatively. The
result suggests that the network is applicable to shift-invariant pattern
recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2354</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2354</id><created>2013-01-10</created><authors><author><keyname>Washizawa</keyname><forenames>Teruyoshi</forenames></author><author><keyname>Asai</keyname><forenames>Akira</forenames></author><author><keyname>Yoshikawa</keyname><forenames>Nobuhiro</forenames></author></authors><title>A New Approach for Solving Singular Systems in Topology Optimization
  Using Krylov Subspace Methods</title><categories>cs.CE math.NA</categories><comments>21 pages, 4 figures</comments><journal-ref>Structural and Multidisciplinary Optimization, vol.28, pp.330-339,
  2004</journal-ref><doi>10.1007/s00158-004-0439-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In topology optimization, the design parameter with no contribution to the
objective function vanishes. This causes the stiffness matrix to become
singular. We show that a local optimal solution is obtained by Conjugate
Residual Method and Conjugate Gradient Method even if the stiffness matrix
becomes singular. We prove that CGMconverges to a local optimal solution in
that case. Computer simulation shows that CGM gives the same solutions obtained
by CRM in case of a cantilever beam problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2362</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2362</id><created>2013-01-10</created><authors><author><keyname>Li</keyname><forenames>Jianxin</forenames></author><author><keyname>Liu</keyname><forenames>Chengfei</forenames></author><author><keyname>Zhou</keyname><forenames>Rui</forenames></author><author><keyname>Yu</keyname><forenames>Jeffrey Xu</forenames></author></authors><title>Quasi-SLCA based Keyword Query Processing over Probabilistic XML Data</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The probabilistic threshold query is one of the most common queries in
uncertain databases, where a result satisfying the query must be also with
probability meeting the threshold requirement. In this paper, we investigate
probabilistic threshold keyword queries (PrTKQ) over XML data, which is not
studied before. We first introduce the notion of quasi-SLCA and use it to
represent results for a PrTKQ with the consideration of possible world
semantics. Then we design a probabilistic inverted (PI) index that can be used
to quickly return the qualified answers and filter out the unqualified ones
based on our proposed lower/upper bounds. After that, we propose two efficient
and comparable algorithms: Baseline Algorithm and PI index-based Algorithm. To
accelerate the performance of algorithms, we also utilize probability density
function. An empirical study using real and synthetic data sets has verified
the effectiveness and the efficiency of our approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2368</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2368</id><created>2013-01-10</created><authors><author><keyname>Iliasov</keyname><forenames>Alexei</forenames></author></authors><title>Event-B/SLP</title><categories>cs.SE</categories><comments>In Proceedings of DS-Event-B 2012: Workshop on the experience of and
  advances in developing dependable systems in Event-B, in conjunction with
  ICFEM 2012 - Kyoto, Japan, November 13, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how the event-based notation offered by Event-B may be augmented by
algorithmic modelling constructs without disrupting the refinement-based
development process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2369</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2369</id><created>2013-01-10</created><updated>2013-01-31</updated><authors><author><keyname>Hayashi</keyname><forenames>Yukio</forenames></author><author><keyname>Komaki</keyname><forenames>Takayuki</forenames></author><author><keyname>Ide</keyname><forenames>Yusuke</forenames></author><author><keyname>Machida</keyname><forenames>Takuya</forenames></author><author><keyname>Konno</keyname><forenames>Norio</forenames></author></authors><title>Combinatorial and approximative analyses in a spatially random division
  process</title><categories>cond-mat.stat-mech cs.DM cs.SI math-ph math.MP nlin.AO</categories><comments>23 pages, 10 figures, 1 table</comments><journal-ref>Physica A 392(9), 2212-2225, 2013</journal-ref><doi>10.1016/j.physa.2013.01.025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a spatial characteristic, there exist commonly fat-tail frequency
distributions of fragment-size and -mass of glass, areas enclosed by city
roads, and pore size/volume in random packings. In order to give a new
analytical approach for the distributions, we consider a simple model which
constructs a fractal-like hierarchical network based on random divisions of
rectangles. The stochastic process makes a Markov chain and corresponds to
directional random walks with splitting into four particles. We derive a
combinatorial analytical form and its continuous approximation for the
distribution of rectangle areas, and numerically show a good fitting with the
actual distribution in the averaging behavior of the divisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2375</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2375</id><created>2013-01-10</created><authors><author><keyname>Li</keyname><forenames>Jianxin</forenames></author><author><keyname>Liu</keyname><forenames>Chengfei</forenames></author><author><keyname>Yao</keyname><forenames>Liang</forenames></author><author><keyname>Yu</keyname><forenames>Jeffrey Xu</forenames></author></authors><title>Context-based Diversification for Keyword Queries over XML Data</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While keyword query empowers ordinary users to search vast amount of data,
the ambiguity of keyword query makes it difficult to effectively answer keyword
queries, especially for short and vague keyword queries. To address this
challenging problem, in this paper we propose an approach that automatically
diversifies XML keyword search based on its different contexts in the XML data.
Given a short and vague keyword query and XML data to be searched, we firstly
derive keyword search candidates of the query by a classifical feature
selection model. And then, we design an effective XML keyword search
diversification model to measure the quality of each candidate. After that,
three efficient algorithms are proposed to evaluate the possible generated
query candidates representing the diversified search intentions, from which we
can find and return top-$k$ qualified query candidates that are most relevant
to the given keyword query while they can cover maximal number of distinct
results.At last, a comprehensive evaluation on real and synthetic datasets
demonstrates the effectiveness of our proposed diversification model and the
efficiency of our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2378</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2378</id><created>2013-01-10</created><authors><author><keyname>Li</keyname><forenames>Jianxin</forenames></author><author><keyname>Liu</keyname><forenames>Chengfei</forenames></author><author><keyname>Yao</keyname><forenames>Liang</forenames></author><author><keyname>Yu</keyname><forenames>Jeffrey Xu</forenames></author><author><keyname>Zhou</keyname><forenames>Rui</forenames></author></authors><title>Query-driven Frequent Co-occurring Term Extraction over Relational Data
  using MapReduce</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study how to efficiently compute \textit{frequent
co-occurring terms} (FCT) in the results of a keyword query in parallel using
the popular MapReduce framework. Taking as input a keyword query q and an
integer k, an FCT query reports the k terms that are not in q, but appear most
frequently in the results of the keyword query q over multiple joined
relations. The returned terms of FCT search can be used to do query expansion
and query refinement for traditional keyword search. Different from the method
of FCT search in a single platform, our proposed approach can efficiently
answer a FCT query using the MapReduce Paradigm without pre-computing the
results of the original keyword query, which is run in parallel platform. In
this work, we can output the final FCT search results by two MapReduce jobs:
the first is to extract the statistical information of the data; and the second
is to calculate the total frequency of each term based on the output of the
first job. At the two MapReduce jobs, we would guarantee the load balance of
mappers and the computational balance of reducers as much as possible.
Analytical and experimental evaluations demonstrate the efficiency and
scalability of our proposed approach using TPC-H benchmark datasets with
different sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2383</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2383</id><created>2013-01-10</created><authors><author><keyname>Zhang</keyname><forenames>Hong-fei</forenames><affiliation>IEEE member</affiliation></author><author><keyname>Wang</keyname><forenames>Jian</forenames><affiliation>IEEE member</affiliation></author><author><keyname>Cui</keyname><forenames>Ke</forenames></author><author><keyname>Luo</keyname><forenames>Chun-li</forenames></author><author><keyname>Lin</keyname><forenames>Sheng-zhao</forenames></author><author><keyname>Zhou</keyname><forenames>Lei</forenames></author><author><keyname>Liang</keyname><forenames>Hao</forenames></author><author><keyname>Chen</keyname><forenames>Teng-yun</forenames></author><author><keyname>Chen</keyname><forenames>Kai</forenames></author><author><keyname>Pan</keyname><forenames>Jian-wei</forenames></author></authors><title>A real-time QKD system based on FPGA</title><categories>quant-ph cs.CR</categories><comments>7 pages, 14 figures</comments><journal-ref>IEEE/OSA Journal of Lightwave Technology, Vol. 30, Issue 20, pp.
  3226-3234 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A real-time Quantum Key Distribution System is developed in this paper. In
the system, based on the feature of Field Programmable Gate Array (FPGA),
secure key extraction control and algorithm have been optimally designed to
perform sifting, error correction and privacy amplification altogether in
real-time. In the QKD experiment information synchronization mechanism and
high-speed classic data channel are designed to ensure the steady operation of
the system. Decoy state and synchronous laser light source are used in the
system, while the length of optical fiber between Alice and Bob is 20 km. With
photons repetition frequency of 20 MHz, the final key rate could reach 17 kbps.
Smooth and robust operation is verified with 6-hour continuous test and
associated with encrypted voice communication test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2390</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2390</id><created>2013-01-11</created><authors><author><keyname>Mehta</keyname><forenames>Shashank K</forenames></author><author><keyname>Aurora</keyname><forenames>Pawan</forenames></author></authors><title>Completely Positive formulation of the Graph Isomorphism Problem</title><categories>cs.DS math.CO</categories><msc-class>68W01</msc-class><acm-class>G.1.6; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two graphs $G_1$ and $G_2$ on $n$ vertices each, we define a graph $G$
on vertex set $V_1\times V_2$ and the edge set as the union of edges of
$G_1\times \bar{G_2}$, $\bar{G_1}\times G_2$, $\{(v,u'),(v,u&quot;))(|u',u&quot;\in
V_2\}$ for each $v\in V_1$, and $\{((u',v),(u&quot;,v))|u',u&quot;\in V_1\}$ for each
$v\in V_2$. We consider the completely-positive Lov\'asz $\vartheta$ function,
i.e., $cp\vartheta$ function for $G$. We show that the function evaluates to
$n$ whenever $G_1$ and $G_2$ are isomorphic and to less than $n-1/(4n^4)$ when
non-isomorphic. Hence this function provides a test for graph isomorphism. We
also provide some geometric insight into the feasible region of the completely
positive program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2397</identifier>
 <datestamp>2013-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2397</id><created>2013-01-11</created><updated>2013-10-07</updated><authors><author><keyname>Ehsani</keyname><forenames>Sepehr</forenames></author></authors><title>Macro-trends in research on the central dogma of molecular biology</title><categories>q-bio.QM cs.DL</categories><comments>9 pages, 2 figures, 1 supplementary table, 3 supplementary figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The central dogma of molecular biology, formulated more than five decades
ago, compartmentalized information exchange in the cell into the DNA, RNA and
protein domains. This formalization has served as an implicit thematic
distinguisher for cell biological research ever since. However, a clear account
of the distribution of research across this formalization over time does not
exist. Abstracts of &gt;3.5 million publications focusing on the cell from 1975 to
2011 were analyzed for the frequency of 100 single-word DNA-, RNA- and
protein-centric search terms and amalgamated to produce domain- and
subdomain-specific trends. A preponderance of protein- over DNA- and in turn
over RNA-centric terms as a percentage of the total word count is evident until
the early 1990s, at which point the trends for protein and DNA begin to
coalesce while RNA percentages remain relatively unchanged. This term-based
census provides a yearly snapshot of the distribution of research interests
across the three domains of the central dogma of molecular biology. A frequency
chart of the most dominantly-studied elements of the periodic table is provided
as an addendum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2405</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2405</id><created>2013-01-11</created><authors><author><keyname>Tilahun</keyname><forenames>Gelila</forenames></author><author><keyname>Feuerverger</keyname><forenames>Andrey</forenames></author><author><keyname>Gervers</keyname><forenames>Michael</forenames></author></authors><title>Dating medieval English charters</title><categories>stat.AP cs.CL</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOAS566 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS566</report-no><journal-ref>Annals of Applied Statistics 2012, Vol. 6, No. 4, 1615-1640</journal-ref><doi>10.1214/12-AOAS566</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deeds, or charters, dealing with property rights, provide a continuous
documentation which can be used by historians to study the evolution of social,
economic and political changes. This study is concerned with charters (written
in Latin) dating from the tenth through early fourteenth centuries in England.
Of these, at least one million were left undated, largely due to administrative
changes introduced by William the Conqueror in 1066. Correctly dating such
charters is of vital importance in the study of English medieval history. This
paper is concerned with computer-automated statistical methods for dating such
document collections, with the goal of reducing the considerable efforts
required to date them manually and of improving the accuracy of assigned dates.
Proposed methods are based on such data as the variation over time of word and
phrase usage, and on measures of distance between documents. The extensive (and
dated) Documents of Early England Data Set (DEEDS) maintained at the University
of Toronto was used for this purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2427</identifier>
 <datestamp>2013-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2427</id><created>2013-01-11</created><updated>2013-06-11</updated><authors><author><keyname>Thomsen</keyname><forenames>Henning</forenames></author><author><keyname>Pratas</keyname><forenames>Nuno K.</forenames></author><author><keyname>Stefanovic</keyname><forenames>Cedomir</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Analysis of the LTE Access Reservation Protocol for Real-Time Traffic</title><categories>cs.IT cs.NI math.IT</categories><comments>4 Pages, 4 Figures, Accepted in IEEE Communication Letters on the
  20th of May 2013</comments><doi>10.1109/LCOMM.2013.13.130995</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LTE is increasingly seen as a system for serving real-time Machine-to-Machine
(M2M) communication needs. The asynchronous M2M user access in LTE is obtained
through a two-phase access reservation protocol (contention and data phase).
Existing analysis related to these protocols is based on the following
assumptions: (1) there are sufficient resources in the data phase for all
detected contention tokens, and (2) the base station is able to detect
collisions, i.e., tokens activated by multiple users. These assumptions are not
always applicable to LTE - specifically, (1) due to the variable amount of
available data resources caused by variable load, and (2) detection of
collisions in contention phase may not be possible. All of this affects
transmission of real-time M2M traffic, where data packets have to be sent
within a deadline and may have only one contention opportunity. We analyze the
features of the two-phase LTE reservation protocol and derive its throughput,
i.e., the number of successful transmissions in the data phase, when
assumptions (1) and (2) do not hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2443</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2443</id><created>2013-01-11</created><authors><author><keyname>Tantius</keyname><forenames>Richard</forenames></author><author><keyname>Speicher</keyname><forenames>Daniel</forenames></author><author><keyname>Behrend</keyname><forenames>Andreas</forenames></author></authors><title>Towards an Application of Update Propagation on Logic Programs
  Representing Java Source Code</title><categories>cs.SE cs.PL</categories><journal-ref>26th Workshop on Logic Programming (WLP 2012)</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Logic programs are now used as a representation of object-oriented source
code in academic prototypes for about a decade. This representation allows a
clear and concise implementation of analyses of the object-oriented source
code. The full potential of this approach is far from being explored. In this
paper, we report about an application of the well-established theory of update
propagation within logic programs. Given the representation of the
object-oriented code as facts in a logic program, a change to the code
corresponds to an update of these facts. We demonstrate how update propagation
provides a generic way to generate incremental versions of such analyses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2444</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2444</id><created>2013-01-11</created><updated>2016-01-28</updated><authors><author><keyname>Romary</keyname><forenames>Laurent</forenames><affiliation>ALPAGE, CMB</affiliation></author></authors><title>TEI and LMF crosswalks</title><categories>cs.CL</categories><proxy>ccsd</proxy><journal-ref>JLCL - Journal for Language Technology and Computational
  Linguistics, 2015, 30 (1)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present paper explores various arguments in favour of making the Text
Encoding Initia-tive (TEI) guidelines an appropriate serialisation for ISO
standard 24613:2008 (LMF, Lexi-cal Mark-up Framework) . It also identifies the
issues that would have to be resolved in order to reach an appropriate
implementation of these ideas, in particular in terms of infor-mational
coverage. We show how the customisation facilities offered by the TEI
guidelines can provide an adequate background, not only to cover missing
components within the current Dictionary chapter of the TEI guidelines, but
also to allow specific lexical projects to deal with local constraints. We
expect this proposal to be a basis for a future ISO project in the context of
the on going revision of LMF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2447</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2447</id><created>2013-01-11</created><authors><author><keyname>Speicher</keyname><forenames>Daniel</forenames></author><author><keyname>Bremm</keyname><forenames>Andri</forenames></author></authors><title>Clone Removal in Java Programs as a Process of Stepwise Unification</title><categories>cs.SE</categories><journal-ref>26th Workshop on Logic Programming (WLP 2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloned code is one of the most important obstacles against consistent
software maintenance and evolution. Although today's clone detection tools find
a variety of clones, they do not offer any advice how to remove such clones. We
explain the problems involved in finding a sequence of changes for clone
removal and suggest to view this problem as a process of stepwise unification
of the clone instances. Consequently the problem can be solved by backtracking
over the possible unification steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2464</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2464</id><created>2013-01-11</created><authors><author><keyname>Miritello</keyname><forenames>Giovanna</forenames></author><author><keyname>Moro</keyname><forenames>Esteban</forenames></author><author><keyname>Lara</keyname><forenames>Rub&#xe9;n</forenames></author><author><keyname>Mart&#xed;nez-L&#xf3;pez</keyname><forenames>Roc&#xed;o</forenames></author><author><keyname>Roberts</keyname><forenames>Sam G. B.</forenames></author><author><keyname>Dunbar</keyname><forenames>Robin I. M.</forenames></author></authors><title>Time as a limited resource: Communication Strategy in Mobile Phone
  Networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>10 pages, 3 figures. Accepted for publication in Social Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We used a large database of 9 billion calls from 20 million mobile users to
examine the relationships between aggregated time spent on the phone, personal
network size, tie strength and the way in which users distributed their limited
time across their network (disparity). Compared to those with smaller networks,
those with large networks did not devote proportionally more time to
communication and had on average weaker ties (as measured by time spent
communicating). Further, there were not substantially different levels of
disparity between individuals, in that mobile users tend to distribute their
time very unevenly across their network, with a large proportion of calls going
to a small number of individuals. Together, these results suggest that there
are time constraints which limit tie strength in large personal networks, and
that even high levels of mobile communication do not fundamentally alter the
disparity of time allocation across networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2466</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2466</id><created>2013-01-11</created><authors><author><keyname>Sychev</keyname><forenames>Oleg</forenames></author><author><keyname>Mamontov</keyname><forenames>Dmitry</forenames></author></authors><title>Determining token sequence mistakes in responses to questions with open
  text answer</title><categories>cs.CL cs.CY</categories><comments>7 pages</comments><acm-class>K.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When learning grammar of the new language, a teacher should routinely check
student's exercises for grammatical correctness. The paper describes a method
of automatically detecting and reporting grammar mistakes, regarding an order
of tokens in the response. It could report extra tokens, missing tokens and
misplaced tokens. The method is useful when teaching language, where order of
tokens is important, which includes most formal languages and some natural ones
(like English). The method was implemented in a question type plug-in
CorrectWriting for the widely used learning manage system Moodle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2474</identifier>
 <datestamp>2014-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2474</id><created>2013-01-11</created><updated>2014-06-27</updated><authors><author><keyname>Bousquet</keyname><forenames>Nicolas</forenames></author><author><keyname>Lagoutte</keyname><forenames>Aur&#xe9;lie</forenames></author><author><keyname>Thomass&#xe9;</keyname><forenames>St&#xe9;phan</forenames></author></authors><title>Clique versus Independent Set</title><categories>cs.DM math.CO</categories><journal-ref>European Journal of Combinatorics, 40:73-92, 2014</journal-ref><doi>10.1016/j.ejc.2014.02.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Yannakakis' Clique versus Independent Set problem (CL-IS) in communication
complexity asks for the minimum number of cuts separating cliques from stable
sets in a graph, called CS-separator. Yannakakis provides a quasi-polynomial
CS-separator, i.e. of size $O(n^{\log n})$, and addresses the problem of
finding a polynomial CS-separator. This question is still open even for perfect
graphs. We show that a polynomial CS-separator almost surely exists for random
graphs. Besides, if H is a split graph (i.e. has a vertex-partition into a
clique and a stable set) then there exists a constant $c_H$ for which we find a
$O(n^{c_H})$ CS-separator on the class of H-free graphs. This generalizes a
result of Yannakakis on comparability graphs. We also provide a $O(n^{c_k})$
CS-separator on the class of graphs without induced path of length k and its
complement. Observe that on one side, $c_H$ is of order $O(|H| \log |H|)$
resulting from Vapnik-Chervonenkis dimension, and on the other side, $c_k$ is
exponential.
  One of the main reason why Yannakakis' CL-IS problem is fascinating is that
it admits equivalent formulations. Our main result in this respect is to show
that a polynomial CS-separator is equivalent to the polynomial
Alon-Saks-Seymour Conjecture, asserting that if a graph has an edge-partition
into k complete bipartite graphs, then its chromatic number is polynomially
bounded in terms of k. We also show that the classical approach to the stubborn
problem (arising in CSP) which consists in covering the set of all solutions by
$O(n^{\log n})$ instances of 2-SAT is again equivalent to the existence of a
polynomial CS-separator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2476</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2476</id><created>2013-01-11</created><updated>2013-07-02</updated><authors><author><keyname>Panella</keyname><forenames>Federica</forenames></author><author><keyname>Pradella</keyname><forenames>Matteo</forenames></author><author><keyname>Mandrioli</keyname><forenames>Dino</forenames></author><author><keyname>Lonati</keyname><forenames>Violetta</forenames></author></authors><title>Operator Precedence \omega-languages</title><categories>cs.FL</categories><comments>38 pages. Added new proofs regarding the relationships among classes
  of Operator precedence \omega-languages and their closure properties</comments><msc-class>68Q45</msc-class><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  \omega-languages are becoming more and more relevant nowadays when most
applications are 'ever-running'. Recent literature, mainly under the motivation
of widening the application of model checking techniques, extended the analysis
of these languages from the simple regular ones to various classes of languages
with 'visible syntax structure', such as visibly pushdown languages (VPLs).
Operator precedence languages (OPLs), instead, were originally defined to
support deterministic parsing and, though seemingly unrelated, exhibit
interesting relations with these classes of languages: OPLs strictly include
VPLs, enjoy all relevant closure properties and have been characterized by a
suitable automata family and a logic notation. In this paper we introduce
operator precedence \omega-languages (\omega OPLs), investigating various
acceptance criteria and their closure properties. Whereas some properties are
natural extensions of those holding for regular languages, others required
novel investigation techniques. Application-oriented examples show the gain in
expressiveness and verifiability offered by \omega OPLs w.r.t. smaller classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2479</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2479</id><created>2013-01-11</created><authors><author><keyname>Yang</keyname><forenames>Jing</forenames></author><author><keyname>Xiong</keyname><forenames>Maosheng</forenames></author><author><keyname>Ding</keyname><forenames>Cunsheng</forenames></author></authors><title>Weight Distribution of a Class of Cyclic Codes with Arbitrary Number of
  Zeros</title><categories>cs.IT math.IT</categories><comments>8 tables</comments><msc-class>11T71, 11T23</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic codes have been widely used in digital communication systems and
consume electronics as they have efficient encoding and decoding algorithms.
The weight distribution of cyclic codes has been an important topic of study
for many years. It is in general hard to determine the weight distribution of
linear codes. In this paper, a class of cyclic codes with any number of zeros
are described and their weight distributions are determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2481</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2481</id><created>2013-01-11</created><authors><author><keyname>Karl</keyname><forenames>Hubert</forenames></author><author><keyname>Karl</keyname><forenames>Sebstian</forenames></author></authors><title>Zur iterativen Loesosung von linearen Gleichungssystemen</title><categories>cs.NA</categories><comments>15 pages, German, 1 figure, translation in work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that a fixed point iteration for solving a linear equation
system converges if and only if the spectral radius of the iteration matrix is
less than one. A method is presented which guarantees the Fixed Point, even if
this condition is not (&quot;spectral radius &lt;1&quot;) fulfilled and demonstrated through
calculation examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2486</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2486</id><created>2013-01-11</created><authors><author><keyname>Johansson</keyname><forenames>Fredrik</forenames></author><author><keyname>Kauers</keyname><forenames>Manuel</forenames></author><author><keyname>Mezzarobba</keyname><forenames>Marc</forenames></author></authors><title>Finding Hyperexponential Solutions of Linear ODEs by Numerical
  Evaluation</title><categories>cs.SC</categories><acm-class>I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for computing hyperexponential solutions of
ordinary linear differential equations with polynomial coefficients. The
algorithm relies on interpreting formal series solutions at the singular points
as analytic functions and evaluating them numerically at some common ordinary
point. The numerical data is used to determine a small number of combinations
of the formal series that may give rise to hyperexponential solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2495</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2495</id><created>2013-01-11</created><authors><author><keyname>Dutta</keyname><forenames>Akashnil</forenames></author><author><keyname>Levi</keyname><forenames>Reut</forenames></author><author><keyname>Ron</keyname><forenames>Dana</forenames></author><author><keyname>Rubinfeld</keyname><forenames>Ronitt</forenames></author></authors><title>A simple online competitive adaptation of Lempel-Ziv compression with
  efficient random access support</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple adaptation of the Lempel Ziv 78' (LZ78) compression
scheme ({\em IEEE Transactions on Information Theory, 1978}) that supports
efficient random access to the input string. Namely, given query access to the
compressed string, it is possible to efficiently recover any symbol of the
input string. The compression algorithm is given as input a parameter $\eps
&gt;0$, and with very high probability increases the length of the compressed
string by at most a factor of $(1+\eps)$. The access time is $O(\log n +
1/\eps^2)$ in expectation, and $O(\log n/\eps^2)$ with high probability. The
scheme relies on sparse transitive-closure spanners. Any (consecutive)
substring of the input string can be retrieved at an additional additive cost
in the running time of the length of the substring. We also formally establish
the necessity of modifying LZ78 so as to allow efficient random access.
Specifically, we construct a family of strings for which $\Omega(n/\log n)$
queries to the LZ78-compressed string are required in order to recover a single
symbol in the input string. The main benefit of the proposed scheme is that it
preserves the online nature and simplicity of LZ78, and that for {\em every}
input string, the length of the compressed string is only a small factor larger
than that obtained by running LZ78.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2497</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2497</id><created>2013-01-11</created><authors><author><keyname>Han</keyname><forenames>Yunghsiang S.</forenames></author><author><keyname>Pai</keyname><forenames>Hong-Ta</forenames></author><author><keyname>Zheng</keyname><forenames>Rong</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Update-Efficient Regenerating Codes with Minimum Per-Node Storage</title><categories>cs.IT cs.DM math.IT</categories><comments>Submitted to IEEE ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regenerating codes provide an efficient way to recover data at failed nodes
in distributed storage systems. It has been shown that regenerating codes can
be designed to minimize the per-node storage (called MSR) or minimize the
communication overhead for regeneration (called MBR). In this work, we propose
a new encoding scheme for [n,d] error- correcting MSR codes that generalizes
our earlier work on error-correcting regenerating codes. We show that by
choosing a suitable diagonal matrix, any generator matrix of the [n,{\alpha}]
Reed-Solomon (RS) code can be integrated into the encoding matrix. Hence, MSR
codes with the least update complexity can be found. An efficient decoding
scheme is also proposed that utilizes the [n,{\alpha}] RS code to perform data
reconstruction. The proposed decoding scheme has better error correction
capability and incurs the least number of node accesses when errors are
present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2498</identifier>
 <datestamp>2014-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2498</id><created>2013-01-11</created><updated>2014-03-26</updated><authors><author><keyname>Bottegal</keyname><forenames>Giulio</forenames></author><author><keyname>Picci</keyname><forenames>Giorgio</forenames></author></authors><title>Modeling complex systems by Generalized Factor Analysis</title><categories>cs.SY</categories><comments>15 pages, preprint submitted for publication to IEEE Trans. on
  Automatic Control</comments><msc-class>93E12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new modeling paradigm for large dimensional aggregates of
stochastic systems by Generalized Factor Analysis (GFA) models. These models
describe the data as the sum of a flocking plus an uncorrelated idiosyncratic
component. The flocking component describes a sort of collective orderly motion
which admits a much simpler mathematical description than the whole ensemble
while the idiosyncratic component describes weakly correlated noise. We first
discuss static GFA representations and characterize in a rigorous way the
properties of the two components. The extraction of the dynamic flocking
component is discussed for time-stationary linear systems and for a simple
classes of separable random fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2506</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2506</id><created>2013-01-11</created><authors><author><keyname>Telle</keyname><forenames>Jan Arne</forenames></author><author><keyname>Villanger</keyname><forenames>Yngve</forenames></author></authors><title>Connecting Terminals and 2-Disjoint Connected Subgraphs</title><categories>cs.DS</categories><comments>13 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph $G=(V,E)$ and a set of terminal vertices $T$ we say that a
superset $S$ of $T$ is $T$-connecting if $S$ induces a connected graph, and $S$
is minimal if no strict subset of $S$ is $T$-connecting. In this paper we prove
that there are at most ${|V \setminus T| \choose |T|-2} \cdot 3^{\frac{|V
\setminus T|}{3}}$ minimal $T$-connecting sets when $|T| \leq n/3$ and that
these can be enumerated within a polynomial factor of this bound. This
generalizes the algorithm for enumerating all induced paths between a pair of
vertices, corresponding to the case $|T|=2$. We apply our enumeration algorithm
to solve the {\sc 2-Disjoint Connected Subgraphs} problem in time
$O^*(1.7804^n)$, improving on the recent $O^*(1.933^n)$ algorithm of Cygan et
al. 2012 LATIN paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2522</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2522</id><created>2013-01-11</created><updated>2013-01-19</updated><authors><author><keyname>Teif</keyname><forenames>Vladimir B.</forenames></author></authors><title>Science 3.0: Corrections to the Science 2.0 paradigm</title><categories>cs.DL cs.CY</categories><comments>7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of Science 2.0 was introduced almost a decade ago to describe the
new generation of online-based tools for researchers allowing easier data
sharing, collaboration and publishing. Although technically sound, the concept
still does not work as expected. Here we provide a systematic line of arguments
to modify the concept of Science 2.0, making it more consistent with the spirit
and traditions of science and Internet. Our first correction to the Science 2.0
paradigm concerns the open-access publication models charging fees to the
authors. As discussed elsewhere, we show that the monopoly of such publishing
models increases biases and inequalities in the representation of scientific
ideas based on the author's income. Our second correction concerns
post-publication comments online, which are all essentially non-anonymous in
the current Science 2.0 paradigm. We conclude that scientific post-publication
discussions require special anonymization systems. We further analyze the
reasons of the failure of the current post-publication peer-review models and
suggest what needs to be changed in Science 3.0 to convert Internet into a
large journal club.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2533</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2533</id><created>2013-01-11</created><authors><author><keyname>Shakarian</keyname><forenames>Paulo</forenames></author><author><keyname>Roos</keyname><forenames>Patrick</forenames></author><author><keyname>Moores</keyname><forenames>Geoffrey</forenames></author></authors><title>A Novel Analytical Method for Evolutionary Graph Theory Problems</title><categories>cs.GT cs.SI q-bio.PE</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Evolutionary graph theory studies the evolutionary dynamics of populations
structured on graphs. A central problem is determining the probability that a
small number of mutants overtake a population. Currently, Monte Carlo
simulations are used for estimating such fixation probabilities on general
directed graphs, since no good analytical methods exist. In this paper, we
introduce a novel deterministic framework for computing fixation probabilities
for strongly connected, directed, weighted evolutionary graphs under neutral
drift. We show how this framework can also be used to calculate the expected
number of mutants at a given time step (even if we relax the assumption that
the graph is strongly connected), how it can extend to other related models
(e.g. voter model), how our framework can provide non-trivial bounds for
fixation probability in the case of an advantageous mutant, and how it can be
used to find a non-trivial lower bound on the mean time to fixation. We provide
various experimental results determining fixation probabilities and expected
number of mutants on different graphs. Among these, we show that our method
consistently outperforms Monte Carlo simulations in speed by several orders of
magnitude. Finally we show how our approach can provide insight into synaptic
competition in neurology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2542</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2542</id><created>2013-01-10</created><authors><author><keyname>Eisa</keyname><forenames>Mohamed</forenames></author><author><keyname>Eletrebi</keyname><forenames>Amira</forenames></author><author><keyname>Elhenawy</keyname><forenames>Ebrahim</forenames></author></authors><title>Enhancing the retrieval performance by combing the texture and edge
  features</title><categories>cs.CV cs.IR</categories><comments>7 pages,8 figures, one table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, anew algorithm which is based on geometrical moments and local
binary patterns (LBP) for content based image retrieval (CBIR) is proposed. In
geometrical moments, each vector is compared with the all other vectors for
edge map generation. The same concept is utilized at LBP calculation which is
generating nine LBP patterns from a given 3x3 pattern. Finally, nine LBP
histograms are calculated which are used as a feature vector for image
retrieval. Moments are important features used in recognition of different
types of images. Two experiments have been carried out for proving the worth of
our algorithm. The results after being investigated shows a significant
improvement in terms of their evaluation measures as compared to LBP and other
existing transform domain techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2556</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2556</id><created>2013-01-11</created><authors><author><keyname>En&#xdf;lin</keyname><forenames>Torsten</forenames></author></authors><title>Information field theory</title><categories>astro-ph.IM cs.IT math.IT physics.data-an stat.ML</categories><comments>8 pages, in-a-nutshell introduction to information field theory (see
  http://www.mpa-garching.mpg.de/ift), accepted for the proceedings of MaxEnt
  2012, the 32nd International Workshop on Bayesian Inference and Maximum
  Entropy Methods in Science and Engineering</comments><doi>10.1063/1.4819999</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-linear image reconstruction and signal analysis deal with complex inverse
problems. To tackle such problems in a systematic way, I present information
field theory (IFT) as a means of Bayesian, data based inference on spatially
distributed signal fields. IFT is a statistical field theory, which permits the
construction of optimal signal recovery algorithms even for non-linear and
non-Gaussian signal inference problems. IFT algorithms exploit spatial
correlations of the signal fields and benefit from techniques developed to
investigate quantum and statistical field theories, such as Feynman diagrams,
re-normalisation calculations, and thermodynamic potentials. The theory can be
used in many areas, and applications in cosmology and numerics are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2561</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2561</id><created>2013-01-11</created><authors><author><keyname>Sayama</keyname><forenames>Hiroki</forenames></author><author><keyname>Pestov</keyname><forenames>Irene</forenames></author><author><keyname>Schmidt</keyname><forenames>Jeffrey</forenames></author><author><keyname>Bush</keyname><forenames>Benjamin James</forenames></author><author><keyname>Wong</keyname><forenames>Chun</forenames></author><author><keyname>Yamanoi</keyname><forenames>Junichi</forenames></author><author><keyname>Gross</keyname><forenames>Thilo</forenames></author></authors><title>Modeling complex systems with adaptive networks</title><categories>cs.SI nlin.AO physics.soc-ph</categories><comments>24 pages, 11 figures, 3 tables</comments><journal-ref>Computers and Mathematics with Applications, 2013</journal-ref><doi>10.1016/j.camwa.2012.12.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive networks are a novel class of dynamical networks whose topologies
and states coevolve. Many real-world complex systems can be modeled as adaptive
networks, including social networks, transportation networks, neural networks
and biological networks. In this paper, we introduce fundamental concepts and
unique properties of adaptive networks through a brief, non-comprehensive
review of recent literature on mathematical/computational modeling and analysis
of such networks. We also report our recent work on several applications of
computational adaptive network modeling and analysis to real-world problems,
including temporal development of search and rescue operational networks,
automated rule discovery from empirical network evolution data, and cultural
integration in corporate merger.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2603</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2603</id><created>2013-01-11</created><updated>2014-05-23</updated><authors><author><keyname>Soltanolkotabi</keyname><forenames>Mahdi</forenames></author><author><keyname>Elhamifar</keyname><forenames>Ehsan</forenames></author><author><keyname>Cand&#xe8;s</keyname><forenames>Emmanuel J.</forenames></author></authors><title>Robust subspace clustering</title><categories>cs.LG cs.IT math.IT math.OC math.ST stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/13-AOS1199 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1199</report-no><journal-ref>Annals of Statistics 2014, Vol. 42, No. 2, 669-699</journal-ref><doi>10.1214/13-AOS1199</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace clustering refers to the task of finding a multi-subspace
representation that best fits a collection of points taken from a
high-dimensional space. This paper introduces an algorithm inspired by sparse
subspace clustering (SSC) [In IEEE Conference on Computer Vision and Pattern
Recognition, CVPR (2009) 2790-2797] to cluster noisy data, and develops some
novel theory demonstrating its correctness. In particular, the theory uses
ideas from geometric functional analysis to show that the algorithm can
accurately recover the underlying subspaces under minimal requirements on their
orientation, and on the number of samples per subspace. Synthetic as well as
real data experiments complement our theoretical study, illustrating our
approach and demonstrating its effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2604</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2604</id><created>2013-01-10</created><authors><author><keyname>Rubido</keyname><forenames>Nicol&#xe1;s</forenames></author><author><keyname>Grebogi</keyname><forenames>Celso</forenames></author><author><keyname>Baptista</keyname><forenames>Murilo S.</forenames></author></authors><title>Structure and function in flow networks</title><categories>physics.soc-ph cs.SI</categories><comments>5 pages, 4 figures</comments><doi>10.1209/0295-5075/101/68001</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This Letter presents a unified approach for the fundamental relationship
between structure and function in flow networks by solving analytically the
voltages in a resistor network, transforming the network structure to an
effective all-to-all topology, and then measuring the resultant flows.
Moreover, it defines a way to study the structural resilience of the graph and
to detect possible communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2609</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2609</id><created>2013-01-11</created><updated>2014-02-03</updated><authors><author><keyname>Russo</keyname><forenames>Daniel</forenames></author><author><keyname>Van Roy</keyname><forenames>Benjamin</forenames></author></authors><title>Learning to Optimize Via Posterior Sampling</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the use of a simple posterior sampling algorithm to
balance between exploration and exploitation when learning to optimize actions
such as in multi-armed bandit problems. The algorithm, also known as Thompson
Sampling, offers significant advantages over the popular upper confidence bound
(UCB) approach, and can be applied to problems with finite or infinite action
spaces and complicated relationships among action rewards. We make two
theoretical contributions. The first establishes a connection between posterior
sampling and UCB algorithms. This result lets us convert regret bounds
developed for UCB algorithms into Bayesian regret bounds for posterior
sampling. Our second theoretical contribution is a Bayesian regret bound for
posterior sampling that applies broadly and can be specialized to many model
classes. This bound depends on a new notion we refer to as the eluder
dimension, which measures the degree of dependence among action rewards.
Compared to UCB algorithm Bayesian regret bounds for specific model classes,
our general bound matches the best available for linear models and is stronger
than the best available for generalized linear models. Further, our analysis
provides insight into performance advantages of posterior sampling, which are
highlighted through simulation results that demonstrate performance surpassing
recently proposed UCB algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2613</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2613</id><created>2013-01-11</created><authors><author><keyname>Huang</keyname><forenames>Yichao</forenames></author><author><keyname>Rao</keyname><forenames>Bhaskar D.</forenames></author></authors><title>An Analytical Framework for Heterogeneous Partial Feedback Design in
  Heterogeneous Multicell OFDMA Networks</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Trans. on Signal Processing</comments><doi>10.1109/TSP.2012.2226447</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The inherent heterogeneous structure resulting from user densities and large
scale channel effects motivates heterogeneous partial feedback design in
heterogeneous networks. In such emerging networks, a distributed scheduling
policy which enjoys multiuser diversity as well as maintains fairness among
users is favored for individual user rate enhancement and guarantees. For a
system employing the cumulative distribution function based scheduling, which
satisfies the two above mentioned desired features, we develop an analytical
framework to investigate heterogeneous partial feedback in a general
OFDMA-based heterogeneous multicell employing the best-M partial feedback
strategy. Exact sum rate analysis is first carried out and closed form
expressions are obtained by a novel decomposition of the probability density
function of the selected user's signal-to-interference-plus-noise ratio. To
draw further insight, we perform asymptotic analysis using extreme value theory
to examine the effect of partial feedback on the randomness of multiuser
diversity, show the asymptotic optimality of best-1 feedback, and derive an
asymptotic approximation for the sum rate in order to determine the minimum
required partial feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2626</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2626</id><created>2013-01-11</created><authors><author><keyname>Woods</keyname><forenames>Damien</forenames></author><author><keyname>Chen</keyname><forenames>Ho-Lin</forenames></author><author><keyname>Goodfriend</keyname><forenames>Scott</forenames></author><author><keyname>Dabby</keyname><forenames>Nadine</forenames></author><author><keyname>Winfree</keyname><forenames>Erik</forenames></author><author><keyname>Yin</keyname><forenames>Peng</forenames></author></authors><title>Active Self-Assembly of Algorithmic Shapes and Patterns in
  Polylogarithmic Time</title><categories>cs.DS cs.CC cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a computational model for studying the complexity of
self-assembled structures with active molecular components. Our model captures
notions of growth and movement ubiquitous in biological systems. The model is
inspired by biology's fantastic ability to assemble biomolecules that form
systems with complicated structure and dynamics, from molecular motors that
walk on rigid tracks and proteins that dynamically alter the structure of the
cell during mitosis, to embryonic development where large-scale complicated
organisms efficiently grow from a single cell. Using this active self-assembly
model, we show how to efficiently self-assemble shapes and patterns from simple
monomers. For example, we show how to grow a line of monomers in time and
number of monomer states that is merely logarithmic in the length of the line.
  Our main results show how to grow arbitrary connected two-dimensional
geometric shapes and patterns in expected time that is polylogarithmic in the
size of the shape, plus roughly the time required to run a Turing machine
deciding whether or not a given pixel is in the shape. We do this while keeping
the number of monomer types logarithmic in shape size, plus those monomers
required by the Kolmogorov complexity of the shape or pattern. This work thus
highlights the efficiency advantages of active self-assembly over passive
self-assembly and motivates experimental effort to construct general-purpose
active molecular self-assembly systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2628</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2628</id><created>2013-01-11</created><updated>2013-06-02</updated><authors><author><keyname>Yin</keyname><forenames>Xu-Cheng</forenames></author><author><keyname>Yin</keyname><forenames>Xuwang</forenames></author><author><keyname>Huang</keyname><forenames>Kaizhu</forenames></author><author><keyname>Hao</keyname><forenames>Hong-Wei</forenames></author></authors><title>Robust Text Detection in Natural Scene Images</title><categories>cs.CV cs.IR cs.LG</categories><comments>A Draft Version (Submitted to IEEE TPAMI)</comments><acm-class>I.5.4</acm-class><journal-ref>IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 36,
  no. 5, pp. 970-983, 2014</journal-ref><doi>10.1109/TPAMI.2013.182</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text detection in natural scene images is an important prerequisite for many
content-based image analysis tasks. In this paper, we propose an accurate and
robust method for detecting texts in natural scene images. A fast and effective
pruning algorithm is designed to extract Maximally Stable Extremal Regions
(MSERs) as character candidates using the strategy of minimizing regularized
variations. Character candidates are grouped into text candidates by the
ingle-link clustering algorithm, where distance weights and threshold of the
clustering algorithm are learned automatically by a novel self-training
distance metric learning algorithm. The posterior probabilities of text
candidates corresponding to non-text are estimated with an character
classifier; text candidates with high probabilities are then eliminated and
finally texts are identified with a text classifier. The proposed system is
evaluated on the ICDAR 2011 Robust Reading Competition dataset; the f measure
is over 76% and is significantly better than the state-of-the-art performance
of 71%. Experimental results on a publicly available multilingual dataset also
show that our proposed method can outperform the other competitive method with
the f measure increase of over 9 percent. Finally, we have setup an online demo
of our proposed scene text detection system at
http://kems.ustb.edu.cn/learning/yin/dtext.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2629</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2629</id><created>2013-01-11</created><authors><author><keyname>Shams</keyname><forenames>Farshad</forenames></author><author><keyname>Luise</keyname><forenames>Marco</forenames></author></authors><title>Upper-Bounding the Capacity of Relay Communications - Part I</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the capacity of point-to-point relay communications
wherein the transmitter is assisted by an intermediate relay. We detail the
mathematical model of cutset and amplify and forward (AF) relaying strategy. We
present the upper bound capacity of each relaying strategy from information
theory viewpoint and also in networks with Gaussian channels. We exemplify
various outer region capacities of the addressed strategies with two different
case studies. The results exhibit that in low signal-to-noise ratio (SNR)
environments the cutset performance is better than amplify and forward
strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2634</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2634</id><created>2013-01-11</created><authors><author><keyname>Zinovyev</keyname><forenames>Andrei</forenames></author><author><keyname>Kairov</keyname><forenames>Ulykbek</forenames></author><author><keyname>Karpenyuk</keyname><forenames>Tatiana</forenames></author><author><keyname>Ramanculov</keyname><forenames>Erlan</forenames></author></authors><title>Blind source separation methods for deconvolution of complex signals in
  cancer biology</title><categories>q-bio.QM cs.CE q-bio.GN</categories><comments>Zinovyev A., Kairov U., Karpenyuk T., Ramanculov E. Blind Source
  Separation Methods For Deconvolution Of Complex Signals In Cancer Biology.
  2012. Biochemical and Biophysical Research Communications. In Press. DOI:
  10.1016/j.bbrc.2012.12.043</comments><journal-ref>2013. Biochemical and Biophysical Research Communications 430(3),
  1182-1187</journal-ref><doi>10.1016/j.bbrc.2012.12.043</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two blind source separation methods (Independent Component Analysis and
Non-negative Matrix Factorization), developed initially for signal processing
in engineering, found recently a number of applications in analysis of
large-scale data in molecular biology. In this short review, we present the
common idea behind these methods, describe ways of implementing and applying
them and point out to the advantages compared to more traditional statistical
approaches. We focus more specifically on the analysis of gene expression in
cancer. The review is finalized by listing available software implementations
for the methods described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2638</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2638</id><created>2013-01-11</created><authors><author><keyname>Yu</keyname><forenames>Tina</forenames></author><author><keyname>Wilkinson</keyname><forenames>Dave</forenames></author><author><keyname>Clark</keyname><forenames>Julian</forenames></author><author><keyname>Sullivan</keyname><forenames>Morgan</forenames></author></authors><title>Computational Intelligence for Deepwater Reservoir Depositional
  Environments Interpretation</title><categories>cs.NE physics.geo-ph</categories><journal-ref>Journal of Natural Gas Science &amp; Engineering, Volume 3, Issue 6,
  pages 716-728, Elsevier, 2011</journal-ref><doi>10.1016/j.jngse.2011.07.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting oil recovery efficiency of a deepwater reservoir is a challenging
task. One approach to characterize a deepwater reservoir and to predict its
producibility is by analyzing its depositional information. This research
proposes a deposition-based stratigraphic interpretation framework for
deepwater reservoir characterization. In this framework, one critical task is
the identification and labeling of the stratigraphic components in the
reservoir, according to their depositional environments. This interpretation
process is labor intensive and can produce different results depending on the
stratigrapher who performs the analysis. To relieve stratigrapher's workload
and to produce more consistent results, we have developed a novel methodology
to automate this process using various computational intelligence techniques.
Using a well log data set, we demonstrate that the developed methodology and
the designed workflow can produce finite state transducer models that interpret
deepwater reservoir depositional environments adequately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2648</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2648</id><created>2013-01-12</created><authors><author><keyname>Diao</keyname><forenames>Yingfei</forenames></author><author><keyname>Lin</keyname><forenames>Zhiyun</forenames></author><author><keyname>Fu</keyname><forenames>Minyue</forenames></author><author><keyname>Zhang</keyname><forenames>Huanshui</forenames></author></authors><title>A New Distributed Localization Method for Sensor Networks</title><categories>cs.IT cs.DC cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of determining the sensor locations in a large
sensor network using relative distance (range) measurements only. Our work
follows from a seminal paper by Khan et al. [1] where a distributed algorithm,
known as DILOC, for sensor localization is given using the barycentric
coordinate. A main limitation of the DILOC algorithm is that all sensor nodes
must be inside the convex hull of the anchor nodes. In this paper, we consider
a general sensor network without the convex hull assumption, which incurs
challenges in determining the sign pattern of the barycentric coordinate. A
criterion is developed to address this issue based on available distance
measurements. Also, a new distributed algorithm is proposed to guarantee the
asymptotic localization of all localizable sensor nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2649</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2649</id><created>2013-01-12</created><authors><author><keyname>Zarrabi</keyname><forenames>Amirreza</forenames></author></authors><title>Dynamic Transparent General Purpose Process Migration For Linux</title><categories>cs.DC cs.OS cs.SE</categories><journal-ref>International Journal of Grid Computing &amp; Applications (IJGCA)
  Vol.3, No.4, December 2012</journal-ref><doi>10.5121/ijgca.2012.3402</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Process migration refers to the act of transferring a process in the middle
of its execution from one machine to another in a network. In this paper, we
proposed a process migration framework for Linux OS. It is a multilayer
architecture to confine every functionality independent section of the system
in separate layer. This architecture is capable of supporting diverse
applications due to generic user space interface and dynamic structure that can
be modified according to demands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2650</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2650</id><created>2013-01-12</created><updated>2013-08-29</updated><authors><author><keyname>Soodhalter</keyname><forenames>Kirk M.</forenames></author><author><keyname>Szyld</keyname><forenames>Daniel B.</forenames></author><author><keyname>Xue</keyname><forenames>Fei</forenames></author></authors><title>Krylov Subspace Recycling for Sequences of Shifted Linear Systems</title><categories>math.NA cs.MS cs.NA</categories><comments>5 figures, 20 pages (main paper 18 pages + refs 2 pages)</comments><msc-class>65F10</msc-class><acm-class>F.2.1; G.1.3</acm-class><journal-ref>Applied Numerical Mathematics 81C (2014), pp. 105-118</journal-ref><doi>10.1016/j.apnum.2014.02.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the use of Krylov subspace recycling for the solution of a sequence
of slowly-changing families of linear systems, where each family consists of
shifted linear systems that differ in the coefficient matrix only by multiples
of the identity. Our aim is to explore the simultaneous solution of each family
of shifted systems within the framework of subspace recycling, using one
augmented subspace to extract candidate solutions for all the shifted systems.
The ideal method would use the same augmented subspace for all systems and have
fixed storage requirements, independent of the number of shifted systems per
family. We show that a method satisfying both requirements cannot exist in this
framework.
  As an alternative, we introduce two schemes. One constructs a separate
deflation space for each shifted system but solves each family of shifted
systems simultaneously. The other builds only one recycled subspace and
constructs approximate corrections to the solutions of the shifted systems at
each cycle of the iterative linear solver while only minimizing the base system
residual. At convergence of the base system solution, we apply the method
recursively to the remaining unconverged systems. We present numerical examples
involving systems arising in lattice quantum chromodynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2655</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2655</id><created>2013-01-12</created><authors><author><keyname>Kadri</keyname><forenames>Hachem</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Rabaoui</keyname><forenames>Asma</forenames><affiliation>IMS</affiliation></author><author><keyname>Preux</keyname><forenames>Philippe</forenames><affiliation>INRIA Lille - Nord Europe, LIFL</affiliation></author><author><keyname>Duflos</keyname><forenames>Emmanuel</forenames><affiliation>INRIA Lille - Nord Europe, LAGIS</affiliation></author><author><keyname>Rakotomamonjy</keyname><forenames>Alain</forenames><affiliation>LITIS</affiliation></author></authors><title>Functional Regularized Least Squares Classi cation with Operator-valued
  Kernels</title><categories>cs.LG stat.ML</categories><proxy>ccsd</proxy><journal-ref>28th International Conference on Machine Learning (ICML), Seattle
  : United States (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although operator-valued kernels have recently received increasing interest
in various machine learning and functional data analysis problems such as
multi-task learning or functional regression, little attention has been paid to
the understanding of their associated feature spaces. In this paper, we explore
the potential of adopting an operator-valued kernel feature space perspective
for the analysis of functional data. We then extend the Regularized Least
Squares Classification (RLSC) algorithm to cover situations where there are
multiple functions per observation. Experiments on a sound recognition problem
show that the proposed method outperforms the classical RLSC algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2656</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2656</id><created>2013-01-12</created><authors><author><keyname>Kadri</keyname><forenames>Hachem</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Preux</keyname><forenames>Philippe</forenames><affiliation>INRIA Lille - Nord Europe, LIFL</affiliation></author><author><keyname>Duflos</keyname><forenames>Emmanuel</forenames><affiliation>INRIA Lille - Nord Europe, LAGIS</affiliation></author><author><keyname>Canu</keyname><forenames>St&#xe9;phane</forenames><affiliation>LITIS</affiliation></author></authors><title>Multiple functional regression with both discrete and continuous
  covariates</title><categories>stat.ML cs.LG</categories><proxy>ccsd</proxy><journal-ref>2nd International Workshop on Functional and Operatorial
  Statistics (IWFOS), Santander : Spain (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a nonparametric method for extending functional
regression methodology to the situation where more than one functional
covariate is used to predict a functional response. Borrowing the idea from
Kadri et al. (2010a), the method, which support mixed discrete and continuous
explanatory variables, is based on estimating a function-valued function in
reproducing kernel Hilbert spaces by virtue of positive operator-valued
kernels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2659</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2659</id><created>2013-01-12</created><authors><author><keyname>Guigour&#xe8;s</keyname><forenames>Romain</forenames><affiliation>SAMM</affiliation></author><author><keyname>Boull&#xe9;</keyname><forenames>Marc</forenames><affiliation>SAMM</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>A Triclustering Approach for Time Evolving Graphs</title><categories>cs.LG cs.SI stat.ML</categories><proxy>ccsd</proxy><journal-ref>Co-clustering and Applications International Conference on Data
  Mining Workshop, Brussels : Belgium (2012)</journal-ref><doi>10.1109/ICDMW.2012.61</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel technique to track structures in time evolving
graphs. The method is based on a parameter free approach for three-dimensional
co-clustering of the source vertices, the target vertices and the time. All
these features are simultaneously segmented in order to build time segments and
clusters of vertices whose edge distributions are similar and evolve in the
same way over the time segments. The main novelty of this approach lies in that
the time segments are directly inferred from the evolution of the edge
distribution between the vertices, thus not requiring the user to make an a
priori discretization. Experiments conducted on a synthetic dataset illustrate
the good behaviour of the technique, and a study of a real-life dataset shows
the potential of the proposed approach for exploratory data analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2661</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2661</id><created>2013-01-12</created><updated>2013-04-22</updated><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames><affiliation>IST Austria</affiliation></author><author><keyname>Fijalkow</keyname><forenames>Nathana&#xeb;l</forenames><affiliation>LIAFA</affiliation></author></authors><title>Infinite-state games with finitary conditions</title><categories>cs.GT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study two-player zero-sum games over infinite-state graphs with
boundedness conditions. Our first contribution is about the strategy
complexity, i.e the memory required for winning strategies: we prove that over
general infinite-state graphs, memoryless strategies are sufficient for
finitary B\&quot;uchi games, and finite-memory suffices for finitary parity games.
We then study pushdown boundedness games, with two contributions. First we
prove a collapse result for pushdown omega B games, implying the decidability
of solving these games. Second we consider pushdown games with finitary parity
along with stack boundedness conditions, and show that solving these games is
EXPTIME-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2662</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2662</id><created>2013-01-12</created><authors><author><keyname>Perchet</keyname><forenames>Vianney</forenames><affiliation>LPMA</affiliation></author></authors><title>Nash equilibria with partial monitoring; Computation and Lemke-Howson
  algorithm</title><categories>cs.GT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In two player bi-matrix games with partial monitoring, actions played are not
observed, only some messages are received. Those games satisfy a crucial
property of usual bi-matrix games: there are only a finite number of required
(mixed) best replies. This is very helpful while investigating sets of Nash
equilibria: for instance, in some cases, it allows to relate it to the set of
equilibria of some auxiliary game with full monitoring. In the general case,
the Lemke-Howson algorithm is extended and, under some genericity assumption,
its output are Nash equilibria of the original game. As a by product, we obtain
an oddness property on their number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2663</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2663</id><created>2013-01-12</created><authors><author><keyname>Perchet</keyname><forenames>Vianney</forenames><affiliation>LPMA</affiliation></author></authors><title>Approachability, Regret and Calibration; implications and equivalences</title><categories>cs.GT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blackwell approachability, regret minimization and calibration are three
criteria evaluating a strategy (or an algorithm) in different sequential
decision problems, or repeated games between a player and Nature. Although they
have at first sight nothing in common, links between have been discovered: both
consistent and calibrated strategies can be constructed by following, in some
auxiliary game, an approachability strategy. We gathered famous or recent
results and provide new ones in order to develop and generalize Blackwell's
elegant theory. The final goal is to show how it can be used as a basic
powerful tool to exhibit a new class of intuitive algorithms, based on simple
geometric properties. In order to be complete, we also prove that
approachability can be seen as a byproduct of the very existence of consistent
or calibrated strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2678</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2678</id><created>2013-01-12</created><updated>2013-01-22</updated><authors><author><keyname>Belardinelli</keyname><forenames>Francesco</forenames></author><author><keyname>Lomuscio</keyname><forenames>Alessio</forenames></author><author><keyname>Patrizi</keyname><forenames>Fabio</forenames></author></authors><title>Verification of Agent-Based Artifact Systems</title><categories>cs.MA cs.AI cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artifact systems are a novel paradigm for specifying and implementing
business processes described in terms of interacting modules called artifacts.
Artifacts consist of data and lifecycles, accounting respectively for the
relational structure of the artifacts' states and their possible evolutions
over time. In this paper we put forward artifact-centric multi-agent systems, a
novel formalisation of artifact systems in the context of multi-agent systems
operating on them. Differently from the usual process-based models of services,
the semantics we give explicitly accounts for the data structures on which
artifact systems are defined. We study the model checking problem for
artifact-centric multi-agent systems against specifications written in a
quantified version of temporal-epistemic logic expressing the knowledge of the
agents in the exchange. We begin by noting that the problem is undecidable in
general. We then identify two noteworthy restrictions, one syntactical and one
semantical, that enable us to find bisimilar finite abstractions and therefore
reduce the model checking problem to the instance on finite models. Under these
assumptions we show that the model checking problem for these systems is
EXPSPACE-complete. We then introduce artifact-centric programs, compact and
declarative representations of the programs governing both the artifact system
and the agents. We show that, while these in principle generate infinite-state
systems, under natural conditions their verification problem can be solved on
finite abstractions that can be effectively computed from the programs. Finally
we exemplify the theoretical results of the paper through a mainstream
procurement scenario from the artifact systems literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2683</identifier>
 <datestamp>2014-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2683</id><created>2013-01-12</created><updated>2014-05-28</updated><authors><author><keyname>Urban</keyname><forenames>Josef</forenames></author></authors><title>BliStr: The Blind Strategymaker</title><categories>cs.AI cs.LG cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  BliStr is a system that automatically develops strategies for E prover on a
large set of problems. The main idea is to interleave (i) iterated
low-timelimit local search for new strategies on small sets of similar easy
problems with (ii) higher-timelimit evaluation of the new strategies on all
problems. The accumulated results of the global higher-timelimit runs are used
to define and evolve the notion of &quot;similar easy problems&quot;, and to control the
selection of the next strategy to be improved. The technique was used to
significantly strengthen the set of E strategies used by the MaLARea, PS-E,
E-MaLeS, and E systems in the CASC@Turing 2012 competition, particularly in the
Mizar division. Similar improvement was obtained on the problems created from
the Flyspeck corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2689</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2689</id><created>2013-01-12</created><authors><author><keyname>Thirumurugan</keyname><forenames>S.</forenames></author><author><keyname>Raj</keyname><forenames>E. George Dharma Prakash</forenames></author></authors><title>An Extended Weighted Partitioning Around Cluster Head Mechanism for Ad
  Hoc Network</title><categories>cs.NI cs.DC</categories><comments>IJASUC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wireless network places vital role in the present day communication
scenario. The ad hoc nature of wireless communication adds flavour to suit
various real world applications. This improves the performance of the network
tremendously while the clustering mechanism gets added to the ad hoc network.
It has been found out that the existing WCA lacks in forming efficient
clusters. Thus, this work proposes an Extended weighted partitioning around
cluster head mechanism by considering W-PAC as a base to form clusters. The
cluster members are configured with IPv6 address. This IPv6 clusters formed
through W-PAC will be taken further for validation to determine the perfectness
of clusters. The cluster formation and maintenance have been implemented in C++
as a programming language. The cluster validation has been carried out using
OMNET++ simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2691</identifier>
 <datestamp>2013-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2691</id><created>2013-01-12</created><updated>2013-06-07</updated><authors><author><keyname>Marsault</keyname><forenames>Victor</forenames></author><author><keyname>Sakarovitch</keyname><forenames>Jacques</forenames></author></authors><title>Ultimate periodicity of b-recognisable sets : a quasilinear procedure</title><categories>cs.FL</categories><comments>presented at DLT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is decidable if a set of numbers, whose representation in a base b is a
regular language, is ultimately periodic. This was established by Honkala in
1986.
  We give here a structural description of minimal automata that accept an
ultimately periodic set of numbers. We then show that it can verified in linear
time if a given minimal automaton meets this description.
  This thus yields a O(n log(n)) procedure for deciding whether a general
deterministic automaton accepts an ultimately periodic set of numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2696</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2696</id><created>2013-01-12</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Sampaio-Neto</keyname><forenames>Raimundo</forenames></author></authors><title>Reduced-Rank Space-Time Interference Suppression with Joint Iterative
  Least Squares Algorithms for Spread Spectrum Systems</title><categories>cs.IT math.IT</categories><comments>8 figures</comments><journal-ref>IEEE Transactions on Vehicular Technology, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents novel adaptive space-time reduced-rank interference
suppression least squares algorithms based on joint iterative optimization of
parameter vectors. The proposed space-time reduced-rank scheme consists of a
joint iterative optimization of a projection matrix that performs
dimensionality reduction and an adaptive reduced-rank parameter vector that
yields the symbol estimates. The proposed techniques do not require singular
value decomposition (SVD) and automatically find the best set of basis for
reduced-rank processing. We present least squares (LS) expressions for the
design of the projection matrix and the reduced-rank parameter vector and we
conduct an analysis of the convergence properties of the LS algorithms. We then
develop recursive least squares (RLS) adaptive algorithms for their
computationally efficient estimation and an algorithm for automatically
adjusting the rank of the proposed scheme. A convexity analysis of the LS
algorithms is carried out along with the development of a proof of convergence
for the proposed algorithms. Simulations for a space-time interference
suppression application with a DS-CDMA system show that the proposed scheme
outperforms in convergence and tracking the state-of-the-art reduced-rank
schemes at a comparable complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2697</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2697</id><created>2013-01-12</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Sampaio-Neto</keyname><forenames>Raimundo</forenames></author></authors><title>Adaptive Reduced-Rank Equalization Algorithms Based on Alternating
  Optimization Design Techniques for Multi-Antenna Systems</title><categories>cs.IT math.IT</categories><comments>9 figures</comments><journal-ref>IEEE Transactions on Vehicular Technology, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel adaptive reduced-rank {multi-input multi-output}
(MIMO) equalization scheme and algorithms based on alternating optimization
design techniques for MIMO spatial multiplexing systems. The proposed
reduced-rank equalization structure consists of a joint iterative optimization
of two equalization stages, namely, a transformation matrix that performs
dimensionality reduction and a reduced-rank estimator that retrieves the
desired transmitted symbol. The proposed reduced-rank architecture is
incorporated into an equalization structure that allows both decision feedback
and linear schemes for mitigating the inter-antenna and inter-symbol
interference. We develop alternating least squares (LS) expressions for the
design of the transformation matrix and the reduced-rank estimator along with
computationally efficient alternating recursive least squares (RLS) adaptive
estimation algorithms. We then present an algorithm for automatically adjusting
the model order of the proposed scheme. An analysis of the LS algorithms is
carried out along with sufficient conditions for convergence and a proof of
convergence of the proposed algorithms to the reduced-rank Wiener filter.
Simulations show that the proposed equalization algorithms outperform the
existing reduced-rank and full-rank algorithms, while requiring a comparable
computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2698</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2698</id><created>2013-01-12</created><authors><author><keyname>Li</keyname><forenames>Jiankou</forenames></author></authors><title>Evaluating community structure in large network with random walks</title><categories>cs.SI physics.soc-ph</categories><comments>10 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Community structure is one of the most important properties of networks. Most
community algorithms are not suitable for large networks because of their time
consuming. In fact there are lots of networks with millons even billons of
nodes. In such case, most algorithms running in time O(n2logn) or even larger
are not practical. What we need are linear or approximately linear time
algorithm. Rising in response to such needs, we propose a quick methods to
evaluate community structure in networks and then put forward a local community
algorithm with nearly linear time based on random walks. Using our community
evaluating measure, we could find some difference results from measures used
before, i.e., the Newman Modularity. Our algorithm are effective in small
benchmark networks with small less accuracy than more complex algorithms but a
great of advantage in time consuming for large networks, especially super large
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2707</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2707</id><created>2013-01-12</created><updated>2015-03-26</updated><authors><author><keyname>Choi</keyname><forenames>Sou-Cheng T.</forenames></author><author><keyname>Saunders</keyname><forenames>Michael A.</forenames></author></authors><title>ALGORITHM 937: MINRES-QLP for Singular Symmetric and Hermitian Linear
  Equations and Least-Squares Problems</title><categories>cs.MS cs.DS cs.NA</categories><comments>14 pages and 1 figure</comments><doi>10.1145/2527267</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe algorithm MINRES-QLP and its FORTRAN 90 implementation for
solving symmetric or Hermitian linear systems or least-squares problems. If the
system is singular, MINRES-QLP computes the unique minimum-length solution
(also known as the pseudoinverse solution), which generally eludes MINRES. In
all cases, it overcomes a potential instability in the original MINRES
algorithm. A positive-definite preconditioner may be supplied. Our FORTRAN 90
implementation illustrates a design pattern that allows users to make problem
data known to the solver but hidden and secure from other program units. In
particular, we circumvent the need for reverse communication. While we focus
here on a FORTRAN 90 implementation, we also provide and maintain MATLAB
versions of MINRES and MINRES-QLP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2710</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2710</id><created>2013-01-12</created><authors><author><keyname>Rhif</keyname><forenames>Ahmed</forenames></author><author><keyname>Kardous</keyname><forenames>Zohra</forenames></author><author><keyname>Braiek</keyname><forenames>Naceur BenHadj</forenames></author></authors><title>A High-Order Sliding Mode Observer: Torpedo Guidance Application</title><categories>cs.SY</categories><comments>arXiv admin note: substantial text overlap with arXiv:1202.2419</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The guidance of a torpedo represents a hard task because of the smooth
nonlinear aspect of this system and because of the extreme external
disturbances. The torpedo guidance reposes on the speed and the position
control. In fact, the control approach which is very solicited for the
electromechanical systems is the sliding mode control (SMC) which proved its
effectiveness through the different studies. The SMC is robust versus
disturbances and model uncertainties; however, a sharp discontinuous control is
needed which induces the chattering phenomenon. The angular velocity
measurement is a hard task because of the high level of disturbances. In this
way, the sliding mode observer could be a solution for the velocity estimation
instead of a sensor. This article deals with torpedo guidance by SMC to reach
the desired path in a short time and with high precision quality. Simulation
results show that this control strategy and observer can attain excellent
control performances with no chattering problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2711</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2711</id><created>2013-01-12</created><authors><author><keyname>Rhif</keyname><forenames>Ahmed</forenames></author><author><keyname>Kardous</keyname><forenames>Zohra</forenames></author><author><keyname>Braiek</keyname><forenames>Naceur BenHadj</forenames></author></authors><title>A Sliding Mode Multimodel Control for a Sensorless Photovoltaic System</title><categories>cs.SY hep-ex</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we will talk about a new control test using the sliding mode
control with a nonlinear sliding mode observer, which are very solicited in
tracking problems, for a sensorless photovoltaic panel. In this case, the panel
system will has as a set point the sun position at every second during the day
for a period of five years; then the tracker, using sliding mode multimodel
controller and a sliding mode observer, will track these positions to make the
sunrays orthogonal to the photovoltaic cell that produces more energy. After
sunset, the tracker goes back to the initial position (which of sunrise).
Experimental measurements show that this autonomic dual axis Sun Tracker
increases the power production by over 40%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2714</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2714</id><created>2013-01-12</created><updated>2013-04-02</updated><authors><author><keyname>Rhif</keyname><forenames>Ahmed</forenames></author><author><keyname>Kardous</keyname><forenames>Zohra</forenames></author><author><keyname>Braiek</keyname><forenames>Naceur Ben Hadj</forenames></author></authors><title>A Sliding Mode-Multimodel Control with Sliding Mode Observer for a
  Sensorless Pumping System</title><categories>cs.SY</categories><comments>This paper has been withdrawn by the author because it may be
  reviewed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work deals with the design of a sliding mode observer with a
multi-surfaces sliding mode multimodel control (SM-MMC) for a mechanical
sensorless pumping system. The observer is designed to estimate the speed and
the mechanical position of the DC motor operating in the process. Robustness
tests validated by simulation show the effectiveness of the sliding mode
observer associated with this control approach (SM-MMC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2715</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2715</id><created>2013-01-12</created><authors><author><keyname>Antonides</keyname><forenames>Joseph</forenames></author><author><keyname>Kubota</keyname><forenames>Toshiro</forenames></author></authors><title>Binocular disparity as an explanation for the moon illusion</title><categories>cs.CV physics.pop-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present another explanation for the moon illusion, in which the moon looks
larger near the horizon than near the zenith. In our model, the sky is
considered a spatially contiguous and geometrically smooth surface. When an
object (like the moon) breaks the contiguity of the surface, humans perceive an
occlusion of the surface rather than an object appearing through a hole.
Binocular vision dictates that the moon is distant, but this perception model
dictates that the moon is closer than the sky. To solve the dilemma, the brain
distorts the projections of the moon to increase the binocular disparity, which
results in increase of the angular size of the moon. The degree of the
distortion depends upon the apparent distance to the sky, which is influenced
by the surrounding objects and the condition of the sky. The closer the sky
appears, the stronger the illusion. At the zenith, few distance cues are
present, causing difficulty with distance estimation and weakening the
illusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2722</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2722</id><created>2013-01-12</created><authors><author><keyname>Hollander</keyname><forenames>Christopher D.</forenames></author><author><keyname>Wu</keyname><forenames>Annie S.</forenames></author></authors><title>Distributed Consensus Formation Through Unconstrained Gossiping</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gossip algorithms are widely used to solve the distributed consensus problem,
but issues can arise when nodes receive multiple signals either at the same
time or before they are able to finish processing their current work load.
Specifically, a node may assume a new state that represents a linear
combination of all received signals; even if such a state makes no sense in the
problem domain. As a solution to this problem, we introduce the notion of
conflict resolution for gossip algorithms and prove that their application
leads to a valid consensus state when the underlying communication network
possesses certain properties. We also introduce a methodology based on
absorbing Markov chains for analyzing gossip algorithms that make use of these
conflict resolution algorithms. This technique allows us to calculate both the
probabilities of converging to a specific consensus state and the time that
such convergence is expected to take. Finally, we make use of simulation to
validate our methodology and explore the temporal behavior of gossip algorithms
as the size of the network, the number of states per node, and the network
density increase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2723</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2723</id><created>2013-01-12</created><updated>2013-02-11</updated><authors><author><keyname>Athanasiou</keyname><forenames>George</forenames></author><author><keyname>Weeraddana</keyname><forenames>Pradeep Chathuranga</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author><author><keyname>Tassiulas</keyname><forenames>Leandros</forenames></author></authors><title>Optimizing Client Association in 60 GHz Wireless Access Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MillimeterWave communications in the 60 GHz band are considered one of the
key technologies for enabling multi-gigabit wireless access. However, the high
propagation loss in such a band poses major obstacles to the optimal
utilization of the wireless resources, where the problem of efficient client
association to access points (APs) is of vital importance. In this paper, the
client association in 60 GHz wireless access networks is investigated. The AP
utilization and the quality of the rapidly vanishing communication links are
the control parameters. Because of the tricky non-convex and combinatorial
nature of the client association optimization problem, a novel solution method
is developed to guarantee balanced and fair resource allocation. A new
distributed, lightweight and easy to implement association algorithm, based on
Lagrangian duality theory and subgradient methods, was proposed. It is shown
that the algorithm is asymptotically optimal, that is, the relative duality gap
diminishes to zero as the number of clients increases. Both theoretical and
numerical results evince numerous useful properties of the algorithm, such as
fast convergence, scalability, time efficiency, and fair execution in
comparison to existing approaches. It is concluded that the proposed solution
can be applied in the forthcoming 60 GHz wireless access networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2725</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2725</id><created>2013-01-12</created><authors><author><keyname>Chen</keyname><forenames>Yudong</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Robust High Dimensional Sparse Regression and Matching Pursuit</title><categories>stat.ML cs.IT cs.LG math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider high dimensional sparse regression, and develop strategies able
to deal with arbitrary -- possibly, severe or coordinated -- errors in the
covariance matrix $X$. These may come from corrupted data, persistent
experimental errors, or malicious respondents in surveys/recommender systems,
etc. Such non-stochastic error-in-variables problems are notoriously difficult
to treat, and as we demonstrate, the problem is particularly pronounced in
high-dimensional settings where the primary goal is {\em support recovery} of
the sparse regressor. We develop algorithms for support recovery in sparse
regression, when some number $n_1$ out of $n+n_1$ total covariate/response
pairs are {\it arbitrarily (possibly maliciously) corrupted}. We are interested
in understanding how many outliers, $n_1$, we can tolerate, while identifying
the correct support. To the best of our knowledge, neither standard outlier
rejection techniques, nor recently developed robust regression algorithms (that
focus only on corrupted response variables), nor recent algorithms for dealing
with stochastic noise or erasures, can provide guarantees on support recovery.
Perhaps surprisingly, we also show that the natural brute force algorithm that
searches over all subsets of $n$ covariate/response pairs, and all subsets of
possible support coordinates in order to minimize regression error, is
remarkably poor, unable to correctly identify the support with even $n_1 =
O(n/k)$ corrupted points, where $k$ is the sparsity. This is true even in the
basic setting we consider, where all authentic measurements and noise are
independent and sub-Gaussian. In this setting, we provide a simple algorithm --
no more computationally taxing than OMP -- that gives stronger performance
guarantees, recovering the support with up to $n_1 = O(n/(\sqrt{k} \log p))$
corrupted points, where $p$ is the dimension of the signal to be recovered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2729</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2729</id><created>2013-01-12</created><authors><author><keyname>Austrin</keyname><forenames>Per</forenames></author><author><keyname>H&#xe5;stad</keyname><forenames>Johan</forenames></author><author><keyname>Pass</keyname><forenames>Rafael</forenames></author></authors><title>On the Power of Many One-Bit Provers</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the class of languages, denoted by $\MIP[k, 1-\epsilon, s]$, which
have $k$-prover games where each prover just sends a \emph{single} bit, with
completeness $1-\epsilon$ and soundness error $s$. For the case that $k=1$
(i.e., for the case of interactive proofs), Goldreich, Vadhan and Wigderson
({\em Computational Complexity'02}) demonstrate that $\SZK$ exactly
characterizes languages having 1-bit proof systems with&quot;non-trivial&quot; soundness
(i.e., $1/2 &lt; s \leq 1-2\epsilon$). We demonstrate that for the case that
$k\geq 2$, 1-bit $k$-prover games exhibit a significantly richer structure:
  + (Folklore) When $s \leq \frac{1}{2^k} - \epsilon$, $\MIP[k, 1-\epsilon, s]
= \BPP$;
  + When $\frac{1}{2^k} + \epsilon \leq s &lt; \frac{2}{2^k}-\epsilon$, $\MIP[k,
1-\epsilon, s] = \SZK$;
  + When $s \ge \frac{2}{2^k} + \epsilon$, $\AM \subseteq \MIP[k, 1-\epsilon,
s]$;
  + For $s \le 0.62 k/2^k$ and sufficiently large $k$, $\MIP[k, 1-\epsilon, s]
\subseteq \EXP$;
  + For $s \ge 2k/2^{k}$, $\MIP[k, 1, 1-\epsilon, s] = \NEXP$.
  As such, 1-bit $k$-prover games yield a natural &quot;quantitative&quot; approach to
relating complexity classes such as $\BPP$,$\SZK$,$\AM$, $\EXP$, and $\NEXP$.
We leave open the question of whether a more fine-grained hierarchy (between
$\AM$ and $\NEXP$) can be established for the case when $s \geq \frac{2}{2^k} +
\epsilon$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2731</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2731</id><created>2013-01-12</created><authors><author><keyname>Austrin</keyname><forenames>Per</forenames></author><author><keyname>Khot</keyname><forenames>Subhash</forenames></author></authors><title>A Characterization of Approximation Resistance for Even $k$-Partite CSPs</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A constraint satisfaction problem (CSP) is said to be \emph{approximation
resistant} if it is hard to approximate better than the trivial algorithm which
picks a uniformly random assignment. Assuming the Unique Games Conjecture, we
give a characterization of approximation resistance for $k$-partite CSPs
defined by an even predicate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2734</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2734</id><created>2013-01-12</created><updated>2013-03-14</updated><authors><author><keyname>B&#xfc;sing</keyname><forenames>Christina</forenames></author><author><keyname>D'Andreagiovanni</keyname><forenames>Fabio</forenames></author></authors><title>Robust Optimization under Multi-band Uncertainty - Part I: Theory</title><categories>math.OC cs.DS math.NA</categories><comments>Modifications w.r.t. version 1: Section 4 revised</comments><msc-class>90C05, 90C35, 90C57, 90C90</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical single-band uncertainty model introduced by Bertsimas and Sim
has represented a breakthrough in the development of tractable robust
counterparts of Linear Programs. However, adopting a single deviation band may
be too limitative in practice: in many real-world problems, observed deviations
indeed present asymmetric distributions over asymmetric ranges, so that getting
a higher modeling resolution by partitioning the band into multiple sub-bands
is advisable. The critical aim of our work is to close the knowledge gap on the
adoption of multi-band uncertainty in Robust Optimization: a general definition
and intensive theoretical study of a multi-band model are actually still
missing. Our new developments have been also strongly inspired and encouraged
by our industrial partners, interested in getting a better modeling of
arbitrary shaped distributions, built on historical data about the uncertainty
affecting the considered real-world problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2750</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2750</id><created>2013-01-13</created><updated>2013-01-22</updated><authors><author><keyname>Karaca</keyname><forenames>Mehmet</forenames></author><author><keyname>Ekici</keyname><forenames>Eylem</forenames></author><author><keyname>Ercetin</keyname><forenames>Ozgur</forenames></author></authors><title>Throughput-Optimal Distributed Algorithm for WLANs over Fading Channels</title><categories>cs.NI cs.IT cs.PF math.IT</categories><comments>This paper has been withdrawn by the author</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, it has been shown that CSMA-type random access algorithms are
throughput optimal since they achieve the maximum throughput while maintaining
the network stability. However, the optimality is established with the
following unrealistic assumptions; i-) the underlaying Markov chain reaches a
stationary distribution immediately, which causes large delay in practice; ii-)
the channel is static and does not change over time. In this paper, we design
fully distributed scheduling algorithms which are provably throughput optimal
for general fading channels. When arbitrary backoff time is allowed, the
proposed distributed algorithm achieves the same performance in terms of rate
region and delay as that of a centralized system without requiring any message
passing. For the case where backoff time is discrete, we show that our
algorithm still maintains throughput-optimality and achieves good delay
performance at the expense of low overhead for collision resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2763</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2763</id><created>2013-01-13</created><authors><author><keyname>Matsuoka</keyname><forenames>Satoshi</forenames></author></authors><title>A New Proof of P-time Completeness of Linear Lambda Calculus</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new proof of P-time completeness of Linear Lambda Calculus, which
was originally given by H. Mairson in 2003. Our proof uses an essentially
different Boolean type from the type Mairson used. Moreover the correctness of
our proof can be machined-checked using an implementation of Standard ML.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2774</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2774</id><created>2013-01-13</created><updated>2014-09-03</updated><authors><author><keyname>Muhammadi</keyname><forenames>Jafar</forenames></author><author><keyname>Rabiee</keyname><forenames>Hamid Reza</forenames></author><author><keyname>Hosseini</keyname><forenames>Abbas</forenames></author></authors><title>Crowd Labeling: a survey</title><categories>cs.AI</categories><comments>Under consideration for publication in Knowledge and Information
  Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been a burst in the number of research projects on human
computation via crowdsourcing. Multiple choice (or labeling) questions could be
referred to as a common type of problem which is solved by this approach. As an
application, crowd labeling is applied to find true labels for large machine
learning datasets. Since crowds are not necessarily experts, the labels they
provide are rather noisy and erroneous. This challenge is usually resolved by
collecting multiple labels for each sample, and then aggregating them to
estimate the true label. Although the mechanism leads to high-quality labels,
it is not actually cost-effective. As a result, efforts are currently made to
maximize the accuracy in estimating true labels, while fixing the number of
acquired labels.
  This paper surveys methods to aggregate redundant crowd labels in order to
estimate unknown true labels. It presents a unified statistical latent model
where the differences among popular methods in the field correspond to
different choices for the parameters of the model. Afterwards, algorithms to
make inference on these models will be surveyed. Moreover, adaptive methods
which iteratively collect labels based on the previously collected labels and
estimated models will be discussed. In addition, this paper compares the
distinguished methods, and provides guidelines for future work required to
address the current open issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2780</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2780</id><created>2013-01-13</created><authors><author><keyname>Basu</keyname><forenames>Joydeep</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Tarun Kanti</forenames></author></authors><title>Microelectromechanical Resonators for Radio Frequency Communication
  Applications</title><categories>cs.OH</categories><journal-ref>Microsystem Technologies, Oct 2011, vol. 17(10-11), pp. 1557-1580</journal-ref><doi>10.1007/s00542-011-1332-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past few years, microelectromechanical system (MEMS) based on-chip
resonators have shown significant potential for sensing and high frequency
signal processing applications. This is due to their excellent features like
small size, large frequency-quality factor product, low power consumption, low
cost batch fabrication, and integrability with CMOS IC technology. Radio
frequency communication circuits like reference oscillators, filters, and
mixers based on such MEMS resonators can be utilized for meeting the increasing
count of RF components likely to be demanded by the next generation
multi-band/multi-mode wireless devices. MEMS resonators can provide a feasible
alternative to the present day well established quartz crystal technology that
is riddled with major drawbacks like relatively large size, high cost, and low
compatibility with IC chips. This article presents a survey of the developments
in this field of resonant MEMS structures with detailed enumeration on the
various micromechanical resonator types, modes of vibration, equivalent
mechanical and electrical models, materials and technologies used for
fabrication, and the application of the resonators for implementing oscillators
and filters. These are followed by a discussion on the challenges for RF MEMS
technology in comparison to quartz crystal technology; like high precision,
stability, reliability, need for hermetic packaging etc. which remain to be
addressed for enabling the inclusion of micromechanical resonators into
tomorrow's highly integrated communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2785</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2785</id><created>2013-01-13</created><authors><author><keyname>Rafi</keyname><forenames>Muhammad</forenames></author><author><keyname>Shaikh</keyname><forenames>Mohammad Shahid</forenames></author></authors><title>A comparison of SVM and RVM for Document Classification</title><categories>cs.IR cs.LG</categories><comments>ICoCSIM 2012, Medan Indonesia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Document classification is a task of assigning a new unclassified document to
one of the predefined set of classes. The content based document classification
uses the content of the document with some weighting criteria to assign it to
one of the predefined classes. It is a major task in library science,
electronic document management systems and information sciences. This paper
investigates document classification by using two different classification
techniques (1) Support Vector Machine (SVM) and (2) Relevance Vector Machine
(RVM). SVM is a supervised machine learning technique that can be used for
classification task. In its basic form, SVM represents the instances of the
data into space and tries to separate the distinct classes by a maximum
possible wide gap (hyper plane) that separates the classes. On the other hand
RVM uses probabilistic measure to define this separation space. RVM uses
Bayesian inference to obtain succinct solution, thus RVM uses significantly
fewer basis functions. Experimental studies on three standard text
classification datasets reveal that although RVM takes more training time, its
classification is much better as compared to SVM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2807</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2807</id><created>2013-01-13</created><authors><author><keyname>Sklan</keyname><forenames>Sophia</forenames></author><author><keyname>Grossman</keyname><forenames>Jeffrey C.</forenames></author></authors><title>Beyond electronics, beyond optics: single circuit parallel computing
  with phonons</title><categories>cond-mat.other cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phononic computing -- the use of (typically thermal) vibrations for
information processing -- is a nascent technology; its capabilities are still
being discovered. We analyze an alternative form of phononic computing inspired
by optical, rather than electronic, computing. Using the acoustic Faraday
effect, we design a phonon gyrator and thereby a means of performing
computation through the manipulation of polarization in transverse phonon
currents. Moreover, we establish that our gyrators act as generalized
transistors and can construct digital logic gates. Exploiting the wave nature
of phonons and the similarity of our logic gates, we demonstrate parallel
computation within a single circuit, an effect presently unique to phonons.
Finally, a generic method of designing these parallel circuits is introduced
and used to analyze the feasibility of magneto-acoustic materials in realizing
these circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2811</identifier>
 <datestamp>2013-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2811</id><created>2013-01-13</created><updated>2013-04-26</updated><authors><author><keyname>Scheible</keyname><forenames>Christian</forenames></author><author><keyname>Schuetze</keyname><forenames>Hinrich</forenames></author></authors><title>Cutting Recursive Autoencoder Trees</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Learning models enjoy considerable success in Natural Language
Processing. While deep architectures produce useful representations that lead
to improvements in various tasks, they are often difficult to interpret. This
makes the analysis of learned structures particularly difficult. In this paper,
we rely on empirical tests to see whether a particular structure makes sense.
We present an analysis of the Semi-Supervised Recursive Autoencoder, a
well-known model that produces structural representations of text. We show that
for certain tasks, the structure of the autoencoder can be significantly
reduced without loss of classification accuracy and we evaluate the produced
structures using human judgment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2820</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2820</id><created>2013-01-13</created><updated>2013-03-13</updated><authors><author><keyname>Culurciello</keyname><forenames>Eugenio</forenames></author><author><keyname>Bates</keyname><forenames>Jordan</forenames></author><author><keyname>Dundar</keyname><forenames>Aysegul</forenames></author><author><keyname>Carrasco</keyname><forenames>Jose</forenames></author><author><keyname>Farabet</keyname><forenames>Clement</forenames></author></authors><title>Clustering Learning for Robotic Vision</title><categories>cs.CV</categories><comments>Code for this paper is available here:
  https://github.com/culurciello/CL_paper1_code</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the clustering learning technique applied to multi-layer
feedforward deep neural networks. We show that this unsupervised learning
technique can compute network filters with only a few minutes and a much
reduced set of parameters. The goal of this paper is to promote the technique
for general-purpose robotic vision systems. We report its use in static image
datasets and object tracking datasets. We show that networks trained with
clustering learning can outperform large networks trained for many hours on
complex datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2831</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2831</id><created>2013-01-13</created><authors><author><keyname>Adcock</keyname><forenames>Ben</forenames></author><author><keyname>Hansen</keyname><forenames>Anders C.</forenames></author><author><keyname>Poon</keyname><forenames>Clarice</forenames></author></authors><title>Beyond consistent reconstructions: optimality and sharp bounds for
  generalized sampling, and application to the uniform resampling problem</title><categories>math.NA cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized sampling is a recently developed linear framework for sampling
and reconstruction in separable Hilbert spaces. It allows one to recover any
element in any finite-dimensional subspace given finitely many of its samples
with respect to an arbitrary frame. Unlike more common approaches for this
problem, such as the consistent reconstruction technique of Eldar et al, it
leads to completely stable numerical methods possessing both guaranteed
stability and accuracy.
  The purpose of this paper is twofold. First, we give a complete and formal
analysis of generalized sampling, the main result of which being the derivation
of new, sharp bounds for the accuracy and stability of this approach. Such
bounds improve those given previously, and result in a necessary and sufficient
condition, the stable sampling rate, which guarantees a priori a good
reconstruction. Second, we address the topic of optimality. Under some
assumptions, we show that generalized sampling is an optimal, stable
reconstruction. Correspondingly, whenever these assumptions hold, the stable
sampling rate is a universal quantity. In the final part of the paper we
illustrate our results by applying generalized sampling to the so-called
uniform resampling problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2840</identifier>
 <datestamp>2013-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2840</id><created>2013-01-13</created><updated>2013-04-25</updated><authors><author><keyname>Osendorfer</keyname><forenames>Christian</forenames></author><author><keyname>Bayer</keyname><forenames>Justin</forenames></author><author><keyname>Urban</keyname><forenames>Sebastian</forenames></author><author><keyname>van der Smagt</keyname><forenames>Patrick</forenames></author></authors><title>Unsupervised Feature Learning for low-level Local Image Descriptors</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised feature learning has shown impressive results for a wide range
of input modalities, in particular for object classification tasks in computer
vision. Using a large amount of unlabeled data, unsupervised feature learning
methods are utilized to construct high-level representations that are
discriminative enough for subsequently trained supervised classification
algorithms. However, it has never been \emph{quantitatively} investigated yet
how well unsupervised learning methods can find \emph{low-level
representations} for image patches without any additional supervision. In this
paper we examine the performance of pure unsupervised methods on a low-level
correspondence task, a problem that is central to many Computer Vision
applications. We find that a special type of Restricted Boltzmann Machines
(RBMs) performs comparably to hand-crafted descriptors. Additionally, a simple
binarization scheme produces compact representations that perform better than
several state-of-the-art descriptors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2848</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2848</id><created>2013-01-13</created><authors><author><keyname>Chen</keyname><forenames>Xu</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author></authors><title>Database-assisted Distributed Spectrum Sharing</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to FCC's ruling for white-space spectrum access, white-space
devices are required to query a database to determine the spectrum
availability. In this paper, we study the database-assisted distributed
white-space access point (AP) network design. We first model the cooperative
and non-cooperative channel selection problems among the APs as the system-wide
throughput optimization and non-cooperative AP channel selection games,
respectively, and design distributed AP channel selection algorithms that
achieve system optimal point and Nash equilibrium, respectively. We then
propose a state-based game formulation for the distributed AP association
problem of the secondary users by taking the cost of mobility into account. We
show that the state-based distributed AP association game has the finite
improvement property, and design a distributed AP association algorithm that
can converge to a state-based Nash equilibrium. Numerical results show that the
algorithm is robust to the perturbation by secondary users' dynamical leaving
and entering the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2851</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2851</id><created>2013-01-13</created><authors><author><keyname>Schneider</keyname><forenames>Christian M.</forenames></author><author><keyname>Ara&#xfa;jo</keyname><forenames>Nuno A. M.</forenames></author><author><keyname>Herrmann</keyname><forenames>Hans J.</forenames></author></authors><title>Efficient algorithm to study interconnected networks</title><categories>physics.comp-ph cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>5 pages, 6 figures</comments><journal-ref>Physical Review E 87, 043302 (2013)</journal-ref><doi>10.1103/PhysRevE.87.043302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interconnected networks have been shown to be much more vulnerable to random
and targeted failures than isolated ones, raising several interesting questions
regarding the identification and mitigation of their risk. The paradigm to
address these questions is the percolation model, where the resilience of the
system is quantified by the dependence of the size of the largest cluster on
the number of failures. Numerically, the major challenge is the identification
of this cluster and the calculation of its size. Here, we propose an efficient
algorithm to tackle this problem. We show that the algorithm scales as O(N log
N), where N is the number of nodes in the network, a significant improvement
compared to O(N^2) for a greedy algorithm, what permits studying much larger
networks. Our new strategy can be applied to any network topology and
distribution of interdependencies, as well as any sequence of failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2857</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2857</id><created>2013-01-13</created><authors><author><keyname>Al-Rfou'</keyname><forenames>Rami</forenames></author><author><keyname>Skiena</keyname><forenames>Steven</forenames></author></authors><title>SpeedRead: A Fast Named Entity Recognition Pipeline</title><categories>cs.CL</categories><comments>Long paper at COLING 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online content analysis employs algorithmic methods to identify entities in
unstructured text. Both machine learning and knowledge-base approaches lie at
the foundation of contemporary named entities extraction systems. However, the
progress in deploying these approaches on web-scale has been been hampered by
the computational cost of NLP over massive text corpora. We present SpeedRead
(SR), a named entity recognition pipeline that runs at least 10 times faster
than Stanford NLP pipeline. This pipeline consists of a high performance Penn
Treebank- compliant tokenizer, close to state-of-art part-of-speech (POS)
tagger and knowledge-based named entity recognizer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2858</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2858</id><created>2013-01-13</created><authors><author><keyname>Jain</keyname><forenames>Abhishek</forenames></author><author><keyname>Bonanno</keyname><forenames>Giuseppe</forenames></author><author><keyname>Gupta</keyname><forenames>Hima</forenames></author><author><keyname>Goyal</keyname><forenames>Ajay</forenames></author></authors><title>Generic System Verilog Universal Verification Methodology based Reusable
  Verification Environment for Efficient Verification of Image Signal
  Processing IPs/SoCs</title><categories>cs.OH</categories><comments>International journal of VLSI design &amp; Communication Systems (VLSICS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper,we present Generic System Verilog Universal Verification
Methodology based Reusable Verification Environment for efficient verification
of Image Signal Processing IP's/SoC's. With the tight schedules on all projects
it is important to have a strong verification methodology which contributes to
First Silicon Success. Deploy methodologies which enforce full functional
coverage and verification of corner cases through pseudo random test scenarios
is required. Also, standardization of verification flow is needed. Previously,
inside imaging group of ST, Specman (e)/Verilog based Verification Environment
for IP/Subsystem level verification and C/C++/Verilog based Directed
Verification Environment for SoC Level Verification was used for Functional
Verification. Different Verification Environments were used at IP level and SoC
level. Different Verification/Validation Methodologies were used for SoC
Verification across multiple sites. Verification teams were also looking for
the ways how to catch bugs early in the design cycle? Thus, Generic System
Verilog Universal Verification Methodology (UVM) based Reusable Verification
Environment is required to avoid the problem of having so many methodologies
and provides a standard unified solution which compiles on all tools. The main
aim of development of this Generic and automatic verification environment is to
develop an efficient and unified verification environment (at IP/Subsystem/SoC
Level) which reuses the already developed Verification components and also
sequences written at IP/Subsystem level can be reused at SoC Level both with
Host BFM and actual Core using Incisive Software Extension (ISX) and Virtual
Register Interface (VRI)/Verification Abstraction Layer (VAL) approaches.
IP-XACT based tools are used for automatically configuring the environment for
various imaging IPs/SoCs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2860</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2860</id><created>2013-01-14</created><authors><author><keyname>Huang</keyname><forenames>Wentao</forenames></author><author><keyname>Ho</keyname><forenames>Tracey</forenames></author><author><keyname>Yao</keyname><forenames>Hongyi</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author></authors><title>Rateless Resilient Network Coding Against Byzantine Adversaries</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers rateless network error correction codes for reliable
multicast in the presence of adversarial errors. Most existing network error
correction codes are designed for a given network capacity and maximum number
of errors known a priori to the encoder and decoder. However, in certain
practical settings it may be necessary to operate without such a priori
knowledge. We present rateless coding schemes for two adversarial models, where
the source sends more redundancy over time, until decoding succeeds. The first
model assumes there is a secret channel between the source and the destination
that the adversaries cannot overhear. The rate of the channel is negligible
compared to the main network. In the second model, instead of a secret channel,
the source and destination share random secrets independent of the input
information. The amount of secret information required is negligible compared
to the amount of information sent. Both schemes are optimal in that decoding
succeeds with high probability when the total amount of information received by
the sink satisfies the cut set bound with respect to the amount of message and
error information. The schemes are distributed, polynomial-time and end-to-end
in that other than the source and destination nodes, other intermediate nodes
carry out classical random linear network coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2866</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2866</id><created>2013-01-14</created><updated>2013-04-28</updated><authors><author><keyname>Efendiev</keyname><forenames>Yalchin</forenames></author><author><keyname>Galvis</keyname><forenames>Juan</forenames></author><author><keyname>Hou</keyname><forenames>Thomas Y.</forenames></author></authors><title>Generalized Multiscale Finite Element Methods (GMsFEM)</title><categories>math.NA cs.CE cs.NA math.AP</categories><comments>Revised version</comments><doi>10.1016/j.jcp.2013.04.045</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a general approach called Generalized Multiscale
Finite Element Method (GMsFEM) for performing multiscale simulations for
problems without scale separation over a complex input space. As in multiscale
finite element methods (MsFEMs), the main idea of the proposed approach is to
construct a small dimensional local solution space that can be used to generate
an efficient and accurate approximation to the multiscale solution with a
potentially high dimensional input parameter space. In the proposed approach,
we present a general procedure to construct the offline space that is used for
a systematic enrichment of the coarse solution space in the online stage. The
enrichment in the online stage is performed based on a spectral decomposition
of the offline space. In the online stage, for any input parameter, a
multiscale space is constructed to solve the global problem on a coarse grid.
The online space is constructed via a spectral decomposition of the offline
space and by choosing the eigenvectors corresponding to the largest
eigenvalues. The computational saving is due to the fact that the construction
of the online multiscale space for any input parameter is fast and this space
can be re-used for solving the forward problem with any forcing and boundary
condition. Compared with the other approaches where global snapshots are used,
the local approach that we present in this paper allows us to eliminate
unnecessary degrees of freedom on a coarse-grid level. We present various
examples in the paper and some numerical results to demonstrate the
effectiveness of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2875</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2875</id><created>2013-01-14</created><updated>2013-12-07</updated><authors><author><keyname>Maurer</keyname><forenames>Alexandre</forenames><affiliation>LIP6, LINCS</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LIP6, LINCS, IUF</affiliation></author></authors><title>On Byzantine Broadcast in Planar Graphs</title><categories>cs.DS cs.CR cs.DC cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of reliably broadcasting information in a multihop
asynchronous network in the presence of Byzantine failures: some nodes may
exhibit unpredictable malicious behavior. We focus on completely decentralized
solutions. Few Byzantine-robust algorithms exist for loosely connected
networks. A recent solution guarantees reliable broadcast on a torus when D &gt;
4, D being the minimal distance between two Byzantine nodes. In this paper, we
generalize this result to 4-connected planar graphs. We show that reliable
broadcast can be guaranteed when D &gt; Z, Z being the maximal number of edges per
polygon. We also show that this bound on D is a lower bound for this class of
graphs. Our solution has the same time complexity as a simple broadcast. This
is also the first solution where the memory required increases linearly
(instead of exponentially) with the size of transmitted information. Important
disclaimer: these results have NOT yet been published in an international
conference or journal. This is just a technical report presenting intermediary
and incomplete results. A generalized version of these results may be under
submission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2880</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2880</id><created>2013-01-14</created><authors><author><keyname>McQuillan</keyname><forenames>Colin</forenames></author></authors><title>Approximating Holant problems by winding</title><categories>cs.CC cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an FPRAS for Holant problems with parity constraints and
not-all-equal constraints, a generalisation of the problem of counting
sink-free-orientations. The approach combines a sampler for near-assignments of
&quot;windable&quot; functions -- using the cycle-unwinding canonical paths technique of
Jerrum and Sinclair -- with a bound on the weight of near-assignments. The
proof generalises to a larger class of Holant problems; we characterise this
class and show that it cannot be extended by expressibility reductions.
  We then ask whether windability is equivalent to expressibility by matchings
circuits (an analogue of matchgates), and give a positive answer for functions
of arity three.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2884</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2884</id><created>2013-01-14</created><authors><author><keyname>Ngo</keyname><forenames>Anh Cat Le</forenames></author><author><keyname>Ang</keyname><forenames>Kenneth Li-Minn</forenames></author><author><keyname>Seng</keyname><forenames>Jasmine Kah-Phooi</forenames></author><author><keyname>Qiu</keyname><forenames>Guoping</forenames></author></authors><title>Wavelet-based Scale Saliency</title><categories>cs.CV</categories><comments>Partly published in ACIIDS 2013 - Kuala Lumpur Malaysia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Both pixel-based scale saliency (PSS) and basis project methods focus on
multiscale analysis of data content and structure. Their theoretical relations
and practical combination are previously discussed. However, no models have
ever been proposed for calculating scale saliency on basis-projected
descriptors since then. This paper extend those ideas into mathematical models
and implement them in the wavelet-based scale saliency (WSS). While PSS uses
pixel-value descriptors, WSS treats wavelet sub-bands as basis descriptors. The
paper discusses different wavelet descriptors: discrete wavelet transform
(DWT), wavelet packet transform (DWPT), quaternion wavelet transform (QWT) and
best basis quaternion wavelet packet transform (QWPTBB). WSS saliency maps of
different descriptors are generated and compared against other saliency methods
by both quantitative and quanlitative methods. Quantitative results, ROC
curves, AUC values and NSS values are collected from simulations on Bruce and
Kootstra image databases with human eye-tracking data as ground-truth.
Furthermore, qualitative visual results of saliency maps are analyzed and
compared against each other as well as eye-tracking data inclusive in the
databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2903</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2903</id><created>2013-01-14</created><authors><author><keyname>Scherer</keyname><forenames>Gabriel</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>R&#xe9;my</keyname><forenames>Didier</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>GADTs meet subtyping</title><categories>cs.PL</categories><comments>arXiv admin note: substantial text overlap with arXiv:1210.5935</comments><proxy>ccsd</proxy><journal-ref>22nd European Symposium on Programming (ESOP), Rome : Italy (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While generalized algebraic datatypes (\GADTs) are now considered
well-understood, adding them to a language with a notion of subtyping comes
with a few surprises. What does it mean for a \GADT parameter to be covariant?
The answer turns out to be quite subtle. It involves fine-grained properties of
the subtyping relation that raise interesting design questions. We allow
variance annotations in \GADT definitions, study their soundness, and present a
sound and complete algorithm to check them. Our work may be applied to
real-world ML-like languages with explicit subtyping such as OCaml, or to
languages with general subtyping constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2907</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2907</id><created>2013-01-14</created><authors><author><keyname>Khadir</keyname><forenames>Omar</forenames></author></authors><title>Conditions on the generator for forging ElGamal signature</title><categories>cs.CR cs.IT math.IT</categories><msc-class>94A60</msc-class><journal-ref>International Journal of Pure and Applied mathematics, (2011),
  Vol.70, No 7, pp.939-949</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes new conditions on parameters selection that lead to an
efficient algorithm for forging ElGamal digital signature. Our work is inspired
by Bleichenbacher's ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2935</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2935</id><created>2013-01-14</created><authors><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Fang</keyname><forenames>Yong</forenames></author><author><keyname>Vandendorpe</keyname><forenames>Luc</forenames></author></authors><title>Novel Subcarrier-pair based Opportunistic DF Protocol for Cooperative
  Downlink OFDMA</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages, accepted by 2013 IEEE Wireless Communications and Networking
  Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel subcarrier-pair based opportunistic DF protocol is proposed for
cooperative downlink OFDMA transmission aided by a decode-and-forward (DF)
relay. Specifically, user message bits are transmitted in two consecutive
equal-duration time slots. A subcarrier in the first slot can be paired with a
subcarrier in the second slot for the DF relay-aided transmission to a user. In
particular, the source and the relay can transmit simultaneously to implement
beamforming at the subcarrier in the second slot for the relay-aided
transmission. Each unpaired subcarrier in either the first or second slot is
used by the source for direct transmission to a user without the relay's
assistance. The sum rate maximized resource allocation (RA) problem is
addressed for this protocol under a total power constraint. It is shown that
the novel protocol leads to a maximum sum rate greater than or equal to that
for a benchmark one, which does not allow the source to implement beamforming
at the subcarrier in the second slot for the relay-aided transmission. Then, a
polynomial-complexity RA algorithm is developed to find an (at least
approximately) optimum resource allocation (i.e., source/relay power,
subcarrier pairing and assignment to users) for either the proposed or
benchmark protocol. Numerical experiments illustrate that the novel protocol
can lead to a much greater sum rate than the benchmark one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2941</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2941</id><created>2013-01-14</created><authors><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Fang</keyname><forenames>Yong</forenames></author><author><keyname>Vandendorpe</keyname><forenames>Luc</forenames></author></authors><title>Power minimization for OFDM Transmission with Subcarrier-pair based
  Opportunistic DF Relaying</title><categories>cs.IT cs.NI math.IT</categories><comments>4 pages, accepted by IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a sum-power minimized resource allocation (RA) algorithm
subject to a sum-rate constraint for cooperative orthogonal frequency division
modulation (OFDM) transmission with subcarrier-pair based opportunistic
decode-and-forward (DF) relaying. The improved DF protocol first proposed in
[1] is used with optimized subcarrier pairing. Instrumental to the RA algorithm
design is appropriate definition of variables to represent source/relay power
allocation, subcarrier pairing and transmission-mode selection elegantly, so
that after continuous relaxation, the dual method and the Hungarian algorithm
can be used to find an (at least approximately) optimum RA with polynomial
complexity. Moreover, the bisection method is used to speed up the search of
the optimum Lagrange multiplier for the dual method. Numerical results are
shown to illustrate the power-reduction benefit of the improved DF protocol
with optimized subcarrier pairing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2944</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2944</id><created>2013-01-14</created><updated>2013-05-22</updated><authors><author><keyname>Rybak</keyname><forenames>Marcin</forenames></author><author><keyname>Kulakowski</keyname><forenames>Krzysztof</forenames></author></authors><title>Competing of Sznajd and voter dynamics in the Watts-Strogatz network</title><categories>physics.soc-ph cs.SI physics.comp-ph</categories><comments>10 pages, 5 figures</comments><journal-ref>Acta Physica Polonica B 44 (2013) 1007</journal-ref><doi>10.5506/APhysPolB.44.1007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the Watts-Strogatz network with the clustering coefficient C
dependent on the rewiring probability. The network is an area of two opposite
contact processes, where nodes can be in two states, S or D. One of the
processes is governed by the Sznajd dynamics: if there are two connected nodes
in D-state, all their neighbors become D with probability p. For the opposite
process it is sufficient to have only one neighbor in state S; this transition
occurs with probability 1. The concentration of S-nodes changes abruptly at
given value of the probability p. The result is that for small p, in
clusterized networks the activation of S nodes prevails. This result is
explained by a comparison of two limit cases: the Watts-Strogatz network
without rewiring, where C=0.5, and the Bethe lattice where C=0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2952</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2952</id><created>2013-01-14</created><updated>2013-07-16</updated><authors><author><keyname>De Domenico</keyname><forenames>M.</forenames></author><author><keyname>Lima</keyname><forenames>A.</forenames></author><author><keyname>Mougel</keyname><forenames>P.</forenames></author><author><keyname>Musolesi</keyname><forenames>M.</forenames></author></authors><title>The Anatomy of a Scientific Rumor</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>11 pages, 8 figures</comments><journal-ref>Scientific Reports 3, 2980 (2013)</journal-ref><doi>10.1038/srep02980</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The announcement of the discovery of a Higgs boson-like particle at CERN will
be remembered as one of the milestones of the scientific endeavor of the 21st
century. In this paper we present a study of information spreading processes on
Twitter before, during and after the announcement of the discovery of a new
particle with the features of the elusive Higgs boson on 4th July 2012. We
report evidence for non-trivial spatio-temporal patterns in user activities at
individual and global level, such as tweeting, re-tweeting and replying to
existing tweets. We provide a possible explanation for the observed
time-varying dynamics of user activities during the spreading of this
scientific &quot;rumor&quot;. We model the information spreading in the corresponding
network of individuals who posted a tweet related to the Higgs boson discovery.
Finally, we show that we are able to reproduce the global behavior of about
500,000 individuals with remarkable accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2957</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2957</id><created>2013-01-14</created><authors><author><keyname>Li</keyname><forenames>Jiankou</forenames></author><author><keyname>Li</keyname><forenames>Angsheng</forenames></author></authors><title>Characters and patterns of communities in networks</title><categories>cs.SI physics.soc-ph</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we propose some new notions to characterize and analyze the
communities. The new notions are general characters of the communities or local
structures of networks. At first, we introduce the notions of internal
dominating set and external dominating set of a community. We show that most
communities in real networks have a small internal dominating set and a small
external dominating set, and that the internal dominating set of a community
keeps much of the information of the community. Secondly, based on the notions
of the internal dominating set and the external dominating set, we define an
internal slope (ISlope, for short) and an external slope (ESlope, for short) to
measure the internal heterogeneity and external heterogeneity of a community
respectively. We show that the internal slope (ISlope) of a community largely
determines the structure of the community, that most communities in real
networks are heterogeneous, meaning that most of the communities have a
core/periphery structure, and that both ISlopes and ESlopes (reflecting the
structure of communities) of all the communities of a network approximately
follow a normal distribution. Therefore typical values of both ISolpes and
ESoples of all the communities of a given network are in a narrow interval, and
there is only a small number of communities having ISlopes or ESlopes out of
the range of typical values of the ISlopes and ESlopes of the network. Finally,
we show that all the communities of the real networks we studied, have a three
degree separation phenomenon, that is, the average distance of communities is
approximately 3, implying a general property of true communities for many real
networks, and that good community finding algorithms find communities that
amplify clustering coefficients of the networks, for many real networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2959</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2959</id><created>2013-01-14</created><updated>2013-02-01</updated><authors><author><keyname>Piniello</keyname><forenames>Jean</forenames></author></authors><title>El\'ements pour une th\'eorie des r\'eseaux en phase d'apprentissage</title><categories>nlin.AO cs.NE nlin.CD</categories><comments>47 pages; Note: This study has not yet been translated in English.
  This Version2 is identical with Version1, but the ties are now operational</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study deals with the evolution of the so called intelligent networks
(insect society without leader, cells of an organism, brain...) during their
apprenticeship period. The used formalism draws one's inspiration from the one
of the Quantum field theory (Principle of stationary action, gauge fields,
invariance by symmetry transformations...). After a recall of some definitions,
we consider at first the free network, that is to say which does not exchange
any information with outside. Then we study the evolution of the network
connected with its environment, that is to say immersed into an information
field created by this environment which so dictates to it the apprenticeship
constraints. At that time, we obtain Lagrange equations which solutions
describe the network evolution during the whole apprenticeship period. Finally,
while proceeding with the same formalism inspiration, we suggest other study
ways capable of evolving the knowledge in the considered scope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2967</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2967</id><created>2013-01-14</created><authors><author><keyname>Bilotta</keyname><forenames>Stefano</forenames></author><author><keyname>Pergola</keyname><forenames>Elisa</forenames></author><author><keyname>Pinzani</keyname><forenames>Renzo</forenames></author><author><keyname>Rinaldi</keyname><forenames>Simone</forenames></author></authors><title>Recurrence relations versus succession rules</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a method to pass from a recurrence relation having
constant coefficients (in short, a C-recurrence) to a finite succession rule
defining the same number sequence. We recall that succession rules are a
recently studied tool for the enumeration of combinatorial objects related to
the ECO method. We also discuss the applicability of our method as a test for
the positivity of a number sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2976</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2976</id><created>2013-01-14</created><authors><author><keyname>Wolff</keyname><forenames>Ran</forenames></author></authors><title>Local Thresholding on Distributed Hash Tables</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a binary routing tree protocol for distributed hash table
overlays. Using this protocol each peer can independently route messages to its
parent and two descendants on the fly without any maintenance, global context,
and synchronization. The protocol is then extended to support tree change
notification with similar efficiency. The resulting tree is almost perfectly
dense and balanced, and has O(1) stretch if the distributed hash table is
symmetric Chord. We use the tree routing protocol to overcome the main
impediment for implementation of local thresholding algorithms in peer-to-peer
systems -- their requirement for cycle free routing. Direct comparison of a
gossip-based algorithm and a corresponding local thresholding algorithm on a
majority voting problem reveals that the latter obtains superior accuracy using
a fraction of the communication overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.2995</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.2995</id><created>2013-01-14</created><updated>2013-05-10</updated><authors><author><keyname>Garc&#xed;a</keyname><forenames>David</forenames></author><author><keyname>Tanase</keyname><forenames>Dorian</forenames></author></authors><title>Measuring Cultural Dynamics Through the Eurovision Song Contest</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>Submitted to Advances in Complex Systems</comments><journal-ref>Advances in Complex Systems, Vol 16, No 8 (2013) pp 33</journal-ref><doi>10.1142/S0219525913500379</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring culture and its dynamics through surveys has important limitations,
but the emerging field of computational social science allows us to overcome
them by analyzing large-scale datasets. In this article, we study cultural
dynamics through the votes in the Eurovision song contest, which are decided by
a crowd-based scheme in which viewers vote through mobile phone messages.
Taking into account asymmetries and imperfect perception of culture, we measure
cultural relations among European countries in terms of cultural affinity. We
propose the Friend-or-Foe coefficient, a metric to measure voting biases among
participants of a Eurovision contest. We validate how this metric represents
cultural affinity through its relation with known cultural distances, and
through numerical analysis of biased Eurovision contests. We apply this metric
to the historical set of Eurovision contests from 1975 to 2012, finding new
patterns of stronger modularity than using votes alone. Furthermore, we define
a measure of polarization that, when applied to empirical data, shows a sharp
increase within EU countries during 2010 and 2011. We empirically validate the
relation between this polarization and economic indicators in the EU, showing
how political decisions influence both the economy and the way citizens relate
to the culture of other EU members.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3003</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3003</id><created>2013-01-14</created><authors><author><keyname>Muralidharan</keyname><forenames>Vijayvaradharaj T.</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>On the Vector Linear Solvability of Networks and Discrete Polymatroids</title><categories>cs.IT math.IT</categories><comments>11 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the vector linear solvability of networks over a field
$\mathbb{F}_q.$ It is well known that a scalar linear solution over
$\mathbb{F}_q$ exists for a network if and only if the network is
\textit{matroidal} with respect to a \textit{matroid} representable over
$\mathbb{F}_q.$ A \textit{discrete polymatroid} is the multi-set analogue of a
matroid. In this paper, a \textit{discrete polymatroidal} network is defined
and it is shown that a vector linear solution over a field $\mathbb{F}_q$
exists for a network if and only if the network is discrete polymatroidal with
respect to a discrete polymatroid representable over $\mathbb{F}_q.$ An
algorithm to construct networks starting from a discrete polymatroid is
provided. Every representation over $\mathbb{F}_q$ for the discrete
polymatroid, results in a vector linear solution over $\mathbb{F}_q$ for the
constructed network. Examples which illustrate the construction algorithm are
provided, in which the resulting networks admit vector linear solution but no
scalar linear solution over $\mathbb{F}_q.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3007</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3007</id><created>2013-01-14</created><authors><author><keyname>Hong</keyname><forenames>Dohy</forenames></author><author><keyname>Mathieu</keyname><forenames>Fabien</forenames></author><author><keyname>Burnside</keyname><forenames>G&#xe9;rard</forenames></author></authors><title>Convergence of the D-iteration algorithm: convergence rate and
  asynchronous distributed scheme</title><categories>cs.NA cs.DC math.NA</categories><comments>9 pages</comments><acm-class>G.1.3; G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we define the general framework to describe the diffusion
operators associated to a positive matrix. We define the equations associated
to diffusion operators and present some general properties of their state
vectors. We show how this can be applied to prove and improve the convergence
of a fixed point problem associated to the matrix iteration scheme, including
for distributed computation framework. The approach can be understood as a
decomposition of the matrix-vector product operation in elementary operations
at the vector entry level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3021</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3021</id><created>2013-01-14</created><authors><author><keyname>Strohmer</keyname><forenames>Thomas</forenames></author><author><keyname>Wang</keyname><forenames>Haichao</forenames></author></authors><title>Accurate detection of moving targets via random sensor arrays and
  Kerdock codes</title><categories>math.NA cs.IT math.IT</categories><doi>10.1088/0266-5611/29/8/085001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The detection and parameter estimation of moving targets is one of the most
important tasks in radar. Arrays of randomly distributed antennas have been
popular for this purpose for about half a century. Yet, surprisingly little
rigorous mathematical theory exists for random arrays that addresses
fundamental question such as how many targets can be recovered, at what
resolution, at which noise level, and with which algorithm. In a different line
of research in radar, mathematicians and engineers have invested significant
effort into the design of radar transmission waveforms which satisfy various
desirable properties. In this paper we bring these two seemingly unrelated
areas together. Using tools from compressive sensing we derive a theoretical
framework for the recovery of targets in the azimuth-range-Doppler domain via
random antennas arrays. In one manifestation of our theory we use Kerdock codes
as transmission waveforms and exploit some of their peculiar properties in our
analysis. Our paper provides two main contributions: (i) We derive the first
rigorous mathematical theory for the detection of moving targets using random
sensor arrays. (ii) The transmitted waveforms satisfy a variety of properties
that are very desirable and important from a practical viewpoint. Thus our
approach does not just lead to useful theoretical insights, but is also of
practical importance. Various extensions of our results are derived and
numerical simulations confirming our theory are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3022</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3022</id><created>2013-01-14</created><authors><author><keyname>Malik</keyname><forenames>Muhammad Yasir</forenames></author></authors><title>An Outline of Security in Wireless Sensor Networks: Threats,
  Countermeasures and Implementations</title><categories>cs.NI cs.CR</categories><journal-ref>Wireless Sensor Networks and Energy Efficiency: Protocols, Routing
  and Management. 2011</journal-ref><doi>10.4018/978-1-4666-0101-7.ch024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the expansion of wireless sensor networks (WSNs), the need for securing
the data flow through these networks is increasing. These sensor networks allow
for easy-to-apply and flexible installations which have enabled them to be used
for numerous applications. Due to these properties, they face distinct
information security threats. Security of the data flowing through across
networks provides the researchers with an interesting and intriguing potential
for research. Design of these networks to ensure the protection of data faces
the constraints of limited power and processing resources. We provide the
basics of wireless sensor network security to help the researchers and
engineers in better understanding of this applications field. In this chapter,
we will provide the basics of information security with special emphasis on
WSNs. The chapter will also give an overview of the information security
requirements in these networks. Threats to the security of data in WSNs and
some of their counter measures are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3047</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3047</id><created>2013-01-14</created><authors><author><keyname>Blech</keyname><forenames>Jan Olaf</forenames></author><author><keyname>Biha</keyname><forenames>Sidi Ould</forenames></author></authors><title>On Formal Reasoning on the Semantics of PLC using Coq</title><categories>cs.SE cs.PL</categories><comments>arXiv admin note: text overlap with arXiv:1102.3529</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programmable Logic Controllers (PLC) and its programming standard IEC 61131-3
are widely used in embedded systems for the industrial automation domain. We
propose a framework for the formal treatment of PLC based on the IEC 61131-3
standard. A PLC system description typically combines code written in different
languages that are defined in IEC 61131-3. For the top-level specification we
regard the Sequential Function Charts (SFC) language, a graphical high-level
language that allows to describe the main control-flow of the system. In
addition to this, we describe the Instruction List (IL) language -- an assembly
like language -- and two other graphical languages: Ladder Diagrams (LD) and
Function Block Diagrams (FBD). IL, LD, and FBD are used to describe more low
level structures of a PLC. We formalize the semantics of these languages and
describe and prove relations between them. Formalization and associated proofs
are carried out using the proof assistant Coq. In addition to this, we present
work on a tool for automatically generating SFC representations from a
graphical description -- the IL and LD languages can be handled in Coq directly
-- and its usage for verification purposes. We sketch possible usages of our
formal framework, and present an example application for a PLC in a project
demonstrator and prove safety properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3093</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3093</id><created>2013-01-14</created><updated>2013-01-15</updated><authors><author><keyname>Nuriyev</keyname><forenames>Dmitriy</forenames></author></authors><title>A DP Approach to Hamiltonian Path Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Dynamic Programming based polynomial worst case time and space algorithm is
described for computing Hamiltonian Path of a directed graph. Complexity
constructive proofs along with a tested C++ implementation are provided as
well. The result is obtained via the use of original colored hypergraph
structures in order to maintain and update the necessary DP states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3106</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3106</id><created>2013-01-14</created><updated>2013-09-29</updated><authors><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>Topological Interference Management through Index Coding</title><categories>cs.IT math.IT</categories><comments>Revised for the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies linear interference networks, both wired and wireless, with
no channel state information at the transmitters (CSIT) except a coarse
knowledge of the end-to-end one-hop topology of the network that only allows a
distinction between weak (zero) and significant (non-zero) channels and no
further knowledge of the channel coefficients' realizations. The network
capacity (wired) and DoF (wireless) are found to be bounded above by the
capacity of an index coding problem for which the antidote graph is the
complement of the given interference graph. The problems are shown to be
equivalent under linear solutions. An interference alignment perspective is
then used to translate the existing index coding solutions into the wired
network capacity and wireless network DoF solutions, as well as to find new and
unified solutions to different classes of all three problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3118</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3118</id><created>2013-01-14</created><authors><author><keyname>Nasar-Ullah</keyname><forenames>Qasim</forenames></author></authors><title>A parallel implementation of a derivative pricing model incorporating
  SABR calibration and probability lookup tables</title><categories>cs.DC cs.CE q-fin.CP</categories><comments>21 pages, 16 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a high performance parallel implementation of a derivative
pricing model, within which we introduce a new parallel method for the
calibration of the industry standard SABR (stochastic-\alpha \beta \rho)
stochastic volatility model using three strike inputs. SABR calibration
involves a non-linear three dimensional minimisation and parallelisation is
achieved by incorporating several assumptions unique to the SABR class of
models. Our calibration method is based on principles of surface intersection,
guarantees convergence to a unique solution and operates by iteratively
refining a two dimensional grid with local mesh refinement. As part of our
pricing model we additionally present a fast parallel iterative algorithm for
the creation of dynamically sized cumulative probability lookup tables that are
able to cap maximum estimated linear interpolation error. We optimise
performance for probability distributions that exhibit clustering of linear
interpolation error. We also make an empirical assessment of error propagation
through our pricing model as a result of changes in accuracy parameters within
the pricing model's multiple algorithmic steps. Algorithms are implemented on a
GPU (graphics processing unit) using Nvidia's Fermi architecture. The pricing
model targets the evaluation of spread options using copula methods, however
the presented algorithms can be applied to a wider class of financial
instruments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3120</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3120</id><created>2013-01-14</created><authors><author><keyname>Ronhovde</keyname><forenames>Richard K. Darst David R. Reichman Peter</forenames></author><author><keyname>Nussinov</keyname><forenames>Zohar</forenames></author></authors><title>An edge density definition of overlapping and weighted graph communities</title><categories>physics.soc-ph cs.SI</categories><comments>22 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection in networks refers to the process of seeking strongly
internally connected groups of nodes which are weakly externally connected. In
this work, we introduce and study a community definition based on internal edge
density. Beginning with the simple concept that edge density equals number of
edges divided by maximal number of edges, we apply this definition to a variety
of node and community arrangements to show that our definition yields sensible
results. Our community definition is equivalent to that of the Absolute Potts
Model community detection method (Phys. Rev. E 81, 046114 (2010)), and the
performance of that method validates the usefulness of our definition across a
wide variety of network types. We discuss how this definition can be extended
to weighted, and multigraphs, and how the definition is capable of handling
overlapping communities and local algorithms. We further validate our
definition against the recently proposed Affiliation Graph Model
(arXiv:1205.6228 [cs.SI]) and show that we can precisely solve these
benchmarks. More than proposing an end-all community definition, we explain how
studying the detailed properties of community definitions is important in order
to validate that definitions do not have negative analytic properties. We urge
that community definitions be separated from community detection algorithms and
propose that community definitions be further evaluated by criteria such as
these.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3127</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3127</id><created>2013-01-14</created><updated>2013-01-18</updated><authors><author><keyname>Herbreteau</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Srivathsan</keyname><forenames>B.</forenames><affiliation>RWTH</affiliation></author><author><keyname>Walukiewicz</keyname><forenames>Igor</forenames><affiliation>LaBRI</affiliation></author></authors><title>Lazy abstractions for timed automata</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the reachability problem for timed automata. A standard solution
to this problem involves computing a search tree whose nodes are abstractions
of zones. For efficiency reasons, they are parametrized by the maximal lower
and upper bounds (LU-bounds) occurring in the guards of the automaton. We
propose an algorithm that is updating LU-bounds during exploration of the
search tree. In order to keep them as small as possible, the bounds are refined
only when they enable a transition that is impossible in the unabstracted
system. So our algorithm can be seen as a kind of lazy CEGAR algorithm for
timed automata. We show that on several standard benchmarks, the algorithm is
capable of keeping very small LU-bounds, and in consequence reduce the search
space substantially.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3154</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3154</id><created>2013-01-14</created><authors><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Coherent Quantum Filtering for Physically Realizable Linear Quantum
  Plants</title><categories>quant-ph cs.SY math.OC math.PR</categories><comments>14 pages, 1 figure, submitted to ECC 2013</comments><msc-class>81Q93, 81S25, 93E11 (Primary) 49J50, 58C20, 49M05 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is concerned with a problem of coherent (measurement-free)
filtering for physically realizable (PR) linear quantum plants. The state
variables of such systems satisfy canonical commutation relations and are
governed by linear quantum stochastic differential equations, dynamically
equivalent to those of an open quantum harmonic oscillator. The problem is to
design another PR quantum system, connected unilaterally to the output of the
plant and playing the role of a quantum filter, so as to minimize a mean square
discrepancy between the dynamic variables of the plant and the output of the
filter. This coherent quantum filtering (CQF) formulation is a simplified
feedback-free version of the coherent quantum LQG control problem which remains
open despite recent studies. The CQF problem is transformed into a constrained
covariance control problem which is treated by using the Frechet
differentiation of an appropriate Lagrange function with respect to the
matrices of the filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3174</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3174</id><created>2013-01-14</created><updated>2013-11-22</updated><authors><author><keyname>Khalek</keyname><forenames>Amin Abdel</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Loss Visibility Optimized Real-time Video Transmission over MIMO Systems</title><categories>cs.IT cs.MM math.IT</categories><comments>Submitted to IEEE Transactions on Circuits and Systems for Video
  Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The structured nature of video data motivates introducing video-aware
decisions that make use of this structure for improved video transmission over
wireless networks. In this paper, we introduce an architecture for real-time
video transmission over multiple-input multiple-output (MIMO) wireless
communication systems using loss visibility side information. We quantify the
perceptual importance of a packet through the packet loss visibility and use
the loss visibility distribution to provide a notion of relative packet
importance. To jointly achieve video quality and low latency, we define the
optimization objective function as the throughput weighted by the loss
visibility of each packet, a proxy for the total perceptual value of successful
packets per unit time. We solve the problem of mapping video packets to MIMO
subchannels and adapting per-stream rates to maximize the proposed objective.
We show that the solution enables jointly reaping gains in terms of improved
video quality and lower latency. Optimized packet-stream mapping enables
transmission of more relevant packets over more reliable streams while unequal
modulation opportunistically increases the transmission rate on the stronger
streams to enable low latency delivery of high priority packets. We extend the
solution to capture codebook-based limited feedback and MIMO mode adaptation.
Results show that the composite quality and throughput gains are significant
under full channel state information as well as limited feedback. Tested on
H.264-encoded video sequences, for a 4x4 MIMO with 3 spatial streams, the
proposed architecture achieves 8 dB power reduction for the same video quality
and supports 2.4x higher throughput due to unequal modulation. Furthermore, the
gains are achieved at the expense of few bits of cross-layer overhead rather
than a complex cross-layer design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3185</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3185</id><created>2013-01-14</created><authors><author><keyname>Jaramillo</keyname><forenames>Juan Jose</forenames></author><author><keyname>Ying</keyname><forenames>Lei</forenames></author></authors><title>Distributed Admission Control without Knowledge of the Capacity Region</title><categories>cs.NI</categories><comments>Extended version of paper presented in IEEE INFOCOM 2013
  Mini-Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of distributed admission control without knowledge of
the capacity region in single-hop wireless networks, for flows that require a
pre-specified bandwidth from the network. We present an optimization framework
that allows us to design a scheduler and resource allocator, and by properly
choosing a suitable utility function in the resource allocator, we prove that
existing flows can be served with a pre-specified bandwidth, while the link
requesting admission can determine the largest rate that it can get such that
it does not interfere with the allocation to the existing flows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3187</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3187</id><created>2013-01-14</created><authors><author><keyname>Rios</keyname><forenames>Mery Yolima Uribe</forenames></author><author><keyname>P&#xe1;ez</keyname><forenames>Rafael V.</forenames></author></authors><title>Recommendation system for information services adapted, over terrestrial
  digital television</title><categories>cs.CY</categories><comments>13 pages, 8 figures and 3 tables</comments><msc-class>68U35</msc-class><doi>10.5121/cseij.2012.2602</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The development of digital television in Colombia has grown in last years,
specially the digital terrestrial television (DTT), which is an essential part
to the projects of National Minister of ICT, thanks to the big distribution and
use of the television network and Internet in the country. This article
explains how joining different technologies like social networks, information
adaptation and DTT, to get an application that offers information services to
users, based on their data, preferences, inclinations, use and interaction with
others users and groups inside the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3189</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3189</id><created>2013-01-14</created><updated>2015-02-16</updated><authors><author><keyname>Seiller</keyname><forenames>Thomas</forenames></author><author><keyname>Aubert</keyname><forenames>Cl&#xe9;ment</forenames></author></authors><title>Logarithmic Space and Permutations</title><categories>cs.LO cs.CC math.LO</categories><comments>Accepted for publication in Information &amp; Computation -- Special
  issue on Implicit Computational Complexity</comments><msc-class>03D15</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In a recent work, Girard proposed a new and innovative approach to
computational complexity based on the proofs-as-programs correspondence. In a
previous paper, the authors showed how Girard proposal succeeds in obtaining a
new characterization of co-NL languages as a set of operators acting on a
Hilbert Space. In this paper, we extend this work by showing that it is also
possible to define a set of operators characterizing the class L of logarithmic
space languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3192</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3192</id><created>2013-01-14</created><authors><author><keyname>Lee</keyname><forenames>Joonseok</forenames></author><author><keyname>Kim</keyname><forenames>Seungyeon</forenames></author><author><keyname>Lebanon</keyname><forenames>Guy</forenames></author><author><keyname>Singer</keyname><forenames>Yoram</forenames></author></authors><title>Matrix Approximation under Local Low-Rank Assumption</title><categories>cs.LG stat.ML</categories><comments>3 pages, 2 figures, Workshop submission to the First International
  Conference on Learning Representations (ICLR)</comments><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix approximation is a common tool in machine learning for building
accurate prediction models for recommendation systems, text mining, and
computer vision. A prevalent assumption in constructing matrix approximations
is that the partially observed matrix is of low-rank. We propose a new matrix
approximation model where we assume instead that the matrix is only locally of
low-rank, leading to a representation of the observed matrix as a weighted sum
of low-rank matrices. We analyze the accuracy of the proposed local low-rank
modeling. Our experiments show improvements in prediction accuracy in
recommendation tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3193</identifier>
 <datestamp>2014-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3193</id><created>2013-01-14</created><authors><author><keyname>Domke</keyname><forenames>Justin</forenames></author></authors><title>Learning Graphical Model Parameters with Approximate Marginal Inference</title><categories>cs.LG cs.CV</categories><comments>To Appear, IEEE Transactions on Pattern Analysis and Machine
  Intelligence</comments><acm-class>I.2.6; I.4.8</acm-class><doi>10.1109/TPAMI.2013.31</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Likelihood based-learning of graphical models faces challenges of
computational-complexity and robustness to model mis-specification. This paper
studies methods that fit parameters directly to maximize a measure of the
accuracy of predicted marginals, taking into account both model and inference
approximations at training time. Experiments on imaging problems suggest
marginalization-based learning performs better than likelihood-based
approximations on difficult problems where the model being fit is approximate
in nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3195</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3195</id><created>2013-01-14</created><updated>2013-03-27</updated><authors><author><keyname>Hu</keyname><forenames>Zhen</forenames></author><author><keyname>Fu</keyname><forenames>Kun</forenames></author><author><keyname>Zhang</keyname><forenames>Changshui</forenames></author></authors><title>Audio Classical Composer Identification by Deep Neural Network</title><categories>cs.NE cs.IR</categories><comments>I will update it</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Audio Classical Composer Identification (ACC) is an important problem in
Music Information Retrieval (MIR) which aims at identifying the composer for
audio classical music clips. The famous annual competition, Music Information
Retrieval Evaluation eXchange (MIREX), also takes it as one of the four
training&amp;testing tasks. We built a hybrid model based on Deep Belief Network
(DBN) and Stacked Denoising Autoencoder (SDA) to identify the composer from
audio signal. As a matter of copyright, sponsors of MIREX cannot publish their
data set. We built a comparable data set to test our model. We got an accuracy
of 76.26% in our data set which is better than some pure models and shallow
models. We think our method is promising even though we test it in a different
data set, since our data set is comparable to that in MIREX by size. We also
found that samples from different classes become farther away from each other
when transformed by more layers in our model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3210</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3210</id><created>2013-01-14</created><authors><author><keyname>Markov</keyname><forenames>Igor L.</forenames></author><author><keyname>Saeedi</keyname><forenames>Mehdi</forenames></author></authors><title>Faster Quantum Number Factoring via Circuit Synthesis</title><categories>quant-ph cs.DS cs.ET</categories><comments>4 pages, 2 figures, 1 table</comments><journal-ref>Phys. Rev. A, 87: 012310 (2013)</journal-ref><doi>10.1103/PhysRevA.87.012310</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major obstacle to implementing Shor's quantum number-factoring algorithm is
the large size of modular-exponentiation circuits. We reduce this bottleneck by
customizing reversible circuits for modular multiplication to individual runs
of Shor's algorithm. Our circuit-synthesis procedure exploits spectral
properties of multiplication operators and constructs optimized circuits from
the traces of the execution of an appropriate GCD algorithm. Empirically, gate
counts are reduced by 4-5 times, and circuit latency is reduced by larger
factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3214</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3214</id><created>2013-01-14</created><authors><author><keyname>Kim</keyname><forenames>Seungyeon</forenames></author><author><keyname>Li</keyname><forenames>Fuxin</forenames></author><author><keyname>Lebanon</keyname><forenames>Guy</forenames></author><author><keyname>Essa</keyname><forenames>Irfan</forenames></author></authors><title>The Manifold of Human Emotions</title><categories>cs.CL</categories><comments>3 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sentiment analysis predicts the presence of positive or negative emotions in
a text document. In this paper, we consider higher dimensional extensions of
the sentiment concept, which represent a richer set of human emotions. Our
approach goes beyond previous work in that our model contains a continuous
manifold rather than a finite set of human emotions. We investigate the
resulting model, compare it to psychological observations, and explore its
predictive capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3220</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3220</id><created>2013-01-14</created><authors><author><keyname>Huang</keyname><forenames>Qin</forenames></author><author><keyname>Tang</keyname><forenames>Li</forenames></author><author><keyname>Wang</keyname><forenames>Zulin</forenames></author><author><keyname>Xiong</keyname><forenames>Zixiang</forenames></author><author><keyname>He</keyname><forenames>Shanbao</forenames></author></authors><title>A Low-Complexity Encoding of Quasi-Cyclic Codes Based on Galois Fourier
  Transform</title><categories>cs.IT math.IT</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The encoding complexity of a general (en,ek) quasi-cyclic code is
O[(e^2)(n-k)k]. This paper presents a novel low-complexity encoding algorithm
for quasi-cyclic (QC) codes based on matrix transformation. First, a message
vector is encoded into a transformed codeword in the transform domain. Then,
the transmitted codeword is obtained from the transformed codeword by the
inverse Galois Fourier transform. For binary QC codes, a simple and fast
mapping is required to post-process the transformed codeword such that the
transmitted codeword is binary as well. The complexity of our proposed encoding
algorithm is O[e(n-k)k] symbol operations for non-binary codes and
O[ek(n-k)(log_2 e)] bit operations for binary codes. These complexities are
much lower than their traditional counterpart O[(e^2)(n-k)k]. For example, our
complexity of encoding a 64-ary (4095,2160) QC code is only 1.59% of that of
traditional encoding, and our complexities of encoding the binary (4095, 2160)
and (8176, 7154) QC codes are respectively 9.52% and 1.77% of those of
traditional encoding. We also study the application of our low-complexity
encoding algorithm to one of the most important subclasses of QC codes, namely
QC-LDPC codes, especially when their parity-check matrices are rank deficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3223</identifier>
 <datestamp>2013-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3223</id><created>2013-01-14</created><updated>2013-06-12</updated><authors><author><keyname>Lewko</keyname><forenames>Allison</forenames></author><author><keyname>Lewko</keyname><forenames>Mark</forenames></author></authors><title>On the Complexity of Asynchronous Agreement Against Powerful Adversaries</title><categories>cs.DC</categories><comments>18 pages, updated discussion of related work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce new techniques for proving lower bounds on the running time of
randomized algorithms for asynchronous agreement against powerful adversaries.
In particular, we define a \emph{strongly adaptive adversary} that is
computationally unbounded and has a limited ability to corrupt a dynamic subset
of processors by erasing their memories. We demonstrate that the randomized
agreement algorithms designed by Ben-Or and Bracha to tolerate crash or
Byzantine failures in the asynchronous setting extend to defeat a strongly
adaptive adversary. These algorithms have essentially perfect correctness and
termination, but at the expense of exponential running time. In the case of the
strongly adaptive adversary, we show that this dismally slow running time is
\emph{inherent}: we prove that any algorithm with essentially perfect
correctness and termination against the strongly adaptive adversary must have
exponential running time. We additionally interpret this result as yielding an
enhanced understanding of the tools needed to simultaneously achieving perfect
correctness and termination as well as fast running time for randomized
algorithms tolerating crash or Byzantine failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3224</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3224</id><created>2013-01-14</created><updated>2013-04-08</updated><authors><author><keyname>Hoffman</keyname><forenames>Judy</forenames></author><author><keyname>Rodner</keyname><forenames>Erik</forenames></author><author><keyname>Donahue</keyname><forenames>Jeff</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author><author><keyname>Saenko</keyname><forenames>Kate</forenames></author></authors><title>Efficient Learning of Domain-invariant Image Representations</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm that learns representations which explicitly
compensate for domain mismatch and which can be efficiently realized as linear
classifiers. Specifically, we form a linear transformation that maps features
from the target (test) domain to the source (training) domain as part of
training the classifier. We optimize both the transformation and classifier
parameters jointly, and introduce an efficient cost function based on
misclassification loss. Our method combines several features previously
unavailable in a single algorithm: multi-class adaptation through
representation learning, ability to map across heterogeneous feature spaces,
and scalability to large datasets. We present experiments on several image
datasets that demonstrate improved accuracy and computational advantages
compared to previous approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3226</identifier>
 <datestamp>2013-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3226</id><created>2013-01-14</created><updated>2013-05-29</updated><authors><author><keyname>Chen</keyname><forenames>Yanqing</forenames></author><author><keyname>Perozzi</keyname><forenames>Bryan</forenames></author><author><keyname>Al-Rfou</keyname><forenames>Rami</forenames></author><author><keyname>Skiena</keyname><forenames>Steven</forenames></author></authors><title>The Expressive Power of Word Embeddings</title><categories>cs.LG cs.CL stat.ML</categories><comments>submitted to ICML 2013, Deep Learning for Audio, Speech and Language
  Processing Workshop. 8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We seek to better understand the difference in quality of the several
publicly released embeddings. We propose several tasks that help to distinguish
the characteristics of different embeddings. Our evaluation of sentiment
polarity and synonym/antonym relations shows that embeddings are able to
capture surprisingly nuanced semantics even in the absence of sentence
structure. Moreover, benchmarking the embeddings shows great variance in
quality and characteristics of the semantics captured by the tested embeddings.
Finally, we show the impact of varying the number of dimensions and the
resolution of each dimension on the effective useful features captured by the
embedding space. Our contributions highlight the importance of embeddings for
NLP tasks and the effect of their quality on the final results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3230</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3230</id><created>2013-01-15</created><updated>2013-12-18</updated><authors><author><keyname>Mangipudi</keyname><forenames>Easwar Vivek</forenames></author><author><keyname>Ramaiyan</keyname><forenames>Venkatesh</forenames></author></authors><title>A Framework for Quality of Service with a Multiple Access Strategy</title><categories>cs.NI</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a problem of scheduling real-time traffic with hard delay
constraints in an unreliable wireless channel. Packets arrive at a constant
rate to the network and have to be delivered within a fixed number of slots in
a fading wireless channel. For an infrastructure mode of traffic with a
centralized scheduler, we are interested in the long time average throughput
achievable for the real time traffic. In [1], the authors have stud- ied the
feasible throughput vectors by identifying the necessary and sufficient
conditions using work load characterization. In our work, we provide a
characterization of the feasible throughput vectors using the notion of the
rate region. We then discuss an extension to the network model studied in [1]
by allowing multiple access during contention and propose an enhancement to the
rate region of the wireless network. We characterize the feasible throughput
vectors with the multiple access technique and study throughput optimal and
utility maximizing strategies for the network scenario. Using simulations, we
evaluate the performance of the proposed strategy and discuss its advantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3235</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3235</id><created>2013-01-15</created><updated>2013-12-13</updated><authors><author><keyname>Kosut</keyname><forenames>Robert L.</forenames></author><author><keyname>Grace</keyname><forenames>Matthew D.</forenames></author><author><keyname>Brif</keyname><forenames>Constantin</forenames></author></authors><title>Robust control of quantum gates via sequential convex programming</title><categories>quant-ph cs.SY</categories><comments>13 pages, 3 figures; published version</comments><journal-ref>Phys. Rev. A, 88, 052326 (2013)</journal-ref><doi>10.1103/PhysRevA.88.052326</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resource tradeoffs can often be established by solving an appropriate robust
optimization problem for a variety of scenarios involving constraints on
optimization variables and uncertainties. Using an approach based on sequential
convex programming, we demonstrate that quantum gate transformations can be
made substantially robust against uncertainties while simultaneously using
limited resources of control amplitude and bandwidth. Achieving such a high
degree of robustness requires a quantitative model that specifies the range and
character of the uncertainties. Using a model of a controlled one-qubit system
for illustrative simulations, we identify robust control fields for a universal
gate set and explore the tradeoff between the worst-case gate fidelity and the
field fluence. Our results demonstrate that, even for this simple model, there
exist a rich variety of control design possibilities. In addition, we study the
effect of noise represented by a stochastic uncertainty model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3248</identifier>
 <datestamp>2013-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3248</id><created>2013-01-15</created><updated>2013-09-08</updated><authors><author><keyname>Lin</keyname><forenames>Junhong</forenames></author><author><keyname>Li</keyname><forenames>Song</forenames></author></authors><title>Sparse Recovery with Coherent Tight Frames via Analysis Dantzig Selector
  and Analysis LASSO</title><categories>cs.IT math.IT math.NA</categories><comments>21 pages; Corrected some typos and grammatical errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article considers recovery of signals that are sparse or approximately
sparse in terms of a (possibly) highly overcomplete and coherent tight frame
from undersampled data corrupted with additive noise. We show that the properly
constrained $l_1$-analysis, called analysis Dantzig selector, stably recovers a
signal which is nearly sparse in terms of a tight frame provided that the
measurement matrix satisfies a restricted isometry property adapted to the
tight frame. As a special case, we consider the Gaussian noise. Further, under
a sparsity scenario, with high probability, the recovery error from noisy data
is within a log-like factor of the minimax risk over the class of vectors which
are at most $s$ sparse in terms of the tight frame. Similar results for the
analysis LASSO are showed.
  The above two algorithms provide guarantees only for noise that is bounded or
bounded with high probability (for example, Gaussian noise). However, when the
underlying measurements are corrupted by sparse noise, these algorithms perform
suboptimally. We demonstrate robust methods for reconstructing signals that are
nearly sparse in terms of a tight frame in the presence of bounded noise
combined with sparse noise. The analysis in this paper is based on the
restricted isometry property adapted to a tight frame, which is a natural
extension to the standard restricted isometry property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3252</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3252</id><created>2013-01-15</created><authors><author><keyname>Kao</keyname><forenames>Mong-Jen</forenames></author><author><keyname>Lee</keyname><forenames>Der-Tsai</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author></authors><title>Approximating Metrics by Tree Metrics of Small Distance-Weighted Average
  Stretch</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of how well a tree metric is able to preserve the sum of
pairwise distances of an arbitrary metric. This problem is closely related to
low-stretch metric embeddings and is interesting by its own flavor from the
line of research proposed in the literature.
  As the structure of a tree imposes great constraints on the pairwise
distances, any embedding of a metric into a tree metric is known to have
maximum pairwise stretch of $\Omega(\log n)$. We show, however, from the
perspective of average performance, there exist tree metrics which preserve the
sum of pairwise distances of the given metric up to a small constant factor,
for which we also show to be no worse than twice what we can possibly expect.
The approach we use to tackle this problem is more direct compared to a
previous result of [4], and also leads to a provably better guarantee. Second,
when the given metric is extracted from a Euclidean point set of finite
dimension $d$, we show that there exist spanning trees of the given point set
such that the sum of pairwise distances is preserved up to a constant which
depends only on $d$. Both of our proofs are constructive. The main ingredient
in our result is a special point-set decomposition which relates two
seemingly-unrelated quantities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3258</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3258</id><created>2013-01-15</created><authors><author><keyname>Khadir</keyname><forenames>Omar</forenames></author></authors><title>New variant of ElGamal signature scheme</title><categories>cs.CR cs.IT math.IT</categories><msc-class>94A60</msc-class><journal-ref>Int. Contemp. Math. Sciences, Vol. 5, 2010, no 34, pp.1653-1662</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new variant of ElGamal signature scheme is presented and its
security analyzed. We also give, for its theoretical interest, a general form
of the signature equation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3281</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3281</id><created>2013-01-15</created><authors><author><keyname>Sanchez-Elez</keyname><forenames>Marcos</forenames></author><author><keyname>Roman</keyname><forenames>Sara</forenames></author></authors><title>Reconfiguration Strategies for Online Hardware Multitasking in Embedded
  Systems</title><categories>cs.AR</categories><comments>Computer Science &amp; Engineering: An International Journal (CSEIJ),
  Vol.2, No.6, December 2012</comments><doi>10.5121/cseij.2012.2601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An intensive use of reconfigurable hardware is expected in future embedded
systems. This means that the system has to decide which tasks are more suitable
for hardware execution. In order to make an efficient use of the FPGA it is
convenient to choose one that allows hardware multitasking, which is
implemented by using partial dynamic reconfiguration. One of the challenges for
hardware multitasking in embedded systems is the online management of the only
reconfiguration port of present FPGA devices. This paper presents different
online reconfiguration scheduling strategies which assign the reconfiguration
interface resource using different criteria: workload distribution or task
deadline. The online scheduling strategies presented take efficient and fast
decisions based on the information available at each moment. Experiments have
been made in order to analyze the performance and convenience of these
reconfiguration strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3297</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3297</id><created>2013-01-15</created><updated>2013-07-19</updated><authors><author><keyname>Bergstra</keyname><forenames>J. A.</forenames></author><author><keyname>Middelburg</keyname><forenames>C. A.</forenames></author></authors><title>Instruction sequence based non-uniform complexity classes</title><categories>cs.CC cs.LO</categories><comments>33 pages, supersedes arXiv:0809.0352 [cs.CC] in many respects (see
  end of introduction); remarks added</comments><acm-class>F.1.1; F.1.3</acm-class><journal-ref>Scientific Annals of Computer Science 24(1):47--89, 2014</journal-ref><doi>10.7561/SACS.2014.1.47</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to non-uniform complexity in which single-pass
instruction sequences play a key part, and answer various questions that arise
from this approach. We introduce several kinds of non-uniform complexity
classes. One kind includes a counterpart of the well-known non-uniform
complexity class P/poly and another kind includes a counterpart of the
well-known non-uniform complexity class NP/poly. Moreover, we introduce a
general notion of completeness for the non-uniform complexity classes of the
latter kind. We also formulate a counterpart of the well-known complexity
theoretic conjecture that NP is not included in P/poly. We think that the
presented approach opens up an additional way of investigating issues
concerning non-uniform complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3299</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3299</id><created>2013-01-15</created><authors><author><keyname>Liu</keyname><forenames>Wanwei</forenames></author><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Fu</keyname><forenames>Xianjin</forenames></author><author><keyname>Wang</keyname><forenames>Ji</forenames></author><author><keyname>Dong</keyname><forenames>Wei</forenames></author><author><keyname>Mao</keyname><forenames>Xiaoguang</forenames></author></authors><title>Counterexample-Preserving Reduction for Symbolic Model Checking</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cost of LTL model checking is highly sensitive to the length of the
formula under verification. We observe that, under some specific conditions,
the input LTL formula can be reduced to an easier-to-handle one before model
checking. In our reduction, these two formulae need not to be logically
equivalent, but they share the same counterexample set w.r.t the model. In the
case that the model is symbolically represented, the condition enabling such
reduction can be detected with a lightweight effort (e.g., with SAT-solving).
In this paper, we tentatively name such technique &quot;Counterexample-Preserving
Reduction&quot; (CePRe for short), and finally the proposed technquie is
experimentally evaluated by adapting NuSMV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3302</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3302</id><created>2013-01-15</created><updated>2013-03-11</updated><authors><author><keyname>Chattopadhyay</keyname><forenames>Arpan</forenames></author><author><keyname>Coupechoux</keyname><forenames>Marceau</forenames></author><author><keyname>Kumar</keyname><forenames>Anurag</forenames></author></authors><title>Measurement Based Impromptu Deployment of a Multi-Hop Wireless Relay
  Network</title><categories>cs.NI</categories><journal-ref>Proc. of the Intl. Symposium on Modeling and Optimization in
  Mobile, Ad Hoc, and Wireless Networks (WiOpt), Tsukuba Science City, Japan,
  May 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of optimal sequential (&quot;as-you-go&quot;) deployment of
wireless relay nodes as a person walks along a line of random length (with a
known distribution). The objective is to create an impromptu multihop wireless
network for connecting a packet source to be placed at the end of the line with
a sink node located at the starting point, to operate in the light traffic
regime. As the deployment person walks along the line from the sink towards the
source, at every step, he measures the channel quality to one (or more)
previously placed relays, and places the relay nodes based on these
measurements, so as to minimize either the sum power or the maximum power from
the source to the sink node in the resultant network, subject to a constraint
on the expected number of relays placed. For each of these two objectives, two
different relay selection strategies are considered: (i) each relay
communicates with the sink via its immediate previous relay, (ii) the
communication path can skip some of the deployed relays. With appropriate
modeling assumptions, we formulate each of these problems as a Markov decision
process (MDP). We provide the optimal policy structures for all these cases,
and provide illustrations, via numerical results, for some typical parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3316</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3316</id><created>2013-01-15</created><authors><author><keyname>Champarnaud</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Dubernard</keyname><forenames>Jean-Philippe</forenames></author><author><keyname>Jeanne</keyname><forenames>Hadrien</forenames></author><author><keyname>Mignot</keyname><forenames>Ludovic</forenames></author></authors><title>Two-Sided Derivatives for Regular Expressions and for Hairpin
  Expressions</title><categories>cs.FL</categories><comments>28 pages</comments><msc-class>68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to design the polynomial construction of a finite
recognizer for hairpin completions of regular languages. This is achieved by
considering completions as new expression operators and by applying derivation
techniques to the associated extended expressions called hairpin expressions.
More precisely, we extend partial derivation of regular expressions to
two-sided partial derivation of hairpin expressions and we show how to deduce a
recognizer for a hairpin expression from its two-sided derived term automaton,
providing an alternative proof of the fact that hairpin completions of regular
languages are linear context-free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3323</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3323</id><created>2013-01-15</created><updated>2013-03-18</updated><authors><author><keyname>Sukhbaatar</keyname><forenames>Sainbayar</forenames></author><author><keyname>Makino</keyname><forenames>Takaki</forenames></author><author><keyname>Aihara</keyname><forenames>Kazuyuki</forenames></author></authors><title>Auto-pooling: Learning to Improve Invariance of Image Features from
  Image Sequences</title><categories>cs.CV cs.LG</categories><comments>9 pages, 10 figures. Submission for ICLR 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning invariant representations from images is one of the hardest
challenges facing computer vision. Spatial pooling is widely used to create
invariance to spatial shifting, but it is restricted to convolutional models.
In this paper, we propose a novel pooling method that can learn soft clustering
of features from image sequences. It is trained to improve the temporal
coherence of features, while keeping the information loss at minimum. Our
method does not use spatial information, so it can be used with
non-convolutional models too. Experiments on images extracted from natural
videos showed that our method can cluster similar features together. When
trained by convolutional features, auto-pooling outperformed traditional
spatial pooling on an image classification task, even though it does not use
the spatial topology of features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3334</identifier>
 <datestamp>2014-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3334</id><created>2013-01-15</created><updated>2014-05-26</updated><authors><author><keyname>B&#x159;inda</keyname><forenames>Karel</forenames></author><author><keyname>Pelantov&#xe1;</keyname><forenames>Edita</forenames></author><author><keyname>Turek</keyname><forenames>Ond&#x159;ej</forenames></author></authors><title>Balances of $m$-bonacci words</title><categories>math.CO cs.DM</categories><journal-ref>Fundamenta Informaticae 132(1), pp. 33-61, 2014</journal-ref><doi>10.3233/fi-2014-1031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $m$-bonacci word is a generalization of the Fibonacci word to the
$m$-letter alphabet $\mathcal{A} = {0,...,m-1}$. It is the unique fixed point
of the Pisot--type substitution $ \varphi_m: 0\to 01, 1\to 02, ...,
(m-2)\to0(m-1), and (m-1)\to0$. A result of Adamczewski implies the existence
of constants $c^{(m)}$ such that the $m$-bonacci word is $c^{(m)}$-balanced,
i.e., numbers of letter $a$ occurring in two factors of the same length differ
at most by $c^{(m)}$ for any letter $a\in \mathcal{A}$. The constants $c^{(m)}$
have been already determined for $m=2$ and $m=3$. In this paper we study the
bounds $c^{(m)}$ for a general $m\geq2$. We show that the $m$-bonacci word is
$(\lfloor \kappa m \rfloor +12)$-balanced, where $\kappa \approx 0.58$. For
$m\leq 12$, we improve the constant $c^{(m)}$ by a computer numerical
calculation to the value $\lceil\frac{m+1}{2}\rceil$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3342</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3342</id><created>2013-01-15</created><updated>2013-03-08</updated><authors><author><keyname>van der Maaten</keyname><forenames>Laurens</forenames></author></authors><title>Barnes-Hut-SNE</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents an O(N log N)-implementation of t-SNE -- an embedding
technique that is commonly used for the visualization of high-dimensional data
in scatter plots and that normally runs in O(N^2). The new implementation uses
vantage-point trees to compute sparse pairwise similarities between the input
data objects, and it uses a variant of the Barnes-Hut algorithm - an algorithm
used by astronomers to perform N-body simulations - to approximate the forces
between the corresponding points in the embedding. Our experiments show that
the new algorithm, called Barnes-Hut-SNE, leads to substantial computational
advantages over standard t-SNE, and that it makes it possible to learn
embeddings of data sets with millions of objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3347</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3347</id><created>2013-01-15</created><authors><author><keyname>Smyrnakis</keyname><forenames>Michalis</forenames></author></authors><title>Multi-agent learning using Fictitious Play and Extended Kalman Filter</title><categories>cs.MA cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decentralised optimisation tasks are important components of multi-agent
systems. These tasks can be interpreted as n-player potential games: therefore
game-theoretic learning algorithms can be used to solve decentralised
optimisation tasks. Fictitious play is the canonical example of these
algorithms. Nevertheless fictitious play implicitly assumes that players have
stationary strategies. We present a novel variant of fictitious play where
players predict their opponents' strategies using Extended Kalman filters and
use their predictions to update their strategies.
  We show that in 2 by 2 games with at least one pure Nash equilibrium and in
potential games where players have two available actions, the proposed
algorithm converges to the pure Nash equilibrium. The performance of the
proposed algorithm was empirically tested, in two strategic form games and an
ad-hoc sensor network surveillance problem. The proposed algorithm performs
better than the classic fictitious play algorithm in these games and therefore
improves the performance of game-theoretical learning in decentralised
optimisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3350</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3350</id><created>2013-01-15</created><updated>2013-11-25</updated><authors><author><keyname>Zhang</keyname><forenames>Yan</forenames></author><author><keyname>Zhu</keyname><forenames>Zhaohui</forenames></author><author><keyname>Zhang</keyname><forenames>Jinjin</forenames></author></authors><title>On Recursive Operations Over Logic LTS</title><categories>cs.LO cs.SE</categories><comments>66 pages</comments><journal-ref>Math. Struct. in Comp. Science 25 (2014) 1382-1431</journal-ref><doi>10.1017/S0960129514000073</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, in order to mix algebraic and logic styles of specification in a
uniform framework, the notion of a logic labelled transition system (Logic LTS
or LLTS for short) has been introduced and explored. A variety of constructors
over LLTS, including usual process-algebraic operators, logic connectives
(conjunction and disjunction) and standard temporal operators (always and
unless), have been given. However, no attempt has made so far to develop
general theory concerning (nested) recursive operations over LLTSs and a few
fundamental problems are still open. This paper intends to study this issue in
pure process-algebraic style. A few fundamental properties, including
precongruence and the uniqueness of consistent solutions for equations, will be
established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3369</identifier>
 <datestamp>2013-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3369</id><created>2013-01-15</created><updated>2013-05-04</updated><authors><author><keyname>Fujiwara</keyname><forenames>Yuichiro</forenames></author></authors><title>Self-synchronizing pulse position modulation with error tolerance</title><categories>cs.IT math.CO math.IT</categories><comments>11 pages, 1 figure, 3 tables. Final accepted version for publication
  in the IEEE Transactions on Information Theory. This version incorporates
  minor corrections and some improvements including additional explicit
  examples, performance comparisons, and a discussion on symbol error rates for
  use in an FSO link</comments><journal-ref>IEEE Transactions on Information Theory 59 (2013) 5352-5362</journal-ref><doi>10.1109/TIT.2013.2262094</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pulse position modulation (PPM) is a popular signal modulation technique
which creates M-ary data by means of the position of a pulse within a time
interval. While PPM and its variations have great advantages in many contexts,
this type of modulation is vulnerable to loss of synchronization, potentially
causing a severe error floor or throughput penalty even when little or no noise
is assumed. Another disadvantage is that this type of modulation typically
offers no error correction mechanism on its own, making them sensitive to
intersymbol interference and environmental noise. In this paper we propose a
coding theoretic variation of PPM that allows for significantly more efficient
symbol and frame synchronization as well as strong error correction. The
proposed scheme can be divided into a synchronization layer and a modulation
layer. This makes our technique compatible with major existing techniques such
as standard PPM, multipluse PPM, and expurgated PPM as well in that the scheme
can be realized by adding a simple synchronization layer to one of these
standard techniques. We also develop a generalization of expurgated PPM suited
for the modulation layer of the proposed self-synchronizing modulation scheme.
This generalized PPM can also be used as stand-alone error-correcting PPM with
a larger number of available symbols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3375</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3375</id><created>2013-01-15</created><updated>2015-04-02</updated><authors><author><keyname>Schnass</keyname><forenames>Karin</forenames></author></authors><title>On the Identifiability of Overcomplete Dictionaries via the Minimisation
  Principle Underlying K-SVD</title><categories>cs.IT math.IT</categories><comments>36 pages (double spaced), 3 figures, equivalent to final accepted
  version</comments><journal-ref>Applied and Computational Harmonic Analysis, Volume 37, Issue 3,
  November 2014, Pages 464-491</journal-ref><doi>10.1016/j.acha.2014.01.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article gives theoretical insights into the performance of K-SVD, a
dictionary learning algorithm that has gained significant popularity in
practical applications. The particular question studied here is when a
dictionary $\Phi\in \mathbb{R}^{d \times K}$ can be recovered as local minimum
of the minimisation criterion underlying K-SVD from a set of $N$ training
signals $y_n =\Phi x_n$. A theoretical analysis of the problem leads to two
types of identifiability results assuming the training signals are generated
from a tight frame with coefficients drawn from a random symmetric
distribution. First, asymptotic results showing, that in expectation the
generating dictionary can be recovered exactly as a local minimum of the K-SVD
criterion if the coefficient distribution exhibits sufficient decay. Second,
based on the asymptotic results it is demonstrated that given a finite number
of training samples $N$, such that $N/\log N = O(K^3d)$, except with
probability $O(N^{-Kd})$ there is a local minimum of the K-SVD criterion within
distance $O(KN^{-1/4})$ to the generating dictionary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3376</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3376</id><created>2013-01-15</created><updated>2013-02-06</updated><authors><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author><author><keyname>Zamboni</keyname><forenames>Luca Q.</forenames></author></authors><title>On the least number of palindromes contained in an infinite word</title><categories>cs.DM cs.FL math.CO</categories><comments>Accepted for publication in Theoretical Computer Science</comments><msc-class>68R15</msc-class><journal-ref>Theoretical Computer Science, 481: 1-8 (2013)</journal-ref><doi>10.1016/j.tcs.2013.02.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the least number of palindromic factors in an infinite word.
We first consider general alphabets, and give answers to this problem for
periodic and non-periodic words, closed or not under reversal of factors. We
then investigate the same problem when the alphabet has size two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3385</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3385</id><created>2013-01-15</created><updated>2013-01-16</updated><authors><author><keyname>Young</keyname><forenames>Steven R.</forenames></author><author><keyname>Arel</keyname><forenames>Itamar</forenames></author></authors><title>Recurrent Online Clustering as a Spatio-Temporal Feature Extractor in
  DeSTIN</title><categories>cs.CV</categories><comments>3 pages, 2 figures, Submitted to ICLR 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a basic enhancement to the DeSTIN deep learning
architecture by replacing the explicitly calculated transition tables that are
used to capture temporal features with a simpler, more scalable mechanism. This
mechanism uses feedback of state information to cluster over a space comprised
of both the spatial input and the current state. The resulting architecture
achieves state-of-the-art results on the MNIST classification benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3388</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3388</id><created>2013-01-14</created><authors><author><keyname>Liljenzin</keyname><forenames>Olle</forenames></author></authors><title>Confluently Persistent Sets and Maps</title><categories>cs.DS cs.DB</categories><comments>11 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ordered sets and maps play important roles as index structures in relational
data models. When a shared index in a multi-user system is modified
concurrently, the current state of the index will diverge into multiple
versions containing the local modifications performed in each work flow. The
confluent persistence problem arises when versions should be melded in commit
and refresh operations so that modifications performed by different users
become merged.
  Confluently Persistent Sets and Maps are functional binary search trees that
support efficient set operations both when operands are disjoint and when they
are overlapping. Treap properties with hash values as priorities are maintained
and with hash-consing of nodes a unique representation is provided.
Non-destructive set merge algorithms that skip inspection of equal subtrees and
a conflict detecting meld algorithm based on set merges are presented. The meld
algorithm is used in commit and refresh operations. With m modifications in one
flow and n items in total, the expected cost of the operations is O(m
log(n/m)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3389</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3389</id><created>2013-01-15</created><updated>2013-03-18</updated><authors><author><keyname>Van hamme</keyname><forenames>Hugo</forenames></author></authors><title>The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization</title><categories>cs.NA cs.LG</categories><comments>8 pages + references; International Conference on Learning
  Representations, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-negative matrix factorization (NMF) has become a popular machine learning
approach to many problems in text mining, speech and image processing,
bio-informatics and seismic data analysis to name a few. In NMF, a matrix of
non-negative data is approximated by the low-rank product of two matrices with
non-negative entries. In this paper, the approximation quality is measured by
the Kullback-Leibler divergence between the data and its low-rank
reconstruction. The existence of the simple multiplicative update (MU)
algorithm for computing the matrix factors has contributed to the success of
NMF. Despite the availability of algorithms showing faster convergence, MU
remains popular due to its simplicity. In this paper, a diagonalized Newton
algorithm (DNA) is proposed showing faster convergence while the implementation
remains simple and suitable for high-rank problems. The DNA algorithm is
applied to various publicly available data sets, showing a substantial speed-up
on modern hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3391</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3391</id><created>2013-01-15</created><updated>2013-03-11</updated><authors><author><keyname>Bauer</keyname><forenames>Felix</forenames></author><author><keyname>Memisevic</keyname><forenames>Roland</forenames></author></authors><title>Feature grouping from spatially constrained multiplicative interaction</title><categories>cs.LG</categories><comments>(new version:) added training formulae; added minor clarifications</comments><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a feature learning model that learns to encode relationships
between images. The model is defined as a Gated Boltzmann Machine, which is
constrained such that hidden units that are nearby in space can gate each
other's connections. We show how frequency/orientation &quot;columns&quot; as well as
topographic filter maps follow naturally from training the model on image
pairs. The model also helps explain why square-pooling models yield feature
groups with similar grouping properties. Experimental results on synthetic
image transformations show that spatially constrained gating is an effective
way to reduce the number of parameters and thereby to regularize a
transformation-learning model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3392</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3392</id><created>2013-01-15</created><authors><author><keyname>Bienvenu</keyname><forenames>Laurent</forenames></author><author><keyname>Romashchenko</keyname><forenames>Andrei</forenames></author><author><keyname>Shen</keyname><forenames>Alexander</forenames></author><author><keyname>Taveneaux</keyname><forenames>Antoine</forenames></author><author><keyname>Vermeeren</keyname><forenames>Stijn</forenames></author></authors><title>The axiomatic power of Kolmogorov complexity</title><categories>math.LO cs.CC</categories><msc-class>03B10, 03D32, 03H15</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The famous G\&quot;odel incompleteness theorem states that for every consistent
sufficiently rich formal theory T there exist true statements that are
unprovable in T. Such statements would be natural candidates for being added as
axioms, but how can we obtain them? One classical (and well studied) approach
is to add to some theory T an axiom that claims the consistency of T. In this
paper we discuss another approach motivated by Chaitin's version of G\&quot;odel's
theorem where axioms claiming the randomness (or incompressibility) of some
strings are probabilistically added, and show that it is not really useful, in
the sense that this does not help us to prove new interesting theorems. This
result answers a question recently asked by Lipton. The situation changes if we
take into account the size of the proofs: randomly chosen axioms may help
making proofs much shorter, unless NP=PSPACE. We then study the axiomatic power
of the statements of type &quot;the Kolmogorov complexity of x exceeds n&quot; in
general. They are \Pi_1 (universally quantified) statements of Peano
arithmetic. We show that by adding all true statements of this type, we obtain
a theory that proves all true \Pi_1-statements, and also provide a more
detailed classification. In particular, to derive all true \Pi_1-statements it
is sufficient to add one statement of this type for each n (or even for
infinitely many n) if strings are chosen in a special way. On the other hand,
one may add statements of this type for most x of length n (for every n) and
still obtain a weak theory. We also study other logical questions related to
&quot;random axioms&quot;. Finally, we consider a theory that claims Martin-L\&quot;of
randomness of a given infinite binary sequence. This claim can be formalized in
different ways. We show that different formalizations are closely related but
not equivalent, and study their properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3393</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3393</id><created>2013-01-15</created><authors><author><keyname>Stay</keyname><forenames>Mike</forenames></author><author><keyname>Vicary</keyname><forenames>Jamie</forenames></author></authors><title>Bicategorical Semantics for Nondeterministic Computation</title><categories>cs.LO cs.CR</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We outline a bicategorical syntax for the interaction between public and
private information in classical information theory. We use this to give
high-level graphical definitions of encrypted communication and secret sharing
protocols, including a characterization of their security properties.
Remarkably, this makes it clear that the protocols have an identical abstract
form to the quantum teleportation and dense coding procedures, yielding
evidence of a deep connection between classical and quantum information
processing. We also formulate public-key cryptography using our scheme.
Specific implementations of these protocols as nondeterministic classical
procedures are recovered by applying our formalism in a symmetric monoidal
bicategory of matrices of relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3402</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3402</id><created>2013-01-13</created><updated>2013-03-13</updated><authors><author><keyname>Crampton</keyname><forenames>Jason</forenames></author><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author></authors><title>Constraint Expressions and Workflow Satisfiability</title><categories>cs.DS cs.CR</categories><comments>arXiv admin note: text overlap with arXiv:1205.0852; to appear in
  Proceedings of SACMAT 2013</comments><acm-class>D.4.6; F.2.2; H.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A workflow specification defines a set of steps and the order in which those
steps must be executed. Security requirements and business rules may impose
constraints on which users are permitted to perform those steps. A workflow
specification is said to be satisfiable if there exists an assignment of
authorized users to workflow steps that satisfies all the constraints. An
algorithm for determining whether such an assignment exists is important, both
as a static analysis tool for workflow specifications, and for the construction
of run-time reference monitors for workflow management systems. We develop new
methods for determining workflow satisfiability based on the concept of
constraint expressions, which were introduced recently by Khan and Fong. These
methods are surprising versatile, enabling us to develop algorithms for, and
determine the complexity of, a number of different problems related to workflow
satisfiability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3443</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3443</id><created>2013-01-15</created><authors><author><keyname>Licata</keyname><forenames>Daniel R.</forenames></author><author><keyname>Shulman</keyname><forenames>Michael</forenames></author></authors><title>Calculating the Fundamental Group of the Circle in Homotopy Type Theory</title><categories>math.LO cs.PL math.AT</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work on homotopy type theory exploits an exciting new correspondence
between Martin-Lof's dependent type theory and the mathematical disciplines of
category theory and homotopy theory. The category theory and homotopy theory
suggest new principles to add to type theory, and type theory can be used in
novel ways to formalize these areas of mathematics. In this paper, we formalize
a basic result in algebraic topology, that the fundamental group of the circle
is the integers. Though simple, this example is interesting for several
reasons: it illustrates the new principles in homotopy type theory; it mixes
ideas from traditional homotopy-theoretic proofs of the result with
type-theoretic inductive reasoning; and it provides a context for understanding
an existing puzzle in type theory---that a universe (type of types) is
necessary to prove that the constructors of inductive types are disjoint and
injective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3451</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3451</id><created>2013-01-15</created><updated>2015-03-22</updated><authors><author><keyname>Dong</keyname><forenames>Fanghu</forenames></author></authors><title>Eigenstructure of Maximum Likelihood from Counts Data</title><categories>stat.ME cs.DS math.OC math.ST stat.TH</categories><comments>Draft</comments><msc-class>62H05, 62G05, 90C56, 62F07, 26D05, 62H17, 13P25, 90C06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The MLE (Maximum Likelihood Estimate) for a multinomial model is proportional
to the data. We call such estimate an eigenestimate and the relationship of it
to the data as the eigenstructure. When the multinomial model is generalized to
deal with data arise from incomplete or censored categorical counts, we would
naturally look for this eigenstructure between MLE and data. The paper finds
the algebraic representation of the eigenstructure (put as Eqn (2.1)), with
which the intuition is visualized geometrically (Figures 2.2 and 4.3) and
elaborated in a theory (Section 4). The eigenestimate constructed from the
eigenstructure must be a stationary point of the likelihood, a result proved in
Theorem 4.42. On the bridge between the algebraic definition of Eqn (2.1) and
the Proof of Theorem 4.42, we have exploited an elementary inequality (Lemma
3.1) that governs the primitive cases, defined the thick objects of fragment
and slice which can be assembled like mechanical parts (Definition 4.1), proved
a few intermediary results that help build up the intuition (Section 4),
conjectured the universal existence of an eigenestimate (Conjecture 4.32),
established a criterion for boundary regularity (Criterion 4.37), and paved way
(the Trivial Slicing Algorithm (TSA)) for the derivation of the Weaver
algorithms (Section 5) that finds the eigenestimate by using it to reconstruct
the observed counts through the eigenstructure; the reconstruction is iterative
but derivative-free and matrix-inversion-free. As new addition to the current
body of algorithmic methods, the Weaver algorithms craftily tighten threads
that are weaved on a rectangular grid (Figure 2.3), and is one incarnation of
the TSA. Finally, we put our method in the context of some existing methods
(Section 6). Softwares are pseudocoded and put online. Visit
http://hku.hk/jdong/eigenstruct2013a.html for demonstrations and download.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3452</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3452</id><created>2013-01-15</created><authors><author><keyname>Montina</keyname><forenames>Alberto</forenames></author></authors><title>Exponential communication gap between weak and strong classical
  simulations of quantum communication</title><categories>quant-ph cs.IT math.IT</categories><journal-ref>Phys. Rev. A 87, 042331 (2013)</journal-ref><doi>10.1103/PhysRevA.87.042331</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most trivial way to simulate classically the communication of a quantum
state is to transmit the classical description of the quantum state itself.
However, this requires an infinite amount of classical communication if the
simulation is exact. A more intriguing and potentially less demanding strategy
would encode the full information about the quantum state into the probability
distribution of the communicated variables, so that this information is never
sent in each single shot. This kind of simulation is called weak, as opposed to
strong simulations, where the quantum state is communicated in individual
shots. In this paper, we introduce a bounded-error weak protocol for simulating
the communication of an arbitrary number of qubits and a subsequent two-outcome
measurement consisting of an arbitrary pure state projector and its complement.
This protocol requires an amount of classical communication independent of the
number of qubits and proportional to Delta^{-1}, where Delta is the error and a
free parameter of the protocol. Conversely, a bounded-error strong protocol
requires an amount of classical communication growing exponentially with the
number of qubits for a fixed error. Our result improves a previous protocol,
based on the Johnson-Lindenstrauss lemma, with communication cost scaling as
Delta^{-2} log Delta^{-1}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3455</identifier>
 <datestamp>2013-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3455</id><created>2013-01-15</created><authors><author><keyname>Mei</keyname><forenames>Gang</forenames></author><author><keyname>Tipper</keyname><forenames>John C.</forenames></author><author><keyname>Xu</keyname><forenames>Nengxiong</forenames></author></authors><title>3D Geological Modeling and Visualization of Rock Masses Based on Google
  Earth: A Case Study</title><categories>cs.GR physics.geo-ph</categories><comments>to appear in the Proceeding of IEEE Conference CSIT2013, in press</comments><journal-ref>Computer Science and Information Technology (CSIT), 2013 5th
  International Conference on, 2013, pp. 210-213</journal-ref><doi>10.1109/CSIT.2013.6588781</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Google Earth (GE) has become a powerful tool for geological modeling and
visualization. An interesting and useful feature of GE, Google Street View, can
allow the GE users to view geological structure such as layers of rock masses
at a field site. In this paper, we introduce a practical solution for building
3D geological models for rock masses based on the data acquired by use with GE.
A real study case at Haut-Barr, France is presented to demonstrate our
solution. We first locate the position of Haut-Barr in GE, and then determine
the shape and scale of the rock masses in the study area, and thirdly acquire
the layout of layers of rock masses in the Google Street View, and finally
create the approximate 3D geological models by extruding and intersecting. The
generated 3D geological models can simply reflect the basic structure of the
rock masses at Haut-Barr, and can be used for visualizing the rock bodies
interactively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3457</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3457</id><created>2013-01-15</created><updated>2013-04-10</updated><authors><author><keyname>Cicconet</keyname><forenames>Marcelo</forenames></author><author><keyname>Lima</keyname><forenames>Italo</forenames></author><author><keyname>Geiger</keyname><forenames>Davi</forenames></author><author><keyname>Gunsalus</keyname><forenames>Kris</forenames></author></authors><title>A Geometric Descriptor for Cell-Division Detection</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author since the review process
  for the conference to which it was applied ended</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a method for cell-division detection based on a geometric-driven
descriptor that can be represented as a 5-layers processing network, based
mainly on wavelet filtering and a test for mirror symmetry between pairs of
pixels. After the centroids of the descriptors are computed for a sequence of
frames, the two-steps piecewise constant function that best fits the sequence
of centroids determines the frame where the division occurs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3461</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3461</id><created>2013-01-15</created><updated>2013-04-23</updated><authors><author><keyname>Zhang</keyname><forenames>Cheng</forenames></author><author><keyname>Ek</keyname><forenames>Carl Henrik</forenames></author><author><keyname>Damianou</keyname><forenames>Andreas</forenames></author><author><keyname>Kjellstrom</keyname><forenames>Hedvig</forenames></author></authors><title>Factorized Topic Models</title><categories>cs.LG cs.CV cs.IR</categories><comments>ICLR 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a modification to a latent topic model, which makes
the model exploit supervision to produce a factorized representation of the
observed data. The structured parameterization separately encodes variance that
is shared between classes from variance that is private to each class by the
introduction of a new prior over the topic space. The approach allows for a
more eff{}icient inference and provides an intuitive interpretation of the data
in terms of an informative signal together with structured noise. The
factorized representation is shown to enhance inference performance for image,
text, and video classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3468</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3468</id><created>2013-01-15</created><updated>2013-03-04</updated><authors><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author></authors><title>Boltzmann Machines and Denoising Autoencoders for Image Denoising</title><categories>stat.ML cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image denoising based on a probabilistic model of local image patches has
been employed by various researchers, and recently a deep (denoising)
autoencoder has been proposed by Burger et al. [2012] and Xie et al. [2012] as
a good model for this. In this paper, we propose that another popular family of
models in the field of deep learning, called Boltzmann machines, can perform
image denoising as well as, or in certain cases of high level of noise, better
than denoising autoencoders. We empirically evaluate the two models on three
different sets of images with different types and levels of noise. Throughout
the experiments we also examine the effect of the depth of the models. The
experiments confirmed our claim and revealed that the performance can be
improved by adding more hidden layers, especially when the level of noise is
high.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3471</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3471</id><created>2013-01-15</created><updated>2013-10-20</updated><authors><author><keyname>Rajabi-Alni</keyname><forenames>Fatemeh</forenames></author><author><keyname>Bagheri</keyname><forenames>Alireza</forenames></author></authors><title>Embedding a balanced binary tree on a bounded point set</title><categories>cs.CG cs.DS</categories><comments>21 pages, 21 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected planar graph G with n vertices and a set S of n points
inside a simple polygon P, a point-set embedding of G on S is a planar drawing
of G such that each vertex is mapped to a distinct point of S and the edges are
polygonal chains surrounded by P. A special case of the embedding problem is
that in which G is a balanced binary tree. In this paper, we present a new
algorithm for embedding an n-vertex balanced binary tree BBT on a set S of n
points bounded by a simple m-gon P in O(m^2 + n(log n)^2 + mn) time with at
most O(m) bends per edge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3476</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3476</id><created>2013-01-15</created><updated>2013-03-11</updated><authors><author><keyname>Vatanen</keyname><forenames>Tommi</forenames></author><author><keyname>Raiko</keyname><forenames>Tapani</forenames></author><author><keyname>Valpola</keyname><forenames>Harri</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Pushing Stochastic Gradient towards Second-Order Methods --
  Backpropagation Learning with Transformations in Nonlinearities</title><categories>cs.LG cs.CV stat.ML</categories><comments>10 pages, 5 figures, ICLR2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, we proposed to transform the outputs of each hidden neuron in a
multi-layer perceptron network to have zero output and zero slope on average,
and use separate shortcut connections to model the linear dependencies instead.
We continue the work by firstly introducing a third transformation to normalize
the scale of the outputs of each hidden neuron, and secondly by analyzing the
connections to second order optimization methods. We show that the
transformations make a simple stochastic gradient behave closer to second-order
optimization methods and thus speed up learning. This is shown both in theory
and with experiments. The experiments on the third transformation show that
while it further increases the speed of learning, it can also hurt performance
by converging to a worse local optimum, where both the inputs and outputs of
many hidden neurons are close to zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3482</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3482</id><created>2013-01-15</created><authors><author><keyname>Rajabi-Alni</keyname><forenames>Fatemeh</forenames></author><author><keyname>Bagheri</keyname><forenames>Alireza</forenames></author></authors><title>Many to Many Matching with Demands and Capacities</title><categories>cs.DS cs.CG</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let A and B be two finite sets of points with total cardinality n, the many
to many point matching with demands and capacities matches each point ai in A
to at least alpha'i and at most alphai points in B, and each point bj in B to
at least betaj and at most beta'j points in A for all 1 &lt;= i &lt;= s and 1 &lt;= j &lt;=
t. In this paper, we present an upper bound for this problem using our new
polynomial time algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3485</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3485</id><created>2013-01-15</created><updated>2013-03-21</updated><authors><author><keyname>Glorot</keyname><forenames>Xavier</forenames></author><author><keyname>Bordes</keyname><forenames>Antoine</forenames></author><author><keyname>Weston</keyname><forenames>Jason</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>A Semantic Matching Energy Function for Learning with Multi-relational
  Data</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale relational learning becomes crucial for handling the huge amounts
of structured data generated daily in many application domains ranging from
computational biology or information retrieval, to natural language processing.
In this paper, we present a new neural network architecture designed to embed
multi-relational graphs into a flexible continuous vector space in which the
original data is kept and enhanced. The network is trained to encode the
semantics of these graphs in order to assign high probabilities to plausible
components. We empirically show that it reaches competitive performance in link
prediction on standard datasets from the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3488</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3488</id><created>2013-01-15</created><authors><author><keyname>Belazzougui</keyname><forenames>Djamal</forenames></author><author><keyname>Kolpakov</keyname><forenames>Roman</forenames></author><author><keyname>Raffinot</keyname><forenames>Mathieu</forenames></author></authors><title>Various improvements to text fingerprinting</title><categories>cs.DS cs.DM cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let s = s_1 .. s_n be a text (or sequence) on a finite alphabet \Sigma of
size \sigma. A fingerprint in s is the set of distinct characters appearing in
one of its substrings. The problem considered here is to compute the set {\cal
F} of all fingerprints of all substrings of s in order to answer efficiently
certain questions on this set. A substring s_i .. s_j is a maximal location for
a fingerprint f in F (denoted by &lt;i,j&gt;) if the alphabet of s_i .. s_j is f and
s_{i-1}, s_{j+1}, if defined, are not in f. The set of maximal locations ins is
{\cal L} (it is easy to see that |{\cal L}| \leq n \sigma). Two maximal
locations &lt;i,j&gt; and &lt;k,l&gt; such that s_i .. s_j = s_k .. s_l are named {\em
copies}, and the quotient set of {\cal L} according to the copy relation is
denoted by {\cal L}_C. We present new exact and approximate efficient
algorithms and data structures for the following three problems: (1) to compute
{\cal F}; (2) given f as a set of distinct characters in \Sigma, to answer if f
represents a fingerprint in {\cal F}; (3) given f, to find all maximal
locations of f in s.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3509</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3509</id><created>2013-01-15</created><updated>2013-04-02</updated><authors><author><keyname>Ashlagi</keyname><forenames>Itai</forenames></author><author><keyname>Jaillet</keyname><forenames>Patrick</forenames></author><author><keyname>Manshadi</keyname><forenames>Vahideh H.</forenames></author></authors><title>Kidney Exchange in Dynamic Sparse Heterogenous Pools</title><categories>cs.DS cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current kidney exchange pools are of moderate size and thin, as they consist
of many highly sensitized patients. Creating a thicker pool can be done by
waiting for many pairs to arrive. We analyze a simple class of matching
algorithms that search periodically for allocations. We find that if only 2-way
cycles are conducted, in order to gain a significant amount of matches over the
online scenario (matching each time a new incompatible pair joins the pool) the
waiting period should be &quot;very long&quot;. If 3-way cycles are also allowed we find
regimes in which waiting for a short period also increases the number of
matches considerably. Finally, a significant increase of matches can be
obtained by using even one non-simultaneous chain while still matching in an
online fashion. Our theoretical findings and data-driven computational
experiments lead to policy recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3516</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3516</id><created>2013-01-15</created><updated>2015-05-05</updated><authors><author><keyname>Malinowski</keyname><forenames>Mateusz</forenames></author><author><keyname>Fritz</keyname><forenames>Mario</forenames></author></authors><title>Learnable Pooling Regions for Image Classification</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biologically inspired, from the early HMAX model to Spatial Pyramid Matching,
pooling has played an important role in visual recognition pipelines. Spatial
pooling, by grouping of local codes, equips these methods with a certain degree
of robustness to translation and deformation yet preserving important spatial
information. Despite the predominance of this approach in current recognition
systems, we have seen little progress to fully adapt the pooling strategy to
the task at hand. This paper proposes a model for learning task dependent
pooling scheme -- including previously proposed hand-crafted pooling schemes as
a particular instantiation. In our work, we investigate the role of different
regularization terms showing that the smooth regularization term is crucial to
achieve strong performance using the presented architecture. Finally, we
propose an efficient and parallel method to train the model. Our experiments
show improved performance over hand-crafted pooling schemes on the CIFAR-10 and
CIFAR-100 datasets -- in particular improving the state-of-the-art to 56.29% on
the latter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3524</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3524</id><created>2013-01-15</created><authors><author><keyname>Zliobaite</keyname><forenames>Indre</forenames></author></authors><title>How good is the Electricity benchmark for evaluating concept drift
  adaptation</title><categories>cs.LG</categories><comments>2 pages of content, 1 appendix, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this correspondence, we will point out a problem with testing adaptive
classifiers on autocorrelated data. In such a case random change alarms may
boost the accuracy figures. Hence, we cannot be sure if the adaptation is
working well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3527</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3527</id><created>2013-01-15</created><updated>2013-03-18</updated><authors><author><keyname>Potluru</keyname><forenames>Vamsi K.</forenames></author><author><keyname>Plis</keyname><forenames>Sergey M.</forenames></author><author><keyname>Roux</keyname><forenames>Jonathan Le</forenames></author><author><keyname>Pearlmutter</keyname><forenames>Barak A.</forenames></author><author><keyname>Calhoun</keyname><forenames>Vince D.</forenames></author><author><keyname>Hayes</keyname><forenames>Thomas P.</forenames></author></authors><title>Block Coordinate Descent for Sparse NMF</title><categories>cs.LG cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonnegative matrix factorization (NMF) has become a ubiquitous tool for data
analysis. An important variant is the sparse NMF problem which arises when we
explicitly require the learnt features to be sparse. A natural measure of
sparsity is the L$_0$ norm, however its optimization is NP-hard. Mixed norms,
such as L$_1$/L$_2$ measure, have been shown to model sparsity robustly, based
on intuitive attributes that such measures need to satisfy. This is in contrast
to computationally cheaper alternatives such as the plain L$_1$ norm. However,
present algorithms designed for optimizing the mixed norm L$_1$/L$_2$ are slow
and other formulations for sparse NMF have been proposed such as those based on
L$_1$ and L$_0$ norms. Our proposed algorithm allows us to solve the mixed norm
sparsity constraints while not sacrificing computation time. We present
experimental evidence on real-world datasets that shows our new algorithm
performs an order of magnitude faster compared to the current state-of-the-art
solvers optimizing the mixed norm and is suitable for large-scale datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3528</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3528</id><created>2013-01-15</created><authors><author><keyname>Xiong</keyname><forenames>Momiao</forenames></author><author><keyname>Ma</keyname><forenames>Long</forenames></author></authors><title>An Efficient Sufficient Dimension Reduction Method for Identifying
  Genetic Variants of Clinical Significance</title><categories>q-bio.GN cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast and cheaper next generation sequencing technologies will generate
unprecedentedly massive and highly-dimensional genomic and epigenomic variation
data. In the near future, a routine part of medical record will include the
sequenced genomes. A fundamental question is how to efficiently extract genomic
and epigenomic variants of clinical utility which will provide information for
optimal wellness and interference strategies. Traditional paradigm for
identifying variants of clinical validity is to test association of the
variants. However, significantly associated genetic variants may or may not be
usefulness for diagnosis and prognosis of diseases. Alternative to association
studies for finding genetic variants of predictive utility is to systematically
search variants that contain sufficient information for phenotype prediction.
To achieve this, we introduce concepts of sufficient dimension reduction and
coordinate hypothesis which project the original high dimensional data to very
low dimensional space while preserving all information on response phenotypes.
We then formulate clinically significant genetic variant discovery problem into
sparse SDR problem and develop algorithms that can select significant genetic
variants from up to or even ten millions of predictors with the aid of dividing
SDR for whole genome into a number of subSDR problems defined for genomic
regions. The sparse SDR is in turn formulated as sparse optimal scoring
problem, but with penalty which can remove row vectors from the basis matrix.
To speed up computation, we develop the modified alternating direction method
for multipliers to solve the sparse optimal scoring problem which can easily be
implemented in parallel. To illustrate its application, the proposed method is
applied to simulation data and the NHLBI's Exome Sequencing Project dataset
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3530</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3530</id><created>2013-01-15</created><updated>2013-01-25</updated><authors><author><keyname>Cadieu</keyname><forenames>Charles F.</forenames></author><author><keyname>Hong</keyname><forenames>Ha</forenames></author><author><keyname>Yamins</keyname><forenames>Dan</forenames></author><author><keyname>Pinto</keyname><forenames>Nicolas</forenames></author><author><keyname>Majaj</keyname><forenames>Najib J.</forenames></author><author><keyname>DiCarlo</keyname><forenames>James J.</forenames></author></authors><title>The Neural Representation Benchmark and its Evaluation on Brain and
  Machine</title><categories>cs.NE cs.CV cs.LG q-bio.NC</categories><comments>The v1 version contained incorrectly computed kernel analysis curves
  and KA-AUC values for V4, IT, and the HT-L3 models. They have been corrected
  in this version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key requirement for the development of effective learning representations
is their evaluation and comparison to representations we know to be effective.
In natural sensory domains, the community has viewed the brain as a source of
inspiration and as an implicit benchmark for success. However, it has not been
possible to directly test representational learning algorithms directly against
the representations contained in neural systems. Here, we propose a new
benchmark for visual representations on which we have directly tested the
neural representation in multiple visual cortical areas in macaque (utilizing
data from [Majaj et al., 2012]), and on which any computer vision algorithm
that produces a feature space can be tested. The benchmark measures the
effectiveness of the neural or machine representation by computing the
classification loss on the ordered eigendecomposition of a kernel matrix
[Montavon et al., 2011]. In our analysis we find that the neural representation
in visual area IT is superior to visual area V4. In our analysis of
representational learning algorithms, we find that three-layer models approach
the representational performance of V4 and the algorithm in [Le et al., 2012]
surpasses the performance of V4. Impressively, we find that a recent supervised
algorithm [Krizhevsky et al., 2012] achieves performance comparable to that of
IT for an intermediate level of image variation difficulty, and surpasses IT at
a higher difficulty level. We believe this result represents a major milestone:
it is the first learning algorithm we have found that exceeds our current
estimate of IT representation performance. We hope that this benchmark will
assist the community in matching the representational performance of visual
cortex and will serve as an initial rallying point for further correspondence
between representations derived in brains and machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3533</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3533</id><created>2013-01-15</created><updated>2013-02-22</updated><authors><author><keyname>Halkias</keyname><forenames>Xanadu</forenames></author><author><keyname>Paris</keyname><forenames>Sebastien</forenames></author><author><keyname>Glotin</keyname><forenames>Herve</forenames></author></authors><title>Sparse Penalty in Deep Belief Networks: Using the Mixed Norm Constraint</title><categories>cs.NE cs.LG stat.ML</categories><comments>8 pages, 7 figures (including subfigures), ICleaR conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Belief Networks (DBN) have been successfully applied on popular machine
learning tasks. Specifically, when applied on hand-written digit recognition,
DBNs have achieved approximate accuracy rates of 98.8%. In an effort to
optimize the data representation achieved by the DBN and maximize their
descriptive power, recent advances have focused on inducing sparse constraints
at each layer of the DBN. In this paper we present a theoretical approach for
sparse constraints in the DBN using the mixed norm for both non-overlapping and
overlapping groups. We explore how these constraints affect the classification
accuracy for digit recognition in three different datasets (MNIST, USPS, RIMES)
and provide initial estimations of their usefulness by altering different
parameters such as the group size and overlap percentage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3535</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3535</id><created>2013-01-15</created><authors><author><keyname>Kim</keyname><forenames>Sang Hyun</forenames></author><author><keyname>Feron</keyname><forenames>Eric</forenames></author><author><keyname>Clarke</keyname><forenames>John-Paul</forenames></author><author><keyname>Marzuoli</keyname><forenames>Aude</forenames></author><author><keyname>Delahaye</keyname><forenames>Daniel</forenames></author></authors><title>Airport Gate Scheduling for Passengers, Aircraft, and Operation</title><categories>cs.SY cs.AI</categories><comments>This paper is submitted to the tenth USA/Europe ATM 2013 seminar</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Passengers' experience is becoming a key metric to evaluate the air
transportation system's performance. Efficient and robust tools to handle
airport operations are needed along with a better understanding of passengers'
interests and concerns. Among various airport operations, this paper studies
airport gate scheduling for improved passengers' experience. Three objectives
accounting for passengers, aircraft, and operation are presented. Trade-offs
between these objectives are analyzed, and a balancing objective function is
proposed. The results show that the balanced objective can improve the
efficiency of traffic flow in passenger terminals and on ramps, as well as the
robustness of gate operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3537</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3537</id><created>2013-01-15</created><authors><author><keyname>Bruna</keyname><forenames>Joan</forenames></author><author><keyname>Szlam</keyname><forenames>Arthur</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Learning Stable Group Invariant Representations with Convolutional
  Networks</title><categories>cs.AI math.NA</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transformation groups, such as translations or rotations, effectively express
part of the variability observed in many recognition problems. The group
structure enables the construction of invariant signal representations with
appealing mathematical properties, where convolutions, together with pooling
operators, bring stability to additive and geometric perturbations of the
input. Whereas physical transformation groups are ubiquitous in image and audio
applications, they do not account for all the variability of complex signal
classes.
  We show that the invariance properties built by deep convolutional networks
can be cast as a form of stable group invariance. The network wiring
architecture determines the invariance group, while the trainable filter
coefficients characterize the group action. We give explanatory examples which
illustrate how the network architecture controls the resulting invariance
group. We also explore the principle by which additional convolutional layers
induce a group factorization enabling more abstract, powerful invariant
representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3539</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3539</id><created>2013-01-15</created><authors><author><keyname>Kang</keyname><forenames>Yoonseop</forenames></author><author><keyname>Choi</keyname><forenames>Seungjin</forenames></author></authors><title>Learning Features with Structure-Adapting Multi-view Exponential Family
  Harmoniums</title><categories>cs.LG</categories><comments>3 pages, 2 figures, ICLR2013 workshop track submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We proposea graphical model for multi-view feature extraction that
automatically adapts its structure to achieve better representation of data
distribution. The proposed model, structure-adapting multi-view harmonium
(SA-MVH) has switch parameters that control the connection between hidden nodes
and input views, and learn the switch parameter while training. Numerical
experiments on synthetic and a real-world dataset demonstrate the useful
behavior of the SA-MVH, compared to existing multi-view feature extraction
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3541</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3541</id><created>2013-01-15</created><updated>2013-03-15</updated><authors><author><keyname>Chalasani</keyname><forenames>Rakesh</forenames></author><author><keyname>Principe</keyname><forenames>Jose C.</forenames></author></authors><title>Deep Predictive Coding Networks</title><categories>cs.LG cs.CV stat.ML</categories><comments>13 Pages, 7 figures, submission for ICLR 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quality of data representation in deep learning methods is directly
related to the prior model imposed on the representations; however, generally
used fixed priors are not capable of adjusting to the context in the data. To
address this issue, we propose deep predictive coding networks, a hierarchical
generative model that empirically alters priors on the latent representations
in a dynamic and context-sensitive manner. This model captures the temporal
dependencies in time-varying signals and uses top-down information to modulate
the representation in lower layers. The centerpiece of our model is a novel
procedure to infer sparse states of a dynamic model which is used for feature
extraction. We also extend this feature extraction block to introduce a pooling
function that captures locally invariant representations. When applied on a
natural video data, we show that our method is able to learn high-level visual
features. We also demonstrate the role of the top-down connections by showing
the robustness of the proposed model to structured noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3545</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3545</id><created>2013-01-15</created><updated>2013-03-16</updated><authors><author><keyname>Desjardins</keyname><forenames>Guillaume</forenames></author><author><keyname>Pascanu</keyname><forenames>Razvan</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines</title><categories>cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for
training Boltzmann Machines. Similar in spirit to the Hessian-Free method of
Martens [8], our algorithm belongs to the family of truncated Newton methods
and exploits an efficient matrix-vector product to avoid explicitely storing
the natural gradient metric $L$. This metric is shown to be the expected second
derivative of the log-partition function (under the model distribution), or
equivalently, the variance of the vector of partial derivatives of the energy
function. We evaluate our method on the task of joint-training a 3-layer Deep
Boltzmann Machine and show that MFNG does indeed have faster per-epoch
convergence compared to Stochastic Maximum Likelihood with centering, though
wall-clock performance is currently not competitive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3547</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3547</id><created>2013-01-15</created><authors><author><keyname>Englard</keyname><forenames>Benjamin</forenames></author></authors><title>A Rhetorical Analysis Approach to Natural Language Processing</title><categories>cs.CL stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this research was to find a way to extend the capabilities of
computers through the processing of language in a more human way, and present
applications which demonstrate the power of this method. This research presents
a novel approach, Rhetorical Analysis, to solving problems in Natural Language
Processing (NLP). The main benefit of Rhetorical Analysis, as opposed to
previous approaches, is that it does not require the accumulation of large sets
of training data, but can be used to solve a multitude of problems within the
field of NLP. The NLP problems investigated with Rhetorical Analysis were the
Author Identification problem - predicting the author of a piece of text based
on its rhetorical strategies, Election Prediction - predicting the winner of a
presidential candidate's re-election campaign based on rhetorical strategies
within that president's inaugural address, Natural Language Generation - having
a computer produce text containing rhetorical strategies, and Document
Summarization. The results of this research indicate that an Author
Identification system based on Rhetorical Analysis could predict the correct
author 100% of the time, that a re-election predictor based on Rhetorical
Analysis could predict the correct winner of a re-election campaign 55% of the
time, that a Natural Language Generation system based on Rhetorical Analysis
could output text with up to 87.3% similarity to Shakespeare in style, and that
a Document Summarization system based on Rhetorical Analysis could extract
highly relevant sentences. Overall, this study demonstrated that Rhetorical
Analysis could be a useful approach to solving problems in NLP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3551</identifier>
 <datestamp>2013-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3551</id><created>2013-01-15</created><updated>2013-06-04</updated><authors><author><keyname>Giraldo</keyname><forenames>Luis G. Sanchez</forenames></author><author><keyname>Principe</keyname><forenames>Jose C.</forenames></author></authors><title>Information Theoretic Learning with Infinitely Divisible Kernels</title><categories>cs.LG cs.CV</categories><comments>Modified submission for International Conference on Learning
  Representations 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a framework for information theoretic learning
based on infinitely divisible matrices. We formulate an entropy-like functional
on positive definite matrices based on Renyi's axiomatic definition of entropy
and examine some key properties of this functional that lead to the concept of
infinite divisibility. The proposed formulation avoids the plug in estimation
of density and brings along the representation power of reproducing kernel
Hilbert spaces. As an application example, we derive a supervised metric
learning algorithm using a matrix based analogue to conditional entropy
achieving results comparable with the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3552</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3552</id><created>2013-01-15</created><authors><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Negative Imaginary Systems Theory in the Robust Control of Highly
  Resonant Flexible Structures</title><categories>cs.SY math.OC</categories><comments>A version of this paper appeared in the proceedings of the 2011
  Australian Control Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper covers recent developments in the theory of negative imaginary
systems and their application to the control of highly resonant flexible
structures. The theory of negative imaginary systems arose out of a desire to
unify a number of classical methods for the control of lightly damped
structures with collocated force actuators and position sensors including
positive position feedback and integral force feedback. The key result is a
stability result which shows why these methods are guaranteed to yield robust
closed loop stability in the face of unmodelled spillover dynamics. Related
results to be presented connect the theory of negative imaginary systems to
positive real systems theory and a negative imaginary lemma has been
established which is analogous to the positive real lemma. The paper also
presents recent controller synthesis results based on the theory of negative
imaginary systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3557</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3557</id><created>2013-01-15</created><authors><author><keyname>Zeiler</keyname><forenames>Matthew D.</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author></authors><title>Stochastic Pooling for Regularization of Deep Convolutional Neural
  Networks</title><categories>cs.LG cs.NE stat.ML</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a simple and effective method for regularizing large
convolutional neural networks. We replace the conventional deterministic
pooling operations with a stochastic procedure, randomly picking the activation
within each pooling region according to a multinomial distribution, given by
the activities within the pooling region. The approach is hyper-parameter free
and can be combined with other regularization approaches, such as dropout and
data augmentation. We achieve state-of-the-art performance on four image
datasets, relative to other approaches that do not utilize data augmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3560</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3560</id><created>2013-01-15</created><authors><author><keyname>Yuille</keyname><forenames>Alan L.</forenames></author><author><keyname>Mottaghi</keyname><forenames>Roozbeh</forenames></author></authors><title>Complexity of Representation and Inference in Compositional Models with
  Part Sharing</title><categories>cs.CV</categories><comments>ICLR 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes serial and parallel compositional models of multiple
objects with part sharing. Objects are built by part-subpart compositions and
expressed in terms of a hierarchical dictionary of object parts. These parts
are represented on lattices of decreasing sizes which yield an executive
summary description. We describe inference and learning algorithms for these
models. We analyze the complexity of this model in terms of computation time
(for serial computers) and numbers of nodes (e.g., &quot;neurons&quot;) for parallel
computers. In particular, we compute the complexity gains by part sharing and
its dependence on how the dictionary scales with the level of the hierarchy. We
explore three regimes of scaling behavior where the dictionary size (i)
increases exponentially with the level, (ii) is determined by an unsupervised
compositional learning algorithm applied to real data, (iii) decreases
exponentially with scale. This analysis shows that in some regimes the use of
shared parts enables algorithms which can perform inference in time linear in
the number of levels for an exponential number of objects. In other regimes
part sharing has little advantage for serial computers but can give linear
processing on parallel computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3568</identifier>
 <datestamp>2013-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3568</id><created>2013-01-15</created><updated>2013-05-01</updated><authors><author><keyname>Goodfellow</keyname><forenames>Ian J.</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Joint Training Deep Boltzmann Machines for Classification</title><categories>stat.ML cs.LG</categories><comments>Major revision with new techniques and experiments. This version
  includes new material put on the poster for the ICLR workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new method for training deep Boltzmann machines jointly. Prior
methods of training DBMs require an initial learning pass that trains the model
greedily, one layer at a time, or do not perform well on classification tasks.
In our approach, we train all layers of the DBM simultaneously, using a novel
training procedure called multi-prediction training. The resulting model can
either be interpreted as a single generative model trained to maximize a
variational approximation to the generalized pseudolikelihood, or as a family
of recurrent networks that share parameters and may be approximately averaged
together using a novel technique we call the multi-inference trick. We show
that our approach performs competitively for classification and outperforms
previous methods in terms of accuracy of approximate inference and
classification with missing inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3572</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3572</id><created>2013-01-15</created><updated>2013-03-14</updated><authors><author><keyname>Couprie</keyname><forenames>Camille</forenames></author><author><keyname>Farabet</keyname><forenames>Cl&#xe9;ment</forenames></author><author><keyname>Najman</keyname><forenames>Laurent</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Indoor Semantic Segmentation using depth information</title><categories>cs.CV</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work addresses multi-class segmentation of indoor scenes with RGB-D
inputs. While this area of research has gained much attention recently, most
works still rely on hand-crafted features. In contrast, we apply a multiscale
convolutional network to learn features directly from the images and the depth
information. We obtain state-of-the-art on the NYU-v2 depth dataset with an
accuracy of 64.5%. We illustrate the labeling of indoor scenes in videos
sequences that could be processed in real-time using appropriate hardware such
as an FPGA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3575</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3575</id><created>2013-01-15</created><authors><author><keyname>Xie</keyname><forenames>Boyi</forenames></author><author><keyname>Zheng</keyname><forenames>Shuheng</forenames></author></authors><title>Kernelized Locality-Sensitive Hashing for Semi-Supervised Agglomerative
  Clustering</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large scale agglomerative clustering is hindered by computational burdens. We
propose a novel scheme where exact inter-instance distance calculation is
replaced by the Hamming distance between Kernelized Locality-Sensitive Hashing
(KLSH) hashed values. This results in a method that drastically decreases
computation time. Additionally, we take advantage of certain labeled data
points via distance metric learning to achieve a competitive precision and
recall comparing to K-Means but in much less computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3577</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3577</id><created>2013-01-15</created><updated>2013-03-20</updated><authors><author><keyname>Goroshin</keyname><forenames>Rostislav</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Saturating Auto-Encoders</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a simple new regularizer for auto-encoders whose hidden-unit
activation functions contain at least one zero-gradient (saturated) region.
This regularizer explicitly encourages activations in the saturated region(s)
of the corresponding activation function. We call these Saturating
Auto-Encoders (SATAE). We show that the saturation regularizer explicitly
limits the SATAE's ability to reconstruct inputs which are not near the data
manifold. Furthermore, we show that a wide variety of features can be learned
when different activation functions are used. Finally, connections are
established with the Contractive and Sparse Auto-Encoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3578</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3578</id><created>2013-01-15</created><updated>2013-01-23</updated><authors><author><keyname>Nielsen</keyname><forenames>Frank</forenames></author></authors><title>Cramer-Rao Lower Bound and Information Geometry</title><categories>cs.IT math.IT</categories><comments>To appear in Connected at Infinity II: On the work of Indian
  mathematicians (R. Bhatia and C.S. Rajan, Eds.), special volume of Texts and
  Readings In Mathematics (TRIM), Hindustan Book Agency, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article focuses on an important piece of work of the world renowned
Indian statistician, Calyampudi Radhakrishna Rao. In 1945, C. R. Rao (25 years
old then) published a pathbreaking paper, which had a profound impact on
subsequent statistical research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3583</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3583</id><created>2013-01-15</created><updated>2013-03-14</updated><authors><author><keyname>Dauphin</keyname><forenames>Yann N.</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Big Neural Networks Waste Capacity</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article exposes the failure of some big neural networks to leverage
added capacity to reduce underfitting. Past research suggest diminishing
returns when increasing the size of neural networks. Our experiments on
ImageNet LSVRC-2010 show that this may be due to the fact there are highly
diminishing returns for capacity in terms of training error, leading to
underfitting. This suggests that the optimization method - first order gradient
descent - fails at this regime. Directly attacking this problem, either through
the optimization method or the choices of parametrization, may allow to improve
the generalization error on large datasets, for which a large capacity is
required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3584</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3584</id><created>2013-01-15</created><updated>2014-02-17</updated><authors><author><keyname>Pascanu</keyname><forenames>Razvan</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Revisiting Natural Gradient for Deep Networks</title><categories>cs.LG cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We evaluate natural gradient, an algorithm originally proposed in Amari
(1997), for learning deep models. The contributions of this paper are as
follows. We show the connection between natural gradient and three other
recently proposed methods for training deep models: Hessian-Free (Martens,
2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux et
al., 2008). We describe how one can use unlabeled data to improve the
generalization error obtained by natural gradient and empirically evaluate the
robustness of the algorithm to the ordering of the training set compared to
stochastic gradient descent. Finally we extend natural gradient to incorporate
second order information alongside the manifold information and provide a
benchmark of the new algorithm using a truncated Newton approach for inverting
the metric matrix instead of using a diagonal approximation of it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3590</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3590</id><created>2013-01-16</created><authors><author><keyname>Szlam</keyname><forenames>Arthur</forenames></author></authors><title>Tree structured sparse coding on cubes</title><categories>cs.IT cs.CV math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A brief description of tree structured sparse coding on the binary cube.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3592</identifier>
 <datestamp>2014-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3592</id><created>2013-01-16</created><updated>2014-08-21</updated><authors><author><keyname>Lenz</keyname><forenames>Ian</forenames></author><author><keyname>Lee</keyname><forenames>Honglak</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>Deep Learning for Detecting Robotic Grasps</title><categories>cs.LG cs.CV cs.RO</categories><comments>Current version was accepted to IJRR Special Issue on Robot Vision
  2014 Workshop version accepted to ICLR 2013. Conference version accepted to
  RSS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of detecting robotic grasps in an RGB-D view of a
scene containing objects. In this work, we apply a deep learning approach to
solve this problem, which avoids time-consuming hand-design of features. This
presents two main challenges. First, we need to evaluate a huge number of
candidate grasps. In order to make detection fast, as well as robust, we
present a two-step cascaded structure with two deep networks, where the top
detections from the first are re-evaluated by the second. The first network has
fewer features, is faster to run, and can effectively prune out unlikely
candidate grasps. The second, with more features, is slower but has to run only
on the top few detections. Second, we need to handle multimodal inputs well,
for which we present a method to apply structured regularization on the weights
based on multimodal group regularization. We demonstrate that our method
outperforms the previous state-of-the-art methods in robotic grasp detection,
and can be used to successfully execute grasps on two different robotic
platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3598</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3598</id><created>2013-01-16</created><updated>2013-11-16</updated><authors><author><keyname>Ji</keyname><forenames>Bo</forenames></author><author><keyname>Gupta</keyname><forenames>Gagan R.</forenames></author><author><keyname>Lin</keyname><forenames>Xiaojun</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author></authors><title>Low-Complexity Scheduling Policies for Achieving Throughput and
  Asymptotic Delay Optimality in Multi-Channel Wireless Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>Accepted for publication by the IEEE/ACM Transactions on Networking.
  arXiv admin note: text overlap with arXiv:1212.1638</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the scheduling problem for downlink transmission in a
multi-channel (e.g., OFDM-based) wireless network. We focus on a single cell,
with the aim of developing a unifying framework for designing low-complexity
scheduling policies that can provide optimal performance in terms of both
throughput and delay. We develop new easy-to-verify sufficient conditions for
rate-function delay optimality (in the many-channel many-user asymptotic
regime) and throughput optimality (in general non-asymptotic setting),
respectively. The sufficient conditions allow us to prove rate-function delay
optimality for a class of Oldest Packets First (OPF) policies and throughput
optimality for a large class of Maximum Weight in the Fluid limit (MWF)
policies, respectively. By exploiting the special features of our carefully
chosen sufficient conditions and intelligently combining policies from the
classes of OPF and MWF policies, we design hybrid policies that are both
rate-function delay-optimal and throughput-optimal with a complexity of
$O(n^{2.5} \log n)$, where $n$ is the number of channels or users. Our
sufficient condition is also used to show that a previously proposed policy
called Delay Weighted Matching (DWM) is rate-function delay-optimal. However,
DWM incurs a high complexity of $O(n^5)$. Thus, our approach yields
significantly lower complexity than the only previously designed delay and
throughput optimal scheduling policy. We also conduct numerical experiments to
validate our theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3599</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3599</id><created>2013-01-16</created><authors><author><keyname>Bader</keyname><forenames>Ahmed</forenames></author><author><keyname>Abed-Meraim</keyname><forenames>Karim</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Technical Report: Beaconless Geo-Routing Under The Spotlight: Practical
  Link Models and Application Scenarios</title><categories>cs.NI cs.PF</categories><comments>14 pages, 5 figures</comments><doi>10.1109/JSYST.2013.2280848</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis and simulation of beaconless geo-routing protocols have been
traditionally conducted assuming equal communication ranges for the data and
control packets. In reality, this is not true since the communication range is
actually function of the packet length. Control packets are typically much
shorter than data packets. As a consequence, a substantial discrepancy exists
in practice between their respective communication ranges. In this paper, we
devise a practical link model for computing the effective communication range.
We further introduce two simple strategies for bridging the gap between the
control and data packet communication ranges. Our primary objective in this
paper is to construct a realistic analytical framework describing the
end-to-end performance of beaconless geo-routing protocols. Two flagship
protocols are selected in this paper for further investigation under the
developed framework. For a better perspective, the two protocols are actually
compared to a hypothetical limit case; one which offers optimal energy and
latency performance. Finally, we present four different application scenarios.
For each scenario, we highlight the geo-routing protocol which performs the
best and discuss the reasons behind it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3601</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3601</id><created>2013-01-16</created><authors><author><keyname>de Lima</keyname><forenames>Carlos H. M.</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Latva-aho</keyname><forenames>Matti</forenames></author></authors><title>Statistical Analysis of Self-Organizing Networks with Biased Cell
  Association and Interference Avoidance</title><categories>cs.NI cs.IT math.IT stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we assess the viability of heterogeneous networks composed of
legacy macrocells which are underlaid with self-organizing picocells. Aiming to
improve coverage, cell-edge throughput and overall system capacity,
self-organizing solutions, such as range expansion bias, almost blank subframe
and distributed antenna systems are considered. Herein, stochastic geometry is
used to model network deployments, while higher-order statistics through the
cumulants concept is utilized to characterize the probability distribution of
the received power and aggregate interference at the user of interest. A
compre- hensive analytical framework is introduced to evaluate the performance
of such self-organizing networks in terms of outage probability and average
channel capacity with respect to the tagged receiver. To conduct our studies,
we consider a shadowed fading channel model incorporating log-normal shadowing
and Nakagami-m fading. Results show that the analytical framework matches well
with numerical results obtained from Monte Carlo simulations. We also observed
that by simply using almost blank subframes the aggregate interference at the
tagged receiver is reduced by about 12dB. Although more elaborated interference
control techniques such as, downlink bitmap and distributed antennas systems
become needed, when the density of picocells in the underlaid tier gets high.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3605</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3605</id><created>2013-01-16</created><updated>2013-03-08</updated><authors><author><keyname>Yu</keyname><forenames>Dong</forenames></author><author><keyname>Seltzer</keyname><forenames>Michael L.</forenames></author><author><keyname>Li</keyname><forenames>Jinyu</forenames></author><author><keyname>Huang</keyname><forenames>Jui-Ting</forenames></author><author><keyname>Seide</keyname><forenames>Frank</forenames></author></authors><title>Feature Learning in Deep Neural Networks - Studies on Speech Recognition
  Tasks</title><categories>cs.LG cs.CL cs.NE</categories><comments>ICLR 2013, 9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies have shown that deep neural networks (DNNs) perform
significantly better than shallow networks and Gaussian mixture models (GMMs)
on large vocabulary speech recognition tasks. In this paper, we argue that the
improved accuracy achieved by the DNNs is the result of their ability to
extract discriminative internal representations that are robust to the many
sources of variability in speech signals. We show that these representations
become increasingly insensitive to small perturbations in the input with
increasing network depth, which leads to better speech recognition performance
with deeper networks. We also show that DNNs cannot extrapolate to test samples
that are substantially different from the training examples. If the training
data are sufficiently representative, however, internal features learned by the
DNN are relatively stable with respect to speaker differences, bandwidth
differences, and environment distortion. This enables DNN-based recognizers to
perform as well or better than state-of-the-art systems based on GMMs or
shallow networks without the need for explicit model adaptation or feature
normalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3609</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3609</id><created>2013-01-16</created><authors><author><keyname>Perchet</keyname><forenames>Vianney</forenames><affiliation>LPMA</affiliation></author><author><keyname>Quincampoix</keyname><forenames>Marc</forenames><affiliation>LM</affiliation></author></authors><title>On an unified framework for approachability in games with or without
  signals</title><categories>cs.GT math.OC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We unify standard frameworks for approachability both in full or partial
monitoring by defining a new abstract game, called the &quot;purely informative
game&quot;, where the outcome at each stage is the maximal information players can
obtain, represented as some probability measure. Objectives of players can be
rewritten as the convergence (to some given set) of sequences of averages of
these probability measures. We obtain new results extending the approachability
theory developed by Blackwell moreover this new abstract framework enables us
to characterize approachable sets with, as usual, a remarkably simple and clear
reformulation for convex sets. Translated into the original games, those
results become the first necessary and sufficient condition under which an
arbitrary set is approachable and they cover and extend previous known results
for convex sets. We also investigate a specific class of games where, thanks to
some unusual definition of averages and convexity, we again obtain a complete
characterization of approachable sets along with rates of convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3614</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3614</id><created>2013-01-16</created><updated>2013-01-20</updated><authors><author><keyname>Okita</keyname><forenames>Tsuyoshi</forenames></author></authors><title>Joint Space Neural Probabilistic Language Model for Statistical Machine
  Translation</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A neural probabilistic language model (NPLM) provides an idea to achieve the
better perplexity than n-gram language model and their smoothed language
models. This paper investigates application area in bilingual NLP, specifically
Statistical Machine Translation (SMT). We focus on the perspectives that NPLM
has potential to open the possibility to complement potentially `huge'
monolingual resources into the `resource-constraint' bilingual resources. We
introduce an ngram-HMM language model as NPLM using the non-parametric Bayesian
construction. In order to facilitate the application to various tasks, we
propose the joint space model of ngram-HMM language model. We show an
experiment of system combination in the area of SMT. One discovery was that our
treatment of noise improved the results 0.20 BLEU points if NPLM is trained in
relatively small corpus, in our case 500,000 sentence pairs, which is often the
case due to the long training time of NPLM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3618</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3618</id><created>2013-01-16</created><updated>2013-03-15</updated><authors><author><keyname>Chen</keyname><forenames>Danqi</forenames></author><author><keyname>Socher</keyname><forenames>Richard</forenames></author><author><keyname>Manning</keyname><forenames>Christopher D.</forenames></author><author><keyname>Ng</keyname><forenames>Andrew Y.</forenames></author></authors><title>Learning New Facts From Knowledge Bases With Neural Tensor Networks and
  Semantic Word Vectors</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge bases provide applications with the benefit of easily accessible,
systematic relational knowledge but often suffer in practice from their
incompleteness and lack of knowledge of new entities and relations. Much work
has focused on building or extending them by finding patterns in large
unannotated text corpora. In contrast, here we mainly aim to complete a
knowledge base by predicting additional true relationships between entities,
based on generalizations that can be discerned in the given knowledgebase. We
introduce a neural tensor network (NTN) model which predicts new relationship
entries that can be added to the database. This model can be improved by
initializing entity representations with word vectors learned in an
unsupervised fashion from text, and when doing this, existing relations can
even be queried for entities that were not present in the database. Our model
generalizes and outperforms existing models for this problem, and can classify
unseen relationships in WordNet with an accuracy of 75.8%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3627</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3627</id><created>2013-01-16</created><updated>2013-05-11</updated><authors><author><keyname>Schuetze</keyname><forenames>Hinrich</forenames></author><author><keyname>Scheible</keyname><forenames>Christian</forenames></author></authors><title>Two SVDs produce more focal deep learning representations</title><categories>cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A key characteristic of work on deep learning and neural networks in general
is that it relies on representations of the input that support generalization,
robust inference, domain adaptation and other desirable functionalities. Much
recent progress in the field has focused on efficient and effective methods for
computing representations. In this paper, we propose an alternative method that
is more efficient than prior work and produces representations that have a
property we call focality -- a property we hypothesize to be important for
neural network representations. The method consists of a simple application of
two consecutive SVDs and is inspired by Anandkumar (2012).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3630</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3630</id><created>2013-01-16</created><updated>2013-03-20</updated><authors><author><keyname>Qiao</keyname><forenames>Qifeng</forenames></author><author><keyname>Beling</keyname><forenames>Peter A.</forenames></author></authors><title>Behavior Pattern Recognition using A New Representation Model</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the use of inverse reinforcement learning (IRL) as a tool for the
recognition of agents' behavior on the basis of observation of their sequential
decision behavior interacting with the environment. We model the problem faced
by the agents as a Markov decision process (MDP) and model the observed
behavior of the agents in terms of forward planning for the MDP. We use IRL to
learn reward functions and then use these reward functions as the basis for
clustering or classification models. Experimental studies with GridWorld, a
navigation problem, and the secretary problem, an optimal stopping problem,
suggest reward vectors found from IRL can be a good basis for behavior pattern
recognition problems. Empirical comparisons of our method with several existing
IRL algorithms and with direct methods that use feature statistics observed in
state-action space suggest it may be superior for recognition problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3632</identifier>
 <datestamp>2013-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3632</id><created>2013-01-16</created><authors><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Karas</keyname><forenames>Maciej</forenames></author><author><keyname>Szczypiorski</keyname><forenames>Krzysztof</forenames></author></authors><title>SkyDe: a Skype-based Steganographic Method</title><categories>cs.CR cs.MM</categories><comments>7 pages, 6 figures, 1 table. The paper is submitted to 1st ACM
  Information Hiding and Multimedia Security Workshop (IH and ACM MMSec
  conferences merged into a single event), June 17-19, 2013, Montpellier,
  France</comments><journal-ref>International Journal of Computers, Communications &amp; Control
  (IJCCC), ISSN: 1841- 9836, 8(3), June 2013, pp. 389-400</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces SkyDe (Skype Hide), a new steganographic method that
utilizes Skype encrypted packets with silence to provide the means for
clandestine communication. It is possible to reuse packets that do not carry
voice signals for steganographic purposes because Skype does not use any
silence suppression mechanism. The method's proof-of-concept implementation and
first experimental results are presented. They prove that the method is
feasible and offers steganographic bandwidth as high as 2.8 kbps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3641</identifier>
 <datestamp>2013-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3641</id><created>2013-01-16</created><updated>2013-05-01</updated><authors><author><keyname>Kiros</keyname><forenames>Ryan</forenames></author></authors><title>Training Neural Networks with Stochastic Hessian-Free Optimization</title><categories>cs.LG cs.NE stat.ML</categories><comments>11 pages, ICLR 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hessian-free (HF) optimization has been successfully used for training deep
autoencoders and recurrent networks. HF uses the conjugate gradient algorithm
to construct update directions through curvature-vector products that can be
computed on the same order of time as gradients. In this paper we exploit this
property and study stochastic HF with gradient and curvature mini-batches
independent of the dataset size. We modify Martens' HF for these settings and
integrate dropout, a method for preventing co-adaptation of feature detectors,
to guard against overfitting. Stochastic Hessian-free optimization gives an
intermediary between SGD and HF that achieves competitive performance on both
classification and deep autoencoder experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3644</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3644</id><created>2013-01-16</created><authors><author><keyname>Kim</keyname><forenames>Kye-Hyeon</forenames></author><author><keyname>Cai</keyname><forenames>Rui</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Choi</keyname><forenames>Seungjin</forenames></author></authors><title>Regularized Discriminant Embedding for Visual Descriptor Learning</title><categories>cs.CV cs.LG</categories><comments>3 pages + 1 additional page containing only cited references; The
  full version of this manuscript is currently under review in an international
  journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Images can vary according to changes in viewpoint, resolution, noise, and
illumination. In this paper, we aim to learn representations for an image,
which are robust to wide changes in such environmental conditions, using
training pairs of matching and non-matching local image patches that are
collected under various environmental conditions. We present a regularized
discriminant analysis that emphasizes two challenging categories among the
given training pairs: (1) matching, but far apart pairs and (2) non-matching,
but close pairs in the original feature space (e.g., SIFT feature space).
Compared to existing work on metric learning and discriminant analysis, our
method can better distinguish relevant images from irrelevant, but look-alike
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3662</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3662</id><created>2013-01-16</created><updated>2014-09-13</updated><authors><author><keyname>Dunjko</keyname><forenames>Vedran</forenames></author><author><keyname>Fitzsimons</keyname><forenames>Joseph F.</forenames></author><author><keyname>Portmann</keyname><forenames>Christopher</forenames></author><author><keyname>Renner</keyname><forenames>Renato</forenames></author></authors><title>Composable security of delegated quantum computation</title><categories>quant-ph cs.CR cs.IT math.IT</categories><comments>37+9 pages, 13 figures. v3: minor changes, new references. v2:
  extended the reduction between composable and local security to include
  entangled inputs, substantially rewritten the introduction to the Abstract
  Cryptography (AC) framework</comments><journal-ref>ASIACRYPT 2014, LNCS Volume 8874, 2014, pp 406-425</journal-ref><doi>10.1007/978-3-662-45608-8_22</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delegating difficult computations to remote large computation facilities,
with appropriate security guarantees, is a possible solution for the
ever-growing needs of personal computing power. For delegated computation
protocols to be usable in a larger context---or simply to securely run two
protocols in parallel---the security definitions need to be composable. Here,
we define composable security for delegated quantum computation. We distinguish
between protocols which provide only blindness---the computation is hidden from
the server---and those that are also verifiable---the client can check that it
has received the correct result. We show that the composable security
definition capturing both these notions can be reduced to a combination of
several distinct &quot;trace-distance-type&quot; criteria---which are, individually,
non-composable security definitions.
  Additionally, we study the security of some known delegated quantum
computation protocols, including Broadbent, Fitzsimons and Kashefi's Universal
Blind Quantum Computation protocol. Even though these protocols were originally
proposed with insufficient security criteria, they turn out to still be secure
given the stronger composable definitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3666</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3666</id><created>2013-01-16</created><updated>2013-03-19</updated><authors><author><keyname>Socher</keyname><forenames>Richard</forenames></author><author><keyname>Ganjoo</keyname><forenames>Milind</forenames></author><author><keyname>Sridhar</keyname><forenames>Hamsa</forenames></author><author><keyname>Bastani</keyname><forenames>Osbert</forenames></author><author><keyname>Manning</keyname><forenames>Christopher D.</forenames></author><author><keyname>Ng</keyname><forenames>Andrew Y.</forenames></author></authors><title>Zero-Shot Learning Through Cross-Modal Transfer</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work introduces a model that can recognize objects in images even if no
training data is available for the objects. The only necessary knowledge about
the unseen categories comes from unsupervised large text corpora. In our
zero-shot framework distributional information in language can be seen as
spanning a semantic basis for understanding what objects look like. Most
previous zero-shot learning models can only differentiate between unseen
classes. In contrast, our model can both obtain state of the art performance on
classes that have thousands of training images and obtain reasonable
performance on unseen classes. This is achieved by first using outlier
detection in the semantic space and then two separate recognition models.
Furthermore, our model does not require any manually defined semantic features
for either words or images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3676</identifier>
 <datestamp>2014-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3676</id><created>2013-01-16</created><updated>2013-09-02</updated><authors><author><keyname>B&#xfc;rger</keyname><forenames>Mathias</forenames></author><author><keyname>Zelazo</keyname><forenames>Daniel</forenames></author><author><keyname>Allg&#xf6;wer</keyname><forenames>Frank</forenames></author></authors><title>Duality and Network Theory in Passivity-based Cooperative Control</title><categories>math.OC cs.SY</categories><comments>submitted to Automatica (revised version)</comments><doi>10.1016/j.automatica.2014.06.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a class of passivity-based cooperative control problems
that have an explicit connection to convex network optimization problems. The
new notion of maximal equilibrium independent passivity is introduced and it is
shown that networks of systems possessing this property asymptotically approach
the solutions of a dual pair of network optimization problems, namely an
optimal potential and an optimal flow problem. This connection leads to an
interpretation of the dynamic variables, such as system inputs and outputs, to
variables in a network optimization framework, such as divergences and
potentials, and reveals that several duality relations known in convex network
optimization theory translate directly to passivity-based cooperative control
problems. The presented results establish a strong and explicit connection
between passivity-based cooperative control theory on the one side and network
optimization theory on the other, and they provide a unifying framework for
network analysis and optimal design. The results are illustrated on a nonlinear
traffic dynamics model that is shown to be asymptotically clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3683</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3683</id><created>2013-01-16</created><updated>2013-07-17</updated><authors><author><keyname>Swoboda</keyname><forenames>Paul</forenames></author><author><keyname>Schn&#xf6;rr</keyname><forenames>Christoph</forenames></author></authors><title>Convex Variational Image Restoration with Histogram Priors</title><categories>math.OC cs.CV</categories><comments>20 pages, 11 figures</comments><acm-class>G.1.6; I.4.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel variational approach to image restoration (e.g.,
denoising, inpainting, labeling) that enables to complement established
variational approaches with a histogram-based prior enforcing closeness of the
solution to some given empirical measure. By minimizing a single objective
function, the approach utilizes simultaneously two quite different sources of
information for restoration: spatial context in terms of some smoothness prior
and non-spatial statistics in terms of the novel prior utilizing the
Wasserstein distance between probability measures. We study the combination of
the functional lifting technique with two different relaxations of the
histogram prior and derive a jointly convex variational approach. Mathematical
equivalence of both relaxations is established and cases where optimality holds
are discussed. Additionally, we present an efficient algorithmic scheme for the
numerical treatment of the presented model. Experiments using the basic
total-variation based denoising approach as a case study demonstrate our novel
regularization approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3698</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3698</id><created>2013-01-16</created><updated>2013-04-17</updated><authors><author><keyname>Starnini</keyname><forenames>Michele</forenames></author><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author><author><keyname>Pastor-Satorras</keyname><forenames>Romualdo</forenames></author></authors><title>Modeling human dynamics of face-to-face interaction networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><journal-ref>Phys. Rev. Lett. 110, 168701 (2013)</journal-ref><doi>10.1103/PhysRevLett.110.168701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face-to-face interaction networks describe social interactions in human
gatherings, and are the substrate for processes such as epidemic spreading and
gossip propagation. The bursty nature of human behavior characterizes many
aspects of empirical data, such as the distribution of conversation lengths, of
conversations per person, or of inter-conversation times. Despite several
recent attempts, a general theoretical understanding of the global picture
emerging from data is still lacking. Here we present a simple model that
reproduces quantitatively most of the relevant features of empirical
face-to-face interaction networks. The model describes agents which perform a
random walk in a two dimensional space and are characterized by an
attractiveness whose effect is to slow down the motion of people around them.
The proposed framework sheds light on the dynamics of human interactions and
can improve the modeling of dynamical processes taking place on the ensuing
dynamical social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3708</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3708</id><created>2013-01-16</created><authors><author><keyname>Katselis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Rojas</keyname><forenames>Cristian R.</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Bombois</keyname><forenames>Xavier</forenames></author><author><keyname>Shariati</keyname><forenames>Nafiseh</forenames></author><author><keyname>Jansson</keyname><forenames>Magnus</forenames></author><author><keyname>Hjalmarsson</keyname><forenames>H&#xe5;kan</forenames></author></authors><title>Training Sequence Design for MIMO Channels: An Application-Oriented
  Approach</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of training optimization for estimating a
multiple-input multiple-output (MIMO) flat fading channel in the presence of
spatially and temporally correlated Gaussian noise is studied in an
application-oriented setup. So far, the problem of MIMO channel estimation has
mostly been treated within the context of minimizing the mean square error
(MSE) of the channel estimate subject to various constraints, such as an upper
bound on the available training energy. We introduce a more general framework
for the task of training sequence design in MIMO systems, which can treat not
only the minimization of channel estimator's MSE, but also the optimization of
a final performance metric of interest related to the use of the channel
estimate in the communication system. First, we show that the proposed
framework can be used to minimize the training energy budget subject to a
quality constraint on the MSE of the channel estimator. A deterministic version
of the &quot;dual&quot; problem is also provided. We then focus on four specific
applications, where the training sequence can be optimized with respect to the
classical channel estimation MSE, a weighted channel estimation MSE and the MSE
of the equalization error due to the use of an equalizer at the receiver or an
appropriate linear precoder at the transmitter. In this way, the intended use
of the channel estimate is explicitly accounted for. The superiority of the
proposed designs over existing methods is demonstrated via numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3720</identifier>
 <datestamp>2014-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3720</id><created>2013-01-16</created><updated>2014-02-25</updated><authors><author><keyname>Schl&#xfc;ter</keyname><forenames>Federico</forenames></author><author><keyname>Bromberg</keyname><forenames>Facundo</forenames></author><author><keyname>Edera</keyname><forenames>Alejandro</forenames></author></authors><title>The IBMAP approach for Markov networks structure learning</title><categories>cs.AI cs.LG</categories><doi>10.1007/s10472-014-9419-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider the problem of learning the structure of Markov
networks from data. We present an approach for tackling this problem called
IBMAP, together with an efficient instantiation of the approach: the IBMAP-HC
algorithm, designed for avoiding important limitations of existing
independence-based algorithms. These algorithms proceed by performing
statistical independence tests on data, trusting completely the outcome of each
test. In practice tests may be incorrect, resulting in potential cascading
errors and the consequent reduction in the quality of the structures learned.
IBMAP contemplates this uncertainty in the outcome of the tests through a
probabilistic maximum-a-posteriori approach. The approach is instantiated in
the IBMAP-HC algorithm, a structure selection strategy that performs a
polynomial heuristic local search in the space of possible structures. We
present an extensive empirical evaluation on synthetic and real data, showing
that our algorithm outperforms significantly the current independence-based
algorithms, in terms of data efficiency and quality of learned structures, with
equivalent computational complexities. We also show the performance of IBMAP-HC
in a real-world application of knowledge discovery: EDAs, which are
evolutionary algorithms that use structure learning on each generation for
modeling the distribution of populations. The experiments show that when
IBMAP-HC is used to learn the structure, EDAs improve the convergence to the
optimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3744</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3744</id><created>2013-01-16</created><authors><author><keyname>Vines</keyname><forenames>Timothy H.</forenames></author><author><keyname>Andrew</keyname><forenames>Rose L.</forenames></author><author><keyname>Bock</keyname><forenames>Dan G.</forenames></author><author><keyname>Franklin</keyname><forenames>Michelle T.</forenames></author><author><keyname>Gilbert</keyname><forenames>Kimberly J.</forenames></author><author><keyname>Kane</keyname><forenames>Nolan C.</forenames></author><author><keyname>Moore</keyname><forenames>Jean-S&#xe9;bastien</forenames></author><author><keyname>Moyers</keyname><forenames>Brook T.</forenames></author><author><keyname>Renaut</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Rennison</keyname><forenames>Diana J.</forenames></author><author><keyname>Veen</keyname><forenames>Thor</forenames></author><author><keyname>Yeaman</keyname><forenames>Sam</forenames></author></authors><title>Mandated data archiving greatly improves access to research data</title><categories>cs.DL physics.soc-ph q-bio.QM</categories><doi>10.1096/fj.12-218164</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The data underlying scientific papers should be accessible to researchers
both now and in the future, but how best can we ensure that these data are
available? Here we examine the effectiveness of four approaches to data
archiving: no stated archiving policy, recommending (but not requiring)
archiving, and two versions of mandating data deposition at acceptance. We
control for differences between data types by trying to obtain data from papers
that use a single, widespread population genetic analysis, STRUCTURE. At one
extreme, we found that mandated data archiving policies that require the
inclusion of a data availability statement in the manuscript improve the odds
of finding the data online almost a thousand-fold compared to having no policy.
However, archiving rates at journals with less stringent policies were only
very slightly higher than those with no policy at all. At one extreme, we found
that mandated data archiving policies that require the inclusion of a data
availability statement in the manuscript improve the odds of finding the data
online almost a thousand fold compared to having no policy. However, archiving
rates at journals with less stringent policies were only very slightly higher
than those with no policy at all. We also assessed the effectiveness of asking
for data directly from authors and obtained over half of the requested
datasets, albeit with about 8 days delay and some disagreement with authors.
Given the long term benefits of data accessibility to the academic community,
we believe that journal based mandatory data archiving policies and mandatory
data availability statements should be more widely adopted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3748</identifier>
 <datestamp>2013-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3748</id><created>2013-01-16</created><updated>2013-05-10</updated><authors><author><keyname>Brembs</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Munaf&#xf2;</keyname><forenames>Marcus</forenames></author></authors><title>Deep Impact: Unintended consequences of journal rank</title><categories>cs.DL physics.soc-ph stat.OT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Most researchers acknowledge an intrinsic hierarchy in the scholarly journals
('journal rank') that they submit their work to, and adjust not only their
submission but also their reading strategies accordingly. On the other hand,
much has been written about the negative effects of institutionalizing journal
rank as an impact measure. So far, contributions to the debate concerning the
limitations of journal rank as a scientific impact assessment tool have either
lacked data, or relied on only a few studies. In this review, we present the
most recent and pertinent data on the consequences of our current scholarly
communication system with respect to various measures of scientific quality
(such as utility/citations, methodological soundness, expert ratings or
retractions). These data corroborate previous hypotheses: using journal rank as
an assessment tool is bad scientific practice. Moreover, the data lead us to
argue that any journal rank (not only the currently-favored Impact Factor)
would have this negative impact. Therefore, we suggest that abandoning journals
altogether, in favor of a library-based scholarly communication system, will
ultimately be necessary. This new system will use modern information technology
to vastly improve the filter, sort and discovery functions of the current
journal system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3751</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3751</id><created>2013-01-16</created><authors><author><keyname>Abdalaoui</keyname><forenames>el Houcein el</forenames></author><author><keyname>Dahmoune</keyname><forenames>Mohamed</forenames></author><author><keyname>Ziadi</keyname><forenames>Djelloul</forenames></author></authors><title>On the transition reduction problem for finite automata</title><categories>cs.FL math.CO</categories><comments>22 pages, 7 figures, submitted; Keywords: language, automata,
  reduction, deterministic minimal automaton, state, regular expression,
  $Z$-partition, $P$-trees</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in the problem of transition reduction of nondeterministic
automata. We present some results on the reduction of the automata recognizing
the language $L(E_n)$ denoted by the regular expression $E_n=(1+\varepsilon)...
(2+\varepsilon)... (3+\varepsilon)... (n+\varepsilon)$. These results can be
used in the general case of the transition reduction problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3753</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3753</id><created>2013-01-16</created><updated>2013-01-19</updated><authors><author><keyname>Johnson</keyname><forenames>Leif</forenames></author><author><keyname>Corcoran</keyname><forenames>Craig</forenames></author></authors><title>Switched linear encoding with rectified linear autoencoders</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several recent results in machine learning have established formal
connections between autoencoders---artificial neural network models that
attempt to reproduce their inputs---and other coding models like sparse coding
and K-means. This paper explores in depth an autoencoder model that is
constructed using rectified linear activations on its hidden units. Our
analysis builds on recent results to further unify the world of sparse linear
coding models. We provide an intuitive interpretation of the behavior of these
coding models and demonstrate this intuition using small, artificial datasets
with known distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3755</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3755</id><created>2013-01-16</created><authors><author><keyname>Rose</keyname><forenames>Derek</forenames></author><author><keyname>Arel</keyname><forenames>Itamar</forenames></author></authors><title>Gradient Driven Learning for Pooling in Visual Pipeline Feature
  Extraction Models</title><categories>cs.CV</categories><comments>3 pages, 2 figures, submitted to ICLR2013 workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyper-parameter selection remains a daunting task when building a pattern
recognition architecture which performs well, particularly in recently
constructed visual pipeline models for feature extraction. We re-formulate
pooling in an existing pipeline as a function of adjustable pooling map weight
parameters and propose the use of supervised error signals from gradient
descent to tune the established maps within the model. This technique allows us
to learn what would otherwise be a design choice within the model and
specialize the maps to aggregate areas of invariance for the task presented.
Preliminary results show moderate potential gains in classification accuracy
and highlight areas of importance within the intermediate feature
representation space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3758</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3758</id><created>2013-01-16</created><updated>2013-04-01</updated><authors><author><keyname>Dhiman</keyname><forenames>Vikas</forenames></author><author><keyname>Ryde</keyname><forenames>Julian</forenames></author><author><keyname>Corso</keyname><forenames>Jason J.</forenames></author></authors><title>Mutual Localization: Two Camera Relative 6-DOF Pose Estimation from
  Reciprocal Fiducial Observation</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concurrently estimating the 6-DOF pose of multiple cameras or
robots---cooperative localization---is a core problem in contemporary robotics.
Current works focus on a set of mutually observable world landmarks and often
require inbuilt egomotion estimates; situations in which both assumptions are
violated often arise, for example, robots with erroneous low quality odometry
and IMU exploring an unknown environment. In contrast to these existing works
in cooperative localization, we propose a cooperative localization method,
which we call mutual localization, that uses reciprocal observations of
camera-fiducials to obviate the need for egomotion estimates and mutually
observable world landmarks. We formulate and solve an algebraic formulation for
the pose of the two camera mutual localization setup under these assumptions.
Our experiments demonstrate the capabilities of our proposal egomotion-free
cooperative localization method: for example, the method achieves 2cm range and
0.7 degree accuracy at 2m sensing for 6-DOF pose. To demonstrate the
applicability of the proposed work, we deploy our method on Turtlebots and we
compare our results with ARToolKit and Bundler, over which our method achieves
a 10 fold improvement in translation estimation accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3764</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3764</id><created>2013-01-16</created><updated>2013-03-27</updated><authors><author><keyname>Schaul</keyname><forenames>Tom</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Adaptive learning rates and parallelization for stochastic, sparse,
  non-smooth gradients</title><categories>cs.LG cs.AI stat.ML</categories><comments>Published at the First International Conference on Learning
  Representations (ICLR-2013). Public reviews are available at
  http://openreview.net/document/c14f2204-fd66-4d91-bed4-153523694041#c14f2204-fd66-4d91-bed4-153523694041</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has established an empirically successful framework for adapting
learning rates for stochastic gradient descent (SGD). This effectively removes
all needs for tuning, while automatically reducing learning rates over time on
stationary problems, and permitting learning rates to grow appropriately in
non-stationary tasks. Here, we extend the idea in three directions, addressing
proper minibatch parallelization, including reweighted updates for sparse or
orthogonal gradients, improving robustness on non-smooth loss functions, in the
process replacing the diagonal Hessian estimation procedure that may not always
be available by a robust finite-difference approximation. The final algorithm
integrates all these components, has linear complexity and is hyper-parameter
free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3771</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3771</id><created>2013-01-16</created><authors><author><keyname>Seki</keyname><forenames>Shinnosuke</forenames></author></authors><title>Combinatorial Optimization in Pattern Assembly</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pattern self-assembly tile set synthesis (PATS) is a combinatorial
optimization problem which aim at minimizing a rectilinear tile assembly system
(RTAS) that uniquely self-assembles a given rectangular pattern, and is known
to be NP-hard. PATS gets practically meaningful when it is parameterized by a
constant c such that any given pattern is guaranteed to contain at most c
colors (c-PATS). We first investigate simple patterns and properties of minimum
RTASs for them. Then based on them, we design a 59-colored pattern to which
3SAT is reduced, and prove that 59-PATS is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3775</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3775</id><created>2013-01-16</created><updated>2013-03-19</updated><authors><author><keyname>Rolfe</keyname><forenames>Jason Tyler</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Discriminative Recurrent Sparse Auto-Encoders</title><categories>cs.LG cs.CV</categories><comments>Added clarifications suggested by reviewers. 15 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the discriminative recurrent sparse auto-encoder model, comprising
a recurrent encoder of rectified linear units, unrolled for a fixed number of
iterations, and connected to two linear decoders that reconstruct the input and
predict its supervised classification. Training via
backpropagation-through-time initially minimizes an unsupervised sparse
reconstruction error; the loss function is then augmented with a discriminative
term on the supervised classification. The depth implicit in the
temporally-unrolled form allows the system to exhibit all the power of deep
networks, while substantially reducing the number of trainable parameters.
  From an initially unstructured network the hidden units differentiate into
categorical-units, each of which represents an input prototype with a
well-defined class; and part-units representing deformations of these
prototypes. The learned organization of the recurrent encoder is hierarchical:
part-units are driven directly by the input, whereas the activity of
categorical-units builds up over time through interactions with the part-units.
Even using a small number of hidden units per layer, discriminative recurrent
sparse auto-encoders achieve excellent performance on MNIST.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3780</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3780</id><created>2013-01-16</created><authors><author><keyname>Brakensiek</keyname><forenames>Joshua</forenames></author><author><keyname>Potechin</keyname><forenames>Aaron</forenames></author></authors><title>Bounds on the Size of Sound Monotone Switching Networks Accepting
  Permutation Sets of Directed Trees</title><categories>cs.DM cs.DS math.CO</categories><comments>32 pages, 9 figures</comments><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we prove almost tight bounds on the size of sound monotone
switching networks accepting permutations sets of directed trees. This roughly
corresponds to proving almost tight bounds bounds on the monotone memory
efficiency of the directed ST-connectivity problem for the special case in
which the input graph is guaranteed to have no path from s to t or be
isomorphic to a specific directed tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3781</identifier>
 <datestamp>2013-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3781</id><created>2013-01-16</created><updated>2013-09-06</updated><authors><author><keyname>Mikolov</keyname><forenames>Tomas</forenames></author><author><keyname>Chen</keyname><forenames>Kai</forenames></author><author><keyname>Corrado</keyname><forenames>Greg</forenames></author><author><keyname>Dean</keyname><forenames>Jeffrey</forenames></author></authors><title>Efficient Estimation of Word Representations in Vector Space</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two novel model architectures for computing continuous vector
representations of words from very large data sets. The quality of these
representations is measured in a word similarity task, and the results are
compared to the previously best performing techniques based on different types
of neural networks. We observe large improvements in accuracy at much lower
computational cost, i.e. it takes less than a day to learn high quality word
vectors from a 1.6 billion words data set. Furthermore, we show that these
vectors provide state-of-the-art performance on our test set for measuring
syntactic and semantic word similarities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3784</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3784</id><created>2013-01-16</created><updated>2015-03-25</updated><authors><author><keyname>Nowak</keyname><forenames>Thomas</forenames></author></authors><title>Asymptotic Consensus Without Self-Confidence</title><categories>math.DS cs.SY</categories><comments>13 pages</comments><msc-class>15B51</msc-class><acm-class>F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies asymptotic consensus in systems in which agents do not
necessarily have self-confidence, i.e., may disregard their own value during
execution of the update rule. We show that the prevalent hypothesis of
self-confidence in many convergence results can be replaced by the existence of
aperiodic cores. These are stable aperiodic subgraphs, which allow to virtually
store information about an agent's value distributedly in the network. Our
results are applicable to systems with message delays and memory loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3791</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3791</id><created>2013-01-16</created><authors><author><keyname>Sathiamoorthy</keyname><forenames>Maheswaran</forenames></author><author><keyname>Asteris</keyname><forenames>Megasthenis</forenames></author><author><keyname>Papailiopoulos</keyname><forenames>Dimitris</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Vadali</keyname><forenames>Ramkumar</forenames></author><author><keyname>Chen</keyname><forenames>Scott</forenames></author><author><keyname>Borthakur</keyname><forenames>Dhruba</forenames></author></authors><title>XORing Elephants: Novel Erasure Codes for Big Data</title><categories>cs.IT cs.DC cs.NI math.IT</categories><comments>Technical report, paper to appear in Proceedings of VLDB, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed storage systems for large clusters typically use replication to
provide reliability. Recently, erasure codes have been used to reduce the large
storage overhead of three-replicated systems. Reed-Solomon codes are the
standard design choice and their high repair cost is often considered an
unavoidable price to pay for high storage efficiency and high reliability.
  This paper shows how to overcome this limitation. We present a novel family
of erasure codes that are efficiently repairable and offer higher reliability
compared to Reed-Solomon codes. We show analytically that our codes are optimal
on a recently identified tradeoff between locality and minimum distance.
  We implement our new codes in Hadoop HDFS and compare to a currently deployed
HDFS module that uses Reed-Solomon codes. Our modified HDFS implementation
shows a reduction of approximately 2x on the repair disk I/O and repair network
traffic. The disadvantage of the new coding scheme is that it requires 14% more
storage compared to Reed-Solomon codes, an overhead shown to be information
theoretically optimal to obtain locality. Because the new codes repair failures
faster, this provides higher reliability, which is orders of magnitude higher
compared to replication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3811</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3811</id><created>2013-01-16</created><updated>2013-01-18</updated><authors><author><keyname>Petrosyan</keyname><forenames>Petros A.</forenames></author><author><keyname>Khachatrian</keyname><forenames>Hrant H.</forenames></author></authors><title>Interval non-edge-colorable bipartite graphs and multigraphs</title><categories>math.CO cs.DM</categories><comments>18 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  An edge-coloring of a graph $G$ with colors $1,...,t$ is called an interval
$t$-coloring if all colors are used, and the colors of edges incident to any
vertex of $G$ are distinct and form an interval of integers. In 1991 Erd\H{o}s
constructed a bipartite graph with 27 vertices and maximum degree 13 which has
no interval coloring. Erd\H{o}s's counterexample is the smallest (in a sense of
maximum degree) known bipartite graph which is not interval colorable. On the
other hand, in 1992 Hansen showed that all bipartite graphs with maximum degree
at most 3 have an interval coloring. In this paper we give some methods for
constructing of interval non-edge-colorable bipartite graphs. In particular, by
these methods, we construct three bipartite graphs which have no interval
coloring, contain 20,19,21 vertices and have maximum degree 11,12,13,
respectively. This partially answers a question that arose in [T.R. Jensen, B.
Toft, Graph coloring problems, Wiley Interscience Series in Discrete
Mathematics and Optimization, 1995, p. 204]. We also consider similar problems
for bipartite multigraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3816</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3816</id><created>2013-01-16</created><authors><author><keyname>Dinuzzo</keyname><forenames>Francesco</forenames></author></authors><title>Learning Output Kernels for Multi-Task Problems</title><categories>cs.LG</categories><doi>10.1016/j.neucom.2013.02.024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneously solving multiple related learning tasks is beneficial under a
variety of circumstances, but the prior knowledge necessary to correctly model
task relationships is rarely available in practice. In this paper, we develop a
novel kernel-based multi-task learning technique that automatically reveals
structural inter-task relationships. Building over the framework of output
kernel learning (OKL), we introduce a method that jointly learns multiple
functions and a low-rank multi-task kernel by solving a non-convex
regularization problem. Optimization is carried out via a block coordinate
descent strategy, where each subproblem is solved using suitable conjugate
gradient (CG) type iterative methods for linear operator equations. The
effectiveness of the proposed approach is demonstrated on pharmacological and
collaborative filtering data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3832</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3832</id><created>2013-01-16</created><authors><author><keyname>Alsinet</keyname><forenames>Teresa</forenames></author><author><keyname>Godo</keyname><forenames>Lluis</forenames></author></authors><title>A Complete Calculus for Possibilistic Logic Programming with Fuzzy
  Propositional Variables</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-1-10</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a propositional logic programming language for
reasoning under possibilistic uncertainty and representing vague knowledge.
Formulas are represented by pairs (A, c), where A is a many-valued proposition
and c is value in the unit interval [0,1] which denotes a lower bound on the
belief on A in terms of necessity measures. Belief states are modeled by
possibility distributions on the set of all many-valued interpretations. In
this framework, (i) we define a syntax and a semantics of the general
underlying uncertainty logic; (ii) we provide a modus ponens-style calculus for
a sublanguage of Horn-rules and we prove that it is complete for determining
the maximum degree of possibilistic belief with which a fuzzy propositional
variable can be entailed from a set of formulas; and finally, (iii) we show how
the computation of a partial matching between fuzzy propositional variables, in
terms of necessity measures for fuzzy sets, can be included in our logic
programming system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3833</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3833</id><created>2013-01-16</created><authors><author><keyname>Andrieu</keyname><forenames>Christophe</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author><author><keyname>Doucet</keyname><forenames>Arnaud</forenames></author></authors><title>Reversible Jump MCMC Simulated Annealing for Neural Networks</title><categories>cs.LG cs.NE stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-11-18</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel reversible jump Markov chain Monte Carlo (MCMC) simulated
annealing algorithm to optimize radial basis function (RBF) networks. This
algorithm enables us to maximize the joint posterior distribution of the
network parameters and the number of basis functions. It performs a global
search in the joint space of the parameters and number of parameters, thereby
surmounting the problem of local minima. We also show that by calibrating a
Bayesian model, we can obtain the classical AIC, BIC and MDL model selection
criteria within a penalized likelihood framework. Finally, we show
theoretically and empirically that the algorithm converges to the modes of the
full posterior distribution in an efficient way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3834</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3834</id><created>2013-01-16</created><authors><author><keyname>Becker</keyname><forenames>Ann</forenames></author><author><keyname>Geiger</keyname><forenames>Dan</forenames></author><author><keyname>Meek</keyname><forenames>Christopher</forenames></author></authors><title>Perfect Tree-Like Markovian Distributions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-19-23</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that if a strictly positive joint probability distribution for a set
of binary random variables factors according to a tree, then vertex separation
represents all and only the independence relations enclosed in the
distribution. The same result is shown to hold also for multivariate strictly
positive normal distributions. Our proof uses a new property of conditional
independence that holds for these two classes of probability distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3835</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3835</id><created>2013-01-16</created><authors><author><keyname>Benferhat</keyname><forenames>Salem</forenames></author><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Kaci</keyname><forenames>Souhila</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>A Principled Analysis of Merging Operations in Possibilistic Logic</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-24-31</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Possibilistic logic offers a qualitative framework for representing pieces of
information associated with levels of uncertainty of priority. The fusion of
multiple sources information is discussed in this setting. Different classes of
merging operators are considered including conjunctive, disjunctive,
reinforcement, adaptive and averaging operators. Then we propose to analyse
these classes in terms of postulates. This is done by first extending the
postulate for merging classical bases to the case where priorites are avaialbe.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3836</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3836</id><created>2013-01-16</created><authors><author><keyname>Bernstein</keyname><forenames>Daniel S</forenames></author><author><keyname>Zilberstein</keyname><forenames>Shlomo</forenames></author><author><keyname>Immerman</keyname><forenames>Neil</forenames></author></authors><title>The Complexity of Decentralized Control of Markov Decision Processes</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-32-37</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Planning for distributed agents with partial state information is considered
from a decision- theoretic perspective. We describe generalizations of both the
MDP and POMDP models that allow for decentralized control. For even a small
number of agents, the finite-horizon problems corresponding to both of our
models are complete for nondeterministic exponential time. These complexity
results illustrate a fundamental difference between centralized and
decentralized control of Markov processes. In contrast to the MDP and POMDP
problems, the problems we consider provably do not admit polynomial-time
algorithms and most likely require doubly exponential time to solve in the
worst case. We have thus provided mathematical evidence corresponding to the
intuition that decentralized planning problems cannot easily be reduced to
centralized problems and solved exactly using established techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3837</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3837</id><created>2013-01-16</created><authors><author><keyname>Bilmes</keyname><forenames>Jeff A.</forenames></author></authors><title>Dynamic Bayesian Multinets</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-38-45</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, dynamic Bayesian multinets are introduced where a Markov chain
state at time t determines conditional independence patterns between random
variables lying within a local time window surrounding t. It is shown how
information-theoretic criterion functions can be used to induce sparse,
discriminative, and class-conditional network structures that yield an optimal
approximation to the class posterior probability, and therefore are useful for
the classification task. Using a new structure learning heuristic, the
resulting models are tested on a medium-vocabulary isolated-word speech
recognition task. It is demonstrated that these discriminatively structured
dynamic Bayesian multinets, when trained in a maximum likelihood setting using
EM, can outperform both HMMs and other dynamic Bayesian networks with a similar
number of parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3838</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3838</id><created>2013-01-16</created><authors><author><keyname>Bishop</keyname><forenames>Christopher M.</forenames></author><author><keyname>Tipping</keyname><forenames>Michael</forenames></author></authors><title>Variational Relevance Vector Machines</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-46-53</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Support Vector Machine (SVM) of Vapnik (1998) has become widely
established as one of the leading approaches to pattern recognition and machine
learning. It expresses predictions in terms of a linear combination of kernel
functions centred on a subset of the training data, known as support vectors.
  Despite its widespread success, the SVM suffers from some important
limitations, one of the most significant being that it makes point predictions
rather than generating predictive distributions. Recently Tipping (1999) has
formulated the Relevance Vector Machine (RVM), a probabilistic model whose
functional form is equivalent to the SVM. It achieves comparable recognition
accuracy to the SVM, yet provides a full predictive distribution, and also
requires substantially fewer kernel functions.
  The original treatment of the RVM relied on the use of type II maximum
likelihood (the `evidence framework') to provide point estimates of the
hyperparameters which govern model sparsity. In this paper we show how the RVM
can be formulated and solved within a completely Bayesian paradigm through the
use of variational inference, thereby giving a posterior distribution over both
parameters and hyperparameters. We demonstrate the practicality and performance
of the variational RVM using both synthetic and real world examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3839</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3839</id><created>2013-01-16</created><authors><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author></authors><title>Approximately Optimal Monitoring of Plan Preconditions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-54-62</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monitoring plan preconditions can allow for replanning when a precondition
fails, generally far in advance of the point in the plan where the precondition
is relevant. However, monitoring is generally costly, and some precondition
failures have a very small impact on plan quality. We formulate a model for
optimal precondition monitoring, using partially-observable Markov decisions
processes, and describe methods for solving this model efficitively, though
approximately. Specifically, we show that the single-precondition monitoring
problem is generally tractable, and the multiple-precondition monitoring
policies can be efficitively approximated using single-precondition soultions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3840</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3840</id><created>2013-01-16</created><authors><author><keyname>Chajewska</keyname><forenames>Urszula</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Utilities as Random Variables: Density Estimation and Structure
  Discovery</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-63-71</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision theory does not traditionally include uncertainty over utility
functions. We argue that the a person's utility value for a given outcome can
be treated as we treat other domain attributes: as a random variable with a
density function over its possible values. We show that we can apply
statistical density estimation techniques to learn such a density function from
a database of partially elicited utility functions. In particular, we define a
Bayesian learning framework for this problem, assuming the distribution over
utilities is a mixture of Gaussians, where the mixture components represent
statistically coherent subpopulations. We can also extend our techniques to the
problem of discovering generalized additivity structure in the utility
functions in the population. We define a Bayesian model selection criterion for
utility function structure and a search procedure over structures. The
factorization of the utilities in the learned model, and the generalization
obtained from density estimation, allows us to provide robust estimates of
utilities using a significantly smaller number of utility elicitation
questions. We experiment with our technique on synthetic utility data and on a
real database of utility functions in the domain of prenatal diagnosis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3841</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3841</id><created>2013-01-16</created><authors><author><keyname>Cheng</keyname><forenames>Jian</forenames></author><author><keyname>Druzdzel</keyname><forenames>Marek J.</forenames></author></authors><title>Computational Investigation of Low-Discrepancy Sequences in Simulation
  Algorithms for Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-72-81</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monte Carlo sampling has become a major vehicle for approximate inference in
Bayesian networks. In this paper, we investigate a family of related simulation
approaches, known collectively as quasi-Monte Carlo methods based on
deterministic low-discrepancy sequences. We first outline several theoretical
aspects of deterministic low-discrepancy sequences, show three examples of such
sequences, and then discuss practical issues related to applying them to belief
updating in Bayesian networks. We propose an algorithm for selecting direction
numbers for Sobol sequence. Our experimental results show that low-discrepancy
sequences (especially Sobol sequence) significantly improve the performance of
simulation algorithms in Bayesian networks compared to Monte Carlo sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3842</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3842</id><created>2013-01-16</created><authors><author><keyname>Chickering</keyname><forenames>David Maxwell</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>A Decision Theoretic Approach to Targeted Advertising</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-82-88</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple advertising strategy that can be used to help increase sales of a
product is to mail out special offers to selected potential customers. Because
there is a cost associated with sending each offer, the optimal mailing
strategy depends on both the benefit obtained from a purchase and how the offer
affects the buying behavior of the customers. In this paper, we describe two
methods for partitioning the potential customers into groups, and show how to
perform a simple cost-benefit analysis to decide which, if any, of the groups
should be targeted. In particular, we consider two decision-tree learning
algorithms. The first is an &quot;off the shelf&quot; algorithm used to model the
probability that groups of customers will buy the product. The second is a new
algorithm that is similar to the first, except that for each group, it
explicitly models the probability of purchase under the two mailing scenarios:
(1) the mail is sent to members of that group and (2) the mail is not sent to
members of that group. Using data from a real-world advertising experiment, we
compare the algorithms to each other and to a naive mail-to-all strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3843</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3843</id><created>2013-01-16</created><authors><author><keyname>Coetzee</keyname><forenames>Frans</forenames></author><author><keyname>Lawrence</keyname><forenames>Steve</forenames></author><author><keyname>Giles</keyname><forenames>C. Lee</forenames></author></authors><title>Bayesian Classification and Feature Selection from Finite Data Sets</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-89-97</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection aims to select the smallest subset of features for a
specified level of performance. The optimal achievable classification
performance on a feature subset is summarized by its Receiver Operating Curve
(ROC). When infinite data is available, the Neyman- Pearson (NP) design
procedure provides the most efficient way of obtaining this curve. In practice
the design procedure is applied to density estimates from finite data sets. We
perform a detailed statistical analysis of the resulting error propagation on
finite alphabets. We show that the estimated performance curve (EPC) produced
by the design procedure is arbitrarily accurate given sufficient data,
independent of the size of the feature set. However, the underlying likelihood
ranking procedure is highly sensitive to errors that reduces the probability
that the EPC is in fact the ROC. In the worst case, guaranteeing that the EPC
is equal to the ROC may require data sizes exponential in the size of the
feature set. These results imply that in theory the NP design approach may only
be valid for characterizing relatively small feature subsets, even when the
performance of any given classifier can be estimated very accurately. We
discuss the practical limitations for on-line methods that ensures that the NP
procedure operates in a statistically valid region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3844</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3844</id><created>2013-01-16</created><authors><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author></authors><title>A Bayesian Method for Causal Modeling and Discovery Under Selection</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-98-106</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a Bayesian method for learning causal networks using
samples that were selected in a non-random manner from a population of
interest. Examples of data obtained by non-random sampling include convenience
samples and case-control data in which a fixed number of samples with and
without some condition is collected; such data are not uncommon. The paper
describes a method for combining data under selection with prior beliefs in
order to derive a posterior probability for a model of the causal processes
that are generating the data in the population of interest. The priors include
beliefs about the nature of the non-random sampling procedure. Although exact
application of the method would be computationally intractable for most
realistic datasets, efficient special-case and approximation methods are
discussed. Finally, the paper describes how to combine learning under selection
with previous methods for learning from observational and experimental data
that are obtained on random samples of the population of interest. The net
result is a Bayesian methodology that supports causal modeling and discovery
from a rich mixture of different types of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3845</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3845</id><created>2013-01-16</created><authors><author><keyname>Cozman</keyname><forenames>Fabio Gagliardi</forenames></author></authors><title>Separation Properties of Sets of Probability Measures</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-107-114</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes independence concepts for sets of probability measures
associated with directed acyclic graphs. The paper shows that epistemic
independence and the standard Markov condition violate desirable separation
properties. The adoption of a contraction condition leads to d-separation but
still fails to guarantee a belief separation property. To overcome this
unsatisfactory situation, a strong Markov condition is proposed, based on
epistemic independence. The main result is that the strong Markov condition
leads to strong independence and does enforce separation properties; this
result implies that (1) separation properties of Bayesian networks do extend to
epistemic independence and sets of probability measures, and (2) strong
independence has a clear justification based on epistemic independence and the
strong Markov condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3846</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3846</id><created>2013-01-16</created><authors><author><keyname>Cussens</keyname><forenames>James</forenames></author></authors><title>Stochastic Logic Programs: Sampling, Inference and Applications</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-115-122</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms for exact and approximate inference in stochastic logic programs
(SLPs) are presented, based respectively, on variable elimination and
importance sampling. We then show how SLPs can be used to represent prior
distributions for machine learning, using (i) logic programs and (ii) Bayes net
structures as examples. Drawing on existing work in statistics, we apply the
Metropolis-Hasting algorithm to construct a Markov chain which samples from the
posterior distribution. A Prolog implementation for this is described. We also
discuss the possibility of constructing explicit representations of the
posterior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3847</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3847</id><created>2013-01-16</created><authors><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author></authors><title>A Differential Approach to Inference in Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-123-132</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach for inference in Bayesian networks, which is mainly
based on partial differentiation. According to this approach, one compiles a
Bayesian network into a multivariate polynomial and then computes the partial
derivatives of this polynomial with respect to each variable. We show that once
such derivatives are made available, one can compute in constant-time answers
to a large class of probabilistic queries, which are central to classical
inference, parameter estimation, model validation and sensitivity analysis. We
present a number of complexity results relating to the compilation of such
polynomials and to the computation of their partial derivatives. We argue that
the combined simplicity, comprehensiveness and computational complexity of the
presented framework is unique among existing frameworks for inference in
Bayesian networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3848</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3848</id><created>2013-01-16</created><authors><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author></authors><title>Any-Space Probabilistic Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-133-142</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have recently introduced an any-space algorithm for exact inference in
Bayesian networks, called Recursive Conditioning, RC, which allows one to trade
space with time at increments of X-bytes, where X is the number of bytes needed
to cache a floating point number. In this paper, we present three key
extensions of RC. First, we modify the algorithm so it applies to more general
factorization of probability distributions, including (but not limited to)
Bayesian network factorizations. Second, we present a forgetting mechanism
which reduces the space requirements of RC considerably and then compare such
requirmenets with those of variable elimination on a number of realistic
networks, showing orders of magnitude improvements in certain cases. Third, we
present a version of RC for computing maximum a posteriori hypotheses (MAP),
which turns out to be the first MAP algorithm allowing a smooth time-space
tradeoff. A key advantage of presented MAP algorithm is that it does not have
to start from scratch each time a new query is presented, but can reuse some of
its computations across multiple queries, leading to significant savings in
ceratain cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3849</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3849</id><created>2013-01-16</created><authors><author><keyname>Dasgupta</keyname><forenames>Sanjoy</forenames></author></authors><title>Experiments with Random Projection</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-143-151</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent theoretical work has identified random projection as a promising
dimensionality reduction technique for learning mixtures of Gausians. Here we
summarize these results and illustrate them by a wide variety of experiments on
synthetic and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3850</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3850</id><created>2013-01-16</created><authors><author><keyname>Dasgupta</keyname><forenames>Sanjoy</forenames></author><author><keyname>Schulman</keyname><forenames>Leonard</forenames></author></authors><title>A Two-round Variant of EM for Gaussian Mixtures</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-152-159</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of possible models (e.g., Bayesian network structures) and a data
sample, in the unsupervised model selection problem the task is to choose the
most accurate model with respect to the domain joint probability distribution.
In contrast to this, in supervised model selection it is a priori known that
the chosen model will be used in the future for prediction tasks involving more
``focused' predictive distributions. Although focused predictive distributions
can be produced from the joint probability distribution by marginalization, in
practice the best model in the unsupervised sense does not necessarily perform
well in supervised domains. In particular, the standard marginal likelihood
score is a criterion for the unsupervised task, and, although frequently used
for supervised model selection also, does not perform well in such tasks. In
this paper we study the performance of the marginal likelihood score
empirically in supervised Bayesian network selection tasks by using a large
number of publicly available classification data sets, and compare the results
to those obtained by alternative model selection criteria, including empirical
crossvalidation methods, an approximation of a supervised marginal likelihood
measure, and a supervised version of Dawids prequential(predictive sequential)
principle.The results demonstrate that the marginal likelihood score does NOT
perform well FOR supervised model selection, WHILE the best results are
obtained BY using Dawids prequential r napproach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3851</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3851</id><created>2013-01-16</created><authors><author><keyname>Davidson</keyname><forenames>Ian</forenames></author></authors><title>Minimum Message Length Clustering Using Gibbs Sampling</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-160-167</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The K-Mean and EM algorithms are popular in clustering and mixture modeling,
due to their simplicity and ease of implementation. However, they have several
significant limitations. Both coverage to a local optimum of their respective
objective functions (ignoring the uncertainty in the model space), require the
apriori specification of the number of classes/clsuters, and are inconsistent.
In this work we overcome these limitations by using the Minimum Message Length
(MML) principle and a variation to the K-Means/EM observation assignment and
parameter calculation scheme. We maintain the simplicity of these approaches
while constructing a Bayesian mixture modeling tool that samples/searches the
model space using a Markov Chain Monte Carlo (MCMC) sampler known as a Gibbs
sampler. Gibbs sampling allows us to visit each model according to its
posterior probability. Therefore, if the model space is multi-modal we will
visit all models and not get stuck in local optima. We call our approach
multiple chains at equilibrium (MCE) MML sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3852</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3852</id><created>2013-01-16</created><authors><author><keyname>Davies</keyname><forenames>Scott</forenames></author><author><keyname>Moore</keyname><forenames>Andrew</forenames></author></authors><title>Mix-nets: Factored Mixtures of Gaussians in Bayesian Networks With Mixed
  Continuous And Discrete Variables</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-168-175</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently developed techniques have made it possible to quickly learn accurate
probability density functions from data in low-dimensional continuous space. In
particular, mixtures of Gaussians can be fitted to data very quickly using an
accelerated EM algorithm that employs multiresolution kd-trees (Moore, 1999).
In this paper, we propose a kind of Bayesian networks in which low-dimensional
mixtures of Gaussians over different subsets of the domain's variables are
combined into a coherent joint probability model over the entire domain. The
network is also capable of modeling complex dependencies between discrete
variables and continuous variables without requiring discretization of the
continuous variables. We present efficient heuristic algorithms for
automatically learning these networks from data, and perform comparative
experiments illustrated how well these networks model real scientific data and
synthetic data. We also briefly discuss some possible improvements to the
networks, as well as possible applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3853</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3853</id><created>2013-01-16</created><authors><author><keyname>Doucet</keyname><forenames>Arnaud</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author><author><keyname>Russell</keyname><forenames>Stuart</forenames></author></authors><title>Rao-Blackwellised Particle Filtering for Dynamic Bayesian Networks</title><categories>cs.LG cs.AI stat.CO</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-176-183</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Particle filters (PFs) are powerful sampling-based inference/learning
algorithms for dynamic Bayesian networks (DBNs). They allow us to treat, in a
principled way, any type of probability distribution, nonlinearity and
non-stationarity. They have appeared in several fields under such names as
&quot;condensation&quot;, &quot;sequential Monte Carlo&quot; and &quot;survival of the fittest&quot;. In this
paper, we show how we can exploit the structure of the DBN to increase the
efficiency of particle filtering, using a technique known as
Rao-Blackwellisation. Essentially, this samples some of the variables, and
marginalizes out the rest exactly, using the Kalman filter, HMM filter,
junction tree algorithm, or any other finite dimensional optimal filter. We
show that Rao-Blackwellised particle filters (RBPFs) lead to more accurate
estimates than standard PFs. We demonstrate RBPFs on two problems, namely
non-stationary online regression with radial basis function networks and robot
localization and map building. We also discuss other potential application
areas and provide references to some finite dimensional optimal filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3854</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3854</id><created>2013-01-16</created><authors><author><keyname>Frey</keyname><forenames>Brendan J.</forenames></author><author><keyname>Jojic</keyname><forenames>Nebojsa</forenames></author></authors><title>Learning Graphical Models of Images, Videos and Their Spatial
  Transformations</title><categories>cs.CV cs.LG stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-184-191</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixtures of Gaussians, factor analyzers (probabilistic PCA) and hidden Markov
models are staples of static and dynamic data modeling and image and video
modeling in particular. We show how topographic transformations in the input,
such as translation and shearing in images, can be accounted for in these
models by including a discrete transformation variable. The resulting models
perform clustering, dimensionality reduction and time-series analysis in a way
that is invariant to transformations in the input. Using the EM algorithm,
these transformation-invariant models can be fit to static data and time
series. We give results on filtering microscopy images, face and facial pose
clustering, handwritten digit modeling and recognition, video clustering,
object tracking, and removal of distractions from video sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3855</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3855</id><created>2013-01-16</created><authors><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Geiger</keyname><forenames>Dan</forenames></author><author><keyname>Lotner</keyname><forenames>Noam</forenames></author></authors><title>Likelihood Computations Using Value Abstractions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-192-200</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we use evidence-specific value abstraction for speeding
Bayesian networks inference. This is done by grouping variable values and
treating the combined values as a single entity. As we show, such abstractions
can exploit regularities in conditional probability distributions and also the
specific values of observed variables. To formally justify value abstraction,
we define the notion of safe value abstraction and devise inference algorithms
that use it to reduce the cost of inference. Our procedure is particularly
useful for learning complex networks with many hidden variables. In such cases,
repeated likelihood computations are required for EM or other parameter
optimization techniques. Since these computations are repeated with respect to
the same evidence set, our methods can provide significant speedup to the
learning procedure. We demonstrate the algorithm on genetic linkage problems
where the use of value abstraction sometimes differentiates between a feasible
and non-feasible solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3856</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3856</id><created>2013-01-16</created><authors><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Being Bayesian about Network Structure</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-201-210</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many domains, we are interested in analyzing the structure of the
underlying distribution, e.g., whether one variable is a direct parent of the
other. Bayesian model-selection attempts to find the MAP model and use its
structure to answer these questions. However, when the amount of available data
is modest, there might be many models that have non-negligible posterior. Thus,
we want compute the Bayesian posterior of a feature, i.e., the total posterior
probability of all models that contain it. In this paper, we propose a new
approach for this task. We first show how to efficiently compute a sum over the
exponential number of networks that are consistent with a fixed ordering over
network variables. This allows us to compute, for a given ordering, both the
marginal probability of the data and the posterior of a feature. We then use
this result as the basis for an algorithm that approximates the Bayesian
posterior of a feature. Our approach uses a Markov Chain Monte Carlo (MCMC)
method, but over orderings rather than over network structures. The space of
orderings is much smaller and more regular than the space of structures, and
has a smoother posterior `landscape'. We present empirical results on synthetic
and real-life datasets that compare our approach to full model averaging (when
possible), to MCMC over network structures, and to a non-Bayesian bootstrap
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3857</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3857</id><created>2013-01-16</created><authors><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Nachman</keyname><forenames>Iftach</forenames></author></authors><title>Gaussian Process Networks</title><categories>cs.AI cs.LG stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-211-219</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of learning the structure of a Bayesian
network in domains with continuous variables. This task requires a procedure
for comparing different candidate structures. In the Bayesian framework, this
is done by evaluating the {em marginal likelihood/} of the data given a
candidate structure. This term can be computed in closed-form for standard
parametric families (e.g., Gaussians), and can be approximated, at some
computational cost, for some semi-parametric families (e.g., mixtures of
Gaussians).
  We present a new family of continuous variable probabilistic networks that
are based on {em Gaussian Process/} priors. These priors are semi-parametric in
nature and can learn almost arbitrary noisy functional relations. Using these
priors, we can directly compute marginal likelihoods for structure learning.
The resulting method can discover a wide range of functional dependencies in
multivariate data. We develop the Bayesian score of Gaussian Process Networks
and describe how to learn them from data. We present empirical results on
artificial data as well as on real-life domains with non-linear dependencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3858</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3858</id><created>2013-01-16</created><authors><author><keyname>Giang</keyname><forenames>Phan H.</forenames></author><author><keyname>Shenoy</keyname><forenames>Prakash P.</forenames></author></authors><title>A Qualitative Linear Utility Theory for Spohn's Theory of Epistemic
  Beliefs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-220-229</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we formulate a qualitative &quot;linear&quot; utility theory for
lotteries in which uncertainty is expressed qualitatively using a Spohnian
disbelief function. We argue that a rational decision maker facing an uncertain
decision problem in which the uncertainty is expressed qualitatively should
behave so as to maximize &quot;qualitative expected utility.&quot; Our axiomatization of
the qualitative utility is similar to the axiomatization developed by von
Neumann and Morgenstern for probabilistic lotteries. We compare our results
with other recent results in qualitative decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3859</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3859</id><created>2013-01-16</created><authors><author><keyname>Gorniak</keyname><forenames>Peter J.</forenames></author><author><keyname>Poole</keyname><forenames>David L.</forenames></author></authors><title>Building a Stochastic Dynamic Model of Application Use</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-230-237</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many intelligent user interfaces employ application and user models to
determine the user's preferences, goals and likely future actions. Such models
require application analysis, adaptation and expansion. Building and
maintaining such models adds a substantial amount of time and labour to the
application development cycle. We present a system that observes the interface
of an unmodified application and records users' interactions with the
application. From a history of such observations we build a coarse state space
of observed interface states and actions between them. To refine the space, we
hypothesize sub-states based upon the histories that led users to a given
state. We evaluate the information gain of possible state splits, varying the
length of the histories considered in such splits. In this way, we
automatically produce a stochastic dynamic model of the application and of how
it is used. To evaluate our approach, we present models derived from real-world
application usage data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3860</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3860</id><created>2013-01-16</created><authors><author><keyname>Grunwald</keyname><forenames>Peter D.</forenames></author></authors><title>Maximum Entropy and the Glasses You Are Looking Through</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-238-246</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an interpretation of the Maximum Entropy (MaxEnt) Principle in
game-theoretic terms. Based on this interpretation, we make a formal
distinction between different ways of {em applying/} Maximum Entropy
distributions. MaxEnt has frequently been criticized on the grounds that it
leads to highly representation dependent results. Our distinction allows us to
avoid this problem in many cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3861</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3861</id><created>2013-01-16</created><authors><author><keyname>Harvey</keyname><forenames>Michael</forenames></author><author><keyname>Neal</keyname><forenames>Radford M.</forenames></author></authors><title>Inference for Belief Networks Using Coupling From the Past</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-256-263</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inference for belief networks using Gibbs sampling produces a distribution
for unobserved variables that differs from the correct distribution by a
(usually) unknown error, since convergence to the right distribution occurs
only asymptotically. The method of &quot;coupling from the past&quot; samples from
exactly the correct distribution by (conceptually) running dependent Gibbs
sampling simulations from every possible starting state from a time far enough
in the past that all runs reach the same state at time t=0. Explicitly
considering every possible state is intractable for large networks, however. We
propose a method for layered noisy-or networks that uses a compact, but often
imprecise, summary of a set of states. This method samples from exactly the
correct distribution, and requires only about twice the time per step as
ordinary Gibbs sampling, but it may require more simulation steps than would be
needed if chains were tracked exactly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3862</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3862</id><created>2013-01-16</created><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Chickering</keyname><forenames>David Maxwell</forenames></author><author><keyname>Meek</keyname><forenames>Christopher</forenames></author><author><keyname>Rounthwaite</keyname><forenames>Robert</forenames></author><author><keyname>Kadie</keyname><forenames>Carl</forenames></author></authors><title>Dependency Networks for Collaborative Filtering and Data Visualization</title><categories>cs.AI cs.IR cs.LG</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-264-273</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a graphical model for probabilistic relationships---an
alternative to the Bayesian network---called a dependency network. The graph of
a dependency network, unlike a Bayesian network, is potentially cyclic. The
probability component of a dependency network, like a Bayesian network, is a
set of conditional distributions, one for each node given its parents. We
identify several basic properties of this representation and describe a
computationally efficient procedure for learning the graph and probability
components from data. We describe the application of this representation to
probabilistic inference, collaborative filtering (the task of predicting
preferences), and the visualization of acausal predictive relationships.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3863</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3863</id><created>2013-01-16</created><authors><author><keyname>Hojsgaard</keyname><forenames>Soren</forenames></author></authors><title>YGGDRASIL - A Statistical Package for Learning Split Models</title><categories>cs.AI cs.MS stat.ME</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-274-281</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are two main objectives of this paper. The first is to present a
statistical framework for models with context specific independence structures,
i.e., conditional independences holding only for sepcific values of the
conditioning variables. This framework is constituted by the class of split
models. Split models are extension of graphical models for contigency tables
and allow for a more sophisticiated modelling than graphical models. The
treatment of split models include estimation, representation and a Markov
property for reading off those independencies holding in a specific context.
The second objective is to present a software package named YGGDRASIL which is
designed for statistical inference in split models, i.e., for learning such
models on the basis of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3864</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3864</id><created>2013-01-16</created><authors><author><keyname>Horsch</keyname><forenames>Michael C.</forenames></author><author><keyname>Havens</keyname><forenames>Bill</forenames></author></authors><title>Probabilistic Arc Consistency: A Connection between Constraint Reasoning
  and Probabilistic Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-282-290</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We document a connection between constraint reasoning and probabilistic
reasoning. We present an algorithm, called {em probabilistic arc consistency},
which is both a generalization of a well known algorithm for arc consistency
used in constraint reasoning, and a specialization of the belief updating
algorithm for singly-connected networks. Our algorithm is exact for singly-
connected constraint problems, but can work well as an approximation for
arbitrary problems. We briefly discuss some empirical results, and related
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3865</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3865</id><created>2013-01-16</created><authors><author><keyname>Jebara</keyname><forenames>Tony S.</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi S.</forenames></author></authors><title>Feature Selection and Dualities in Maximum Entropy Discrimination</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-291-300</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incorporating feature selection into a classification or regression method
often carries a number of advantages. In this paper we formalize feature
selection specifically from a discriminative perspective of improving
classification/regression accuracy. The feature selection method is developed
as an extension to the recently proposed maximum entropy discrimination (MED)
framework. We describe MED as a flexible (Bayesian) regularization approach
that subsumes, e.g., support vector classification, regression and exponential
family models. For brevity, we restrict ourselves primarily to feature
selection in the context of linear classification/regression methods and
demonstrate that the proposed approach indeed carries substantial improvements
in practice. Moreover, we discuss and develop various extensions of feature
selection, including the problem of dealing with example specific but
unobserved degrees of freedom -- alignments or invariants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3866</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3866</id><created>2013-01-16</created><authors><author><keyname>Jirousek</keyname><forenames>Radim</forenames></author></authors><title>Marginalization in Composed Probabilistic Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-301-308</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Composition of low-dimensional distributions, whose foundations were laid in
the papaer published in the Proceeding of UAI'97 (Jirousek 1997), appeared to
be an alternative apparatus to describe multidimensional probabilistic models.
In contrast to Graphical Markov Models, which define multidomensinoal
distributions in a declarative way, this approach is rather procedural.
Ordering of low-dimensional distributions into a proper sequence fully defines
the resepctive computational procedure; therefore, a stury of different type of
generating sequences is one fo the central problems in this field. Thus, it
appears that an important role is played by special sequences that are called
perfect. Their main characterization theorems are presetned in this paper.
However, the main result of this paper is a solution to the problem of
margnialization for general sequences. The main theorem describes a way to
obtain a generating sequence that defines the model corresponding to the
marginal of the distribution defined by an arbitrary genearting sequence. From
this theorem the reader can see to what extent these comutations are local;
i.e., the sequence consists of marginal distributions whose computation must be
made by summing up over the values of the variable eliminated (the paper deals
with finite model).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3867</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3867</id><created>2013-01-16</created><authors><author><keyname>Kearns</keyname><forenames>Michael</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Singh</keyname><forenames>Satinder</forenames></author></authors><title>Fast Planning in Stochastic Games</title><categories>cs.GT cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-309-316</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic games generalize Markov decision processes (MDPs) to a multiagent
setting by allowing the state transitions to depend jointly on all player
actions, and having rewards determined by multiplayer matrix games at each
state. We consider the problem of computing Nash equilibria in stochastic
games, the analogue of planning in MDPs. We begin by providing a generalization
of finite-horizon value iteration that computes a Nash strategy for each player
in generalsum stochastic games. The algorithm takes an arbitrary Nash selection
function as input, which allows the translation of local choices between
multiple Nash equilibria into the selection of a single global Nash
equilibrium.
  Our main technical result is an algorithm for computing near-Nash equilibria
in large or infinite state spaces. This algorithm builds on our finite-horizon
value iteration algorithm, and adapts the sparse sampling methods of Kearns,
Mansour and Ng (1999) to stochastic games. We conclude by descrbing a
counterexample showing that infinite-horizon discounted value iteration, which
was shown by shaplely to converge in the zero-sum case (a result we give extend
slightly here), does not converge in the general-sum case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3868</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3868</id><created>2013-01-16</created><authors><author><keyname>Kj&#xe6;rulff</keyname><forenames>Uffe</forenames></author><author><keyname>van der Gaag</keyname><forenames>Linda C.</forenames></author></authors><title>Making Sensitivity Analysis Computationally Efficient</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-317-325</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To investigate the robustness of the output probabilities of a Bayesian
network, a sensitivity analysis can be performed. A one-way sensitivity
analysis establishes, for each of the probability parameters of a network, a
function expressing a posterior marginal probability of interest in terms of
the parameter. Current methods for computing the coefficients in such a
function rely on a large number of network evaluations. In this paper, we
present a method that requires just a single outward propagation in a junction
tree for establishing the coefficients in the functions for all possible
parameters; in addition, an inward propagation is required for processing
evidence. Conversely, the method requires a single outward propagation for
computing the coefficients in the functions expressing all possible posterior
marginals in terms of a single parameter. We extend these results to an n-way
sensitivity analysis in which sets of parameters are studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3869</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3869</id><created>2013-01-16</created><authors><author><keyname>Koller</keyname><forenames>Daphne</forenames></author><author><keyname>Parr</keyname><forenames>Ron</forenames></author></authors><title>Policy Iteration for Factored MDPs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-326-334</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many large MDPs can be represented compactly using a dynamic Bayesian
network. Although the structure of the value function does not retain the
structure of the process, recent work has shown that value functions in
factored MDPs can often be approximated well using a decomposed value function:
a linear combination of &lt;I&gt;restricted&lt;/I&gt; basis functions, each of which refers
only to a small subset of variables. An approximate value function for a
particular policy can be computed using approximate dynamic programming, but
this approach (and others) can only produce an approximation relative to a
distance metric which is weighted by the stationary distribution of the current
policy. This type of weighted projection is ill-suited to policy improvement.
We present a new approach to value determination, that uses a simple
closed-form computation to directly compute a least-squares decomposed
approximation to the value function &lt;I&gt;for any weights&lt;/I&gt;. We then use this
value determination algorithm as a subroutine in a policy iteration process. We
show that, under reasonable restrictions, the policies induced by a factored
value function are compactly represented, and can be manipulated efficiently in
a policy iteration process. We also present a method for computing error bounds
for decomposed value functions using a variable-elimination algorithm for
function optimization. The complexity of all of our algorithms depends on the
factorization of system dynamics and of the approximate value function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3870</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3870</id><created>2013-01-16</created><authors><author><keyname>La Mura</keyname><forenames>Pierfrancesco</forenames></author></authors><title>Game Networks</title><categories>cs.GT cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-335-342</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Game networks (G nets), a novel representation for multi-agent
decision problems. Compared to other game-theoretic representations, such as
strategic or extensive forms, G nets are more structured and more compact; more
fundamentally, G nets constitute a computationally advantageous framework for
strategic inference, as both probability and utility independencies are
captured in the structure of the network and can be exploited in order to
simplify the inference process. An important aspect of multi-agent reasoning is
the identification of some or all of the strategic equilibria in a game; we
present original convergence methods for strategic equilibrium which can take
advantage of strategic separabilities in the G net structure in order to
simplify the computations. Specifically, we describe a method which identifies
a unique equilibrium as a function of the game payoffs, and one which
identifies all equilibria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3871</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3871</id><created>2013-01-16</created><authors><author><keyname>Larra&#xf1;aga</keyname><forenames>Pedro</forenames></author><author><keyname>Etxeberria</keyname><forenames>Ramon</forenames></author><author><keyname>Lozano</keyname><forenames>Jose A.</forenames></author><author><keyname>Pena</keyname><forenames>Jose M.</forenames></author></authors><title>Combinatorial Optimization by Learning and Simulation of Bayesian
  Networks</title><categories>cs.AI cs.DS</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-343-352</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows how the Bayesian network paradigm can be used in order to
solve combinatorial optimization problems. To do it some methods of structure
learning from data and simulation of Bayesian networks are inserted inside
Estimation of Distribution Algorithms (EDA). EDA are a new tool for
evolutionary computation in which populations of individuals are created by
estimation and simulation of the joint probability distribution of the selected
individuals. We propose new approaches to EDA for combinatorial optimization
based on the theory of probabilistic graphical models. Experimental results are
also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3872</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3872</id><created>2013-01-16</created><authors><author><keyname>Lu</keyname><forenames>Tsai-Ching</forenames></author><author><keyname>Druzdzel</keyname><forenames>Marek J.</forenames></author><author><keyname>Leong</keyname><forenames>Tze-Yun</forenames></author></authors><title>Causal Mechanism-based Model Construction</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-353-362</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a framework for building graphical causal model that is based on
the concept of causal mechanisms. Causal models are intuitive for human users
and, more importantly, support the prediction of the effect of manipulation. We
describe an implementation of the proposed framework as an interactive model
construction module, ImaGeNIe, in SMILE (Structural Modeling, Inference, and
Learning Engine) and in GeNIe (SMILE's Windows user interface).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3873</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3873</id><created>2013-01-16</created><authors><author><keyname>Lukasiewicz</keyname><forenames>Thomas</forenames></author></authors><title>Credal Networks under Maximum Entropy</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-363-370</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply the principle of maximum entropy to select a unique joint
probability distribution from the set of all joint probability distributions
specified by a credal network. In detail, we start by showing that the unique
joint distribution of a Bayesian tree coincides with the maximum entropy model
of its conditional distributions. This result, however, does not hold anymore
for general Bayesian networks. We thus present a new kind of maximum entropy
models, which are computed sequentially. We then show that for all general
Bayesian networks, the sequential maximum entropy model coincides with the
unique joint distribution. Moreover, we apply the new principle of sequential
maximum entropy to interval Bayesian networks and more generally to credal
networks. We especially show that this application is equivalent to a number of
small local entropy maximizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3874</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3874</id><created>2013-01-16</created><authors><author><keyname>McBurney</keyname><forenames>Peter</forenames></author><author><keyname>Parsons</keyname><forenames>Simon</forenames></author></authors><title>Risk Agoras: Dialectical Argumentation for Scientific Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-371-379</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a formal framework for intelligent systems which can reason about
scientific domains, in particular about the carcinogenicity of chemicals, and
we study its properties. Our framework is grounded in a philosophy of
scientific enquiry and discourse, and uses a model of dialectical
argumentation. The formalism enables representation of scientific uncertainty
and conflict in a manner suitable for qualitative reasoning about the domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3875</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3875</id><created>2013-01-16</created><authors><author><keyname>Meila</keyname><forenames>Marina</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi S.</forenames></author></authors><title>Tractable Bayesian Learning of Tree Belief Networks</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-380-388</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present decomposable priors, a family of priors over
structure and parameters of tree belief nets for which Bayesian learning with
complete observations is tractable, in the sense that the posterior is also
decomposable and can be completely determined analytically in polynomial time.
This follows from two main results: First, we show that factored distributions
over spanning trees in a graph can be integrated in closed form. Second, we
examine priors over tree parameters and show that a set of assumptions similar
to (Heckerman and al. 1995) constrain the tree parameter priors to be a
compactly parameterized product of Dirichlet distributions. Beside allowing for
exact Bayesian learning, these results permit us to formulate a new class of
tractable latent variable models in which the likelihood of a data point is
computed through an ensemble average over tree structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3876</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3876</id><created>2013-01-16</created><authors><author><keyname>Milch</keyname><forenames>Brian</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Probabilistic Models for Agents' Beliefs and Decisions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-389-396</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications of intelligent systems require reasoning about the mental
states of agents in the domain. We may want to reason about an agent's beliefs,
including beliefs about other agents; we may also want to reason about an
agent's preferences, and how his beliefs and preferences relate to his
behavior. We define a probabilistic epistemic logic (PEL) in which belief
statements are given a formal semantics, and provide an algorithm for asserting
and querying PEL formulas in Bayesian networks. We then show how to reason
about an agent's behavior by modeling his decision process as an influence
diagram and assuming that he behaves rationally. PEL can then be used for
reasoning from an agent's observed actions to conclusions about other aspects
of the domain, including unobserved domain variables and the agent's mental
states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3877</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3877</id><created>2013-01-16</created><authors><author><keyname>Moore</keyname><forenames>Andrew</forenames></author></authors><title>The Anchors Hierachy: Using the triangle inequality to survive high
  dimensional data</title><categories>cs.LG cs.DS stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-397-405</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is about metric data structures in high-dimensional or
non-Euclidean space that permit cached sufficient statistics accelerations of
learning algorithms.
  It has recently been shown that for less than about 10 dimensions, decorating
kd-trees with additional &quot;cached sufficient statistics&quot; such as first and
second moments and contingency tables can provide satisfying acceleration for a
very wide range of statistical learning tasks such as kernel regression,
locally weighted regression, k-means clustering, mixture modeling and Bayes Net
learning.
  In this paper, we begin by defining the anchors hierarchy - a fast data
structure and algorithm for localizing data based only on a
triangle-inequality-obeying distance metric. We show how this, in its own
right, gives a fast and effective clustering of data. But more importantly we
show how it can produce a well-balanced structure similar to a Ball-Tree
(Omohundro, 1991) or a kind of metric tree (Uhlmann, 1991; Ciaccia, Patella, &amp;
Zezula, 1997) in a way that is neither &quot;top-down&quot; nor &quot;bottom-up&quot; but instead
&quot;middle-out&quot;. We then show how this structure, decorated with cached sufficient
statistics, allows a wide variety of statistical learning algorithms to be
accelerated even in thousands of dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3878</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3878</id><created>2013-01-16</created><authors><author><keyname>Ng</keyname><forenames>Andrew Y.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>PEGASUS: A Policy Search Method for Large MDPs and POMDPs</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-406-415</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new approach to the problem of searching a space of policies for
a Markov decision process (MDP) or a partially observable Markov decision
process (POMDP), given a model. Our approach is based on the following
observation: Any (PO)MDP can be transformed into an &quot;equivalent&quot; POMDP in which
all state transitions (given the current state and action) are deterministic.
This reduces the general problem of policy search to one in which we need only
consider POMDPs with deterministic transitions. We give a natural way of
estimating the value of all policies in these transformed POMDPs. Policy search
is then simply performed by searching for a policy with high estimated value.
We also establish conditions under which our value estimates will be good,
recovering theoretical results similar to those of Kearns, Mansour and Ng
(1999), but with &quot;sample complexity&quot; bounds that have only a polynomial rather
than exponential dependence on the horizon time. Our method applies to
arbitrary POMDPs, including ones with infinite state and action spaces. We also
present empirical results for our approach on a small discrete problem, and on
a complex continuous state/continuous action problem involving learning to ride
a bicycle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3879</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3879</id><created>2013-01-16</created><authors><author><keyname>Nielsen</keyname><forenames>Thomas D.</forenames></author><author><keyname>Jensen</keyname><forenames>Finn Verner</forenames></author></authors><title>Representing and Solving Asymmetric Bayesian Decision Problems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-416-425</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the representation and solution of asymmetric Bayesian
decision problems. We present a formal framework, termed asymmetric influence
diagrams, that is based on the influence diagram and allows an efficient
representation of asymmetric decision problems. As opposed to existing
frameworks, the asymmetric influece diagram primarily encodes asymmetry at the
qualitative level and it can therefore be read directly from the model. We give
an algorithm for solving asymmetric influence diagrams. The algorithm initially
decomposes the asymmetric decision problem into a structure of symmetric
subproblems organized as a tree. A solution to the decision problem can then be
found by propagating from the leaves toward the root using existing evaluation
methods to solve the sub-problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3880</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3880</id><created>2013-01-16</created><authors><author><keyname>Nielsen</keyname><forenames>Thomas D.</forenames></author><author><keyname>Wuillemin</keyname><forenames>Pierre-Henri</forenames></author><author><keyname>Jensen</keyname><forenames>Finn Verner</forenames></author><author><keyname>Kj&#xe6;rulff</keyname><forenames>Uffe</forenames></author></authors><title>Using ROBDDs for Inference in Bayesian Networks with Troubleshooting as
  an Example</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-426-435</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When using Bayesian networks for modelling the behavior of man-made
machinery, it usually happens that a large part of the model is deterministic.
For such Bayesian networks deterministic part of the model can be represented
as a Boolean function, and a central part of belief updating reduces to the
task of calculating the number of satisfying configurations in a Boolean
function. In this paper we explore how advances in the calculation of Boolean
functions can be adopted for belief updating, in particular within the context
of troubleshooting. We present experimental results indicating a substantial
speed-up compared to traditional junction tree propagation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3881</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3881</id><created>2013-01-16</created><authors><author><keyname>Nilsson</keyname><forenames>Dennis</forenames></author><author><keyname>Lauritzen</keyname><forenames>Steffen L.</forenames></author></authors><title>Evaluating Influence Diagrams using LIMIDs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-436-445</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach to the solution of decision problems formulated as
influence diagrams. The approach converts the influence diagram into a simpler
structure, the LImited Memory Influence Diagram (LIMID), where only the
requisite information for the computation of optimal policies is depicted.
Because the requisite information is explicitly represented in the diagram, the
evaluation procedure can take advantage of it. In this paper we show how to
convert an influence diagram to a LIMID and describe the procedure for finding
an optimal strategy. Our approach can yield significant savings of memory and
computational time when compared to traditional methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3882</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3882</id><created>2013-01-16</created><authors><author><keyname>Ortiz</keyname><forenames>Luis E.</forenames></author><author><keyname>Kaelbling</keyname><forenames>Leslie Pack</forenames></author></authors><title>Adaptive Importance Sampling for Estimation in Structured Domains</title><categories>cs.AI cs.LG stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-446-454</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling is an important tool for estimating large, complex sums and
integrals over high dimensional spaces. For instance, important sampling has
been used as an alternative to exact methods for inference in belief networks.
Ideally, we want to have a sampling distribution that provides optimal-variance
estimators. In this paper, we present methods that improve the sampling
distribution by systematically adapting it as we obtain information from the
samples. We present a stochastic-gradient-descent method for sequentially
updating the sampling distribution based on the direct minization of the
variance. We also present other stochastic-gradient-descent methods based on
the minimizationof typical notions of distance between the current sampling
distribution and approximations of the target, optimal distribution. We finally
validate and compare the different methods empirically by applying them to the
problem of action evaluation in influence diagrams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3883</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3883</id><created>2013-01-16</created><authors><author><keyname>Paek</keyname><forenames>Tim</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author></authors><title>Conversation as Action Under Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-455-464</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conversations abound with uncetainties of various kinds. Treating
conversation as inference and decision making under uncertainty, we propose a
task independent, multimodal architecture for supporting robust continuous
spoken dialog called Quartet. We introduce four interdependent levels of
analysis, and describe representations, inference procedures, and decision
strategies for managing uncertainties within and between the levels. We
highlight the approach by reviewing interactions between a user and two spoken
dialog systems developed using the Quartet architecture: Prsenter, a prototype
system for navigating Microsoft PowerPoint presentations, and the Bayesian
Receptionist, a prototype system for dealing with tasks typically handled by
front desk receptionists at the Microsoft corporate campus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3884</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3884</id><created>2013-01-16</created><authors><author><keyname>Pavlov</keyname><forenames>Dmitry Y.</forenames></author><author><keyname>Mannila</keyname><forenames>Heikki</forenames></author><author><keyname>Smyth</keyname><forenames>Padhraic</forenames></author></authors><title>Probabilistic Models for Query Approximation with Large Sparse Binary
  Datasets</title><categories>cs.AI cs.DB</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-465-472</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large sparse sets of binary transaction data with millions of records and
thousands of attributes occur in various domains: customers purchasing
products, users visiting web pages, and documents containing words are just
three typical examples. Real-time query selectivity estimation (the problem of
estimating the number of rows in the data satisfying a given predicate) is an
important practical problem for such databases.
  We investigate the application of probabilistic models to this problem. In
particular, we study a Markov random field (MRF) approach based on frequent
sets and maximum entropy, and compare it to the independence model and the
Chow-Liu tree model. We find that the MRF model provides substantially more
accurate probability estimates than the other methods but is more expensive
from a computational and memory viewpoint. To alleviate the computational
requirements we show how one can apply bucket elimination and clique tree
approaches to take advantage of structure in the models and in the queries. We
provide experimental results on two large real-world transaction datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3885</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3885</id><created>2013-01-16</created><authors><author><keyname>Pennock</keyname><forenames>David M.</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author><author><keyname>Lawrence</keyname><forenames>Steve</forenames></author><author><keyname>Giles</keyname><forenames>C. Lee</forenames></author></authors><title>Collaborative Filtering by Personality Diagnosis: A Hybrid Memory- and
  Model-Based Approach</title><categories>cs.IR</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-473-480</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growth of Internet commerce has stimulated the use of collaborative
filtering (CF) algorithms as recommender systems. Such systems leverage
knowledge about the known preferences of multiple users to recommend items of
interest to other users. CF methods have been harnessed to make recommendations
about such items as web pages, movies, books, and toys. Researchers have
proposed and evaluated many approaches for generating recommendations. We
describe and evaluate a new method called emph{personality diagnosis (PD)}.
Given a user's preferences for some items, we compute the probability that he
or she is of the same &quot;personality type&quot; as other users, and, in turn, the
probability that he or she will like new items. PD retains some of the
advantages of traditional similarity-weighting techniques in that all data is
brought to bear on each prediction and new data can be added easily and
incrementally. Additionally, PD has a meaningful probabilistic interpretation,
which may be leveraged to justify, explain, and augment results. We report
empirical results on the EachMovie database of movie ratings, and on user
profile data collected from the CiteSeer digital library of Computer Science
research papers. The probabilistic framework naturally supports a variety of
descriptive measurements - in particular, we consider the applicability of a
value of information (VOI) computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3886</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3886</id><created>2013-01-16</created><authors><author><keyname>Pennock</keyname><forenames>David M.</forenames></author><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author></authors><title>Compact Securities Markets for Pareto Optimal Reallocation of Risk</title><categories>cs.GT q-fin.GN</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-481-488</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emph{securities market} is the fundamental theoretical framework in
economics and finance for resource allocation under uncertainty. Securities
serve both to reallocate risk and to disseminate probabilistic information.
emph{Complete} securities markets - which contain one security for every
possible state of nature - support Pareto optimal allocations of risk. Complete
markets suffer from the same exponential dependence on the number of underlying
events as do joint probability distributions. We examine whether markets can be
structured and &quot;compacted&quot; in the same manner as Bayesian network
representations of joint distributions. We show that, if all agents'
risk-neutral independencies agree with the independencies encoded in the market
structure, then the market is emph{operationally complete}: risk is still
Pareto optimally allocated, yet the number of securities can be exponentially
smaller. For collections of agents of a certain type, agreement on Markov
independencies is sufficient to admit compact and operationally complete
markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3887</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3887</id><created>2013-01-16</created><authors><author><keyname>Poupart</keyname><forenames>Pascal</forenames></author><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author></authors><title>Value-Directed Belief State Approximation for POMDPs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-497-506</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem belief-state monitoring for the purposes of
implementing a policy for a partially-observable Markov decision process
(POMDP), specifically how one might approximate the belief state. Other schemes
for belief-state approximation (e.g., based on minimixing a measures such as
KL-diveregence between the true and estimated state) are not necessarily
appropriate for POMDPs. Instead we propose a framework for analyzing
value-directed approximation schemes, where approximation quality is determined
by the expected error in utility rather than by the error in the belief state
itself. We propose heuristic methods for finding good projection schemes for
belief state estimation - exhibiting anytime characteristics - given a POMDP
value fucntion. We also describe several algorithms for constructing bounds on
the error in decision quality (expected utility) associated with acting in
accordance with a given belief state approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3888</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3888</id><created>2013-01-16</created><authors><author><keyname>Pynadath</keyname><forenames>David V.</forenames></author><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author></authors><title>Probabilistic State-Dependent Grammars for Plan Recognition</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-507-514</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Techniques for plan recognition under uncertainty require a stochastic model
of the plan-generation process. We introduce Probabilistic State-Dependent
Grammars (PSDGs) to represent an agent's plan-generation process. The PSDG
language model extends probabilistic context-free grammars (PCFGs) by allowing
production probabilities to depend on an explicit model of the planning agent's
internal and external state. Given a PSDG description of the plan-generation
process, we can then use inference algorithms that exploit the particular
independence properties of the PSDG language to efficiently answer
plan-recognition queries. The combination of the PSDG language model and
inference algorithms extends the range of plan-recognition domains for which
practical probabilistic inference is possible, as illustrated by applications
in traffic monitoring and air combat.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3889</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3889</id><created>2013-01-16</created><authors><author><keyname>Renooij</keyname><forenames>Silja</forenames></author><author><keyname>van der Gaag</keyname><forenames>Linda C.</forenames></author><author><keyname>Parsons</keyname><forenames>Simon</forenames></author><author><keyname>Green</keyname><forenames>Shaw</forenames></author></authors><title>Pivotal Pruning of Trade-offs in QPNs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-515-522</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Qualitative probabilistic networks have been designed for probabilistic
reasoning in a qualitative way. Due to their coarse level of representation
detail, qualitative probabilistic networks do not provide for resolving
trade-offs and typically yield ambiguous results upon inference. We present an
algorithm for computing more insightful results for unresolved trade-offs. The
algorithm builds upon the idea of using pivots to zoom in on the trade-offs and
identifying the information that would serve to resolve them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3890</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3890</id><created>2013-01-16</created><authors><author><keyname>Schuurmans</keyname><forenames>Dale</forenames></author><author><keyname>Southey</keyname><forenames>Finnegan</forenames></author></authors><title>Monte Carlo Inference via Greedy Importance Sampling</title><categories>cs.LG stat.CO stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-523-532</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method for conducting Monte Carlo inference in graphical
models which combines explicit search with generalized importance sampling. The
idea is to reduce the variance of importance sampling by searching for
significant points in the target distribution. We prove that it is possible to
introduce search and still maintain unbiasedness. We then demonstrate our
procedure on a few simple inference tasks and show that it can improve the
inference quality of standard MCMC methods, including Gibbs sampling,
Metropolis sampling, and Hybrid Monte Carlo. This paper extends previous work
which showed how greedy importance sampling could be correctly realized in the
one-dimensional case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3891</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3891</id><created>2013-01-16</created><authors><author><keyname>Sebban</keyname><forenames>Marc</forenames></author><author><keyname>Nock</keyname><forenames>Richard</forenames></author></authors><title>Combining Feature and Prototype Pruning by Uncertainty Minimization</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-533-540</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus in this paper on dataset reduction techniques for use in k-nearest
neighbor classification. In such a context, feature and prototype selections
have always been independently treated by the standard storage reduction
algorithms. While this certifying is theoretically justified by the fact that
each subproblem is NP-hard, we assume in this paper that a joint storage
reduction is in fact more intuitive and can in practice provide better results
than two independent processes. Moreover, it avoids a lot of distance
calculations by progressively removing useless instances during the feature
pruning. While standard selection algorithms often optimize the accuracy to
discriminate the set of solutions, we use in this paper a criterion based on an
uncertainty measure within a nearest-neighbor graph. This choice comes from
recent results that have proven that accuracy is not always the suitable
criterion to optimize. In our approach, a feature or an instance is removed if
its deletion improves information of the graph. Numerous experiments are
presented in this paper and a statistical analysis shows the relevance of our
approach, and its tolerance in the presence of noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3892</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3892</id><created>2013-01-16</created><authors><author><keyname>Singh</keyname><forenames>Satinder</forenames></author><author><keyname>Kearns</keyname><forenames>Michael</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author></authors><title>Nash Convergence of Gradient Dynamics in Iterated General-Sum Games</title><categories>cs.GT</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-541-548</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-agent games are becoming an increasing prevalent formalism for the
study of electronic commerce and auctions. The speed at which transactions can
take place and the growing complexity of electronic marketplaces makes the
study of computationally simple agents an appealing direction. In this work, we
analyze the behavior of agents that incrementally adapt their strategy through
gradient ascent on expected payoff, in the simple setting of two-player,
two-action, iterated general-sum games, and present a surprising result. We
show that either the agents will converge to Nash equilibrium, or if the
strategies themselves do not converge, then their average payoffs will
nevertheless converge to the payoffs of a Nash equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3893</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3893</id><created>2013-01-16</created><authors><author><keyname>Skaanning</keyname><forenames>Claus</forenames></author></authors><title>A Knowledge Acquisition Tool for Bayesian-Network Troubleshooters</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-549-557</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a domain-specific knowledge acquisition tool for
intelligent automated troubleshooters based on Bayesian networks. No Bayesian
network knowledge is required to use the tool, and troubleshooting information
can be specified as natural and intuitive as possible. Probabilities can be
specified in the direction that is most natural to the domain expert. Thus, the
knowledge acquisition efficiently removes the traditional knowledge acquisition
bottleneck of Bayesian networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3894</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3894</id><created>2013-01-16</created><authors><author><keyname>Steck</keyname><forenames>Harald</forenames></author></authors><title>On the Use of Skeletons when Learning in Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-558-565</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a heuristic operator which aims at simultaneously
optimizing the orientations of all the edges in an intermediate Bayesian
network structure during the search process. This is done by alternating
between the space of directed acyclic graphs (DAGs) and the space of skeletons.
The found orientations of the edges are based on a scoring function rather than
on induced conditional independences. This operator can be used as an extension
to commonly employed search strategies. It is evaluated in experiments with
artificial and real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3895</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3895</id><created>2013-01-16</created><authors><author><keyname>Storkey</keyname><forenames>Amos J.</forenames></author></authors><title>Dynamic Trees: A Structured Variational Method Giving Efficient
  Propagation Rules</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-566-573</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic trees are mixtures of tree structured belief networks. They solve
some of the problems of fixed tree networks at the cost of making exact
inference intractable. For this reason approximate methods such as sampling or
mean field approaches have been used. However, mean field approximations assume
a factorized distribution over node states. Such a distribution seems unlickely
in the posterior, as nodes are highly correlated in the prior. Here a
structured variational approach is used, where the posterior distribution over
the non-evidential nodes is itself approximated by a dynamic tree. It turns out
that this form can be used tractably and efficiently. The result is a set of
update rules which can propagate information through the network to obtain both
a full variational approximation, and the relevant marginals. The progagtion
rules are more efficient than the mean field approach and give noticeable
quantitative and qualitative improvement in the inference. The marginals
calculated give better approximations to the posterior than loopy propagation
on a small toy problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3896</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3896</id><created>2013-01-16</created><authors><author><keyname>Teow</keyname><forenames>Loo-Nin</forenames></author><author><keyname>Loe</keyname><forenames>Kia-Fock</forenames></author></authors><title>An Uncertainty Framework for Classification</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-574-579</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a generalized likelihood function based on uncertainty measures and
show that maximizing such a likelihood function for different measures induces
different types of classifiers. In the probabilistic framework, we obtain
classifiers that optimize the cross-entropy function. In the possibilistic
framework, we obtain classifiers that maximize the interclass margin.
Furthermore, we show that the support vector machine is a sub-class of these
maximum-margin classifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3897</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3897</id><created>2013-01-16</created><authors><author><keyname>Tian</keyname><forenames>Jin</forenames></author></authors><title>A Branch-and-Bound Algorithm for MDL Learning Bayesian Networks</title><categories>cs.AI cs.LG stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-580-588</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends the work in [Suzuki, 1996] and presents an efficient
depth-first branch-and-bound algorithm for learning Bayesian network
structures, based on the minimum description length (MDL) principle, for a
given (consistent) variable ordering. The algorithm exhaustively searches
through all network structures and guarantees to find the network with the best
MDL score. Preliminary experiments show that the algorithm is efficient, and
that the time complexity grows slowly with the sample size. The algorithm is
useful for empirically studying both the performance of suboptimal heuristic
search algorithms and the adequacy of the MDL principle in learning Bayesian
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3898</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3898</id><created>2013-01-16</created><authors><author><keyname>Tian</keyname><forenames>Jin</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Probabilities of Causation: Bounds and Identification</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-589-598</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the problem of estimating the probability that one
event was a cause of another in a given scenario. Using structural-semantical
definitions of the probabilities of necessary or sufficient causation (or
both), we show how to optimally bound these quantities from data obtained in
experimental and observational studies, making minimal assumptions concerning
the data-generating process. In particular, we strengthen the results of Pearl
(1999) by weakening the data-generation assumptions and deriving theoretically
sharp bounds on the probabilities of causation. These results delineate
precisely how empirical data can be used both in settling questions of
attribution and in solving attribution-related problems of decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3899</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3899</id><created>2013-01-16</created><authors><author><keyname>Vaithyanathan</keyname><forenames>Shivakumar</forenames></author><author><keyname>Dom</keyname><forenames>Byron E</forenames></author></authors><title>Model-Based Hierarchical Clustering</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-599-608</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to model-based hierarchical clustering by formulating
an objective function based on a Bayesian analysis. This model organizes the
data into a cluster hierarchy while specifying a complex feature-set
partitioning that is a key component of our model. Features can have either a
unique distribution in every cluster or a common distribution over some (or
even all) of the clusters. The cluster subsets over which these features have
such a common distribution correspond to the nodes (clusters) of the tree
representing the hierarchy. We apply this general model to the problem of
document clustering for which we use a multinomial likelihood function and
Dirichlet priors. Our algorithm consists of a two-stage process wherein we
first perform a flat clustering followed by a modified hierarchical
agglomerative merging process that includes determining the features that will
have common distributions over the merged clusters. The regularization induced
by using the marginal likelihood automatically determines the optimal model
structure including number of clusters, the depth of the tree and the subset of
features to be modeled as having a common distribution at each node. We present
experimental results on both synthetic data and a real document collection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3900</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3900</id><created>2013-01-16</created><authors><author><keyname>Vejnarova</keyname><forenames>Jirina</forenames></author></authors><title>Conditional Independence and Markov Properties in Possibility Theory</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-609-616</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditional independence and Markov properties are powerful tools allowing
expression of multidimensional probability distributions by means of
low-dimensional ones. As multidimensional possibilistic models have been
studied for several years, the demand for analogous tools in possibility theory
seems to be quite natural. This paper is intended to be a promotion of de
Cooman's measure-theoretic approcah to possibility theory, as this approach
allows us to find analogies to many important results obtained in probabilistic
framework. First, we recall semi-graphoid properties of conditional
possibilistic independence, parameterized by a continuous t-norm, and find
sufficient conditions for a class of Archimedean t-norms to have the graphoid
property. Then we introduce Markov properties and factorization of possibility
distrubtions (again parameterized by a continuous t-norm) and find the
relationships between them. These results are accompanied by a number of
conterexamples, which show that the assumptions of specific theorems are
substantial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3901</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3901</id><created>2013-01-16</created><authors><author><keyname>Wiegerinck</keyname><forenames>Wim</forenames></author></authors><title>Variational Approximations between Mean Field Theory and the Junction
  Tree Algorithm</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-626-633</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, variational approximations such as the mean field approximation
have received much interest. We extend the standard mean field method by using
an approximating distribution that factorises into cluster potentials. This
includes undirected graphs, directed acyclic graphs and junction trees. We
derive generalized mean field equations to optimize the cluster potentials. We
show that the method bridges the gap between the standard mean field
approximation and the exact junction tree algorithm. In addition, we address
the problem of how to choose the graphical structure of the approximating
distribution. From the generalised mean field equations we derive rules to
simplify the structure of the approximating distribution in advance without
affecting the quality of the approximation. We also show how the method fits
into some other variational approximations that are currently popular.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3902</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3902</id><created>2013-01-16</created><authors><author><keyname>Williamson</keyname><forenames>David M.</forenames></author><author><keyname>Almond</keyname><forenames>Russell</forenames></author><author><keyname>Mislevy</keyname><forenames>Robert</forenames></author></authors><title>Model Criticism of Bayesian Networks with Latent Variables</title><categories>cs.AI stat.AP stat.ME</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-634-643</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The application of Bayesian networks (BNs) to cognitive assessment and
intelligent tutoring systems poses new challenges for model construction. When
cognitive task analyses suggest constructing a BN with several latent
variables, empirical model criticism of the latent structure becomes both
critical and complex. This paper introduces a methodology for criticizing
models both globally (a BN in its entirety) and locally (observable nodes), and
explores its value in identifying several kinds of misfit: node errors, edge
errors, state errors, and prior probability errors in the latent structure. The
results suggest the indices have potential for detecting model misfit and
assisting in locating problematic components of the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3903</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3903</id><created>2013-01-16</created><authors><author><keyname>Wittig</keyname><forenames>Frank</forenames></author><author><keyname>Jameson</keyname><forenames>Anthony</forenames></author></authors><title>Exploiting Qualitative Knowledge in the Learning of Conditional
  Probabilities of Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-644-652</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms for learning the conditional probabilities of Bayesian networks
with hidden variables typically operate within a high-dimensional search space
and yield only locally optimal solutions. One way of limiting the search space
and avoiding local optima is to impose qualitative constraints that are based
on background knowledge concerning the domain. We present a method for
integrating formal statements of qualitative constraints into two learning
algorithms, APN and EM. In our experiments with synthetic data, this method
yielded networks that satisfied the constraints almost perfectly. The accuracy
of the learned networks was consistently superior to that of corresponding
networks learned without constraints. The exploitation of qualitative
constraints therefore appears to be a promising way to increase both the
interpretability and the accuracy of learned Bayesian networks with known
structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3931</identifier>
 <datestamp>2013-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3931</id><created>2013-01-16</created><updated>2013-09-27</updated><authors><author><keyname>Comin</keyname><forenames>Carlo</forenames></author></authors><title>(Extended Version) Algebraic Characterization of the Class of Languages
  recognized by Measure Only Quantum Automata</title><categories>cs.FL</categories><comments>The author wish to thank Prof. Alberto Bertoni for having introduced
  him to the problem and for the stimulating discussions and research
  directions that finally lead to the results of this paper, Dr. Maria-Paola
  Bianchi for the kind collaboration on the structural drawing-up of the
  article</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a model of one-way quantum automaton where only measurement
operations are allowed ($\mon$). We give an algebraic characterization of
$\lmo(\Sigma)$, showing that the syntactic monoids of the languages in
$\lmo(\Sigma)$ are exactly the $J$-trivial literally idempotent syntactic
monoids, where $J$ is the Green's relation determined by two-sided ideals. We
also prove that $\lmo(\Sigma)$ coincides with the literal variety of literally
idempotent piecewise testable regular languages. This allows us to prove the
existence of a polynomial time algorithm for deciding whether a regular
language belongs to $\lmo(\Sigma)$ and to discuss definability issues in terms
of the existential first-order logic $\Sigma_1[&lt;]$ and the linear temporal
logic without the next operator LTLWN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3934</identifier>
 <datestamp>2013-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3934</id><created>2013-01-16</created><updated>2013-08-20</updated><authors><author><keyname>Scott</keyname><forenames>Jacob G.</forenames></author><author><keyname>Chinnaiyan</keyname><forenames>Prakash</forenames></author><author><keyname>Anderson</keyname><forenames>Alexander R. A.</forenames></author><author><keyname>Hjelmeland</keyname><forenames>Anita</forenames></author><author><keyname>Basanta</keyname><forenames>David</forenames></author></authors><title>Intrinsic cell factors that influence tumourigenicity in cancer stem
  cells - towards hallmarks of cancer stem cells</title><categories>q-bio.TO cs.CE</categories><comments>8 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the discovery of a cancer initiating side population in solid tumours,
studies focussing on the role of so-called cancer stem cells in cancer
initiation and progression have abounded. The biological interrogation of these
cells has yielded volumes of information about their behaviour, but there has,
as of yet, not been many actionable generalised theoretical conclusions. To
address this point, we have created a hybrid, discrete/continuous computational
cellular automaton model of a generalised stem-cell driven tissue and explored
the phenotypic traits inherent in the inciting cell and the resultant tissue
growth. We identify the regions in phenotype parameter space where these
initiating cells are able to cause a disruption in homeostasis, leading to
tissue overgrowth and tumour formation. As our parameters and model are
non-specific, they could apply to any tissue cancer stem-cell and do not assume
specific genetic mutations. In this way, our model suggests that targeting
these phenotypic traits could represent generalizable strategies across cancer
types and represents a first attempt to identify the hallmarks of cancer stem
cells.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3946</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3946</id><created>2013-01-16</created><updated>2013-02-18</updated><authors><author><keyname>Koepke</keyname><forenames>Hoyt</forenames></author><author><keyname>Thompson</keyname><forenames>Elizabeth</forenames></author></authors><title>Efficient Identification of Equivalences in Dynamic Graphs and Pedigree
  Structures</title><categories>cs.DS q-bio.QM stat.CO</categories><comments>Code for paper available at
  http://www.stat.washington.edu/~hoytak/code/hashreduce</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new framework for designing test and query functions for complex
structures that vary across a given parameter such as genetic marker position.
The operations we are interested in include equality testing, set operations,
isolating unique states, duplication counting, or finding equivalence classes
under identifiability constraints. A motivating application is locating
equivalence classes in identity-by-descent (IBD) graphs, graph structures in
pedigree analysis that change over genetic marker location. The nodes of these
graphs are unlabeled and identified only by their connecting edges, a
constraint easily handled by our approach. The general framework introduced is
powerful enough to build a range of testing functions for IBD graphs, dynamic
populations, and other structures using a minimal set of operations. The
theoretical and algorithmic properties of our approach are analyzed and proved.
Computational results on several simulations demonstrate the effectiveness of
our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3964</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3964</id><created>2013-01-16</created><authors><author><keyname>Ngo</keyname><forenames>Anh Cat Le</forenames></author><author><keyname>Li-Minn</keyname><forenames>Kenneth Ang</forenames></author><author><keyname>Qiu</keyname><forenames>Guoping</forenames></author><author><keyname>Kah-Phooi</keyname><forenames>Jasmine Seng</forenames></author></authors><title>Multiscale Discriminant Saliency for Visual Attention</title><categories>cs.CV</categories><comments>16 pages, ICCSA 2013 - BIOCA session</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bottom-up saliency, an early stage of humans' visual attention, can be
considered as a binary classification problem between center and surround
classes. Discriminant power of features for the classification is measured as
mutual information between features and two classes distribution. The estimated
discrepancy of two feature classes very much depends on considered scale
levels; then, multi-scale structure and discriminant power are integrated by
employing discrete wavelet features and Hidden markov tree (HMT). With wavelet
coefficients and Hidden Markov Tree parameters, quad-tree like label structures
are constructed and utilized in maximum a posterior probability (MAP) of hidden
class variables at corresponding dyadic sub-squares. Then, saliency value for
each dyadic square at each scale level is computed with discriminant power
principle and the MAP. Finally, across multiple scales is integrated the final
saliency map by an information maximization rule. Both standard quantitative
tools such as NSS, LCC, AUC and qualitative assessments are used for evaluating
the proposed multiscale discriminant saliency method (MDIS) against the
well-know information-based saliency method AIM on its Bruce Database wity
eye-tracking data. Simulation results are presented and analyzed to verify the
validity of MDIS as well as point out its disadvantages for further research
direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3966</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3966</id><created>2013-01-16</created><authors><author><keyname>Zhao</keyname><forenames>Tingting</forenames></author><author><keyname>Hachiya</keyname><forenames>Hirotaka</forenames></author><author><keyname>Tangkaratt</keyname><forenames>Voot</forenames></author><author><keyname>Morimoto</keyname><forenames>Jun</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author></authors><title>Efficient Sample Reuse in Policy Gradients with Parameter-based
  Exploration</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The policy gradient approach is a flexible and powerful reinforcement
learning method particularly for problems with continuous actions such as robot
control. A common challenge in this scenario is how to reduce the variance of
policy gradient estimates for reliable policy updates. In this paper, we
combine the following three ideas and give a highly effective policy gradient
method: (a) the policy gradients with parameter based exploration, which is a
recently proposed policy search method with low variance of gradient estimates,
(b) an importance sampling technique, which allows us to reuse previously
gathered data in a consistent way, and (c) an optimal baseline, which minimizes
the variance of gradient estimates with their unbiasedness being maintained.
For the proposed method, we give theoretical analysis of the variance of
gradient estimates and show its usefulness through extensive experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3979</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3979</id><created>2013-01-16</created><updated>2013-03-23</updated><authors><author><keyname>Kloks</keyname><forenames>Ton</forenames></author><author><keyname>Wang</keyname><forenames>Yue-Li</forenames></author></authors><title>On retracts, absolute retracts, and folds in cographs</title><categories>cs.DM</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G and H be two cographs. We show that the problem to determine whether H
is a retract of G is NP-complete. We show that this problem is fixed-parameter
tractable when parameterized by the size of H. When restricted to the class of
threshold graphs or to the class of trivially perfect graphs, the problem
becomes tractable in polynomial time. The problem is also soluble when one
cograph is given as an induced subgraph of the other. We characterize absolute
retracts of cographs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3991</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3991</id><created>2013-01-17</created><authors><author><keyname>Chen</keyname><forenames>Zhenghong</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoxian</forenames></author><author><keyname>Xia</keyname><forenames>Bican</forenames></author></authors><title>Generic Regular Decompositions for Parametric Polynomial Systems</title><categories>cs.SC cs.IT math.IT</categories><comments>It is the latest version. arXiv admin note: text overlap with
  arXiv:1208.6112</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a generalization of our earlier work in [19]. In this
paper, the two concepts, generic regular decomposition (GRD) and
regular-decomposition-unstable (RDU) variety introduced in [19] for generic
zero-dimensional systems, are extended to the case where the parametric systems
are not necessarily zero-dimensional. An algorithm is provided to compute GRDs
and the associated RDU varieties of parametric systems simultaneously on the
basis of the algorithm for generic zero-dimensional systems proposed in [19].
Then the solutions of any parametric system can be represented by the solutions
of finitely many regular systems and the decomposition is stable at any
parameter value in the complement of the associated RDU variety of the
parameter space. The related definitions and the results presented in [19] are
also generalized and a further discussion on RDU varieties is given from an
experimental point of view. The new algorithm has been implemented on the basis
of DISCOVERER with Maple 16 and experimented with a number of benchmarks from
the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3996</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3996</id><created>2013-01-17</created><updated>2013-12-08</updated><authors><author><keyname>Maurer</keyname><forenames>Alexandre</forenames><affiliation>LIP6, LINCS</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LIP6, LINCS, IUF</affiliation></author></authors><title>Parameterizable Byzantine Broadcast in Loosely Connected Networks</title><categories>cs.DC cs.DS cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of reliably broadcasting information in a multihop
asynchronous network, despite the presence of Byzantine failures: some nodes
are malicious and behave arbitrarly. We focus on non-cryptographic solutions.
Most existing approaches give conditions for perfect reliable broadcast (all
correct nodes deliver the good information), but require a highly connected
network. A probabilistic approach was recently proposed for loosely connected
networks: the Byzantine failures are randomly distributed, and the correct
nodes deliver the good information with high probability. A first solution
require the nodes to initially know their position on the network, which may be
difficult or impossible in self-organizing or dynamic networks. A second
solution relaxed this hypothesis but has much weaker Byzantine tolerance
guarantees. In this paper, we propose a parameterizable broadcast protocol that
does not require nodes to have any knowledge about the network. We give a
deterministic technique to compute a set of nodes that always deliver authentic
information, for a given set of Byzantine failures. Then, we use this technique
to experimentally evaluate our protocol, and show that it significantely
outperforms previous solutions with the same hypotheses. Important disclaimer:
these results have NOT yet been published in an international conference or
journal. This is just a technical report presenting intermediary and incomplete
results. A generalized version of these results may be under submission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3997</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3997</id><created>2013-01-17</created><authors><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Jain</keyname><forenames>Satbir</forenames></author></authors><title>Performance Comparison of Proposed Lifetime Maximizing Trees for Data
  Aggregation in Wireless Sensor Networks</title><categories>cs.NI</categories><comments>13 pages, 8 figures; International Journal on Computer Science and
  Engineering (IJCSE)Volume 3 Issue 1 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a packet level simulator is used to explore the performance of
the proposed DLMT and CLMT algorithms under various traffic conditions.
Performance of the proposed algorithms is compared with already existing E-Span
tree structure. These proposed algorithms tend to extend the node lifetime in
order to increase the amount of information gathered by the tree root.
Decentralized lifetime maximizing tree (DLMT) features in nodes with higher
energy to be chosen as data aggregating parents while Centralized Lifetime
Maximizing Tree (CLMT) features with the identification of the bottleneck node
to collect data in a central manner among given set of nodes. By choosing
Forwarded Diffusion as our underlying routing platform the simulations are
carried on J-Sim. Our simulation results have shown that the functional
lifetime of event sources can be enhanced by a maximum of 147% when data is
aggregated via DLMT and by 139% when data is aggregated via CLMT. Our proposed
DLMT algorithm has shown maximum of 13% additional lifetime saving without
increasing the delay. Packet delivery ratio has also shown a remarkable
increase when the tree depth is considered in these proposed tree structures.
Furthermore, the delay is also reduced by using DLMT &amp; CLMT in comparison with
E-Span.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.3999</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.3999</id><created>2013-01-17</created><authors><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Jain</keyname><forenames>Satbir</forenames></author></authors><title>Stable Routing for achieving Quality of Service in wireless Sensor
  Networks</title><categories>cs.NI</categories><comments>7 pages,6 figures; IJCA Special Issue on MANETs, 2010</comments><doi>10.5120/1010-47</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networking in Wireless Sensor networks is a challenging task due to the lack
of resources in the network as well as the frequent changes in network
topology. Although lots of research has been done on supporting QoS in the
Internet and other networks, but they are not suitable for wireless sensor
networks and still QoS support for such networks remains an open problem. In
this paper, a new scheme has been proposed for achieving QoS in terms of packet
delivery, multiple connections, better power management and stable routes in
case of failure. It offers quick adaptation to distributed processing, dynamic
linking, low processing overhead and loop freedom at all times. The proposed
scheme has been incorporated using QDPRA protocol and by extensive simulation
the performance has been studied, and it is clearly shown that the proposed
scheme performs very well for different network scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4010</identifier>
 <datestamp>2014-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4010</id><created>2013-01-17</created><updated>2014-03-10</updated><authors><author><keyname>Rothvoss</keyname><forenames>Thomas</forenames></author></authors><title>Approximating Bin Packing within O(log OPT * log log OPT) bins</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For bin packing, the input consists of n items with sizes s_1,...,s_n in
[0,1] which have to be assigned to a minimum number of bins of size 1. The
seminal Karmarkar-Karp algorithm from '82 produces a solution with at most OPT
+ O(log^2 OPT) bins.
  We provide the first improvement in now 3 decades and show that one can find
a solution of cost OPT + O(log OPT * log log OPT) in polynomial time. This is
achieved by rounding a fractional solution to the Gilmore-Gomory LP relaxation
using the Entropy Method from discrepancy theory. The result is constructive
via algorithms of Bansal and Lovett-Meka.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4016</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4016</id><created>2013-01-17</created><authors><author><keyname>Hernando</keyname><forenames>Fernando</forenames></author><author><keyname>McGuire</keyname><forenames>Gary</forenames></author><author><keyname>Monserrat</keyname><forenames>Francisco</forenames></author></authors><title>On the Classification of Exceptional Planar Functions over
  $\mathbb{F}_{p}$</title><categories>math.AG cs.IT math.IT</categories><msc-class>51E20, 11T71, 94B27, 05B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We will present many strong partial results towards a classification of
exceptional planar/PN monomial functions on finite fields. The techniques we
use are the Weil bound, Bezout's theorem, and Bertini's theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4019</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4019</id><created>2013-01-17</created><updated>2015-06-11</updated><authors><author><keyname>Murray</keyname><forenames>Lawrence M.</forenames></author><author><keyname>Lee</keyname><forenames>Anthony</forenames></author><author><keyname>Jacob</keyname><forenames>Pierre E.</forenames></author></authors><title>Parallel resampling in the particle filter</title><categories>stat.CO cs.DC</categories><comments>21 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern parallel computing devices, such as the graphics processing unit
(GPU), have gained significant traction in scientific and statistical
computing. They are particularly well-suited to data-parallel algorithms such
as the particle filter, or more generally Sequential Monte Carlo (SMC), which
are increasingly used in statistical inference. SMC methods carry a set of
weighted particles through repeated propagation, weighting and resampling
steps. The propagation and weighting steps are straightforward to parallelise,
as they require only independent operations on each particle. The resampling
step is more difficult, as standard schemes require a collective operation,
such as a sum, across particle weights. Focusing on this resampling step, we
analyse two alternative schemes that do not involve a collective operation
(Metropolis and rejection resamplers), and compare them to standard schemes
(multinomial, stratified and systematic resamplers). We find that, in certain
circumstances, the alternative resamplers can perform significantly faster on a
GPU, and to a lesser extent on a CPU, than the standard approaches. Moreover,
in single precision, the standard approaches are numerically biased for upwards
of hundreds of thousands of particles, while the alternatives are not. This is
particularly important given greater single- than double-precision throughput
on modern devices, and the consequent temptation to use single precision with a
greater number of particles. Finally, we provide auxiliary functions useful for
implementation, such as for the permutation of ancestry vectors to enable
in-place propagation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4028</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4028</id><created>2013-01-17</created><authors><author><keyname>Schreiber</keyname><forenames>Michael</forenames></author></authors><title>Do we need the g-index?</title><categories>physics.soc-ph cs.DL</categories><comments>7 pages, 3 figures accepted for publication in Journal of the
  American Society for Information Science and Technology</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Using a very small sample of 8 datasets it was recently shown by De Visscher
(2011) that the g-index is very close to the square root of the total number of
citations. It was argued that there is no bibliometrically meaningful
difference. Using another somewhat larger empirical sample of 26 datasets I
show that the difference may be larger and I argue in favor of the g-index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4039</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4039</id><created>2013-01-17</created><updated>2013-08-01</updated><authors><author><keyname>Nikolov</keyname><forenames>Aleksandar</forenames></author></authors><title>The Komlos Conjecture Holds for Vector Colorings</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Komlos conjecture in discrepancy theory states that for some constant K
and for any m by n matrix A whose columns lie in the unit ball there exists a
+/- 1 vector x such that the infinity norm of Ax is bounded above by K. This
conjecture also implies the Beck-Fiala conjecture on the discrepancy of bounded
degree hypergraphs. Here we prove a natural relaxation of the Komlos
conjecture: if the columns of A are assigned unit real vectors rather than +/-
1 then the Komlos conjecture holds with K=1. Our result rules out the
possibility of a counterexample to the conjecture based on semidefinite
programming. It also opens the way to proving tighter efficient
(polynomial-time computable) upper bounds for the conjecture using semidefinite
programming techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4050</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4050</id><created>2013-01-17</created><authors><author><keyname>Schuh</keyname><forenames>Fabian</forenames></author><author><keyname>Schenk</keyname><forenames>Andreas</forenames></author><author><keyname>Huber</keyname><forenames>Johannes B.</forenames></author></authors><title>Punctured Trellis-Coded Modulation</title><categories>cs.IT math.IT</categories><comments>5 pages, 10 figures, submitted to IEEE International Symposium on
  Information Theory 2013 (ISIT)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In classic trellis-coded modulation (TCM) signal constellations of twice the
cardinality are applied when compared to an uncoded transmission enabling
transmission of one bit of redundancy per PAM-symbol, i.e., rates of
$\frac{K}{K+1}$ when $2^{K+1}$ denotes the cardinality of the signal
constellation. In order to support different rates, multi-dimensional (i.e.,
$\mathcal{D}$-dimensional) constellations had been proposed by means of
combining subsequent one- or two-dimensional modulation steps, resulting in
TCM-schemes with $\frac{1}{\mathcal{D}}$ bit redundancy per real dimension. In
contrast, in this paper we propose to perform rate adjustment for TCM by means
of puncturing the convolutional code (CC) on which a TCM-scheme is based on. It
is shown, that due to the nontrivial mapping of the output symbols of the CC to
signal points in the case of puncturing, a modification of the corresponding
Viterbi-decoder algorithm and an optimization of the CC and the puncturing
scheme are necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4055</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4055</id><created>2013-01-17</created><updated>2014-04-09</updated><authors><author><keyname>Dyer</keyname><forenames>Martin</forenames></author><author><keyname>Greenhill</keyname><forenames>Catherine</forenames></author><author><keyname>Ullrich</keyname><forenames>Mario</forenames></author></authors><title>Structure and eigenvalues of heat-bath Markov chains</title><categories>math.CO cs.DM</categories><comments>15 pages. Minor edits to address referee's comments</comments><journal-ref>Linear Algebra Appl. 454 (2014), 57-71</journal-ref><doi>10.1016/j.laa.2014.04.018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that heat-bath chains (which we define in a general setting) have no
negative eigenvalues. Two applications of this result are presented: one to
single-site heat-bath chains for spin systems and one to a heat-bath Markov
chain for sampling contingency tables. Some implications of our main result for
the analysis of the mixing time of heat-bath Markov chains are discussed. We
also prove an alternative characterisation of heat-bath chains, and consider
possible generalisations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4072</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4072</id><created>2013-01-17</created><authors><author><keyname>Li</keyname><forenames>Zijia</forenames></author><author><keyname>Schicho</keyname><forenames>Josef</forenames></author></authors><title>Classification of Angle-Symmetric 6R Linkage</title><categories>math.AG cs.RO cs.SC</categories><doi>10.1016/j.mechmachtheory.2013.08.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a special kind of overconstrained 6R closed
linkages which we call angle-symmetric 6R linkages. These are linkages with the
property that the rotation angles are equal for each of the three pairs of
opposite joints. We give a classification of these linkages. It turns that
there are three types. First, we have the linkages with line symmetry. The
second type is new. The third type is related to cubic motion polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4083</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4083</id><created>2013-01-17</created><updated>2013-07-13</updated><authors><author><keyname>G&#xfc;l&#xe7;ehre</keyname><forenames>&#xc7;a&#x11f;lar</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Knowledge Matters: Importance of Prior Information for Optimization</title><categories>cs.LG cs.CV cs.NE stat.ML</categories><comments>37 Pages, 5 figures, 5 tables JMLR Special Topics on Representation
  Learning Submission</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We explore the effect of introducing prior information into the intermediate
level of neural networks for a learning task on which all the state-of-the-art
machine learning algorithms tested failed to learn. We motivate our work from
the hypothesis that humans learn such intermediate concepts from other
individuals via a form of supervision or guidance using a curriculum. The
experiments we have conducted provide positive evidence in favor of this
hypothesis. In our experiments, a two-tiered MLP architecture is trained on a
dataset with 64x64 binary inputs images, each image with three sprites. The
final task is to decide whether all the sprites are the same or one of them is
different. Sprites are pentomino tetris shapes and they are placed in an image
with different locations using scaling and rotation transformations. The first
part of the two-tiered MLP is pre-trained with intermediate-level targets being
the presence of sprites at each location, while the second part takes the
output of the first part as input and predicts the final task's target binary
event. The two-tiered MLP architecture, with a few tens of thousand examples,
was able to learn the task perfectly, whereas all other algorithms (include
unsupervised pre-training, but also traditional algorithms like SVMs, decision
trees and boosting) all perform no better than chance. We hypothesize that the
optimization difficulty involved when the intermediate pre-training is not
performed is due to the {\em composition} of two highly non-linear tasks. Our
findings are also consistent with hypotheses on cultural learning inspired by
the observations of optimization problems with deep learning, presumably
because of effective local minima.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4089</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4089</id><created>2013-01-17</created><authors><author><keyname>Li</keyname><forenames>Chengqing</forenames></author><author><keyname>Xie</keyname><forenames>Tao</forenames></author><author><keyname>Liu</keyname><forenames>Qi</forenames></author></authors><title>Cryptanalyzing an image encryption scheme based on logistic map</title><categories>cs.CR</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, an image encryption scheme based on logistic map was proposed. It
has been reported by a research group that its equivalent secret key can be
reconstructed with only one pair of known-plaintext and the corresponding
cipher-text. Utilizing stable distribution of the chaotic states generated by
iterating the logistic map, this paper further demonstrates that much more
information about the secret key can be derived under the same condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4096</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4096</id><created>2013-01-17</created><authors><author><keyname>Doerr</keyname><forenames>Benjamin</forenames></author><author><keyname>Eremeev</keyname><forenames>Anton</forenames></author><author><keyname>Neumann</keyname><forenames>Frank</forenames></author><author><keyname>Theile</keyname><forenames>Madeleine</forenames></author><author><keyname>Thyssen</keyname><forenames>Christian</forenames></author></authors><title>Evolutionary Algorithms and Dynamic Programming</title><categories>cs.NE cs.DS</categories><comments>This is an updated version of journal publication where few misprints
  are fixed</comments><journal-ref>Theoretical Computer Science, Vol. 412, Issue 43, 2011,
  P.6020-6035</journal-ref><doi>10.1016/j.tcs.2011.07.024</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, it has been proven that evolutionary algorithms produce good
results for a wide range of combinatorial optimization problems. Some of the
considered problems are tackled by evolutionary algorithms that use a
representation which enables them to construct solutions in a dynamic
programming fashion. We take a general approach and relate the construction of
such algorithms to the development of algorithms using dynamic programming
techniques. Thereby, we give general guidelines on how to develop evolutionary
algorithms that have the additional ability of carrying out dynamic programming
steps. Finally, we show that for a wide class of the so-called DP-benevolent
problems (which are known to admit FPTAS) there exists a fully polynomial-time
randomized approximation scheme based on an evolutionary algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4117</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4117</id><created>2013-01-17</created><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Another look at expurgated bounds and their statistical-mechanical
  interpretation</title><categories>cs.IT cond-mat.stat-mech math.IT</categories><comments>18 pages, submitted to IEEE Trans. on Inform. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the derivation of expurgated error exponents using a method of
type class enumeration, which is inspired by statistical-mechanical methods,
and which has already been used in the derivation of random coding exponents in
several other scenarios. We compare our version of the expurgated bound to both
the one by Gallager and the one by Csiszar, Korner and Marton (CKM). For
expurgated ensembles of fixed composition codes over finite alphabets, our
basic expurgated bound coincides with the CKM expurgated bound, which is in
general tighter than Gallager's bound, but with equality for the optimum type
class of codewords. Our method, however, extends beyond fixed composition codes
and beyond finite alphabets, where it is natural to impose input constraints
(e.g., power limitation). In such cases, the CKM expurgated bound may not apply
directly, and our bound is in general tighter than Gallager's bound. In
addition, while both the CKM and the Gallager expurgated bounds are based on
Bhattacharyya bound for bounding the pairwise error probabilities, our bound
allows the more general Chernoff distance measure, thus giving rise to
additional improvement using the Chernoff parameter as a degree of freedom to
be optimized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4121</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4121</id><created>2013-01-17</created><updated>2014-09-06</updated><authors><author><keyname>Oliveira</keyname><forenames>Igor C.</forenames></author><author><keyname>Thatte</keyname><forenames>Bhalchandra D.</forenames></author></authors><title>An algebraic formulation of the graph reconstruction conjecture</title><categories>math.CO cs.DM</categories><comments>12 pages, 2 figures</comments><msc-class>05C60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The graph reconstruction conjecture asserts that every finite simple graph on
at least three vertices can be reconstructed up to isomorphism from its deck -
the collection of its vertex-deleted subgraphs. Kocay's Lemma is an important
tool in graph reconstruction. Roughly speaking, given the deck of a graph $G$
and any finite sequence of graphs, it gives a linear constraint that every
reconstruction of $G$ must satisfy.
  Let $\psi(n)$ be the number of distinct (mutually non-isomorphic) graphs on
$n$ vertices, and let $d(n)$ be the number of distinct decks that can be
constructed from these graphs. Then the difference $\psi(n) - d(n)$ measures
how many graphs cannot be reconstructed from their decks. In particular, the
graph reconstruction conjecture is true for $n$-vertex graphs if and only if
$\psi(n) = d(n)$.
  We give a framework based on Kocay's lemma to study this discrepancy. We
prove that if $M$ is a matrix of covering numbers of graphs by sequences of
graphs, then $d(n) \geq \mathsf{rank}_\mathbb{R}(M)$. In particular, all
$n$-vertex graphs are reconstructible if one such matrix has rank $\psi(n)$. To
complement this result, we prove that it is possible to choose a family of
sequences of graphs such that the corresponding matrix $M$ of covering numbers
satisfies $d(n) = \mathsf{rank}_\mathbb{R}(M)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4127</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4127</id><created>2013-01-17</created><updated>2013-09-28</updated><authors><author><keyname>Baldoni</keyname><forenames>Velleda</forenames></author><author><keyname>Boysal</keyname><forenames>Arzu</forenames></author><author><keyname>Vergne</keyname><forenames>Mich&#xe8;le</forenames></author></authors><title>Multiple Bernoulli series and volumes of moduli spaces of flat bundles
  over surfaces</title><categories>math.RT cs.CG math.AG</categories><comments>51 pages, 3 figures; formula in Proposition 3.1 for the Lie group of
  type G_2 is corrected; new references added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using Szenes formula for multiple Bernoulli series we explain how to compute
Witten series associated to classical Lie algebras. Particular instances of
these series compute volumes of moduli spaces of flat bundles over surfaces,
and also certain multiple zeta values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4131</identifier>
 <datestamp>2013-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4131</id><created>2013-01-17</created><updated>2013-09-14</updated><authors><author><keyname>Jin</keyname><forenames>Xibo</forenames></author><author><keyname>Zhang</keyname><forenames>Fa</forenames></author><author><keyname>Song</keyname><forenames>Ying</forenames></author><author><keyname>Fan</keyname><forenames>Liya</forenames></author><author><keyname>Liu</keyname><forenames>Zhiyong</forenames></author></authors><title>Energy-Efficient Scheduling with Time and Processors Eligibility
  Restrictions</title><categories>cs.DS</categories><comments>18 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While previous work on energy-efficient algorithms focused on assumption that
tasks can be assigned to any processor, we initially study the problem of task
scheduling on restricted parallel processors. The objective is to minimize the
overall energy consumption while speed scaling (SS) method is used to reduce
energy consumption under the execution time constraint (Makespan $C_{max}$). In
this work, we discuss the speed setting in the continuous model that processors
can run at arbitrary speed in $[s_{min},s_{max}]$. The energy-efficient
scheduling problem, involving task assignment and speed scaling, is inherently
complicated as it is proved to be NP-Complete. We formulate the problem as an
Integer Programming (IP) problem. Specifically, we devise a polynomial time
optimal scheduling algorithm for the case tasks have a uniform size. Our
algorithm runs in $O(mn^3logn)$ time, where $m$ is the number of processors and
$n$ is the number of tasks. We then present a polynomial time algorithm that
achieves an approximation factor of $2^{\alpha-1}(2-\frac{1}{m^{\alpha}})$
($\alpha$ is the power parameter) when the tasks have arbitrary size work.
Experimental results demonstrate that our algorithm could provide an efficient
scheduling for the problem of task scheduling on restricted parallel
processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4137</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4137</id><created>2012-12-19</created><authors><author><keyname>Diamant</keyname><forenames>Emanuel</forenames></author></authors><title>When you talk about &quot;Information processing&quot; what actually do you have
  in mind?</title><categories>cs.AI q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Information Processing&quot; is a recently launched buzzword whose meaning is
vague and obscure even for the majority of its users. The reason for this is
the lack of a suitable definition for the term &quot;information&quot;. In my attempt to
amend this bizarre situation, I have realized that, following the insights of
Kolmogorov's Complexity theory, information can be defined as a description of
structures observable in a given data set. Two types of structures could be
easily distinguished in every data set - in this regard, two types of
information (information descriptions) should be designated: physical
information and semantic information. Kolmogorov's theory also posits that the
information descriptions should be provided as a linguistic text structure.
This inevitably leads us to an assertion that information processing has to be
seen as a kind of text processing. The idea is not new - inspired by the
observation that human information processing is deeply rooted in natural
language handling customs, Lotfi Zadeh and his followers have introduced the
so-called &quot;Computing With Words&quot; paradigm. Despite of promotional efforts, the
idea is not taking off yet. The reason - a lack of a coherent understanding of
what should be called &quot;information&quot;, and, as a result, misleading research
roadmaps and objectives. I hope my humble attempt to clarify these issues would
be helpful in avoiding common traps and pitfalls.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4155</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4155</id><created>2013-01-17</created><updated>2014-12-13</updated><authors><author><keyname>Weng</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Djuric</keyname><forenames>Petar</forenames></author></authors><title>A Search-free DOA Estimation Algorithm for Coprime Arrays</title><categories>cs.IT math.IT stat.CO</categories><comments>final version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, coprime arrays have been in the focus of research because of their
potential in exploiting redundancy in spanning large apertures with fewer
elements than suggested by theory. A coprime array consists of two uniform
linear subarrays with inter-element spacings $M\lambda/2$ and $N\lambda/2$,
where $M$ and $N$ are coprime integers and $\lambda$ is the wavelength of the
signal. In this paper, we propose a fast search-free method for
direction-of-arrival (DOA) estimation with coprime arrays. It is based on the
use of methods that operate on the uniform linear subarrays of the coprime
array and that enjoy many processing advantages. We first estimate the DOAs for
each uniform linear subarray separately and then combine the estimates from the
subarrays. For combining the estimates, we propose a method that projects the
estimated point in the two-dimensional plane onto one-dimensional line segments
that correspond to the entire angular domain. By doing so, we avoid the search
step and consequently, we greatly reduce the computational complexity of the
method. We demonstrate the performance of the method with computer simulations
and compare it with that of the FD-root MUSIC method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4157</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4157</id><created>2013-01-17</created><authors><author><keyname>Cicconet</keyname><forenames>Marcelo</forenames></author></authors><title>On the Product Rule for Classification Problems</title><categories>cs.LG cs.CV stat.ML</categories><comments>3 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss theoretical aspects of the product rule for classification
problems in supervised machine learning for the case of combining classifiers.
We show that (1) the product rule arises from the MAP classifier supposing
equivalent priors and conditional independence given a class; (2) under some
conditions, the product rule is equivalent to minimizing the sum of the squared
distances to the respective centers of the classes related with different
features, such distances being weighted by the spread of the classes; (3)
observing some hypothesis, the product rule is equivalent to concatenating the
vectors of features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4168</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4168</id><created>2013-01-17</created><updated>2013-03-15</updated><authors><author><keyname>Bornn</keyname><forenames>Luke</forenames></author><author><keyname>Chen</keyname><forenames>Yutian</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author><author><keyname>Eskelin</keyname><forenames>Mareija</forenames></author><author><keyname>Fang</keyname><forenames>Jing</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Herded Gibbs Sampling</title><categories>cs.LG stat.CO stat.ML</categories><comments>19 pages, including the appendix. Submission for ICLR 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Gibbs sampler is one of the most popular algorithms for inference in
statistical models. In this paper, we introduce a herding variant of this
algorithm, called herded Gibbs, that is entirely deterministic. We prove that
herded Gibbs has an $O(1/T)$ convergence rate for models with independent
variables and for fully connected probabilistic graphical models. Herded Gibbs
is shown to outperform Gibbs in the tasks of image denoising with MRFs and
named entity recognition with CRFs. However, the convergence for herded Gibbs
for sparsely connected probabilistic graphical models is still an open problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4171</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4171</id><created>2013-01-17</created><authors><author><keyname>Weston</keyname><forenames>Jason</forenames></author><author><keyname>Weiss</keyname><forenames>Ron</forenames></author><author><keyname>Yee</keyname><forenames>Hector</forenames></author></authors><title>Affinity Weighted Embedding</title><categories>cs.IR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supervised (linear) embedding models like Wsabie and PSI have proven
successful at ranking, recommendation and annotation tasks. However, despite
being scalable to large datasets they do not take full advantage of the extra
data due to their linear nature, and typically underfit. We propose a new class
of models which aim to provide improved performance while retaining many of the
benefits of the existing class of embedding models. Our new approach works by
iteratively learning a linear embedding model where the next iteration's
features and labels are reweighted as a function of the previous iteration. We
describe several variants of the family, and give some initial results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4177</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4177</id><created>2013-01-17</created><authors><author><keyname>Tomic</keyname><forenames>Ratko V.</forenames></author></authors><title>Network Throughput Optimization via Error Correcting Codes</title><categories>cs.IT cs.DM cs.NI math.IT</categories><comments>45 pages</comments><msc-class>94C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new network construction method is presented for building of scalable, high
throughput, low latency networks. The method is based on the exact equivalence
discovered between the problem of maximizing network throughput (measured as
bisection bandwidth) for a large class of practically interesting Cayley graphs
and the problem of maximizing codeword distance for linear error correcting
codes. Since the latter problem belongs to a more mature research field with
large collections of optimal solutions available, a simple translation recipe
is provided for converting the existent optimal error correcting codes into
optimal throughput networks. The resulting networks, called here Long Hop
networks, require 1.5-5 times fewer switches, 2-6 times fewer internal cables
and 1.2-2 times fewer `average hops' than the best presently known networks for
the same number of ports provided and the same total throughput. These
advantage ratios increase with the network size and switch radix. Independently
interesting byproduct of the discovered equivalence is an efficient O(n*log(n))
algorithm based on Walsh-Hadamard transform for computing exact bisections of
this class of Cayley graphs (this is NP complete problem for general graphs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4184</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4184</id><created>2013-01-17</created><authors><author><keyname>Piemontese</keyname><forenames>Amina</forenames></author><author><keyname>Modenini</keyname><forenames>Andrea</forenames></author><author><keyname>Colavolpe</keyname><forenames>Giulio</forenames></author><author><keyname>Alagha</keyname><forenames>Nader</forenames></author></authors><title>Improving the Spectral Efficiency of Nonlinear Satellite Systems through
  Time-Frequency Packing and Advanced Processing</title><categories>cs.IT math.IT</categories><comments>8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider realistic satellite communications systems for broadband and
broadcasting applications, based on frequency-division-multiplexed linear
modulations, where spectral efficiency is one of the main figures of merit. For
these systems, we investigate their ultimate performance limits by using a
framework to compute the spectral efficiency when suboptimal receivers are
adopted and evaluating the performance improvements that can be obtained
through the adoption of the time-frequency packing technique. Our analysis
reveals that introducing controlled interference can significantly increase the
efficiency of these systems. Moreover, if a receiver which is able to account
for the interference and the nonlinear impairments is adopted, rather than a
classical predistorter at the transmitter coupled with a simpler receiver, the
benefits in terms of spectral efficiency can be even larger. Finally, we
consider practical coded schemes and show the potential advantages of the
optimized signaling formats when combined with iterative detection/decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4185</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4185</id><created>2013-01-17</created><authors><author><keyname>Haghighatshoar</keyname><forenames>Saeid</forenames></author><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author><author><keyname>Telatar</keyname><forenames>Emre</forenames></author></authors><title>A new entropy power inequality for integer-valued random variables</title><categories>cs.IT math.IT</categories><comments>10 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The entropy power inequality (EPI) provides lower bounds on the differential
entropy of the sum of two independent real-valued random variables in terms of
the individual entropies. Versions of the EPI for discrete random variables
have been obtained for special families of distributions with the differential
entropy replaced by the discrete entropy, but no universal inequality is known
(beyond trivial ones). More recently, the sumset theory for the entropy
function provides a sharp inequality $H(X+X')-H(X)\geq 1/2 -o(1)$ when $X,X'$
are i.i.d. with high entropy. This paper provides the inequality $H(X+X')-H(X)
\geq g(H(X))$, where $X,X'$ are arbitrary i.i.d. integer-valued random
variables and where $g$ is a universal strictly positive function on $\mR_+$
satisfying $g(0)=0$. Extensions to non identically distributed random variables
and to conditional entropies are also obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4192</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4192</id><created>2013-01-17</created><updated>2013-04-30</updated><authors><author><keyname>Moriano</keyname><forenames>P.</forenames></author><author><keyname>Finke</keyname><forenames>J.</forenames></author></authors><title>On the formation of structure in growing networks</title><categories>physics.soc-ph cs.SI</categories><comments>17 pages, 9 figures, we apply the proposed mechanism to generate
  network realizations that resemble the degree distribution and clustering
  properties of an empirical network with no directed cycles (i.e., when the
  model parameter n=0), updated references</comments><msc-class>90B15, 91D30, 94C15</msc-class><journal-ref>Journal of Statistical Mechanics: Theory and Experiment 2013 (06),
  P06010</journal-ref><doi>10.1088/1742-5468/2013/06/P06010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the formation of triad junctions, the proposed mechanism generates
networks that exhibit extended rather than single power law behavior. Triad
formation guarantees strong neighborhood clustering and community-level
characteristics as the network size grows to infinity. The asymptotic behavior
is of interest in the study of directed networks in which (i) the formation of
links cannot be described according to the principle of preferential
attachment; (ii) the in-degree distribution fits a power law for nodes with a
high degree and an exponential form otherwise; (iii) clustering properties
emerge at multiple scales and depend on both the number of links that newly
added nodes establish and the probability of forming triads; and (iv) groups of
nodes form modules that feature less links to the rest of the nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4194</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4194</id><created>2013-01-17</created><authors><author><keyname>Dangi</keyname><forenames>Ankit</forenames></author></authors><title>Financial Portfolio Optimization: Computationally guided agents to
  investigate, analyse and invest!?</title><categories>q-fin.PM cs.CE cs.NE q-fin.CP stat.ML</categories><comments>Thesis work under the guidance of Dr. Abhijit Kulkarni, Advanced
  Analytics Lab. (SSO), SAS Research &amp; Development, India. Submitted at Centre
  for Modeling and Simulation, University of Pune for completion of Master of
  Technology (M. Tech.) in Modeling and Simulation (M&amp;S)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Financial portfolio optimization is a widely studied problem in mathematics,
statistics, financial and computational literature. It adheres to determining
an optimal combination of weights associated with financial assets held in a
portfolio. In practice, it faces challenges by virtue of varying math.
formulations, parameters, business constraints and complex financial
instruments. Empirical nature of data is no longer one-sided; thereby
reflecting upside and downside trends with repeated yet unidentifiable cyclic
behaviours potentially caused due to high frequency volatile movements in asset
trades. Portfolio optimization under such circumstances is theoretically and
computationally challenging. This work presents a novel mechanism to reach an
optimal solution by encoding a variety of optimal solutions in a solution bank
to guide the search process for the global investment objective formulation. It
conceptualizes the role of individual solver agents that contribute optimal
solutions to a bank of solutions, a super-agent solver that learns from the
solution bank, and, thus reflects a knowledge-based computationally guided
agents approach to investigate, analyse and reach to optimal solution for
informed investment decisions.
  Conceptual understanding of classes of solver agents that represent varying
problem formulations and, mathematically oriented deterministic solvers along
with stochastic-search driven evolutionary and swarm-intelligence based
techniques for optimal weights are discussed. Algorithmic implementation is
presented by an enhanced neighbourhood generation mechanism in Simulated
Annealing algorithm. A framework for inclusion of heuristic knowledge and human
expertise from financial literature related to investment decision making
process is reflected via introduction of controlled perturbation strategies
using a decision matrix for neighbourhood generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4200</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4200</id><created>2013-01-17</created><authors><author><keyname>Hueske</keyname><forenames>Fabian</forenames></author><author><keyname>Krettek</keyname><forenames>Aljoscha</forenames></author><author><keyname>Tzoumas</keyname><forenames>Kostas</forenames></author></authors><title>Enabling Operator Reordering in Data Flow Programs Through Static Code
  Analysis</title><categories>cs.DB cs.DC cs.PL</categories><comments>4 pages, accepted and presented at the First International Workshop
  on Cross-model Language Design and Implementation (XLDI), affiliated with
  ICFP 2012, Copenhagen</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many massively parallel data management platforms, programs are
represented as small imperative pieces of code connected in a data flow. This
popular abstraction makes it hard to apply algebraic reordering techniques
employed by relational DBMSs and other systems that use an algebraic
programming abstraction. We present a code analysis technique based on reverse
data and control flow analysis that discovers a set of properties from user
code, which can be used to emulate algebraic optimizations in this setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4204</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4204</id><created>2013-01-17</created><authors><author><keyname>Shakya</keyname><forenames>Rajeev K.</forenames></author><author><keyname>Agarwal</keyname><forenames>Satyam</forenames></author><author><keyname>Singh</keyname><forenames>Y. N.</forenames></author><author><keyname>Verma</keyname><forenames>Nishchal K.</forenames></author><author><keyname>Roy</keyname><forenames>Amitabha</forenames></author></authors><title>DSAT-MAC : Dynamic Slot Allocation based TDMA MAC protocol for Cognitive
  Radio Networks</title><categories>cs.NI cs.DC</categories><comments>19 pages, 20 figures, Initial work in proc. of the Ninth IEEE
  International Conference on Wireless and Optical Communications Networks
  (IEEE WOCN-2012), Indore, INDIA, 20-22 September, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive Radio Networks (CRN) have enabled us to efficiently reuse the
underutilized radio spectrum. The MAC protocol in CRN defines the spectrum
usage by sharing the channels efficiently among users. In this paper we propose
a novel TDMA based MAC protocol with dynamically allocated slots. Most of the
MAC protocols proposed in the literature employ Common Control Channel (CCC) to
manage the resources among Cognitive Radio (CR) users. Control channel
saturation in case of large number of CR users is one of the main drawbacks of
the CCC based MAC protocols. In contrast with CCC based MAC protocols, DSAT-MAC
protocol is based on the TDMA mechanism, without using any CCC for control
information exchange. The channels are divided into time slots and CR users
send their control or data packets over their designated slot. The protocol
ensures that no slot is left vacant. This guarantees full use of the available
spectrum. The protocol includes the provision for Quality of Service, where
real-time and safety critical data is transmitted with highest priority and
least delay. The protocol also ensures a fair sharing of available spectrum
among the CR users, with the mechanism to regulate the transmission of
malicious nodes. Energy saving techniques are also presented for longer life of
battery operated CR nodes. Theoretical analysis and simulations over ns-2 of
the proposed protocol reveal that the protocol performs better in various CR
adhoc network applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4211</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4211</id><created>2013-01-17</created><authors><author><keyname>Perevalov</keyname><forenames>Eugene</forenames></author><author><keyname>Grace</keyname><forenames>David</forenames></author></authors><title>Information-related complexity: a problem-oriented approach</title><categories>physics.data-an cs.IT math.IT</categories><comments>20 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general notion of information-related complexity applicable to both natural
and man-made systems is proposed. The overall approach is to explicitly
consider a rational agent performing a certain task with a quantifiable degree
of success. The complexity is defined as the minimum (quasi-)quantity of
information that's necessary to complete the task to the given extent --
measured by the corresponding loss. The complexity so defined is shown to
generalize the existing notion of statistical complexity when the system in
question can be described by a discrete-time stochastic process. The proposed
definition also applies, in particular, to optimization and decision making
problems under uncertainty in which case it gives the agent a useful measure of
the problem's &quot;susceptibility&quot; to additional information and allows for an
estimation of the potential value of the latter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4231</identifier>
 <datestamp>2013-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4231</id><created>2013-01-17</created><updated>2013-05-17</updated><authors><author><keyname>Briat</keyname><forenames>Corentin</forenames></author></authors><title>Convex conditions for robust stabilization of uncertain switched systems
  with guaranteed minimum dwell-time</title><categories>math.OC cs.SY math.CA math.DS</categories><comments>Paper withdrawn by the author</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Paper withdrawn by the author
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4240</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4240</id><created>2013-01-17</created><updated>2014-02-03</updated><authors><author><keyname>Javanmard</keyname><forenames>Adel</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>Hypothesis Testing in High-Dimensional Regression under the Gaussian
  Random Design Model: Asymptotic Theory</title><categories>stat.ME cs.IT math.IT math.ST stat.ML stat.TH</categories><comments>63 pages, 10 figures, 11 tables, Section 5 and Theorem 4.5 are added.
  Other modifications to improve presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider linear regression in the high-dimensional regime where the number
of observations $n$ is smaller than the number of parameters $p$. A very
successful approach in this setting uses $\ell_1$-penalized least squares
(a.k.a. the Lasso) to search for a subset of $s_0&lt; n$ parameters that best
explain the data, while setting the other parameters to zero. Considerable
amount of work has been devoted to characterizing the estimation and model
selection problems within this approach.
  In this paper we consider instead the fundamental, but far less understood,
question of \emph{statistical significance}. More precisely, we address the
problem of computing p-values for single regression coefficients.
  On one hand, we develop a general upper bound on the minimax power of tests
with a given significance level. On the other, we prove that this upper bound
is (nearly) achievable through a practical procedure in the case of random
design matrices with independent entries. Our approach is based on a debiasing
of the Lasso estimator. The analysis builds on a rigorous characterization of
the asymptotic distribution of the Lasso estimator and its debiased version.
Our result holds for optimal sample size, i.e., when $n$ is at least on the
order of $s_0 \log(p/s_0)$.
  We generalize our approach to random design matrices with i.i.d. Gaussian
rows $x_i\sim N(0,\Sigma)$. In this case we prove that a similar distributional
characterization (termed `standard distributional limit') holds for $n$ much
larger than $s_0(\log p)^2$.
  Finally, we show that for optimal sample size, $n$ being at least of order
$s_0 \log(p/s_0)$, the standard distributional limit for general Gaussian
designs can be derived from the replica heuristics in statistical physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4258</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4258</id><created>2013-01-17</created><authors><author><keyname>Lasseter</keyname><forenames>John</forenames></author><author><keyname>Cipriano</keyname><forenames>John</forenames></author></authors><title>Design Pattern-Based Extension of Class Hierarchies to Support Runtime
  Invariant Checks</title><categories>cs.PL cs.SE</categories><comments>To appear in FASE 2013 proceedings. The final publication is
  available at http://www.springerlink.com</comments><acm-class>D.1.5; D.2.4; D.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a technique for automatically weaving structural invariant checks
into an existing collection of classes. Using variations on existing design
patterns, we use a concise specification to generate from this collection a new
set of classes that implement the interfaces of the originals, but with the
addition of user-specified class invariant checks. Our work is notable in the
scarcity of assumptions made. Unlike previous design pattern approaches to this
problem, our technique requires no modification of the original source code,
relies only on single inheritance, and does not require that the attributes
used in the checks be publicly visible. We are able to instrument a wide
variety of class hierarchies, including those with pure interfaces, abstract
classes and classes with type parameters. We have implemented the construction
as an Eclipse plug-in for Java development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4269</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4269</id><created>2013-01-17</created><updated>2013-01-21</updated><authors><author><keyname>Apon</keyname><forenames>Daniel</forenames></author><author><keyname>Katz</keyname><forenames>Jonathan</forenames></author><author><keyname>Malozemoff</keyname><forenames>Alex J.</forenames></author></authors><title>One-Round Multi-Party Communication Complexity of Distinguishing Sums</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an instance of the following problem: Parties P_1,..., P_k each
receive an input x_i, and a coordinator (distinct from each of these parties)
wishes to compute f(x_1,..., x_k) for some predicate f. We are interested in
one-round protocols where each party sends a single message to the coordinator;
there is no communication between the parties themselves. What is the minimum
communication complexity needed to compute f, possibly with bounded error?
  We prove tight bounds on the one-round communication complexity when f
corresponds to the promise problem of distinguishing sums (namely, determining
which of two possible values the {x_i} sum to) or the problem of determining
whether the {x_i} sum to a particular value. Similar problems were studied
previously by Nisan and in concurrent work by Viola. Our proofs rely on basic
theorems from additive combinatorics, but are otherwise elementary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4272</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4272</id><created>2013-01-17</created><authors><author><keyname>Correia</keyname><forenames>Marco</forenames></author><author><keyname>Barahona</keyname><forenames>Pedro</forenames></author></authors><title>View-based propagation of decomposable constraints</title><categories>cs.AI</categories><comments>The final publication is available at link.springer.com</comments><doi>10.1007/s10601-013-9140-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constraints that may be obtained by composition from simpler constraints are
present, in some way or another, in almost every constraint program. The
decomposition of such constraints is a standard technique for obtaining an
adequate propagation algorithm from a combination of propagators designed for
simpler constraints. The decomposition approach is appealing in several ways.
Firstly because creating a specific propagator for every constraint is clearly
infeasible since the number of constraints is infinite. Secondly, because
designing a propagation algorithm for complex constraints can be very
challenging. Finally, reusing existing propagators allows to reduce the size of
code to be developed and maintained. Traditionally, constraint solvers
automatically decompose constraints into simpler ones using additional
auxiliary variables and propagators, or expect the users to perform such
decomposition themselves, eventually leading to the same propagation model. In
this paper we explore views, an alternative way to create efficient propagators
for such constraints in a modular, simple and correct way, which avoids the
introduction of auxiliary variables and propagators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4287</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4287</id><created>2013-01-17</created><authors><author><keyname>Lee</keyname><forenames>Hyang-Won</forenames></author><author><keyname>Lee</keyname><forenames>Kayi</forenames></author><author><keyname>Modiano</keyname><forenames>Eytan</forenames></author></authors><title>Maximizing Reliability in WDM Networks through Lightpath Routing</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the reliability maximization problem in WDM networks with random
link failures. Reliability in these networks is defined as the probability that
the logical network is connected, and it is determined by the underlying
lightpath routing, network topologies and the link failure probability. By
introducing the notion of lexicographical ordering for lightpath routings, we
characterize precise optimization criteria for maximum reliability in the low
failure probability regime. Based on the optimization criteria, we develop
lightpath routing algorithms that maximize the reliability, and logical
topology augmentation algorithms for further improving reliability. We also
study the reliability maximization problem in the high failure probability
regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4289</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4289</id><created>2013-01-17</created><updated>2013-01-24</updated><authors><author><keyname>Cord&#xf3;n-Franco</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>van Ditmarsch</keyname><forenames>Hans</forenames></author><author><keyname>Fern&#xe1;ndez-Duque</keyname><forenames>David</forenames></author><author><keyname>Soler-Toscano</keyname><forenames>Fernando</forenames></author></authors><title>A geometric protocol for cryptography with cards</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the generalized Russian cards problem, the three players Alice, Bob and
Cath draw a,b and c cards, respectively, from a deck of a+b+c cards. Players
only know their own cards and what the deck of cards is. Alice and Bob are then
required to communicate their hand of cards to each other by way of public
messages. The communication is said to be safe if Cath does not learn the
ownership of any specific card; in this paper we consider a strengthened notion
of safety introduced by Swanson and Stinson which we call k-safety.
  An elegant solution by Atkinson views the cards as points in a finite
projective plane. We propose a general solution in the spirit of Atkinson's,
although based on finite vector spaces rather than projective planes, and call
it the `geometric protocol'. Given arbitrary c,k&gt;0, this protocol gives an
informative and k-safe solution to the generalized Russian cards problem for
infinitely many values of (a,b,c) with b=O(ac). This improves on the collection
of parameters for which solutions are known. In particular, it is the first
solution which guarantees $k$-safety when Cath has more than one card.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4293</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4293</id><created>2013-01-17</created><updated>2013-01-28</updated><authors><author><keyname>Riedel</keyname><forenames>Sebastian</forenames></author><author><keyname>Yao</keyname><forenames>Limin</forenames></author><author><keyname>McCallum</keyname><forenames>Andrew</forenames></author></authors><title>Latent Relation Representations for Universal Schemas</title><categories>cs.LG stat.ML</categories><comments>4 pages, ICLR workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional relation extraction predicts relations within some fixed and
finite target schema. Machine learning approaches to this task require either
manual annotation or, in the case of distant supervision, existing structured
sources of the same schema. The need for existing datasets can be avoided by
using a universal schema: the union of all involved schemas (surface form
predicates as in OpenIE, and relations in the schemas of pre-existing
databases). This schema has an almost unlimited set of relations (due to
surface forms), and supports integration with existing structured data (through
the relation types of existing databases). To populate a database of such
schema we present a family of matrix factorization models that predict affinity
between database tuples and relations. We show that this achieves substantially
higher accuracy than the traditional classification approach. More importantly,
by operating simultaneously on relations observed in text and in pre-existing
structured DBs such as Freebase, we are able to reason about unstructured and
structured data in mutually-supporting ways. By doing so our approach
outperforms state-of-the-art distant supervision systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4300</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4300</id><created>2013-01-18</created><authors><author><keyname>Hollmann</keyname><forenames>Henk D. L.</forenames></author></authors><title>Storage codes -- coding rate and repair locality</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in ICNC'13, San Diego, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The {\em repair locality} of a distributed storage code is the maximum number
of nodes that ever needs to be contacted during the repair of a failed node.
Having small repair locality is desirable, since it is proportional to the
number of disk accesses during repair. However, recent publications show that
small repair locality comes with a penalty in terms of code distance or storage
overhead if exact repair is required.
  Here, we first review some of the main results on storage codes under various
repair regimes and discuss the recent work on possible
(information-theoretical) trade-offs between repair locality and other code
parameters like storage overhead and code distance, under the exact repair
regime.
  Then we present some new information theoretical lower bounds on the storage
overhead as a function of the repair locality, valid for all common coding and
repair models. In particular, we show that if each of the $n$ nodes in a
distributed storage system has storage capacity $\ga$ and if, at any time, a
failed node can be {\em functionally} repaired by contacting {\em some} set of
$r$ nodes (which may depend on the actual state of the system) and downloading
an amount $\gb$ of data from each, then in the extreme cases where $\ga=\gb$ or
$\ga = r\gb$, the maximal coding rate is at most $r/(r+1)$ or 1/2, respectively
(that is, the excess storage overhead is at least $1/r$ or 1, respectively).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4313</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4313</id><created>2013-01-18</created><updated>2013-04-21</updated><authors><author><keyname>Bostan</keyname><forenames>Alin</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Lairez</keyname><forenames>Pierre</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Salvy</keyname><forenames>Bruno</forenames><affiliation>Inria Grenoble Rh&#xf4;ne-Alpes / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme</affiliation></author></authors><title>Creative telescoping for rational functions using the Griffiths-Dwork
  method</title><categories>cs.SC</categories><proxy>ccsd</proxy><journal-ref>Proceedings of ISSAC 2013, ACM, pp 93-100</journal-ref><doi>10.1145/2465506.2465935</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Creative telescoping algorithms compute linear differential equations
satisfied by multiple integrals with parameters. We describe a precise and
elementary algorithmic version of the Griffiths-Dwork method for the creative
telescoping of rational functions. This leads to bounds on the order and degree
of the coefficients of the differential equation, and to the first complexity
result which is simply exponential in the number of variables. One of the
important features of the algorithm is that it does not need to compute
certificates. The approach is vindicated by a prototype implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4315</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4315</id><created>2013-01-18</created><authors><author><keyname>Liu</keyname><forenames>Yi</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Chen</keyname><forenames>Jiming</forenames></author><author><keyname>Cao</keyname><forenames>Xianghui</forenames></author></authors><title>A Scalable Hybrid MAC Protocol for Massive M2M Networks</title><categories>cs.NI</categories><journal-ref>IEEE WCNC 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Machine to Machine (M2M) networks, a robust Medium Access Control (MAC)
protocol is crucial to enable numerous machine-type devices to concurrently
access the channel. Most literatures focus on developing simplex (reservation
or contention based)MAC protocols which cannot provide a scalable solution for
M2M networks with large number of devices. In this paper, a frame-based Hybrid
MAC scheme, which consists of a contention period and a transmission period, is
proposed for M2M networks. In the proposed scheme, the devices firstly contend
the transmission opportunities during the contention period, only the
successful devices will be assigned a time slot for transmission during the
transmission period. To balance the tradeoff between the contention and
transmission period in each frame, an optimization problem is formulated to
maximize the system throughput by finding the optimal contending probability
during contention period and optimal number of devices that can transmit during
transmission period. A practical hybrid MAC protocol is designed to implement
the proposed scheme. The analytical and simulation results demonstrate the
effectiveness of the proposed Hybrid MAC protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4334</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4334</id><created>2013-01-18</created><authors><author><keyname>Sottile</keyname><forenames>Matthew J.</forenames></author><author><keyname>Hulette</keyname><forenames>Geoffrey C.</forenames></author></authors><title>Deriving program transformations by demonstration</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic code transformation in which transformations are tuned for specific
applications and contexts are difficult to achieve in an accessible manner. In
this paper, we present an approach to build application specific code
transformations. Our approach is based on analysis of the abstract syntax
representation of exemplars of the essential change to the code before and
after the transformation is applied. This analysis entails a sequence of steps
to identify the change, determine how to generalize it, and map it to term
rewriting rules for the Stratego term rewriting system. The methods described
in this paper assume programs are represented in a language-neutral term
format, allowing tools based on our methods to be applied to programs written
in the major languages used by computational scientists utilizing high
performance computing systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4337</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4337</id><created>2013-01-18</created><updated>2013-01-22</updated><authors><author><keyname>Pandya</keyname><forenames>Mahimn</forenames></author><author><keyname>Joshi</keyname><forenames>Hiren</forenames></author><author><keyname>Jani</keyname><forenames>Ashish</forenames></author></authors><title>A Novel Digital Watermarking Algorithm using Random Matrix Image</title><categories>cs.MM cs.CR</categories><comments>4 pages, 8 figures</comments><journal-ref>International Journal of Computer Applications, Volume 61, Number
  2, pp. 18-12, 2013</journal-ref><doi>10.5120/9900-4481</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The availability of bandwidth for internet access is sufficient enough to
communicate digital assets. These digital assets are subjected to various types
of threats. [19] As a result of this, protection mechanism required for the
protection of digital assets is of priority in research. The threat of current
focus is unauthorized copying of digital assets which give boost to piracy.
This under the copyright act is illegal and a robust mechanism is required to
curb this kind of unauthorized copy. To safeguard the copyright digital assets,
a robust digital watermarking technique is needed. The existing digital
watermarking techniques protect digital assets by embedding a digital watermark
into a host digital image. This embedding does induce slight distortion in the
host image but the distortion is usually too small to be noticed. At the same
time the embedded watermark must be robust enough to with stand deliberate
attacks. There are various techniques of digital watermarking but researchers
are making constant efforts to increase the robustness of the watermark image.
The layered approach of watermarking based on Huffman coding [5] can soon
increase the robustness of digital watermark.[11] Ultimately, increasing the
security of copyright of protection. The proposed work is in similar direction
where in RMI (Random Matrix Image) is used in place of Huffman coding. This
innovative algorithm has considerably increased the robustness in digital
watermark while also enhancing security of production
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4351</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4351</id><created>2013-01-18</created><authors><author><keyname>Bouneffouf</keyname><forenames>Djallel</forenames></author></authors><title>Applying machine learning techniques to improve user acceptance on
  ubiquitous environement</title><categories>cs.IR cs.AI</categories><report-no>urn:nbn:de:0074-731-7 Vol-731</report-no><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ubiquitous information access becomes more and more important nowadays and
research is aimed at making it adapted to users. Our work consists in applying
machine learning techniques in order to adapt the information access provided
by ubiquitous systems to users when the system only knows the user social
group, without knowing anything about the user interest. The adaptation
procedures associate actions to perceived situations of the user. Associations
are based on feedback given by the user as a reaction to the behavior of the
system. Our method brings a solution to some of the problems concerning the
acceptance of the system by users when applying machine learning techniques to
systems at the beginning of the interaction between the system and the user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4356</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4356</id><created>2013-01-18</created><authors><author><keyname>Felhi</keyname><forenames>Fa&#xee;&#xe7;al</forenames></author><author><keyname>Akaichi</keyname><forenames>Jalel</forenames></author></authors><title>A new approach towards the self-adaptability of Service-Oriented
  Architectures to the context based on workflow</title><categories>cs.SE</categories><comments>6 pages,4 Figures. arXiv admin note: substantial text overlap with
  arXiv:1211.4867; and overlap with arXiv:1203.0400 by other authors</comments><journal-ref>(IJACSA) International Journal of Advanced Computer Science and
  Applications, Vol. 3, No. 12, 2012 U.S ISSN : 2156-5570(Online)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed information systems are needed to be autonomous, heterogeneous
and adaptable to the context. This is the reason why they resort Web services
based on SOA Based on the advanced technology of SOA. These technologies can
evolve in a dynamic environment in a well-defined context and according to
events automatically, such as time, temperature, location, authentication,
etc... This is what we call self-adaptability to context. In this paper, we are
interested in improving the different needs of this criterion and we propose a
new approach towards a self-adaptability of SOA to the context based on
workflow and showing the feasibility of this approach by integration the
workflow under a platform and test this integration by a case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4372</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4372</id><created>2013-01-18</created><authors><author><keyname>S&#xe1;nchez</keyname><forenames>C&#xe9;sar</forenames></author><author><keyname>S&#xe1;nchez</keyname><forenames>Alejandro</forenames></author></authors><title>A Decidable Theory of Skiplists of Unbounded Size and Arbitrary Height</title><categories>cs.LO</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a theory of skiplists of arbitrary height, and shows
decidability of the satisfiability problem for quantifier-free formulas.
  A skiplist is an imperative software data structure that implements sets by
maintaining several levels of ordered singly-linked lists in memory, where each
level is a sublist of its lower levels. Skiplists are widely used in practice
because they offer a performance comparable to balanced binary trees, and can
be implemented more efficiently. To achieve this performance, most
implementations dynamically increment the height (the number of levels).
Skiplists are difficult to reason about because of the dynamic size (number of
nodes) and the sharing between the different layers. Furthermore, reasoning
about dynamic height adds the challenge of dealing with arbitrary many levels.
  The first contribution of this paper is the theory TSL that allows to express
the heap memory layout of a skiplist of arbitrary height. The second
contribution is a decision procedure for the satisfiability prob- lem of
quantifier-free TSL formulas. The last contribution is to illustrate the formal
verification of a practical skiplist implementation using this decision
procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4377</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4377</id><created>2013-01-18</created><authors><author><keyname>Mahjoub</keyname><forenames>Mohamed Ali</forenames></author><author><keyname>Ghanmy</keyname><forenames>Nabil</forenames></author><author><keyname>jayech</keyname><forenames>Khlifia</forenames></author><author><keyname>Miled</keyname><forenames>Ikram</forenames></author></authors><title>Multiple models of Bayesian networks applied to offline recognition of
  Arabic handwritten city names</title><categories>cs.CV</categories><comments>arXiv admin note: substantial text overlap with arXiv:1204.1679</comments><report-no>ISSN 0974-0627</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of offline Arabic handwriting word
recognition. Off-line recognition of handwritten words is a difficult task due
to the high variability and uncertainty of human writing. The majority of the
recent systems are constrained by the size of the lexicon to deal with and the
number of writers. In this paper, we propose an approach for multi-writers
Arabic handwritten words recognition using multiple Bayesian networks. First,
we cut the image in several blocks. For each block, we compute a vector of
descriptors. Then, we use K-means to cluster the low-level features including
Zernik and Hu moments. Finally, we apply four variants of Bayesian networks
classifiers (Na\&quot;ive Bayes, Tree Augmented Na\&quot;ive Bayes (TAN), Forest
Augmented Na\&quot;ive Bayes (FAN) and DBN (dynamic bayesian network) to classify
the whole image of tunisian city name. The results demonstrate FAN and DBN
outperform good recognition rates
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4394</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4394</id><created>2013-01-16</created><authors><author><keyname>Odhner</keyname><forenames>Lael U.</forenames></author><author><keyname>Jentoft</keyname><forenames>Leif P.</forenames></author><author><keyname>Claffee</keyname><forenames>Mark R.</forenames></author><author><keyname>Corson</keyname><forenames>Nicholas</forenames></author><author><keyname>Tenzer</keyname><forenames>Yaroslav</forenames></author><author><keyname>Ma</keyname><forenames>Raymond R.</forenames></author><author><keyname>Buehler</keyname><forenames>Martin</forenames></author><author><keyname>Kohout</keyname><forenames>Robert</forenames></author><author><keyname>Howe</keyname><forenames>Robert D.</forenames></author><author><keyname>Dollar</keyname><forenames>Aaron M.</forenames></author></authors><title>A Compliant, Underactuated Hand for Robust Manipulation</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the i-HY Hand, an underactuated hand driven by 5
actuators that is capable of performing a wide range of grasping and in-hand
manipulation tasks. This hand was designed to address the need for a durable,
inexpensive, moderately dexterous hand suitable for use on mobile robots. The
primary focus of this paper will be on the novel minimalistic design of i-HY,
which was developed by choosing a set of target tasks around which the design
of the hand was optimized. Particular emphasis is placed on the development of
underactuated fingers that are capable of both firm power grasps and low-
stiffness fingertip grasps using only the passive mechanics of the finger
mechanism. Experimental results demonstrate successful grasping of a wide range
of target objects, the stability of fingertip grasping, as well as the ability
to adjust the force exerted on grasped objects using the passive finger
mechanics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4397</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4397</id><created>2013-01-18</created><authors><author><keyname>Seidl</keyname><forenames>Mathis</forenames></author><author><keyname>Schenk</keyname><forenames>Andreas</forenames></author><author><keyname>Stierstorfer</keyname><forenames>Clemens</forenames></author><author><keyname>Huber</keyname><forenames>Johannes B.</forenames></author></authors><title>Multilevel Polar-Coded Modulation</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A framework is proposed that allows for a joint description and optimization
of both binary polar coding and the multilevel coding (MLC) approach for
$2^m$-ary digital pulse-amplitude modulation (PAM). The conceptual equivalence
of polar coding and multilevel coding is pointed out in detail. Based on a
novel characterization of the channel polarization phenomenon, rules for the
optimal choice of the bit labeling in this coded modulation scheme employing
polar codes are developed. Simulation results for the AWGN channel are
included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4417</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4417</id><created>2013-01-18</created><authors><author><keyname>Cimini</keyname><forenames>Giulio</forenames></author><author><keyname>Zeng</keyname><forenames>An</forenames></author><author><keyname>Medo</keyname><forenames>Matus</forenames></author><author><keyname>Chen</keyname><forenames>Duanbing</forenames></author></authors><title>The role of taste affinity in agent-based models for social
  recommendation</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Advances in Complex Systems 16 No. 04n05, 1350009 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Internet era, online social media emerged as the main tool for sharing
opinions and information among individuals. In this work we study an adaptive
model of a social network where directed links connect users with similar
tastes, and over which information propagates through social recommendation.
Agent-based simulations of two different artificial settings for modeling user
tastes are compared with patterns seen in real data, suggesting that users
differing in their scope of interests is a more realistic assumption than users
differing only in their particular interests. We further introduce an extensive
set of similarity metrics based on users' past assessments, and evaluate their
use in the given social recommendation model with both artificial simulations
and real data. Superior recommendation performance is observed for similarity
metrics that give preference to users with small scope---who thus act as
selective filters in social recommendation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4430</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4430</id><created>2013-01-18</created><authors><author><keyname>Wang</keyname><forenames>Haiqin</forenames></author><author><keyname>Druzdzel</keyname><forenames>Marek J.</forenames></author></authors><title>User Interface Tools for Navigation in Conditional Probability Tables
  and Elicitation of Probabilities in Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2000)</comments><proxy>auai</proxy><report-no>UAI-P-2000-PG-617-625</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Elicitation of probabilities is one of the most laborious tasks in building
decision-theoretic models, and one that has so far received only moderate
attention in decision-theoretic systems. We propose a set of user interface
tools for graphical probabilistic models, focusing on two aspects of
probability elicitation: (1) navigation through conditional probability tables
and (2) interactive graphical assessment of discrete probability distributions.
We propose two new graphical views that aid navigation in very large
conditional probability tables: the CPTree (Conditional Probability Tree) and
the SCPT (shrinkable Conditional Probability Table). Based on what is known
about graphical presentation of quantitative data to humans, we offer several
useful enhancements to probability wheel and bar graph, including different
chart styles and options that can be adapted to user preferences and needs. We
present the results of a simple usability study that proves the value of the
proposed tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4432</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4432</id><created>2013-01-18</created><authors><author><keyname>Hsu</keyname><forenames>Anne S.</forenames><affiliation>Department of Cognitive, Perceptual and Brain Sciences, University College London</affiliation></author><author><keyname>Chater</keyname><forenames>Nick</forenames><affiliation>Behavioural Science Group, Warwick Business School, University of Warwick</affiliation></author><author><keyname>Vit&#xe1;nyi</keyname><forenames>Paul M. B.</forenames><affiliation>CWI, Amsterdam</affiliation></author></authors><title>Language learning from positive evidence, reconsidered: A
  simplicity-based approach</title><categories>cs.CL</categories><comments>39 pages, pdf, 1 figure</comments><journal-ref>A.S. Hsu, N. Chater, P.M.B. Vitanyi, Language learning from
  positive evidence, reconsidered: A simplicity-based approach. Topics in
  Cognitive Science, 5:1(2013), 35-55</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Children learn their native language by exposure to their linguistic and
communicative environment, but apparently without requiring that their mistakes
are corrected. Such learning from positive evidence has been viewed as raising
logical problems for language acquisition. In particular, without correction,
how is the child to recover from conjecturing an over-general grammar, which
will be consistent with any sentence that the child hears? There have been many
proposals concerning how this logical problem can be dissolved. Here, we review
recent formal results showing that the learner has sufficient data to learn
successfully from positive evidence, if it favours the simplest encoding of the
linguistic input. Results include the ability to learn a linguistic prediction,
grammaticality judgements, language production, and form-meaning mappings. The
simplicity approach can also be scaled-down to analyse the ability to learn a
specific linguistic constructions, and is amenable to empirical test as a
framework for describing human language acquisition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4438</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4438</id><created>2013-01-18</created><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK</affiliation></author><author><keyname>Pernet</keyname><forenames>Cl&#xe9;ment</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Sultan</keyname><forenames>Ziad</forenames><affiliation>LJK, INRIA Grenoble Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author></authors><title>Simultaneous computation of the row and column rank profiles</title><categories>cs.NA cs.MS</categories><proxy>ccsd</proxy><journal-ref>ISSAC 2013, Boston, MA : \'Etats-Unis (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian elimination with full pivoting generates a PLUQ matrix
decomposition. Depending on the strategy used in the search for pivots, the
permutation matrices can reveal some information about the row or the column
rank profiles of the matrix. We propose a new pivoting strategy that makes it
possible to recover at the same time both row and column rank profiles of the
input matrix and of any of its leading sub-matrices. We propose a
rank-sensitive and quad-recursive algorithm that computes the latter PLUQ
triangular decomposition of an m \times n matrix of rank r in O(mnr^{\omega-2})
field operations, with \omega the exponent of matrix multiplication. Compared
to the LEU decomposition by Malashonock, sharing a similar recursive structure,
its time complexity is rank sensitive and has a lower leading constant. Over a
word size finite field, this algorithm also improveLs the practical efficiency
of previously known implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4441</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4441</id><created>2013-01-18</created><updated>2013-12-20</updated><authors><author><keyname>Montina</keyname><forenames>A.</forenames></author><author><keyname>Pfaffhauser</keyname><forenames>M.</forenames></author><author><keyname>Wolf</keyname><forenames>S.</forenames></author></authors><title>Communication Complexity of Channels in General Probabilistic Theories</title><categories>quant-ph cs.IT math.IT</categories><comments>Added analytical and numerical calculations for the quantum
  depolarizing channel</comments><journal-ref>Phys. Rev. Lett. 111, 160502 (2013)</journal-ref><doi>10.1103/PhysRevLett.111.160502</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The communication complexity of a quantum channel is the minimal amount of
classical communication required for classically simulating the process of
preparation, transmission through the channel, and subsequent measurement of a
quantum state. At present, only little is known about this quantity. In this
paper, we present a procedure for systematically evaluating the communication
complexity of channels in any general probabilistic theory, in particular
quantum theory. The procedure is constructive and provides the most efficient
classical protocols. We illustrate this procedure by evaluating the
communication complexity of a quantum depolarizing channel with some finite
sets of quantum states and measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4444</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4444</id><created>2013-01-18</created><authors><author><keyname>Savin</keyname><forenames>Valentin</forenames></author><author><keyname>Declercq</keyname><forenames>David</forenames></author></authors><title>Binary Diversity for Non-Binary LDPC Codes over the Rayleigh Channel</title><categories>cs.IT math.IT</categories><comments>5 pages, 7 figures, IEEE Wireless Communications and Networking
  Conference (WCNC), Paris, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze the performance of several bit-interleaving
strategies applied to Non-Binary Low-Density Parity-Check (LDPC) codes over the
Rayleigh fading channel. The technique of bit-interleaving used over fading
channel introduces diversity which could provide important gains in terms of
frame error probability and detection.
  This paper demonstrates the importance of the way of implementing the
bit-interleaving, and proposes a design of an optimized bit-interleaver
inspired from the Progressive Edge Growth algorithm. This optimization
algorithm depends on the topological structure of a given LDPC code and can
also be applied to any degree distribution and code realization.
  In particular, we focus on non-binary LDPC codes based on graph with constant
symbol-node connection $d_v = 2$. These regular $(2,dc)$-NB-LDPC codes
demonstrate best performance, thanks to their large girths and improved
decoding thresholds growing with the order of Finite Field. Simulations show
excellent results of the proposed interleaving technique compared to the random
interleaver as well as to the system without interleaver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4451</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4451</id><created>2013-01-18</created><updated>2013-07-05</updated><authors><author><keyname>Antunes</keyname><forenames>L.</forenames><affiliation>University of Porto</affiliation></author><author><keyname>Souto</keyname><forenames>A.</forenames><affiliation>Technical University of Lissabon</affiliation></author><author><keyname>Teixeira</keyname><forenames>A.</forenames><affiliation>University of Porto</affiliation></author><author><keyname>Vitanyi</keyname><forenames>P. M. B.</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>On the logical depth function</title><categories>cs.CC</categories><comments>11 pages LaTeX; previous version was incorrect, this is a new version
  with almost the same results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a finite binary string $x$ its logical depth $d$ for significance $b$ is
the shortest running time of a program for $x$ of length $K(x)+b$. There is
another definition of logical depth. We give a new proof that the two versions
are close. There is an infinite sequence of strings of consecutive lengths such
that for every string there is a $b$ such that incrementing $b$ by 1 makes the
associated depths go from incomputable to computable. The maximal gap between
depths resulting from incrementing appropriate $b$'s by 1 is incomputable. The
size of this gap is upper bounded by the Busy Beaver function. Both the upper
and the lower bound hold for the depth with significance 0. As a consequence,
the minimal computation time of the associated shortest programs rises faster
than any computable function but not so fast as the Busy Beaver function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4478</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4478</id><created>2013-01-18</created><authors><author><keyname>Ahmadian</keyname><forenames>Sara</forenames></author><author><keyname>Friggstad</keyname><forenames>Zachary</forenames></author><author><keyname>Swamy</keyname><forenames>Chaitanya</forenames></author></authors><title>Local-Search based Approximation Algorithms for Mobile Facility Location
  Problems</title><categories>cs.DS cs.DM</categories><acm-class>F.2.2; G.1.6; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the {\em mobile facility location} (\mfl) problem. We are given a
set of facilities and clients located in a common metric space. The goal is to
move each facility from its initial location to a destination and assign each
client to the destination of some facility so as to minimize the sum of the
movement-costs of the facilities and the client-assignment costs. This
abstracts facility-location settings where one has the flexibility of moving
facilities from their current locations to other destinations so as to serve
clients more efficiently by reducing their assignment costs.
  We give the first {\em local-search based} approximation algorithm for this
problem and achieve the best-known approximation guarantee. Our main result is
$(3+\epsilon)$-approximation for this problem for any constant $\epsilon&gt;0$
using local search. The previous best guarantee was an 8-approximation
algorithm based on LP-rounding. Our guarantee {\em matches} the best-known
approximation guarantee for the $k$-median problem. Since there is an
approximation-preserving reduction from the $k$-median problem to \mfl, any
improvement of our result would imply an analogous improvement for the
$k$-median problem. Furthermore, {\em our analysis is tight} (up to $o(1)$
factors) since the tight example for the local-search based 3-approximation
algorithm for $k$-median can be easily adapted to show that our local-search
algorithm has a tight approximation ratio of 3. One of the chief novelties of
the analysis is that in order to generate a suitable collection of local-search
moves whose resulting inequalities yield the desired bound on the cost of a
local-optimum, we define a tree-like structure that (loosely speaking)
functions as a &quot;recursion tree&quot;, using which we spawn off local-search moves by
exploring this tree to a constant depth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4490</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4490</id><created>2013-01-18</created><authors><author><keyname>Ramesh</keyname><forenames>Bharath</forenames></author><author><keyname>Ribbens</keyname><forenames>Calvin J.</forenames></author><author><keyname>Varadarajan</keyname><forenames>Srinidhi</forenames></author></authors><title>Regional Consistency: Programmability and Performance for
  Non-Cache-Coherent Systems</title><categories>cs.DC</categories><comments>8 pages, 7 figures, 1 table; as submitted to CCGRID 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallel programmers face the often irreconcilable goals of programmability
and performance. HPC systems use distributed memory for scalability, thereby
sacrificing the programmability advantages of shared memory programming models.
Furthermore, the rapid adoption of heterogeneous architectures, often with
non-cache-coherent memory systems, has further increased the challenge of
supporting shared memory programming models. Our primary objective is to define
a memory consistency model that presents the familiar thread-based shared
memory programming model, but allows good application performance on
non-cache-coherent systems, including distributed memory clusters and
accelerator-based systems. We propose regional consistency (RegC), a new
consistency model that achieves this objective. Results on up to 256 processors
for representative benchmarks demonstrate the potential of RegC in the context
of our prototype distributed shared memory system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4499</identifier>
 <datestamp>2013-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4499</id><created>2013-01-18</created><updated>2013-06-05</updated><authors><author><keyname>Selig</keyname><forenames>Marco</forenames></author><author><keyname>Bell</keyname><forenames>Michael R.</forenames></author><author><keyname>Junklewitz</keyname><forenames>Henrik</forenames></author><author><keyname>Oppermann</keyname><forenames>Niels</forenames></author><author><keyname>Reinecke</keyname><forenames>Martin</forenames></author><author><keyname>Greiner</keyname><forenames>Maksim</forenames></author><author><keyname>Pachajoa</keyname><forenames>Carlos</forenames></author><author><keyname>En&#xdf;lin</keyname><forenames>Torsten A.</forenames></author></authors><title>NIFTY - Numerical Information Field Theory - a versatile Python library
  for signal inference</title><categories>astro-ph.IM cs.IT cs.MS math-ph math.IT math.MP physics.data-an stat.CO</categories><comments>9 pages, 3 tables, 4 figures, accepted by Astronomy &amp; Astrophysics;
  refereed version, 1 figure added, results unchanged</comments><doi>10.1051/0004-6361/201321236</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NIFTY, &quot;Numerical Information Field Theory&quot;, is a software package designed
to enable the development of signal inference algorithms that operate
regardless of the underlying spatial grid and its resolution. Its
object-oriented framework is written in Python, although it accesses libraries
written in Cython, C++, and C for efficiency. NIFTY offers a toolkit that
abstracts discretized representations of continuous spaces, fields in these
spaces, and operators acting on fields into classes. Thereby, the correct
normalization of operations on fields is taken care of automatically without
concerning the user. This allows for an abstract formulation and programming of
inference algorithms, including those derived within information field theory.
Thus, NIFTY permits its user to rapidly prototype algorithms in 1D, and then
apply the developed code in higher-dimensional settings of real world problems.
The set of spaces on which NIFTY operates comprises point sets, n-dimensional
regular grids, spherical spaces, their harmonic counterparts, and product
spaces constructed as combinations of those. The functionality and diversity of
the package is demonstrated by a Wiener filter code example that successfully
runs without modification regardless of the space on which the inference
problem is defined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4524</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4524</id><created>2013-01-18</created><authors><author><keyname>Gugercin</keyname><forenames>Serkan</forenames></author><author><keyname>Stykel</keyname><forenames>Tatjana</forenames></author><author><keyname>Wyatt</keyname><forenames>Sarah</forenames></author></authors><title>Model Reduction of Descriptor Systems by Interpolatory Projection
  Methods</title><categories>math.NA cs.SY math.DS</categories><comments>22 pages</comments><msc-class>41A05, 93A15, 93C05, 37M99</msc-class><journal-ref>SIAM Journal on Scientific Computing, Vol. 35, Iss. 5, pp.
  B1010-B1033, 2013</journal-ref><doi>10.1137/130906635</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate interpolatory projection framework for model
reduction of descriptor systems. With a simple numerical example, we first
illustrate that employing subspace conditions from the standard state space
settings to descriptor systems generically leads to unbounded H2 or H-infinity
errors due to the mismatch of the polynomial parts of the full and
reduced-order transfer functions. We then develop modified interpolatory
subspace conditions based on the deflating subspaces that guarantee a bounded
error. For the special cases of index-1 and index-2 descriptor systems, we also
show how to avoid computing these deflating subspaces explicitly while still
enforcing interpolation. The question of how to choose interpolation points
optimally naturally arises as in the standard state space setting. We answer
this question in the framework of the H2-norm by extending the Iterative
Rational Krylov Algorithm (IRKA) to descriptor systems. Several numerical
examples are used to illustrate the theoretical discussion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4529</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4529</id><created>2013-01-18</created><updated>2013-11-26</updated><authors><author><keyname>Mastin</keyname><forenames>Andrew</forenames></author><author><keyname>Jaillet</keyname><forenames>Patrick</forenames></author></authors><title>Average-Case Performance of Rollout Algorithms for Knapsack Problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rollout algorithms have demonstrated excellent performance on a variety of
dynamic and discrete optimization problems. Interpreted as an approximate
dynamic programming algorithm, a rollout algorithm estimates the value-to-go at
each decision stage by simulating future events while following a greedy
policy, referred to as the base policy. While in many cases rollout algorithms
are guaranteed to perform as well as their base policies, there have been few
theoretical results showing additional improvement in performance. In this
paper we perform a probabilistic analysis of the subset sum problem and
knapsack problem, giving theoretical evidence that rollout algorithms perform
strictly better than their base policies. Using a stochastic model from the
existing literature, we analyze two rollout methods that we refer to as the
consecutive rollout and exhaustive rollout, both of which employ a simple
greedy base policy. For the subset sum problem, we prove that after only a
single iteration of the rollout algorithm, both methods yield at least a 30%
reduction in the expected gap between the solution value and capacity, relative
to the base policy. Analogous results are shown for the knapsack problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4535</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4535</id><created>2013-01-19</created><authors><author><keyname>Kageyama</keyname><forenames>Akira</forenames></author><author><keyname>Masada</keyname><forenames>Youhei</forenames></author></authors><title>Applications and a Three-dimensional Desktop Environment for an
  Immersive Virtual Reality System</title><categories>physics.comp-ph cs.GR</categories><comments>13 pages, 15 figures</comments><doi>10.1088/1742-6596/454/1/012077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We developed an application launcher called Multiverse for scientific
visualizations in a CAVE-type virtual reality (VR) system. Multiverse can be
regarded as a type of three-dimensional (3D) desktop environment. In
Multiverse, a user in a CAVE room can browse multiple visualization
applications with 3D icons and explore movies that float in the air. Touching
one of the movies causes &quot;teleportation&quot; into the application's VR space. After
analyzing the simulation data using the application, the user can jump back
into Multiverse's VR desktop environment in the CAVE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4539</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4539</id><created>2013-01-19</created><authors><author><keyname>Cessenat</keyname><forenames>Olivier</forenames><affiliation>CEA CESTA</affiliation></author></authors><title>Sophie, an FDTD code on the way to multicore, getting rid of the memory
  bandwidth bottleneck better using cache</title><categories>cs.DC</categories><comments>21 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FDTD codes, such as Sophie developed at CEA/DAM, no longer take advantage of
the processor's increased computing power, especially recently with the raising
multicore technology. This is rooted in the fact that low order numerical
schemes need an important memory bandwidth to bring and store the computed
fields. The aim of this article is to present a programming method at the
software's architecture level that improves the memory access pattern in order
to reuse data in cache instead of constantly accessing RAM memory. We will
exhibit a more than two computing time improvement in practical applications.
The target audience of this article is made of computing scientists and of
electrical engineers that develop simulation codes with no specific knowledge
in computer science or electronics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4540</identifier>
 <datestamp>2013-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4540</id><created>2013-01-19</created><authors><author><keyname>Vigeral</keyname><forenames>Guillaume</forenames><affiliation>CEREMADE</affiliation></author></authors><title>A Zero-Sum Stochastic Game with Compact Action Sets and no Asymptotic
  Value</title><categories>math.OC cs.GT</categories><proxy>ccsd</proxy><journal-ref>Dynamic Games and Applications 3, 2 (2013) 172-186</journal-ref><doi>10.1007/s13235-013-0073-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an example of a zero-sum stochastic game with four states, compact
action sets for each player, and continuous payoff and transition functions,
such that the discounted value does not converge as the discount factor tends
to 0, and the value of the n-stage game does not converge as n goes to
infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4546</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4546</id><created>2013-01-19</created><updated>2013-09-13</updated><authors><author><keyname>Kageyama</keyname><forenames>Akira</forenames></author><author><keyname>Yamada</keyname><forenames>Tomoki</forenames></author></authors><title>An Approach to Exascale Visualization: Interactive Viewing of In-Situ
  Visualization</title><categories>physics.comp-ph cs.GR</categories><comments>Will appear in Comput. Phys. Comm</comments><doi>10.1016/j.cpc.2013.08.017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the coming era of exascale supercomputing, in-situ visualization will be a
crucial approach for reducing the output data size. A problem of in-situ
visualization is that it loses interactivity if a steering method is not
adopted. In this paper, we propose a new method for the interactive analysis of
in-situ visualization images produced by a batch simulation job. A key idea is
to apply numerous (thousands to millions) in-situ visualizations
simultaneously. The viewer then analyzes the image database interactively
during postprocessing. If each movie can be compressed to 100 MB, one million
movies will only require 100 TB, which is smaller than the size of the raw
numerical data in exascale supercomputing. We performed a feasibility study
using the proposed method. Multiple movie files were produced by a simulation
and they were analyzed using a specially designed movie player. The user could
change the viewing angle, the visualization method, and the parameters
interactively by retrieving an appropriate sequence of images from the movie
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4551</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4551</id><created>2013-01-19</created><authors><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Jain</keyname><forenames>Satbir</forenames></author></authors><title>Decentralized Lifetime Minimizing Tree for Data Aggregation in Wireless
  Sensor Networks</title><categories>cs.NI</categories><comments>12 pages, 4 figures</comments><journal-ref>WORLD ACADEMY OF SCIENCE, ENGINEERING AND TECHNOLOGY ISSUE 28
  APRIL 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To meet the demands of wireless sensor networks (WSNs) where data are usually
aggregated at a single source prior to transmitting to any distant user, there
is a need to establish a tree structure inside any given event region. In this
paper, we propose a novel technique to create one such tree, which preserves
the energy and minimizes the lifetime of event sources while they are
constantly transmitting for data aggregation in future WSNs. We use the term
Decentralized Lifetime-Minimizing Tree (DLMT) to denote this tree. DLMT
features in nodes with higher energy tend to be chosen as data aggregating
parents so that the time to detect the first broken tree link can be extended
and less energy is involved in tree maintenance. In addition, by constructing
the tree in such a way, the protocol is also able to reduce the frequency of
tree reconstruction, minimizes the amount of data loss, minimizes the delay
during data collection and preserves the energy. Forwarded directed Diffusion
protocol is chosen as the routing platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4552</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4552</id><created>2013-01-19</created><updated>2016-01-03</updated><authors><author><keyname>Rhif</keyname><forenames>Ahmed</forenames></author><author><keyname>Kardous</keyname><forenames>Zohra</forenames></author><author><keyname>Braiek</keyname><forenames>Naceur BenHadj</forenames></author></authors><title>Sliding Mode Control for Torque Evolution of a Double Feed Asynchronous
  Generator</title><categories>cs.SY</categories><comments>This paper has been withdrawn by the author because it may be
  reviewed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a robust control of doublefed induction generator of wind
turbine to optimize its production: that means the energy quality and
efficiency. The proposed control reposes in the sliding mode control using a
multimodel approach which contributes on the minimization of the static error
and the chattering phenomenon. This new approach is called sliding mode
multimodel control (SMMC). Simulation results show good performances of this
control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4558</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4558</id><created>2013-01-19</created><authors><author><keyname>Werda</keyname><forenames>Salah</forenames></author><author><keyname>Mahdi</keyname><forenames>Walid</forenames></author><author><keyname>Hamadou</keyname><forenames>Abdelmajid Ben</forenames></author></authors><title>Lip Localization and Viseme Classification for Visual Speech Recognition</title><categories>cs.CV</categories><comments>14 pages</comments><report-no>Vol.5, No.1, pp. 62-75, 2006</report-no><journal-ref>International Journal of Computing and Information Sciences ISSN:
  1708-0460 (print) - 1708-0479 (online) Volume 5, Number 1, December 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need for an automatic lip-reading system is ever increasing. Infact,
today, extraction and reliable analysis of facial movements make up an
important part in many multimedia systems such as videoconference, low
communication systems, lip-reading systems. In addition, visual information is
imperative among people with special needs. We can imagine, for example, a
dependent person ordering a machine with an easy lip movement or by a simple
syllable pronunciation. Moreover, people with hearing problems compensate for
their special needs by lip-reading as well as listening to the person with
whome they are talking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4587</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4587</id><created>2013-01-19</created><authors><author><keyname>Pasqualetti</keyname><forenames>Fabio</forenames></author><author><keyname>Borra</keyname><forenames>Domenica</forenames></author><author><keyname>Bullo</keyname><forenames>Francesco</forenames></author></authors><title>Consensus Networks over Finite Fields</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies consensus strategies for networks of agents with limited
memory, computation, and communication capabilities. We assume that agents can
process only values from a finite alphabet, and we adopt the framework of
finite fields, where the alphabet consists of the integers {0,...,p-1}, for
some prime number p, and operations are performed modulo p. Thus, we define a
new class of consensus dynamics, which can be exploited in certain applications
such as pose estimation in capacity and memory constrained sensor networks. For
consensus networks over finite fields, we provide necessary and sufficient
conditions on the network topology and weights to ensure convergence. We show
that consensus networks over finite fields converge in finite time, a feature
that can be hardly achieved over the field of real numbers. For the design of
finite-field consensus networks, we propose a general design method, with high
computational complexity, and a network composition rule to generate large
consensus networks from smaller components. Finally, we discuss the application
of finite-field consensus networks to distributed averaging and pose estimation
in sensor networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4597</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4597</id><created>2013-01-19</created><updated>2013-04-02</updated><authors><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author><author><keyname>van Eck</keyname><forenames>Nees Jan</forenames></author><author><keyname>Wouters</keyname><forenames>Paul</forenames></author></authors><title>Counting publications and citations: Is more always better?</title><categories>cs.DL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is more always better? We address this question in the context of
bibliometric indices that aim to assess the scientific impact of individual
researchers by counting their number of highly cited publications. We propose a
simple model in which the number of citations of a publication depends not only
on the scientific impact of the publication but also on other 'random' factors.
Our model indicates that more need not always be better. It turns out that the
most influential researchers may have a systematically lower performance, in
terms of highly cited publications, than some of their less influential
colleagues. The model also suggests an improved way of counting highly cited
publications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4600</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4600</id><created>2013-01-19</created><authors><author><keyname>Franch</keyname><forenames>Xavier</forenames></author></authors><title>Requirements Management for Service Providers: the Case of Services for
  Citizens</title><categories>cs.SE cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Take the Internet of Things, a piece of cloud computing, a handful of smart
cities, don't forget social platforms, flavour it with mobile technologies and
ever-changing environments, shake it up and... voila! What a wonderful service!
Oops! Wait a minute, where did my requirements go?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4604</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4604</id><created>2013-01-19</created><updated>2014-08-28</updated><authors><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author></authors><title>Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial
  Intelligence (2012)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI2012</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Twenty-Eighth Conference on Uncertainty in
Artificial Intelligence, which was held on Catalina Island, CA August 14-18
2012.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4606</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4606</id><created>2013-01-19</created><updated>2014-08-28</updated><authors><author><keyname>Meek</keyname><forenames>Christopher</forenames></author><author><keyname>Kjaerulff</keyname><forenames>Uffe</forenames></author></authors><title>Proceedings of the Nineteenth Conference on Uncertainty in Artificial
  Intelligence (2003)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI2003</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Nineteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Acapulco, Mexico, August 7-10 2003
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4607</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4607</id><created>2013-01-19</created><updated>2014-08-28</updated><authors><author><keyname>Breese</keyname><forenames>John</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Proceedings of the Seventeenth Conference on Uncertainty in Artificial
  Intelligence (2001)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI2001</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Seventeenth Conference on Uncertainty in
Artificial Intelligence, which was held in Seattle, WA, August 2-5 2001
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4608</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4608</id><created>2013-01-19</created><updated>2014-08-28</updated><authors><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author><author><keyname>Friedman</keyname><forenames>Nir</forenames></author></authors><title>Proceedings of the Eighteenth Conference on Uncertainty in Artificial
  Intelligence (2002)</title><categories>cs.AI</categories><proxy>Martijn de Jongh</proxy><report-no>UAI2002</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Eighteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Alberta, Canada, August 1-4 2002
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4609</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4609</id><created>2013-01-19</created><authors><author><keyname>Poncet</keyname><forenames>Paul</forenames></author></authors><title>Two-valued sigma-maxitive measures and Mesiar's hypothesis</title><categories>math.FA cs.IT math.IT</categories><comments>3 pages</comments><msc-class>28B15, 03E72, 49J52</msc-class><journal-ref>Fuzzy Sets and Systems 158 (2007) 1843-1845</journal-ref><doi>10.1016/j.fss.2007.04.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We reformulate Mesiar's hypothesis [Possibility measures, integration and
fuzzy possibility measures, Fuzzy Sets and Systems 92 (1997) 191-196], which as
such was shown to be untrue by Murofushi [Two-valued possibility measures
induced by $\sigma$-finite $\sigma$-additive measures, Fuzzy Sets and Systems
126 (2002) 265-268]. We prove that a two-valued $\sigma$-maxitive measure can
be induced by a $\sigma$-additive measure under the additional condition that
it is $\sigma$-principal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4620</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4620</id><created>2013-01-19</created><updated>2014-06-24</updated><authors><author><keyname>Han</keyname><forenames>Yunghsiang</forenames></author><author><keyname>Pai</keyname><forenames>Hung-Ta</forenames></author><author><keyname>Zheng</keyname><forenames>Rong</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Update-Efficient Error-Correcting Product-Matrix Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. on Information Theory. arXiv admin note:
  substantial text overlap with arXiv:1301.2497</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regenerating codes provide an efficient way to recover data at failed nodes
in distributed storage systems. It has been shown that regenerating codes can
be designed to minimize the per-node storage (called MSR) or minimize the
communication overhead for regeneration (called MBR). In this work, we propose
new encoding schemes for $[n,d]$ error-correcting MSR and MBR codes that
generalize our earlier work on error-correcting regenerating codes. We show
that by choosing a suitable diagonal matrix, any generator matrix of the
$[n,\alpha]$ Reed-Solomon (RS) code can be integrated into the encoding matrix.
Hence, MSR codes with the least update complexity can be found. By using the
coefficients of generator polynomials of $[n,k]$ and $[n,d]$ RS codes, we
present a least-update-complexity encoding scheme for MBR codes. A decoding
scheme is proposed that utilizes the $[n,\alpha]$ RS code to perform data
reconstruction for MSR codes. The proposed decoding scheme has better error
correction capability and incurs the least number of node accesses when errors
are present. A new decoding scheme is also proposed for MBR codes that can
correct more error-patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4625</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4625</id><created>2013-01-19</created><authors><author><keyname>Huang</keyname><forenames>Chao-Wei</forenames></author><author><keyname>Chang</keyname><forenames>Tsung-Hui</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author><author><keyname>Hong</keyname><forenames>Y. -W. Peter</forenames></author></authors><title>Two-Way Training for Discriminatory Channel Estimation in Wireless MIMO
  Systems</title><categories>cs.IT math.IT</categories><comments>14</comments><doi>10.1109/TSP.2013.2245124</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work examines the use of two-way training to efficiently discriminate
the channel estimation performances at a legitimate receiver (LR) and an
unauthorized receiver (UR) in a multiple-input multiple-output (MIMO) wireless
system. This work improves upon the original discriminatory channel estimation
(DCE) scheme proposed by Chang et al where multiple stages of feedback and
retraining were used. While most studies on physical layer secrecy are under
the information-theoretic framework and focus directly on the data transmission
phase, studies on DCE focus on the training phase and aim to provide a
practical signal processing technique to discriminate between the channel
estimation performances at LR and UR. A key feature of DCE designs is the
insertion of artificial noise (AN) in the training signal to degrade the
channel estimation performance at UR. To do so, AN must be placed in a
carefully chosen subspace based on the transmitter's knowledge of LR's channel
in order to minimize its effect on LR. In this paper, we adopt the idea of
two-way training that allows both the transmitter and LR to send training
signals to facilitate channel estimation at both ends. Both reciprocal and
non-reciprocal channels are considered and a two-way DCE scheme is proposed for
each scenario. {For mathematical tractability, we assume that all terminals
employ the linear minimum mean square error criterion for channel estimation.
Based on the mean square error (MSE) of the channel estimates at all
terminals,} we formulate and solve an optimization problem where the optimal
power allocation between the training signal and AN is found by minimizing the
MSE of LR's channel estimate subject to a constraint on the MSE achievable at
UR. Numerical results show that the proposed DCE schemes can effectively
discriminate between the channel estimation and hence the data detection
performances at LR and UR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4634</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4634</id><created>2013-01-20</created><authors><author><keyname>Shirazi</keyname><forenames>A. H.</forenames></author><author><keyname>Namaki</keyname><forenames>A.</forenames></author><author><keyname>Roohi</keyname><forenames>A. A.</forenames></author><author><keyname>Jafari</keyname><forenames>G. R.</forenames></author></authors><title>Transparency effect in the emergence of monopolies in social networks</title><categories>physics.soc-ph cs.SI</categories><comments>14 Pages, 3 figures</comments><msc-class>Social networks</msc-class><journal-ref>Journal of Artificial Societies and Social Simulation 16 (1) 1
  (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power law degree distribution was shown in many complex networks. However, in
most real systems, deviation from power-law behavior is observed in social and
economical networks and emergence of giant hubs is obvious in real network
structures far from the tail of power law. We propose a model based on the
information transparency (transparency means how much the information is
obvious to others). This model can explain power structure in societies with
non-transparency in information delivery. The emergence of ultra powerful nodes
is explained as a direct result of censorship. Based on these assumptions, we
define four distinct transparency regions: perfect non-transparent, low
transparent, perfect transparent and exaggerated regions. We observe the
emergence of some ultra powerful (very high degree) nodes in low transparent
networks, in accordance with the economical and social systems. We show that
the low transparent networks are more vulnerable to attacks and the
controllability of low transparent networks is harder than the others. Also,
the ultra powerful nodes in the low transparent networks have a smaller mean
length and higher clustering coefficients than the other regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4643</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4643</id><created>2013-01-20</created><updated>2013-07-18</updated><authors><author><keyname>Wachter-Zeh</keyname><forenames>Antonia</forenames></author></authors><title>Bounds on List Decoding of Rank-Metric Codes</title><categories>cs.IT math.IT</categories><comments>10 pages, 2 figures, submitted to IEEE Transactions on Information
  Theory, short version presented at ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  So far, there is no polynomial-time list decoding algorithm (beyond half the
minimum distance) for Gabidulin codes. These codes can be seen as the
rank-metric equivalent of Reed--Solomon codes. In this paper, we provide bounds
on the list size of rank-metric codes in order to understand whether
polynomial-time list decoding is possible or whether it works only with
exponential time complexity. Three bounds on the list size are proven. The
first one is a lower exponential bound for Gabidulin codes and shows that for
these codes no polynomial-time list decoding beyond the Johnson radius exists.
Second, an exponential upper bound is derived, which holds for any rank-metric
code of length $n$ and minimum rank distance $d$. The third bound proves that
there exists a rank-metric code over $\Fqm$ of length $n \leq m$ such that the
list size is exponential in the length for any radius greater than half the
minimum rank distance. This implies that there cannot exist a polynomial upper
bound depending only on $n$ and $d$ similar to the Johnson bound in Hamming
metric. All three rank-metric bounds reveal significant differences to bounds
for codes in Hamming metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4646</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4646</id><created>2013-01-20</created><authors><author><keyname>Namboodiri</keyname><forenames>Vishnu</forenames></author><author><keyname>Venugopal</keyname><forenames>Kiran</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Physical Layer Network Coding for Two-Way Relaying with QAM</title><categories>cs.IT math.IT</categories><comments>13 pages, 14 figures, submitted to IEEE Trans. Wireless
  Communications. arXiv admin note: substantial text overlap with
  arXiv:1203.3269</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of modulation schemes for the physical layer network-coded two way
relaying scenario was studied in [1], [3], [4] and [5]. In [7] it was shown
that every network coding map that satisfies the exclusive law is representable
by a Latin Square and conversely, and this relationship can be used to get the
network coding maps satisfying the exclusive law. But, only the scenario in
which the end nodes use $M$-PSK signal sets is addressed in [7] and [8]. In
this paper, we address the case in which the end nodes use $M$-QAM signal sets.
In a fading scenario, for certain channel conditions $\gamma e^{j \theta}$,
termed singular fade states, the MA phase performance is greatly reduced. By
formulating a procedure for finding the exact number of singular fade states
for QAM, we show that square QAM signal sets give lesser number of singular
fade states compared to PSK signal sets. This results in superior performance
of $M$-QAM over $M$-PSK. It is shown that the criterion for partitioning the
complex plane, for the purpose of using a particular network code for a
particular fade state, is different from that used for $M$-PSK. Using a
modified criterion, we describe a procedure to analytically partition the
complex plane representing the channel condition. We show that when $M$-QAM ($M
&gt;4$) signal set is used, the conventional XOR network mapping fails to remove
the ill effects of $\gamma e^{j \theta}=1$, which is a singular fade state for
all signal sets of arbitrary size. We show that a doubly block circulant Latin
Square removes this singular fade state for $M$-QAM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4650</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4650</id><created>2013-01-20</created><authors><author><keyname>Moraz&#xe1;n</keyname><forenames>Marco T.</forenames><affiliation>Seton Hall University</affiliation></author><author><keyname>Achten</keyname><forenames>Peter</forenames><affiliation>Radboud University Nijmegen</affiliation></author></authors><title>Proceedings First International Workshop on Trends in Functional
  Programming in Education</title><categories>cs.PL cs.CY</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 106, 2013</journal-ref><doi>10.4204/EPTCS.106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The First International Workshop on Trends in Functional Programming in
Education, TFPIE 2012, was held on June 11, 2012 at the University of St
Andrews in Scotland. The goal of TFPIE is to gather researchers, professors,
teachers, and all professionals that use or are interested in the use of
functional programming in education. Submissions were vetted by the TFPIE 2012
program committee using prevailing academic standards. The 4 articles in this
volume were selected for publication as the result of this process. These
articles cover a wide range of novel approaches in education using functional
programming. Page and Gamboa describe how they introduce students to
computational thinking and problem solving using logic and equation-based
reasoning. O'Donnell describes efforts to make computer systems courses come
alive through the use of simulation and a functional hardware description
language. Radge describes a novel approach to CS1 exploiting the interplay of
the imprecision found in mathematical abstractions and the precision that is
required in programming. Stutterheim, Swierstra, and Swierstra describe a new
approach to introduce high school students to programming and to important
ideas in Computer Science using a web-based interpreter and theorem prover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4654</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4654</id><created>2013-01-20</created><authors><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Jain</keyname><forenames>Satbir</forenames></author></authors><title>Real Time scheduling with Virtual Nodes for Self Stabilization in
  Wireless Sensor Networks</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:cs/0608069 by
  other authors without attribution</comments><journal-ref>International Journal of Information Technology and Knowledge
  Management July-December 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new scheduling algorithm called Real Time
Scheduling (RTS) which uses virtual nodes for self stabilization. This
algorithm deals with all the contributing components of the end-to-end
travelling delay of data packets in sensor network and with virtual nodes
algorithm achieves QoS in terms of packet delivery, multiple connections,
better power management and stable routes in case of failure. RTS delays
packets at intermediate hops (not just prioritizes them) for a duration that is
a function of their deadline. Delaying packets allows the network to avoid hot
spotting while maintaining deadline-faithfulness. We compare RTS with another
prioritizing and scheduling algorithm for real-time data dissemination in
sensor networks, velocity monotonic scheduling. This paper simulates RTS based
on two typical routing protocols, shortest path routing and greedy forwarding
with J-Sim.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4655</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4655</id><created>2013-01-20</created><authors><author><keyname>Batagelj</keyname><forenames>Vladimir</forenames></author><author><keyname>Cerin&#x161;ek</keyname><forenames>Monika</forenames></author></authors><title>On bibliographic networks</title><categories>cs.SI cs.DL physics.soc-ph</categories><msc-class>2010: 91D30, 62H30, 68W40, 93A15</msc-class><journal-ref>Scientometrics 96 (2013) 3, 845-864</journal-ref><doi>10.1007/s11192-012-0940-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper we show that the bibliographic data can be transformed into a
collection of compatible networks. Using network multiplication different
interesting derived networks can be obtained. In defining them an appropriate
normalization should be considered. The proposed approach can be applied also
to other collections of compatible networks. We also discuss the question when
the multiplication of sparse networks preserves sparseness. The proposed
approaches are illustrated with analyses of collection of networks on the topic
&quot;social network&quot; obtained from the Web of Science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4659</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4659</id><created>2013-01-20</created><authors><author><keyname>Parwej</keyname><forenames>Firoj</forenames></author></authors><title>English Sentence Recognition using Artificial Neural Network through
  Mouse-based Gestures</title><categories>cs.AI</categories><comments>6 Pages, 7 Figures. arXiv admin note: text overlap with
  arXiv:1007.0627 by other authors without attribution</comments><journal-ref>International Journal of Computer Applications (IJCA)USA, Volume
  61, No.17, January 2013 ISSN 0975 - 8887, http://www.ijcaonline.org,
  http://www.ijcaonline.org/archives/volume61/number17/10023-4998</journal-ref><doi>10.5120/10023-4998</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Handwriting is one of the most important means of daily communication.
Although the problem of handwriting recognition has been considered for more
than 60 years there are still many open issues, especially in the task of
unconstrained handwritten sentence recognition. This paper focuses on the
automatic system that recognizes continuous English sentence through a
mouse-based gestures in real-time based on Artificial Neural Network. The
proposed Artificial Neural Network is trained using the traditional
backpropagation algorithm for self supervised neural network which provides the
system with great learning ability and thus has proven highly successful in
training for feed-forward Artificial Neural Network. The designed algorithm is
not only capable of translating discrete gesture moves, but also continuous
gestures through the mouse. In this paper we are using the efficient neural
network approach for recognizing English sentence drawn by mouse. This approach
shows an efficient way of extracting the boundary of the English Sentence and
specifies the area of the recognition English sentence where it has been drawn
in an image and then used Artificial Neural Network to recognize the English
sentence. The proposed approach English sentence recognition (ESR) system is
designed and tested successfully. Experimental results show that the higher
speed and accuracy were examined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4662</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4662</id><created>2013-01-20</created><authors><author><keyname>Perwej</keyname><forenames>Yusuf</forenames></author></authors><title>Recurrent Neural Network Method in Arabic Words Recognition System</title><categories>cs.NE</categories><comments>6 Pages, 5 Figures, Vol. 3, Issue 11, pages 43-48</comments><journal-ref>International Journal of Computer Science and Telecommunications
  (IJCST)UK, London,(http://www.ijcst.org) Vol. 3, Issue 11, pages 43-48,
  November 2012, http://www.ijcst.org/Volume3/Issue11/p8_3_11.pdf</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recognition of unconstrained handwriting continues to be a difficult task
for computers despite active research for several decades. This is because
handwritten text offers great challenges such as character and word
segmentation, character recognition, variation between handwriting styles,
different character size and no font constraints as well as the background
clarity. In this paper primarily discussed Online Handwriting Recognition
methods for Arabic words which being often used among then across the Middle
East and North Africa people. Because of the characteristic of the whole body
of the Arabic words, namely connectivity between the characters, thereby the
segmentation of An Arabic word is very difficult. We introduced a recurrent
neural network to online handwriting Arabic word recognition. The key
innovation is a recently produce recurrent neural networks objective function
known as connectionist temporal classification. The system consists of an
advanced recurrent neural network with an output layer designed for sequence
labeling, partially combined with a probabilistic language model. Experimental
results show that unconstrained Arabic words achieve recognition rates about
79%, which is significantly higher than the about 70% using a previously
developed hidden markov model based recognition system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4666</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4666</id><created>2013-01-20</created><updated>2015-08-14</updated><authors><author><keyname>Garber</keyname><forenames>Dan</forenames></author><author><keyname>Hazan</keyname><forenames>Elad</forenames></author></authors><title>A Linearly Convergent Conditional Gradient Algorithm with Applications
  to Online and Stochastic Optimization</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear optimization is many times algorithmically simpler than non-linear
convex optimization. Linear optimization over matroid polytopes, matching
polytopes and path polytopes are example of problems for which we have simple
and efficient combinatorial algorithms, but whose non-linear convex counterpart
is harder and admits significantly less efficient algorithms. This motivates
the computational model of convex optimization, including the offline, online
and stochastic settings, using a linear optimization oracle. In this
computational model we give several new results that improve over the previous
state-of-the-art. Our main result is a novel conditional gradient algorithm for
smooth and strongly convex optimization over polyhedral sets that performs only
a single linear optimization step over the domain on each iteration and enjoys
a linear convergence rate. This gives an exponential improvement in convergence
rate over previous results.
  Based on this new conditional gradient algorithm we give the first algorithms
for online convex optimization over polyhedral sets that perform only a single
linear optimization step over the domain while having optimal regret
guarantees, answering an open question of Kalai and Vempala, and Hazan and
Kale. Our online algorithms also imply conditional gradient algorithms for
non-smooth and stochastic convex optimization with the same convergence rates
as projected (sub)gradient methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4668</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4668</id><created>2013-01-20</created><authors><author><keyname>P</keyname><forenames>Kirana Kumara</forenames></author></authors><title>A MATLAB Code for Three Dimensional Linear Elastostatics using Constant
  Boundary Elements</title><categories>cs.CE physics.comp-ph</categories><comments>12 pages (pdf), 8 supplementary files, accepted author manuscript</comments><journal-ref>International Journal of Advances in Engineering Sciences(IJAES)
  Vol 2 No 3 (2012) pp. 9-20 [e-ISSN: 2231-0347, Print-ISSN: 2231-2013]</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Present work presents a code written in the very simple programming language
MATLAB, for three dimensional linear elastostatics, using constant boundary
elements. The code, in full or in part, is not a translation or a copy of any
of the existing codes. Present paper explains how the code is written, and
lists all the formulae used. Code is verified by using the code to solve a
simple problem which has the well known approximate analytical solution. Of
course, present work does not make any contribution to research on boundary
elements, in terms of theory. But the work is justified by the fact that, to
the best of author's knowledge, as of now, one cannot find an open access
MATLAB code for three dimensional linear elastostatics using constant boundary
elements. Author hopes this paper to be of help to beginners who wish to
understand how a simple but complete boundary element code works, so that they
can build upon and modify the present open access code to solve complex
engineering problems quickly and easily. The code is available online for open
access (as supplementary file for the present paper), and may be downloaded
from the website for the present journal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4679</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4679</id><created>2013-01-20</created><updated>2013-06-25</updated><authors><author><keyname>Biau</keyname><forenames>G&#xe9;rard</forenames><affiliation>LPMA, LSTA, DMA, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Devroye</keyname><forenames>Luc</forenames><affiliation>SOCS</affiliation></author></authors><title>Cellular Tree Classifiers</title><categories>stat.ML cs.LG math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cellular tree classifier model addresses a fundamental problem in the
design of classifiers for a parallel or distributed computing world: Given a
data set, is it sufficient to apply a majority rule for classification, or
shall one split the data into two or more parts and send each part to a
potentially different computer (or cell) for further processing? At first
sight, it seems impossible to define with this paradigm a consistent classifier
as no cell knows the &quot;original data size&quot;, $n$. However, we show that this is
not so by exhibiting two different consistent classifiers. The consistency is
universal but is only shown for distributions with nonatomic marginals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4682</identifier>
 <datestamp>2015-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4682</id><created>2013-01-20</created><authors><author><keyname>Mercas</keyname><forenames>Robert</forenames></author><author><keyname>Ochem</keyname><forenames>Pascal</forenames></author><author><keyname>Samsonov</keyname><forenames>Alexei V.</forenames></author><author><keyname>Shur</keyname><forenames>Arseny M.</forenames></author></authors><title>Binary Patterns in Binary Cube-Free Words: Avoidability and Growth</title><categories>cs.FL math.CO</categories><comments>18 pages, 2 tables; submitted to RAIRO TIA (Special issue of Mons
  Days 2012)</comments><msc-class>68R15</msc-class><journal-ref>RAIRO - Theoretical Informatics and Applications Vol. 48(4)
  (2014), 369-389</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The avoidability of binary patterns by binary cube-free words is investigated
and the exact bound between unavoidable and avoidable patterns is found. All
avoidable patterns are shown to be D0L-avoidable. For avoidable patterns, the
growth rates of the avoiding languages are studied. All such languages, except
for the overlap-free language, are proved to have exponential growth. The exact
growth rates of languages avoiding minimal avoidable patterns are approximated
through computer-assisted upper bounds. Finally, a new example of a
pattern-avoiding language of polynomial growth is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4691</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4691</id><created>2013-01-20</created><authors><author><keyname>Redieteab</keyname><forenames>Getachew</forenames></author></authors><title>Cross-layer Optimization for Next Generation Wi-Fi</title><categories>cs.ET cs.NI</categories><comments>221 pages, PhD thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From the initial 1997 specification to the undergoing IEEE 802.11ac
standardization, a leap in throughput has been observed with every new
generation. The expectations for next generations on issues like throughput,
range, reliability, and power consumption are even higher. This is quite a
challenge considering all the work already done. Cross-layer optimization of
physical (PHY) and medium access control (MAC) layers can be an interesting
exploration path for further enhancement. During this thesis we have studied
cross-layer optimization techniques, with a focus on the IEEE 802.11ac
standard. A new multichannel aggregation scheme involving cross-knowledge
between PHY and MAC layers has been proposed to improve performance in
collision-prone environments. We have shown that some functionalities directly
involved PHY and MAC layers. An accurate modeling of both PHY and MAC
mechanisms is thus needed to have a realistic characterization of such
functionalities. A cross-layer simulator, compliant with IEEE 802.11ac
specifications, has thus been implemented. To the best of our knowledge, this
is the first simulator incorporating detailed PHY and MAC functionalities for
the IEEE 802.11ac standard. The multiple-user multiple-input, multiple-output
(MU-MIMO) technique, which is one of the main innovations of the IEEE 802.11ac,
needs both PHY and MAC layer considerations. We have thus used the implemented
cross-layer simulator to evaluate the performance of MU-MIMO and compared it
with the single-user MIMO (SU-MIMO). The aim of these studies was to evaluate
the 'real' gains of MU-MIMO solutions (accounting for PHY+MAC) over SU-MIMO
solutions and not the generally accepted ones. The impact of the channel
sounding interval has particularly been studied. Finally, we have proposed a
short PHY layer version of acknowledgment frames for overhead reduction in IEEE
802.11ah communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4723</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4723</id><created>2013-01-20</created><authors><author><keyname>Mamadolimov</keyname><forenames>Abdurashid</forenames></author><author><keyname>Isa</keyname><forenames>Herman</forenames></author><author><keyname>Mohamad</keyname><forenames>Moesfa Soeheila</forenames></author></authors><title>Practical Bijective S-box Design</title><categories>cs.CR math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct 8 x 8 bijective cryptographically strong S-boxes. Our
construction is based on using non-bijective power functions over the finite
filed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4728</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4728</id><created>2013-01-20</created><authors><author><keyname>Brazil</keyname><forenames>Marcus</forenames></author><author><keyname>Ras</keyname><forenames>Charl</forenames></author><author><keyname>Thomas</keyname><forenames>Doreen</forenames></author></authors><title>Relay Augmentation for Lifetime Extension of Wireless Sensor Networks</title><categories>math.OC cs.DS</categories><journal-ref>IET Wireless Sensor Systems. 3:145-152. 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel relay augmentation strategy for extending the lifetime of
a certain class of wireless sensor networks. In this class sensors are located
at fixed and pre-determined positions and all communication takes place via
multi-hop paths in a fixed routing tree rooted at the base station. It is
assumed that no accumulation of data takes place along the communication paths
and that there is no restriction on where additional relays may be located.
Under these assumptions the optimal extension of network lifetime is modelled
as the Euclidean $k$-bottleneck Steiner tree problem. Only two approximation
algorithms for this NP-hard problem exist in the literature: a minimum spanning
tree heuristic (MSTH) with performance ratio 2, and a probabilistic 3-regular
hypergraph heuristic (3RHH) with performance ratio $\sqrt{3}+\epsilon$. We
present a new iterative heuristic that incorporates MSTH and show via
simulation that our algorithm performs better than MSTH in extending lifetime,
and outperforms 3RHH in terms of efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4729</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4729</id><created>2013-01-20</created><authors><author><keyname>Liu</keyname><forenames>An</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author><author><keyname>Liu</keyname><forenames>Youjian</forenames></author></authors><title>Duality and Optimization for Generalized Multi-hop MIMO
  Amplify-and-Forward Relay Networks with Linear Constraints</title><categories>cs.IT math.IT</categories><comments>30 pages, 8 figures</comments><journal-ref>An Liu; Lau, V.K.N.; Youjian Liu, &quot;Duality and Optimization for
  Generalized Multi-Hop MIMO Amplify-and-Forward Relay Networks With Linear
  Constraints,&quot; IEEE Transactions on Signal Processing, vol.61, no.9,
  pp.2356,2365, May1, 2013</journal-ref><doi>10.1109/TSP.2013.2245126</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a generalized multi-hop MIMO amplify-and-forward (AF) relay
network with multiple sources/destinations and arbitrarily number of relays. We
establish two dualities and the corresponding dual transformations between such
a network and its dual, respectively under single network linear constraint and
per-hop linear constraint. The result is a generalization of the previous
dualities under different special cases and is proved using new techniques
which reveal more insight on the duality structure that can be exploited to
optimize MIMO precoders. A unified optimization framework is proposed to find a
stationary point for an important class of non-convex optimization problems of
AF relay networks based on a local Lagrange dual method, where the primal
algorithm only finds a stationary point for the inner loop problem of
maximizing the Lagrangian w.r.t. the primal variables. The input covariance
matrices are shown to satisfy a polite water-filling structure at a stationary
point of the inner loop problem. The duality and polite water-filling are
exploited to design fast primal algorithms. Compared to the existing
algorithms, the proposed optimization framework with duality-based primal
algorithms can be used to solve more general problems with lower computation
cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4730</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4730</id><created>2013-01-20</created><updated>2014-02-05</updated><authors><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author><author><keyname>Kellett</keyname><forenames>Christopher M.</forenames></author></authors><title>Optimal Coding Functions for Pairwise Message Sharing on Finite-Field
  Multi-Way Relay Channels</title><categories>cs.IT math.IT</categories><comments>Author's final version (accepted for presentation at the 2014 IEEE
  International Conference on Communications [ICC 2014])</comments><journal-ref>Proceedings of the 2014 IEEE International Conference on
  Communications (ICC 2014), Sydney, Australia, pp. 1866-1871, June 10-14, 2014</journal-ref><doi>10.1109/ICC.2014.6883595</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the finite-field multi-way relay channel with pairwise
message sharing, where multiple users exchange messages through a single relay
and where the users may share parts of their source messages (meaning that some
message parts are known/common to more than one user). In this paper, we design
an optimal functional-decode-forward coding scheme that takes the shared
messages into account. More specifically, we design an optimal function for the
relay to decode (from the users on the uplink) and forward (back to the users
on the downlink). We then show that this proposed function-decode-forward
coding scheme can achieve the capacity region of the finite-field multi-way
relay channel with pairwise message sharing. This paper generalizes our
previous result for the case of three users to any number of users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4732</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4732</id><created>2013-01-20</created><updated>2013-03-19</updated><authors><author><keyname>Kuhn</keyname><forenames>Nicolas</forenames></author><author><keyname>Mehani</keyname><forenames>Olivier</forenames></author><author><keyname>Bui</keyname><forenames>Huyen-Chi</forenames></author><author><keyname>Lacan</keyname><forenames>Jerome</forenames></author><author><keyname>Radzik</keyname><forenames>Jose</forenames></author><author><keyname>Lochin</keyname><forenames>Emmanuel</forenames></author></authors><title>Physical Channel Access (PCA): Time and Frequency Access Methods
  Emulation in NS-2</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an NS-2 module, Physical Channel Access (PCA), to simulate
different access methods on a link shared with Multi-Frequency Time Division
Multiple Access (MF-TDMA). This tech- nique is widely used in various network
technologies, such as satellite communication. In this context, different
access methods at the gateway induce different queuing delays and available
capacities, which strongly impact transport layer performance. Depending on QoS
requirements, design of new congestion and flow control mechanisms and/or
access methods requires evaluation through simulations.
  PCA module emulates the delays that packets will experience using the shared
link, based on descriptive parameters of lower layers characteris- tics. Though
PCA has been developed with DVB-RCS2 considerations in mind (for which we
present a use case), other MF-TDMA-based appli- cations can easily be simulated
by adapting input parameters. Moreover, the presented implementation details
highlight the main methods that might need modifications to implement more
specific functionality or emulate other similar access methods (e.g., OFDMA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4738</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4738</id><created>2013-01-20</created><updated>2013-01-24</updated><authors><author><keyname>Zhou</keyname><forenames>Yaqin</forenames></author><author><keyname>Li</keyname><forenames>Xiangyang</forenames></author><author><keyname>Liu</keyname><forenames>Min</forenames></author><author><keyname>Mao</keyname><forenames>Xufei</forenames></author><author><keyname>Tang</keyname><forenames>Shaojie</forenames></author><author><keyname>Li</keyname><forenames>Zhongcheng</forenames></author></authors><title>Throughput Optimizing Localized Link Scheduling for Multihop Wireless
  Networks Under Physical Interference Model</title><categories>cs.NI cs.DC</categories><comments>A earlier work &quot;Distributed Link Scheduling for Throughput
  Maximization under Physical Interference Model&quot; is presented in Proc. IEEE
  Infocom 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study throughput-optimum localized link scheduling in wireless networks.
The majority of results on link scheduling assume binary interference models
that simplify interference constraints in actual wireless communication. While
the physical interference model reflects the physical reality more precisely,
the problem becomes notoriously harder under the physical interference model.
There have been just a few existing results on link scheduling under the
physical interference model, and even fewer on more practical distributed or
localized scheduling. In this paper, we tackle the challenges of localized link
scheduling posed by the complex physical interference constraints. By
cooperating the partition and shifting strategies into the pick-and-compare
scheme, we present a class of localized scheduling algorithms with provable
throughput guarantee subject to physical interference constraints. The
algorithm in the linear power setting is the first localized algorithm that
achieves at least a constant fraction of the optimal capacity region subject to
physical interference constraints. The algorithm in the uniform power setting
is the first localized algorithm with a logarithmic approximation ratio to the
optimal solution. Our extensive simulation results demonstrate correctness and
performance efficiency of our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4749</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4749</id><created>2013-01-20</created><authors><author><keyname>Washizawa</keyname><forenames>Teruyoshi</forenames></author></authors><title>On the Behavior of the Residual in Conjugate Gradient Method</title><categories>cs.NA math.NA</categories><comments>8 pages</comments><journal-ref>Applied Mathematics, vol.1, no.3, pp.211-214, 2010</journal-ref><doi>10.4236/am.2010.13025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In conjugate gradient method, it is well known that the recursively computed
residual differs from true one as the iteration proceeds in finite arithmetic.
Some work have been devoted to analyze this be-havior and to evaluate the lower
and the upper bounds of the difference. This paper focuses on the behavior of
these two kinds of residuals, especially their lower bounds caused by the loss
of trailing digit, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4753</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4753</id><created>2013-01-20</created><authors><author><keyname>Rizvandi</keyname><forenames>Nikzad Babaii</forenames></author><author><keyname>Taheri</keyname><forenames>Javid</forenames></author><author><keyname>Zomaya</keyname><forenames>Albert Y.</forenames></author></authors><title>Pattern Matching for Self- Tuning of MapReduce Jobs</title><categories>cs.DC cs.AI cs.LG</categories><comments>7 pages, previously published as &quot;On Using Pattern Matching
  Algorithms in MapReduce Applications&quot; at ISPA 2011. arXiv admin note:
  substantial text overlap with arXiv:1112.5505</comments><doi>10.1109/ISPA.2011.24</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study CPU utilization time patterns of several MapReduce
applications. After extracting running patterns of several applications, they
are saved in a reference database to be later used to tweak system parameters
to efficiently execute unknown applications in future. To achieve this goal,
CPU utilization patterns of new applications are compared with the already
known ones in the reference database to find/predict their most probable
execution patterns. Because of different patterns lengths, the Dynamic Time
Warping (DTW) is utilized for such comparison; a correlation analysis is then
applied to DTWs outcomes to produce feasible similarity patterns. Three real
applications (WordCount, Exim Mainlog parsing and Terasort) are used to
evaluate our hypothesis in tweaking system parameters in executing similar
applications. Results were very promising and showed effectiveness of our
approach on pseudo-distributed MapReduce platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4765</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4765</id><created>2013-01-21</created><authors><author><keyname>Cui</keyname><forenames>Hongyu</forenames></author><author><keyname>Zhang</keyname><forenames>Rongqing</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Jiao</keyname><forenames>Bingli</forenames></author></authors><title>Capacity Analysis of Bidirectional AF Relay Selection with Imperfect
  Channel State Information</title><categories>cs.IT math.IT</categories><comments>11 pages, 4 figures, accepted by IEEE Wireless Communications Letters</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this letter, we analyze the ergodic capacity of bidirectional
amplify-and-forward relay selection (RS) with imperfect channel state
information (CSI), i.e., outdated CSI and imperfect channel estimation.
Practically, the optimal RS scheme in maximizing the ergodic capacity cannot be
achieved, due to the imperfect CSI. Therefore, two suboptimal RS schemes are
discussed and analyzed, in which the first RS scheme is based on the imperfect
channel coefficients, and the second RS scheme is based on the predicted
channel coefficients. The lower bound of the ergodic capacity with imperfect
CSI is derived in a closed-form, which matches tightly with the simulation
results. The results reveal that once CSI is imperfect, the ergodic capacity of
bidirectional RS degrades greatly, whereas the RS scheme based on the predicted
channel has better performance, and it approaches infinitely to the optimal
performance, when the prediction length is sufficiently large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4767</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4767</id><created>2013-01-21</created><updated>2013-02-28</updated><authors><author><keyname>Cesa-Bianchi</keyname><forenames>Nicolo</forenames></author><author><keyname>Gentile</keyname><forenames>Claudio</forenames></author><author><keyname>Vitale</keyname><forenames>Fabio</forenames></author><author><keyname>Zappella</keyname><forenames>Giovanni</forenames></author></authors><title>A Linear Time Active Learning Algorithm for Link Classification -- Full
  Version --</title><categories>cs.LG cs.SI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present very efficient active learning algorithms for link classification
in signed networks. Our algorithms are motivated by a stochastic model in which
edge labels are obtained through perturbations of a initial sign assignment
consistent with a two-clustering of the nodes. We provide a theoretical
analysis within this model, showing that we can achieve an optimal (to whithin
a constant factor) number of mistakes on any graph G = (V,E) such that |E| =
\Omega(|V|^{3/2}) by querying O(|V|^{3/2}) edge labels. More generally, we show
an algorithm that achieves optimality to within a factor of O(k) by querying at
most order of |V| + (|V|/k)^{3/2} edge labels. The running time of this
algorithm is at most of order |E| + |V|\log|V|.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4769</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4769</id><created>2013-01-21</created><updated>2013-02-28</updated><authors><author><keyname>Cesa-Bianchi</keyname><forenames>Nicolo</forenames></author><author><keyname>Gentile</keyname><forenames>Claudio</forenames></author><author><keyname>Vitale</keyname><forenames>Fabio</forenames></author><author><keyname>Zappella</keyname><forenames>Giovanni</forenames></author></authors><title>A Correlation Clustering Approach to Link Classification in Signed
  Networks -- Full Version --</title><categories>cs.LG cs.DS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by social balance theory, we develop a theory of link
classification in signed networks using the correlation clustering index as
measure of label regularity. We derive learning bounds in terms of correlation
clustering within three fundamental transductive learning settings: online,
batch and active. Our main algorithmic contribution is in the active setting,
where we introduce a new family of efficient link classifiers based on covering
the input graph with small circuits. These are the first active algorithms for
link classification with mistake bounds that hold for arbitrary signed
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4773</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4773</id><created>2013-01-21</created><authors><author><keyname>Feng</keyname><forenames>Tao</forenames></author><author><keyname>Leung</keyname><forenames>Ka Hin</forenames></author><author><keyname>Xiang</keyname><forenames>Qing</forenames></author></authors><title>Binary Cyclic codes with two primitive nonzeros</title><categories>cs.IT math.CO math.IT</categories><comments>11 pages, submitted to the special volumn of Science Sinica
  Mathematics on Coding, Cryptography and Combinatorics</comments><doi>10.1007/s11425-013-4668-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we make some progress towards a well-known conjecture on the
minimum weights of binary cyclic codes with two primitive nonzeros. We also
determine the Walsh spectrum of $\Tr(x^d)$ over $\F_{2^{m}}$ in the case where
$m=2t$, $d=3+2^{t+1}$ and $\gcd(d, 2^{m}-1)=1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4779</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4779</id><created>2013-01-21</created><authors><author><keyname>Braibant</keyname><forenames>Thomas</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Chlipala</keyname><forenames>Adam</forenames><affiliation>CSAIL</affiliation></author></authors><title>Formal Verification of Hardware Synthesis</title><categories>cs.PL</categories><proxy>ccsd</proxy><journal-ref>Computer Aided Verification, Saint Petersburg : Russie,
  F\'ed\'eration De (2013)</journal-ref><doi>10.1007/978-3-642-39799-8_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on the implementation of a certified compiler for a high-level
hardware description language (HDL) called Fe-Si (FEatherweight SynthesIs).
Fe-Si is a simplified version of Bluespec, an HDL based on a notion of guarded
atomic actions. Fe-Si is defined as a dependently typed deep embedding in Coq.
The target language of the compiler corresponds to a synthesisable subset of
Verilog or VHDL. A key aspect of our approach is that input programs to the
compiler can be defined and proved correct inside Coq. Then, we use extraction
and a Verilog back-end (written in OCaml) to get a certified version of a
hardware design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4780</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4780</id><created>2013-01-21</created><authors><author><keyname>Hmida</keyname><forenames>Helmi Ben</forenames><affiliation>i3mainz</affiliation></author><author><keyname>Cruz</keyname><forenames>Christophe</forenames><affiliation>Le2i</affiliation></author><author><keyname>Boochs</keyname><forenames>Frank</forenames><affiliation>i3mainz</affiliation></author><author><keyname>Nicolle</keyname><forenames>Christophe</forenames><affiliation>Le2i</affiliation></author></authors><title>From Quantitative Spatial Operator to Qualitative Spatial Relation Using
  Constructive Solid Geometry, Logic Rules and Optimized 9-IM Model, A Semantic
  Based Approach</title><categories>cs.CG cs.AI</categories><proxy>ccsd</proxy><journal-ref>IEEE International Conference on Computer Science and Automation
  Engineering (CSAE),, Zhangjiajie : China (2012)</journal-ref><doi>10.1109/CSAE.2012.6272992</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Constructive Solid Geometry (CSG) is a data model providing a set of
binary Boolean operators such as Union, Difference and Intersection. In this
work, these operators are used to compute topological relations between objects
defined by the constraints of the nine Intersection Model (9-IM) from
Egenhofer. With the help of these constraints, we define a procedure to compute
the topological relations on CSG objects. These topological relations are
Disjoint, Contains, Inside, Covers, CoveredBy, Equals and Overlaps, and are
defined in a top-level ontology with a specific semantic definition on relation
such as Transitive, Symmetric, Asymmetric, Functional, Reflexive, and
Irreflexive. The results of topological relations computation are stored in the
ontology allowing after what to infer on these topological relationships. In
addition, logic rules based on the Semantic Web Language allows the definition
of logic programs that define which topological relationships have to be
computed on which kind of objects. For instance, a &quot;Building&quot; that overlaps a
&quot;Railway&quot; is a &quot;RailStation&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4781</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4781</id><created>2013-01-21</created><authors><author><keyname>Werner</keyname><forenames>David</forenames><affiliation>Le2i</affiliation></author><author><keyname>Cruz</keyname><forenames>Christophe</forenames><affiliation>Le2i</affiliation></author><author><keyname>Nicolle</keyname><forenames>Christophe</forenames><affiliation>Le2i</affiliation></author></authors><title>Ontology-based Recommender System of Economic Articles</title><categories>cs.IR cs.DL</categories><proxy>ccsd</proxy><journal-ref>8th International Conference on Web Information Systems and
  Technologies, Porto : Portugal (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision makers need economical information to drive their decisions. The
Company Actualis SARL is specialized in the production and distribution of a
press review about French regional economic actors. This economic review
represents for a client a prospecting tool on partners and competitors. To
reduce the overload of useless information, the company is moving towards a
customized review for each customer. Three issues appear to achieve this goal.
First, how to identify the elements in the text in order to extract objects
that match with the recommendation's criteria presented? Second, How to define
the structure of these objects, relationships and articles in order to provide
a source of knowledge usable by the extraction process to produce new knowledge
from articles? The latter issue is the feedback on customer experience to
identify the quality of distributed information in real-time and to improve the
relevance of the recommendations. This paper presents a new type of
recommendation based on the semantic description of both articles and user
profile.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4783</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4783</id><created>2013-01-21</created><authors><author><keyname>Hmida</keyname><forenames>Helmi Ben</forenames><affiliation>i3mainz</affiliation></author><author><keyname>Cruz</keyname><forenames>Christophe</forenames><affiliation>Le2i</affiliation></author><author><keyname>Boochs</keyname><forenames>Frank</forenames><affiliation>i3mainz</affiliation></author><author><keyname>Nicolle</keyname><forenames>Christophe</forenames><affiliation>Le2i</affiliation></author></authors><title>From 3D Point Clouds To Semantic Objects An Ontology-Based Detection
  Approach</title><categories>cs.CG cs.AI</categories><proxy>ccsd</proxy><journal-ref>International Conference on Knowledge Engineering and Ontology
  Development, Paris : France (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a knowledge-based detection of objects approach using the
OWL ontology language, the Semantic Web Rule Language, and 3D processing
built-ins aiming at combining geometrical analysis of 3D point clouds and
specialist's knowledge. This combination allows the detection and the
annotation of objects contained in point clouds. The context of the study is
the detection of railway objects such as signals, technical cupboards, electric
poles, etc. Thus, the resulting enriched and populated ontology, that contains
the annotations of objects in the point clouds, is used to feed a GIS systems
or an IFC file for architecture purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4786</identifier>
 <datestamp>2013-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4786</id><created>2013-01-21</created><updated>2013-05-27</updated><authors><author><keyname>Chia</keyname><forenames>Yeow-Khiang</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Energy Cooperation in Cellular Networks with Renewable Powered Base
  Stations</title><categories>cs.IT math.IT</categories><comments>V1: Presented at IEEE WCNC 2013. V2: Extended version. Submitted to
  IEEE Trans. Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a model for energy cooperation between cellular
base stations (BSs) with individual hybrid power supplies (including both the
conventional grid and renewable energy sources), limited energy storages, and
connected by resistive power lines for energy sharing. When the renewable
energy profile and energy demand profile at all BSs are deterministic or known
ahead of time, we show that the optimal energy cooperation policy for the BSs
can be found by solving a linear program. We show the benefits of energy
cooperation in this regime. When the renewable energy and demand profiles are
stochastic and only causally known at the BSs, we propose an online energy
cooperation algorithm and show the optimality properties of this algorithm
under certain conditions. Furthermore, the energy-saving performances of the
developed offline and online algorithms are compared by simulations, and the
effect of the availability of energy state information (ESI) on the performance
gains of the BSs' energy cooperation is investigated. Finally, we propose a
hybrid algorithm that can incorporate offline information about the energy
profiles, but operates in an online manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4793</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4793</id><created>2013-01-21</created><authors><author><keyname>Bolliger</keyname><forenames>Lukas</forenames></author><author><keyname>Loeliger</keyname><forenames>Hans-Andrea</forenames></author><author><keyname>Vogel</keyname><forenames>Christian</forenames></author></authors><title>LMMSE Estimation and Interpolation of Continuous-Time Signals from
  Discrete-Time Samples Using Factor Graphs</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The factor graph approach to discrete-time linear Gaussian state space models
is well developed. The paper extends this approach to continuous-time linear
systems/filters that are driven by white Gaussian noise. By Gaussian message
passing, we then obtain MAP/MMSE/LMMSE estimates of the input signal, or of the
state, or of the output signal from noisy observations of the output signal.
These estimates may be obtained with arbitrary temporal resolution. The
proposed input signal estimation does not seem to have appeared in the prior
Kalman filtering literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4795</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4795</id><created>2013-01-21</created><authors><author><keyname>Nandi</keyname><forenames>Mrinal</forenames></author><author><keyname>Dewanji</keyname><forenames>Anup</forenames></author><author><keyname>Roy</keyname><forenames>Bimal</forenames></author><author><keyname>Sarkar</keyname><forenames>Santanu</forenames></author></authors><title>Model Selection Approach for Distributed Fault Detection in Wireless
  Sensor Networks</title><categories>cs.NI</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensor networks aim at monitoring their surroundings for event detection and
object tracking. But, due to failure, or death of sensors, false signal can be
transmitted. In this paper, we consider the problems of distributed fault
detection in wireless sensor network (WSN). In particular, we consider how to
take decision regarding fault detection in a noisy environment as a result of
false detection or false response of event by some sensors, where the sensors
are placed at the center of regular hexagons and the event can occur at only
one hexagon. We propose fault detection schemes that explicitly introduce the
error probabilities into the optimal event detection process. We introduce two
types of detection probabilities, one for the center node, where the event
occurs and the other one for the adjacent nodes. This second type of detection
probability is new in sensor network literature. We develop schemes under the
model selection procedure, multiple model selection procedure and use the
concept of Bayesian model averaging to identify a set of likely fault sensors
and obtain an average predictive error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4798</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4798</id><created>2013-01-21</created><updated>2014-07-21</updated><authors><author><keyname>Ju</keyname><forenames>Hyungsik</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>A Novel Mode Switching Scheme Utilizing Random Beamforming for
  Opportunistic Energy Harvesting</title><categories>cs.IT math.IT</categories><comments>36 pages, 17 figures</comments><journal-ref>IEEE Transactions on Wireless Communications, vol. 13, no. 4, pp.
  2150-2162, April 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since radio signals carry both energy and information at the same time, a
unified study on simultaneous wireless information and power transfer (SWIPT)
has recently drawn a significant attention for achieving wireless powered
communication networks. In this paper, we study a multiple-input single-output
(MISO) multicast SWIPT network with one multi-antenna transmitter sending
common information to multiple single-antenna receivers simultaneously along
with opportunistic wireless energy harvesting at each receiver. From the
practical consideration, we assume that the channel state information (CSI) is
only known at each respective receiver but is unavailable at the transmitter.
We propose a novel receiver mode switching scheme for SWIPT based on a new
application of the conventional random beamforming technique at the
multi-antenna transmitter, which generates artificial channel fading to enable
more efficient energy harvesting at each receiver when the received power
exceeds a certain threshold. For the proposed scheme, we investigate the
achievable information rate, harvested average power and/or power outage
probability, as well as their various trade-offs in both AWGN and quasi-static
fading channels. Compared to a reference scheme of periodic receiver mode
switching without random transmit beamforming, the proposed scheme is shown to
be able to achieve better rate-energy trade-offs when the harvested energy
target is sufficiently large. Particularly, it is revealed that employing one
single random beam for the proposed scheme is asymptotically optimal as the
transmit power increases to infinity, and also performs the best with finite
transmit power for the high harvested energy regime of most practical
interests, thus leading to an appealing low-complexity implementation. Finally,
we compare the rate-energy performances of the proposed scheme with different
random beam designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4800</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4800</id><created>2013-01-21</created><authors><author><keyname>Kermia</keyname><forenames>Omar</forenames></author></authors><title>Schedulability Analysis of Distributed Real-Time Applications under
  Dependence and Several Latency Constraints</title><categories>cs.OS cs.DC</categories><comments>8 pages, 6 figures, Published with International Journal of Computer
  Applications (IJCA)</comments><journal-ref>International Journal of Computer Applications 62(14):1-7, January
  2013. Published by Foundation of Computer Science, New York, USA</journal-ref><doi>10.5120/10145-4978</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the analysis of real-time non preemptive multiprocessor
scheduling with precedence and several latency constraints. It aims to specify
a schedulability condition which enables a designer to check a priori -without
executing or simulating- if its scheduling of tasks will hold the precedences
between tasks as well as several latency constraints imposed on determined
pairs of tasks. It is shown that the required analysis is closely linked to the
topological structure of the application graph. More precisely, it depends on
the configuration of tasks paths subject to latency constraints. As a result of
the study, a sufficient schedulability condition is introduced for precedences
and latency constraints in the hardest configuration in term of complexity with
an optimal number of processors in term of applications parallelism. In
addition, the proposed conditions provides a practical lower bounds for general
cases. Performances results and comparisons with an optimal approach
demonstrate the effectiveness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4809</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4809</id><created>2013-01-21</created><authors><author><keyname>Megson</keyname><forenames>G. M.</forenames></author><author><keyname>Cadenas</keyname><forenames>J.</forenames></author></authors><title>A Rank-based Convex Hull method for Dense Data Sets</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel 2-D method for computing the convex hull of a sufficiently dense set
of n integer points is introduced. The approach employs a ranking function that
avoids sorting the points directly thus reducing the overall time complexity.
The ranked points create a simple polygonal chain from which the Convex Hull
can be found using a suitable O(n) method. The result is achieved by placing a
bound on the density (or ratio) of points to m, where m is the maximum value of
the ranking function required to represent the set of points yielding an O(n+m)
method. A fast method is then developed based on the bit length, p, of the data
set which reduces this time further. The required conditions are easily
satisfied by image processing methods which determine the Hulls of polygonal
regions where the densities are in the range of 3%. Our experiments on a range
problem domains show that this is not atypical. Since the complexity of the
method is related to the bit size p for current machines (p=32, p=64) the
method is for all practical purposes O(n). A short proof is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4816</identifier>
 <datestamp>2013-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4816</id><created>2013-01-21</created><updated>2013-06-06</updated><authors><author><keyname>Valent&#xed;n</keyname><forenames>Oriol</forenames></author></authors><title>The Hidden Structural Rules of the Discontinuous Lambek Calculus</title><categories>cs.LO</categories><comments>Submitted to Lambek Festschrift volume</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sequent calculus sL for the Lambek calculus L (lambek 58) has no
structural rules. Interestingly, sL is equivalent to a multimodal calculus mL,
which consists of the nonassociative Lambek calculus with the structural rule
of associativity. This paper proves that the sequent calculus or hypersequent
calculus hD of the discontinuous Lambek calculus (Morrill and Valent\'in),
which like sL has no structural rules, is also equivalent to an omega-sorted
multimodal calculus mD. More concretely, we present a faithful embedding
translation between mD and hD in such a way that it can be said that hD absorbs
the structural rules of mD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4824</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4824</id><created>2013-01-21</created><updated>2013-01-24</updated><authors><author><keyname>Zhou</keyname><forenames>Zhengchun</forenames></author><author><keyname>Zhang</keyname><forenames>Aixian</forenames></author><author><keyname>Ding</keyname><forenames>Cunsheng</forenames></author><author><keyname>Xiong</keyname><forenames>Maosheng</forenames></author></authors><title>The Weight Enumerator of Three Families of Cyclic Codes</title><categories>cs.IT math.IT</categories><comments>13 Pages, 3 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic codes are a subclass of linear codes and have wide applications in
consumer electronics, data storage systems, and communication systems due to
their efficient encoding and decoding algorithms. Cyclic codes with many zeros
and their dual codes have been a subject of study for many years. However,
their weight distributions are known only for a very small number of cases. In
general the calculation of the weight distribution of cyclic codes is heavily
based on the evaluation of some exponential sums over finite fields. Very
recently, Li, Hu, Feng and Ge studied a class of $p$-ary cyclic codes of length
$p^{2m}-1$, where $p$ is a prime and $m$ is odd. They determined the weight
distribution of this class of cyclic codes by establishing a connection between
the involved exponential sums with the spectrum of Hermitian forms graphs. In
this paper, this class of $p$-ary cyclic codes is generalized and the weight
distribution of the generalized cyclic codes is settled for both even $m$ and
odd $m$ alone with the idea of Li, Hu, Feng, and Ge. The weight distributions
of two related families of cyclic codes are also determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4832</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4832</id><created>2013-01-21</created><authors><author><keyname>Breuer</keyname><forenames>Thomas</forenames></author><author><keyname>Csiszar</keyname><forenames>Imre</forenames></author></authors><title>Measuring Model Risk</title><categories>q-fin.RM cs.IT math.IT</categories><comments>30 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to interpret distribution model risk as sensitivity of expected
loss to changes in the risk factor distribution, and to measure the
distribution model risk of a portfolio by the maximum expected loss over a set
of plausible distributions defined in terms of some divergence from an
estimated distribution. The divergence may be relative entropy, a Bregman
distance, or an $f$-divergence. We give formulas for the calculation of
distribution model risk and explicitly determine the worst case distribution
from the set of plausible distributions. We also give formulas for the
evaluation of divergence preferences describing ambiguity averse decision
makers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4839</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4839</id><created>2013-01-21</created><authors><author><keyname>Klein</keyname><forenames>Adrian</forenames></author><author><keyname>Ishikawa</keyname><forenames>Fuyuki</forenames></author><author><keyname>Honiden</keyname><forenames>Shinichi</forenames></author></authors><title>A Scalable Distributed Architecture for Network- and QoS-aware Service
  Composition</title><categories>cs.DC</categories><comments>8 pages, 12 figures. This paper has been accepted and published at
  the 3rd International Joint Agent Workshop and Symposium (IJAWS) 2012,
  Kakegawa, Shizuoka, Japan, October 2012. This version has been copy-edited
  for publication at arXiv.org, but left unchanged besides. Refer to
  http://www.adrianobits.de for the original submission in the IJAWS format</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service-Oriented Computing (SOC) enables the composition of loosely coupled
service agents provided with varying Quality of Service (QoS) levels,
effectively forming a multiagent system (MAS). Selecting a (near-)optimal set
of services for a composition in terms of QoS is crucial when many functionally
equivalent services are available. As the number of distributed services,
especially in the cloud, is rising rapidly, the impact of the network on the
QoS keeps increasing. Despite this and opposed to most MAS approaches, current
service approaches depend on a centralized architecture which cannot adapt to
the network. Thus, we propose a scalable distributed architecture composed of a
flexible number of distributed control nodes. Our architecture requires no
changes to existing services and adapts from a centralized to a completely
distributed realization by adding control nodes as needed. Also, we propose an
extended QoS aggregation algorithm that allows to accurately estimate network
QoS. Finally, we evaluate the benefits and optimality of our architecture in a
distributed environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4845</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4845</id><created>2013-01-21</created><authors><author><keyname>Hedges</keyname><forenames>Julian</forenames></author></authors><title>A generalisation of Nash's theorem with higher-order functionals</title><categories>cs.LO cs.GT</categories><doi>10.1098/rspa.2013.0041</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent theory of sequential games and selection functions by Mar- tin
Escardo and Paulo Oliva is extended to games in which players move
simultaneously. The Nash existence theorem for mixed-strategy equilibria of
finite games is generalised to games defined by selection functions. A normal
form construction is given which generalises the game-theoretic normal form,
and its soundness is proven. Minimax strategies also gener- alise to the new
class of games and are computed by the Berardi-Bezem- Coquand functional,
studied in proof theory as an interpretation of the axiom of countable choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4848</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4848</id><created>2013-01-21</created><authors><author><keyname>Boochs</keyname><forenames>Frank</forenames><affiliation>i3mainz</affiliation></author><author><keyname>Marbs</keyname><forenames>Andreas</forenames><affiliation>i3mainz</affiliation></author><author><keyname>Truong</keyname><forenames>Hung</forenames><affiliation>i3mainz, Le2i</affiliation></author><author><keyname>Hmida</keyname><forenames>Helmi Ben</forenames><affiliation>i3mainz</affiliation></author><author><keyname>Karmacharya</keyname><forenames>Ashish</forenames><affiliation>i3mainz, Le2i</affiliation></author><author><keyname>Cruz</keyname><forenames>Christophe</forenames><affiliation>Le2i</affiliation></author><author><keyname>Habed</keyname><forenames>Adlane</forenames><affiliation>Le2i</affiliation></author><author><keyname>Voisin</keyname><forenames>Yvon</forenames><affiliation>Le2i</affiliation></author><author><keyname>Nicolle</keyname><forenames>Christophe</forenames><affiliation>Le2i</affiliation></author></authors><title>Integration of knowledge to support automatic object reconstruction from
  images and 3D data</title><categories>cs.CG cs.AI</categories><proxy>ccsd</proxy><journal-ref>Systems, Signals and Devices (SSD), 2011 8th International
  Multi-Conference on, Chemnitz : Germany (2011)</journal-ref><doi>10.1109/SSD.2011.5993558</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object reconstruction is an important task in many fields of application as
it allows to generate digital representations of our physical world used as
base for analysis, planning, construction, visualization or other aims. A
reconstruction itself normally is based on reliable data (images, 3D point
clouds for example) expressing the object in his complete extent. This data
then has to be compiled and analyzed in order to extract all necessary
geometrical elements, which represent the object and form a digital copy of it.
Traditional strategies are largely based on manual interaction and
interpretation, because with increasing complexity of objects human
understanding is inevitable to achieve acceptable and reliable results. But
human interaction is time consuming and expensive, why many researches has
already been invested to use algorithmic support, what allows to speed up the
process and to reduce manual work load. Presently most of such supporting
algorithms are data-driven and concentate on specific features of the objects,
being accessible to numerical models. By means of these models, which normally
will represent geometrical (flatness, roughness, for example) or physical
features (color, texture), the data is classified and analyzed. This is
successful for objects with low complexity, but gets to its limits with
increasing complexness of objects. Then purely numerical strategies are not
able to sufficiently model the reality. Therefore, the intention of our
approach is to take human cognitive strategy as an example, and to simulate
extraction processes based on available human defined knowledge for the objects
of interest. Such processes will introduce a semantic structure for the objects
and guide the algorithms used to detect and recognize objects, which will yield
a higher effectiveness. Hence, our research proposes an approach using
knowledge to guide the algorithms in 3D point cloud and image processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4858</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4858</id><created>2013-01-21</created><authors><author><keyname>Quesada</keyname><forenames>Luis</forenames></author><author><keyname>Berzal</keyname><forenames>Fernando</forenames></author><author><keyname>Cubero</keyname><forenames>Juan-Carlos</forenames></author></authors><title>A DSL for Mapping Abstract Syntax Models to Concrete Syntax Models in
  ModelCC</title><categories>cs.PL</categories><comments>arXiv admin note: substantial text overlap with arXiv:1202.6593</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ModelCC is a model-based parser generator that decouples language design from
language processing. ModelCC provides two different mechanisms to specify the
mapping from an abstract syntax model to a concrete syntax model: metadata
annotations defined on top of the abstract syntax model specification and a
domain-specific language for defining ASM-CSM mappings. Using a domain-specific
language to specify the mapping from abstract to concrete syntax models allows
the definition of multiple concrete syntax models for the same abstract syntax
model. In this paper, we describe the ModelCC domain-specific language for
abstract syntax model to concrete syntax model mappings and we showcase its
capabilities by providing a meta-definition of that domain-specific language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4862</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4862</id><created>2013-01-21</created><authors><author><keyname>Baranes</keyname><forenames>Adrien</forenames></author><author><keyname>Oudeyer</keyname><forenames>Pierre-Yves</forenames></author></authors><title>Active Learning of Inverse Models with Intrinsically Motivated Goal
  Exploration in Robots</title><categories>cs.LG cs.AI cs.CV cs.NE cs.RO</categories><journal-ref>Baranes, A., Oudeyer, P-Y. (2013) Active Learning of Inverse
  Models with Intrinsically Motivated Goal Exploration in Robots, Robotics and
  Autonomous Systems, 61(1), pp. 49-73</journal-ref><doi>10.1016/j.robot.2012.05.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the Self-Adaptive Goal Generation - Robust Intelligent Adaptive
Curiosity (SAGG-RIAC) architecture as an intrinsi- cally motivated goal
exploration mechanism which allows active learning of inverse models in
high-dimensional redundant robots. This allows a robot to efficiently and
actively learn distributions of parameterized motor skills/policies that solve
a corresponding distribution of parameterized tasks/goals. The architecture
makes the robot sample actively novel parameterized tasks in the task space,
based on a measure of competence progress, each of which triggers low-level
goal-directed learning of the motor policy pa- rameters that allow to solve it.
For both learning and generalization, the system leverages regression
techniques which allow to infer the motor policy parameters corresponding to a
given novel parameterized task, and based on the previously learnt
correspondences between policy and task parameters. We present experiments with
high-dimensional continuous sensorimotor spaces in three different robotic
setups: 1) learning the inverse kinematics in a highly-redundant robotic arm,
2) learning omnidirectional locomotion with motor primitives in a quadruped
robot, 3) an arm learning to control a fishing rod with a flexible wire. We
show that 1) exploration in the task space can be a lot faster than exploration
in the actuator space for learning inverse models in redundant robots; 2)
selecting goals maximizing competence progress creates developmental
trajectories driving the robot to progressively focus on tasks of increasing
complexity and is statistically significantly more efficient than selecting
tasks randomly, as well as more efficient than different standard active motor
babbling methods; 3) this architecture allows the robot to actively discover
which parts of its task space it can learn to reach and which part it cannot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4870</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4870</id><created>2013-01-21</created><updated>2014-01-23</updated><authors><author><keyname>Mehlhorn</keyname><forenames>Kurt</forenames></author><author><keyname>Sagraloff</keyname><forenames>Michael</forenames></author><author><keyname>Wang</keyname><forenames>Pengming</forenames></author></authors><title>From Approximate Factorization to Root Isolation with Application to
  Cylindrical Algebraic Decomposition</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for isolating the roots of an arbitrary complex
polynomial $p$ that also works for polynomials with multiple roots provided
that the number $k$ of distinct roots is given as part of the input. It outputs
$k$ pairwise disjoint disks each containing one of the distinct roots of $p$,
and its multiplicity. The algorithm uses approximate factorization as a
subroutine.
  In addition, we apply the new root isolation algorithm to a recent algorithm
for computing the topology of a real planar algebraic curve specified as the
zero set of a bivariate integer polynomial and for isolating the real solutions
of a bivariate polynomial system. For input polynomials of degree $n$ and
bitsize $\tau$, we improve the currently best running time from
$\tO(n^{9}\tau+n^{8}\tau^{2})$ (deterministic) to $\tO(n^{6}+n^{5}\tau)$
(randomized) for topology computation and from $\tO(n^{8}+n^{7}\tau)$
(deterministic) to $\tO(n^{6}+n^{5}\tau)$ (randomized) for solving bivariate
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4874</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4874</id><created>2013-01-21</created><updated>2013-02-26</updated><authors><author><keyname>Leroux</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>LABRI, CNRS</affiliation></author></authors><title>Vector Addition System Reversible Reachability Problem</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>F.3.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 1 (February
  27, 2013) lmcs:881</journal-ref><doi>10.2168/LMCS-9(1:5)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reachability problem for vector addition systems is a central problem of
net theory. This problem is known to be decidable but the complexity is still
unknown. Whereas the problem is EXPSPACE-hard, no elementary upper bounds
complexity are known. In this paper we consider the reversible reachability
problem. This problem consists to decide if two configurations are reachable
one from each other, or equivalently if they are in the same strongly connected
component of the reachability graph. We show that this problem is
EXPSPACE-complete. As an application of the introduced materials we
characterize the reversibility domains of a vector addition system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4876</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4876</id><created>2013-01-21</created><authors><author><keyname>Minguillo</keyname><forenames>David</forenames></author><author><keyname>Thelwall</keyname><forenames>Mike</forenames></author></authors><title>The entrepreneurial role of the University: a link analysis of York
  Science Park</title><categories>cs.CY</categories><comments>Proceedings of the ISSI 2001 Conference</comments><journal-ref>Proceedings of the ISSI 2001 Conference - 13th International
  Conference of the International Society for Scientometrics &amp; Informetrics,
  Durban, South Africa, July 4-8. South Africa, pp. 570-583</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study introduces a structured analysis of science parks as arenas
designed to stimulate institutional collaboration and the commercialization of
academic knowledge and technology, and the promotion of social welfare. A
framework for key actors and their potential behaviour in this context is
introduced based on the Triple Helix (TH) model and related literature. A link
analysis was conducted to build an inter-linking network that may map the
infrastructure support network through the online interactions of the
organisations involved in York Science Park. A comparison between the framework
and the diagram shows that the framework can be used to identify most of the
actors and assess their interconnections. The web patterns found correspond to
previous evaluations based on traditional indicators and suggest that the
network, which is developed to foster and support innovation, arises from the
functional cooperation between the University of York and regional authorities,
which both serve as the major driving forces in the trilateral linkages and the
development of an innovation infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4909</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4909</id><created>2013-01-21</created><authors><author><keyname>Ahmed</keyname><forenames>Mohamed</forenames></author><author><keyname>Traverso</keyname><forenames>Stefano</forenames></author><author><keyname>Giaccone</keyname><forenames>Paolo</forenames></author><author><keyname>Leonardi</keyname><forenames>Emilio</forenames></author><author><keyname>Niccolini</keyname><forenames>Saverio</forenames></author></authors><title>Analyzing the Performance of LRU Caches under Non-Stationary Traffic
  Patterns</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents, to the best of our knowledge of the literature, the first
analytic model to address the performance of an LRU (Least Recently Used)
implementing cache under non-stationary traffic conditions, i.e., when the
popularity of content evolves with time. We validate the accuracy of the model
using Monte Carlo simulations. We show that the model is capable of accurately
estimating the cache hit probability, when the popularity of content is
non-stationary.
  We find that there exists a dependency between the performance of an LRU
implementing cache and i) the lifetime of content in a system, ii) the volume
of requests associated with it, iii) the distribution of content request
volumes and iv) the shape of the popularity profile over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4910</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4910</id><created>2013-01-21</created><authors><author><keyname>Alvim</keyname><forenames>M&#xe1;rio S.</forenames></author></authors><title>Computational Aspects of the Calculus of Structure</title><categories>cs.LO cs.AI</categories><comments>Thesis presented by M\'ario S. Alvim as part of the requirements for
  the degree or Master of Science in Computer Science granted by the
  Universidade Federal de Minas Gerais. April, 04th, 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logic is the science of correct inferences and a logical system is a tool to
prove assertions in a certain logic in a correct way. There are many logical
systems, and many ways of formalizing them, e.g., using natural deduction or
sequent calculus. Calculus of structures (CoS) is a new formalism proposed by
Alessio Guglielmi in 2004 that generalizes sequent calculus in the sense that
inference rules can be applied at any depth inside a formula, rather than only
to the main connective. With this feature, proofs in CoS are shorter than in
any other formalism supporting analytical proofs. Although it is great to have
the freedom and expressiveness of CoS, under the point of view of proof search
more freedom means a larger search space. And that should be restricted when
looking for complete automation of deductive systems. Some efforts were made to
reduce this non-determinism, but they are all basically operational approaches,
and no solid theoretical result regarding the computational behaviour of CoS
has been achieved so far. The main focus of this thesis is to discuss ways to
propose a proof search strategy for CoS suitable to implementation. This
strategy should be theoretical instead of purely operational. We introduce the
concept of incoherence number of substructures inside structures and we use
this concept to achieve our main result: there is an algorithm that, according
to our conjecture, corresponds to a proof search strategy to every provable
structure in the subsystem of FBV (the multiplicative linear logic MLL plus the
rule mix) containing only pairwise distinct atoms. Our algorithm is implemented
and we believe our strategy is a good starting point to exploit the
computational aspects of CoS in more general systems, like BV itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4916</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4916</id><created>2013-01-21</created><authors><author><keyname>Correa</keyname><forenames>Denzil</forenames></author><author><keyname>Sureka</keyname><forenames>Ashish</forenames></author></authors><title>Solutions to Detect and Analyze Online Radicalization : A Survey</title><categories>cs.IR cs.SI physics.soc-ph</categories><acm-class>A.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online Radicalization (also called Cyber-Terrorism or Extremism or
Cyber-Racism or Cyber- Hate) is widespread and has become a major and growing
concern to the society, governments and law enforcement agencies around the
world. Research shows that various platforms on the Internet (low barrier to
publish content, allows anonymity, provides exposure to millions of users and a
potential of a very quick and widespread diffusion of message) such as YouTube
(a popular video sharing website), Twitter (an online micro-blogging service),
Facebook (a popular social networking website), online discussion forums and
blogosphere are being misused for malicious intent. Such platforms are being
used to form hate groups, racist communities, spread extremist agenda, incite
anger or violence, promote radicalization, recruit members and create virtual
organi- zations and communities. Automatic detection of online radicalization
is a technically challenging problem because of the vast amount of the data,
unstructured and noisy user-generated content, dynamically changing content and
adversary behavior. There are several solutions proposed in the literature
aiming to combat and counter cyber-hate and cyber-extremism. In this survey, we
review solutions to detect and analyze online radicalization. We review 40
papers published at 12 venues from June 2003 to November 2011. We present a
novel classification scheme to classify these papers. We analyze these
techniques, perform trend analysis, discuss limitations of existing techniques
and find out research gaps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4917</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4917</id><created>2013-01-21</created><authors><author><keyname>Telgarsky</keyname><forenames>Matus</forenames></author></authors><title>Dirichlet draws are sparse with high probability</title><categories>cs.LG math.PR stat.ML</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note provides an elementary proof of the folklore fact that draws from a
Dirichlet distribution (with parameters less than 1) are typically sparse (most
coordinates are small).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4926</identifier>
 <datestamp>2013-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4926</id><created>2013-01-21</created><updated>2013-09-23</updated><authors><author><keyname>Liu</keyname><forenames>Xin-Ji</forenames></author><author><keyname>Xia</keyname><forenames>Shu-Tao</forenames></author></authors><title>Reconstruction Guarantee Analysis of Binary Measurement Matrices Based
  on Girth</title><categories>cs.IT math.IT</categories><comments>accepted by IEEE ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary 0-1 measurement matrices, especially those from coding theory, were
introduced to compressed sensing (CS) recently. Good measurement matrices with
preferred properties, e.g., the restricted isometry property (RIP) and
nullspace property (NSP), have no known general ways to be efficiently checked.
Khajehnejad \emph{et al.} made use of \emph{girth} to certify the good
performances of sparse binary measurement matrices. In this paper, we examine
the performance of binary measurement matrices with uniform column weight and
arbitrary girth under basis pursuit. Explicit sufficient conditions of exact
reconstruction %only including $\gamma$ and $g$ are obtained, which improve the
previous results derived from RIP for any girth $g$ and results from NSP when
$g/2$ is odd. Moreover, we derive explicit $l_1/l_1$, $l_2/l_1$ and
$l_\infty/l_1$ sparse approximation guarantees. These results further show that
large girth has positive impacts on the performance of binary measurement
matrices under basis pursuit, and the binary parity-check matrices of good LDPC
codes are important candidates of measurement matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4927</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4927</id><created>2013-01-21</created><updated>2013-10-31</updated><authors><author><keyname>Morgan</keyname><forenames>Ciara</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>&quot;Pretty strong&quot; converse for the quantum capacity of degradable channels</title><categories>quant-ph cs.IT math.IT</categories><comments>17 pages, requires IEEEtran.cls. v2 has new title and updated
  references; several small errors are corrected and some explanations
  expanded. v3 is final version accepted by journal; improved presentation,
  more references and a few more typos corrected</comments><journal-ref>IEEE Trans. Inf. Theory 60(1):317-333, 2014</journal-ref><doi>10.1109/TIT.2013.2288971</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We exhibit a possible road towards a strong converse for the quantum capacity
of degradable channels. In particular, we show that all degradable channels
obey what we call a &quot;pretty strong&quot; converse: When the code rate increases
above the quantum capacity, the fidelity makes a discontinuous jump from 1 to
at most 0.707, asymptotically. A similar result can be shown for the private
(classical) capacity. Furthermore, we can show that if the strong converse
holds for symmetric channels (which have quantum capacity zero), then
degradable channels obey the strong converse: The above-mentioned asymptotic
jump of the fidelity at the quantum capacity is then from 1 down to 0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4933</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4933</id><created>2013-01-21</created><authors><author><keyname>Minguillo</keyname><forenames>David</forenames></author><author><keyname>Thelwall</keyname><forenames>Mike</forenames></author></authors><title>Mapping the network structure of science parks: An exploratory study of
  cross-sectoral interactions reflected on the web</title><categories>cs.DL cs.CY</categories><doi>10.1108/00012531211244716</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study introduces a method based on link analysis to investigate the
structure of the R&amp;D support infrastructure associated with science parks in
order to determine whether this webometric approach gives plausible results.
Three science parks from Yorkshire and the Humber in the UK were analysed with
webometric and social network analysis techniques. Interlinking networks were
generated through the combination of two different data sets extracted from
three sources (Yahoo!, Bing, SocSciBot). These networks suggest that
institutional sectors, representing business, universities and public bodies,
are primarily tied together by a core formed by research institutions, support
structure organisations and business developers. The comparison of the findings
with traditional indicators suggests that the web-based networks reflect the
offline conditions and policy measures adopted in the region, giving some
evidence that the webometric approach is plausible to investigating science
park networks. This is the first study that applies a web-based approach to
investigate to what extent the science parks facilitate a closer interaction
between the heterogeneous organisations that converge in R&amp;D networks. This
indicates that link analysis may help to get a first insight into the
organisation of the R&amp;D support infrastructure provided by science parks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4938</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4938</id><created>2013-01-21</created><updated>2014-01-03</updated><authors><author><keyname>Retor&#xe9;</keyname><forenames>Christian</forenames><affiliation>LaBRI, IRIT</affiliation></author></authors><title>A type theoretical framework for natural language semantics: the
  Montagovian generative lexicon</title><categories>cs.LO cs.CL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework, named the Montagovian generative lexicon, for
computing the semantics of natural language sentences, expressed in many sorted
higher order logic. Word meaning is depicted by lambda terms of second order
lambda calculus (Girard's system F) with base types including a type for
propositions and many types for sorts of a many sorted logic. This framework is
able to integrate a proper treatment of lexical phenomena into a Montagovian
compositional semantics, including the restriction of selection which imposes
the nature of the arguments of a predicate, and the possible adaptation of a
word meaning to some contexts. Among these adaptations of a word's sense to the
context, ontological inclusions are handled by an extension of system F with
coercive subtyping that is introduced in the present paper. The benefits of
this framework for lexical pragmatics are illustrated on meaning transfers and
coercions, on possible and impossible copredication over different senses, on
deverbal ambiguities, and on &quot;fictive motion&quot;. Next we show that the
compositional treatment of determiners, quantifiers, plurals,... are finer
grained in our framework. We then conclude with the linguistic, logical and
computational perspectives opened by the Montagovian generative lexicon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4941</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4941</id><created>2013-01-21</created><updated>2013-04-02</updated><authors><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author><author><keyname>van Eck</keyname><forenames>Nees Jan</forenames></author></authors><title>A systematic empirical comparison of different approaches for
  normalizing citation impact indicators</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the question how citation-based bibliometric indicators can best
be normalized to ensure fair comparisons between publications from different
scientific fields and different years. In a systematic large-scale empirical
analysis, we compare a traditional normalization approach based on a field
classification system with three source normalization approaches. We pay
special attention to the selection of the publications included in the
analysis. Publications in national scientific journals, popular scientific
magazines, and trade magazines are not included. Unlike earlier studies, we use
algorithmically constructed classification systems to evaluate the different
normalization approaches. Our analysis shows that a source normalization
approach based on the recently introduced idea of fractional citation counting
does not perform well. Two other source normalization approaches generally
outperform the classification-system-based normalization approach that we
study. Our analysis therefore offers considerable support for the use of
source-normalized bibliometric indicators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4944</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4944</id><created>2013-01-21</created><authors><author><keyname>Lauretto</keyname><forenames>Marcelo S.</forenames></author><author><keyname>Silva</keyname><forenames>Barbara B. C.</forenames></author><author><keyname>Andrade</keyname><forenames>Pablo M.</forenames></author></authors><title>Evaluation of a Supervised Learning Approach for Stock Market Operations</title><categories>stat.ML cs.LG stat.AP</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining methods have been widely applied in financial markets, with the
purpose of providing suitable tools for prices forecasting and automatic
trading. Particularly, learning methods aim to identify patterns in time series
and, based on such patterns, to recommend buy/sell operations. The objective of
this work is to evaluate the performance of Random Forests, a supervised
learning method based on ensembles of decision trees, for decision support in
stock markets. Preliminary results indicate good rates of successful operations
and good rates of return per operation, providing a strong motivation for
further research in this topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4952</identifier>
 <datestamp>2013-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4952</id><created>2013-01-21</created><updated>2013-04-25</updated><authors><author><keyname>Belazzougui</keyname><forenames>Djamal</forenames></author><author><keyname>Pierrot</keyname><forenames>Adeline</forenames></author><author><keyname>Raffinot</keyname><forenames>Mathieu</forenames></author><author><keyname>Vialette</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Single and multiple consecutive permutation motif search</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $t$ be a permutation (that shall play the role of the {\em text}) on
$[n]$ and a pattern $p$ be a sequence of $m$ distinct integer(s) of $[n]$,
$m\leq n$. The pattern $p$ occurs in $t$ in position $i$ if and only if $p_1...
p_m$ is order-isomorphic to $t_i... t_{i+m-1}$, that is, for all $1 \leq k&lt;
\ell \leq m$, $p_k&gt;p_\ell$ if and only if $t_{i+k-1}&gt;t_{i+\ell-1}$. Searching
for a pattern $p$ in a text $t$ consists in identifying all occurrences of $p$
in $t$. We first present a forward automaton which allows us to search for $p$
in $t$ in $O(m^2\log \log m +n)$ time. We then introduce a Morris-Pratt
automaton representation of the forward automaton which allows us to reduce
this complexity to $O(m\log \log m +n)$ at the price of an additional amortized
constant term by integer of the text. Both automata occupy $O(m)$ space. We
then extend the problem to search for a set of patterns and exhibit a specific
Aho-Corasick like algorithm. Next we present a sub-linear average case search
algorithm running in $O(\frac{m\log m}{\log\log m}+\frac{n\log m}{m\log\log
m})$ time, that we eventually prove to be optimal on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4958</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4958</id><created>2013-01-21</created><authors><author><keyname>Harris</keyname><forenames>David</forenames></author><author><keyname>Purohit</keyname><forenames>Manish</forenames></author></authors><title>Improved algorithms and analysis for the laminar matroid secretary
  problem</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a matroid secretary problem, one is presented with a sequence of objects
of various weights in a random order, and must choose irrevocably to accept or
reject each item. There is a further constraint that the set of items selected
must form an independent set of an associated matroid. Constant-competitive
algorithms (algorithms whose expected solution weight is within a constant
factor of the optimal) are known for many types of matroid secretary problems.
We examine the laminar matroid and show an algorithm achieving provably 0.053
competitive ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4972</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4972</id><created>2013-01-21</created><updated>2013-07-19</updated><authors><author><keyname>Currie</keyname><forenames>James D.</forenames></author><author><keyname>Rampersad</keyname><forenames>Narad</forenames></author><author><keyname>Saari</keyname><forenames>Kalle</forenames></author><author><keyname>Zamboni</keyname><forenames>Luca Q.</forenames></author></authors><title>Extremal words in morphic subshifts</title><categories>math.CO cs.DM cs.FL</categories><comments>Replaces a previous version entitled &quot;Extremal words in the shift
  orbit closure of a morphic sequence&quot; with an added result on primitive
  morphic sequences. Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an infinite word X over an alphabet A a letter b occurring in X, and a
total order \sigma on A, we call the smallest word with respect to \sigma
starting with b in the shift orbit closure of X an extremal word of X. In this
paper we consider the extremal words of morphic words. If X = g(f^{\omega}(a))
for some morphisms f and g, we give two simple conditions on f and g that
guarantees that all extremal words are morphic. This happens, in particular,
when X is a primitive morphic or a binary pure morphic word. Our techniques
provide characterizations of the extremal words of the Period-doubling word and
the Chacon word and give a new proof of the form of the lexicographically least
word in the shift orbit closure of the Rudin-Shapiro word.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4973</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4973</id><created>2013-01-21</created><authors><author><keyname>R&#xfc;mmer</keyname><forenames>Philipp</forenames><affiliation>Uppsala University</affiliation></author><author><keyname>Hojjat</keyname><forenames>Hossein</forenames><affiliation>EPFL Lausanne</affiliation></author><author><keyname>Kuncak</keyname><forenames>Viktor</forenames><affiliation>EPFL Lausanne</affiliation></author></authors><title>Disjunctive Interpolants for Horn-Clause Verification (Extended
  Technical Report)</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main challenges in software verification is efficient and precise
compositional analysis of programs with procedures and loops. Interpolation
methods remain one of the most promising techniques for such verification, and
are closely related to solving Horn clause constraints. We introduce a new
notion of interpolation, disjunctive interpolation, which solve a more general
class of problems in one step compared to previous notions of interpolants,
such as tree interpolants or inductive sequences of interpolants. We present
algorithms and complexity for construction of disjunctive interpolants, as well
as their use within an abstraction-refinement loop. We have implemented Horn
clause verification algorithms that use disjunctive interpolants and evaluate
them on benchmarks expressed as Horn clauses over the theory of integer linear
arithmetic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4981</identifier>
 <datestamp>2014-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4981</id><created>2013-01-21</created><updated>2014-04-23</updated><authors><author><keyname>Bonfante</keyname><forenames>Guillaume</forenames></author><author><keyname>Deloup</keyname><forenames>Florian</forenames></author></authors><title>The genus of regular languages</title><categories>cs.FL cs.DM math.CO</categories><comments>36 pages, about 30 pdf figures; table of contents and new references
  added; pages numbered; other minor changes; email and addresses of authors
  added; new example and figure added; improvement of the upper bound in the
  main theorem</comments><msc-class>68R10, 68R15</msc-class><acm-class>F.1.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article defines and studies the genus of finite state deterministic
automata (FSA) and regular languages. Indeed, a FSA can be seen as a graph for
which the notion of genus arises. At the same time, a FSA has a semantics via
its underlying language. It is then natural to make a connection between the
languages and the notion of genus. After we introduce and justify the the
notion of the genus for regular languages, the following questions are
addressed. First, depending on the size of the alphabet, we provide upper and
lower bounds on the genus of regular languages : we show that under a
relatively generic condition on the alphabet and the geometry of the automata,
the genus grows at least linearly in terms of the size of the automata. Second,
we show that the topological cost of the powerset determinization procedure is
exponential. Third, we prove that the notion of minimization is orthogonal to
the notion of genus. Fourth, we build regular languages of arbitrary large
genus: the notion of genus defines a proper hierarchy of regular languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4983</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4983</id><created>2013-01-21</created><authors><author><keyname>Cha</keyname><forenames>Yongjae</forenames></author></authors><title>Closed form solutions of linear difference equations in terms of
  symmetric products</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show how to find a closed form solution for third order
difference operators in terms of solutions of second order operators. This work
is an extension of previous results on finding closed form solutions of
recurrence equations and a counterpart to existing results on differential
equations. As motivation and application for this work, we discuss the problem
of proving positivity of sequences given merely in terms of their defining
recurrence relation. The main advantage of the present approach to earlier
methods attacking the same problem is that our algorithm provides
human-readable and verifiable, i.e., certified proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4988</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4988</id><created>2013-01-19</created><authors><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Jain</keyname><forenames>Satbir</forenames></author></authors><title>Centralized Lifetime Maximizing Tree For Wireless Sensor Networks</title><categories>cs.NI</categories><comments>6 pages</comments><journal-ref>International Journal of Computer and Electrical Engineering vol.
  1, no. 5,2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To enable data aggregation among the event sources in wireless sensor
networks and to reduce the communication cost there is a need to establish a
coveraged tree structure inside any given event region to allow data reports to
be aggregated at a single processing point prior to transmission to the
network. In this paper we propose a novel technique to create one such tree
which maximizes the lifetime of the event sources while they are constantly
transmitting for data aggregation. We use the term Centralized Lifetime
Maximizing Tree (CLMT) to denote this tree. CLMT features with identification
of bottleneck node among the given set of nodes. This node collects the data
from every other node via routes with the highest branch energy subject to
condition loop is not created. By constructing tree in such a way, protocol is
able to reduce the frequency of tree reconstruction, minimize the delay and
maximize the functional lifetime of source nodes by minimizing the additional
energy involved in tree reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4991</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4991</id><created>2013-01-21</created><authors><author><keyname>Hmida</keyname><forenames>Helmi Ben</forenames><affiliation>i3mainz</affiliation></author><author><keyname>Cruz</keyname><forenames>Christophe</forenames><affiliation>Le2i</affiliation></author><author><keyname>Boochs</keyname><forenames>Frank</forenames><affiliation>i3mainz</affiliation></author><author><keyname>Nicolle</keyname><forenames>Christophe</forenames><affiliation>Le2i</affiliation></author></authors><title>Knowledge Base Approach for 3D Objects Detection in Point Clouds Using
  3D Processing and Specialists Knowledge</title><categories>cs.AI</categories><comments>ISSN: 1942-2679. arXiv admin note: text overlap with arXiv:1301.4783</comments><proxy>ccsd</proxy><journal-ref>International Journal On Advances in Intelligent Systems 5, 1 et 2
  (2012) 1-14</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a knowledge-based detection of objects approach using the
OWL ontology language, the Semantic Web Rule Language, and 3D processing
built-ins aiming at combining geometrical analysis of 3D point clouds and
specialist's knowledge. Here, we share our experience regarding the creation of
3D semantic facility model out of unorganized 3D point clouds. Thus, a
knowledge-based detection approach of objects using the OWL ontology language
is presented. This knowledge is used to define SWRL detection rules. In
addition, the combination of 3D processing built-ins and topological Built-Ins
in SWRL rules allows a more flexible and intelligent detection, and the
annotation of objects contained in 3D point clouds. The created WiDOP prototype
takes a set of 3D point clouds as input, and produces as output a populated
ontology corresponding to an indexed scene visualized within VRML language. The
context of the study is the detection of railway objects materialized within
the Deutsche Bahn scene such as signals, technical cupboards, electric poles,
etc. Thus, the resulting enriched and populated ontology, that contains the
annotations of objects in the point clouds, is used to feed a GIS system or an
IFC file for architecture purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.4992</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.4992</id><created>2013-01-21</created><authors><author><keyname>Hmida</keyname><forenames>Helmi Ben</forenames><affiliation>i3mainz</affiliation></author><author><keyname>Cruz</keyname><forenames>Christophe</forenames><affiliation>Le2i</affiliation></author><author><keyname>Boochs</keyname><forenames>Frank</forenames><affiliation>i3mainz</affiliation></author><author><keyname>Nicolle</keyname><forenames>Christophe</forenames><affiliation>Le2i</affiliation></author></authors><title>From 9-IM Topological Operators to Qualitative Spatial Relations using
  3D Selective Nef Complexes and Logic Rules for bodies</title><categories>cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1301.4780</comments><proxy>ccsd</proxy><journal-ref>International Conference on Knowledge Engineering and Ontology
  Development, Barcelone : Spain (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method to compute automatically topological relations
using SWRL rules. The calculation of these rules is based on the definition of
a Selective Nef Complexes Nef Polyhedra structure generated from standard
Polyhedron. The Selective Nef Complexes is a data model providing a set of
binary Boolean operators such as Union, Difference, Intersection and Symmetric
difference, and unary operators such as Interior, Closure and Boundary. In this
work, these operators are used to compute topological relations between objects
defined by the constraints of the 9 Intersection Model (9-IM) from Egenhofer.
With the help of these constraints, we defined a procedure to compute the
topological relations on Nef polyhedra. These topological relationships are
Disjoint, Meets, Contains, Inside, Covers, CoveredBy, Equals and Overlaps, and
defined in a top-level ontology with a specific semantic definition on relation
such as Transitive, Symmetric, Asymmetric, Functional, Reflexive, and
Irreflexive. The results of the computation of topological relationships are
stored in an OWL-DL ontology allowing after what to infer on these new
relationships between objects. In addition, logic rules based on the Semantic
Web Rule Language allows the definition of logic programs that define which
topological relationships have to be computed on which kind of objects with
specific attributes. For instance, a &quot;Building&quot; that overlaps a &quot;Railway&quot; is a
&quot;RailStation&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5003</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5003</id><created>2013-01-21</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Sampaio-Neto</keyname><forenames>Raimundo</forenames></author></authors><title>Adaptive Interference Suppression for CDMA Systems using Interpolated
  FIR Filters with Adaptive Interpolators in Multipath Channels</title><categories>cs.IT math.IT</categories><comments>9 figures; IEEE Transactions on Vehicular Technology, 2007</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we propose an adaptive linear receiver structure based on
interpolated finite impulse response (FIR) filters with adaptive interpolators
for direct sequence code division multiple access (DS-CDMA) systems in
multipath channels. The interpolated minimum mean-squared error (MMSE) and the
interpolated constrained minimum variance (CMV) solutions are described for a
novel scheme where the interpolator is rendered time-varying in order to
mitigate multiple access interference (MAI) and multiple-path propagation
effects. Based upon the interpolated MMSE and CMV solutions we present
computationally efficient stochastic gradient (SG) and exponentially weighted
recursive least squares type (RLS) algorithms for both receiver and
interpolator filters in the supervised and blind modes of operation. A
convergence analysis of the algorithms and a discussion of the convergence
properties of the method are carried out for both modes of operation.
Simulation experiments for a downlink scenario show that the proposed
structures achieve a superior BER convergence and steady-state performance to
previously reported reduced-rank receivers at lower complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5004</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5004</id><created>2013-01-21</created><authors><author><keyname>Zieve</keyname><forenames>Michael</forenames></author></authors><title>Planar functions and perfect nonlinear monomials over finite fields</title><categories>math.CO cs.IT math.IT math.NT</categories><comments>10 pages</comments><msc-class>51E20, 11T06, 11T71, 05B05</msc-class><journal-ref>Designs, Codes and Cryptography 75 (2015), 71-80</journal-ref><doi>10.1007/s10623-013-9890-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of finite projective planes involves planar functions, namely,
functions f : F_q --&gt; F_q such that, for each nonzero a in F_q, the function c
--&gt; f(c+a) - f(c) is a bijection on F_q. Planar functions are also used in the
construction of DES-like cryptosystems, where they are called perfect nonlinear
functions. We determine all planar functions on F_q of the form c --&gt; c^t,
under the assumption that q &gt;= (t-1)^4. This implies two conjectures of
Hernando, McGuire and Monserrat. Our arguments also yield a new proof of a
conjecture of Segre and Bartocci from 1971 about monomial hyperovals in finite
Desarguesian projective planes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5006</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5006</id><created>2013-01-21</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Sampaio-Neto</keyname><forenames>Raimundo</forenames></author></authors><title>Blind Adaptive Algorithms for Decision Feedback DS-CDMA Receivers in
  Multipath Channels</title><categories>cs.IT math.IT</categories><comments>10 figures; IEEE Transactions on Vehicular Technology, 2006. arXiv
  admin note: substantial text overlap with arXiv:1205.4389</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we examine blind adaptive and iterative decision feedback (DF)
receivers for direct sequence code division multiple access (DS-CDMA) systems
in frequency selective channels. Code-constrained minimum variance (CMV) and
constant modulus (CCM) design criteria for DF receivers based on constrained
optimization techniques are investigated for scenarios subject to multipath.
Computationally efficient blind adaptive stochastic gradient (SG) and recursive
least squares (RLS) algorithms are developed for estimating the parameters of
DF detectors along with successive, parallel and iterative DF structures. A
novel successive parallel arbitrated DF scheme is presented and combined with
iterative techniques for use with cascaded DF stages in order to mitigate the
deleterious effects of error propagation. Simulation results for an uplink
scenario assess the algorithms, the blind adaptive DF detectors against linear
receivers and evaluate the effects of error propagation of the new
cancellations techniques against previously reported approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5011</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5011</id><created>2013-01-21</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Sampaio-Neto</keyname><forenames>Raimundo</forenames></author></authors><title>Adaptive Space-Time Decision Feedback Neural Detectors with Data
  Selection for High-Data Rate Users in DS-CDMA Systems</title><categories>cs.IT math.IT</categories><comments>6 figures; IEEE Transactions on Neural Networks, 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A space-time adaptive decision feedback (DF) receiver using recurrent neural
networks (RNN) is proposed for joint equalization and interference suppression
in direct-sequence code-division-multiple-access (DS-CDMA) systems equipped
with antenna arrays. The proposed receiver structure employs dynamically driven
RNNs in the feedforward section for equalization and multi-access interference
suppression and a finite impulse response (FIR) linear filter in the feedback
section for performing interference cancellation. A data selective gradient
algorithm, based upon the set-membership design framework, is proposed for the
estimation of the coefficients of RNN structures and is applied to the
estimation of the parameters of the proposed neural receiver structure.
Simulation results show that the proposed techniques achieve significant
performance gains over existing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5022</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5022</id><created>2013-01-21</created><authors><author><keyname>Torra</keyname><forenames>Vicen&#xe7;</forenames></author><author><keyname>Stokes</keyname><forenames>Klara</forenames></author></authors><title>A formalization of re-identification in terms of compatible
  probabilities</title><categories>cs.CR cs.AI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Re-identification algorithms are used in data privacy to measure disclosure
risk. They model the situation in which an adversary attacks a published
database by means of linking the information of this adversary with the
database.
  In this paper we formalize this type of algorithm in terms of true
probabilities and compatible belief functions. The purpose of this work is to
leave aside as re-identification algorithms those algorithms that do not
satisfy a minimum requirement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5027</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5027</id><created>2013-01-21</created><updated>2013-01-28</updated><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author><author><keyname>Slavkovsky</keyname><forenames>Elizabeth</forenames></author></authors><title>Thinking like Archimedes with a 3D printer</title><categories>math.HO cs.ET</categories><comments>16 pages, 13 figures, (fixed some typos)</comments><msc-class>01A20, 00A66, 97U99, 97Q60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We illustrate Archimedes' method using models produced with 3D printers. This
approach allowed us to create physical proofs of results known to Archimedes
and illustrate ideas of a mathematician who is known both for his for his
mechanical inventions as well as his breakthroughs in geometry and calculus. We
use technology from the 21st century to trace intellectual achievements from
the 3rd century BC. While we celebrate the 2300th birthday of Archimedes
(287-212 BC) in 2013, we also live in an exciting time, where 3D printing is
becoming popular and affordable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5033</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5033</id><created>2013-01-21</created><authors><author><keyname>Li</keyname><forenames>Shang</forenames></author><author><keyname>McKay</keyname><forenames>Matthew R.</forenames></author><author><keyname>Chen</keyname><forenames>Yang</forenames></author></authors><title>On the Distribution of MIMO Mutual Information: An In-Depth Painlev\'{e}
  Based Characterization</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transaction on Information Theory (under revision)</comments><journal-ref>IEEE Trans. Inf. The., 59(9), pp. 5271-5296, 2013</journal-ref><doi>10.1109/TIT.2013.2264505</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper builds upon our recent work which computed the moment generating
function of the MIMO mutual information exactly in terms of a Painlev\'{e} V
differential equation. By exploiting this key analytical tool, we provide an
in-depth characterization of the mutual information distribution for
sufficiently large (but finite) antenna numbers. In particular, we derive
systematic closed-form expansions for the high order cumulants. These results
yield considerable new insight, such as providing a technical explanation as to
why the well known Gaussian approximation is quite robust to large SNR for the
case of unequal antenna arrays, whilst it deviates strongly for equal antenna
arrays. In addition, by drawing upon our high order cumulant expansions, we
employ the Edgeworth expansion technique to propose a refined Gaussian
approximation which is shown to give a very accurate closed-form
characterization of the mutual information distribution, both around the mean
and for moderate deviations into the tails (where the Gaussian approximation
fails remarkably). For stronger deviations where the Edgeworth expansion
becomes unwieldy, we employ the saddle point method and asymptotic integration
tools to establish new analytical characterizations which are shown to be very
simple and accurate. Based on these results we also recover key well
established properties of the tail distribution, including the
diversity-multiplexing-tradeoff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5034</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5034</id><created>2013-01-21</created><updated>2013-08-05</updated><authors><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Downlink MIMO HetNets: Modeling, Ordering Results and Performance
  Analysis</title><categories>cs.IT cs.NI math.IT</categories><comments>IEEE Transactions on Wireless Communications, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a general downlink model for multi-antenna heterogeneous cellular
networks (HetNets), where base stations (BSs) across tiers may differ in terms
of transmit power, target signal-to-interference-ratio (SIR), deployment
density, number of transmit antennas and the type of multi-antenna
transmission. In particular, we consider and compare space division multiple
access (SDMA), single user beamforming (SU-BF), and baseline single-input
single-output (SISO) transmission. For this general model, the main
contributions are: (i) ordering results for both coverage probability and per
user rate in closed form for any BS distribution for the three considered
techniques, using novel tools from stochastic orders, (ii) upper bounds on the
coverage probability assuming a Poisson BS distribution, and (iii) a comparison
of the area spectral efficiency (ASE). The analysis concretely demonstrates,
for example, that for a given total number of transmit antennas in the network,
it is preferable to spread them across many single-antenna BSs vs. fewer
multi-antenna BSs. Another observation is that SU-BF provides higher coverage
and per user data rate than SDMA, but SDMA is in some cases better in terms of
ASE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5038</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5038</id><created>2013-01-21</created><authors><author><keyname>Bostan</keyname><forenames>Alin</forenames></author><author><keyname>Chen</keyname><forenames>Shaoshi</forenames></author><author><keyname>Chyzak</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Li</keyname><forenames>Ziming</forenames></author><author><keyname>Xin</keyname><forenames>Guoce</forenames></author></authors><title>Hermite Reduction and Creative Telescoping for Hyperexponential
  Functions</title><categories>cs.SC math.CO</categories><comments>8 pages</comments><msc-class>33F10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a reduction algorithm that simultaneously extends Hermite's
reduction for rational functions and the Hermite-like reduction for
hyperexponential functions. It yields a unique additive decomposition and
allows to decide hyperexponential integrability. Based on this reduction
algorithm, we design a new method to compute minimal telescopers for bivariate
hyperexponential functions. One of its main features is that it can avoid the
costly computation of certificates. Its implementation outperforms Maple's
function DEtools[Zeilberger]. Moreover, we derive an order bound on minimal
telescopers, which is more general and tighter than the known one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5044</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5044</id><created>2013-01-21</created><authors><author><keyname>Huang</keyname><forenames>Yichao</forenames></author><author><keyname>Rao</keyname><forenames>Bhaskar D.</forenames></author></authors><title>Performance Analysis of Heterogeneous Feedback Design in an OFDMA
  Downlink with Partial and Imperfect Feedback</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Trans. on Signal Processing</comments><doi>10.1109/TSP.2012.2229275</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current OFDMA systems group resource blocks into subband to form the basic
feedback unit. Homogeneous feedback design with a common subband size is not
aware of the heterogeneous channel statistics among users. Under a general
correlated channel model, we demonstrate the gain of matching the subband size
to the underlying channel statistics motivating heterogeneous feedback design
with different subband sizes and feedback resources across clusters of users.
Employing the best-M partial feedback strategy, users with smaller subband size
would convey more partial feedback to match the frequency selectivity. In order
to develop an analytical framework to investigate the impact of partial
feedback and potential imperfections, we leverage the multi-cluster subband
fading model. The perfect feedback scenario is thoroughly analyzed, and the
closed form expression for the average sum rate is derived for the
heterogeneous partial feedback system. We proceed to examine the effect of
imperfections due to channel estimation error and feedback delay, which leads
to additional consideration of system outage. Two transmission strategies: the
fix rate and the variable rate, are considered for the outage analysis. We also
investigate how to adapt to the imperfections in order to maximize the average
goodput under heterogeneous partial feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5045</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5045</id><created>2013-01-21</created><authors><author><keyname>Bostan</keyname><forenames>Alin</forenames></author><author><keyname>Chen</keyname><forenames>Shaoshi</forenames></author><author><keyname>Chyzak</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Li</keyname><forenames>Ziming</forenames></author></authors><title>Complexity of Creative Telescoping for Bivariate Rational Functions</title><categories>cs.SC</categories><comments>8 pages</comments><msc-class>33F10</msc-class><journal-ref>Proceedings of the 2010 International Symposium on Symbolic and
  Algebraic Computation, pages 203--210, 2010, ACM</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The long-term goal initiated in this work is to obtain fast algorithms and
implementations for definite integration in Almkvist and Zeilberger's framework
of (differential) creative telescoping. Our complexity-driven approach is to
obtain tight degree bounds on the various expressions involved in the method.
To make the problem more tractable, we restrict to bivariate rational
functions. By considering this constrained class of inputs, we are able to
blend the general method of creative telescoping with the well-known Hermite
reduction. We then use our new method to compute diagonals of rational power
series arising from combinatorics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5046</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5046</id><created>2013-01-21</created><updated>2013-01-22</updated><authors><author><keyname>Chen</keyname><forenames>Shaoshi</forenames></author><author><keyname>Feng</keyname><forenames>Ruyong</forenames></author><author><keyname>Fu</keyname><forenames>Guofeng</forenames></author><author><keyname>Li</keyname><forenames>Ziming</forenames></author></authors><title>On the Structure of Compatible Rational Functions</title><categories>cs.SC math.CO</categories><msc-class>33F10</msc-class><journal-ref>Proceedings of the 2011 International Symposium on Symbolic and
  Algebraic Computation, pages 91--98, 2011, ACM</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A finite number of rational functions are compatible if they satisfy the
compatibility conditions of a first-order linear functional system involving
differential, shift and q-shift operators. We present a theorem that describes
the structure of compatible rational functions. The theorem enables us to
decompose a solution of such a system as a product of a rational function,
several symbolic powers, a hyperexponential function, a hypergeometric term,
and a q-hypergeometric term. We outline an algorithm for computing this
product, and present an application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5047</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5047</id><created>2013-01-21</created><updated>2014-02-01</updated><authors><author><keyname>Kar</keyname><forenames>Soummya</forenames></author><author><keyname>Moura</keyname><forenames>Jose</forenames></author></authors><title>Asymptotically Efficient Distributed Estimation With Exponential Family
  Statistics</title><categories>math.PR cs.IT math.IT math.OC</categories><comments>Submitted to the IEEE Transactions on Information Theory. Initial
  Submission: Jan. 2013. Revised: Jul. 2014. 43 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies the problem of distributed parameter estimation in
multi-agent networks with exponential family observation statistics. A
certainty-equivalence type distributed estimator of the consensus + innovations
form is proposed in which, at each each observation sampling epoch agents
update their local parameter estimates by appropriately combining the data
received from their neighbors and the locally sensed new information
(innovation). Under global observability of the networked sensing model, i.e.,
the ability to distinguish between different instances of the parameter value
based on the joint observation statistics, and mean connectivity of the
inter-agent communication network, the proposed estimator is shown to yield
consistent parameter estimates at each network agent. Further, it is shown that
the distributed estimator is asymptotically efficient, in that, the asymptotic
covariances of the agent estimates coincide with that of the optimal
centralized estimator, i.e., the inverse of the centralized Fisher information
rate. From a technical viewpoint, the proposed distributed estimator leads to
non-Markovian mixed timescale stochastic recursions and the analytical methods
developed in the paper contribute to the general theory of distributed
stochastic approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5054</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5054</id><created>2013-01-21</created><updated>2013-07-11</updated><authors><author><keyname>Bouchard-C&#xf4;t&#xe9;</keyname><forenames>Alexandre</forenames></author></authors><title>A Note on Probabilistic Models over Strings: the Linear Algebra Approach</title><categories>q-bio.PE cs.FL stat.CO</categories><comments>17 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic models over strings have played a key role in developing
methods allowing indels to be treated as phylogenetically informative events.
There is an extensive literature on using automata and transducers on
phylogenies to do inference on these probabilistic models, in which an
important theoretical question in the field is the complexity of computing the
normalization of a class of string-valued graphical models. This question has
been investigated using tools from combinatorics, dynamic programming, and
graph theory, and has practical applications in Bayesian phylogenetics. In this
work, we revisit this theoretical question from a different point of view,
based on linear algebra. The main contribution is a new proof of a known result
on the complexity of inference on TKF91, a well-known probabilistic model over
strings. Our proof uses a different approach based on classical linear algebra
results, and is in some cases easier to extend to other models. The proving
method also has consequences on the implementation and complexity of inference
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5055</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5055</id><created>2013-01-21</created><authors><author><keyname>Isgur</keyname><forenames>Abraham</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Vitaly</forenames></author><author><keyname>Rahman</keyname><forenames>Mustazee</forenames></author><author><keyname>Tanny</keyname><forenames>Stephen</forenames></author></authors><title>Nested Recursions, Simultaneous Parameters and Tree Superpositions</title><categories>math.CO cs.DM</categories><comments>38 pages, 20 figures</comments><msc-class>11B37, 05C05 (Primary) 05A15, 05A19 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply a tree-based methodology to solve new, very broadly defined families
of nested recursions of the general form R(n)=sum_{i=1}^k R(n-a_i-sum_{j=1}^p
R(n-b_{ij})), where a_i are integers, b_{ij} are natural numbers, and k,p are
natural numbers that we use to denote &quot;arity&quot; and &quot;order,&quot; respectively, and
with some specified initial conditions. The key idea of the tree-based solution
method is to associate such recursions with infinite labelled trees in a
natural way so that the solution to the recursions solves a counting question
relating to the corresponding trees. We characterize certain recursion families
within R(n) by introducing &quot;simultaneous parameters&quot; that appear both within
the recursion itself and that also specify structural properties of the
corresponding tree. First, we extend and unify recently discovered results
concerning two families of arity k=2, order p=1 recursions. Next, we
investigate the solution of nested recursion families by taking linear
combinations of solution sequence frequencies for simpler nested recursions,
which correspond to superpositions of the associated trees; this leads us to
identify and solve two new recursion families for arity k=2 and general order
p. Finally, we extend these results to general arity k&gt;2. We conclude with
several related open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5061</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5061</id><created>2013-01-21</created><updated>2013-03-24</updated><authors><author><keyname>He</keyname><forenames>Fei</forenames></author><author><keyname>Sun</keyname><forenames>Yin</forenames></author><author><keyname>Xiao</keyname><forenames>Limin</forenames></author><author><keyname>Chen</keyname><forenames>Xiang</forenames></author><author><keyname>Chi</keyname><forenames>Chong-Yung</forenames></author><author><keyname>Zhou</keyname><forenames>Shidong</forenames></author></authors><title>Capacity Region Bounds and Resource Allocation for Two-Way OFDM Relay
  Channels</title><categories>cs.IT math.IT</categories><comments>Technical Report Version, 35 pages, 8 figures. Manuscript submitted
  to IEEE Transactions on Wireless Communications, August 13, 2012; revised
  December 22, 2012 and February 23, 2013; accepted February 26, 2013</comments><doi>10.1109/TWC.2013.13.121205.</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider two-way orthogonal frequency division multiplexing
(OFDM) relay channels, where the direct link between the two terminal nodes is
too weak to be used for data transmission. The widely known per-subcarrier
decode-and-forward (DF) relay strategy, treats each subcarrier as a separate
channel, and performs independent channel coding over each subcarrier. We show
that this per-subcarrier DF relay strategy is only a suboptimal DF relay
strategy, and present a multi-subcarrier DF relay strategy which utilizes
cross-subcarrier channel coding to achieve a larger rate region. We then
propose an optimal resource allocation algorithm to characterize the achievable
rate region of the multi-subcarrier DF relay strategy. The computational
complexity of this algorithm is much smaller than that of standard Lagrangian
duality optimization algorithms. We further analyze the asymptotic performance
of two-way relay strategies including the above two DF relay strategies and an
amplify-and-forward (AF) relay strategy. The analysis shows that the
multi-subcarrier DF relay strategy tends to achieve the capacity region of the
two-way OFDM relay channels in the low signal-to-noise ratio (SNR) regime,
while the AF relay strategy tends to achieve the multiplexing gain region of
the two-way OFDM relay channels in the high SNR regime. Numerical results are
provided to justify all the analytical results and the efficacy of the proposed
optimal resource allocation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5063</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5063</id><created>2013-01-21</created><updated>2013-04-03</updated><authors><author><keyname>Rudovic</keyname><forenames>Ognjen</forenames></author><author><keyname>Pantic</keyname><forenames>Maja</forenames></author><author><keyname>Pavlovic</keyname><forenames>Vladimir</forenames></author></authors><title>Heteroscedastic Conditional Ordinal Random Fields for Pain Intensity
  Estimation from Facial Images</title><categories>cs.CV cs.LG stat.ML</categories><comments>This paper has been withdrawn by the authors due to a crucial sign
  error in equation 2&amp;3</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We propose a novel method for automatic pain intensity estimation from facial
images based on the framework of kernel Conditional Ordinal Random Fields
(KCORF). We extend this framework to account for heteroscedasticity on the
output labels(i.e., pain intensity scores) and introduce a novel dynamic
features, dynamic ranks, that impose temporal ordinal constraints on the static
ranks (i.e., intensity scores). Our experimental results show that the proposed
approach outperforms state-of-the art methods for sequence classification with
ordinal data and other ordinal regression models. The approach performs
significantly better than other models in terms of Intra-Class Correlation
measure, which is the most accepted evaluation measure in the tasks of facial
behaviour intensity estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5065</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5065</id><created>2013-01-21</created><authors><author><keyname>Sen</keyname><forenames>Jaydip</forenames></author></authors><title>Security in Wireless Sensor Networks</title><categories>cs.CR cs.NI</categories><comments>51 pages. 5 Figures and 7 Tables. Book Chapter in the book &quot;Wireless
  Sensor Networks: Current Status and Future Trends&quot;, A.S.K. Pathan et al.
  (eds.), CRC Press USA, November 2012; This book chapter is an extended
  version of author's earlier arXiv submission: arXiv:1011.1529 and also has
  substantial text overlap with arXiv:1101.2759</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks have attracted a lot of interest over the last
decade in wireless and mobile computing research community. Applications of
these networks are numerous and growing, which range from indoor deployment
scenarios in the home and office to outdoor deployment in adversary's territory
in a tactical battleground. However, due to distributed nature and their
deployment in remote areas, these networks are vulnerable to numerous security
threats that can adversely affect their performance. This chapter provides a
comprehensive discussion on the state of the art in security technologies for
wireless sensor networks. It identifies various possible attacks at different
layers of the communication protocol stack in a typical sensor network and
their possible countermeasures. A brief discussion on the future direction of
research in WSN security is also included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5069</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5069</id><created>2013-01-21</created><authors><author><keyname>Grigoriev</keyname><forenames>Dima</forenames></author><author><keyname>Shpilrain</keyname><forenames>Vladimir</forenames></author></authors><title>Secrecy without one-way functions</title><categories>cs.CR cs.IT math.IT</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that some problems in information security can be solved without
using one-way functions. The latter are usually regarded as a central concept
of cryptography, but the very existence of one-way functions depends on
difficult conjectures in complexity theory, most notably on the notorious &quot;$P
\ne NP$&quot; conjecture.
  In this paper, we suggest protocols for secure computation of the sum,
product, and some other functions, without using any one-way functions. A new
input that we offer here is that, in contrast with other proposals, we conceal
&quot;intermediate results&quot; of a computation. For example, when we compute the sum
of $k$ numbers, only the final result is known to the parties; partial sums are
not known to anybody. Other applications of our method include voting/rating
over insecure channels and a rather elegant and efficient solution of Yao's
&quot;millionaires' problem&quot;.
  Then, while it is fairly obvious that a secure (bit) commitment between two
parties is impossible without a one-way function, we show that it is possible
if the number of parties is at least 3. We also show how our (bit) commitment
scheme for 3 parties can be used to arrange an unconditionally secure (bit)
commitment between just two parties if they use a &quot;dummy&quot; (e.g., a computer) as
the third party. We explain how our concept of a &quot;dummy&quot; is different from a
well-known concept of a &quot;trusted third party&quot;.
  We also suggest a protocol, without using a one-way function, for &quot;mental
poker&quot;, i.e., a fair card dealing (and playing) over distance. We also propose
a secret sharing scheme where an advantage over Shamir's and other known secret
sharing schemes is that nobody, including the dealer, ends up knowing the
shares owned by any particular player.
  It should be mentioned that computational cost of our protocols is negligible
to the point that all of them can be executed without a computer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5070</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5070</id><created>2013-01-21</created><updated>2013-04-03</updated><authors><author><keyname>Guo</keyname><forenames>Longkun</forenames></author><author><keyname>Shen</keyname><forenames>Hong</forenames></author><author><keyname>Liao</keyname><forenames>Kewen</forenames></author></authors><title>Improved Approximation Algorithms for Computing k Disjoint Paths Subject
  to Two Constraints</title><categories>cs.DS cs.NI</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a given graph $G$ with positive integral cost and delay on edges,
distinct vertices $s$ and $t$, cost bound $C\in Z^{+}$ and delay bound $D\in
Z^{+}$, the $k$ bi-constraint path ($k$BCP) problem is to compute $k$ disjoint
$st$-paths subject to $C$ and $D$. This problem is known NP-hard, even when
$k=1$ \cite{garey1979computers}. This paper first gives a simple approximation
algorithm with factor-$(2,2)$, i.e. the algorithm computes a solution with
delay and cost bounded by $2*D$ and $2*C$ respectively. Later, a novel improved
approximation algorithm with ratio
$(1+\beta,\,\max\{2,\,1+\ln\frac{1}{\beta}\})$ is developed by constructing
interesting auxiliary graphs and employing the cycle cancellation method. As a
consequence, we can obtain a factor-$(1.369,\,2)$ approximation algorithm by
setting $1+\ln\frac{1}{\beta}=2$ and a factor-$(1.567,\,1.567)$ algorithm by
setting $1+\beta=1+\ln\frac{1}{\beta}$. Besides, by setting $\beta=0$, an
approximation algorithm with ratio $(1,\, O(\ln n))$, i.e. an algorithm with
only a single factor ratio $O(\ln n)$ on cost, can be immediately obtained. To
the best of our knowledge, this is the first non-trivial approximation
algorithm for the $k$BCP problem that strictly obeys the delay constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5074</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5074</id><created>2013-01-22</created><authors><author><keyname>Page</keyname><forenames>Rex</forenames><affiliation>University of Oklahoma</affiliation></author><author><keyname>Gamboa</keyname><forenames>Ruben</forenames><affiliation>University of Wyoming</affiliation></author></authors><title>How Computers Work: Computational Thinking for Everyone</title><categories>cs.CY</categories><comments>In Proceedings TFPIE 2012, arXiv:1301.4650</comments><proxy>EPTCS</proxy><acm-class>F.3.1; K.3.2</acm-class><journal-ref>EPTCS 106, 2013, pp. 1-19</journal-ref><doi>10.4204/EPTCS.106.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What would you teach if you had only one course to help students grasp the
essence of computation and perhaps inspire a few of them to make computing a
subject of further study? Assume they have the standard college prep
background. This would include basic algebra, but not necessarily more advanced
mathematics. They would have written a few term papers, but would not have
written computer programs. They could surf and twitter, but could not
exclusive-or and nand. What about computers would interest them or help them
place their experience in context? This paper provides one possible answer to
this question by discussing a course that has completed its second iteration.
Grounded in classical logic, elucidated in digital circuits and computer
software, it expands into areas such as CPU components and massive databases.
The course has succeeded in garnering the enthusiastic attention of students
with a broad range of interests, exercising their problem solving skills, and
introducing them to computational thinking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5075</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5075</id><created>2013-01-22</created><authors><author><keyname>O'Donnell</keyname><forenames>John T.</forenames><affiliation>School of Computing Science, University of Glasgow</affiliation></author></authors><title>Connecting the Dots: Computer Systems Education using a Functional
  Hardware Description Language</title><categories>cs.CY cs.PL</categories><comments>In Proceedings TFPIE 2012, arXiv:1301.4650</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 106, 2013, pp. 20-39</journal-ref><doi>10.4204/EPTCS.106.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A functional hardware description language enables students to gain a working
understanding of computer systems, and to see how the levels of abstraction fit
together. By simulating circuits, digital design becomes a living topic, like
programming, and not just a set of inert facts to memorise. Experiences gained
from more than 20 years of teaching computer systems via functional programming
are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5076</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5076</id><created>2013-01-22</created><authors><author><keyname>Ragde</keyname><forenames>Prabhakar</forenames><affiliation>University of Waterloo</affiliation></author></authors><title>Mathematics Is Imprecise</title><categories>cs.PL cs.CY</categories><comments>In Proceedings TFPIE 2012, arXiv:1301.4650</comments><proxy>EPTCS</proxy><acm-class>D.1.1;K.3.2</acm-class><journal-ref>EPTCS 106, 2013, pp. 40-49</journal-ref><doi>10.4204/EPTCS.106.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We commonly think of mathematics as bringing precision to application
domains, but its relationship with computer science is more complex. This
experience report on the use of Racket and Haskell to teach a required first
university CS course to students with very good mathematical skills focusses on
the ways that programming forces one to get the details right, with consequent
benefits in the mathematical domain. Conversely, imprecision in mathematical
abstractions and notation can work to the benefit of beginning programmers, if
handled carefully.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5077</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5077</id><created>2013-01-22</created><authors><author><keyname>Stutterheim</keyname><forenames>Jurri&#xeb;n</forenames><affiliation>Utrecht University</affiliation></author><author><keyname>Swierstra</keyname><forenames>Wouter</forenames><affiliation>Utrecht University</affiliation></author><author><keyname>Swierstra</keyname><forenames>Doaitse</forenames><affiliation>Utrecht University</affiliation></author></authors><title>Forty hours of declarative programming: Teaching Prolog at the Junior
  College Utrecht</title><categories>cs.CY cs.PL</categories><comments>In Proceedings TFPIE 2012, arXiv:1301.4650</comments><proxy>EPTCS</proxy><acm-class>K.3.2; D.1.6; D.1.1; D.2.6</acm-class><journal-ref>EPTCS 106, 2013, pp. 50-62</journal-ref><doi>10.4204/EPTCS.106.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper documents our experience using declarative languages to give
secondary school students a first taste of Computer Science. The course aims to
teach students a bit about programming in Prolog, but also exposes them to
important Computer Science concepts, such as unification or searching
strategies. Using Haskell's Snap Framework in combination with our own
NanoProlog library, we have developed a web application to teach this course.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5083</identifier>
 <datestamp>2015-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5083</id><created>2013-01-22</created><authors><author><keyname>Matsumoto</keyname><forenames>Ryutaroh</forenames></author></authors><title>Improved Asymptotic Key Rate of the B92 Protocol</title><categories>quant-ph cs.IT math.IT</categories><comments>IEEEtran.sty, 3 pages, 1 figure. Submitted to IEEE ISIT 2013</comments><journal-ref>Proc. 2013 IEEE International Symposium on Information Theory, pp.
  351-353</journal-ref><doi>10.1109/ISIT.2013.6620246</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the asymptotic key rate of the single photon B92 protocol by using
Renner's security analysis given in 2005. The new analysis shows that the B92
protocol can securely generate key at 6.5% depolarizing rate, while the
previous analyses cannot guarantee the secure key generation at 4.2%
depolarizing rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5088</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5088</id><created>2013-01-22</created><authors><author><keyname>Goodfellow</keyname><forenames>Ian J.</forenames></author></authors><title>Piecewise Linear Multilayer Perceptrons and Dropout</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new type of hidden layer for a multilayer perceptron, and
demonstrate that it obtains the best reported performance for an MLP on the
MNIST dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5089</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5089</id><created>2013-01-22</created><updated>2015-01-29</updated><authors><author><keyname>Ilik</keyname><forenames>Danko</forenames></author></authors><title>An interpretation of the Sigma-2 fragment of classical Analysis in
  System T</title><categories>math.LO cs.LO</categories><msc-class>03F25, 03F10, 03F60, 03B30, 03B20, 03E25, 03D65, 68N15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that it is possible to define a realizability interpretation for the
$\Sigma_2$-fragment of classical Analysis using G\&quot;odel's System T only. This
supplements a previous result of Schwichtenberg regarding bar recursion at
types 0 and 1 by showing how to avoid using bar recursion altogether. Our
result is proved via a conservative extension of System T with an operator for
composable continuations from the theory of programming languages due to Danvy
and Filinski. The fragment of Analysis is therefore essentially constructive,
even in presence of the full Axiom of Choice schema: Weak Church's Rule holds
of it in spite of the fact that it is strong enough to refute the formal
arithmetical version of Church's Thesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5091</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5091</id><created>2013-01-22</created><authors><author><keyname>Sun</keyname><forenames>Haiyan</forenames></author><author><keyname>Wen</keyname><forenames>Qiaoyan</forenames></author><author><keyname>Zhang</keyname><forenames>Hua</forenames></author><author><keyname>Jin</keyname><forenames>Zhengping</forenames></author><author><keyname>Li</keyname><forenames>Wenmin</forenames></author></authors><title>Cryptanalysis and improvement of two certificateless three-party
  authenticated key agreement protocols</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, two certificateless three-party authenticated key agreement
protocols were proposed, and both protocols were claimed they can meet the
desirable security properties including forward security, key compromise
impersonation resistance and so on. Through cryptanalysis, we show that one
neither meets forward security and key compromise impersonation resistance nor
resists an attack by an adversary who knows all users' secret values, and the
other cannot resist key compromise impersonation attack. Finally, we propose
improved protocols to make up two original protocols' security weaknesses,
respectively. Further security analysis shows that our improved protocols can
remove such security weaknesses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5096</identifier>
 <datestamp>2014-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5096</id><created>2013-01-22</created><updated>2014-07-07</updated><authors><author><keyname>No</keyname><forenames>Albert</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Minimax Filtering via Relations between Information and Estimation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of continuous-time causal estimation under a
minimax criterion. Let $X^T = \{X_t,0\leq t\leq T\}$ be governed by the
probability law $P_{\theta}$ from a class of possible laws indexed by $\theta
\in \Lambda$, and $Y^T$ be the noise corrupted observations of $X^T$ available
to the estimator. We characterize the estimator minimizing the worst case
regret, where regret is the difference between the causal estimation loss of
the estimator and that of the optimum estimator.
  One of the main contributions of this paper is characterizing the minimax
estimator, showing that it is in fact a Bayesian estimator. We then relate
minimax regret to the channel capacity when the channel is either Gaussian or
Poisson. In this case, we characterize the minimax regret and the minimax
estimator more explicitly. If we further assume that the uncertainty set
consists of deterministic signals, the worst case regret is exactly equal to
the corresponding channel capacity, namely the maximal mutual information
attainable across the channel among all possible distributions on the
uncertainty set of signals. The corresponding minimax estimator is the Bayesian
estimator assuming the capacity-achieving prior. Using this relation, we also
show that the capacity achieving prior coincides with the least favorable
input. Moreover, we show that this minimax estimator is not only minimizing the
worst case regret but also essentially minimizing regret for &quot;most&quot; of the
other sources in the uncertainty set.
  We present a couple of examples for the construction of an minimax filter via
an approximation of the associated capacity achieving distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5104</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5104</id><created>2013-01-22</created><authors><author><keyname>Karhumaki</keyname><forenames>Juhani</forenames></author><author><keyname>Saarela</keyname><forenames>Aleksi</forenames></author><author><keyname>Zamboni</keyname><forenames>Luca Q.</forenames></author></authors><title>On a generalization of Abelian equivalence and complexity of infinite
  words</title><categories>math.CO cs.DM</categories><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce and study a family of complexity functions of
infinite words indexed by $k \in \ints ^+ \cup {+\infty}.$ Let $k \in \ints ^+
\cup {+\infty}$ and $A$ be a finite non-empty set. Two finite words $u$ and $v$
in $A^*$ are said to be $k$-Abelian equivalent if for all $x\in A^*$ of length
less than or equal to $k,$ the number of occurrences of $x$ in $u$ is equal to
the number of occurrences of $x$ in $v.$ This defines a family of equivalence
relations $\thicksim_k$ on $A^*,$ bridging the gap between the usual notion of
Abelian equivalence (when $k=1$) and equality (when $k=+\infty).$ We show that
the number of $k$-Abelian equivalence classes of words of length $n$ grows
polynomially, although the degree is exponential in $k.$ Given an infinite word
$\omega \in A^\nats,$ we consider the associated complexity function $\mathcal
{P}^{(k)}_\omega :\nats \rightarrow \nats$ which counts the number of
$k$-Abelian equivalence classes of factors of $\omega$ of length $n.$ We show
that the complexity function $\mathcal {P}^{(k)}$ is intimately linked with
periodicity. More precisely we define an auxiliary function $q^k: \nats
\rightarrow \nats$ and show that if $\mathcal {P}^{(k)}_{\omega}(n)&lt;q^k(n)$ for
some $k \in \ints ^+ \cup {+\infty}$ and $n\geq 0,$ the $\omega$ is ultimately
periodic. Moreover if $\omega$ is aperiodic, then $\mathcal
{P}^{(k)}_{\omega}(n)=q^k(n)$ if and only if $\omega$ is Sturmian. We also
study $k$-Abelian complexity in connection with repetitions in words. Using
Szemer\'edi's theorem, we show that if $\omega$ has bounded $k$-Abelian
complexity, then for every $D\subset \nats$ with positive upper density and for
every positive integer $N,$ there exists a $k$-Abelian $N$ power occurring in
$\omega$ at some position $j\in D.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5107</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5107</id><created>2013-01-22</created><updated>2013-12-11</updated><authors><author><keyname>Zhang</keyname><forenames>Shaoquan</forenames></author></authors><title>Broadcasting Algorithm with Per-neighbor Queues</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Broadcasting systems such as P2P streaming systems represent important
network applications that support up to millions of online users. An efficient
broadcasting mechanism is at the core of the system design. Despite substantial
efforts on developing efficient broadcasting algorithms, the following
important question remains open: How to achieve the maximum broadcast rate in a
distributed manner with each user maintaining information queues only for its
direct neighbors? In this work, we first derive an innovative formulation of
the problem over acyclic overlay networks with arbitrary underlay capacity
constraints. Then, based on the formulation, we develop a distributed algorithm
to achieve the maximum broadcast rate and every user only maintains one queue
per-neighbor. Due to its lightweight nature, our algorithm scales very well
with the network size and remains robust against high system dynamics. Finally,
by conducting simulations we validate the optimality of our algorithm under
different network capacity models. Simulation results further indicate that the
convergence time of our algorithm grows linearly with the network size, which
suggests an interesting direction for future investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5108</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5108</id><created>2013-01-22</created><authors><author><keyname>Dau</keyname><forenames>Son Hoang</forenames></author><author><keyname>Song</keyname><forenames>Wentu</forenames></author><author><keyname>Dong</keyname><forenames>Zheng</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author></authors><title>Balanced Sparsest Generator Matrices for MDS Codes</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that given $n$ and $k$, for $q$ sufficiently large, there always
exists an $[n, k]_q$ MDS code that has a generator matrix $G$ satisfying the
following two conditions: (C1) Sparsest: each row of $G$ has Hamming weight $n
- k + 1$; (C2) Balanced: Hamming weights of the columns of $G$ differ from each
other by at most one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5109</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5109</id><created>2013-01-22</created><authors><author><keyname>Lapidoth</keyname><forenames>Amos</forenames></author><author><keyname>Mal&#xe4;r</keyname><forenames>Andreas</forenames></author><author><keyname>Wigger</keyname><forenames>Mich&#xe8;le</forenames></author></authors><title>Constrained Source Coding with Side Information</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The source-coding problem with side information at the decoder is studied
subject to a constraint that the encoder---to whom the side information is
unavailable---be able to compute the decoder's reconstruction sequence to
within some distortion. For discrete memoryless sources and finite
single-letter distortion measures, an expression is given for the minimal
description rate as a function of the joint law of the source and side
information and of the allowed distortions at the encoder and at the decoder.
The minimal description rate is also computed for a memoryless Gaussian source
with squared-error distortion measures. A solution is also provided to a more
general problem where there are more than two distortion constraints and each
distortion function may be a function of three arguments: the source symbol,
the encoder's reconstruction symbol, and the decoder's reconstruction symbol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5112</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5112</id><created>2013-01-22</created><authors><author><keyname>Cesa-Bianchi</keyname><forenames>Nicolo</forenames></author><author><keyname>Gentile</keyname><forenames>Claudio</forenames></author><author><keyname>Vitale</keyname><forenames>Fabio</forenames></author><author><keyname>Zappella</keyname><forenames>Giovanni</forenames></author></authors><title>Active Learning on Trees and Graphs</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of active learning on a given tree whose nodes are
assigned binary labels in an adversarial way. Inspired by recent results by
Guillory and Bilmes, we characterize (up to constant factors) the optimal
placement of queries so to minimize the mistakes made on the non-queried nodes.
Our query selection algorithm is extremely efficient, and the optimal number of
mistakes on the non-queried nodes is achieved by a simple and efficient mincut
classifier. Through a simple modification of the query selection algorithm we
also show optimality (up to constant factors) with respect to the trade-off
between number of queries and number of mistakes on non-queried nodes. By using
spanning trees, our algorithms can be efficiently applied to general graphs,
although the problem of finding optimal and efficient active learning
algorithms for general graphs remains open. Towards this end, we provide a
lower bound on the number of mistakes made on arbitrary graphs by any active
learning algorithm using a number of queries which is up to a constant fraction
of the graph size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5121</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5121</id><created>2013-01-22</created><authors><author><keyname>Averbuch</keyname><forenames>Alex</forenames></author><author><keyname>Neumann</keyname><forenames>Martin</forenames></author></authors><title>Partitioning Graph Databases - A Quantitative Evaluation</title><categories>cs.DB cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electronic data is growing at increasing rates, in both size and
connectivity: the increasing presence of, and interest in, relationships
between data. An example is the Twitter social network graph. Due to this
growth demand is increasing for technologies that can process such data.
Currently relational databases are the predominant technology, but they are
poorly suited to processing connected data as they are optimized for
index-intensive operations. Conversely, graph databases are optimized for graph
computation. They link records by direct references, avoiding index lookups,
and enabling retrieval of adjacent elements in constant time, regardless of
graph size. However, as data volume increases these databases outgrow the
resources of one computer and data partitioning becomes necessary. We evaluate
the viability of using graph partitioning algorithms to partition graph
databases. A prototype partitioned database was developed. Three partitioning
algorithms explored and one implemented. Three graph datasets were used: two
real and one synthetically generated. These were partitioned in various ways
and the impact on database performance measured. We defined one synthetic
access pattern per dataset and executed each on the partitioned datasets.
Evaluation took place in a simulation environment, ensuring repeatability and
allowing measurement of metrics like network traffic and load balance. Results
show that compared to random partitioning the partitioning algorithm reduced
traffic by 40-90%. Executing the algorithm intermittently during usage
maintained partition quality, while requiring only 1% the computation of
initial partitioning. Strong correlations were found between theoretic quality
metrics and generated network traffic under non-uniform access patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5131</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5131</id><created>2013-01-22</created><authors><author><keyname>Cardona</keyname><forenames>Gabriel</forenames></author><author><keyname>Mir</keyname><forenames>Arnau</forenames></author><author><keyname>Rossello</keyname><forenames>Francesc</forenames></author></authors><title>The expected value of the squared euclidean cophenetic metric under the
  Yule and the uniform models</title><categories>q-bio.PE cs.DM math.PR</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cophenetic metrics $d_{\varphi,p}$, for $p\in {0}\cup[1,\infty[$, are a
recent addition to the kit of available distances for the comparison of
phylogenetic trees. Based on a fifty years old idea of Sokal and Rohlf, these
metrics compare phylogenetic trees on a same set of taxa by encoding them by
means of their vectors of cophenetic values of pairs of taxa and depths of
single taxa, and then computing the $L^p$ norm of the difference of the
corresponding vectors. In this paper we compute the expected value of the
square of $d_{\varphi,2}$ on the space of fully resolved rooted phylogenetic
trees with $n$ leaves, under the Yule and the uniform probability
distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5132</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5132</id><created>2013-01-22</created><authors><author><keyname>Kanrar</keyname><forenames>Soumen</forenames></author></authors><title>Parametric Estimation of Handoff</title><categories>cs.NI</categories><comments>5 Pages,3 figures, NCCCS-12,ISBN:978-1-4673-2837-1</comments><doi>10.1109/NCCCS.2012.6412991</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The efficiency of wireless technology depends upon the seamless connectivity
to the user at anywhere any time.Heterogeneous wireless networks are an
integration of different networks with diversified technologies. The most
essential requirement for Seamless vertical handover is that the received
signal strength should always be healthy. Mobile device enabled with multiple
wireless technologies makes it possible to maintain seamless connectivity in
highly dynamic environment.Since the available bandwidth is limited and the
number of users is growing rapidly, it is a real challenge to maintain the
received signal strength in a healthy stage.In this work, the proposed, cost
effective parametric estimation for vertical handover shows that the received
signal strength maintains a healthy level by considering the novel concept.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5136</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5136</id><created>2013-01-22</created><authors><author><keyname>Bahrami</keyname><forenames>Mohsen</forenames></author><author><keyname>Bereyhi</keyname><forenames>Ali</forenames></author><author><keyname>Mirmohseni</keyname><forenames>Mahtab</forenames></author><author><keyname>Aref</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Secret Key Agreement Using Conferencing in State- Dependent Multiple
  Access Channels with An Eavesdropper</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of secret key agreement in state-dependent
multiple access channels with an eavesdropper is studied. For this model, the
channel state information is non-causally available at the transmitters;
furthermore, a legitimate receiver observes a degraded version of the channel
state information. The transmitters can partially cooperate with each other
using a conferencing link with a limited rate. In addition, a backward public
channel is assumed between the terminals. The problem of secret key sharing
consists of two rounds. In the first round, the transmitters wish to share a
common key with the legitimate receiver. Lower and upper bounds on the common
key capacity are established. In a special case, the capacity of the common key
is obtained. In the second round, the legitimate receiver agrees on two
independent private keys with the corresponding transmitters using the public
channel. Inner and outer bounds on the private key capacity region are
characterized. In a special case, the inner bound coincides with the outer
bound. We provide some examples to illustrate our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5139</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5139</id><created>2013-01-22</created><updated>2013-03-30</updated><authors><author><keyname>Iosif</keyname><forenames>Radu</forenames></author><author><keyname>Rogalewicz</keyname><forenames>Adam</forenames></author><author><keyname>Simacek</keyname><forenames>Jiri</forenames></author></authors><title>The Tree Width of Separation Logic with Recursive Definitions</title><categories>cs.LO cs.FL</categories><comments>30 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separation Logic is a widely used formalism for describing dynamically
allocated linked data structures, such as lists, trees, etc. The decidability
status of various fragments of the logic constitutes a long standing open
problem. Current results report on techniques to decide satisfiability and
validity of entailments for Separation Logic(s) over lists (possibly with
data). In this paper we establish a more general decidability result. We prove
that any Separation Logic formula using rather general recursively defined
predicates is decidable for satisfiability, and moreover, entailments between
such formulae are decidable for validity. These predicates are general enough
to define (doubly-) linked lists, trees, and structures more general than
trees, such as trees whose leaves are chained in a list. The decidability
proofs are by reduction to decidability of Monadic Second Order Logic on graphs
with bounded tree width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5142</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5142</id><created>2013-01-22</created><updated>2013-05-16</updated><authors><author><keyname>Bahrami</keyname><forenames>Mohsen</forenames></author><author><keyname>Bereyhi</keyname><forenames>Ali</forenames></author><author><keyname>Salehkalaibar</keyname><forenames>Sadaf</forenames></author><author><keyname>Aref</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Key agreement over a 3-receiver broadcast channel</title><categories>cs.CR</categories><comments>Accepted in IWCIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of secret key agreement in
state-dependent 3-receiver broadcast channels. In the proposed model, there are
two legitimate receivers, an eavesdropper and a transmitter where the channel
state information is non-causally available at the transmitter. We consider two
setups. In the first setup, the transmitter tries to agree on a common key with
the legitimate receivers while keeping it concealed from the eavesdropper.
Simultaneously, the transmitter agrees on a private key with each of the
legitimate receivers that needs to be kept secret from the other legitimate
receiver and the eavesdropper. For this setup, we derive inner and outer bounds
on the secret key capacity region. In the second setup, we assume that a
backward public channel is available among the receivers and the transmitter.
Each legitimate receiver wishes to share a private key with the transmitter.
For this setup, an inner bound on the private key capacity region is found.
Furthermore, the capacity region of the secret key in the state-dependent
wiretap channel can be deduced from our inner and outer bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5149</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5149</id><created>2013-01-22</created><updated>2015-05-22</updated><authors><author><keyname>Trotignon</keyname><forenames>Nicolas</forenames></author></authors><title>Perfect graphs: a survey</title><categories>math.CO cs.DM</categories><comments>52 pages; published in Topics in Chromatic Graph Theory, Cambridge
  University Press, 2015, pp. 137-160</comments><msc-class>05C17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Perfect graphs were defined by Claude Berge in the 1960s. They are important
objects for graph theory, linear programming and combinatorial optimization.
Claude Berge made a conjecture about them, that was proved by Chudnovsky,
Robertson, Seymour and Thomas in 2002, and is now called the strong perfect
graph theorem. This is a survey about perfect graphs, mostly focused on the
strong perfect graph theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5154</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5154</id><created>2013-01-22</created><authors><author><keyname>Delhibabu</keyname><forenames>Radhakrishnan</forenames></author><author><keyname>Lakemeyer</keyname><forenames>Gerhard</forenames></author></authors><title>A Rational and Efficient Algorithm for View Revision in Databases</title><categories>cs.LO cs.AI cs.DB</categories><journal-ref>Applied Mathematics &amp; Information Sciences, Volume 7, No. 3
  (2013), PP:843-856</journal-ref><doi>10.12785/amis/070302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamics of belief and knowledge is one of the major components of any
autonomous system that should be able to incorporate new pieces of information.
In this paper, we argue that to apply rationality result of belief dynamics
theory to various practical problems, it should be generalized in two respects:
first of all, it should allow a certain part of belief to be declared as
immutable; and second, the belief state need not be deductively closed. Such a
generalization of belief dynamics, referred to as base dynamics, is presented,
along with the concept of a generalized revision algorithm for Horn knowledge
bases. We show that Horn knowledge base dynamics has interesting connection
with kernel change and abduction. Finally, we also show that both variants are
rational in the sense that they satisfy certain rationality postulates stemming
from philosophical works on belief dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5159</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5159</id><created>2013-01-22</created><authors><author><keyname>Adams</keyname><forenames>Jonathan</forenames></author><author><keyname>Gurney</keyname><forenames>Karen</forenames></author><author><keyname>Hook</keyname><forenames>Daniel</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>International collaboration clusters in Africa</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>12 pp, 5 Figs including map links to viewer</comments><journal-ref>Scientometrics 98, 547-556 (2014)</journal-ref><doi>10.1007/s11192-013-1060-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent discussion about the increase in international research collaboration
suggests a comprehensive global network centred around a group of core
countries and driven by generic socio-economic factors where the global system
influences all national and institutional outcomes. In counterpoint, we
demonstrate that the collaboration pattern for countries in Africa is far from
universal. Instead, it exhibits layers of internal clusters and external links
that are explained not by monotypic global influences but by regional geography
and, perhaps even more strongly, by history, culture and language. Analysis of
these bottom-up, subjective, human factors is required in order to provide the
fuller explanation useful for policy and management purposes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5160</identifier>
 <datestamp>2013-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5160</id><created>2013-01-22</created><updated>2013-02-28</updated><authors><author><keyname>Vitale</keyname><forenames>Fabio</forenames></author><author><keyname>Cesa-Bianchi</keyname><forenames>Nicolo</forenames></author><author><keyname>Gentile</keyname><forenames>Claudio</forenames></author><author><keyname>Zappella</keyname><forenames>Giovanni</forenames></author></authors><title>See the Tree Through the Lines: The Shazoo Algorithm -- Full Version --</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting the nodes of a given graph is a fascinating theoretical problem
with applications in several domains. Since graph sparsification via spanning
trees retains enough information while making the task much easier, trees are
an important special case of this problem. Although it is known how to predict
the nodes of an unweighted tree in a nearly optimal way, in the weighted case a
fully satisfactory algorithm is not available yet. We fill this hole and
introduce an efficient node predictor, Shazoo, which is nearly optimal on any
weighted tree. Moreover, we show that Shazoo can be viewed as a common
nontrivial generalization of both previous approaches for unweighted trees and
weighted lines. Experiments on real-world datasets confirm that Shazoo performs
well in that it fully exploits the structure of the input tree, and gets very
close to (and sometimes better than) less scalable energy minimization methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5177</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5177</id><created>2013-01-22</created><updated>2013-04-22</updated><authors><author><keyname>Reijnhoudt</keyname><forenames>Linda</forenames></author><author><keyname>Costas</keyname><forenames>Rodrigo</forenames></author><author><keyname>Noyons</keyname><forenames>Ed</forenames></author><author><keyname>Boerner</keyname><forenames>Katy</forenames></author><author><keyname>Scharnhorst</keyname><forenames>Andrea</forenames></author></authors><title>&quot;Seed+Expand&quot;: A validated methodology for creating high quality
  publication oeuvres of individual researchers</title><categories>cs.DL cs.IR</categories><comments>Paper accepted for the ISSI 2013, small changes in the text due to
  referee comments, one figure added (Fig 3)</comments><acm-class>H.3.3; H.3.7; J.1</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The study of science at the individual micro-level frequently requires the
disambiguation of author names. The creation of author's publication oeuvres
involves matching the list of unique author names to names used in publication
databases. Despite recent progress in the development of unique author
identifiers, e.g., ORCID, VIVO, or DAI, author disambiguation remains a key
problem when it comes to large-scale bibliometric analysis using data from
multiple databases. This study introduces and validates a new methodology
called seed+expand for semi-automatic bibliographic data collection for a given
set of individual authors. Specifically, we identify the oeuvre of a set of
Dutch full professors during the period 1980-2011. In particular, we combine
author records from the National Research Information System (NARCIS) with
publication records from the Web of Science. Starting with an initial list of
8,378 names, we identify &quot;seed publications&quot; for each author using five
different approaches. Subsequently, we &quot;expand&quot; the set of publication in three
different approaches. The different approaches are compared and resulting
oeuvres are evaluated on precision and recall using a &quot;gold standard&quot; dataset
of authors for which verified publications in the period 2001-2010 are
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5197</identifier>
 <datestamp>2013-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5197</id><created>2013-01-22</created><updated>2013-04-26</updated><authors><author><keyname>Filiot</keyname><forenames>Emmanuel</forenames></author><author><keyname>Gauwin</keyname><forenames>Olivier</forenames></author><author><keyname>Reynier</keyname><forenames>Pierre-Alain</forenames></author><author><keyname>Servais</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author></authors><title>From Two-Way to One-Way Finite State Transducers</title><categories>cs.FL cs.LO</categories><comments>18 pages, published in LICS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any two-way finite state automaton is equivalent to some one-way finite state
automaton. This well-known result, shown by Rabin and Scott and independently
by Shepherdson, states that two-way finite state automata (even
non-deterministic) characterize the class of regular languages. It is also
known that this result does not extend to finite string transductions:
(deterministic) two-way finite state transducers strictly extend the expressive
power of (functional) one-way transducers. In particular deterministic two-way
transducers capture exactly the class of MSO-transductions of finite strings.
In this paper, we address the following definability problem: given a function
defined by a two-way finite state transducer, is it definable by a one-way
finite state transducer? By extending Rabin and Scott's proof to transductions,
we show that this problem is decidable. Our procedure builds a one-way
transducer, which is equivalent to the two-way transducer, whenever one exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5201</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5201</id><created>2013-01-22</created><authors><author><keyname>Gliwa</keyname><forenames>Bogdan</forenames></author><author><keyname>Ko&#x17a;lak</keyname><forenames>Jaros&#x142;aw</forenames></author><author><keyname>Zygmunt</keyname><forenames>Anna</forenames></author><author><keyname>Cetnarowicz</keyname><forenames>Krzysztof</forenames></author></authors><title>Models of Social Groups in Blogosphere Based on Information about
  Comment Addressees and Sentiments</title><categories>cs.SI physics.soc-ph</categories><comments>Gliwa B., Ko\'zlak J., Zygmunt A., Models of Social Groups in
  Blogosphere Based on Information about Comment Addressees and Sentiments, in
  the K. Aberer et al. (Eds.): SocInfo 2012, LNCS 7710, pp. 475-488, Best Paper
  Award</comments><journal-ref>SocInfo 2012, LNCS 7710, pp. 475-488</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work concerns the analysis of number, sizes and other characteristics of
groups identified in the blogosphere using a set of models identifying social
relations. These models differ regarding identification of social relations,
influenced by methods of classifying the addressee of the comments (they are
either the post author or the author of a comment on which this comment is
directly addressing) and by a sentiment calculated for comments considering the
statistics of words present and connotation. The state of a selected blog
portal was analyzed in sequential, partly overlapping time intervals. Groups in
each interval were identified using a version of the CPM algorithm, on the
basis of them, stable groups, existing for at least a minimal assumed duration
of time, were identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5216</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5216</id><created>2013-01-22</created><updated>2013-02-04</updated><authors><author><keyname>Huang</keyname><forenames>Sangxia</forenames></author></authors><title>Improved Hardness of Approximating Chromatic Number</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that for sufficiently large K, it is NP-hard to color K-colorable
graphs with less than 2^{K^{1/3}} colors. This improves the previous result of
K versus K^{O(log K)} in Khot [14].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5220</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5220</id><created>2013-01-22</created><updated>2015-04-03</updated><authors><author><keyname>Ciosek</keyname><forenames>Kamil</forenames></author></authors><title>Properties of the Least Squares Temporal Difference learning algorithm</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents four different ways of looking at the well-known Least
Squares Temporal Differences (LSTD) algorithm for computing the value function
of a Markov Reward Process, each of them leading to different insights: the
operator-theory approach via the Galerkin method, the statistical approach via
instrumental variables, the linear dynamical system view as well as the limit
of the TD iteration. We also give a geometric view of the algorithm as an
oblique projection. Furthermore, there is an extensive comparison of the
optimization problem solved by LSTD as compared to Bellman Residual
Minimization (BRM). We then review several schemes for the regularization of
the LSTD solution. We then proceed to treat the modification of LSTD for the
case of episodic Markov Reward Processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5258</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5258</id><created>2013-01-22</created><authors><author><keyname>Alsan</keyname><forenames>Mine</forenames></author></authors><title>Extremality Properties for the Basic Polarization Transformations</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the extremality of the BEC and the BSC for Gallager's reliability
function $E_0$ evaluated under the uniform input distribution for binary input
DMCs from the aspect of channel polarization. In particular, we show that
amongst all B-DMCs of a given $E_0(\rho)$ value, for a fixed $\rho \geq 0$, the
BEC and BSC are extremal in the evolution of $E_0$ under the one-step
polarization transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5263</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5263</id><created>2013-01-22</created><authors><author><keyname>de Luca</keyname><forenames>Aldo</forenames></author><author><keyname>Pribavkina</keyname><forenames>Elena V.</forenames></author><author><keyname>Zamboni</keyname><forenames>Luca Q.</forenames></author></authors><title>A Coloring Problem for Sturmian and Episturmian Words</title><categories>math.CO cs.DM</categories><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following open question in the spirit of Ramsey theory: Given
an aperiodic infinite word $w$, does there exist a finite coloring of its
factors such that no factorization of $w$ is monochromatic? We show that such a
coloring always exists whenever $w$ is a Sturmian word or a standard
episturmian word.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5266</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5266</id><created>2013-01-22</created><authors><author><keyname>Miszczak</keyname><forenames>Jaros&#x142;aw Adam</forenames></author><author><keyname>Zawadzki</keyname><forenames>Piotr</forenames></author></authors><title>General method for the security analysis in a quantum direct
  communication protocol</title><categories>quant-ph cs.CR</categories><comments>8 pages, 1 plot</comments><msc-class>81P45</msc-class><acm-class>D.4.6; H.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a general approach for the analysis of a quantum direct
communication protocol. The method is based on the investigation of the
superoperator acting on a joint system of the communicating parties and the
eavesdropper. The introduced method is more versatile than the approaches used
so far as it permits to incorporate different noise models in a unified way.
Moreover, it make use of a well grounded theory of quantum discrimination for
the purpose of estimating the eavesdropper's information gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5271</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5271</id><created>2013-01-22</created><updated>2015-02-25</updated><authors><author><keyname>Joret</keyname><forenames>Gwena&#xeb;l</forenames></author><author><keyname>Micek</keyname><forenames>Piotr</forenames></author><author><keyname>Milans</keyname><forenames>Kevin G.</forenames></author><author><keyname>Trotter</keyname><forenames>William T.</forenames></author><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author><author><keyname>Wang</keyname><forenames>Ruidong</forenames></author></authors><title>Tree-width and dimension</title><categories>math.CO cs.DM</categories><comments>Updates on solutions of problems and on bibliography</comments><msc-class>06A07, 05C35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last 30 years, researchers have investigated connections between
dimension for posets and planarity for graphs. Here we extend this line of
research to the structural graph theory parameter tree-width by proving that
the dimension of a finite poset is bounded in terms of its height and the
tree-width of its cover graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5273</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5273</id><created>2013-01-19</created><updated>2013-02-05</updated><authors><author><keyname>Jenison</keyname><forenames>Rick B.</forenames></author></authors><title>Using Periodicity of Nucleotide Sequences</title><categories>q-bio.GN cs.CE</categories><comments>arXiv admin note: entirely plagiarized from
  http://www.ncbi.nlm.nih.gov/pubmed/19261626</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Withdrawn by arXiv administrators due to content entirely plagiarized from
other authors (not in arXiv).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5288</identifier>
 <datestamp>2013-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5288</id><created>2013-01-22</created><updated>2013-07-17</updated><authors><author><keyname>Aravkin</keyname><forenames>Aleksandr Y.</forenames></author><author><keyname>Bell</keyname><forenames>Bradley M.</forenames></author><author><keyname>Burke</keyname><forenames>James V.</forenames></author><author><keyname>Pillonetto</keyname><forenames>Gianluigi</forenames></author></authors><title>The connection between Bayesian estimation of a Gaussian random field
  and RKHS</title><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>8 pages, 2 figures</comments><msc-class>47N30, 65K10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconstruction of a function from noisy data is often formulated as a
regularized optimization problem over an infinite-dimensional reproducing
kernel Hilbert space (RKHS). The solution describes the observed data and has a
small RKHS norm. When the data fit is measured using a quadratic loss, this
estimator has a known statistical interpretation. Given the noisy measurements,
the RKHS estimate represents the posterior mean (minimum variance estimate) of
a Gaussian random field with covariance proportional to the kernel associated
with the RKHS. In this paper, we provide a statistical interpretation when more
general losses are used, such as absolute value, Vapnik or Huber. Specifically,
for any finite set of sampling locations (including where the data were
collected), the MAP estimate for the signal samples is given by the RKHS
estimate evaluated at these locations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5290</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5290</id><created>2013-01-22</created><updated>2014-04-29</updated><authors><author><keyname>Elbassioni</keyname><forenames>Khaled</forenames></author><author><keyname>Makino</keyname><forenames>Kazuhisa</forenames></author><author><keyname>Mehlhorn</keyname><forenames>Kurt</forenames></author><author><keyname>Ramezani</keyname><forenames>Fahimeh</forenames></author></authors><title>On Randomized Fictitious Play for Approximating Saddle Points Over
  Convex Sets</title><categories>cs.GT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two bounded convex sets $X\subseteq\RR^m$ and $Y\subseteq\RR^n,$
specified by membership oracles, and a continuous convex-concave function
$F:X\times Y\to\RR$, we consider the problem of computing an $\eps$-approximate
saddle point, that is, a pair $(x^*,y^*)\in X\times Y$ such that $\sup_{y\in Y}
F(x^*,y)\le \inf_{x\in
  X}F(x,y^*)+\eps.$ Grigoriadis and Khachiyan (1995) gave a simple randomized
variant of fictitious play for computing an $\eps$-approximate saddle point for
matrix games, that is, when $F$ is bilinear and the sets $X$ and $Y$ are
simplices. In this paper, we extend their method to the general case. In
particular, we show that, for functions of constant &quot;width&quot;, an
$\eps$-approximate saddle point can be computed using
$O^*(\frac{(n+m)}{\eps^2}\ln R)$ random samples from log-concave distributions
over the convex sets $X$ and $Y$. It is assumed that $X$ and $Y$ have inscribed
balls of radius $1/R$ and circumscribing balls of radius $R$. As a consequence,
we obtain a simple randomized polynomial-time algorithm that computes such an
approximation faster than known methods for problems with bounded width and
when $\eps \in (0,1)$ is a fixed, but arbitrarily small constant. Our main tool
for achieving this result is the combination of the randomized fictitious play
with the recently developed results on sampling from convex sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5293</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5293</id><created>2013-01-22</created><updated>2013-04-24</updated><authors><author><keyname>Bach</keyname><forenames>Eric</forenames></author><author><keyname>Sorenson</keyname><forenames>Jonathan</forenames></author></authors><title>Approximately counting semismooth integers</title><categories>cs.DS math.NT</categories><comments>To appear in ISSAC 2013, Boston MA</comments><msc-class>11y16, 11y05, 11a51</msc-class><acm-class>F.2.1; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An integer $n$ is $(y,z)$-semismooth if $n=pm$ where $m$ is an integer with
all prime divisors $\le y$ and $p$ is 1 or a prime $\le z$. arge quantities of
semismooth integers are utilized in modern integer factoring algorithms, such
as the number field sieve, that incorporate the so-called large prime variant.
Thus, it is useful for factoring practitioners to be able to estimate the value
of $\Psi(x,y,z)$, the number of $(y,z)$-semismooth integers up to $x$, so that
they can better set algorithm parameters and minimize running times, which
could be weeks or months on a cluster supercomputer. In this paper, we explore
several algorithms to approximate $\Psi(x,y,z)$ using a generalization of
Buchstab's identity with numeric integration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5296</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5296</id><created>2013-01-22</created><updated>2013-11-19</updated><authors><author><keyname>Dvo&#x159;&#xe1;k</keyname><forenames>Zden&#x11b;k</forenames><affiliation>IUUK</affiliation></author><author><keyname>Sereni</keyname><forenames>Jean-S&#xe9;bastien</forenames><affiliation>CNRS</affiliation></author><author><keyname>Volec</keyname><forenames>Jan</forenames><affiliation>DIMAP</affiliation></author></authors><title>Subcubic triangle-free graphs have fractional chromatic number at most
  14/5</title><categories>math.CO cs.DM</categories><proxy>ccsd</proxy><msc-class>05C15, 05C72, 05C69</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that every subcubic triangle-free graph has fractional chromatic
number at most 14/5, thus confirming a conjecture of Heckman and Thomas [A new
proof of the independence ratio of triangle-free cubic graphs. Discrete Math.
233 (2001), 233--237].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5304</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5304</id><created>2013-01-22</created><authors><author><keyname>Abrusci</keyname><forenames>Michele</forenames><affiliation>LaBRI, IRIT</affiliation></author><author><keyname>Retor&#xe9;</keyname><forenames>Christian</forenames><affiliation>LaBRI, IRIT</affiliation></author></authors><title>Some proof theoretical remarks on quantification in ordinary language</title><categories>math.LO cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper surveys the common approach to quantification and generalised
quantification in formal linguistics and philosophy of language. We point out
how this general setting departs from empirical linguistic data, and give some
hints for a different view based on proof theory, which on many aspects gets
closer to the language itself. We stress the importance of Hilbert's oper- ator
epsilon and tau for, respectively, existential and universal quantifications.
Indeed, these operators help a lot to construct semantic representation close
to natural language, in particular with quantified noun phrases as individual
terms. We also define guidelines for the design of the proof rules
corresponding to generalised quantifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5309</identifier>
 <datestamp>2014-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5309</id><created>2013-01-22</created><updated>2014-07-29</updated><authors><author><keyname>Vahid</keyname><forenames>Alireza</forenames></author><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Avestimehr</keyname><forenames>Amir Salman</forenames></author></authors><title>Capacity Results for Binary Fading Interference Channels with Delayed
  CSIT</title><categories>cs.IT math.IT</categories><comments>39 pages, two columns, accepted for publication in IEEE Transactions
  on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To study the effect of lack of up-to-date channel state information at the
transmitters (CSIT), we consider two-user binary fading interference channels
with Delayed-CSIT. We characterize the capacity region for such channels under
homogeneous assumption where channel gains have identical and independent
distributions across time and space, eliminating the possibility of exploiting
time/space correlation. We introduce and discuss several novel coding
opportunities created by outdated CSIT that can enlarge the achievable rate
region. The capacity-achieving scheme relies on accurate combination,
concatenation, and merging of these opportunities, depending on the channel
statistics. The outer-bounds are based on an extremal inequality we develop for
a binary broadcast channel with Delayed-CSIT. We further extend the results and
characterize the capacity region when output feedback links are available from
the receivers to the transmitters in addition to the delayed knowledge of the
channel state information. We also discuss the extension of our results to the
non-homogeneous setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5332</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5332</id><created>2013-01-22</created><authors><author><keyname>Wang</keyname><forenames>Yuyang</forenames></author><author><keyname>Khardon</keyname><forenames>Roni</forenames></author><author><keyname>Pechyony</keyname><forenames>Dmitry</forenames></author><author><keyname>Jones</keyname><forenames>Rosie</forenames></author></authors><title>Online Learning with Pairwise Loss Functions</title><categories>stat.ML cs.LG</categories><comments>This is an extension of our COLT paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient online learning with pairwise loss functions is a crucial component
in building large-scale learning system that maximizes the area under the
Receiver Operator Characteristic (ROC) curve. In this paper we investigate the
generalization performance of online learning algorithms with pairwise loss
functions. We show that the existing proof techniques for generalization bounds
of online algorithms with a univariate loss can not be directly applied to
pairwise losses. In this paper, we derive the first result providing
data-dependent bounds for the average risk of the sequence of hypotheses
generated by an arbitrary online learner in terms of an easily computable
statistic, and show how to extract a low risk hypothesis from the sequence. We
demonstrate the generality of our results by applying it to two important
problems in machine learning. First, we analyze two online algorithms for
bipartite ranking; one being a natural extension of the perceptron algorithm
and the other using online convex optimization. Secondly, we provide an
analysis for the risk bound for an online algorithm for supervised metric
learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5334</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5334</id><created>2013-01-22</created><authors><author><keyname>Salimi</keyname><forenames>Amir</forenames></author><author><keyname>Liu</keyname><forenames>Tie</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>Generalized Cut-Set Bounds for Broadcast Networks</title><categories>cs.IT math.IT</categories><comments>30 pages, 4 figures, submitted to the IEEE Transaction on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A broadcast network is a classical network with all source messages
collocated at a single source node. For broadcast networks, the standard
cut-set bounds, which are known to be loose in general, are closely related to
union as a specific set operation to combine the basic cuts of the network.
This paper provides a new set of network coding bounds for general broadcast
networks. These bounds combine the basic cuts of the network via a variety of
set operations (not just the union) and are established via only the
submodularity of Shannon entropy. The tightness of these bounds are
demonstrated via applications to combination networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5348</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5348</id><created>2013-01-15</created><updated>2013-04-15</updated><authors><author><keyname>Vinyals</keyname><forenames>Oriol</forenames></author><author><keyname>Jia</keyname><forenames>Yangqing</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>Why Size Matters: Feature Coding as Nystrom Sampling</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the computer vision and machine learning community has been in
favor of feature extraction pipelines that rely on a coding step followed by a
linear classifier, due to their overall simplicity, well understood properties
of linear classifiers, and their computational efficiency. In this paper we
propose a novel view of this pipeline based on kernel methods and Nystrom
sampling. In particular, we focus on the coding of a data point with a local
representation based on a dictionary with fewer elements than the number of
data points, and view it as an approximation to the actual function that would
compute pair-wise similarity to all data points (often too many to compute in
practice), followed by a Nystrom sampling step to select a subset of all data
points.
  Furthermore, since bounds are known on the approximation power of Nystrom
sampling as a function of how many samples (i.e. dictionary size) we consider,
we can derive bounds on the approximation of the exact (but expensive to
compute) kernel matrix, and use it as a proxy to predict accuracy as a function
of the dictionary size, which has been observed to increase but also to
saturate as we increase its size. This model may help explaining the positive
effect of the codebook size and justifying the need to stack more layers (often
referred to as deep learning), as flat models empirically saturate as we add
more complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5349</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5349</id><created>2013-01-21</created><authors><author><keyname>Hmida</keyname><forenames>Helmi Ben</forenames><affiliation>i3mainz</affiliation></author><author><keyname>Cruz</keyname><forenames>Christophe</forenames><affiliation>Le2i</affiliation></author><author><keyname>Nicolle</keyname><forenames>Christophe</forenames><affiliation>Le2i</affiliation></author><author><keyname>Boochs</keyname><forenames>Frank</forenames><affiliation>i3mainz</affiliation></author></authors><title>Toward the Automatic Generation of a Semantic VRML Model from
  Unorganized 3D Point Clouds</title><categories>cs.CG cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1301.4991,
  arXiv:1301.4783</comments><proxy>ccsd</proxy><journal-ref>The Fifth International Conference on Advances in Semantic
  Processing, Lisbon : Portugal (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents our experience regarding the creation of 3D semantic
facility model out of unorganized 3D point clouds. Thus, a knowledge-based
detection approach of objects using the OWL ontology language is presented.
This knowledge is used to define SWRL detection rules. In addition, the
combination of 3D processing built-ins and topological Built-Ins in SWRL rules
aims at combining geometrical analysis of 3D point clouds and specialist's
knowledge. This combination allows more flexible and intelligent detection and
the annotation of objects contained in 3D point clouds. The created WiDOP
prototype takes a set of 3D point clouds as input, and produces an indexed
scene of colored objects visualized within VRML language as output. The context
of the study is the detection of railway objects materialized within the
Deutsche Bahn scene such as signals, technical cupboards, electric poles, etc.
Therefore, the resulting enriched and populated domain ontology, that contains
the annotations of objects in the point clouds, is used to feed a GIS system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5356</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5356</id><created>2013-01-22</created><updated>2014-10-26</updated><authors><author><keyname>Sener</keyname><forenames>Ozan</forenames></author><author><keyname>Ugur</keyname><forenames>Kemal</forenames></author><author><keyname>Alatan</keyname><forenames>A. Aydin</forenames></author></authors><title>Efficient MRF Energy Propagation for Video Segmentation via Bilateral
  Filters</title><categories>cs.CV</categories><comments>Multimedia, IEEE Transactions on (Volume:16, Issue: 5, Aug. 2014)</comments><doi>10.1109/TMM.2014.2314069</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Segmentation of an object from a video is a challenging task in multimedia
applications. Depending on the application, automatic or interactive methods
are desired; however, regardless of the application type, efficient computation
of video object segmentation is crucial for time-critical applications;
specifically, mobile and interactive applications require near real-time
efficiencies. In this paper, we address the problem of video segmentation from
the perspective of efficiency. We initially redefine the problem of video
object segmentation as the propagation of MRF energies along the temporal
domain. For this purpose, a novel and efficient method is proposed to propagate
MRF energies throughout the frames via bilateral filters without using any
global texture, color or shape model. Recently presented bi-exponential filter
is utilized for efficiency, whereas a novel technique is also developed to
dynamically solve graph-cuts for varying, non-lattice graphs in general linear
filtering scenario. These improvements are experimented for both automatic and
interactive video segmentation scenarios. Moreover, in addition to the
efficiency, segmentation quality is also tested both quantitatively and
qualitatively. Indeed, for some challenging examples, significant time
efficiency is observed without loss of segmentation quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5359</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5359</id><created>2013-01-22</created><updated>2013-02-08</updated><authors><author><keyname>Shanmugam</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author></authors><title>Local Graph Coloring and Index Coding</title><categories>cs.IT cs.DM math.IT</categories><comments>14 Pages, 3 Figures; A conference version submitted to ISIT 2013;
  typos corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel upper bound for the optimal index coding rate. Our bound
uses a graph theoretic quantity called the local chromatic number. We show how
a good local coloring can be used to create a good index code. The local
coloring is used as an alignment guide to assign index coding vectors from a
general position MDS code. We further show that a natural LP relaxation yields
an even stronger index code. Our bounds provably outperform the state of the
art on index coding but at most by a constant factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5363</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5363</id><created>2013-01-22</created><authors><author><keyname>M&#xe4;der</keyname><forenames>Patrick</forenames></author></authors><title>Interactive Traceability Querying and Visualization for Coping With
  Development Complexity</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Requirements traceability can in principle support stakeholders coping with
rising development complexity. However, studies showed that practitioners
rarely use available traceability information after its initial creation. In
the position paper for the Dagstuhl seminar 1242, we argued that a more
integrated approach allowing interactive traceability queries and
context-specific traceability visualizations is needed to let practitioner
access and use valuable traceability information. The information retrieved via
traceability can be very specific to a current task of a stakeholder,
abstracting from everything that is not required to solve the task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5375</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5375</id><created>2013-01-22</created><authors><author><keyname>Zainab</keyname><forenames>A. N.</forenames></author><author><keyname>Anyi</keyname><forenames>K. W. U.</forenames></author><author><keyname>Anuar</keyname><forenames>N. B.</forenames></author></authors><title>A Single Journal Study : Malaysian Journal of Computer Science</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single journal studies are reviewed and measures used in the studies are
highlighted. The following quantitative measures are used to study 272 articles
published in Malaysian Journal of Computer Science, (1) the article
productivity of the journal from 1985 to 2007, (2) the observed and expected
authorship productivity tested using Lotka's Law of author productivity,
identification and listing of core authors; (3) the authorship, co-authorship
pattern by authors' country of origin and institutional affiliations; (4) the
subject areas of research; (5) the citation analysis of resources referenced as
well as the age and half-life of citations; the journals referenced and tested
for zonal distribution using Bradford's law of journal scattering; the extent
of web citations; and (6) the citations received by articles published in MJCS
and impact factor of the journal based on information obtained from Google
Scholar, the level of author and journal self-citation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5379</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5379</id><created>2013-01-22</created><authors><author><keyname>Zainab</keyname><forenames>A. N.</forenames></author><author><keyname>Sanni</keyname><forenames>S. A.</forenames></author><author><keyname>Edzan</keyname><forenames>N. N.</forenames></author><author><keyname>Koh</keyname><forenames>A. P.</forenames></author></authors><title>Auditing scholarly journals published in Malaysia and assessing their
  visibility</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem with the identification of Malaysian scholarly journals lies in
the lack of a current and complete listing of journals published in Malaysia.
As a result, librarians are deprived of a tool that can be used for journal
selection and identification of gaps in their serials collection. This study
describes the audit carried out on scholarly journals, with the objectives (a)
to trace and characterized scholarly journal titles published in Malaysia, and
(b) to determine their visibility in international and national indexing
databases. A total of 464 titles were traced and their yearly trends, publisher
and publishing characteristics, bibliometrics and indexation in national,
international and subject-based indexes were described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5380</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5380</id><created>2013-01-22</created><authors><author><keyname>Sanni</keyname><forenames>S. A.</forenames></author></authors><title>Publication productivity and citation analysis of the Medical Journal of
  Malaysia: 2004 - 2008</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analysed 580 articles (original articles only) published in Medical
Journal of Malaysia between 2004 and 2008, the resources referenced by the
articles and the citations and impact received. Our aim was to examine article
and author productivity, the age of references used and impact of the journal.
Publication data was obtained from MyAIS database and Google Scholar provided
the citation data. From the 580 articles analyzed, contributors mainly come
from the hospitals, universities and clinics. Contributions from foreign
authors are low. The useful lives of references cited were between 3 to 11
years. ISI derived Impact factor for MJM ranged between 0.378 to 0.616. Journal
self-citation is low. Out of the 580 sampled articles, 76.8% have been cited at
least once over the 5 years and the ratio of total publications to citations is
1: 2.6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5381</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5381</id><created>2013-01-22</created><authors><author><keyname>Zahidah</keyname><forenames>Z.</forenames></author><author><keyname>Noorhidawati</keyname><forenames>A.</forenames></author><author><keyname>Zainab</keyname><forenames>A. N.</forenames></author></authors><title>Exploring the needs of Malay manuscript studies community for an
  e-learning platform</title><categories>cs.DL cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Philology studies are often associated with traditional methods of teaching
and learning. This study explores the possibility of e-learning adoption
amongst Malay manuscripts learning community. The Soft System Methodology (SSM)
is used to guide the investigation. SSM emphasises on understanding the problem
situations faced by Malay manuscript learning community and expresses the
situations in rich pictures. The manuscript learning community comprises
lecturers, students and researchers in the field of philology. Data were
gathered from interviews, focus group discussions and observations. Academy of
Malay Studies, University of Malaya is the case study setting, focusing on
lecturers who teach and students who enrol in a philology course as well as
doctoral students researching on manuscript studies. The findings highlight
problems faced by the various stakeholders and propose solutions in the form of
a conceptual model for a collaborative electronic platform to improve teaching
and learning as well as utilizing digitized manuscript surrogates held in a
digital library of Malay manuscripts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5383</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5383</id><created>2013-01-22</created><authors><author><keyname>Sanni</keyname><forenames>S. A.</forenames></author><author><keyname>Zainab</keyname><forenames>A. N.</forenames></author></authors><title>Measuring the influence of a journal using impact and diffusion factors</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Presents the result of the calculated IS! equivalent Impact Factor, Relative
Diffusion Factor (RDF), and Journal Diffusion Factor (JDF) for articles
published in the Medical Journal of Malaysia (MJM) between the years 2004 and
2008 in both their synchronous and diachronous versions. The publication data
are collected from MyAis (Malaysian Abstracting &amp; Indexing system) while the
citation data are collected from Google Scholar. The values of the synchronous
JDF ranges from 0.057 - 0.14 while the diachronous JDF ranges from 0.46 - 1.98.
The high diachronous JDF is explained by a relatively high number of different
citing journals against the number of publications. This implies that the
results of diachronous JDF is influenced by the numbers of publications and a
good comparison may be one of which the subject of analysis have similar number
of publications and citations period. The yearly values of the synchronous RDF
vary in the range of 0.66 - 1.00 while diachronous RDF ranges from 0.62 - 0.88.
The result shows that diachronous RDF is negatively correlated with the number
of citations, resulting in a low RDF value for highly cited publication years.
What this implies in practice is that the diffusion factors can be calculated
for every additional year at any journal level of analysis. This study
demonstrates that these indicators are valuable tools that help to show
development of journals as it changes through time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5384</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5384</id><created>2013-01-22</created><authors><author><keyname>Safahieh</keyname><forenames>H.</forenames></author><author><keyname>Sanni</keyname><forenames>S. A.</forenames></author><author><keyname>Zainab</keyname><forenames>A. N.</forenames></author></authors><title>International Contribution to Nipah Virus Research 1999-2010</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study examines 462 papers on Nipah virus research published from 1999 to
2010, identifying the active authors, institutions and citations received. Data
was extracted from SCI-Expanded database, (Web of Science) and analyzed using
descriptive figures and tables. The results show the growth of publication is
incremental up to 2010 even though the average citations received is
decreasing. The ratio of authors to articles is 1330: 426. The active
contributing countries are USA (41.0%), Australia (19.3%), Malaysia (16.0%),
England (6.5%) and France (5.6%). The productive authors are mainly affiliated
to the Centre for Disease Control and Prevention, USA and Commonwealth
Scientific and Industrial Research Organization (CSIRO) in Australia and
University of Malaya Medical Centre, Malaysia. A total of 10572 citations were
received and the ratio of articles to citation is 1: 24.8. Collaboration with
the bigger laboratories in USA and Australia is contributive to the sustained
growth of published literature and to access diverse expertise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5385</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5385</id><created>2013-01-22</created><authors><author><keyname>Maidabino</keyname><forenames>A. A.</forenames></author><author><keyname>Zainab</keyname><forenames>A. N.</forenames></author></authors><title>Collection security management at university libraries: assessment of
  its implementation status</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study examines the literature on library security and collection
security to identify factors to be considered to develop a collection security
management assessment instrument for university libraries. A &quot;house&quot; model was
proposed consisting of five factors; collection security governance, operations
and processes, people issues, physical and technical issues and the security
culture in libraries. An assessment instrument listing items covering the five
factors was pilot tested on 61 samples comprising chief librarians, deputy
librarians, departmental, sectional heads and professional staff working in
four university libraries in Nigeria. The level of security implementation is
assessed on a scale of 1=not-implemented, 2=planning stage, 3=partial
implementation, 4=close to completion, and 5=full implementation. The
instrument was also tested for reliability. Reliability tests indicate that all
five factors are reliable with Cronbach's alpha values between 0.7 and 0.9,
indicating that the instrument can be used for wider distribution to explore
and assess the level of collection security implementation in university
libraries from a holistic perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5386</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5386</id><created>2013-01-22</created><authors><author><keyname>Ismail</keyname><forenames>R.</forenames></author><author><keyname>Zainab</keyname><forenames>A. N.</forenames></author></authors><title>Information systems security in special and public libraries: an
  assessment of status</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Explores the use of an assessment instrument based on a model named library
information systems security assessment model (LISSAM) to assess the 155 status
in special and public libraries in Malaysia. The study aims to determine the
implementation status of technological and organizational components of the
LISSAM model. An implementation index as well as a scoring tool is presented to
assess the IS safeguarding measures in a library. Data used was based on
questionnaires distributed to a total of 50 individuals who are responsible for
the information systems (IS) or IT in the special and public libraries in
Malaysia. Findings revealed that over 95% of libraries have high level of
technological implementation but 54% were fair poorly on organizational
measures, especially on lack of security procedures, administrative tools and
awareness creation activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5387</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5387</id><created>2013-01-22</created><authors><author><keyname>Zainab</keyname><forenames>A. N.</forenames></author></authors><title>Open Access repositories and journals for visibility: Implications for
  Malaysian libraries</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the growth of Open Access (OA) repositories and journals
as reported by monitoring initiatives such as ROAR (Registry of Open Access
Repositories), Open DOAR (Open Directory of Open Access Repositories), DOAJ
(Directory of Open Access Journals), Directory of Web Ranking of World
Repositories by the Cybermetrics Laboratory in Spain and published literature.
The performance of Malaysian OA repositories and journals is highlighted. The
strength of OA channels in increasing visibility and citations are evidenced by
research findings. It is proposed that libraries champion OA initiatives by
making university or institutional governance aware; encouraging institutional
journal publishers to adopt OA platform; collaborating with research groups to
jumpstart OA institutional initiatives and to embed OA awareness into user and
researcher education programmes. By actively involved, libraries will be free
of permission, licensing and archiving barriers usually imposed in traditional
publishing situation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5392</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5392</id><created>2013-01-22</created><authors><author><keyname>Zainab</keyname><forenames>A. N.</forenames></author></authors><title>Internationalization of Malaysian Mathematical and Computer Science
  Journal</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The internationalization characteristics of two Malaysian journals, Bulletin
of the Malaysian Mathematical Sciences Society (indexed by ISI) and the
Malaysian Journal of Computer Science (indexed by Inspec and Scopus) is
observed. All issues for the years 2000 to 2007 were looked at to obtain the
following information, (i) total articles published between 2000 and 2007; (ii)
the distribution of foreign and Malaysian authors publishing in the journals;
(iii) the distribution of articles by country and (iv) the geographical
distribution of authors citing articles published in the journals. Citation to
articles is derived from information given by Google scholar. The results
indicate that both journals exhibit average internationalization
characteristics as they are current in their publications but with between 19%
-30% international composition of reviewers or editorials, publish between
36%-79% of foreign articles and receive between 60%-70% of citations from
foreign authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5398</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5398</id><created>2013-01-22</created><authors><author><keyname>Abdullah</keyname><forenames>A.</forenames></author><author><keyname>Zainab</keyname><forenames>A. N.</forenames></author></authors><title>Collaborative digital library of historical resources: Evaluation of
  first users</title><categories>cs.HC cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the digital library of historical resources, a research
project which involves building a testbed for the purpose of developing and
testing new collaborative digital library functionality and presents an initial
analysis of the digital library's public use on the web. The digital library is
modeled to focus on serving secondary students information needs in conducting
history projects. As such, in the implementation of the digital library, the
use of online resources would be an integral part of history project based
learning activities. Students should be enabled to access digital resources,
create and publish their own documents in the digital library and share them
with others. As a testbed system, the collaborative digital library known as
CoreDev has demonstrated its capabilities in serving an educational community
as has been reflected by the positive feedback on the functional requirements
from 44 users. Over 75% of the respondents in the user survey considered
themselves capable of using the digital library easily. The beta tester
demographics (n = 105) indicate that the digital library is reaching its target
communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5399</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5399</id><created>2013-01-22</created><authors><author><keyname>Tabatabaii</keyname><forenames>Hoda S. Ayatollahi</forenames></author><author><keyname>Rabiee</keyname><forenames>Hamid R.</forenames></author><author><keyname>Rohban</keyname><forenames>Mohammad Hossein</forenames></author><author><keyname>Salehi</keyname><forenames>Mostafa</forenames></author></authors><title>Incorporating Betweenness Centrality in Compressive Sensing for
  Congestion Detection</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new Compressive Sensing (CS) scheme for detecting
network congested links. We focus on decreasing the required number of
measurements to detect all congested links in the context of network
tomography. We have expanded the LASSO objective function by adding a new term
corresponding to the prior knowledge based on the relationship between the
congested links and the corresponding link Betweenness Centrality (BC). The
accuracy of the proposed model is verified by simulations on two real datasets.
The results demonstrate that our model outperformed the state-of-the-art CS
based method with significant improvements in terms of F-Score.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5400</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5400</id><created>2013-01-22</created><authors><author><keyname>Roesnita</keyname><forenames>Ismail</forenames></author><author><keyname>Zainab</keyname><forenames>A. N.</forenames></author></authors><title>The Pattern of E-Book Use amongst Undergraduates an Malaysia: A Case of
  to Know is to Use</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This exploratory study focuses on identifying the usage pattern of e-books
especially on how, when, where and why undergraduates at the Faculty of
Computer Science and Information Technology (FCSIT), University of Malaya (UM),
Kuala Lumpur use or do not use the e-books service provided by the University
of Malaya library. A total of 206 (82%) useable questionnaires form the basis
of analysis. The results indicate even though the students are heavy users of
the Internet, rate themselves as skilled in Internet use and have positive
attitude towards the e-book service, the level of e-book use is still low
(39%). The students become aware of the e-book service mainly while visiting
the University of Malaya Library Website, or are referred to it by their
lecturers, friends or the librarians. About 70% rate positively on the e-book
service. Those who are users of e-books find e-books easy to use and their
usages are mainly for writing assignment or project work. Most respondents
prefer to use e-versions of textbooks and reference sources. Generally, both
users and non-users of e-books prefer to use the printed version of textbooks
especially if the text is continuously used. There are significant difference
between the frequency of e-book use and gender; between past usage of e-book
and preference for electronic textbooks and reference books. The possible
factors which may be related to e-book use are categorized into 4 groups and
presented in a model, which comprises the ICT competencies of the students,
their cognitive makeup, the degree of user access to the e-books and the
functional or use factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5412</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5412</id><created>2013-01-23</created><updated>2013-06-19</updated><authors><author><keyname>Miki</keyname><forenames>Yuichiro</forenames></author><author><keyname>Washizawa</keyname><forenames>Teruyoshi</forenames></author></authors><title>A2ILU: Auto-accelerated ILU Preconditioner for Sparse Linear Systems</title><categories>cs.NA</categories><comments>21 pages, 14 figures</comments><msc-class>65F10, 65F50, 65N06</msc-class><journal-ref>SIAM Journal on Scientific Computing, Vol.35, No.2, pp.1212-1232,
  2013</journal-ref><doi>10.1137/110842685</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ILU-based preconditioning methods in previous work have their own
parameters to improve their performances. Although the parameters may degrade
the performance, their determination is left to users. Thus, these previous
methods are not reliable in practical computer-aided engineering use. This
paper proposes a novel ILU-based preconditioner called the auto-accelerated
ILU, or A2ILU. In order to improve the convergence, A2ILU introduces
acceleration parameters which modify the ILU factorized preconditioning matrix.
A$^2$ILU needs no more operations than the original ILU because the
acceleration parameters are optimized automatically by A2ILU itself. Numerical
tests reveal the performance of A2ILU is superior to previous ILU-based methods
with manually optimized parameters. The numerical tests also demonstrate the
ability to apply auto-acceleration to ILU-based methods to improve their
performances and robustness of parameter sensitivities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5414</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5414</id><created>2013-01-23</created><updated>2013-04-23</updated><authors><author><keyname>Bostan</keyname><forenames>Alin</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Chyzak</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>De Panafieu</keyname><forenames>&#xc9;lie</forenames><affiliation>LIAFA</affiliation></author></authors><title>Complexity Estimates for Two Uncoupling Algorithms</title><categories>cs.SC</categories><comments>To appear in Proceedings of ISSAC'13 (21/01/2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uncoupling algorithms transform a linear differential system of first order
into one or several scalar differential equations. We examine two approaches to
uncoupling: the cyclic-vector method (CVM) and the
Danilevski-Barkatou-Z\&quot;urcher algorithm (DBZ). We give tight size bounds on the
scalar equations produced by CVM, and design a fast variant of CVM whose
complexity is quasi-optimal with respect to the output size. We exhibit a
strong structural link between CVM and DBZ enabling to show that, in the
generic case, DBZ has polynomial complexity and that it produces a single
equation, strongly related to the output of CVM. We prove that algorithm CVM is
faster than DBZ by almost two orders of magnitude, and provide experimental
results that validate the theoretical complexity analyses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5434</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5434</id><created>2013-01-23</created><authors><author><keyname>Velimirovic</keyname><forenames>Lazar</forenames></author><author><keyname>Peric</keyname><forenames>Zoran</forenames></author><author><keyname>Stankovic</keyname><forenames>Miomir</forenames></author><author><keyname>Nikolic</keyname><forenames>Jelena</forenames></author></authors><title>Design of Compandor Based on Approximate the First-Degree Spline
  Function</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the approximation of the optimal compressor function using
spline function of the first-degree is done. For the companding quantizer
designed on the basis of the approximative spline function of the first-degree,
the support region is numerically optimized to provide the minimum of the total
distortion for the last segment. It is shown that the companding quantizer with
the optimized support region threshold provides the signal to quantization
noise ratio that is very close to the one of the optimal companding quantizer
having an equal number of levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5435</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5435</id><created>2013-01-23</created><updated>2013-11-06</updated><authors><author><keyname>Harase</keyname><forenames>Shin</forenames></author></authors><title>On the $\mathbb{F}_2$-linear relations of Mersenne Twister pseudorandom
  number generators</title><categories>cs.NA</categories><comments>19 pages</comments><msc-class>65C10, 11K45</msc-class><doi>10.1016/j.matcom.2014.02.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequence generators obtained by linear recursions over the two-element field
$\mathbb{F}_2$, i.e., $\mathbb{F}_2$-linear generators, are widely used as
pseudorandom number generators. For example, the Mersenne Twister MT19937 is
one of the most successful applications. An advantage of such generators is
that we can assess them quickly by using theoretical criteria, such as the
dimension of equidistribution with $v$-bit accuracy. To compute these
dimensions, several polynomial-time lattice reduction algorithms have been
proposed in the case of $\mathbb{F}_2$-linear generators.
  In this paper, in order to assess non-random bit patterns in dimensions that
are higher than the dimension of equidistribution with $v$-bit accuracy,we
focus on the relationship between points in the Couture--L'Ecuyer dual lattices
and $\mathbb{F}_2$-linear relations on the most significant $v$ bits of output
sequences, and consider a new figure of merit $N_v$ based on the minimum weight
of $\mathbb{F}_2$-linear relations whose degrees are minimal for $v$. Next, we
numerically show that MT19937 has low-weight $\mathbb{F}_2$-linear relations in
dimensions higher than 623, and show that some output vectors with specific
lags are rejected or have small $p$-values in the birthday spacings tests. We
also report that some variants of Mersenne Twister, such as WELL generators,
are significantly improved from the perspective of $N_v$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5438</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5438</id><created>2013-01-23</created><authors><author><keyname>Fridgen</keyname><forenames>Gilbert</forenames></author><author><keyname>Heidemann</keyname><forenames>Julia</forenames></author></authors><title>The Importance of Continuous Value Based Project Management in the
  Context of Requirements Engineering</title><categories>cs.SE</categories><report-no>DPA-12442</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite several scientific achievements in the last years, there are still a
lot of IT projects that fail. Researchers found that one out of five
IT-projects run out of time, budget or value. Major reasons for this failure
are unexpected economic risk factors that emerge during the runtime of
projects. In order to be able to identify emerging risks early and to
counteract reasonably, financial methods for a continuous IT-project-steering
are necessary, which as of today to the best of our knowledge are missing
within scientific literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5451</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5451</id><created>2013-01-23</created><authors><author><keyname>Qu</keyname><forenames>Xiaobo</forenames></author><author><keyname>Chen</keyname><forenames>Ying</forenames></author><author><keyname>Zhuang</keyname><forenames>Xiaoxing</forenames></author><author><keyname>Yan</keyname><forenames>Zhiyu</forenames></author><author><keyname>Guo</keyname><forenames>Di</forenames></author><author><keyname>Chen</keyname><forenames>Zhong</forenames></author></authors><title>Spread spectrum compressed sensing MRI using chirp radio frequency
  pulses</title><categories>cs.CV math.OC physics.med-ph</categories><comments>4 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing has shown great potential in reducing data acquisition
time in magnetic resonance imaging (MRI). Recently, a spread spectrum
compressed sensing MRI method modulates an image with a quadratic phase. It
performs better than the conventional compressed sensing MRI with variable
density sampling, since the coherence between the sensing and sparsity bases
are reduced. However, spread spectrum in that method is implemented via a shim
coil which limits its modulation intensity and is not convenient to operate. In
this letter, we propose to apply chirp (linear frequency-swept) radio frequency
pulses to easily control the spread spectrum. To accelerate the image
reconstruction, an alternating direction algorithm is modified by exploiting
the complex orthogonality of the quadratic phase encoding. Reconstruction on
the acquired data demonstrates that more image features are preserved using the
proposed approach than those of conventional CS-MRI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5468</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5468</id><created>2013-01-23</created><updated>2013-01-24</updated><authors><author><keyname>Vigna</keyname><forenames>Sebastiano</forenames></author></authors><title>Broadword Implementation of Parenthesis Queries</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We continue the line of research started in &quot;Broadword Implementation of
Rank/Select Queries&quot; proposing broadword (a.k.a. SWAR, &quot;SIMD Within A
Register&quot;) algorithms for finding matching closed parentheses and the k-th far
closed parenthesis. Our algorithms work in time O(log w) on a word of w bits,
and contain no branch and no test instruction. On 64-bit (and wider)
architectures, these algorithms make it possible to avoid costly tabulations,
while providing a very significant speedup with respect to for-loop
implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5470</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5470</id><created>2013-01-23</created><authors><author><keyname>Zisman</keyname><forenames>Andrea</forenames></author></authors><title>Requirements Issues in SoC and SoS</title><categories>cs.SE</categories><comments>3 pages</comments><report-no>DPA-12442</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This position paper outlines some of the existing issues and challenges
concerned with the complexity of the requirements for service-oriented
computing and systems-of-systems applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5482</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5482</id><created>2013-01-23</created><updated>2015-05-14</updated><authors><author><keyname>Kurihara</keyname><forenames>Jun</forenames></author><author><keyname>Matsumoto</keyname><forenames>Ryutaroh</forenames></author><author><keyname>Uyematsu</keyname><forenames>Tomohiko</forenames></author></authors><title>Relative Generalized Rank Weight of Linear Codes and Its Applications to
  Network Coding</title><categories>cs.IT cs.CR math.CO math.IT</categories><comments>IEEEtran.cls, 25 pages, no figure, accepted for publication in IEEE
  Transactions on Information Theory</comments><doi>10.1109/TIT.2015.2429713</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By extending the notion of minimum rank distance, this paper introduces two
new relative code parameters of a linear code C_1 of length n over a field
extension and its subcode C_2. One is called the relative
dimension/intersection profile (RDIP), and the other is called the relative
generalized rank weight (RGRW). We clarify their basic properties and the
relation between the RGRW and the minimum rank distance. As applications of the
RDIP and the RGRW, the security performance and the error correction capability
of secure network coding, guaranteed independently of the underlying network
code, are analyzed and clarified. We propose a construction of secure network
coding scheme, and analyze its security performance and error correction
capability as an example of applications of the RDIP and the RGRW. Silva and
Kschischang showed the existence of a secure network coding in which no part of
the secret message is revealed to the adversary even if any dim C_1-1 links are
wiretapped, which is guaranteed over any underlying network code. However, the
explicit construction of such a scheme remained an open problem. Our new
construction is just one instance of secure network coding that solves this
open problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5488</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5488</id><created>2013-01-23</created><authors><author><keyname>Melo</keyname><forenames>Francisco</forenames></author><author><keyname>Lopes</keyname><forenames>Manuel</forenames></author></authors><title>Multi-class Generalized Binary Search for Active Inverse Reinforcement
  Learning</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of learning a task from demonstration. We
adopt the framework of inverse reinforcement learning, where tasks are
represented in the form of a reward function. Our contribution is a novel
active learning algorithm that enables the learning agent to query the expert
for more informative demonstrations, thus leading to more sample-efficient
learning. For this novel algorithm (Generalized Binary Search for Inverse
Reinforcement Learning, or GBS-IRL), we provide a theoretical bound on sample
complexity and illustrate its applicability on several different tasks. To our
knowledge, GBS-IRL is the first active IRL algorithm with provable sample
complexity bounds. We also discuss our method in light of other existing
methods in the literature and its general applicability in multi-class
classification problems. Finally, motivated by recent work on learning from
demonstration in robots, we also discuss how different forms of human feedback
can be integrated in a transparent manner in our learning framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5491</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5491</id><created>2013-01-23</created><authors><author><keyname>Bennett</keyname><forenames>Stuart</forenames></author><author><keyname>Lasenby</keyname><forenames>Joan</forenames></author></authors><title>ChESS - Quick and Robust Detection of Chess-board Features</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Localization of chess-board vertices is a common task in computer vision,
underpinning many applications, but relatively little work focusses on
designing a specific feature detector that is fast, accurate and robust. In
this paper the `Chess-board Extraction by Subtraction and Summation' (ChESS)
feature detector, designed to exclusively respond to chess-board vertices, is
presented. The method proposed is robust against noise, poor lighting and poor
contrast, requires no prior knowledge of the extent of the chess-board pattern,
is computationally very efficient, and provides a strength measure of detected
features. Such a detector has significant application both in the key field of
camera calibration, as well as in Structured Light 3D reconstruction. Evidence
is presented showing its robustness, accuracy, and efficiency in comparison to
other commonly used detectors both under simulation and in experimental 3D
reconstruction of flat plate and cylindrical objects
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5500</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5500</id><created>2013-01-23</created><updated>2014-12-02</updated><authors><author><keyname>Haase</keyname><forenames>Christoph</forenames><affiliation>CNRS &amp; ENS Cachan</affiliation></author><author><keyname>Schmitz</keyname><forenames>Sylvain</forenames><affiliation>ENS Cachan &amp; INRIA</affiliation></author><author><keyname>Schnoebelen</keyname><forenames>Philippe</forenames><affiliation>CNRS &amp; ENS Cachan</affiliation></author></authors><title>The Power of Priority Channel Systems</title><categories>cs.LO</categories><comments>Extended version of an article presented at CONCUR 2013, LNCS 8052,
  pp. 319--333, Springer, doi:10.1007/978-3-642-40184-8_23</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (December
  3, 2014) lmcs:1049</journal-ref><doi>10.2168/LMCS-10(4:4)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Priority Channel Systems, a new class of channel systems where
messages carry a numeric priority and where higher-priority messages can
supersede lower-priority messages preceding them in the fifo communication
buffers. The decidability of safety and inevitability properties is shown via
the introduction of a priority embedding, a well-quasi-ordering that has not
previously been used in well-structured systems. We then show how Priority
Channel Systems can compute Fast-Growing functions and prove that the
aforementioned verification problems are
$\mathbf{F}_{\varepsilon_{0}}$-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5522</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5522</id><created>2013-01-23</created><authors><author><keyname>Cardone</keyname><forenames>Martina</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author><author><keyname>Knopp</keyname><forenames>Raymond</forenames></author><author><keyname>Salim</keyname><forenames>Umer</forenames></author></authors><title>On Gaussian Half-Duplex Relay Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers Gaussian relay networks where a source transmits a
message to a sink terminal with the help of one or more relay nodes. The relays
work in half-duplex mode, in the sense that they can not transmit and receive
at the same time. For the case of one relay, the generalized Degrees-of-Freedom
is characterized first and then it is shown that capacity can be achieved to
within a constant gap regardless of the actual value of the channel parameters.
Different achievable schemes are presented with either deterministic or random
switch for the relay node. It is shown that random switch in general achieves
higher rates than deterministic switch. For the case of K relays, it is shown
that the generalized Degrees-of-Freedom can be obtained by solving a linear
program and that capacity can be achieved to within a constant gap of
K/2log(4K). This gap may be further decreased by considering more structured
networks such as, for example, the diamond network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5535</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5535</id><created>2013-01-23</created><authors><author><keyname>Ghasemi-Goojani</keyname><forenames>Shahab</forenames></author><author><keyname>Behroozi</keyname><forenames>Hamid</forenames></author></authors><title>On the Achievable Rate-Regions for State-Dependent Gaussian Interference
  Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a general additive state-dependent Gaussian
interference channel (ASD-GIC) where we consider two-user interference channel
with two independent states known non-causally at both transmitters, but
unknown to either of the receivers. An special case, where the additive states
over the two links are the same is studied in [1], [2], in which it is shown
that the gap between the achievable symmetric rate and the upper bound is less
than 1/4 bit for the strong interference case. Here, we also consider the case
where each channel state has unbounded variance [3], which is referred to as
the strong interferences. We first obtain an outer bound on the capacity
region. By utilizing lattice-based coding schemes, we obtain four achievable
rate regions. Depend on noise variance and channel power constraint, achievable
rate regions can coincide with the channel capacity region. For the symmetric
model, the achievable sum-rate reaches to within 0.661 bit of the channel
capacity for signal to noise ratio (SNR) greater than one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5536</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5536</id><created>2013-01-23</created><updated>2013-05-10</updated><authors><author><keyname>Parizi</keyname><forenames>Mani Bastani</forenames></author><author><keyname>Telatar</keyname><forenames>Emre</forenames></author></authors><title>On the Correlation Between Polarized BECs</title><categories>cs.IT math.IT</categories><comments>9 pages, Extended version of a paper submitted to ISIT 2013</comments><doi>10.1109/ISIT.2013.6620333</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the $2^n$ channels synthesized by the $n$-fold application of
Ar\i{}kan's polar transform to a binary erasure channel (BEC). The synthetic
channels are BECs themselves, and we show that, asymptotically for almost all
these channels, the pairwise correlations between their erasure events are
extremely small: the correlation coefficients vanish faster than any
exponential in $n$. Such a fast decay of correlations allows us to conclude
that the union bound on the block error probability of polar codes is very
tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5545</identifier>
 <datestamp>2013-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5545</id><created>2013-01-23</created><updated>2013-02-15</updated><authors><author><keyname>Devi</keyname><forenames>Gadi Gayathri</forenames></author><author><keyname>Kumari</keyname><forenames>Priya</forenames></author><author><keyname>Jyoshna</keyname><forenames>Eslavath</forenames></author><author><keyname>Deepika</keyname></author><author><keyname>Murthy</keyname><forenames>Garimella Rama</forenames></author></authors><title>Time/Computationally Optimal Network Architecture: Wireless Sensor
  Fusion</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this research paper, the problems dealing with sensor network
architecture, sensor fusion are addressed. Time/Computationally optimal network
architectures are investigated. Some novel ideas on sensor fusion are proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5582</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5582</id><created>2013-01-23</created><authors><author><keyname>Zhang</keyname><forenames>Cheng</forenames></author><author><keyname>Kjellstrom</keyname><forenames>Hedvig</forenames></author></authors><title>Multi-Class Detection and Segmentation of Objects in Depth</title><categories>cs.CV cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quality of life of many people could be improved by autonomous humanoid
robots in the home. To function in the human world, a humanoid household robot
must be able to locate itself and perceive the environment like a human; scene
perception, object detection and segmentation, and object spatial localization
in 3D are fundamental capabilities for such humanoid robots. This paper
presents a 3D multi-class object detection and segmentation method. The
contributions are twofold. Firstly, we present a multi-class detection method,
where a minimal joint codebook is learned in a principled manner. Secondly, we
incorporate depth information using RGB-D imagery, which increases the
robustness of the method and gives the 3D location of objects -- necessary
since the robot reasons in 3D space. Experiments show that the multi-class
extension improves the detection efficiency with respect to the number of
classes and the depth extension improves the detection robustness and give
sufficient natural 3D location of the objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5584</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5584</id><created>2013-01-23</created><authors><author><keyname>Kwok</keyname><forenames>Tsz Chiu</forenames></author><author><keyname>Lau</keyname><forenames>Lap Chi</forenames></author><author><keyname>Lee</keyname><forenames>Yin Tat</forenames></author><author><keyname>Gharan</keyname><forenames>Shayan Oveis</forenames></author><author><keyname>Trevisan</keyname><forenames>Luca</forenames></author></authors><title>Improved Cheeger's Inequality: Analysis of Spectral Partitioning
  Algorithms through Higher Order Spectral Gap</title><categories>cs.DS cs.DM math.CO math.SP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let \phi(G) be the minimum conductance of an undirected graph G, and let
0=\lambda_1 &lt;= \lambda_2 &lt;=... &lt;= \lambda_n &lt;= 2 be the eigenvalues of the
normalized Laplacian matrix of G. We prove that for any graph G and any k &gt;= 2,
  \phi(G) = O(k) \lambda_2 / \sqrt{\lambda_k}, and this performance guarantee
is achieved by the spectral partitioning algorithm. This improves Cheeger's
inequality, and the bound is optimal up to a constant factor for any k. Our
result shows that the spectral partitioning algorithm is a constant factor
approximation algorithm for finding a sparse cut if \lambda_k$ is a constant
for some constant k. This provides some theoretical justification to its
empirical performance in image segmentation and clustering problems. We extend
the analysis to other graph partitioning problems, including multi-way
partition, balanced separator, and maximum cut.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5585</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5585</id><created>2013-01-23</created><authors><author><keyname>Brzozowski</keyname><forenames>Janusz</forenames></author><author><keyname>Tamm</keyname><forenames>Hellis</forenames></author></authors><title>Minimal Nondeterministic Finite Automata and Atoms of Regular Languages</title><categories>cs.FL</categories><comments>15 pages, 29 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the NFA minimization problem in terms of atomic NFA's, that is,
NFA's in which the right language of every state is a union of atoms, where the
atoms of a regular language are non-empty intersections of complemented and
uncomplemented left quotients of the language. We characterize all reduced
atomic NFA's of a given language, that is, those NFA's that have no equivalent
states. Using atomic NFA's, we formalize Sengoku's approach to NFA minimization
and prove that his method fails to find all minimal NFA's. We also formulate
the Kameda-Weiner NFA minimization in terms of quotients and atoms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5586</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5586</id><created>2013-01-23</created><authors><author><keyname>Lee</keyname><forenames>Conrad</forenames></author><author><keyname>McDaid</keyname><forenames>Aaron</forenames></author><author><keyname>Cunningham</keyname><forenames>P&#xe1;draig</forenames></author></authors><title>Measuring the Significance of the Geographic Flow of Music</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work, our results suggested that some cities tend to be ahead of
others in their musical preferences. We concluded that work by noting that to
properly test this claim, we would try to exploit the leader-follower
relationships that we identified to make predictions. Here we present the
results of our predictive evaluation. We find that information on the past
musical preferences in other cities allows a linear model to improve its
predictions by approx. 5% over a simple baseline. This suggests that at best,
previously found leader-follower relationships are rather weak.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5588</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5588</id><created>2013-01-22</created><updated>2014-07-24</updated><authors><author><keyname>Moore</keyname><forenames>Matthew</forenames></author></authors><title>The Undecidability of the Definability of Principal Subcongruences</title><categories>math.LO cs.LO</categories><msc-class>03C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For each Turing machine T, we construct an algebra A'(T) such that the
variety generated by A'(T) has definable principal subcongruences if and only
if T halts, thus proving that the property of having definable principal
subcongruences is undecidable for a finite algebra. A consequence of this is
that there is no algorithm that takes as input a finite algebra a decides
whether that algebra is finitely based.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5593</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5593</id><created>2013-01-23</created><authors><author><keyname>Zhang</keyname><forenames>Bowen</forenames></author><author><keyname>Baillieul</keyname><forenames>John</forenames></author></authors><title>A Packetized Direct Load Control Mechanism for Demand Side Management</title><categories>cs.SY</categories><comments>the 51st IEEE Conference on Decision and Control,December 10-13,
  Maui, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electricity peaks can be harmful to grid stability and result in additional
generation costs to balance supply with demand. By developing a network of
smart appliances together with a quasi-decentralized control protocol, direct
load control (DLC) provides an opportunity to reduce peak consumption by
directly controlling the on/off switch of the networked appliances. This paper
proposes a packetized DLC (PDLC) solution that is illustrated by an application
to air conditioning temperature control. Here the term packetized refers to a
fixed time energy usage authorization. The consumers in each room choose their
preferred set point, and then an operator of the local appliance pool will
determine the comfort band around the set point. We use a thermal dynamic model
to investigate the duty cycle of thermostatic appliances. Three theorems are
proposed in this paper. The first two theorems evaluate the performance of the
PDLC in both transient and steady state operation. The first theorem proves
that the average room temperature would converge to the average room set point
with fixed number of packets applied in each discrete interval. The second
theorem proves that the PDLC solution guarantees to control the temperature of
all the rooms within their individual comfort bands. The third theorem proposes
an allocation method to link the results in theorem 1 and assumptions in
theorem 2 such that the overall PDLC solution works. The direct result of the
theorems is that we can reduce the consumption oscillation that occurs when no
control is applied. Simulation is provided to verify theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5595</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5595</id><created>2013-01-20</created><authors><author><keyname>Karam</keyname><forenames>Antoine</forenames></author><author><keyname>Play</keyname><forenames>Daniel</forenames></author></authors><title>A discrete analysis of metal-v belt drive</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The metal-V belt drive includes a large number of parts which interact
between them to transmit power from the input to the output pulleys. A
compression belt composed of a great number of struts is maintained by a
tension flat belt. Power is them shared into the two belts that moves generally
in opposite directions. Due to the particular geometry of the elements and to
the great number of parts, a numerical approach achieves the global equilibrium
of the mechanism from the elementary part equilibrium. Sliding arc on each
pulley can be thus defined both for the compression and tension belts. Finally,
power sharing can be calculated as differential motion between the belts, is
defined. The first part of the paper will present the different steps of the
quasi-static mechanical analysis and their numerical implementations. Load
distributions, speed profiles and sliding angle values will be discussed. The
second part of the paper will deal to a systematic use of the computer
software. Speed ratio, transmitted torque, strut geometry and friction
coefficients effect will be analysed with the output parameter variations.
Finally, the effect pulley deformable flanges will be discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5596</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5596</id><created>2013-01-23</created><authors><author><keyname>Hurley</keyname><forenames>Barry</forenames></author><author><keyname>Hurley</keyname><forenames>Ted</forenames></author></authors><title>Systems of MDS codes from units and idempotents</title><categories>cs.IT math.IT</categories><msc-class>94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algebraic systems are constructed from which series of maximum distance
separable (mds) codes are derived. The methods use unit and idempotent schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5607</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5607</id><created>2013-01-23</created><updated>2013-01-25</updated><authors><author><keyname>Ellerman</keyname><forenames>David</forenames></author></authors><title>Information as Distinctions: New Foundations for Information Theory</title><categories>cs.IT math.IT math.LO</categories><msc-class>94A17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The logical basis for information theory is the newly developed logic of
partitions that is dual to the usual Boolean logic of subsets. The key concept
is a &quot;distinction&quot; of a partition, an ordered pair of elements in distinct
blocks of the partition. The logical concept of entropy based on partition
logic is the normalized counting measure of the set of distinctions of a
partition on a finite set--just as the usual logical notion of probability
based on the Boolean logic of subsets is the normalized counting measure of the
subsets (events). Thus logical entropy is a measure on the set of ordered
pairs, and all the compound notions of entropy (join entropy, conditional
entropy, and mutual information) arise in the usual way from the measure (e.g.,
the inclusion-exclusion principle)--just like the corresponding notions of
probability. The usual Shannon entropy of a partition is developed by replacing
the normalized count of distinctions (dits) by the average number of binary
partitions (bits) necessary to make all the distinctions of the partition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5612</identifier>
 <datestamp>2013-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5612</id><created>2013-01-23</created><updated>2013-05-03</updated><authors><author><keyname>Faug&#xe8;re</keyname><forenames>Jean-Charles</forenames><affiliation>INRIA Paris-Rocquencourt, LIP6</affiliation></author><author><keyname>Din</keyname><forenames>Mohab Safey El</forenames><affiliation>INRIA Paris-Rocquencourt, LIP6</affiliation></author><author><keyname>Verron</keyname><forenames>Thibaut</forenames><affiliation>INRIA Paris-Rocquencourt, LIP6</affiliation></author></authors><title>On the Complexity of Computing Gr\&quot;obner Bases for Quasi-homogeneous
  Systems</title><categories>cs.SC</categories><proxy>ccsd</proxy><journal-ref>ISSAC'13 - Proceedings of the 38th International Symposium on
  Symbolic and Algebraic Computation (2013) (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\K$ be a field and $(f_1, \ldots, f_n)\subset \K[X_1, \ldots, X_n]$ be a
sequence of quasi-homogeneous polynomials of respective weighted degrees $(d_1,
\ldots, d_n)$ w.r.t a system of weights $(w_{1},\dots,w_{n})$. Such systems are
likely to arise from a lot of applications, including physics or cryptography.
We design strategies for computing Gr\&quot;obner bases for quasi-homogeneous
systems by adapting existing algorithms for homogeneous systems to the
quasi-homogeneous case. Overall, under genericity assumptions, we show that for
a generic zero-dimensional quasi-homogeneous system, the complexity of the full
strategy is polynomial in the weighted B\'ezout bound $\prod_{i=1}^{n}d_{i} /
\prod_{i=1}^{n}w_{i}$. We provide some experimental results based on generic
systems as well as systems arising from a cryptography problem. They show that
taking advantage of the quasi-homogeneous structure of the systems allow us to
solve systems that were out of reach otherwise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5650</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5650</id><created>2013-01-23</created><updated>2013-06-20</updated><authors><author><keyname>Pachitariu</keyname><forenames>Marius</forenames></author><author><keyname>Sahani</keyname><forenames>Maneesh</forenames></author></authors><title>Regularization and nonlinearities for neural language models: when are
  they needed?</title><categories>stat.ML cs.LG</categories><comments>Added new experiments on large datasets and on the Microsoft Research
  Sentence Completion Challenge</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural language models (LMs) based on recurrent neural networks (RNN) are
some of the most successful word and character-level LMs. Why do they work so
well, in particular better than linear neural LMs? Possible explanations are
that RNNs have an implicitly better regularization or that RNNs have a higher
capacity for storing patterns due to their nonlinearities or both. Here we
argue for the first explanation in the limit of little training data and the
second explanation for large amounts of text data. We show state-of-the-art
performance on the popular and small Penn dataset when RNN LMs are regularized
with random dropout. Nonetheless, we show even better performance from a
simplified, much less expressive linear RNN model without off-diagonal entries
in the recurrent matrix. We call this model an impulse-response LM (IRLM).
Using random dropout, column normalization and annealed learning rates, IRLMs
develop neurons that keep a memory of up to 50 words in the past and achieve a
perplexity of 102.5 on the Penn dataset. On two large datasets however, the
same regularization methods are unsuccessful for both models and the RNN's
expressivity allows it to overtake the IRLM by 10 and 20 percent perplexity,
respectively. Despite the perplexity gap, IRLMs still outperform RNNs on the
Microsoft Research Sentence Completion (MRSC) task. We develop a slightly
modified IRLM that separates long-context units (LCUs) from short-context units
and show that the LCUs alone achieve a state-of-the-art performance on the MRSC
task of 60.8%. Our analysis indicates that a fruitful direction of research for
neural LMs lies in developing more accessible internal representations, and
suggests an optimization regime of very high momentum terms for effectively
training such models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5655</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5655</id><created>2013-01-23</created><updated>2013-05-18</updated><authors><author><keyname>Padakandla</keyname><forenames>Arun</forenames></author><author><keyname>Pradhan</keyname><forenames>Sandeep</forenames></author></authors><title>Achievable rate region based on coset codes for multiple access channel
  with states</title><categories>cs.IT math.IT</categories><comments>Contains substantial additions to previous version. In particular 1)
  All proofs are provided, 2) An achievable rate region based on Abelian group
  codes is developed 3) Characterization of the largest known achievable rate
  region for the MAC with states is provided. Three examples from previous
  version are removed in the interest of brevity. This work has been accepted
  for presentation at ISIT2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the ensemble the nested coset codes built on finite fields
achieves the capacity of arbitrary discrete memoryless point-to-point channels.
Exploiting it's algebraic structure, we develop a coding technique for
communication over general discrete multiple access channel with channel state
information distributed at the transmitters. We build an algebraic coding
framework for this problem using the ensemble of Abelian group codes and
thereby derive a new achievable rate region. We identify non-additive and
non-symmteric examples for which the proposed achievable rate region is
strictly larger than the one achievable using random unstructured codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5676</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5676</id><created>2013-01-23</created><updated>2015-08-26</updated><authors><author><keyname>Giurgiu</keyname><forenames>Andrei</forenames></author><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author><author><keyname>Urbanke</keyname><forenames>R&#xfc;diger</forenames></author></authors><title>Spatial Coupling as a Proof Technique</title><categories>cs.IT math.IT</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to show that spatial coupling can be viewed not only
as a means to build better graphical models, but also as a tool to better
understand uncoupled models. The starting point is the observation that some
asymptotic properties of graphical models are easier to prove in the case of
spatial coupling. In such cases, one can then use the so-called interpolation
method to transfer known results for the spatially coupled case to the
uncoupled one.
  Our main use of this framework is for LDPC codes, where we use interpolation
to show that the average entropy of the codeword conditioned on the observation
is asymptotically the same for spatially coupled as for uncoupled ensembles.
  We give three applications of this result for a large class of LDPC
ensembles. The first one is a proof of the so-called Maxwell construction
stating that the MAP threshold is equal to the Area threshold of the BP GEXIT
curve. The second is a proof of the equality between the BP and MAP GEXIT
curves above the MAP threshold. The third application is the intimately related
fact that the replica symmetric formula for the conditional entropy in the
infinite block length limit is exact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5683</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5683</id><created>2013-01-23</created><authors><author><keyname>Duersch</keyname><forenames>Peter</forenames></author><author><keyname>Oechssler</keyname><forenames>Joerg</forenames></author><author><keyname>Schipper</keyname><forenames>Burkhard C.</forenames></author></authors><title>When is tit-for-tat unbeatable?</title><categories>cs.GT q-bio.PE</categories><comments>arXiv admin note: text overlap with arXiv:1003.4274</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the class of symmetric two-player games in which tit-for-tat
cannot be beaten even by very sophisticated opponents in a repeated game. It
turns out to be the class of exact potential games. More generally, there is a
class of simple imitation rules that includes tit-for-tat but also
imitate-the-best and imitate-if-better. Every decision rule in this class is
essentially unbeatable in exact potential games. Our results apply to many
interesting games including all symmetric 2x2 games, and standard examples of
Cournot duopoly, price competition, public goods games, common pool resource
games, and minimum effort coordination games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5684</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5684</id><created>2013-01-23</created><updated>2013-05-18</updated><authors><author><keyname>Padakandla</keyname><forenames>Arun</forenames></author><author><keyname>Pradhan</keyname><forenames>S. Sandeep</forenames></author></authors><title>Computing sum of sources over an arbitrary multiple access channel</title><categories>cs.IT math.IT</categories><comments>Contains proof of the main theorem and a few minor corrections.
  Contents of this article have been accepted for presentation at ISIT2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of computing sum of sources over a multiple access channel (MAC)
is considered. Building on the technique of linear computation coding (LCC)
proposed by Nazer and Gastpar [2007], we employ the ensemble of nested coset
codes to derive a new set of sufficient conditions for computing the sum of
sources over an \textit{arbitrary} MAC. The optimality of nested coset codes
[Padakandla, Pradhan 2011] enables this technique outperform LCC even for
linear MAC with a structural match. Examples of nonadditive MAC for which the
technique proposed herein outperforms separation and systematic based
computation are also presented. Finally, this technique is enhanced by
incorporating separation based strategy, leading to a new set of sufficient
conditions for computing the sum over a MAC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5686</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5686</id><created>2013-01-23</created><updated>2013-01-26</updated><authors><author><keyname>Kang</keyname><forenames>Jeon-Hyung</forenames></author><author><keyname>Ma</keyname><forenames>Jun</forenames></author><author><keyname>Liu</keyname><forenames>Yan</forenames></author></authors><title>Transfer Topic Modeling with Ease and Scalability</title><categories>cs.CL cs.LG stat.ML</categories><comments>2012 SIAM International Conference on Data Mining (SDM12) Pages:
  {564-575}</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing volume of short texts generated on social media sites, such as
Twitter or Facebook, creates a great demand for effective and efficient topic
modeling approaches. While latent Dirichlet allocation (LDA) can be applied, it
is not optimal due to its weakness in handling short texts with fast-changing
topics and scalability concerns. In this paper, we propose a transfer learning
approach that utilizes abundant labeled documents from other domains (such as
Yahoo! News or Wikipedia) to improve topic modeling, with better model fitting
and result interpretation. Specifically, we develop Transfer Hierarchical LDA
(thLDA) model, which incorporates the label information from other domains via
informative priors. In addition, we develop a parallel implementation of our
model for large-scale applications. We demonstrate the effectiveness of our
thLDA model on both a microblogging dataset and standard text collections
including AP and RCV1 datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5687</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5687</id><created>2013-01-23</created><authors><author><keyname>Mohammadi</keyname><forenames>M.</forenames></author><author><keyname>Suraweera</keyname><forenames>H. A.</forenames></author><author><keyname>Zhou</keyname><forenames>X.</forenames></author></authors><title>Outage Probability of Wireless Ad Hoc Networks with Cooperative Relaying</title><categories>cs.IT math.IT</categories><comments>7 pages; IEEE Globecom 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the performance of cooperative transmissions in
wireless ad hoc networks with random node locations. According to a contention
probability for message transmission, each source node can either transmits its
own message signal or acts as a potential relay for others. Hence, each
destination node can potentially receive two copies of the message signal, one
from the direct link and the other from the relay link. Taking the random node
locations and interference into account, we derive closed-form expressions for
the outage probability with different combining schemes at the destination
nodes. In particular, the outage performance of optimal combining, maximum
ratio combining, and selection combining strategies are studied and quantified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5695</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5695</id><created>2013-01-23</created><updated>2013-07-07</updated><authors><author><keyname>Liu</keyname><forenames>Binyue</forenames></author><author><keyname>Yang</keyname><forenames>Ye</forenames></author><author><keyname>Cai</keyname><forenames>Ning</forenames></author></authors><title>Optimal Amplify-and-Forward Schemes for Relay Channels with Correlated
  Relay Noise</title><categories>cs.IT math.IT</categories><comments>The present pre-print paper is not submitted to anywhere else
  currently. However, some reviewers always consider the arXiv submission as an
  evidence that the paper is a submitted one or even a published one. So, we
  decided to remove it from the arXiv system to avoid the confusion and
  misunderstanding</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates amplify-and-forward (AF) schemes for both one and
two-way relay channels. Unlike most existing works assuming independent noise
at the relays, we consider a more general scenario with correlated relay noise.
We first propose an approach to efficiently solve a class of quadratically
constrained fractional problems via second-order cone programming (SOCP). Then
it is shown that the AF relay optimization problems studied in this paper can
be incorporated into such quadratically constrained fractional problems. As a
consequence, the proposed approach can be used as a unified framework to solve
the optimal AF rate for the one-way relay channel and the optimal AF rate
region for the two-way relay channel under both sum and individual relay power
constraints.
  In particular, for one-way relay channel under individual relay power
constraints, we propose two suboptimal AF schemes in closed-form. It is shown
that they are approximately optimal in certain conditions of interest.
Furthermore, we find an interesting result that, on average, noise correlation
is beneficial no matter the relays know the noise covariance matrix or not for
such scenario. Overall, the obtained results recover and generalize several
existing results for the uncorrelated counterpart. (unsubmitted)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5701</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5701</id><created>2013-01-24</created><updated>2014-12-17</updated><authors><author><keyname>Yilmaz</keyname><forenames>Yasin</forenames></author><author><keyname>Moustakides</keyname><forenames>George V.</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Sequential and Decentralized Estimation of Linear Regression Parameters
  in Wireless Sensor Networks</title><categories>stat.AP cs.IT math.IT math.OC math.PR stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequential estimation of a vector of linear regression coefficients is
considered under both centralized and decentralized setups. In sequential
estimation, the number of observations used for estimation is determined by the
observed samples, hence is random, as opposed to fixed-sample-size estimation.
Specifically, after receiving a new sample, if a target accuracy level is
reached, we stop and estimate using the samples collected so far; otherwise we
continue to receive another sample. It is known that finding an optimum
sequential estimator, which minimizes the average sample number for a given
target accuracy level, is an intractable problem with a general stopping rule
that depends on the complete observation history. By properly restricting the
search space to stopping rules that depend on a specific subset of the complete
observation history, we derive the optimum sequential estimator in the
centralized case via optimal stopping theory. However, finding the optimum
stopping rule in this case requires numerical computations that {\em
quadratically} scales with the number of parameters to be estimated. For the
decentralized setup with stringent energy constraints, under an alternative
problem formulation that is conditional on the observed regressors, we first
derive a simple optimum scheme whose computational complexity is {\em constant}
with respect to the number of parameters. Then, following this simple optimum
scheme we propose a decentralized sequential estimator whose computational
complexity and energy consumption scales {\em linearly} with the number of
parameters. Specifically, in the proposed decentralized scheme a
close-to-optimum average stopping time performance is achieved by infrequently
transmitting a single pulse with very short duration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5728</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5728</id><created>2013-01-24</created><updated>2013-04-18</updated><authors><author><keyname>Takeuchi</keyname><forenames>Keigo</forenames></author><author><keyname>Tanaka</keyname><forenames>Toshiyuki</forenames></author><author><keyname>Kasai</keyname><forenames>Kenta</forenames></author></authors><title>A Potential Theory of General Spatially-Coupled Systems via a Continuum
  Approximation</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE ITW2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes general spatially-coupled (SC) systems with
multi-dimensional coupling. A continuum approximation is used to derive
potential functions that characterize the performance of the SC systems. For
any dimension of coupling, it is shown that, if the boundary of the SC systems
is fixed to the unique stable solution that minimizes the potential over all
stationary solutions, the systems can approach the optimal performance as the
number of coupled systems tends to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5734</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5734</id><created>2013-01-24</created><authors><author><keyname>Laslier</keyname><forenames>Benoit</forenames></author><author><keyname>Laslier</keyname><forenames>Jean-Francois</forenames></author></authors><title>Reinforcement learning from comparisons: Three alternatives is enough,
  two is not</title><categories>math.OC cs.LG math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper deals with the problem of finding the best alternatives on the
basis of pairwise comparisons when these comparisons need not be transitive. In
this setting, we study a reinforcement urn model. We prove convergence to the
optimal solution when reinforcement of a winning alternative occurs each time
after considering three random alternatives. The simpler process, which
reinforces the winner of a random pair does not always converges: it may cycle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5738</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5738</id><created>2013-01-24</created><authors><author><keyname>Southwell</keyname><forenames>Richard</forenames></author><author><keyname>Cannings</keyname><forenames>Chris</forenames></author></authors><title>Best Response Games on Regular Graphs</title><categories>cs.GT cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the growth of the internet it is becoming increasingly important to
understand how the behaviour of players is affected by the topology of the
network interconnecting them. Many models which involve networks of interacting
players have been proposed and best response games are amongst the simplest. In
best response games each vertex simultaneously updates to employ the best
response to their current surroundings. We concentrate upon trying to
understand the dynamics of best response games on regular graphs with many
strategies. When more than two strategies are present highly complex dynamics
can ensue. We focus upon trying to understand exactly how best response games
on regular graphs sample from the space of possible cellular automata. To
understand this issue we investigate convex divisions in high dimensional space
and we prove that almost every division of $k-1$ dimensional space into $k$
convex regions includes a single point where all regions meet. We then find
connections between the convex geometry of best response games and the theory
of alternating circuits on graphs. Exploiting these unexpected connections
allows us to gain an interesting answer to our question of when cellular
automata are best response games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5765</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5765</id><created>2013-01-24</created><updated>2013-02-14</updated><authors><author><keyname>Kang</keyname><forenames>Du Ho</forenames></author><author><keyname>Sung</keyname><forenames>Ki Won</forenames></author><author><keyname>Zander</keyname><forenames>Jens</forenames></author></authors><title>High Capacity Indoor &amp; Hotspot Wireless System in Shared Spectrum - A
  Techno-Economic Analysis</title><categories>cs.NI cs.IT math.IT</categories><comments>This paper has been withdrawn by the author's own decision for future
  update for publishing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predictions for wireless and mobile Internet access suggest exponential
traffic increase particularly in inbuilding environments. Non-traditional
actors such as facility owners have a growing interest in deploying and
operating their own indoor networks to fulfill the capacity demand. Such local
operators will need spectrum sharing with neighboring networks because they are
not likely to have their own dedicated spectrum. Management of inter-network
interference then becomes a key issue for high capacity provision. Tight
operator-wise cooperation provides superior performance, but at the expense of
high infrastructure cost and business-related barriers. Limited coordination on
the other hand causes harmful interference between operators which in turn will
require even denser networks. In this paper, we propose a techno-economic
analysis framework for investigating and comparing the strategies of the indoor
operators. We refine a traditional network cost model by introducing new
inter-operator cost factors. Then, we present a numerical example to
demonstrate how the proposed framework can help us comparing different operator
strategies. Finally, we suggest areas for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5782</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5782</id><created>2013-01-24</created><authors><author><keyname>Andersen</keyname><forenames>Jens Peter</forenames></author></authors><title>Association between quality of clinical practice guidelines and
  citations given to their references</title><categories>cs.DL</categories><comments>Paper submitted to 14th International Society of Scientometrics and
  Informetrics Conference</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  It has been suggested that bibliometric analysis of different document types
may reveal new aspects of research performance. In medical research a number of
study types play different roles in the research process and it has been shown,
that the evidence-level of study types is associated with varying citation
rates. This study focuses on clinical practice guidelines, which are supposed
to gather the highest evidence on a given topic to give the best possible
recommendation for practitioners. The quality of clinical practice guidelines,
measured using the AGREE score, is compared to the citations given to the
references used in these guidelines, as it is hypothesised, that better
guidelines are based on higher cited references. AGREE scores are gathered from
reviews of clinical practice guidelines on a number of diseases and treatments.
Their references are collected from Web of Science and citation counts are
normalised using the item-oriented z-score and the PPtop-10% indicators. A
positive correlation between both citation indicators and the AGREE score of
clinical practice guidelines is found. Some potential confounding factors are
identified. While confounding cannot be excluded, results indicate low
likelihood for the identified confounders. The results provide a new
perspective to and application of citation analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5793</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5793</id><created>2013-01-24</created><authors><author><keyname>Ucar</keyname><forenames>I&#xf1;aki</forenames><affiliation>Universidad P&#xfa;blica de Navarra</affiliation></author><author><keyname>Navarro-Ortiz</keyname><forenames>Jorge</forenames><affiliation>Universidad de Granada</affiliation></author><author><keyname>Ameigeiras</keyname><forenames>Pablo</forenames><affiliation>Universidad de Granada</affiliation></author><author><keyname>Lopez-Soler</keyname><forenames>Juan M.</forenames><affiliation>Universidad de Granada</affiliation></author></authors><title>Video Tester -- A multiple-metric framework for video quality assessment
  over IP networks</title><categories>cs.MM cs.PF</categories><comments>5 pages, 5 figures. For the Google Code project, see
  http://video-tester.googlecode.com/</comments><acm-class>C.2.3; C.4</acm-class><journal-ref>IEEE International Symposium on Broadband Multimedia Systems and
  Broadcasting, pp.1-5, 2012</journal-ref><doi>10.1109/BMSB.2012.6264243</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an extensible and reusable framework which addresses the
problem of video quality assessment over IP networks. The proposed tool
(referred to as Video-Tester) supports raw uncompressed video encoding and
decoding. It also includes different video over IP transmission methods (i.e.:
RTP over UDP unicast and multicast, as well as RTP over TCP). In addition, it
is furnished with a rich set of offline analysis capabilities. Video-Tester
analysis includes QoS and bitstream parameters estimation (i.e.: bandwidth,
packet inter-arrival time, jitter and loss rate, as well as GOP size and
I-frame loss rate). Our design facilitates the integration of virtually any
existing video quality metric thanks to the adopted Python-based modular
approach. Video-Tester currently provides PSNR, SSIM, ITU-T G.1070 video
quality metric, DIV and PSNR-based MOS estimations. In order to promote its use
and extension, Video-Tester is open and publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5798</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5798</id><created>2013-01-24</created><authors><author><keyname>Khalil</keyname><forenames>Houssam</forenames><affiliation>INRIA Sophia Antipolis, ICJ</affiliation></author><author><keyname>Mourrain</keyname><forenames>Bernard</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Schatzman</keyname><forenames>Michelle</forenames><affiliation>ICJ</affiliation></author></authors><title>Superfast solution of Toeplitz systems based on syzygy reduction</title><categories>cs.SC math.NA</categories><comments>(07/11/2011). arXiv admin note: substantial text overlap with
  arXiv:0903.1244</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new superfast algorithm for solving Toeplitz systems. This
algorithm is based on a relation between the solution of such problems and
syzygies of polynomials or moving lines. We show an explicit connection between
the generators of a Toeplitz matrix and the generators of the corresponding
module of syzygies. We show that this module is generated by two elements and
the solution of a Toeplitz system T u=g can be reinterpreted as the remainder
of a vector depending on g, by these two generators. We obtain these generators
and this remainder with computational complexity O(n log^2 n) for a Toeplitz
matrix of size nxn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5804</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5804</id><created>2013-01-24</created><authors><author><keyname>Bostan</keyname><forenames>Alin</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Schost</keyname><forenames>Eric</forenames></author></authors><title>A simple and fast algorithm for computing exponentials of power series</title><categories>cs.SC</categories><proxy>ccsd</proxy><journal-ref>Information Processing Letters 109, 13 (2009) 754-756</journal-ref><doi>10.1016/j.ipl.2009.03.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As was initially shown by Brent, exponentials of truncated power series can
be computed using a constant number of polynomial multiplications. This note
gives a relatively simple algorithm with a low constant factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5809</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5809</id><created>2013-01-24</created><updated>2013-02-18</updated><authors><author><keyname>Greene</keyname><forenames>Derek</forenames></author><author><keyname>Cunningham</keyname><forenames>P&#xe1;draig</forenames></author></authors><title>Producing a Unified Graph Representation from Multiple Social Network
  Views</title><categories>cs.SI physics.soc-ph</categories><comments>13 pages. Clarify notation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many social networks, several different link relations will exist between
the same set of users. Additionally, attribute or textual information will be
associated with those users, such as demographic details or user-generated
content. For many data analysis tasks, such as community finding and data
visualisation, the provision of multiple heterogeneous types of user data makes
the analysis process more complex. We propose an unsupervised method for
integrating multiple data views to produce a single unified graph
representation, based on the combination of the k-nearest neighbour sets for
users derived from each view. These views can be either relation-based or
feature-based. The proposed method is evaluated on a number of annotated
multi-view Twitter datasets, where it is shown to support the discovery of the
underlying community structure in the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5831</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5831</id><created>2013-01-24</created><updated>2013-01-25</updated><authors><author><keyname>Marques-Pita</keyname><forenames>Manuel</forenames></author><author><keyname>Rocha</keyname><forenames>Luis M.</forenames></author></authors><title>Canalization and control in automata networks: body segmentation in
  Drosophila melanogaster</title><categories>q-bio.MN cs.CE cs.DM cs.FL nlin.AO</categories><comments>77 pages, 21 figures and 4 tables. Supplementary information not
  included. PLoS ONE (in press)</comments><doi>10.1371/journal.pone.0055946</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present schema redescription as a methodology to characterize canalization
in automata networks used to model biochemical regulation and signalling. In
our formulation, canalization becomes synonymous with redundancy present in the
logic of automata. This results in straightforward measures to quantify
canalization in an automaton (micro-level), which is in turn integrated into a
highly scalable framework to characterize the collective dynamics of
large-scale automata networks (macro-level). This way, our approach provides a
method to link micro- to macro-level dynamics -- a crux of complexity. Several
new results ensue from this methodology: uncovering of dynamical modularity
(modules in the dynamics rather than in the structure of networks),
identification of minimal conditions and critical nodes to control the
convergence to attractors, simulation of dynamical behaviour from incomplete
information about initial conditions, and measures of macro-level canalization
and robustness to perturbations. We exemplify our methodology with a well-known
model of the intra- and inter cellular genetic regulation of body segmentation
in Drosophila melanogaster. We use this model to show that our analysis does
not contradict any previous findings. But we also obtain new knowledge about
its behaviour: a better understanding of the size of its wild-type attractor
basin (larger than previously thought), the identification of novel minimal
conditions and critical nodes that control wild-type behaviour, and the
resilience of these to stochastic interventions. Our methodology is applicable
to any complex network that can be modelled using automata, but we focus on
biochemical regulation and signalling, towards a better understanding of the
(decentralized) control that orchestrates cellular activity -- with the
ultimate goal of explaining how do cells and tissues 'compute'.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5842</identifier>
 <datestamp>2013-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5842</id><created>2013-01-24</created><updated>2013-11-07</updated><authors><author><keyname>Je&#x17c;</keyname><forenames>Artur</forenames></author></authors><title>Approximation of grammar-based compression via recompression</title><categories>cs.DS cs.FL</categories><comments>22 pages, some many small improvements, to be submited to a journal</comments><acm-class>E.4; F.4.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a simple linear-time algorithm constructing a
context-free grammar of size O(g log(N/g)) for the input string, where N is the
size of the input string and g the size of the optimal grammar generating this
string. The algorithm works for arbitrary size alphabets, but the running time
is linear assuming that the alphabet \Sigma of the input string can be
identified with numbers from {1, ..., N^c} for some constant c. Otherwise,
additional cost of O(n log|\Sigma|) is needed.
  Algorithms with such approximation guarantees and running time are known, the
novelty of this paper is a particular simplicity of the algorithm as well as
the analysis of the algorithm, which uses a general technique of recompression
recently introduced by the author. Furthermore, contrary to the previous
results, this work does not use the LZ representation of the input string in
the construction, nor in the analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5844</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5844</id><created>2013-01-24</created><authors><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>Goldberg</keyname><forenames>Paul W.</forenames></author><author><keyname>Krysta</keyname><forenames>Piotr</forenames></author><author><keyname>Ventre</keyname><forenames>Carmine</forenames></author></authors><title>Ranking Games that have Competitiveness-based Strategies</title><categories>cs.GT cs.DS</categories><comments>21 pages; preliminary version in ACM-EC 2010; accepted for
  publication in Theoretical Computer Science</comments><journal-ref>TCS 476 24-37 (2013)</journal-ref><doi>10.1016/j.tcs.2013.01.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An extensive literature in economics and social science addresses contests,
in which players compete to outperform each other on some measurable criterion,
often referred to as a player's score, or output. Players incur costs that are
an increasing function of score, but receive prizes for obtaining higher score
than their competitors. In this paper we study finite games that are
discretized contests, and the problems of computing exact and approximate Nash
equilibria. Our motivation is the worst-case hardness of Nash equilibrium
computation, and the resulting interest in important classes of games that
admit polynomial-time algorithms. For games that have a tie-breaking rule for
players' scores, we present a polynomial-time algorithm for computing an exact
equilibrium in the 2-player case, and for multiple players, a characterization
of Nash equilibria that shows an interesting parallel between these games and
unrestricted 2-player games in normal form. When ties are allowed, via a
reduction from these games to a subclass of anonymous games, we give
approximation schemes for two special cases: constant-sized set of strategies,
and constant number of players.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5848</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5848</id><created>2013-01-24</created><updated>2014-03-28</updated><authors><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Niesen</keyname><forenames>Urs</forenames></author></authors><title>Decentralized Coded Caching Attains Order-Optimal Memory-Rate Tradeoff</title><categories>cs.IT cs.NI math.IT</categories><comments>To appear in IEEE/ACM Transactions on Networking</comments><journal-ref>IEEE/ACM Transactions on Networking, vol. 23, pp. 1029 - 1040,
  August 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Replicating or caching popular content in memories distributed across the
network is a technique to reduce peak network loads. Conventionally, the main
performance gain of this caching was thought to result from making part of the
requested data available closer to end users. Instead, we recently showed that
a much more significant gain can be achieved by using caches to create
coded-multicasting opportunities, even for users with different demands,
through coding across data streams. These coded-multicasting opportunities are
enabled by careful content overlap at the various caches in the network,
created by a central coordinating server.
  In many scenarios, such a central coordinating server may not be available,
raising the question if this multicasting gain can still be achieved in a more
decentralized setting. In this paper, we propose an efficient caching scheme,
in which the content placement is performed in a decentralized manner. In other
words, no coordination is required for the content placement. Despite this lack
of coordination, the proposed scheme is nevertheless able to create
coded-multicasting opportunities and achieves a rate close to the optimal
centralized scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5852</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5852</id><created>2013-01-24</created><authors><author><keyname>Frolov</keyname><forenames>Alexey</forenames></author><author><keyname>Zyablov</keyname><forenames>Victor</forenames></author><author><keyname>Sidorenko</keyname><forenames>Vladimir</forenames></author><author><keyname>Fischer</keyname><forenames>Robert</forenames></author></authors><title>On a Multiple-Access in a Vector Disjunctive Channel</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures, submitted to IEEE ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of increasing the sum rate in a multiple-access system
from [1] for small number of users. We suggest an improved signal-code
construction in which in case of a small number of users we give more resources
to them. For the resulting multiple-access system a lower bound on the relative
sum rate is derived. It is shown to be very close to the maximal value of
relative sum rate in [1] even for small number of users. The bound is obtained
for the case of decoding by exhaustive search. We also suggest
reduced-complexity decoding and compare the maximal number of users in this
case and in case of decoding by exhaustive search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5871</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5871</id><created>2013-01-24</created><authors><author><keyname>Fuad</keyname><forenames>Muhammad Marwan Muhammad</forenames><affiliation>VALORIA</affiliation></author><author><keyname>Marteau</keyname><forenames>Pierre-Fran&#xe7;ois</forenames><affiliation>VALORIA</affiliation></author></authors><title>Towards a faster symbolic aggregate approximation method</title><categories>cs.DB cs.IR</categories><comments>ICSOFT 2010 - Fifth International Conference on Software and Data
  Technologies, Athens : Greece (2010)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The similarity search problem is one of the main problems in time series data
mining. Traditionally, this problem was tackled by sequentially comparing the
given query against all the time series in the database, and returning all the
time series that are within a predetermined threshold of that query. But the
large size and the high dimensionality of time series databases that are in use
nowadays make that scenario inefficient. There are many representation
techniques that aim at reducing the dimensionality of time series so that the
search can be handled faster at a lower-dimensional space level. The symbolic
aggregate approximation (SAX) is one of the most competitive methods in the
literature. In this paper we present a new method that improves the performance
of SAX by adding to it another exclusion condition that increases the exclusion
power. This method is based on using two representations of the time series:
one of SAX and the other is based on an optimal approximation of the time
series. Pre-computed distances are calculated and stored offline to be used
online to exclude a wide range of the search space using two exclusion
conditions. We conduct experiments which show that the new method is faster
than SAX.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5878</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5878</id><created>2013-01-24</created><authors><author><keyname>Bewig</keyname><forenames>Philip L.</forenames></author></authors><title>How do you know your spreadsheet is right?</title><categories>cs.SE</categories><comments>14 Pages, numerous figures, diagrams, references and code samples.
  Continuously available on the EuSpRIG website since late 2005. Permission
  granted by the author in January 2013 for Arxiv upload</comments><proxy>Grenville Croll</proxy><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Principles, Techniques and Practice of Spreadsheet Style
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5885</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5885</id><created>2013-01-24</created><authors><author><keyname>Geng</keyname><forenames>Weihua</forenames></author><author><keyname>Jacob</keyname><forenames>Ferosh</forenames></author></authors><title>A GPU-accelerated Direct-sum Boundary Integral Poisson-Boltzmann Solver</title><categories>math.NA cs.NA physics.comp-ph</categories><doi>10.1016/j.cpc.2013.01.017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a GPU-accelerated direct-sum boundary integral
method to solve the linear Poisson-Boltzmann (PB) equation. In our method, a
well-posed boundary integral formulation is used to ensure the fast convergence
of Krylov subspace based linear algebraic solver such as the GMRES. The
molecular surfaces are discretized with flat triangles and centroid
collocation. To speed up our method, we take advantage of the parallel nature
of the boundary integral formulation and parallelize the schemes within CUDA
shared memory architecture on GPU. The schemes use only $11N+6N_c$
size-of-double device memory for a biomolecule with $N$ triangular surface
elements and $N_c$ partial charges. Numerical tests of these schemes show
well-maintained accuracy and fast convergence. The GPU implementation using one
GPU card (Nvidia Tesla M2070) achieves 120-150X speed-up to the implementation
using one CPU (Intel L5640 2.27GHz). With our approach, solving PB equations on
well-discretized molecular surfaces with up to 300,000 boundary elements will
take less than about 10 minutes, hence our approach is particularly suitable
for fast electrostatics computations on small to medium biomolecules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5887</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5887</id><created>2013-01-24</created><updated>2013-12-09</updated><authors><author><keyname>Kolda</keyname><forenames>Tamara G.</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author><author><keyname>Plantenga</keyname><forenames>Todd</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author><author><keyname>Task</keyname><forenames>Christine</forenames></author></authors><title>Counting Triangles in Massive Graphs with MapReduce</title><categories>cs.SI cs.DC</categories><journal-ref>SIAM Journal on Scientific Computing, Vol. 36, No. 5, pp. S44-S77,
  October 2014</journal-ref><doi>10.1137/13090729X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs and networks are used to model interactions in a variety of contexts.
There is a growing need to quickly assess the characteristics of a graph in
order to understand its underlying structure. Some of the most useful metrics
are triangle-based and give a measure of the connectedness of mutual friends.
This is often summarized in terms of clustering coefficients, which measure the
likelihood that two neighbors of a node are themselves connected. Computing
these measures exactly for large-scale networks is prohibitively expensive in
both memory and time. However, a recent wedge sampling algorithm has proved
successful in efficiently and accurately estimating clustering coefficients. In
this paper, we describe how to implement this approach in MapReduce to deal
with massive graphs. We show results on publicly-available networks, the
largest of which is 132M nodes and 4.7B edges, as well as artificially
generated networks (using the Graph500 benchmark), the largest of which has
240M nodes and 8.5B edges. We can estimate the clustering coefficient by degree
bin (e.g., we use exponential binning) and the number of triangles per bin, as
well as the global clustering coefficient and total number of triangles, in an
average of 0.33 seconds per million edges plus overhead (approximately 225
seconds total for our configuration). The technique can also be used to study
triangle statistics such as the ratio of the highest and lowest degree, and we
highlight differences between social and non-social networks. To the best of
our knowledge, these are the largest triangle-based graph computations
published to date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5896</identifier>
 <datestamp>2013-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5896</id><created>2013-01-24</created><authors><author><keyname>Katsikarelis</keyname><forenames>Ioannis</forenames></author></authors><title>Computing bounded-width tree and branch decompositions of k-outerplanar
  graphs</title><categories>cs.DS cs.CC</categories><comments>18 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By a well known result the treewidth of k-outerplanar graphs is at most 3k-1.
This paper gives, besides a rigorous proof of this fact, an algorithmic
implementation of the proof, i.e. it is shown that, given a k-outerplanar graph
G, a tree decomposition of G of width at most 3k-1 can be found in O(kn) time
and space. Similarly, a branch decomposition of a k-outerplanar graph of width
at most 2k+1 can be also obtained in O(kn) time, the algorithm for which is
also analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5898</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5898</id><created>2013-01-24</created><authors><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>M&#xe9;zard</keyname><forenames>Marc</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Phase Diagram and Approximate Message Passing for Blind Calibration and
  Dictionary Learning</title><categories>cs.IT cond-mat.stat-mech cs.LG math.IT</categories><comments>5 pages</comments><journal-ref>Information Theory Proceedings (ISIT), 2013 IEEE International
  Symposium on, page(s) 659 - 663</journal-ref><doi>10.1109/ISIT.2013.6620308</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider dictionary learning and blind calibration for signals and
matrices created from a random ensemble. We study the mean-squared error in the
limit of large signal dimension using the replica method and unveil the
appearance of phase transitions delimiting impossible, possible-but-hard and
possible inference regions. We also introduce an approximate message passing
algorithm that asymptotically matches the theoretical performance, and show
through numerical tests that it performs very well, for the calibration
problem, for tractable system sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5912</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5912</id><created>2013-01-24</created><authors><author><keyname>Clarke</keyname><forenames>Patrick</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Resource Allocation and Interference Mitigation Techniques for
  Cooperative Multi-Antenna and Spread Spectrum Wireless Networks</title><categories>cs.IT math.IT</categories><comments>10 figures. arXiv admin note: substantial text overlap with
  arXiv:1301.0094</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter presents joint interference suppression and power allocation
algorithms for DS-CDMA and MIMO networks with multiple hops and
amplify-and-forward and decode-and-forward (DF) protocols. A scheme for joint
allocation of power levels across the relays and linear interference
suppression is proposed. We also consider another strategy for joint
interference suppression and relay selection that maximizes the diversity
available in the system. Simulations show that the proposed cross-layer
optimization algorithms obtain significant gains in capacity and performance
over existing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5914</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5914</id><created>2013-01-24</created><authors><author><keyname>Geng</keyname><forenames>Weihua</forenames></author></authors><title>Parallel Higher-order Boundary Integral Electrostatics Computation on
  Molecular Surfaces with Curved Triangulation</title><categories>math.NA cs.DC physics.bio-ph</categories><comments>arXiv admin note: text overlap with arXiv:1301.5885</comments><doi>10.1016/j.jcp.2013.01.029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a parallel higher-order boundary integral method to
solve the linear Poisson-Boltzmann (PB) equation. In our method, a well-posed
boundary integral formulation is used to ensure the fast convergence of Krylov
subspace linear solver such as GMRES. The molecular surfaces are first
discretized with flat triangles and then converted to curved triangles with the
assistance of normal information at vertices. To maintain the desired accuracy,
four-point Gauss-Radau quadratures are used on regular triangles and
sixteen-point Gauss-Legendre quadratures together with regularization
transformations are applied on singular triangles. To speed up our method, we
take advantage of the embarrassingly parallel feature of boundary integral
formulation, and parallelize the schemes with the message passing interface
(MPI) implementation. Numerical tests show significantly improved accuracy and
convergence of the proposed higher-order boundary integral Poisson-Boltzmann
(HOBI-PB) solver compared with boundary integral PB solver using often-seen
centroid collocation on flat triangles. The higher-order accuracy results
achieved by present method are important to sensitive solvation analysis of
biomolecules, particularly when accurate electrostatic surface potentials are
critical in the molecular simulation. In addition, the higher-order boundary
integral schemes presented here and their associated parallelization
potentially can be applied to solving boundary integral equations in a general
sense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5915</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5915</id><created>2013-01-24</created><authors><author><keyname>D'Oliveira</keyname><forenames>Rafael Gregorio Lucas</forenames></author><author><keyname>Firer</keyname><forenames>Marcelo</forenames></author></authors><title>The Packing Radius of a Code and Partitioning Problems: the Case for
  Poset Metrics</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Until this work, the packing radius of a poset code was only known in the
cases where the poset was a chain, a hierarchy, a union of disjoint chains of
the same size, and for some families of codes. Our objective is to approach the
general case of any poset. To do this, we will divide the problem into two
parts.
  The first part consists in finding the packing radius of a single vector. We
will show that this is equivalent to a generalization of a famous NP-hard
problem known as &quot;the partition problem&quot;. Then, we will review the main results
known about this problem giving special attention to the algorithms to solve
it. The main ingredient to these algorithms is what is known as the
differentiating method, and therefore, we will extend it to the general case.
  The second part consists in finding the vector that determines the packing
radius of the code. For this, we will show how it is sometimes possible to
compare the packing radius of two vectors without calculating them explicitly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5928</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5928</id><created>2013-01-24</created><updated>2013-01-27</updated><authors><author><keyname>Jin</keyname><forenames>Tao</forenames></author><author><keyname>Vo-Huu</keyname><forenames>Triet</forenames></author><author><keyname>Blass</keyname><forenames>Erik-Oliver</forenames></author><author><keyname>Noubir</keyname><forenames>Guevara</forenames></author></authors><title>BaPu: Efficient and Practical Bunching of Access Point Uplinks</title><categories>cs.NI</categories><comments>15 pages, 17 figures, experiment results obtained from real prototype
  system. BaPu is a sub-project under the Open Infrastructure project at the
  Wireless Networks Lab, College of Computer and Information Science in
  Northeastern University.
  (http://www.ccs.neu.edu/home/noubir/projects/openinfrastructure)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today's increasing demand for wirelessly uploading a large volume of User
Generated Content (UGC) is still significantly limited by the throttled
backhaul of residential broadband (typically between 1 and 3Mbps). We propose
BaPu, a carefully designed system with implementation for bunching WiFi access
points' backhaul to achieve a high aggregated throughput. BaPu is inspired by a
decade of networking design principles and techniques to enable efficient TCP
over wireless links and multipath. BaPu aims to achieve two major goals:1)
requires no client modification for easy incremental adoption; 2) supports not
only UDP, but also TCP traffic to greatly extend its applicability to a broad
class of popular applications such as HD streaming or large file transfer. We
prototyped BaPu with commodity hardware. Our extensive experiments shows that
despite TCP's sensitivity to typical channel factors such as high wireless
packet loss, out-of-order packets arrivals due to multipath, heterogeneous
backhaul capacity, and dynamic delays, BaPu achieves a backhaul aggregation up
to 95% of the theoretical maximum throughput for UDP and 88% for TCP. We also
empirically estimate the potential idle bandwidth that can be harnessed from
residential broadband.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5931</identifier>
 <datestamp>2013-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5931</id><created>2013-01-24</created><updated>2013-10-01</updated><authors><author><keyname>Kuhn</keyname><forenames>Nicolas</forenames></author><author><keyname>Bui</keyname><forenames>Huyen-Chi</forenames></author><author><keyname>Lacan</keyname><forenames>Jerome</forenames></author><author><keyname>Radzik</keyname><forenames>Jose</forenames></author><author><keyname>Lochin</keyname><forenames>Emmanuel</forenames></author></authors><title>On the Trade-off Between Spectrum Efficiency with Dedicated Access and
  Short End-to-End Transmission Delays with Random Access in DVB-RCS2</title><categories>cs.NI</categories><comments>ACMLCDNet 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyses the performance of TCP over random and dedicated access
methods in the context of DVB-RCS2. Random access methods introduce a lower
connection delay compared to dedicated methods. We investigate the poten- tial
to improve the performance of short flows in regards to transmission delay,
over random access methods for DVB- RCS2 that is currently under development.
Our simulation experiments show that the transmission of the first ten IP
datagrams of each TCP flow can be 500 ms faster with ran- dom access than with
dedicated access making the former of interest to carry Internet traffic. Such
methods, however, are less efficient in regards to bandwidth usage than
dedicated access mecanisms and less reliable in overloaded network conditions.
Two aspects of channel usage optimization can be distinguished: reducing the
duration of ressource utiliza- tion with random access methods, or increasing
the spec- trum efficiency with dedicated access methods. This article argues
that service providers may let low-cost users exploit the DVB-RCS2 to browse
the web by introducing different services, which choice is based on the channel
access method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5933</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5933</id><created>2013-01-24</created><updated>2013-10-31</updated><authors><author><keyname>Salsano</keyname><forenames>Stefano</forenames></author><author><keyname>Blefari-Melazzi</keyname><forenames>Nicola</forenames></author><author><keyname>Detti</keyname><forenames>Andrea</forenames></author><author><keyname>Morabito</keyname><forenames>Giacomo</forenames></author><author><keyname>Veltri</keyname><forenames>Luca</forenames></author></authors><title>Information Centric Networking over SDN and OpenFlow: Architectural
  Aspects and Experiments on the OFELIA Testbed</title><categories>cs.NI</categories><comments>submitted on January 2013 to Computer Networks (Elsevier)</comments><journal-ref>Computer Networks, Volume 57, Issue 16, 13 November 2013, Pages
  3207-3221, ISSN 1389-1286</journal-ref><doi>10.1016/j.comnet.2013.07.031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information Centric Networking (ICN) has been proposed as a new networking
paradigm in which the network provides users with content instead communication
channels between hosts. The Software Defined Networking (SDN) approach promises
to be a solution to enable the continuous evolution of networking
architectures. In this paper we propose and discuss solutions to support ICN
using SDN concepts. We focus on an ICN framework called CONET, which grounds
its roots in the CCN/NDN architecture. We face the problem in two complementary
ways. First we discuss a general and long term solution based on SDN concepts
without taking into account specific limitations of SDN standards and
equipment. Then we focus on an experiment to support ICN functionality over a
large scale SDN testbed based on OpenFlow, developed in the context of the
OFELIA European research project. The current OFELIA testbed is based on
OpenFlow 1.0 equipment from a variety of vendors, therefore we had to design
the experiment taking into account the features that are currently available on
off-the-shelf OpenFlow equipment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5937</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5937</id><created>2013-01-24</created><updated>2013-01-27</updated><authors><author><keyname>Stefani</keyname><forenames>A. G.</forenames></author><author><keyname>Huber</keyname><forenames>J. B.</forenames></author><author><keyname>Jardin</keyname><forenames>C.</forenames></author><author><keyname>Sticht</keyname><forenames>H.</forenames></author></authors><title>A Tight Lower Bound on the Mutual Information of a Binary and an
  Arbitrary Finite Random Variable in Dependence of the Variational Distance</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a numerical method is presented, which finds a lower bound for
the mutual information between a binary and an arbitrary finite random variable
with joint distributions that have a variational distance not greater than a
known value to a known joint distribution. This lower bound can be applied to
mutual information estimation with confidence intervals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5938</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5938</id><created>2013-01-24</created><updated>2015-08-13</updated><authors><author><keyname>Orsini</keyname><forenames>Chiara</forenames></author><author><keyname>Gregori</keyname><forenames>Enrico</forenames></author><author><keyname>Lenzini</keyname><forenames>Luciano</forenames></author><author><keyname>Krioukov</keyname><forenames>Dmitri</forenames></author></authors><title>Evolution of the Internet k-dense structure</title><categories>cs.SI cs.NI physics.soc-ph</categories><comments>13 pages</comments><journal-ref>IEEE/ACM Transactions on Networking (TON), Volume 22 Issue 6,
  December 2014, Pages 1769-1780</journal-ref><doi>10.1109/TNET.2013.2282756</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the Internet AS-level topology grows over time, some of its structural
properties remain unchanged. Such time- invariant properties are generally
interesting, because they tend to reflect some fundamental processes or
constraints behind Internet growth. As has been shown before, the
time-invariant structural properties of the Internet include some most basic
ones, such as the degree distribution or clustering. Here we add to this
time-invariant list a non-trivial property - k-dense decomposition. This
property is derived from a recursive form of edge multiplicity, defined as the
number of triangles that share a given edge. We show that after proper
normalization, the k- dense decomposition of the Internet has remained stable
over the last decade, even though the Internet size has approximately doubled,
and so has the k-density of its k-densest core. This core consists mostly of
content providers peering at Internet eXchange Points, and it only loosely
overlaps with the high-degree or high-rank AS core, consisting mostly of tier-1
transit providers. We thus show that high degrees and high k-densities reflect
two different Internet-specific properties of ASes (transit versus content
providers). As a consequence, even though degrees and k-densities of nodes are
correlated, the relative fluctuations are strong, and related to that, random
graphs with the same degree distribution or even degree correlations as in the
Internet, do not reproduce its k-dense decomposition. Therefore an interesting
open question is what Internet topology models or generators can fully explain
or at least reproduce the k-dense properties of the Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5942</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5942</id><created>2013-01-24</created><updated>2013-01-27</updated><authors><author><keyname>Stefani</keyname><forenames>A. G.</forenames></author><author><keyname>Huber</keyname><forenames>J. B.</forenames></author><author><keyname>Jardin</keyname><forenames>C.</forenames></author><author><keyname>Sticht</keyname><forenames>H.</forenames></author></authors><title>Confidence Intervals for the Mutual Information</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By combining a bound on the absolute value of the difference of mutual
information between two joint probablity distributions with a fixed variational
distance, and a bound on the probability of a maximal deviation in variational
distance between a true joint probability distribution and an empirical joint
probability distribution, confidence intervals for the mutual information of
two random variables with finite alphabets are established. Different from
previous results, these intervals do not need any assumptions on the
distribution and the sample size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5943</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5943</id><created>2013-01-24</created><authors><author><keyname>Te&#xf3;filo</keyname><forenames>Lu&#xed;s Filipe</forenames></author><author><keyname>Reis</keyname><forenames>Luis Paulo</forenames></author></authors><title>Identifying Player\'s Strategies in No Limit Texas Hold\'em Poker
  through the Analysis of Individual Moves</title><categories>cs.AI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of competitive artificial Poker playing agents has proven to
be a challenge, because agents must deal with unreliable information and
deception which make it essential to model the opponents in order to achieve
good results. This paper presents a methodology to develop opponent modeling
techniques for Poker agents. The approach is based on applying clustering
algorithms to a Poker game database in order to identify player types based on
their actions. First, common game moves were identified by clustering all
players\' moves. Then, player types were defined by calculating the frequency
with which the players perform each type of movement. With the given dataset, 7
different types of players were identified with each one having at least one
tactic that characterizes him. The identification of player types may improve
the overall performance of Poker agents, because it helps the agents to predict
the opponent\'s moves, by associating each opponent to a distinct cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5946</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5946</id><created>2013-01-24</created><authors><author><keyname>Te&#xf3;filo</keyname><forenames>Lu&#xed;s Filipe</forenames></author><author><keyname>Reis</keyname><forenames>Lu&#xed;s Paulo</forenames></author><author><keyname>Cardoso</keyname><forenames>Henrique Lopes</forenames></author><author><keyname>F&#xe9;lix</keyname><forenames>Dinis</forenames></author><author><keyname>S&#xea;ca</keyname><forenames>Rui</forenames></author><author><keyname>Ferreira</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Mendes</keyname><forenames>Pedro</forenames></author><author><keyname>Cruz</keyname><forenames>Nuno</forenames></author><author><keyname>Pereira</keyname><forenames>Vitor</forenames></author><author><keyname>Passos</keyname><forenames>Nuno</forenames></author></authors><title>Computer Poker Research at LIACC</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer Poker's unique characteristics present a well-suited challenge for
research in artificial intelligence. For that reason, and due to the Poker's
market increase in popularity in Portugal since 2008, several members of LIACC
have researched in this field. Several works were published as papers and
master theses and more recently a member of LIACC engaged on a research in this
area as a Ph.D. thesis in order to develop a more extensive and in-depth work.
This paper describes the existing research in LIACC about Computer Poker, with
special emphasis on the completed master's theses and plans for future work.
This paper means to present a summary of the lab's work to the research
community in order to encourage the exchange of ideas with other labs /
individuals. LIACC hopes this will improve research in this area so as to reach
the goal of creating an agent that surpasses the best human players.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5952</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5952</id><created>2013-01-24</created><updated>2014-09-27</updated><authors><author><keyname>Xia</keyname><forenames>Shu-Tao</forenames></author><author><keyname>Liu</keyname><forenames>Xin-Ji</forenames></author><author><keyname>Jiang</keyname><forenames>Yong</forenames></author><author><keyname>Zheng</keyname><forenames>Hai-Tao</forenames></author></authors><title>Deterministic Constructions of Binary Measurement Matrices from Finite
  Geometry</title><categories>cs.IT math.IT</categories><comments>12 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deterministic constructions of measurement matrices in compressed sensing
(CS) are considered in this paper. The constructions are inspired by the recent
discovery of Dimakis, Smarandache and Vontobel which says that parity-check
matrices of good low-density parity-check (LDPC) codes can be used as
{provably} good measurement matrices for compressed sensing under
$\ell_1$-minimization. The performance of the proposed binary measurement
matrices is mainly theoretically analyzed with the help of the analyzing
methods and results from (finite geometry) LDPC codes. Particularly, several
lower bounds of the spark (i.e., the smallest number of columns that are
linearly dependent, which totally characterizes the recovery performance of
$\ell_0$-minimization) of general binary matrices and finite geometry matrices
are obtained and they improve the previously known results in most cases.
Simulation results show that the proposed matrices perform comparably to,
sometimes even better than, the corresponding Gaussian random matrices.
Moreover, the proposed matrices are sparse, binary, and most of them have
cyclic or quasi-cyclic structure, which will make the hardware realization
convenient and easy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5953</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5953</id><created>2013-01-24</created><authors><author><keyname>Broersma</keyname><forenames>Hajo</forenames></author><author><keyname>Fiala</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Kaiser</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Paulusma</keyname><forenames>Dani&#xeb;l</forenames></author><author><keyname>Proskurowski</keyname><forenames>Andrzej</forenames></author></authors><title>Linear-Time Algorithms for Scattering Number and Hamilton-Connectivity
  of Interval Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hung and Chang showed that for all k&gt;=1 an interval graph has a path cover of
size at most k if and only if its scattering number is at most k. They also
showed that an interval graph has a Hamilton cycle if and only if its
scattering number is at most 0. We complete this characterization by proving
that for all k&lt;=-1 an interval graph is -(k+1)-Hamilton-connected if and only
if its scattering number is at most k. We also give an O(m+n) time algorithm
for computing the scattering number of an interval graph with n vertices an m
edges, which improves the O(n^4) time bound of Kratsch, Kloks and M\&quot;uller. As
a consequence of our two results the maximum k for which an interval graph is
k-Hamilton-connected can be computed in O(m+n) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5954</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5954</id><created>2013-01-24</created><authors><author><keyname>Liu</keyname><forenames>Yuan</forenames></author><author><keyname>Mo</keyname><forenames>Jianhua</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author></authors><title>QoS-Aware Transmission Policies for OFDM Bidirectional
  Decode-and-Forward Relaying</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-way relaying can considerably improve spectral efficiency in
relay-assisted bidirectional communications. However, the benefits and flexible
structure of orthogonal frequency division multiplexing (OFDM)-based two-way
decode-and-forward (DF) relay systems is much less exploited. Moreover, most of
existing works have not considered quality-of-service (QoS) provisioning for
two-way relaying. In this paper, we consider the OFDM-based bidirectional
transmission where a pair of users exchange information with or without the
assistance of a single DF relay. Each user can communicate with the other via
three transmission modes: direct transmission, one-way relaying, and two-way
relaying. We jointly optimize the transmission policies, including power
allocation, transmission mode selection, and subcarrier assignment for
maximizing the weighted sum rates of the two users with diverse
quality-of-service (QoS) guarantees. We formulate the joint optimization
problem as a mixed integer programming problem. By using the dual method, we
efficiently solve the problem in an asymptotically optimal manner. Moreover, we
derive the capacity region of two-way DF relaying in parallel channels.
Simulation results show that the proposed resource-allocation scheme can
substantially improve system performance compared with the conventional
schemes. A number of interesting insights are also provided via comprehensive
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5961</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5961</id><created>2013-01-24</created><authors><author><keyname>Silberstein</keyname><forenames>Natalia</forenames></author><author><keyname>Trautmann</keyname><forenames>Anna-Lena</forenames></author></authors><title>New Lower Bounds for Constant Dimension Codes</title><categories>cs.IT math.IT</categories><doi>10.1109/ISIT.2013.6620279</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides new constructive lower bounds for constant dimension
codes, using different techniques such as Ferrers diagram rank metric codes and
pending blocks. Constructions for two families of parameters of constant
dimension codes are presented. The examples of codes obtained by these
constructions are the largest known constant dimension codes for the given
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5964</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5964</id><created>2013-01-24</created><authors><author><keyname>Javdani</keyname><forenames>Taghi</forenames></author><author><keyname>Zulzalil</keyname><forenames>Hazura</forenames></author><author><keyname>Ghani</keyname><forenames>Abdul Azim Abd</forenames></author><author><keyname>Sultan</keyname><forenames>Abu Bakar Md</forenames></author><author><keyname>Parizi</keyname><forenames>Reza Meimandi</forenames></author></authors><title>On the Current Measurement Practices in Agile Software Development</title><categories>cs.SE</categories><comments>7 pages</comments><journal-ref>Taghi Javdani , Hazura Zulzalil, Abd. Azim Abd. Ghani, Abubakar
  Md. Sultan, On the current measurement practices in agile software
  development, International Journal of Computer Science Issues, 2012, Vol. 9,
  Issue 4, No. 3, pp. 127-133</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agile software development (ASD) methods were introduced as a reaction to
traditional software development methods. Principles of these methods are
different from traditional methods and so there are some different processes
and activities in agile methods comparing to traditional methods. Thus ASD
methods require different measurement practices comparing to traditional
methods. Agile teams often do their projects in the simplest and most effective
way so, measurement practices in agile methods are more important than
traditional methods, because lack of appropriate and effective measurement
practices, will increase risk of project. The aims of this paper are
investigation on current measurement practices in ASD methods, collecting them
together in one study and also reviewing agile version of Common Software
Measurement International Consortium (COSMIC) publication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5969</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5969</id><created>2013-01-25</created><updated>2013-03-18</updated><authors><author><keyname>Erickson</keyname><forenames>Alejandro</forenames></author></authors><title>Tatami Maker: A combinatorially rich mechanical game board</title><categories>math.CO cs.CG</categories><comments>8 pages, Accepted at Bridges Conference 2013</comments><msc-class>05B50, 68R05, 97K20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Japanese tatami mats are often arranged so that no four mats meet. This local
restriction imposes a rich combinatorial structure when applied to
monomino-domino coverings of rectilinear grids. We describe a modular,
mechanical game board, prototyped with a desktop 3D printer, that enforces this
restriction, and transforms tatami pen-and- paper puzzles into interactive
sculptures. We review some recent mathematical discoveries on tatami coverings
and present five new combinatorial games implemented on the game board.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5973</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5973</id><created>2013-01-25</created><authors><author><keyname>Nabaee</keyname><forenames>Mahdy</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author></authors><title>Non-Adaptive Distributed Compression in Networks</title><categories>cs.IT math.IT</categories><comments>Submitted for 2013 IEEE International Symposium on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discuss non-adaptive distributed compression of inter-node
correlated real-valued messages. To do so, we discuss the performance of
conventional packet forwarding via routing, in terms of the total network load
versus the resulting quality of service (distortion level). As a better
alternative for packet forwarding, we briefly describe our previously proposed
one-step Quantized Network Coding (QNC), and make motivating arguments on its
advantage when the appropriate marginal rates for distributed source coding are
not available at the encoder source nodes. We also derive analytic guarantees
on the resulting distortion of our one-step QNC scenario. Finally, we conclude
the paper by providing a mathematical comparison between the total network
loads of one-step QNC and conventional packet forwarding, showing a significant
reduction in the case of one-step QNC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5979</identifier>
 <datestamp>2013-08-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5979</id><created>2013-01-25</created><updated>2013-07-04</updated><authors><author><keyname>Sun</keyname><forenames>Lijun</forenames></author><author><keyname>Axhausen</keyname><forenames>Kay W.</forenames></author><author><keyname>Lee</keyname><forenames>Der-Horng</forenames></author><author><keyname>Huang</keyname><forenames>Xianfeng</forenames></author></authors><title>Understanding metropolitan patterns of daily encounters</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>7 pages, 3 figures</comments><doi>10.1073/pnas.1306440110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding of the mechanisms driving our daily face-to-face encounters is
still limited; the field lacks large-scale datasets describing both individual
behaviors and their collective interactions. However, here, with the help of
travel smart card data, we uncover such encounter mechanisms and structures by
constructing a time-resolved in-vehicle social encounter network on public
buses in a city (about 5 million residents). This is the first time that such a
large network of encounters has been identified and analyzed. Using a
population scale dataset, we find physical encounters display reproducible
temporal patterns, indicating that repeated encounters are regular and
identical. On an individual scale, we find that collective regularities
dominate distinct encounters' bounded nature. An individual's encounter
capability is rooted in his/her daily behavioral regularity, explaining the
emergence of &quot;familiar strangers&quot; in daily life. Strikingly, we find
individuals with repeated encounters are not grouped into small communities,
but become strongly connected over time, resulting in a large, but
imperceptible, small-world contact network or &quot;structure of co-presence&quot; across
the whole metropolitan area. Revealing the encounter pattern and identifying
this large-scale contact network are crucial to understanding the dynamics in
patterns of social acquaintances, collective human behaviors, and --
particularly -- disclosing the impact of human behavior on various
diffusion/spreading processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5986</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5986</id><created>2013-01-25</created><updated>2013-05-16</updated><authors><author><keyname>Edemskiy</keyname><forenames>Vladimir</forenames></author><author><keyname>Ivanov</keyname><forenames>Andrew</forenames></author></authors><title>Autocorrelation and Linear Complexity of Quaternary Sequences of Period
  2p Based on Cyclotomic Classes of Order Four</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the linear complexity and the autocorrelation properties of new
quaternary cyclotomic sequences of period 2p. The sequences are constructed via
the cyclotomic classes of order four.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.5993</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.5993</id><created>2013-01-25</created><authors><author><keyname>Safaei</keyname><forenames>Farshad</forenames></author><author><keyname>ValadBeigi</keyname><forenames>Majed</forenames></author></authors><title>A Probabilistic Approach to Analysis of Reliability in n-D Meshes with
  Interconnect Router Failures</title><categories>cs.DC cs.NI</categories><comments>11 pages, 1 figure, 2 tables</comments><journal-ref>International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.3, No.4, July 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The routing algorithms for parallel computers, on-chip networks, multi-core
processors, and multiprocessors system-on-chip (MP-SoCs) exhibit router
failures must be able to handle interconnect router failures that render a
symmetrical mesh non-symmetrically. When developing a routing methodology, the
time complexity of calculation should be minimal, and thus complicated routing
strategies to introduce profitable paths may not be appropriate. Several
reports have been released in the literature on using the concept of fault
rings to provide detour paths to messages blocked by faults and to route
messages around the fault regions. In order to analyze the performance of such
algorithms, it is required to investigate the characteristics of fault rings.
In this paper, we introduce a novel performance index of network reliability
presenting the probability of message facing fault rings, and evaluating the
performance-related reliability of adaptive routing schemes in n-D mesh-based
interconnection networks with a variety of common cause fault patterns.
Sufficient simulation results of Monte-Carlo method are conducted to
demonstrate the correctness of the proposed analytical model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6007</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6007</id><created>2013-01-25</created><authors><author><keyname>Kageyama</keyname><forenames>Akira</forenames></author><author><keyname>Ohno</keyname><forenames>Nobuaki</forenames></author></authors><title>Immersive VR Visualizations by VFIVE. Part 1: Development</title><categories>cs.GR physics.comp-ph</categories><comments>9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have been developing a visualization application for CAVE-type virtual
reality (VR) systems for more than a decade. This application, VFIVE, is
currently used in several CAVE systems in Japan for routine visualizations. It
is also used as a base system of further developments of advanced
visualizations. The development of VFIVE is summarized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6008</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6008</id><created>2013-01-25</created><authors><author><keyname>Kageyama</keyname><forenames>Akira</forenames></author><author><keyname>Ohno</keyname><forenames>Nobuaki</forenames></author><author><keyname>Kawahara</keyname><forenames>Shintaro</forenames></author><author><keyname>Kashiyama</keyname><forenames>Kazuo</forenames></author><author><keyname>Ohtani</keyname><forenames>Hiroaki</forenames></author></authors><title>Immersive VR Visualizations by VFIVE. Part 2: Applications</title><categories>cs.GR physics.comp-ph</categories><comments>11 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  VFIVE is a scientific visualization application for CAVE-type immersive
virtual reality systems. The source codes are freely available. VFIVE is used
as a research tool in various VR systems. It also lays the groundwork for
developments of new visualization software for CAVEs. In this paper, we pick up
five CAVE systems in four different institutions in Japan. Applications of
VFIVE in each CAVE system are summarized. Special emphases will be placed on
scientific and technical achievements made possible by VFIVE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6011</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6011</id><created>2013-01-25</created><authors><author><keyname>Tripathy</keyname><forenames>B. K.</forenames></author><author><keyname>Acharjya</keyname><forenames>D. P.</forenames></author><author><keyname>Cynthya</keyname><forenames>V.</forenames></author></authors><title>A Framework for Intelligent Medical Diagnosis using Rough Set with
  Formal Concept Analysis</title><categories>cs.AI</categories><comments>22 pages</comments><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), Vol.2, No.2, April 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Medical diagnosis process vary in the degree to which they attempt to deal
with different complicating aspects of diagnosis such as relative importance of
symptoms, varied symptom pattern and the relation between diseases them selves.
Based on decision theory, in the past many mathematical models such as crisp
set, probability distribution, fuzzy set, intuitionistic fuzzy set were
developed to deal with complicating aspects of diagnosis. But, many such models
are failed to include important aspects of the expert decisions. Therefore, an
effort has been made to process inconsistencies in data being considered by
Pawlak with the introduction of rough set theory. Though rough set has major
advantages over the other methods, but it generates too many rules that create
many difficulties while taking decisions. Therefore, it is essential to
minimize the decision rules. In this paper, we use two processes such as pre
process and post process to mine suitable rules and to explore the relationship
among the attributes. In pre process we use rough set theory to mine suitable
rules, whereas in post process we use formal concept analysis from these
suitable rules to explore better knowledge and most important factors affecting
the decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6021</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6021</id><created>2013-01-25</created><authors><author><keyname>De Feo</keyname><forenames>Luca</forenames></author><author><keyname>Doliskani</keyname><forenames>Javad</forenames></author><author><keyname>Schost</keyname><forenames>&#xc9;ric</forenames></author></authors><title>Fast algorithms for ell-adic towers over finite fields</title><categories>cs.SC math.NT</categories><acm-class>F.2.1; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by previous work of Shoup, Lenstra-De Smit and Couveignes-Lercier,
we give fast algorithms to compute in (the first levels of) the ell-adic
closure of a finite field. In many cases, our algorithms have quasi-linear
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6022</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6022</id><created>2013-01-25</created><authors><author><keyname>Romero-Garces</keyname><forenames>A.</forenames></author><author><keyname>Manso</keyname><forenames>L. J.</forenames></author><author><keyname>Gutierez</keyname><forenames>Marco A.</forenames></author><author><keyname>Cintas</keyname><forenames>R.</forenames></author><author><keyname>Bustos</keyname><forenames>P.</forenames></author></authors><title>Improving the lifecycle of robotics components using Domain-Specific
  Languages</title><categories>cs.RO cs.SE</categories><comments>Presented at DSLRob 2011 (arXiv:cs/1212.3308)</comments><report-no>DSLRob/2011/06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is currently a large amount of robotics software using the
component-oriented programming paradigm. However, the rapid growth in number
and complexity of components may compromise the scalability and the whole
lifecycle of robotics software systems. Model-Driven Engineering can be used to
mitigate these problems. This paper describes how using Domain-Specific
Languages to generate and describe critical parts of robotic systems helps
developers to perform component managerial tasks such as component creation,
modification, monitoring and deployment. Four different DSLs are proposed in
this paper: i) CDSL for specifying the structure of the components, ii) IDSL
for the description of their interfaces, iii) DDSL for describing the
deployment process of component networks and iv) PDSL to define and configure
component parameters. Their benefits have been demonstrated after their
implementation in RoboComp, a general-purpose and component-based robotics
framework. Examples of the usage of these DSLs are shown along with experiments
that demonstrate the benefits they bring to the lifecycle of the components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6039</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6039</id><created>2013-01-25</created><updated>2014-03-07</updated><authors><author><keyname>Heras</keyname><forenames>J&#xf3;nathan</forenames></author><author><keyname>Komendantskaya</keyname><forenames>Ekaterina</forenames></author></authors><title>Recycling Proof Patterns in Coq: Case Studies</title><categories>cs.AI cs.LG cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Development of Interactive Theorem Provers has led to the creation of big
libraries and varied infrastructures for formal proofs. However, despite (or
perhaps due to) their sophistication, the re-use of libraries by non-experts or
across domains is a challenge. In this paper, we provide detailed case studies
and evaluate the machine-learning tool ML4PG built to interactively data-mine
the electronic libraries of proofs, and to provide user guidance on the basis
of proof patterns found in the existing libraries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6058</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6058</id><created>2013-01-25</created><authors><author><keyname>Moroshko</keyname><forenames>Edward</forenames></author><author><keyname>Crammer</keyname><forenames>Koby</forenames></author></authors><title>Weighted Last-Step Min-Max Algorithm with Improved Sub-Logarithmic
  Regret</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In online learning the performance of an algorithm is typically compared to
the performance of a fixed function from some class, with a quantity called
regret. Forster proposed a last-step min-max algorithm which was somewhat
simpler than the algorithm of Vovk, yet with the same regret. In fact the
algorithm he analyzed assumed that the choices of the adversary are bounded,
yielding artificially only the two extreme cases. We fix this problem by
weighing the examples in such a way that the min-max problem will be well
defined, and provide analysis with logarithmic regret that may have better
multiplicative factor than both bounds of Forster and Vovk. We also derive a
new bound that may be sub-logarithmic, as a recent bound of Orabona et.al, but
may have better multiplicative factor. Finally, we analyze the algorithm in a
weak-type of non-stationary setting, and show a bound that is sub-linear if the
non-stationarity is sub-linear as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6063</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6063</id><created>2013-01-25</created><updated>2013-10-13</updated><authors><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Noetzel</keyname><forenames>Janis</forenames></author></authors><title>Arbitrarily Small Amounts of Correlation for Arbitrarily Varying Quantum
  Channels</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>25 pages, no figures. Considerably revised version, accepted for
  publication in J. Math. Phys</comments><doi>10.1063/1.4825159</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As our main result we show that, in order to achieve the randomness assisted
message - and entanglement transmission capacities of a finite arbitrarily
varying quantum channel it is not necessary that sender and receiver share
(asymptotically perfect) common randomness. Rather, it is sufficient that they
each have access to an unlimited amount of uses of one part of a correlated
bipartite source. This access might be restricted to an arbitrary small
(nonzero) fraction per channel use, without changing the main result. We
investigate the notion of common randomness. It turns out that this is a very
costly resource - generically, it cannot be obtained just by local processing
of a bipartite source. This result underlines the importance of our main
result. Also, the asymptotic equivalence of the maximal- and average error
criterion for classical message transmission over finite arbitrarily varying
quantum channels is proven. At last, we prove a simplifed symmetrizability
condition for finite arbitrarily varying quantum channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6111</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6111</id><created>2013-01-25</created><updated>2013-12-26</updated><authors><author><keyname>Kumar</keyname><forenames>Santhosh</forenames></author><author><keyname>Young</keyname><forenames>Andrew J.</forenames></author><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author><author><keyname>Pfister</keyname><forenames>Henry D.</forenames></author></authors><title>A Proof of Threshold Saturation for Spatially-Coupled LDPC Codes on BMS
  Channels</title><categories>cs.IT math.IT</categories><comments>(v1) In proceedings of Allerton 2012; Corrected a typo in equation
  (5). (v2) This update corrects an error in Definition 13 and typos in
  equations (7) and (8). An extended version of this article with complete
  proofs is at arXiv:1309.7543</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-density parity-check (LDPC) convolutional codes have been shown to
exhibit excellent performance under low-complexity belief-propagation decoding
[1], [2]. This phenomenon is now termed threshold saturation via spatial
coupling. The underlying principle behind this appears to be very general and
spatially-coupled (SC) codes have been successfully applied in numerous areas.
Recently, SC regular LDPC codes have been proven to achieve capacity
universally, over the class of binary memoryless symmetric (BMS) channels,
under belief-propagation decoding [3], [4].
  In [5], [6], potential functions are used to prove that the BP threshold of
SC irregular LDPC ensembles saturates, for the binary erasure channel, to the
conjectured MAP threshold (known as the Maxwell threshold) of the underlying
irregular ensembles. In this paper, that proof technique is generalized to BMS
channels, thereby extending some results of [4] to irregular LDPC ensembles. We
also believe that this approach can be expanded to cover a wide class of
graphical models whose message-passing rules are associated with a Bethe free
energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6117</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6117</id><created>2013-01-25</created><authors><author><keyname>Limburg</keyname><forenames>Steve</forenames></author><author><keyname>Grant</keyname><forenames>David</forenames></author><author><keyname>Varanasi</keyname><forenames>Mahesh K.</forenames></author></authors><title>Higher genus universally decodable matrices (UDMG)</title><categories>cs.IT math.IT</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of Universally Decodable Matrices of Genus g (UDMG),
which for g=0 reduces to the notion of Universally Decodable Matrices (UDM)
introduced in [8]. A UDMG is a set of L matrices over a finite field, each with
K rows, and a linear independence condition satisfied by collections of K+g
columns formed from the initial segments of the matrices. We consider the
mathematical structure of UDMGs and their relation to linear vector codes. We
then give a construction of UDMG based on curves of genus g over the finite
field, which is a natural generalization of the UDM constructed in [8]. We
provide upper (and constructable lower) bounds for L in terms of K, q, g, and
the number of columns of the matrices. We will show there is a fundamental
trade off (Theorem 5.4) between L and g, akin to the Singleton bound for the
minimal Hamming distance of linear vector codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6118</identifier>
 <datestamp>2014-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6118</id><created>2013-01-25</created><updated>2014-06-20</updated><authors><author><keyname>Fitzsimmons</keyname><forenames>Zack</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author></authors><title>X THEN X: Manipulation of Same-System Runoff Elections</title><categories>cs.GT cs.CC cs.MA</categories><acm-class>I.2.11; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Do runoff elections, using the same voting rule as the initial election but
just on the winning candidates, increase or decrease the complexity of
manipulation? Does allowing revoting in the runoff increase or decrease the
complexity relative to just having a runoff without revoting? For both weighted
and unweighted voting, we show that even for election systems with simple
winner problems the complexity of manipulation, manipulation with runoffs, and
manipulation with revoting runoffs are independent, in the abstract. On the
other hand, for some important, well-known election systems we determine what
holds for each of these cases. For no such systems do we find runoffs lowering
complexity, and for some we find that runoffs raise complexity. Ours is the
first paper to show that for natural, unweighted election systems, runoffs can
increase the manipulation complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6120</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6120</id><created>2013-01-25</created><updated>2014-02-06</updated><authors><author><keyname>Pastore</keyname><forenames>Adriano</forenames></author><author><keyname>Koch</keyname><forenames>Tobias</forenames></author><author><keyname>Fonollosa</keyname><forenames>Javier Rodr&#xed;guez</forenames></author></authors><title>A Rate-Splitting Approach to Fading Channels with Imperfect
  Channel-State Information</title><categories>cs.IT math.IT</categories><comments>28 pages, 8 figures, submitted to IEEE Transactions on Information
  Theory. Revised according to first round of reviews</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As shown by M\'edard, the capacity of fading channels with imperfect
channel-state information (CSI) can be lower-bounded by assuming a Gaussian
channel input $X$ with power $P$ and by upper-bounding the conditional entropy
$h(X|Y,\hat{H})$ by the entropy of a Gaussian random variable with variance
equal to the linear minimum mean-square error in estimating $X$ from
$(Y,\hat{H})$. We demonstrate that, using a rate-splitting approach, this lower
bound can be sharpened: by expressing the Gaussian input $X$ as the sum of two
independent Gaussian variables $X_1$ and $X_2$ and by applying M\'edard's lower
bound first to bound the mutual information between $X_1$ and $Y$ while
treating $X_2$ as noise, and by applying it a second time to the mutual
information between $X_2$ and $Y$ while assuming $X_1$ to be known, we obtain a
capacity lower bound that is strictly larger than M\'edard's lower bound. We
then generalize this approach to an arbitrary number $L$ of layers, where $X$
is expressed as the sum of $L$ independent Gaussian random variables of
respective variances $P_{\ell}$, $\ell = 1,\dotsc,L$ summing up to $P$. Among
all such rate-splitting bounds, we determine the supremum over power
allocations $P_\ell$ and total number of layers $L$. This supremum is achieved
for $L\to\infty$ and gives rise to an analytically expressible capacity lower
bound. For Gaussian fading, this novel bound is shown to converge to the
Gaussian-input mutual information as the signal-to-noise ratio (SNR) grows,
provided that the variance of the channel estimation error $H-\hat{H}$ tends to
zero as the SNR tends to infinity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6125</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6125</id><created>2013-01-25</created><authors><author><keyname>Leistedt</keyname><forenames>Boris</forenames></author><author><keyname>McEwen</keyname><forenames>Jason D.</forenames></author></authors><title>Flaglets: Exact Wavelets on the Ball</title><categories>cs.IT astro-ph.IM math.IT</categories><comments>1 page, 1 figure, Proceedings of International BASP Frontiers
  Workshop 2013. Codes are publicly available at http://www.s2let.org and
  http://www.flaglets.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We summarise the construction of exact axisymmetric scale-discretised
wavelets on the sphere and on the ball. The wavelet transform on the ball
relies on a novel 3D harmonic transform called the Fourier-Laguerre transform
which combines the spherical harmonic transform with damped Laguerre
polynomials on the radial half-line. The resulting wavelets, called flaglets,
extract scale-dependent, spatially localised features in three-dimensions while
treating the tangential and radial structures separately. Both the
Fourier-Laguerre and the flaglet transforms are theoretically exact thanks to a
novel sampling theorem on the ball. Our implementation of these methods is
publicly available and achieves floating-point accuracy when applied to
band-limited signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6127</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6127</id><created>2013-01-25</created><updated>2014-06-28</updated><authors><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Hermelin</keyname><forenames>Danny</forenames></author><author><keyname>Landau</keyname><forenames>Gad M.</forenames></author><author><keyname>Weimann</keyname><forenames>Oren</forenames></author></authors><title>Binary Jumbled Pattern Matching on Trees and Tree-Like Structures</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary jumbled pattern matching asks to preprocess a binary string $S$ in
order to answer queries $(i,j)$ which ask for a substring of $S$ that is of
length $i$ and has exactly $j$ 1-bits. This problem naturally generalizes to
vertex-labeled trees and graphs by replacing &quot;substring&quot; with &quot;connected
subgraph&quot;. In this paper, we give an $O(n^2 / \log^2 n)$-time solution for
trees, matching the currently best bound for (the simpler problem of) strings.
We also give an $\Oh{g^{2 / 3} n^{4 / 3}/(\log n)^{4/3}}$-time solution for
strings that are compressed by a grammar of size $g$. This solution improves
the known bounds when the string is compressible under many popular compression
schemes. Finally, we prove that the problem is fixed-parameter tractable with
respect to the treewidth $w$ of the graph, thus improving the previous best
$n^{O(w)}$ algorithm [ICALP'07].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6150</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6150</id><created>2013-01-25</created><authors><author><keyname>Goela</keyname><forenames>Naveen</forenames></author><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author></authors><title>Polar Codes For Broadcast Channels</title><categories>cs.IT math.IT</categories><comments>25 pages, double-column, 7 figures</comments><journal-ref>IEEE Transactions on Information Theory, vol. 61, no. 2, pp.
  758-782, Feb. 2015</journal-ref><doi>10.1109/TIT.2014.2378172</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polar codes are introduced for discrete memoryless broadcast channels. For
$m$-user deterministic broadcast channels, polarization is applied to map
uniformly random message bits from $m$ independent messages to one codeword
while satisfying broadcast constraints. The polarization-based codes achieve
rates on the boundary of the private-message capacity region. For two-user
noisy broadcast channels, polar implementations are presented for two
information-theoretic schemes: i) Cover's superposition codes; ii) Marton's
codes. Due to the structure of polarization, constraints on the auxiliary and
channel-input distributions are identified to ensure proper alignment of
polarization indices in the multi-user setting. The codes achieve rates on the
capacity boundary of a few classes of broadcast channels (e.g., binary-input
stochastically degraded). The complexity of encoding and decoding is $O(n*log
n)$ where $n$ is the block length. In addition, polar code sequences obtain a
stretched-exponential decay of $O(2^{-n^{\beta}})$ of the average block error
probability where $0 &lt; \beta &lt; 0.5$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6157</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6157</id><created>2013-01-25</created><updated>2013-03-11</updated><authors><author><keyname>Sasidharan</keyname><forenames>Birenjith</forenames></author><author><keyname>Kumar</keyname><forenames>P. Vijay</forenames></author></authors><title>High-Rate Regenerating Codes Through Layering</title><categories>cs.IT math.IT</categories><comments>20 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide explicit constructions for a class of exact-repair
regenerating codes that possess a layered structure. These regenerating codes
correspond to interior points on the storage-repair-bandwidth tradeoff, and
compare very well in comparison to scheme that employs space-sharing between
MSR and MBR codes. For the parameter set $(n,k,d=k)$ with $n &lt; 2k-1$, we
construct a class of codes with an auxiliary parameter $w$, referred to as
canonical codes. With $w$ in the range $n-k &lt; w &lt; k$, these codes operate in
the region between the MSR point and the MBR point, and perform significantly
better than the space-sharing line. They only require a field size greater than
$w+n-k$. For the case of $(n,n-1,n-1)$, canonical codes can also be shown to
achieve an interior point on the line-segment joining the MSR point and the
next point of slope-discontinuity on the storage-repair-bandwidth tradeoff.
Thus we establish the existence of exact-repair codes on a point other than the
MSR and the MBR point on the storage-repair-bandwidth tradeoff. We also
construct layered regenerating codes for general parameter set $(n,k&lt;d,k)$,
which we refer to as non-canonical codes. These codes also perform
significantly better than the space-sharing line, though they require a
significantly higher field size. All the codes constructed in this paper are
high-rate, can repair multiple node-failures and do not require any computation
at the helper nodes. We also construct optimal codes with locality in which the
local codes are layered regenerating codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6176</identifier>
 <datestamp>2013-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6176</id><created>2013-01-25</created><authors><author><keyname>Laarhoven</keyname><forenames>Thijs</forenames></author><author><keyname>Mosca</keyname><forenames>Michele</forenames></author><author><keyname>van de Pol</keyname><forenames>Joop</forenames></author></authors><title>Solving the Shortest Vector Problem in Lattices Faster Using Quantum
  Search</title><categories>cs.CR quant-ph</categories><comments>19 pages</comments><journal-ref>5th International Workshop on Post-Quantum Cryptography
  (PQCrypto), pp. 83-101, 2013</journal-ref><doi>10.1007/978-3-642-38616-9_6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By applying Grover's quantum search algorithm to the lattice algorithms of
Micciancio and Voulgaris, Nguyen and Vidick, Wang et al., and Pujol and
Stehl\'{e}, we obtain improved asymptotic quantum results for solving the
shortest vector problem. With quantum computers we can provably find a shortest
vector in time $2^{1.799n + o(n)}$, improving upon the classical time
complexity of $2^{2.465n + o(n)}$ of Pujol and Stehl\'{e} and the $2^{2n +
o(n)}$ of Micciancio and Voulgaris, while heuristically we expect to find a
shortest vector in time $2^{0.312n + o(n)}$, improving upon the classical time
complexity of $2^{0.384n + o(n)}$ of Wang et al. These quantum complexities
will be an important guide for the selection of parameters for post-quantum
cryptosystems based on the hardness of the shortest vector problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6179</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6179</id><created>2013-01-25</created><authors><author><keyname>Solnushkin</keyname><forenames>Konstantin S.</forenames></author></authors><title>Automated Design of Two-Layer Fat-Tree Networks</title><categories>cs.DC cs.NI</categories><comments>10 pages, 6 figures</comments><acm-class>C.2.1; K.6.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an algorithm to automatically design two-level fat-tree
networks, such as ones widely used in large-scale data centres and cluster
supercomputers. The two levels may each use a different type of switches from
design database to achieve an optimal network structure. Links between layers
can run in bundles to simplify cabling. Several sample network designs are
examined and their technical and economic characteristics are discussed.
  The characteristic feature of our approach is that real life equipment prices
and values of technical characteristics are used. This allows to select an
optimal combination of hardware to build the network (including semi-populated
configurations of modular switches) and accurately estimate the cost of this
network. We also show how technical characteristics of the network can be
derived from its per-port metrics and suggest heuristics for equipment
placement.
  The algorithm is useful as a part of a bigger design procedure that selects
optimal hardware of cluster supercomputer as a whole. Therefore the article is
focused on the use of fat-trees for high-performance computing, although the
results are valid for any type of data centres.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6180</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6180</id><created>2013-01-25</created><authors><author><keyname>Solnushkin</keyname><forenames>Konstantin S.</forenames></author></authors><title>Automated Design of Torus Networks</title><categories>cs.DC cs.NI</categories><comments>6 pages, 2 figures, 4 tables</comments><acm-class>C.2.1; K.6.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an algorithm to automatically design networks with torus
topologies, such as ones widely used in large-scale supercomputers. The
characteristic feature of our approach is that real life equipment prices and
values of technical characteristics are used. As a result, we also have the
opportunity to compare costs of torus and fat-tree networks.
  The algorithm is useful as a part of a bigger design procedure that selects
optimal hardware of cluster supercomputer as a whole.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6190</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6190</id><created>2013-01-25</created><authors><author><keyname>Trillingsgaard</keyname><forenames>Kasper Fl&#x153;</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author><author><keyname>Larsen</keyname><forenames>Torben</forenames></author></authors><title>Blahut-Arimoto Algorithm and Code Design for Action-Dependent Source
  Coding Problems</title><categories>cs.IT math.IT</categories><comments>Extended version of a paper submitted to ISIT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The source coding problem with action-dependent side information at the
decoder has recently been introduced to model data acquisition in
resource-constrained systems. In this paper, an efficient algorithm for
numerical computation of the rate-distortion-cost function for this problem is
proposed, and a convergence proof is provided. Moreover, a two-stage code
design based on multiplexing is put forth, whereby the first stage encodes the
actions and the second stage is composed of an array of classical Wyner-Ziv
codes, one for each action. Specific coding/decoding strategies are designed
based on LDGM codes and message passing. Through numerical examples, the
proposed code design is shown to achieve performance close to the lower bound
dictated by the rate-distortion-cost function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6191</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6191</id><created>2013-01-25</created><authors><author><keyname>Santos-Neto</keyname><forenames>Elizeu</forenames></author><author><keyname>Condon</keyname><forenames>David</forenames></author><author><keyname>Andrade</keyname><forenames>Nazareno</forenames></author><author><keyname>Iamnitchi</keyname><forenames>Adriana</forenames></author><author><keyname>Ripeanu</keyname><forenames>Matei</forenames></author></authors><title>Reuse, Temporal Dynamics, Interest Sharing, and Collaboration in Social
  Tagging Systems</title><categories>cs.IR cs.DL cs.SI physics.soc-ph</categories><comments>Part of this work has been publish in the ACM International
  Conference on Hypertext'2009</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  User-generated content is shaping the dynamics of the World Wide Web. Indeed,
an increasingly large number of systems provide mechanisms to support the
growing demand for content creation, sharing, and management. Tagging systems
are a particular class of these systems where users share and collaboratively
annotate content such as photos and URLs. This collaborative behavior and the
pool of user-generated metadata create opportunities to improve existing
systems and to design new mechanisms. However, to realize this potential, it is
necessary to understand the usage characteristics of current systems. This work
addresses this issue characterizing three tagging systems (CiteULike, Connotea
and del.icio.us) while focusing on three aspects: i) the patterns of
information (tags and items) production; ii) the temporal dynamics of users'
tag vocabularies; and, iii) the social aspects of tagging systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6195</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6195</id><created>2013-01-25</created><authors><author><keyname>Al-Kiswany</keyname><forenames>Samer</forenames></author><author><keyname>Vairavanathan</keyname><forenames>Emalayan</forenames></author><author><keyname>Costa</keyname><forenames>Lauro B.</forenames></author><author><keyname>Yang</keyname><forenames>Hao</forenames></author><author><keyname>Ripeanu</keyname><forenames>Matei</forenames></author></authors><title>The Case for Cross-Layer Optimizations in Storage: A Workflow-Optimized
  Storage System</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes using file system custom metadata as a bidirectional
communication channel between applications and the storage system. This channel
can be used to pass hints that enable cross-layer optimizations, an option
hindered today by the ossified file-system interface. We study this approach in
context of storage system support for large-scale workflow execution systems:
Our workflow optimized storage system (WOSS), exploits application hints to
provide per-file optimized operations, and exposes data location to enable
location-aware scheduling.
  This paper argues that an incremental adoption path for adopting cross-layer
optimizations in storage systems exists, presents the system architecture for a
workflow-optimized storage system and its integration with a workflow runtime
engine, and evaluates the proposed approach using synthetic as well as real
applications workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6196</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6196</id><created>2013-01-25</created><updated>2014-05-29</updated><authors><author><keyname>Gonz&#xe1;lez</keyname><forenames>&#xd3;scar</forenames></author><author><keyname>Beltr&#xe1;n</keyname><forenames>Carlos</forenames></author><author><keyname>Santamar&#xed;a</keyname><forenames>Ignacio</forenames></author></authors><title>On the Number of Interference Alignment Solutions for the K-User MIMO
  Channel with Constant Coefficients</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the number of different interference alignment (IA)
solutions in a K-user multiple-input multiple-output (MIMO) interference
channel, when the alignment is performed via beamforming and no symbol
extensions are allowed. We focus on the case where the number of IA equations
matches the number of variables. In this situation, the number of IA solutions
is finite and constant for any channel realization out of a zero-measure set
and, as we prove in the paper, it is given by an integral formula that can be
numerically approximated using Monte Carlo integration methods. More precisely,
the number of alignment solutions is the scaled average of the determinant of a
certain Hermitian matrix related to the geometry of the problem. Interestingly,
while the value of this determinant at an arbitrary point can be used to check
the feasibility of the IA problem, its average (properly scaled) gives the
number of solutions. For single-beam systems the asymptotic growth rate of the
number of solutions is analyzed and some connections with classical
combinatorial problems are presented. Nonetheless, our results can be applied
to arbitrary interference MIMO networks, with any number of users, antennas and
streams per user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6198</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6198</id><created>2013-01-25</created><authors><author><keyname>Maamari</keyname><forenames>Diana</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author><author><keyname>Devroye</keyname><forenames>Natasha</forenames></author></authors><title>Approximate Sum-Capacity of K-user Cognitive Interference Channels with
  Cumulative Message Sharing</title><categories>cs.IT math.IT</categories><comments>Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the K user cognitive interference channel with one
primary and K-1 secondary/cognitive transmitters with a cumulative message
sharing structure, i.e cognitive transmitter $i\in [2:K]$ knows non-causally
all messages of the users with index less than i. We propose a computable outer
bound valid for any memoryless channel. We first evaluate the sum-rate outer
bound for the high- SNR linear deterministic approximation of the Gaussian
noise channel. This is shown to be capacity for the 3-user channel with
arbitrary channel gains and the sum-capacity for the symmetric K-user channel.
Interestingly. for the K user channel having only the K th cognitive know all
the other messages is sufficient to achieve capacity i.e cognition at
transmitter 2 to K-1 is not needed. Next the sum capacity of the symmetric
Gaussian noise channel is characterized to within a constant additive and
multiplicative gap. The proposed achievable scheme for the additive gap is
based on Dirty paper coding and can be thought of as a MIMO-broadcast scheme
where only one encoding order is possible due to the message sharing structure.
As opposed to other multiuser interference channel models, a single scheme
suffices for both the weak and strong interference regimes. With this scheme
the generalized degrees of freedom (gDOF) is shown to be a function of K, in
contrast to the non cognitive case and the broadcast channel case.
Interestingly, it is show that as the number of users grows to infinity the
gDoF of the K-user cognitive interference channel with cumulative message
sharing tends to the gDoF of a broadcast channel with a K-antenna transmitter
and K single-antenna receivers. The analytical additive additive and
multiplicative gaps are a function of the number of users. Numerical
evaluations of inner and outer bounds show that the actual gap is less than the
analytical one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6199</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6199</id><created>2013-01-25</created><updated>2014-02-05</updated><authors><author><keyname>Sakata</keyname><forenames>Ayaka</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author></authors><title>Sample Complexity of Bayesian Optimal Dictionary Learning</title><categories>cs.LG cond-mat.dis-nn cond-mat.stat-mech cs.IT math.IT</categories><comments>5pages, 5figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a learning problem of identifying a dictionary matrix D (M times
N dimension) from a sample set of M dimensional vectors Y = N^{-1/2} DX, where
X is a sparse matrix (N times P dimension) in which the density of non-zero
entries is 0&lt;rho&lt; 1. In particular, we focus on the minimum sample size P_c
(sample complexity) necessary for perfectly identifying D of the optimal
learning scheme when D and X are independently generated from certain
distributions. By using the replica method of statistical mechanics, we show
that P_c=O(N) holds as long as alpha = M/N &gt;rho is satisfied in the limit of N
to infinity. Our analysis also implies that the posterior distribution given Y
is condensed only at the correct dictionary D when the compression rate alpha
is greater than a certain critical value alpha_M(rho). This suggests that
belief propagation may allow us to learn D with a low computational complexity
using O(N) samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6209</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6209</id><created>2013-01-25</created><authors><author><keyname>Bae</keyname><forenames>Jung Hyun</forenames></author><author><keyname>Lee</keyname><forenames>Jungwon</forenames></author><author><keyname>Kang</keyname><forenames>Inyup</forenames></author></authors><title>On the achievable region for interference networks with point-to-point
  codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies evaluation of the capacity region for interference
networks with point-to-point (p2p) capacity-achieving codes. Such capacity
region has recently been characterized as union of several sub-regions each of
which has distinctive operational characteristics. Detailed evaluation of this
region, therefore, can be accomplished in a very simple manner by acknowledging
such characteristics, which, in turn, provides an insight for a simple
implementation scenario. Completely generalized message assignment which is
also practically relevant is considered in this paper, and it is shown to
provide strictly larger achievable rates than what traditional message
assignment does when a receiver with joint decoding capability is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6228</identifier>
 <datestamp>2013-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6228</id><created>2013-01-26</created><updated>2013-11-18</updated><authors><author><keyname>Luckow</keyname><forenames>Andre</forenames></author><author><keyname>Santcroos</keyname><forenames>Mark</forenames></author><author><keyname>Zebrowski</keyname><forenames>Ashley</forenames></author><author><keyname>Jha</keyname><forenames>Shantenu</forenames></author></authors><title>Pilot-Data: An Abstraction for Distributed Data</title><categories>cs.DC</categories><acm-class>C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific problems that depend on processing large amounts of data require
overcoming challenges in multiple areas: managing large-scale data
distribution, controlling co-placement and scheduling of data with compute
resources, and storing, transferring, and managing large volumes of data.
Although there exist multiple approaches to addressing each of these
challenges, an integrative approach is missing; furthermore, extending existing
functionality or enabling interoperable capabilities remains difficult at best.
We propose the concept of Pilot-Data to address the fundamental challenges of
co-placement and scheduling of data and compute in heterogeneous and
distributed environments with interoperability and extensibility as first-order
concerns. Pilot-Data is an extension of the Pilot-Job abstraction for
supporting the management of data in conjunction with compute tasks. Pilot-Data
separates logical data units from physical storage, thereby providing the basis
for efficient compute/data placement and scheduling. In this paper, we discuss
the design and implementation of the Pilot-Data prototype, demonstrate its use
by data-intensive applications on multiple production distributed
cyberinfrastructure and illustrate the advantages arising from flexible
execution modes enabled by Pilot-Data. Our experiments utilize an
implementation of Pilot-Data in conjunction with a scalable Pilot-Job (BigJob)
to establish the application performance that can be enabled by the use of
Pilot-Data. We demonstrate how the concept of Pilot-Data also provides the
basis upon which to build tools and support capabilities like affinity which in
turn can be used for advanced data-compute co-placement and scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6230</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6230</id><created>2013-01-26</created><authors><author><keyname>Borisevich</keyname><forenames>Alex</forenames></author></authors><title>Numerical homotopy continuation for control and online identification of
  nonlinear systems: the survey of selected results</title><categories>math.OC cs.SY</categories><comments>Review of recent results. To be published</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article gives an overview of the parameter numerical continuation
methodology applied to setpoint control and parameter identification of
nonlinear systems. The control problems for affine systems as well as general
(nonaffine) nonlinear systems are considered. Online parameter identification
is also presented in two versions: with linear and nonlinear nonconvex
parameterization. Simulation results for illustrative examples are shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6231</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6231</id><created>2013-01-26</created><updated>2013-06-27</updated><authors><author><keyname>Zeh</keyname><forenames>Alexander</forenames></author><author><keyname>Wachter-Zeh</keyname><forenames>Antonia</forenames></author><author><keyname>Gadouleau</keyname><forenames>Maximilien</forenames></author><author><keyname>Bezzateev</keyname><forenames>Sergey</forenames></author></authors><title>Generalizing Bounds on the Minimum Distance of Cyclic Codes Using Cyclic
  Product Codes</title><categories>cs.IT math.IT</categories><comments>5 pages, no figure, accepted for ISIT2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two generalizations of the Hartmann--Tzeng (HT) bound on the minimum distance
of q-ary cyclic codes are proposed. The first one is proven by embedding the
given cyclic code into a cyclic product code. Furthermore, we show that unique
decoding up to this bound is always possible and outline a quadratic-time
syndrome-based error decoding algorithm. The second bound is stronger and the
proof is more involved.
  Our technique of embedding the code into a cyclic product code can be applied
to other bounds, too and therefore generalizes them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6236</identifier>
 <datestamp>2013-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6236</id><created>2013-01-26</created><updated>2013-01-31</updated><authors><author><keyname>Nielsen</keyname><forenames>Johan Sebastian Rosenkilde</forenames><affiliation>Technical University of Denmark</affiliation></author><author><keyname>Zeh</keyname><forenames>Alexander</forenames><affiliation>INT - University of Ulm., INRIA Saclay - Ile de France</affiliation></author></authors><title>Multi-Trial Guruswami--Sudan Decoding for Generalised Reed--Solomon
  Codes</title><categories>cs.IT math.IT</categories><comments>WCC 2013 International Workshop on Coding and Cryptography (2013)</comments><proxy>ccsd</proxy><journal-ref>International Workshop on Coding and Cryptography (WCC) (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An iterated refinement procedure for the Guruswami--Sudan list decoding
algorithm for Generalised Reed--Solomon codes based on Alekhnovich's module
minimisation is proposed. The method is parametrisable and allows variants of
the usual list decoding approach. In particular, finding the list of
\emph{closest} codewords within an intermediate radius can be performed with
improved average-case complexity while retaining the worst-case complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6255</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6255</id><created>2013-01-26</created><authors><author><keyname>Subramanian</keyname><forenames>Ramanan</forenames></author><author><keyname>Vellambi</keyname><forenames>Badri</forenames></author><author><keyname>Land</keyname><forenames>Ingmar</forenames></author></authors><title>Information Loss due to Finite Block Length in a Gaussian Line Network:
  An Improved Bound</title><categories>cs.IT math.IT math.PR stat.AP</categories><comments>7 pages, 4 figures. To be submitted to ISIT 2013 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A bound on the maximum information transmission rate through a cascade of
Gaussian links is presented. The network model consists of a source node
attempting to send a message drawn from a finite alphabet to a sink, through a
cascade of Additive White Gaussian Noise links each having an input power
constraint. Intermediate nodes are allowed to perform arbitrary
encoding/decoding operations, but the block length and the encoding rate are
fixed. The bound presented in this paper is fundamental and depends only on the
design parameters namely, the network size, block length, transmission rate,
and signal-to-noise ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6256</identifier>
 <datestamp>2014-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6256</id><created>2013-01-26</created><updated>2013-07-01</updated><authors><author><keyname>Shi</keyname><forenames>Hailong</forenames></author><author><keyname>Zhang</keyname><forenames>Hao</forenames></author></authors><title>Tight is better: Performance Improvement of the Compressive Classifier
  Using Equi-Norm Tight Frames</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>6pages,Accepted by IEEE DSP/SPE 2013</comments><journal-ref>Digital Signal Processing and Signal Processing Education Meeting
  (DSP/SPE), 2013 IEEE , vol., no., pp.12,17, 11-14 Aug. 2013</journal-ref><doi>10.1109/DSP-SPE.2013.6642557</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting or classifying already known sparse signals contaminated by
Gaussian noise from compressive measurements is different from reconstructing
sparse signals, as its objective is to minimize the error probability which
describes performance of the detectors or classifiers. This paper is concerned
about the performance improvement of a commonly used Compressive Classifier. We
prove that when the arbitrary sensing matrices used to get the Compressive
Measurements are transformed into Equi-Norm Tight Frames, i.e. the matrices
that are row-orthogonal, The Compressive Classifier achieves better
performance. Although there are other proofs that among all Equi-Norm Tight
Frames the Equiangular tight Frames (ETFs) bring best worst-case performance,
the existence and construction of ETFs on some dimensions is still an open
problem. As the construction of Equi-Norm Tight Frames from any arbitrary
matrices is very easy and practical compared with ETF matrices, the result of
this paper can also provide a practical method to design an improved sensing
matrix for Compressive Classification. We can conclude that: Tight is Better!
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6260</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6260</id><created>2013-01-26</created><authors><author><keyname>Tee</keyname><forenames>Sim-Hui</forenames></author></authors><title>Problems of Inheritance at Java Inner Class</title><categories>cs.PL</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single inheritance has been widely accepted in the current programming
practice to avoid the complication that incurred by multiple inheritance.
Single inheritance enhances the reusability of codes and eliminates the
confusion of identical methods that possibly defined in two superclasses.
However, the mechanism of inner class in Java potentially reintroduces the
problems encountered by multiple inheritance. When the depth of Java inner
class is increased, the problem becomes severe. This paper aims at exposing the
problems of inheritance at the Java inner class. In addition, a measure is
proposed to evaluate the potential problem of inheritance for Java inner class
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6262</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6262</id><created>2013-01-26</created><authors><author><keyname>Tee</keyname><forenames>Sim-Hui</forenames></author></authors><title>Developing Parallel Dependency Graph In Improving Game Balancing</title><categories>cs.AI</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dependency graph is a data architecture that models all the dependencies
between the different types of assets in the game. It depicts the
dependency-based relationships between the assets of a game. For example, a
player must construct an arsenal before he can build weapons. It is vital that
the dependency graph of a game is designed logically to ensure a logical
sequence of game play. However, a mere logical dependency graph is not
sufficient in sustaining the players' enduring interests in a game, which
brings the problem of game balancing into picture. The issue of game balancing
arises when the players do not feel the chances of winning the game over their
AI opponents who are more skillful in the game play. At the current state of
research, the architecture of dependency graph is monolithic for the players.
The sequence of asset possession is always foreseeable because there is only a
single dependency graph. Game balancing is impossible when the assets of AI
players are overwhelmingly outnumbering that of human players. This paper
proposes a parallel architecture of dependency graph for the AI players and
human players. Instead of having a single dependency graph, a parallel
architecture is proposed where the dependency graph of AI player is adjustable
with that of human player using a support dependency as a game balancing
mechanism. This paper exhibits that the parallel dependency graph helps to
improve game balancing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6263</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6263</id><created>2013-01-26</created><authors><author><keyname>Roth</keyname><forenames>Volker</forenames></author><author><keyname>G&#xfc;ldenring</keyname><forenames>Benjamin</forenames></author><author><keyname>Rieffel</keyname><forenames>Eleanor</forenames></author><author><keyname>Dietrich</keyname><forenames>Sven</forenames></author><author><keyname>Ries</keyname><forenames>Lars</forenames></author></authors><title>A Secure Submission System for Online Whistleblowing Platforms</title><categories>cs.CR</categories><comments>An abridged version has been accepted for publication in the
  proceedings of Financial Cryptography and Data Security 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whistleblower laws protect individuals who inform the public or an authority
about governmental or corporate misconduct. Despite these laws, whistleblowers
frequently risk reprisals and sites such as WikiLeaks emerged to provide a
level of anonymity to these individuals. However, as countries increase their
level of network surveillance and Internet protocol data retention, the mere
act of using anonymizing software such as Tor, or accessing a whistleblowing
website through an SSL channel might be incriminating enough to lead to
investigations and repercussions. As an alternative submission system we
propose an online advertising network called AdLeaks. AdLeaks leverages the
ubiquity of unsolicited online advertising to provide complete sender
unobservability when submitting disclosures. AdLeaks ads compute a random
function in a browser and submit the outcome to the AdLeaks infrastructure.
Such a whistleblower's browser replaces the output with encrypted information
so that the transmission is indistinguishable from that of a regular browser.
Its back-end design assures that AdLeaks must process only a fraction of the
resulting traffic in order to receive disclosures with high probability. We
describe the design of AdLeaks and evaluate its performance through analysis
and experimentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6265</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6265</id><created>2013-01-26</created><updated>2013-05-31</updated><authors><author><keyname>Karbasi</keyname><forenames>Amin</forenames></author><author><keyname>Salavati</keyname><forenames>Amir Hesam</forenames></author><author><keyname>Shokrollahi</keyname><forenames>Amin</forenames></author><author><keyname>Varshney</keyname><forenames>Lav</forenames></author></authors><title>Neural Networks Built from Unreliable Components</title><categories>cs.NE cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to some personal
  restrictions on the publication policy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in associative memory design through strutured pattern sets
and graph-based inference algorithms have allowed the reliable learning and
retrieval of an exponential number of patterns. Both these and classical
associative memories, however, have assumed internally noiseless computational
nodes. This paper considers the setting when internal computations are also
noisy. Even if all components are noisy, the final error probability in recall
can often be made exceedingly small, as we characterize. There is a threshold
phenomenon. We also show how to optimize inference algorithm parameters when
knowing statistical properties of internal noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6268</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6268</id><created>2013-01-26</created><updated>2014-08-29</updated><authors><author><keyname>Rudelson</keyname><forenames>Mark</forenames></author><author><keyname>Zeitouni</keyname><forenames>Ofer</forenames></author></authors><title>Singular values of Gaussian matrices and permanent estimators</title><categories>math.PR cs.DS</categories><comments>small revision, no major changes. Changed terminology to &quot;broadly
  connected&quot;. Corrected error in probability estimate in statement of theorems
  1.4 and 1.5</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present estimates on the small singular values of a class of matrices with
independent Gaussian entries and inhomogeneous variance profile, satisfying a
broad-connectedness condition. Using these estimates and concentration of
measure for the spectrum of Gaussian matrices with independent entries, we
prove that for a large class of graphs satisfying an appropriate expansion
property, the Barvinok--Godsil-Gutman estimator for the permanent achieves
sub-exponential errors with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6272</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6272</id><created>2013-01-26</created><updated>2015-06-01</updated><authors><author><keyname>Hajizadeh</keyname><forenames>Saeed</forenames></author><author><keyname>Monemizadeh</keyname><forenames>Mostafa</forenames></author></authors><title>State-Dependent Z Channel</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper we study the Z channel with side information non-causally
available at the encoders. We use Marton encoding along with Gelfand-Pinsker
random binning scheme and Chong-Motani-Garg-El Gamal (CMGE) jointly decoding to
find an achievable rate region. We will see that our achievable rate region
gives the achievable rate of the multiple access channel with side information
and also degraded broadcast channel with side information. We will also derive
an inner bound and an outer bound on the capacity region of the state-dependent
degraded discrete memoryless Z channel and also will observe that our outer
bound meets the inner bound for the rates corresponding to the second
transmitter. Also, by assuming the high signal to noise ratio and strong
interference regime, and using the lattice strategies, we derive an achievable
rate region for the Gaussian degraded Z channel with additive interference
non-causally available at both of the encoders. Our method is based on lattice
transmission scheme, jointly decoding at the first decoder and successive
decoding at the second decoder. Using such coding scheme we remove the effect
of the interference completely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6277</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6277</id><created>2013-01-26</created><authors><author><keyname>Kang</keyname><forenames>Jeon-Hyung</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author><author><keyname>Getoor</keyname><forenames>Lise</forenames></author></authors><title>LA-LDA: A Limited Attention Topic Model for Social Recommendation</title><categories>cs.SI cs.IR cs.LG</categories><comments>The 2013 International Conference on Social Computing,
  Behavioral-Cultural Modeling, &amp; Prediction (SBP 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media users have finite attention which limits the number of incoming
messages from friends they can process. Moreover, they pay more attention to
opinions and recommendations of some friends more than others. In this paper,
we propose LA-LDA, a latent topic model which incorporates limited,
non-uniformly divided attention in the diffusion process by which opinions and
information spread on the social network. We show that our proposed model is
able to learn more accurate user models from users' social network and item
adoption behavior than models which do not take limited attention into account.
We analyze voting on news items on the social news aggregator Digg and show
that our proposed model is better able to predict held out votes than
alternative models. Our study demonstrates that psycho-socially motivated
models have better ability to describe and predict observed behavior than
models which only consider topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6291</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6291</id><created>2013-01-26</created><authors><author><keyname>Ghasemi-Goojani</keyname><forenames>Shahab</forenames></author><author><keyname>Behroozi</keyname><forenames>Hamid</forenames></author></authors><title>Nested Lattice Codes for Gaussian Two-Way Relay Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a Gaussian two-way relay channel (GTRC), where two
sources exchange messages with each other through a relay. We assume that there
is no direct link between sources, and all nodes operate in full-duplex mode.
By utilizing nested lattice codes for the uplink (i.e., MAC phase), and
structured binning for the downlink (i.e., broadcast phase), we propose two
achievable schemes. Scheme 1 is based on compute and forward scheme of [1]
while scheme 2 utilizes two different lattices for source nodes based on a
three-stage lattice partition chain. We show that scheme 2 can achieve capacity
region at the high signal-to-noise ratio (SNR). Regardless all channel
parameters, the achievable rate of scheme 2 is within 0.2654 bit from the
cut-set outer bound for user 1. For user 2, the proposed scheme achieves within
0.167 bit from the outer bound if channel coefficient is larger than one, and
achieves within 0.2658 bit from the outer bound if channel coefficient is
smaller than one. Moreover, sum rate of the proposed scheme is within 0.334
bits from the sum capacity. These gaps for GTRC are the best gap-to-capacity
results to date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6295</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6295</id><created>2013-01-26</created><updated>2015-09-01</updated><authors><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author><author><keyname>Riegler</keyname><forenames>Erwin</forenames></author><author><keyname>Fletcher</keyname><forenames>Alyson</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author></authors><title>Fixed Points of Generalized Approximate Message Passing with Arbitrary
  Matrices</title><categories>cs.IT math.IT</categories><comments>10 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The estimation of a random vector with independent components passed through
a linear transform followed by a componentwise (possibly nonlinear) output map
arises in a range of applications. Approximate message passing (AMP) methods,
based on Gaussian approximations of loopy belief propagation, have recently
attracted considerable attention for such problems. For large random
transforms, these methods exhibit fast convergence and admit precise analytic
characterizations with testable conditions for optimality, even for certain
non-convex problem instances. However, the behavior of AMP under general
transforms is not fully understood. In this paper, we consider the generalized
AMP (GAMP) algorithm and relate the method to more common optimization
techniques. This analysis enables a precise characterization of the GAMP
algorithm fixed-points that applies to arbitrary transforms. In particular, we
show that the fixed points of the so-called max-sum GAMP algorithm for MAP
estimation are critical points of a constrained maximization of the posterior
density. The fixed-points of the sum-product GAMP algorithm for estimation of
the posterior marginals can be interpreted as critical points of a certain free
energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6297</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6297</id><created>2013-01-26</created><updated>2013-04-10</updated><authors><author><keyname>Attiya</keyname><forenames>Hagit</forenames></author><author><keyname>Hans</keyname><forenames>Sandeep</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Petr</forenames></author><author><keyname>Ravi</keyname><forenames>Srivatsan</forenames></author></authors><title>Safety of Deferred Update in Transactional Memory</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transactional memory allows the user to declare sequences of instructions as
speculative \emph{transactions} that can either \emph{commit} or \emph{abort}.
If a transaction commits, it appears to be executed sequentially, so that the
committed transactions constitute a correct sequential execution. If a
transaction aborts, none of its instructions can affect other transactions.
  The popular criterion of \emph{opacity} requires that the views of aborted
transactions must also be consistent with the global sequential order
constituted by committed ones. This is believed to be important, since
inconsistencies observed by an aborted transaction may cause a fatal
irrecoverable error or waste of the system in an infinite loop. Intuitively, an
opaque implementation must ensure that no intermediate view a transaction
obtains before it commits or aborts can be affected by a transaction that has
not started committing yet, so called \emph{deferred-update} semantics.
  In this paper, we intend to grasp this intuition formally. We propose a
variant of opacity that explicitly requires the sequential order to respect the
deferred-update semantics. We show that our criterion is a safety property,
i.e., it is prefix- and limit-closed. Unlike opacity, our property also ensures
that a serialization of a history implies serializations of its prefixes.
Finally, we show that our property is equivalent to opacity if we assume that
no two transactions commit identical values on the same variable, and present a
counter-example for scenarios when the &quot;unique-write&quot; assumption does not hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6299</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6299</id><created>2013-01-26</created><authors><author><keyname>Adjiashvili</keyname><forenames>David</forenames></author></authors><title>Fault-Tolerant Shortest Paths - Beyond the Uniform Failure Model</title><categories>cs.DS math.OC</categories><acm-class>F.5.1; F.5.2; F.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The overwhelming majority of survivable (fault-tolerant) network design
models assume a uniform scenario set. Such a scenario set assumes that every
subset of the network resources (edges or vertices) of a given cardinality $k$
comprises a scenario. While this approach yields problems with clean
combinatorial structure and good algorithms, it often fails to capture the true
nature of the scenario set coming from applications.
  One natural refinement of the uniform model is obtained by partitioning the
set of resources into faulty and secure resources. The scenario set contains
every subset of at most $k$ faulty resources. This work studies the
Fault-Tolerant Path (FTP) problem, the counterpart of the Shortest Path problem
in this failure model. We present complexity results alongside exact and
approximation algorithms for FTP. We emphasize the vast increase in the
complexity of the problem with respect to its uniform analogue, the
Edge-Disjoint Paths problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6301</identifier>
 <datestamp>2013-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6301</id><created>2013-01-26</created><updated>2013-05-21</updated><authors><author><keyname>Pradhan</keyname><forenames>Asit Kumar</forenames></author><author><keyname>Subramanian</keyname><forenames>Arunkumar</forenames></author><author><keyname>Thangaraj</keyname><forenames>Andrew</forenames></author></authors><title>Deterministic Constructions for Large Girth Protograph LDPC Codes</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures; To appear in ISIT 2013; Minor changes in
  presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bit-error threshold of the standard ensemble of Low Density Parity Check
(LDPC) codes is known to be close to capacity, if there is a non-zero fraction
of degree-two bit nodes. However, the degree-two bit nodes preclude the
possibility of a block-error threshold. Interestingly, LDPC codes constructed
using protographs allow the possibility of having both degree-two bit nodes and
a block-error threshold. In this paper, we analyze density evolution for
protograph LDPC codes over the binary erasure channel and show that their
bit-error probability decreases double exponentially with the number of
iterations when the erasure probability is below the bit-error threshold and
long chain of degree-two variable nodes are avoided in the protograph. We
present deterministic constructions of such protograph LDPC codes with girth
logarithmic in blocklength, resulting in an exponential fall in bit-error
probability below the threshold. We provide optimized protographs, whose
block-error thresholds are better than that of the standard ensemble with
minimum bit-node degree three. These protograph LDPC codes are theoretically of
great interest, and have applications, for instance, in coding with strong
secrecy over wiretap channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6302</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6302</id><created>2013-01-26</created><authors><author><keyname>Shen</keyname><forenames>Chao</forenames></author><author><keyname>Li</keyname><forenames>Wei-Chiang</forenames></author><author><keyname>Chang</keyname><forenames>Tsung-Hui</forenames></author></authors><title>Simultaneous Information and Energy Transfer: A Two-User MISO
  Interference Channel Case</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures</comments><journal-ref>IEEE GLOBECOM'12, 3-7 Dec. 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the sum rate maximization problem of a two-user
multiple-input single-output interference channel with receivers that can
scavenge energy from the radio signals transmitted by the transmitters. We
first study the optimal transmission strategy for an ideal scenario where the
two receivers can simultaneously decode the information signal and harvest
energy. Then, considering the limitations of the current circuit technology, we
propose two practical schemes based on TDMA, where, at each time slot, the
receiver either operates in the energy harvesting mode or in the information
detection mode. Optimal transmission strategies for the two practical schemes
are respectively investigated. Simulation results show that the three schemes
exhibit interesting tradeoff between achievable sum rate and energy harvesting
requirement, and do not dominate each other in terms of maximum achievable sum
rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6312</identifier>
 <datestamp>2013-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6312</id><created>2013-01-26</created><updated>2013-05-09</updated><authors><author><keyname>Dong</keyname><forenames>Wenxiang</forenames></author><author><keyname>Zhang</keyname><forenames>Wenyi</forenames></author><author><keyname>Tan</keyname><forenames>Chee Wei</forenames></author></authors><title>Rooting out the Rumor Culprit from Suspects</title><categories>cs.SI cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that a rumor originating from a single source among a set of suspects
spreads in a network, how to root out this rumor source? With the a priori
knowledge of suspect nodes and an observation of infected nodes, we construct a
maximum a posteriori (MAP) estimator to identify the rumor source using the
susceptible-infected (SI) model. The a priori suspect set and its associated
connectivity bring about new ingredients to the problem, and thus we propose to
use local rumor center, a generalized concept based on rumor centrality, to
identify the source from suspects. For regular tree-type networks of node
degree {\delta}, we characterize Pc(n), the correct detection probability of
the estimator upon observing n infected nodes, in both the finite and
asymptotic regimes. First, when every infected node is a suspect, Pc(n)
asymptotically grows from 0.25 to 0.307 with {\delta} from 3 to infinity, a
result first established in Shah and Zaman (2011, 2012) via a different
approach; and it monotonically decreases with n and increases with {\delta}.
Second, when the suspects form a connected subgraph of the network, Pc(n)
asymptotically significantly exceeds the a priori probability if {\delta}&gt;2,
and reliable detection is achieved as {\delta} becomes large; furthermore, it
monotonically decreases with n and increases with {\delta}. Third, when there
are only two suspects, Pc(n) is asymptotically at least 0.75 if {\delta}&gt;2; and
it increases with the distance between the two suspects. Fourth, when there are
multiple suspects, among all possible connection patterns, that they form a
connected subgraph of the network achieves the smallest detection probability.
Our analysis leverages ideas from the Polya's urn model in probability theory
and sheds insight into the behavior of the rumor spreading process not only in
the asymptotic regime but also for the general finite-n regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6314</identifier>
 <datestamp>2013-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6314</id><created>2013-01-26</created><updated>2013-08-14</updated><authors><author><keyname>Reshef</keyname><forenames>David</forenames></author><author><keyname>Reshef</keyname><forenames>Yakir</forenames></author><author><keyname>Mitzenmacher</keyname><forenames>Michael</forenames></author><author><keyname>Sabeti</keyname><forenames>Pardis</forenames></author></authors><title>Equitability Analysis of the Maximal Information Coefficient, with
  Comparisons</title><categories>cs.LG q-bio.QM stat.ML</categories><comments>22 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A measure of dependence is said to be equitable if it gives similar scores to
equally noisy relationships of different types. Equitability is important in
data exploration when the goal is to identify a relatively small set of
strongest associations within a dataset as opposed to finding as many non-zero
associations as possible, which often are too many to sift through. Thus an
equitable statistic, such as the maximal information coefficient (MIC), can be
useful for analyzing high-dimensional data sets. Here, we explore both
equitability and the properties of MIC, and discuss several aspects of the
theory and practice of MIC. We begin by presenting an intuition behind the
equitability of MIC through the exploration of the maximization and
normalization steps in its definition. We then examine the speed and optimality
of the approximation algorithm used to compute MIC, and suggest some directions
for improving both. Finally, we demonstrate in a range of noise models and
sample sizes that MIC is more equitable than natural alternatives, such as
mutual information estimation and distance correlation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6315</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6315</id><created>2013-01-26</created><authors><author><keyname>Zamanighomi</keyname><forenames>Mahdi</forenames></author><author><keyname>Wang</keyname><forenames>Zhengdao</forenames></author></authors><title>Multiple-Antenna Interference Channel with Receive Antenna Joint
  Processing and Real Interference Alignment</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a constant $K$-user Gaussian interference channel with $M$
antennas at each transmitter and $N$ antennas at each receiver, denoted as a
$(K,M,N)$ channel. Relying on a result on simultaneous Diophantine
approximation, a real interference alignment scheme with joint receive antenna
processing is developed. The scheme is used to provide new proofs for two
previously known results, namely 1) the total degrees of freedom (DoF) of a
$(K, N, N)$ channel is $NK/2$; and 2) the total DoF of a $(K, M, N)$ channel is
at least $KMN/(M+N)$. We also derive the DoF region of the $(K,N,N)$ channel,
and an inner bound on the DoF region of the $(K,M,N)$ channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6316</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6316</id><created>2013-01-26</created><updated>2013-03-18</updated><authors><author><keyname>Song</keyname><forenames>Hyun Ah</forenames></author><author><keyname>Lee</keyname><forenames>Soo-Young</forenames></author></authors><title>Hierarchical Data Representation Model - Multi-layer NMF</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a data representation model that demonstrates
hierarchical feature learning using nsNMF. We extend unit algorithm into
several layers. Experiments with document and image data successfully
discovered feature hierarchies. We also prove that proposed method results in
much better classification and reconstruction performance, especially for small
number of features. feature hierarchies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6318</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6318</id><created>2013-01-27</created><updated>2013-07-01</updated><authors><author><keyname>Shi</keyname><forenames>Hailong</forenames></author><author><keyname>Zhang</keyname><forenames>Hao</forenames></author></authors><title>Quasi-Equiangular Frame (QEF) : A New Flexible Configuration of Frame</title><categories>cs.IT math.AG math.IT</categories><comments>4 Pages, Submitted to GlobalSIP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frame theory is a powerful tool in the domain of signal processing and
communication. Among its numerous configurations, the ones which have drawn
much attention recently are Equiangular Tight Frame (ETF) and Grassmannian
Frame. These frames both have some kind of optimality in coherence, thus bring
robustness or optimal performance in applications such as digital fingerprint,
erasure channels, and Compressive Sensing. However, too strict constraint on
existence and construction of ETF and Grassmannian Frame became the main
obstacle for widespread use. In this paper, we propose a new configuration of
frame: Quasi-Equiangular Frame, as a compromise but more convenient and
flexible approximation of ETF and Grassmannian Frame. We will give formal
definition of Quasi-Equiangular Frame and analyze its relationship with ETF and
Grassmannian frame. Furthermore, for popularity of ETF and Grassmannian frame
in Compressive Sensing, we utilize the technique of random matrices to obtain
asymptotical concentration estimation of the Restricted Isometry Constant (RIC)
of Quasi-Equiangular Frame with respect to its key parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6319</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6319</id><created>2013-01-27</created><authors><author><keyname>Sobri</keyname><forenames>Muhamd</forenames></author><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author></authors><title>Aplikasi belajar membaca iqro' berbasis mobile</title><categories>cs.OH</categories><comments>Seminar Nasional Teknologi Informasi &amp; Multimedia (Semnasteknomedia),
  STMIK AMIKOM Yogyakarta, 2013. 5 pages. 11 figures</comments><journal-ref>M. Sobri and L. A. Abdillah, &quot;Aplikasi belajar membaca iqro'
  berbasis mobile,&quot; in Seminar Nasional Teknologi Informasi &amp; Multimedia
  (Semnasteknomedia), STMIK AMIKOM Yogyakarta, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IPTEK and IMTAQ should be followed by knowledge of the ability in reading the
hijaiyah letters as Al Qur-an base. Current people are so busy with their
activities thats way authors develop this mobile application using pocket pc.
The development of this research using waterfall model. Authors use the
programming language of Microsoft Visual BASIC.Net. Authors also use Photoshop
to prepare the image of every letter. In Indonesia, there six level in reading
Al Qur-an, but for the purpose of thi research authors only use Iqro-1 until
Iqro-4. This mobile application also enriched with the voice for every letter
image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6324</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6324</id><created>2013-01-27</created><authors><author><keyname>Sarma</keyname><forenames>T. Hitendra</forenames></author><author><keyname>Viswanath</keyname><forenames>P.</forenames></author><author><keyname>Reddy</keyname><forenames>D. Sai Koti</forenames></author><author><keyname>Raghava</keyname><forenames>S. Sri</forenames></author></authors><title>An improvement to k-nearest neighbor classifier</title><categories>cs.CV cs.LG stat.ML</categories><comments>Appeared in Third International Conference on Data Management, IMT
  Ghaziabad, March 11-12, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  K-Nearest neighbor classifier (k-NNC) is simple to use and has little design
time like finding k values in k-nearest neighbor classifier, hence these are
suitable to work with dynamically varying data-sets. There exists some
fundamental improvements over the basic k-NNC, like weighted k-nearest
neighbors classifier (where weights to nearest neighbors are given based on
linear interpolation), using artificially generated training set called
bootstrapped training set, etc. These improvements are orthogonal to space
reduction and classification time reduction techniques, hence can be coupled
with any of them. The paper proposes another improvement to the basic k-NNC
where the weights to nearest neighbors are given based on Gaussian distribution
(instead of linear interpolation as done in weighted k-NNC) which is also
independent of any space reduction and classification time reduction technique.
We formally show that our proposed method is closely related to non-parametric
density estimation using a Gaussian kernel. We experimentally demonstrate using
various standard data-sets that the proposed method is better than the existing
ones in most cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6328</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6328</id><created>2013-01-27</created><authors><author><keyname>Thomas</keyname><forenames>Eldho K.</forenames></author><author><keyname>Oggier</keyname><forenames>Frederique</forenames></author></authors><title>Explicit Constructions of Quasi-Uniform Codes from Groups</title><categories>math.GR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the question of constructing explicitly quasi-uniform codes from
groups. We determine the size of the codebook, the alphabet and the minimum
distance as a function of the corresponding group, both for abelian and some
nonabelian groups. Potentials applications comprise the design of almost affine
codes and non-linear network codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6330</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6330</id><created>2013-01-27</created><updated>2014-05-22</updated><authors><author><keyname>Kerfriden</keyname><forenames>Pierre</forenames><affiliation>DIMM</affiliation></author><author><keyname>Garc&#xed;a</keyname><forenames>Juan Jos&#xe9; R&#xf3;denas</forenames><affiliation>DIMM</affiliation></author><author><keyname>Bordas</keyname><forenames>St&#xe9;phane Pierre-Alain</forenames></author></authors><title>Certification of projection-based reduced order modelling in
  computational homogenisation by the Constitutive Relation Error</title><categories>math.NA cs.NA physics.class-ph</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose upper and lower error bounding techniques for
reduced order modelling applied to the computational homogenisation of random
composites. The upper bound relies on the construction of a reduced model for
the stress field. Upon ensuring that the reduced stress satisfies the
equilibrium in the finite element sense, the desired bounding property is
obtained. The lower bound is obtained by defining a hierarchical enriched
reduced model for the displacement. We show that the sharpness of both error
estimates can be seamlessly controlled by adapting the parameters of the
corresponding reduced order model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6331</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6331</id><created>2013-01-27</created><authors><author><keyname>Silberstein</keyname><forenames>Natalia</forenames></author><author><keyname>Rawat</keyname><forenames>Ankit Singh</forenames></author><author><keyname>Koyluoglu</keyname><forenames>O. Ozan</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Optimal Locally Repairable Codes via Rank-Metric Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new explicit construction for locally repairable codes
(LRCs) for distributed storage systems which possess all-symbols locality and
maximal possible minimum distance, or equivalently, can tolerate the maximal
number of node failures. This construction, based on maximum rank distance
(MRD) Gabidulin codes, provides new optimal vector and scalar LRCs. In
addition, the paper also discusses mechanisms by which codes obtained using
this construction can be used to construct LRCs with efficient repair of failed
nodes by combination of LRC with regenerating codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6336</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6336</id><created>2013-01-27</created><authors><author><keyname>Lipman</keyname><forenames>Yaron</forenames></author></authors><title>Approximation of Polyhedral Surface Uniformization</title><categories>cs.CG cs.GR cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a constructive approach for approximating the conformal map
(uniformization) of a polyhedral surface to a canonical domain in the plane.
The main tool is a characterization of convex spaces of quasiconformal
simplicial maps and their approximation properties. As far as we are aware,
this is the first algorithm proved to approximate the uniformization of general
polyhedral surfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6339</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6339</id><created>2013-01-27</created><updated>2013-05-20</updated><authors><author><keyname>Dalai</keyname><forenames>Marco</forenames></author></authors><title>Lov\'asz's Theta Function, R\'enyi's Divergence and the Sphere-Packing
  Bound</title><categories>cs.IT math.IT quant-ph</categories><comments>An excerpt from arXiv:1201.5411v3 (with a different notation)
  accepted at ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lov\'asz's bound to the capacity of a graph and the the sphere-packing bound
to the probability of error in channel coding are given a unified presentation
as information radii of the Csisz\'ar type using the R{\'e}nyi divergence in
the classical-quantum setting. This brings together two results in coding
theory that are usually considered as being of a very different nature, one
being a &quot;combinatorial&quot; result and the other being &quot;probabilistic&quot;. In the
context of quantum information theory, this difference disappears.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6340</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6340</id><created>2013-01-27</created><updated>2013-05-20</updated><authors><author><keyname>Dalai</keyname><forenames>Marco</forenames></author></authors><title>An &quot;Umbrella&quot; Bound of the Lov\'asz-Gallager Type</title><categories>cs.IT math.IT</categories><comments>An excerpt from arXiv:1201.5411v3 (with a classical notation)
  accepted at ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel approach for bounding the probability of error of discrete
memoryless channels with a zero-error capacity based on a combination of
Lov\'asz' and Gallager's ideas. The obtained bounds are expressed in terms of a
function $\vartheta(\rho)$, introduced here, that varies from the cut-off rate
of the channel to the Lov\'azs theta function as $\rho$ varies from 1 to
$\infty$ and which is intimately related to Gallager's expurgated coefficient.
The obtained bound to the reliability function, though loose in its present
form, is finite for all rates larger than the Lov\'asz theta function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6345</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6345</id><created>2013-01-27</created><authors><author><keyname>Haddadpour</keyname><forenames>Farzin</forenames></author><author><keyname>Siavoshani</keyname><forenames>Mahdi Jafari</forenames></author><author><keyname>Bakshi</keyname><forenames>Mayank</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author></authors><title>On AVCs with Quadratic Constraints</title><categories>cs.IT math.IT</categories><comments>A shorter version of this work will be send to ISIT13, Istanbul. 8
  pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study an Arbitrarily Varying Channel (AVC) with quadratic
power constraints on the transmitter and a so-called &quot;oblivious&quot; jammer (along
with additional AWGN) under a maximum probability of error criterion, and no
private randomness between the transmitter and the receiver. This is in
contrast to similar AVC models under the average probability of error criterion
considered in [1], and models wherein common randomness is allowed [2] -- these
distinctions are important in some communication scenarios outlined below.
  We consider the regime where the jammer's power constraint is smaller than
the transmitter's power constraint (in the other regime it is known no positive
rate is possible). For this regime we show the existence of stochastic codes
(with no common randomness between the transmitter and receiver) that enables
reliable communication at the same rate as when the jammer is replaced with
AWGN with the same power constraint. This matches known information-theoretic
outer bounds. In addition to being a stronger result than that in [1] (enabling
recovery of the results therein), our proof techniques are also somewhat more
direct, and hence may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6348</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6348</id><created>2013-01-27</created><authors><author><keyname>Foukalas</keyname><forenames>Fotis</forenames></author><author><keyname>Karetsos</keyname><forenames>George T.</forenames></author><author><keyname>Merakos</keyname><forenames>Lazaros</forenames></author></authors><title>Capacity Optimization through Sensing Threshold Adaptation for Cognitive
  Radio Networks</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose the capacity optimization over sensing threshold for
sensing-based cognitive radio networks. The objective function of the proposed
optimization is to maximize the capacity at the secondary user subject to the
constraints on the transmit power and the sensing threshold in order to protect
the primary user. The defined optimization problem is a convex optimization
over the transmit power and the sensing threshold where the concavity on
sensing threshold is proved. The problem is solved by using Lagrange duality
decomposition method in conjunction with a subgradient iterative algorithm and
the numerical results show that the proposed optimization can lead to
significant capacity maximization for the secondary user as long as the primary
user can afford.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6356</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6356</id><created>2013-01-27</created><updated>2013-05-13</updated><authors><author><keyname>Christiansen</keyname><forenames>Mark M.</forenames></author><author><keyname>Duffy</keyname><forenames>Ken R.</forenames></author><author><keyname>Calmon</keyname><forenames>Flavio du Pin</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Brute force searching, the typical set and Guesswork</title><categories>cs.IT cs.CR math.IT</categories><comments>ISIT 2013, with extended proof</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the situation where a word is chosen probabilistically from a finite
list. If an attacker knows the list and can inquire about each word in turn,
then selecting the word via the uniform distribution maximizes the attacker's
difficulty, its Guesswork, in identifying the chosen word. It is tempting to
use this property in cryptanalysis of computationally secure ciphers by
assuming coded words are drawn from a source's typical set and so, for all
intents and purposes, uniformly distributed within it. By applying recent
results on Guesswork, for i.i.d. sources it is this equipartition ansatz that
we investigate here. In particular, we demonstrate that the expected Guesswork
for a source conditioned to create words in the typical set grows, with word
length, at a lower exponential rate than that of the uniform approximation,
suggesting use of the approximation is ill-advised.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6357</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6357</id><created>2013-01-27</created><updated>2013-05-12</updated><authors><author><keyname>Mori</keyname><forenames>H.</forenames></author><author><keyname>Matsumoto</keyname><forenames>Y.</forenames></author><author><keyname>Struzik</keyname><forenames>Z. R.</forenames></author><author><keyname>Mori</keyname><forenames>K.</forenames></author><author><keyname>Makino</keyname><forenames>S.</forenames></author><author><keyname>Mandic</keyname><forenames>D.</forenames></author><author><keyname>Rutkowski</keyname><forenames>T. M.</forenames></author></authors><title>Multi-command Tactile and Auditory Brain Computer Interface based on
  Head Position Stimulation</title><categories>q-bio.NC cs.HC</categories><comments>Proceedings of the Fifth International Brain-Computer Interface
  Meeting 2013, 2 pages, 1 figure</comments><doi>10.3217/978-4-83452-381-5/095</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the extent to which vibrotactile stimuli delivered to the head of a
subject can serve as a platform for a brain computer interface (BCI) paradigm.
Six head positions are used to evoke combined somatosensory and auditory (via
the bone conduction effect) brain responses, in order to define a multimodal
tactile and auditory brain computer interface (taBCI). Experimental results of
subjects performing online taBCI, using stimuli with a moderately fast
inter-stimulus interval (ISI), validate the taBCI paradigm, while the
feasibility of the concept is illuminated through information transfer rate
case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6359</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6359</id><created>2013-01-27</created><updated>2013-01-29</updated><authors><author><keyname>Serov</keyname><forenames>Alexander</forenames></author></authors><title>Subjective Reality and Strong Artificial Intelligence</title><categories>cs.AI</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main prospective aim of modern research related to Artificial
Intelligence is the creation of technical systems that implement the idea of
Strong Intelligence. According our point of view the path to the development of
such systems comes through the research in the field related to perceptions.
Here we formulate the model of the perception of external world which may be
used for the description of perceptual activity of intelligent beings. We
consider a number of issues related to the development of the set of patterns
which will be used by the intelligent system when interacting with environment.
The key idea of the presented perception model is the idea of subjective
reality. The principle of the relativity of perceived world is formulated. It
is shown that this principle is the immediate consequence of the idea of
subjective reality. In this paper we show how the methodology of subjective
reality may be used for the creation of different types of Strong AI systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6360</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6360</id><created>2013-01-27</created><updated>2013-05-12</updated><authors><author><keyname>Chang</keyname><forenames>M.</forenames></author><author><keyname>Nishikawa</keyname><forenames>N.</forenames></author><author><keyname>Struzik</keyname><forenames>Z. R.</forenames></author><author><keyname>Mori</keyname><forenames>K.</forenames></author><author><keyname>Makino</keyname><forenames>S.</forenames></author><author><keyname>Mandic</keyname><forenames>D.</forenames></author><author><keyname>Rutkowski</keyname><forenames>T. M.</forenames></author></authors><title>Comparison of P300 Responses in Auditory, Visual and Audiovisual Spatial
  Speller BCI Paradigms</title><categories>q-bio.NC cs.HC</categories><comments>Proceedings of the Fifth International Brain-Computer Interface
  Meeting 2013, 2 pages, 1 figure</comments><doi>10.3217/978-3-85125-260-6-156</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this study is to provide a comprehensive test of three spatial
speller settings, for the auditory, visual, and audiovisual paradigms. For
rigour, the study is conducted with 16 BCI-na\&quot;ive subjects in an experimental
set-up based on five Japanese hiragana characters. Auditory P300 responses give
encouragingly longer target vs. non-target latencies during the training phase,
however, real-world online BCI experiments in the multimodal setting do not
validate this potential advantage. Our case studies indicate that the auditory
spatial unimodal paradigm needs further development in order to be a viable
alternative to the established visual domain speller applications, as far as
BCI-na\&quot;ive subjects are concerned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6362</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6362</id><created>2013-01-27</created><updated>2013-01-29</updated><authors><author><keyname>Ghatak</keyname><forenames>Anirban</forenames></author></authors><title>Subspace Codes for Random Networks Based on Pl\&quot;{u}cker Coordinates and
  Schubert Cells</title><categories>cs.IT math.IT</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Pl\&quot;{u}cker coordinate description of subspaces has been recently
discussed in the context of constant dimension subspace codes for random
networks, as well as the Schubert cell description of certain code parameters.
In this paper this classical tool is used to reformulate some standard
constructions of constant dimension codes so as to give a unified framework. A
general method of constructing non-constant dimension subspace codes with
respect to a given minimum subspace distance or minimum injection distance
among subspaces is presented. These codes may be described as the union of
constant dimension subspace codes restricted to selected Schubert cells. The
selection of these Schubert cells is based on the subset distance of tuples
corresponding to the Pl\&quot;{u}cker coordinate matrices associated with the
subspaces contained in the respective Schubert cells. In this context, it is
shown that a recent construction of non-constant dimension Ferrers-diagram
rank-metric subspace codes (Khaleghi and Kschischang) is subsumed in the
present framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6363</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6363</id><created>2013-01-27</created><authors><author><keyname>Helmling</keyname><forenames>Michael</forenames></author><author><keyname>Ruzika</keyname><forenames>Stefan</forenames></author></authors><title>Towards An Exact Combinatorial Algorithm for LP Decoding of Turbo Codes</title><categories>cs.IT math.IT</categories><comments>10 pages, 6 figures</comments><acm-class>E.4</acm-class><doi>10.1109/ISIT.2013.6620475</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel algorithm that solves the turbo code LP decoding problem
in a fininte number of steps by Euclidean distance minimizations, which in turn
rely on repeated shortest path computations in the trellis graph representing
the turbo code. Previous attempts to exploit the combinatorial graph structure
only led to algorithms which are either of heuristic nature or do not guarantee
finite convergence. A numerical study shows that our algorithm clearly beats
the running time, up to a factor of 100, of generic commercial LP solvers for
medium-sized codes, especially for high SNR values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6386</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6386</id><created>2013-01-27</created><authors><author><keyname>Zhang</keyname><forenames>Bowen</forenames></author><author><keyname>Baillieul</keyname><forenames>John</forenames></author></authors><title>A Two Level Feedback System Design to Regulation Service Provision</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Demand side management has gained increasing importance as the penetration of
renewable energy grows. Based on a Markov jump process modelling of a group of
thermostatic loads, this paper proposes a two level feedback system design
between the independent system operator (ISO) and the regulation service
provider such that two objectives are achieved: (1) the ISO can optimally
dispatch regulation signals to multiple providers in real time in order to
reduce the requirement for expensive spinning reserves, and (2) each regulation
provider can control its thermostatic loads to respond the ISO signal. It is
also shown that the amount of regulation service that can be provided is
implicitly restricted by a few fundamental parameters of the provider itself,
such as the allowable set point choice and its thermal constant. An interesting
finding is that the regulation provider's ability to provide a large amount of
long term accumulated regulation and short term signal tracking restrict each
other. Simulation results are presented to verify and illustrate the
performance of the proposed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6388</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6388</id><created>2013-01-27</created><authors><author><keyname>Haghighatshoar</keyname><forenames>Saeid</forenames></author><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author></authors><title>Polarization of the R\'enyi Information Dimension for Single and Multi
  Terminal Analog Compression</title><categories>cs.IT math.IT</categories><comments>16 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that the R\'enyi information dimension (RID) of an i.i.d.
sequence of mixture random variables polarizes to the extremal values of 0 and
1 (fully discrete and continuous distributions) when transformed by an Hadamard
matrix. This provides a natural counter-part over the reals of the entropy
polarization phenomenon over finite fields. It is further shown that the
polarization pattern of the RID is equivalent to the BEC polarization pattern,
which admits a closed form expression. These results are used to construct
universal and deterministic partial Hadamard matrices for analog to analog
(A2A) compression of random i.i.d. signals. In addition, a framework for the
A2A compression of multiple correlated signals is developed, providing a first
counter-part of the Slepian-Wolf coding problem in the A2A setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6393</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6393</id><created>2013-01-27</created><updated>2014-11-03</updated><authors><author><keyname>Ordentlich</keyname><forenames>Or</forenames></author><author><keyname>Erez</keyname><forenames>Uri</forenames></author></authors><title>Precoded Integer-Forcing Universally Achieves the MIMO Capacity to
  Within a Constant Gap</title><categories>cs.IT math.IT</categories><comments>to appear in the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An open-loop single-user multiple-input multiple-output communication scheme
is considered where a transmitter, equipped with multiple antennas, encodes the
data into independent streams all taken from the same linear code. The coded
streams are then linearly precoded using the encoding matrix of a perfect
linear dispersion space-time code. At the receiver side, integer-forcing
equalization is applied, followed by standard single-stream decoding. It is
shown that this communication architecture achieves the capacity of any
Gaussian multiple-input multiple-output channel up to a gap that depends only
on the number of transmit antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6397</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6397</id><created>2013-01-27</created><authors><author><keyname>Heindlmaier</keyname><forenames>Michael</forenames></author><author><keyname>Iscan</keyname><forenames>Onurcan</forenames></author><author><keyname>Rosanka</keyname><forenames>Christopher</forenames></author></authors><title>Scalar Quantize-and-Forward for Symmetric Half-duplex Two-Way Relay
  Channels</title><categories>cs.IT math.IT</categories><comments>Extended version of ISIT submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scalar Quantize &amp; Forward (QF) schemes are studied for the Two-Way Relay
Channel. Different QF approaches are compared in terms of rates as well as
relay and decoder complexity. A coding scheme not requiring Slepian-Wolf coding
at the relay is proposed and properties of the corresponding sum-rate
optimization problem are presented. A numerical scheme similar to the
Blahut-Arimoto algorithm is derived that guides optimized quantizer design. The
results are supported by simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6398</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6398</id><created>2013-01-27</created><authors><author><keyname>Koyuncu</keyname><forenames>Erdem</forenames></author><author><keyname>Jafarkhani</keyname><forenames>Hamid</forenames></author></authors><title>Variable-Length Channel Quantizers for Maximum Diversity and Array Gains</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a $t \times 1$ multiple-antenna fading channel with quantized
channel state information at the transmitter (CSIT). Our goal is to maximize
the diversity and array gains that are associated with the symbol error rate
(SER) performance of the system. It is well-known that for both beamforming and
precoding strategies, finite-rate fixed-length quantizers (FLQs) cannot achieve
the full-CSIT diversity and array gains. In this work, for any function
$f(P)\in\omega(1)$, we construct variable-length quantizers (VLQs) that can
achieve these full-CSIT gains with rates $1+(f(P) \log P)/P$ and $1+f(P)/P^t$
for the beamforming and precoding strategies, respectively, where $P$ is the
power constraint of the transmitter. We also show that these rates are the best
possible up to $o(1)$ multipliers in their $P$-dependent terms. In particular,
although the full-CSIT SER is not achievable at any (even infinite) feedback
rate, the full-CSIT diversity and array gains can be achieved with a feedback
rate of 1 bit per channel state asymptotically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6400</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6400</id><created>2013-01-27</created><authors><author><keyname>Skowron</keyname><forenames>Piotr</forenames></author><author><keyname>Faliszewski</keyname><forenames>Piotr</forenames></author><author><keyname>Slinko</keyname><forenames>Arkadii</forenames></author></authors><title>Achieving Fully Proportional Representation is Easy in Practice</title><categories>cs.MA cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide experimental evaluation of a number of known and new algorithms
for approximate computation of Monroe's and Chamberlin-Courant's rules. Our
experiments, conducted both on real-life preference-aggregation data and on
synthetic data, show that even very simple and fast algorithms can in many
cases find near-perfect solutions. Our results confirm and complement very
recent theoretical analysis of Skowron et al., who have shown good lower bounds
on the quality of (some of) the algorithms that we study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6406</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6406</id><created>2013-01-27</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Joint Power Adjustment and Interference Mitigation Techniques for
  Cooperative Spread Spectrum Systems</title><categories>cs.IT math.IT</categories><comments>6 figures. arXiv admin note: text overlap with arXiv:1301.0094</comments><journal-ref>MONET, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents joint power allocation and interference mitigation
techniques for the downlink of spread spectrum systems which employ multiple
relays and the amplify and forward cooperation strategy. We propose a joint
constrained optimization framework that considers the allocation of power
levels across the relays subject to an individual power constraint and the
design of linear receivers for interference suppression. We derive constrained
minimum mean-squared error (MMSE) expressions for the parameter vectors that
determine the optimal power levels across the relays and the linear receivers.
In order to solve the proposed optimization problem efficiently, we develop
joint adaptive power allocation and interference suppression algorithms that
can be implemented in a distributed fashion. The proposed stochastic gradient
(SG) and recursive least squares (RLS) algorithms mitigate the interference by
adjusting the power levels across the relays and estimating the parameters of
the linear receiver. SG and RLS channel estimation algorithms are also derived
to determine the coefficients of the channels across the base station, the
relays and the destination terminal. The results of simulations show that the
proposed techniques obtain significant gains in performance and capacity over
non-cooperative systems and cooperative schemes with equal power allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6408</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6408</id><created>2013-01-27</created><authors><author><keyname>Lomnitz</keyname><forenames>Yuval</forenames></author><author><keyname>Feder</keyname><forenames>Meir</forenames></author></authors><title>A Universal Probability Assignment for Prediction of Individual
  Sequences</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is it a good idea to use the frequency of events in the past, as a guide to
their frequency in the future (as we all do anyway)? In this paper the question
is attacked from the perspective of universal prediction of individual
sequences. It is shown that there is a universal sequential probability
assignment, such that for a large class loss functions (optimization goals),
the predictor minimizing the expected loss under this probability, is a good
universal predictor. The proposed probability assignment is based on randomly
dithering the empirical frequencies of states in the past, and it is easy to
show that randomization is essential. This yields a very simple universal
prediction scheme which is similar to Follow-the-Perturbed-Leader (FPL) and
works for a large class of loss functions, as well as a partial justification
for using probabilistic assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6410</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6410</id><created>2013-01-27</created><updated>2013-03-05</updated><authors><author><keyname>Bazzi</keyname><forenames>Louay</forenames></author><author><keyname>Ghazi</keyname><forenames>Badih</forenames></author><author><keyname>Urbanke</keyname><forenames>Rudiger</forenames></author></authors><title>Linear Programming Decoding of Spatially Coupled Codes</title><categories>cs.IT math.IT</categories><comments>37 pages; Added tightness construction, expanded abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a given family of spatially coupled codes, we prove that the LP threshold
on the BSC of the graph cover ensemble is the same as the LP threshold on the
BSC of the derived spatially coupled ensemble. This result is in contrast with
the fact that the BP threshold of the derived spatially coupled ensemble is
believed to be larger than the BP threshold of the graph cover ensemble as
noted by the work of Kudekar et al. (2011, 2012). To prove this, we establish
some properties related to the dual witness for LP decoding which was
introduced by Feldman et al. (2007) and simplified by Daskalakis et al. (2008).
More precisely, we prove that the existence of a dual witness which was
previously known to be sufficient for LP decoding success is also necessary and
is equivalent to the existence of certain acyclic hyperflows. We also derive a
sublinear (in the block length) upper bound on the weight of any edge in such
hyperflows, both for regular LPDC codes and for spatially coupled codes and we
prove that the bound is asymptotically tight for regular LDPC codes. Moreover,
we show how to trade crossover probability for &quot;LP excess&quot; on all the variable
nodes, for any binary linear code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6412</identifier>
 <datestamp>2013-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6412</id><created>2013-01-27</created><updated>2013-09-18</updated><authors><author><keyname>Farkas</keyname><forenames>L&#xf3;r&#xe1;nt</forenames></author><author><keyname>K&#xf3;i</keyname><forenames>Tam&#xe1;s</forenames></author></authors><title>Random Access and Source-Channel Coding Error Exponents for Multiple
  Access Channels</title><categories>cs.IT math.IT</categories><comments>This paper is submitted to IEEE transactions on information theory.
  It was presented in part at ISIT2013 (IEEE International Symposium on
  Information Theory, Istanbul)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new universal coding/decoding scheme for random access with collision
detection is given in the case of two senders. The result is used to give an
achievable joint source-channel coding error exponent for multiple access
channels in the case of independent sources. This exponent is improved in a
modified model that admits error free 0 rate communication between the senders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6422</identifier>
 <datestamp>2013-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6422</id><created>2013-01-27</created><updated>2013-05-01</updated><authors><author><keyname>Krishnan</keyname><forenames>B. Santhana</forenames></author><author><keyname>Ganesh</keyname><forenames>Ayalvadi</forenames></author><author><keyname>Manjunath</keyname><forenames>D.</forenames></author></authors><title>On Connectivity Thresholds in the Intersection of Random Key Graphs on
  Random Geometric Graphs</title><categories>cs.IT math.CO math.IT math.PR</categories><comments>Accepted for Publication at ISIT 2013. 5 Pages (main text) + 6 pages
  (appendix)</comments><acm-class>G.2.2; G.2.3; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a random key graph (RKG) of $n$ nodes each node is randomly assigned a key
ring of $K_n$ cryptographic keys from a pool of $P_n$ keys. Two nodes can
communicate directly if they have at least one common key in their key rings.
We assume that the $n$ nodes are distributed uniformly in $[0,1]^2.$ In
addition to the common key requirement, we require two nodes to also be within
$r_n$ of each other to be able to have a direct edge. Thus we have a random
graph in which the RKG is superposed on the familiar random geometric graph
(RGG). For such a random graph, we obtain tight bounds on the relation between
$K_n,$ $P_n$ and $r_n$ for the graph to be asymptotically almost surely
connected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6426</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6426</id><created>2013-01-27</created><authors><author><keyname>Koller</keyname><forenames>Christian</forenames></author><author><keyname>Haenggi</keyname><forenames>Martin</forenames></author><author><keyname>Kliewer</keyname><forenames>Joerg</forenames></author><author><keyname>Costello</keyname><forenames>Daniel J.</forenames><suffix>Jr</suffix></author></authors><title>Joint Design of Channel and Network Coding for Star Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Communications, Dec. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Channel coding alone is not sufficient to reliably transmit a message of
finite length $K$ from a source to one or more destinations as in, e.g., file
transfer. To ensure that no data is lost, it must be combined with rateless
erasure correcting schemes on a higher layer, such as a time-division multiple
access (TDMA) system paired with automatic repeat request (ARQ) or random
linear network coding (RLNC). We consider binary channel coding on a binary
symmetric channel (BSC) and q-ary RLNC for erasure correction in a star
network, where Y sources send messages to each other with the help of a central
relay. In this scenario RLNC has been shown to have a throughput advantage over
TDMA schemes as K and q tend to infinity. In this paper we focus on finite
block lengths and compare the expected throughputs of RLNC and TDMA. For a
total message length of K bits, which can be subdivided into blocks of smaller
size prior to channel coding, we obtain the channel coding rate and the number
of blocks that maximize the expected throughput of both RLNC and TDMA, and we
find that TDMA is more throughput-efficient for small message lengths K and
small q.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6427</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6427</id><created>2013-01-27</created><authors><author><keyname>Derpich</keyname><forenames>Milan S.</forenames></author><author><keyname>Silva</keyname><forenames>Eduardo I.</forenames></author><author><keyname>&#xd8;stergaard</keyname><forenames>Jan</forenames></author></authors><title>Fundamental Inequalities and Identities Involving Mutual and Directed
  Informations in Closed-Loop Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. on Information Theory</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present several novel identities and inequalities relating the mutual
information and the directed information in systems with feedback. The internal
blocks within such systems are restricted only to be causal mappings, but are
allowed to be non-linear, stochastic and time varying. Moreover, the involved
signals can be arbitrarily distributed. We bound the directed information
between signals inside the feedback loop by the mutual information between
signals inside and outside the feedback loop. This fundamental result has an
interesting interpretation as a law of conservation of information flow.
Building upon it, we derive several novel identities and inequalities, which
allow us to prove some existing information inequalities under less restrictive
assumptions. Finally, we establish new relationships between nested directed
informations inside a feedback loop. This yields a new and general
data-processing inequality for systems with feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6428</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6428</id><created>2013-01-27</created><authors><author><keyname>Sokol</keyname><forenames>Shoshana Marcus Dina</forenames></author></authors><title>Engineering Small Space Dictionary Matching</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dictionary matching problem is to locate occurrences of any pattern among
a set of patterns in a given text. Massive data sets abound and at the same
time, there are many settings in which working space is extremely limited. We
introduce dictionary matching software for the space-constrained environment
whose running time is close to linear. We use the compressed suffix tree as the
underlying data structure of our algorithm, thus, the working space of our
algorithm is proportional to the optimal compression of the dictionary. We also
contribute a succinct tool for performing constant-time lowest marked ancestor
queries on a tree that is succinctly encoded as a sequence of balanced
parentheses, with linear time preprocessing of the tree. This tool should be
useful in many other applications. Our source code is available at
http://www.sci.brooklyn.cuny.edu/~sokol/dictmatch.html
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6431</identifier>
 <datestamp>2014-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6431</id><created>2013-01-27</created><updated>2014-05-23</updated><authors><author><keyname>Kouvaros</keyname><forenames>Panagiotis</forenames></author><author><keyname>Lomuscio</keyname><forenames>Alessio</forenames></author></authors><title>Automatic Verification of Parameterised Interleaved Multi-Agent Systems</title><categories>cs.MA cs.LO</categories><comments>8 pages with 1 figure; Published in the Proceedings of the 12th
  International Conference on Autonomous Agents and Multi-Agent systems
  (AAMAS13). Saint Paul, MN, USA</comments><acm-class>D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key problem in verification of multi-agent systems by model checking
concerns the fact that the state-space of the system grows exponentially with
the number of agents present. This makes practical model checking unfeasible
whenever the system contains more than a few agents. In this paper we put
forward a technique to establish a cutoff result, thereby showing that all
systems of arbitrary number of agents can be verified by model checking a
single system containing a number of agents equal to the cutoff of the system.
While this problem is undecidable in general, we here define a class of
parameterised interpreted systems and a parameterised temporal-epistemic logic
for which the result can be shown. We exemplify the theoretical results on a
robotic example and present an implementation of the technique on top of mcmas,
an open-source model checker for multi-agent systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6433</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6433</id><created>2013-01-27</created><authors><author><keyname>Dau</keyname><forenames>Son Hoang</forenames></author><author><keyname>Dong</keyname><forenames>Zheng</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Chan</keyname><forenames>Terence H.</forenames></author></authors><title>Delay Minimization in Varying-Bandwidth Direct Multicast with Side
  Information</title><categories>cs.IT math.CO math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the delay minimization in a direct multicast communication scheme
where a base station wishes to transmit a set of original packets to a group of
clients. Each of the clients already has in its cache a subset of the original
packets, and requests for all the remaining packets. The base station
communicates directly with the clients by broadcasting information to them.
Assume that bandwidths vary between the station and different clients. We
propose a method to minimize the total delay required for the base station to
satisfy requests from all clients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6447</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6447</id><created>2013-01-27</created><authors><author><keyname>Fawaz</keyname><forenames>Nadia</forenames></author><author><keyname>Muthukrishnan</keyname><forenames>S.</forenames></author><author><keyname>Nikolov</keyname><forenames>Aleksandar</forenames></author></authors><title>Nearly Optimal Private Convolution</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study computing the convolution of a private input $x$ with a public input
$h$, while satisfying the guarantees of $(\epsilon, \delta)$-differential
privacy. Convolution is a fundamental operation, intimately related to Fourier
Transforms. In our setting, the private input may represent a time series of
sensitive events or a histogram of a database of confidential personal
information. Convolution then captures important primitives including linear
filtering, which is an essential tool in time series analysis, and aggregation
queries on projections of the data.
  We give a nearly optimal algorithm for computing convolutions while
satisfying $(\epsilon, \delta)$-differential privacy. Surprisingly, we follow
the simple strategy of adding independent Laplacian noise to each Fourier
coefficient and bounding the privacy loss using the composition theorem of
Dwork, Rothblum, and Vadhan. We derive a closed form expression for the optimal
noise to add to each Fourier coefficient using convex programming duality. Our
algorithm is very efficient -- it is essentially no more computationally
expensive than a Fast Fourier Transform.
  To prove near optimality, we use the recent discrepancy lowerbounds of
Muthukrishnan and Nikolov and derive a spectral lower bound using a
characterization of discrepancy in terms of determinants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6449</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6449</id><created>2013-01-28</created><updated>2013-01-31</updated><authors><author><keyname>Basciftci</keyname><forenames>Y. Ozan</forenames></author><author><keyname>Koksal</keyname><forenames>C. Emre</forenames></author><author><keyname>Ozguner</keyname><forenames>Fusun</forenames></author></authors><title>To Obtain or not to Obtain CSI in the Presence of Hybrid Adversary</title><categories>cs.IT cs.CR math.IT</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the wiretap channel model under the presence of a hybrid, half
duplex adversary that is capable of either jamming or eavesdropping at a given
time. We analyzed the achievable rates under a variety of scenarios involving
different methods for obtaining transmitter CSI. Each method provides a
different grade of information, not only to the transmitter on the main
channel, but also to the adversary on all channels. Our analysis shows that
main CSI is more valuable for the adversary than the jamming CSI in both
delay-limited and ergodic scenarios. Similarly, in certain cases under the
ergodic scenario, interestingly, no CSI may lead to higher achievable secrecy
rates than with CSI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6453</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6453</id><created>2013-01-28</created><authors><author><keyname>Hong</keyname><forenames>Song-Nam</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Structured Lattice Codes for 2 \times 2 \times 2 MIMO Interference
  Channel</title><categories>cs.IT math.IT</categories><comments>submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the 2\times 2\times 2 multiple-input multipleoutput interference
channel where two source-destination pairs wish to communicate with the aid of
two intermediate relays. In this paper, we propose a novel lattice strategy
called Aligned Precoded Compute-and-Forward (PCoF). This scheme consists of two
phases: 1) Using the CoF framework based on signal alignment we transform the
Gaussian network into a deterministic finite field network. 2) Using linear
precoding (over finite field) we eliminate the end-to-end interference in the
finite field domain. Further, we exploit the algebraic structure of lattices to
enhance the performance at finite SNR, such that beyond a degree of freedom
result (also achievable by other means). We can also show that Aligned PCoF
outperforms time sharing in a range of reasonably moderate SNR, with increasing
gain as SNR increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6454</identifier>
 <datestamp>2014-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6454</id><created>2013-01-28</created><updated>2014-04-03</updated><authors><author><keyname>Aistleitner</keyname><forenames>Christoph</forenames></author></authors><title>On the limit distribution of the normality measure of random binary
  sequences</title><categories>math.CO cs.DM math.NT</categories><comments>Second version. Some minor changes. To appear in the Bulletin of the
  London Mathematical Society</comments><msc-class>68R15, 11K45, 60C05, 60F05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the existence of a limit distribution for the normalized normality
measure $\mathcal{N}(E_N)/\sqrt{N}$ (as $N \to \infty$) for random binary
sequences $E_N$, by this means confirming a conjecture of Alon, Kohayakawa,
Mauduit, Moreira and R{\&quot;o}dl. The key point of the proof is to approximate the
distribution of the normality measure by the exiting probabilities of a
multidimensional Wiener process from a certain polytope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6456</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6456</id><created>2013-01-28</created><updated>2015-06-16</updated><authors><author><keyname>Pai</keyname><forenames>Srikanth B.</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>A Singleton Bound for Lattice Schemes</title><categories>cs.IT math.IT</categories><comments>17 pages, 8 figures. Submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive a Singleton bound for lattice schemes and obtain
Singleton bounds known for binary codes and subspace codes as special cases. It
is shown that the modular structure affects the strength of the Singleton
bound. We also obtain a new upper bound on the code size for non-constant
dimension codes. The plots of this bound along with plots of the code sizes of
known non-constant dimension codes in the literature reveal that our bound is
tight for certain parameters of the code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6465</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6465</id><created>2013-01-28</created><updated>2013-05-19</updated><authors><author><keyname>Harremo&#xeb;s</keyname><forenames>Peter</forenames></author></authors><title>Extendable MDL</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>9 pages</comments><msc-class>62B10, 94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that combination of the minimum description length
principle and a exchange-ability condition leads directly to the use of
Jeffreys prior. This approach works in most cases even when Jeffreys prior
cannot be normalized. Kraft's inequality links codes and distributions but a
closer look at this inequality demonstrates that this link only makes sense
when sequences are considered as prefixes of potential longer sequences. For
technical reasons only results for exponential families are stated. Results on
when Jeffreys prior can be normalized after conditioning on a initializing
string are given. An exotic case where no initial string allow Jeffreys prior
to be normalized is given and some way of handling such exotic cases are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6467</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6467</id><created>2013-01-28</created><updated>2014-12-24</updated><authors><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author><author><keyname>Kuzuoka</keyname><forenames>Shigeaki</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>Non-Asymptotic and Second-Order Achievability Bounds for Coding With
  Side-Information</title><categories>cs.IT math.IT</categories><comments>32 pages (two column), 8 figures, v2 fixed some minor errors in the
  WZ problem, v2 included cost constraint in the GP problem, v3 added
  cardinality bounds, v4 fixed an error of the numerical calculation in the GP
  problem, v5 is an accepted version for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present novel non-asymptotic or finite blocklength achievability bounds
for three side-information problems in network information theory. These
include (i) the Wyner-Ahlswede-Korner (WAK) problem of almost-lossless source
coding with rate-limited side-information, (ii) the Wyner-Ziv (WZ) problem of
lossy source coding with side-information at the decoder and (iii) the
Gel'fand-Pinsker (GP) problem of channel coding with noncausal state
information available at the encoder. The bounds are proved using ideas from
channel simulation and channel resolvability. Our bounds for all three problems
improve on all previous non-asymptotic bounds on the error probability of the
WAK, WZ and GP problems--in particular those derived by Verdu. Using our novel
non-asymptotic bounds, we recover the general formulas for the optimal rates of
these side-information problems. Finally, we also present achievable
second-order coding rates by applying the multidimensional Berry-Esseen theorem
to our new non-asymptotic bounds. Numerical results show that the second-order
coding rates obtained using our non-asymptotic achievability bounds are
superior to those obtained using existing finite blocklength bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6471</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6471</id><created>2013-01-28</created><authors><author><keyname>Aktas</keyname><forenames>Tugcan</forenames></author><author><keyname>Yilmaz</keyname><forenames>Ali Ozgur</forenames></author><author><keyname>Aktas</keyname><forenames>Emre</forenames></author></authors><title>Generalizing the Sampling Property of the Q-function for Error Rate
  Analysis of Cooperative Communication in Fading Channels</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures, Submitted to IEEE International Symposium on
  Information Theory, ISIT 2013, Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends some approximation methods that are used to identify
closed form Bit Error Rate (BER) expressions which are frequently utilized in
investigation and comparison of performance for wireless communication systems
in the literature. By using this group of approximation methods, some
expectation integrals, which are complicated to analyze and have high
computational complexity to evaluate through Monte Carlo simulations, are
computed. For these integrals, by using the sampling property of the integrand
functions of one or more arguments, reliable BER expressions revealing the
diversity and coding gains are derived. Although the methods we present are
valid for a larger class of integration problems, in this work we show the step
by step derivation of the BER expressions for a canonical cooperative
communication scenario in addition to a network coded system starting from
basic building blocks. The derived expressions agree with the simulation
results for a very wide range of signal-to-noise ratio (SNR) values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6473</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6473</id><created>2013-01-28</created><authors><author><keyname>Gregori</keyname><forenames>Maria</forenames></author><author><keyname>Payar&#xf3;</keyname><forenames>Miquel</forenames></author></authors><title>On the precoder design of a wireless energy harvesting node in linear
  vector Gaussian channels with arbitrary input distribution</title><categories>cs.IT math.IT</categories><comments>Extended version of a manuscript accepted in IEEE Transactions on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Wireless Energy Harvesting Node (WEHN) operating in linear vector Gaussian
channels with arbitrarily distributed input symbols is considered in this
paper. The precoding strategy that maximizes the mutual information along N
independent channel accesses is studied under non-causal knowledge of the
channel state and harvested energy (commonly known as offline approach). It is
shown that, at each channel use, the left singular vectors of the precoder are
equal to the eigenvectors of the Gram channel matrix. Additionally, an
expression that relates the optimal singular values of the precoder with the
energy harvesting profile through the Minimum Mean-Square Error (MMSE) matrix
is obtained. Then, the specific situation in which the right singular vectors
of the precoder are set to the identity matrix is considered. In this scenario,
the optimal offline power allocation, named Mercury Water-Flowing, is derived
and an intuitive graphical representation is presented. Two optimal offline
algorithms to compute the Mercury Water- Flowing solution are proposed and an
exhaustive study of their computational complexity is performed. Moreover, an
online algorithm is designed, which only uses causal knowledge of the harvested
energy and channel state. Finally, the achieved mutual information is evaluated
through simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6479</identifier>
 <datestamp>2013-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6479</id><created>2013-01-28</created><updated>2013-06-06</updated><authors><author><keyname>Bienvenu</keyname><forenames>Meghyn</forenames></author><author><keyname>Cate</keyname><forenames>Balder ten</forenames></author><author><keyname>Lutz</keyname><forenames>Carsten</forenames></author><author><keyname>Wolter</keyname><forenames>Frank</forenames></author></authors><title>Ontology-based Data Access: A Study through Disjunctive Datalog, CSP,
  and MMSNP</title><categories>cs.DB cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ontology-based data access is concerned with querying incomplete data sources
in the presence of domain-specific knowledge provided by an ontology. A central
notion in this setting is that of an ontology-mediated query, which is a
database query coupled with an ontology. In this paper, we study several
classes of ontology-mediated queries, where the database queries are given as
some form of conjunctive query and the ontologies are formulated in description
logics or other relevant fragments of first-order logic, such as the guarded
fragment and the unary-negation fragment. The contributions of the paper are
three-fold. First, we characterize the expressive power of ontology-mediated
queries in terms of fragments of disjunctive datalog. Second, we establish
intimate connections between ontology-mediated queries and constraint
satisfaction problems (CSPs) and their logical generalization, MMSNP formulas.
Third, we exploit these connections to obtain new results regarding (i)
first-order rewritability and datalog-rewritability of ontology-mediated
queries, (ii) P/NP dichotomies for ontology-mediated queries, and (iii) the
query containment problem for ontology-mediated queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6484</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6484</id><created>2013-01-28</created><authors><author><keyname>Weber</keyname><forenames>Jos H.</forenames></author><author><keyname>Immink</keyname><forenames>Kees A. Schouhamer</forenames></author><author><keyname>Siegel</keyname><forenames>Paul H.</forenames></author><author><keyname>Swart</keyname><forenames>Theo G.</forenames></author></authors><title>Perspectives on Balanced Sequences</title><categories>cs.IT math.IT</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine and compare several different classes of &quot;balanced&quot; block codes
over q-ary alphabets, namely symbol-balanced (SB) codes, charge-balanced (CB)
codes, and polarity-balanced (PB) codes. Known results on the maximum size and
asymptotic minimal redundancy of SB and CB codes are reviewed. We then
determine the maximum size and asymptotic minimal redundancy of PB codes and of
codes which are both CB and PB. We also propose efficient Knuth-like encoders
and decoders for all these types of balanced codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6491</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6491</id><created>2013-01-28</created><updated>2013-05-16</updated><authors><author><keyname>Keeler</keyname><forenames>Holger Paul</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Blaszczyszyn</keyname><forenames>Bartlomiej</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Karray</keyname><forenames>Mohamed Kadhem</forenames><affiliation>FT RD</affiliation></author></authors><title>SINR-based k-coverage probability in cellular networks with arbitrary
  shadowing</title><categories>cs.NI cs.IT math.IT math.PR</categories><proxy>ccsd</proxy><doi>10.1109/ISIT.2013.6620410</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give numerically tractable, explicit integral expressions for the
distribution of the signal-to-interference-and-noise-ratio (SINR) experienced
by a typical user in the down-link channel from the k-th strongest base
stations of a cellular network modelled by Poisson point process on the plane.
Our signal propagation-loss model comprises of a power-law path-loss function
with arbitrarily distributed shadowing, independent across all base stations,
with and without Rayleigh fading. Our results are valid in the whole domain of
SINR, in particular for SINR&lt;1, where one observes multiple coverage. In this
latter aspect our paper complements previous studies reported in [Dhillon et
al. JSAC 2012].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6512</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6512</id><created>2013-01-28</created><updated>2013-04-03</updated><authors><author><keyname>Mohapatra</keyname><forenames>Parthajit</forenames></author><author><keyname>Murthy</keyname><forenames>Chandra R.</forenames></author></authors><title>Secrecy in the 2-User Symmetric Deterministic Interference Channel with
  Transmitter Cooperation</title><categories>cs.IT math.IT</categories><comments>5 pages, submitted to SPAWC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents novel achievable schemes for the 2-user symmetric linear
deterministic interference channel with limited-rate transmitter cooperation
and perfect secrecy constraints at the receivers. The proposed achievable
scheme consists of a combination of interference cancelation, relaying of the
other user's data bits, time sharing, and transmission of random bits,
depending on the rate of the cooperative link and the relative strengths of the
signal and the interference. The results show, for example, that the proposed
scheme achieves the same rate as the capacity without the secrecy constraints,
in the initial part of the weak interference regime. Also, sharing random bits
through the cooperative link can achieve a higher secrecy rate compared to
sharing data bits, in the very high interference regime. The results highlight
the importance of limited transmitter cooperation in facilitating secure
communications over 2-user interference channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6520</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6520</id><created>2013-01-28</created><updated>2013-05-16</updated><authors><author><keyname>Stavrou</keyname><forenames>Photios A.</forenames></author><author><keyname>Charalambous</keyname><forenames>Charalambos D.</forenames></author></authors><title>Variational Equalities of Directed Information and Applications</title><categories>cs.IT math.IT</categories><comments>5 pages, to appear in proceedings of International Symposium on
  Information Theory (ISIT), 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce two variational equalities of directed
information, which are analogous to those of mutual information employed in the
Blahut-Arimoto Algorithm (BAA). Subsequently, we introduce nonanticipative Rate
Distortion Function (RDF) ${R}^{na}_{0,n}(D)$ defined via directed information
introduced in [1], and we establish its equivalence to Gorbunov-Pinsker's
nonanticipatory $\epsilon$-entropy $R^{\varepsilon}_{0,n}(D)$. By invoking
certain results we first establish existence of the infimizing reproduction
distribution for ${R}^{na}_{0,n}(D)$, and then we give its implicit form for
the stationary case. Finally, we utilize one of the variational equalities and
the closed form expression of the optimal reproduction distribution to provide
an algorithm for the computation of ${R}^{na}_{0,n}(D)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6522</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6522</id><created>2013-01-28</created><authors><author><keyname>Stavrou</keyname><forenames>Photios A.</forenames></author><author><keyname>Charalambous</keyname><forenames>Charalambos D.</forenames></author><author><keyname>Kourtellaris</keyname><forenames>Christos K.</forenames></author></authors><title>Optimal Nonstationary Reproduction Distribution for Nonanticipative RDF
  on Abstract Alphabets</title><categories>cs.IT cs.SY math.IT</categories><comments>5 pages, submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a definition for nonanticipative Rate Distortion
Function (RDF) on abstract alphabets, and we invoke weak convergence of
probability measures to show various of its properties, such as, existence of
the optimal reproduction conditional distribution, compactness of the fidelity
set, lower semicontinuity of the RDF functional, etc. Further, we derive the
closed form expression of the optimal nonstationary reproduction distribution.
This expression is computed recursively backward in time. Throughout the paper
we point out an operational meaning of the nonanticipative RDF by recalling the
coding theorem derive in \cite{tatikonda2000}, and we state relations to
Gorbunov-Pinsker's nonanticipatory $\epsilon-$entropy \cite{gorbunov-pinsker}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6529</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6529</id><created>2013-01-28</created><updated>2013-11-29</updated><authors><author><keyname>Nielsen</keyname><forenames>Johan S. R.</forenames></author></authors><title>Generalised Multi-sequence Shift-Register Synthesis using Module
  Minimisation</title><categories>cs.IT math.IT</categories><comments>Version with full proofs. Fixed some typos in algorithm from last
  version. Presented at ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to solve a generalised version of the Multi-sequence Linear
Feedback Shift-Register (MLFSR) problem using minimisation of free modules over
$\mathbb F[x]$. We show how two existing algorithms for minimising such modules
run particularly fast on these instances. Furthermore, we show how one of them
can be made even faster for our use. With our modeling of the problem,
classical algebraic results tremendously simplify arguing about the algorithms.
For the non-generalised MLFSR, these algorithms are as fast as what is
currently known. We then use our generalised MLFSR to give a new fast decoding
algorithm for Reed Solomon codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6532</identifier>
 <datestamp>2014-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6532</id><created>2013-01-28</created><updated>2014-06-28</updated><authors><author><keyname>Dutta</keyname><forenames>Dushyanta</forenames></author><author><keyname>Saikia</keyname><forenames>Dilip Kr.</forenames></author><author><keyname>Karmakar</keyname><forenames>Arindam</forenames></author></authors><title>Analysis of IEEE 802.15.4 MAC under low duty cycle</title><categories>cs.NI</categories><comments>This paper has been withdrawn by the author due to some crucial
  errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this Letter, a discrete-time Markov Chain model is developed to study
performance of IEEE 802.15.4 under low duty cycle. The performance is measured
in terms of aggregate throughput and average power consumption per packet. The
proposed analytical model is verified through ns2 simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6553</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6553</id><created>2013-01-28</created><authors><author><keyname>Couronne</keyname><forenames>Thomas</forenames></author><author><keyname>Smoreda</keyname><forenames>Zbigniew</forenames></author><author><keyname>Olteanu</keyname><forenames>Ana-Maria</forenames></author></authors><title>Chatty Mobiles:Individual mobility and communication patterns</title><categories>cs.CY</categories><comments>NetMob 2011, Boston</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human mobility analysis is an important issue in social sciences, and
mobility data are among the most sought-after sources of information in ur-
Data ban studies, geography, transportation and territory management. In
network sciences mobility studies have become popular in the past few years,
especially using mobile phone location data. For preserving the customer
privacy, datasets furnished by telecom operators are anonymized. At the same
time, the large size of datasets often makes the task of calculating all
observed trajectories very difficult and time-consuming. One solution could be
to sample users. However, the fact of not having information about the mobile
user makes the sampling delicate. Some researchers select randomly a sample of
users from their dataset. Others try to optimize this method, for example,
taking into account only users with a certain number or frequency of locations
recorded. At the first glance, the second choice seems to be more efficient:
having more individual traces makes the analysis more precise. However, the
most frequently used CDR data (Call Detail Records) have location generated
only at the moment of communication (call, SMS, data connection). Due to this
fact, users mobility patterns cannot be precisely built upon their
communication patterns. Hence, these data have evident short-comings both in
terms of spatial and temporal scale. In this paper we propose to estimate the
correlation between the users communication and mo- bility in order to better
assess the bias of frequency based sampling. Using technical GSM network data
(including communication but also independent mobility records), we will
analyze the relationship between communication and mobility patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6568</identifier>
 <datestamp>2013-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6568</id><created>2013-01-28</created><updated>2013-05-04</updated><authors><author><keyname>Glen</keyname><forenames>Amy</forenames></author><author><keyname>Simpson</keyname><forenames>Jamie</forenames></author></authors><title>The total run length of a word</title><categories>math.CO cs.DM</categories><comments>11 pages; minor corrections; accepted for publication in Theoretical
  Computer Science</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A run in a word is a periodic factor whose length is at least twice its
period and which cannot be extended to the left or right (by a letter) to a
factor with greater period. In recent years a great deal of work has been done
on estimating the maximum number of runs that can occur in a word of length
$n$. A number of associated problems have also been investigated. In this paper
we consider a new variation on the theme. We say that the total run length
(TRL) of a word is the sum of the lengths of the runs in the word and that
$\tau(n)$ is the maximum TRL over all words of length $n$. We show that $n^2/8
&lt; \tau(n) &lt; 47n^2/72 + 2n$ for all $n$. We also give a formula for the average
total run length of words of length $n$ over an alphabet of size $\alpha$, and
some other results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6572</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6572</id><created>2013-01-28</created><authors><author><keyname>Geeraerts</keyname><forenames>Gilles</forenames></author><author><keyname>Heu&#xdf;ner</keyname><forenames>Alexander</forenames></author><author><keyname>Praveen</keyname><forenames>M.</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>{\omega}-Petri nets</title><categories>cs.LO</categories><comments>37 pages, 6 figures, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce {\omega}-Petri nets ({\omega}PN), an extension of plain Petri
nets with {\omega}-labeled input and output arcs, that is well-suited to
analyse parametric concurrent systems with dynamic thread creation. Most
techniques (such as the Karp and Miller tree or the Rackoff technique) that
have been proposed in the setting of plain Petri nets do not apply directly to
{\omega}PN because {\omega}PN define transition systems that have infinite
branching. This motivates a thorough analysis of the computational aspects of
{\omega}PN. We show that an {\omega}PN can be turned into an plain Petri net
that allows to recover the reachability set of the {\omega}PN, but that does
not preserve termination. This yields complexity bounds for the reachability,
(place) boundedness and coverability problems on {\omega}PN. We provide a
practical algorithm to compute a coverability set of the {\omega}PN and to
decide termination by adapting the classical Karp and Miller tree construction.
We also adapt the Rackoff technique to {\omega}PN, to obtain the exact
complexity of the termination problem. Finally, we consider the extension of
{\omega}PN with reset and transfer arcs, and show how this extension impacts
the decidability and complexity of the aforementioned problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6574</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6574</id><created>2013-01-28</created><authors><author><keyname>Couronne</keyname><forenames>Thomas</forenames></author><author><keyname>Beuscart</keyname><forenames>Jean-Samuel</forenames></author><author><keyname>Chamayou</keyname><forenames>Cedric</forenames></author></authors><title>Self-Organizing Map and social networks: Unfolding online social
  popularity</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present study uses the Kohonen self organizing map (SOM) to represent the
popularity patterns of Myspace music artists from their attributes on the
platform and their position in the social network. The method is applied to
cluster the profiles (the nodes of the social network) and the best friendship
links (the edges). It shows that the SOM is an efficient tool to interpret the
complex links between the audience and the influence of the musicians. It
finally provides a robust classifier of the online social network behaviors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6587</identifier>
 <datestamp>2013-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6587</id><created>2013-01-28</created><updated>2013-04-30</updated><authors><author><keyname>Rodolakis</keyname><forenames>Georgios</forenames></author></authors><title>Information Theoretic Cut-set Bounds on the Capacity of Poisson Wireless
  Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a stochastic geometry model for the investigation of
fundamental information theoretic limitations in wireless networks. We derive a
new unified multi-parameter cut-set bound on the capacity of networks of
arbitrary Poisson node density, size, power and bandwidth, under fast fading in
a rich scattering environment. In other words, we upper-bound the optimal
performance in terms of total communication rate, under any scheme, that can be
achieved between a subset of network nodes (defined by the cut) with all the
remaining nodes. Additionally, we identify four different operating regimes,
depending on the magnitude of the long-range and short-range signal to noise
ratios. Thus, we confirm previously known scaling laws (e.g., in bandwidth
and/or power limited wireless networks), and we extend them with specific
bounds. Finally, we use our results to provide specific numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6588</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6588</id><created>2013-01-28</created><authors><author><keyname>Delfosse</keyname><forenames>Nicolas</forenames></author></authors><title>Tradeoffs for reliable quantum information storage in surface codes and
  color codes</title><categories>quant-ph cs.IT math.IT</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The family of hyperbolic surface codes is one of the rare families of quantum
LDPC codes with non-zero rate and unbounded minimum distance. First, we
introduce a family of hyperbolic color codes. This produces a new family of
quantum LDPC codes with non-zero rate and with minimum distance logarithmic in
the blocklength. Second, we study the tradeoff between the length n, the number
of encoded qubits k and the distance d of surface codes and color codes. We
prove that kd^2 is upper bounded by C(log k)^2n, where C is a constant that
depends only on the row weight of the parity-check matrix. Our results prove
that the best asymptotic minimum distance of LDPC surface codes and color codes
with non-zero rate is logarithmic in the length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6589</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6589</id><created>2013-01-28</created><updated>2015-08-23</updated><authors><author><keyname>Huang</keyname><forenames>Yu-Chih</forenames></author><author><keyname>Niesen</keyname><forenames>Urs</forenames></author><author><keyname>Gupta</keyname><forenames>Piyush</forenames></author></authors><title>Energy-Efficient Communication in the Presence of Synchronization Errors</title><categories>cs.IT math.IT</categories><comments>23 pages, to appear in IEEE Transactions on Information Theory</comments><journal-ref>IEEE Transactions on Information Theory, vol. 61, pp. 6131 - 6144,
  November 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication systems are traditionally designed to have tight
transmitter-receiver synchronization. This requirement has negligible overhead
in the high-SNR regime. However, in many applications, such as wireless sensor
networks, communication needs to happen primarily in the energy-efficient
regime of low SNR, where requiring tight synchronization can be highly
suboptimal.
  In this paper, we model the noisy channel with synchronization errors as an
insertion/deletion/substitution channel. For this channel, we propose a new
communication scheme that requires only loose transmitter-receiver
synchronization. We show that the proposed scheme is asymptotically optimal for
the Gaussian channel with synchronization errors in terms of energy efficiency
as measured by the rate per unit energy. In the process, we also establish that
the lack of synchronization causes negligible loss in energy efficiency. We
further show that, for a general discrete memoryless channel with
synchronization errors and a general input cost function admitting a zero-cost
symbol, the rate per unit cost achieved by the proposed scheme is within a
factor two of the information-theoretic optimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6591</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6591</id><created>2013-01-28</created><authors><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author></authors><title>PDF articles metadata harvester</title><categories>cs.DL cs.IR</categories><comments>6 Pages, 9 images, 1 table</comments><journal-ref>Jurnal Komputer dan Informatika (JKI). 10 (2012) 1-7</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific journals are very important in recording the finding from
researchers around the world. The recent media to disseminate scientific
journals is PDF. On scheme to find the scientific journals over the internet is
via metadata. Metadata stores information about article summary. Embedding
metadata into PDF of scientific article will grant the consistency of metadata
readness. Harvesting the metadata from scientific journal is very interesting
field at the moment. This paper will discuss about scientific journal metadata
harvesters involving XMP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6599</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6599</id><created>2013-01-28</created><updated>2013-05-08</updated><authors><author><keyname>Rahmati</keyname><forenames>Mojtaba</forenames></author><author><keyname>Duman</keyname><forenames>Tolga M.</forenames></author></authors><title>An Upper Bound on the Capacity of non-Binary Deletion Channels</title><categories>cs.IT math.IT</categories><comments>accepted for presentation in ISIT 2013</comments><doi>10.1109/ISIT.2013.6620764</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive an upper bound on the capacity of non-binary deletion channels.
Although binary deletion channels have received significant attention over the
years, and many upper and lower bounds on their capacity have been derived,
such studies for the non-binary case are largely missing. The state of the art
is the following: as a trivial upper bound, capacity of an erasure channel with
the same input alphabet as the deletion channel can be used, and as a lower
bound the results by Diggavi and Grossglauser are available. In this paper, we
derive the first non-trivial non-binary deletion channel capacity upper bound
and reduce the gap with the existing achievable rates. To derive the results we
first prove an inequality between the capacity of a 2K-ary deletion channel
with deletion probability $d$, denoted by $C_{2K}(d)$, and the capacity of the
binary deletion channel with the same deletion probability, $C_2(d)$, that is,
$C_{2K}(d)\leq C_2(d)+(1-d)\log(K)$. Then by employing some existing upper
bounds on the capacity of the binary deletion channel, we obtain upper bounds
on the capacity of the 2K-ary deletion channel. We illustrate via examples the
use of the new bounds and discuss their asymptotic behavior as $d \rightarrow
0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6600</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6600</id><created>2013-01-28</created><authors><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Glineur</keyname><forenames>Francois</forenames></author><author><keyname>Louveaux</keyname><forenames>Jerome</forenames></author><author><keyname>Vandendorpe</keyname><forenames>Luc</forenames></author></authors><title>Weighted Sum Rate Maximization for Downlink OFDMA with Subcarrier-pair
  based Opportunistic DF Relaying</title><categories>cs.SY</categories><comments>8 figures, accepted and to be published in IEEE Transactions on
  Signal Processing. arXiv admin note: text overlap with arXiv:1301.2935</comments><doi>10.1109/TSP.2013.2245326</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses a weighted sum rate (WSR) maximization problem for
downlink OFDMA aided by a decode-and-forward (DF) relay under a total power
constraint. A novel subcarrier-pair based opportunistic DF relaying protocol is
proposed. Specifically, user message bits are transmitted in two time slots. A
subcarrier in the first slot can be paired with a subcarrier in the second slot
for the DF relay-aided transmission to a user. In particular, the source and
the relay can transmit simultaneously to implement beamforming at the
subcarrier in the second slot. Each unpaired subcarrier in either the first or
second slot is used for the source's direct transmission to a user. A benchmark
protocol, same as the proposed one except that the transmit beamforming is not
used for the relay-aided transmission, is also considered. For each protocol, a
polynomial-complexity algorithm is developed to find at least an approximately
optimum resource allocation (RA), by using continuous relaxation, the dual
method, and Hungarian algorithm. Instrumental to the algorithm design is an
elegant definition of optimization variables, motivated by the idea of
regarding the unpaired subcarriers as virtual subcarrier pairs in the direct
transmission mode. The effectiveness of the RA algorithm and the impact of
relay position and total power on the protocols' performance are illustrated by
numerical experiments. The proposed protocol always leads to a maximum WSR
equal to or greater than that for the benchmark one, and the performance gain
of using the proposed one is significant especially when the relay is in close
proximity to the source and the total power is low. Theoretical analysis is
presented to interpret these observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6626</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6626</id><created>2013-01-28</created><authors><author><keyname>Kong</keyname><forenames>Xiangnan</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author><author><keyname>Wang</keyname><forenames>Xue</forenames></author><author><keyname>Ragin</keyname><forenames>Ann B.</forenames></author></authors><title>Discriminative Feature Selection for Uncertain Graph Classification</title><categories>cs.LG cs.DB stat.ML</categories><msc-class>H.2.8 Database Management, Database Applications-Data Mining</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mining discriminative features for graph data has attracted much attention in
recent years due to its important role in constructing graph classifiers,
generating graph indices, etc. Most measurement of interestingness of
discriminative subgraph features are defined on certain graphs, where the
structure of graph objects are certain, and the binary edges within each graph
represent the &quot;presence&quot; of linkages among the nodes. In many real-world
applications, however, the linkage structure of the graphs is inherently
uncertain. Therefore, existing measurements of interestingness based upon
certain graphs are unable to capture the structural uncertainty in these
applications effectively. In this paper, we study the problem of discriminative
subgraph feature selection from uncertain graphs. This problem is challenging
and different from conventional subgraph mining problems because both the
structure of the graph objects and the discrimination score of each subgraph
feature are uncertain. To address these challenges, we propose a novel
discriminative subgraph feature selection method, DUG, which can find
discriminative subgraph features in uncertain graphs based upon different
statistical measures including expectation, median, mode and phi-probability.
We first compute the probability distribution of the discrimination scores for
each subgraph feature based on dynamic programming. Then a branch-and-bound
algorithm is proposed to search for discriminative subgraphs efficiently.
Extensive experiments on various neuroimaging applications (i.e., Alzheimer's
Disease, ADHD and HIV) have been performed to analyze the gain in performance
by taking into account structural uncertainties in identifying discriminative
subgraph features for graph classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6628</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6628</id><created>2013-01-28</created><authors><author><keyname>Kelner</keyname><forenames>Jonathan A.</forenames></author><author><keyname>Orecchia</keyname><forenames>Lorenzo</forenames></author><author><keyname>Sidford</keyname><forenames>Aaron</forenames></author><author><keyname>Zhu</keyname><forenames>Zeyuan Allen</forenames></author></authors><title>A Simple, Combinatorial Algorithm for Solving SDD Systems in
  Nearly-Linear Time</title><categories>cs.DS cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a simple combinatorial algorithm that solves
symmetric diagonally dominant (SDD) linear systems in nearly-linear time. It
uses very little of the machinery that previously appeared to be necessary for
a such an algorithm. It does not require recursive preconditioning, spectral
sparsification, or even the Chebyshev Method or Conjugate Gradient. After
constructing a &quot;nice&quot; spanning tree of a graph associated with the linear
system, the entire algorithm consists of the repeated application of a simple
(non-recursive) update rule, which it implements using a lightweight data
structure. The algorithm is numerically stable and can be implemented without
the increased bit-precision required by previous solvers. As such, the
algorithm has the fastest known running time under the standard unit-cost RAM
model. We hope that the simplicity of the algorithm and the insights yielded by
its analysis will be useful in both theory and practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6630</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6630</id><created>2013-01-28</created><updated>2013-02-08</updated><authors><author><keyname>Monti</keyname><forenames>Corrado</forenames></author><author><keyname>Rozza</keyname><forenames>Alessandro</forenames></author><author><keyname>Zappella</keyname><forenames>Giovanni</forenames></author><author><keyname>Zignani</keyname><forenames>Matteo</forenames></author><author><keyname>Arvidsson</keyname><forenames>Adam</forenames></author><author><keyname>Poletti</keyname><forenames>Monica</forenames></author></authors><title>Political Disaffection: a case study on the Italian Twitter community</title><categories>cs.SI cs.LG physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our work we analyse the political disaffection or &quot;the subjective feeling
of powerlessness, cynicism, and lack of confidence in the political process,
politicians, and democratic institutions, but with no questioning of the
political regime&quot; by exploiting Twitter data through machine learning
techniques. In order to validate the quality of the time-series generated by
the Twitter data, we highlight the relations of these data with political
disaffection as measured by means of public opinion surveys. Moreover, we show
that important political news of Italian newspapers are often correlated with
the highest peaks of the produced time-series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6643</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6643</id><created>2013-01-28</created><updated>2013-10-30</updated><authors><author><keyname>Sharifi</keyname><forenames>Shahrouz</forenames></author><author><keyname>Duman</keyname><forenames>Tolga M.</forenames></author></authors><title>On the Performance of Low Density Parity Check Codes for Gaussian
  Interference Channels</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, two-user Gaussian interference channel(GIC) is revisited with
the objective of developing implementable (explicit) channel codes.
Specifically, low density parity check (LDPC) codes are adopted for use over
these channels, and their benefits are studied. Different scenarios on the
level of interference are considered. In particular, for strong interference
channel examples with binary phase shift keying (BPSK), it is demonstrated that
rates better than those offered by single user codes with time sharing are
achievable. Promising results are also observed with quadrature-shift-keying
(QPSK). Under general interference a Han-Kobayashi coding based scheme is
employed splitting the information into public and private parts, and utilizing
appropriate iterative decoders at the receivers. Using QPSK modulation at the
two transmitters, it is shown that rate points higher than those achievable by
time sharing are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6644</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6644</id><created>2013-01-28</created><authors><author><keyname>Halperin</keyname><forenames>Daniel Chaim</forenames></author></authors><title>Simplifying the Configuration of 802.11 Wireless Networks with Effective
  SNR</title><categories>cs.NI</categories><comments>Ph.D. Thesis, Department of Computer Science and Engineering, College
  of Engineering, University of Washington, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in the price, performance, and power consumption of Wi-Fi (IEEE
802.11) technology have led to the adoption of wireless functionality in
diverse consumer electronics. These trends have enabled an exciting vision of
rich wireless applications that combine the unique features of different
devices for a better user experience. To meet the needs of these applications,
a wireless network must be configured well to provide good performance at the
physical layer. But because of wireless technology and usage trends, finding
these configurations is an increasingly challenging problem.
  Wireless configuration objectives range from simply choosing the fastest way
to encode data on a single wireless link to the global optimization of many
interacting parameters over multiple sets of communicating devices. As more
links are involved, as technology advances (e.g., the adoption of OFDM and MIMO
techniques in Wi-Fi), and as devices are used in changing wireless channels,
the size of the configuration space grows. Thus algorithms must find good
operating points among a growing number of options.
  ... continued inside thesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6646</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6646</id><created>2013-01-28</created><updated>2013-07-04</updated><authors><author><keyname>Fawzi</keyname><forenames>Alhussein</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Image registration with sparse approximations in parametric dictionaries</title><categories>cs.CV</categories><journal-ref>SIAM Journal on Imaging Sciences 2013 6:4, 2370-2403</journal-ref><doi>10.1137/130907872</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine in this paper the problem of image registration from the new
perspective where images are given by sparse approximations in parametric
dictionaries of geometric functions. We propose a registration algorithm that
looks for an estimate of the global transformation between sparse images by
examining the set of relative geometrical transformations between the
respective features. We propose a theoretical analysis of our registration
algorithm and we derive performance guarantees based on two novel important
properties of redundant dictionaries, namely the robust linear independence and
the transformation inconsistency. We propose several illustrations and insights
about the importance of these dictionary properties and show that common
properties such as coherence or restricted isometry property fail to provide
sufficient information in registration problems. We finally show with
illustrative experiments on simple visual objects and handwritten digits images
that our algorithm outperforms baseline competitor methods in terms of
transformation-invariant distance computation and classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6648</identifier>
 <datestamp>2013-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6648</id><created>2013-01-28</created><updated>2013-05-09</updated><authors><author><keyname>Wang</keyname><forenames>Liming</forenames></author><author><keyname>Rodrigues</keyname><forenames>Miguel</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author></authors><title>Generalized Bregman Divergence and Gradient of Mutual Information for
  Vector Poisson Channels</title><categories>cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate connections between information-theoretic and
estimation-theoretic quantities in vector Poisson channel models. In
particular, we generalize the gradient of mutual information with respect to
key system parameters from the scalar to the vector Poisson channel model. We
also propose, as another contribution, a generalization of the classical
Bregman divergence that offers a means to encapsulate under a unifying
framework the gradient of mutual information results for scalar and vector
Poisson and Gaussian channel models. The so-called generalized Bregman
divergence is also shown to exhibit various properties akin to the properties
of the classical version. The vector Poisson channel model is drawing
considerable attention in view of its application in various domains: as an
example, the availability of the gradient of mutual information can be used in
conjunction with gradient descent methods to effect compressive-sensing
projection designs in emerging X-ray and document classification applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6658</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6658</id><created>2013-01-28</created><authors><author><keyname>Zorzi</keyname><forenames>Mattia</forenames></author><author><keyname>Ticozzi</keyname><forenames>Francesco</forenames></author><author><keyname>Ferrante</keyname><forenames>Augusto</forenames></author></authors><title>Minimum Relative Entropy for Quantum Estimation: Feasibility and General
  Solution</title><categories>quant-ph cs.IT math.IT</categories><comments>9 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general framework for solving quantum state estimation problems
using the minimum relative entropy criterion. A convex optimization approach
allows us to decide the feasibility of the problem given the data and, whenever
necessary, to relax the constraints in order to allow for a physically
admissible solution. Building on these results, the variational analysis can be
completed ensuring existence and uniqueness of the optimum. The latter can then
be computed by standard, efficient standard algorithms for convex optimization,
without resorting to approximate methods or restrictive assumptions on its
rank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6659</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6659</id><created>2013-01-28</created><updated>2013-08-01</updated><authors><author><keyname>Mirbakhsh</keyname><forenames>Nima</forenames></author><author><keyname>Ling</keyname><forenames>Charles X.</forenames></author></authors><title>Clustering-Based Matrix Factorization</title><categories>cs.LG</categories><comments>This paper has been withdrawn by the author due to crucial typo and
  the poor grammatical text</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender systems are emerging technologies that nowadays can be found in
many applications such as Amazon, Netflix, and so on. These systems help users
to find relevant information, recommendations, and their preferred items.
Slightly improvement of the accuracy of these recommenders can highly affect
the quality of recommendations. Matrix Factorization is a popular method in
Recommendation Systems showing promising results in accuracy and complexity. In
this paper we propose an extension of matrix factorization which adds general
neighborhood information on the recommendation model. Users and items are
clustered into different categories to see how these categories share
preferences. We then employ these shared interests of categories in a fusion by
Biased Matrix Factorization to achieve more accurate recommendations. This is a
complement for the current neighborhood aware matrix factorization models which
rely on using direct neighborhood information of users and items. The proposed
model is tested on two well-known recommendation system datasets: Movielens100k
and Netflix. Our experiment shows applying the general latent features of
categories into factorized recommender models improves the accuracy of
recommendations. The current neighborhood-aware models need a great number of
neighbors to acheive good accuracies. To the best of our knowledge, the
proposed model is better than or comparable with the current neighborhood-aware
models when they consider fewer number of neighbors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6662</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6662</id><created>2013-01-28</created><authors><author><keyname>Chitsaz</keyname><forenames>Hamidreza</forenames></author></authors><title>On Time-optimal Trajectories for a Car-like Robot with One Trailer</title><categories>math.OC cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In addition to the theoretical value of challenging optimal control problmes,
recent progress in autonomous vehicles mandates further research in optimal
motion planning for wheeled vehicles. Since current numerical optimal control
techniques suffer from either the curse of dimens ionality, e.g. the
Hamilton-Jacobi-Bellman equation, or the curse of complexity, e.g.
pseudospectral optimal control and max-plus methods, analytical
characterization of geodesics for wheeled vehicles becomes important not only
from a theoretical point of view but also from a prac tical one. Such an
analytical characterization provides a fast motion planning algorithm that can
be used in robust feedback loops. In this work, we use the Pontryagin Maximum
Principle to characterize extremal trajectories, i.e. candidate geodesics, for
a car-like robot with one trailer. We use time as the distance function. In
spite of partial progress, this problem has remained open in the past two
decades. Besides straight motion and turn with maximum allowed curvature, we
identify planar elastica as the third piece of motion that occurs along our
extr emals. We give a detailed characterization of such curves, a special case
of which, called \emph{merging curve}, connects maximum curvature turns to
straight line segments. The structure of extremals in our case is revealed
through analytical integration of the system and adjoint equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6667</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6667</id><created>2013-01-28</created><authors><author><keyname>Aichholzer</keyname><forenames>O.</forenames></author><author><keyname>Caraballo</keyname><forenames>L. E.</forenames></author><author><keyname>D&#xed;az-B&#xe1;&#xf1;ez</keyname><forenames>J. M.</forenames></author><author><keyname>Fabila-Monroy</keyname><forenames>R.</forenames></author><author><keyname>Ochoa</keyname><forenames>C.</forenames></author><author><keyname>Nigsch</keyname><forenames>P.</forenames></author></authors><title>Extremal antipodal polygons and polytopes</title><categories>math.MG cs.CG math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $S$ be a set of $2n$ points on a circle such that for each point $p \in
S$ also its antipodal (mirrored with respect to the circle center) point $p'$
belongs to $S$. A polygon $P$ of size $n$ is called \emph{antipodal} if it
consists of precisely one point of each antipodal pair $(p,p')$ of $S$.
  We provide a complete characterization of antipodal polygons which maximize
(minimize, respectively) the area among all antipodal polygons of $S$. Based on
this characterization, a simple linear time algorithm is presented for
computing extremal antipodal polygons. Moreover, for the generalization of
antipodal polygons to higher dimensions we show that a similar characterization
does not exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6675</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6675</id><created>2013-01-23</created><authors><author><keyname>Arroyo-Figueroa</keyname><forenames>Gustavo</forenames></author><author><keyname>Sucar</keyname><forenames>Luis Enrique</forenames></author></authors><title>A Temporal Bayesian Network for Diagnosis and Prediction</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-13-20</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diagnosis and prediction in some domains, like medical and industrial
diagnosis, require a representation that combines uncertainty management and
temporal reasoning. Based on the fact that in many cases there are few state
changes in the temporal range of interest, we propose a novel representation
called Temporal Nodes Bayesian Networks (TNBN). In a TNBN each node represents
an event or state change of a variable, and an arc corresponds to a
causal-temporal relationship. The temporal intervals can differ in number and
size for each temporal node, so this allows multiple granularity. Our approach
is contrasted with a dynamic Bayesian network for a simple medical example. An
empirical evaluation is presented for a more complex problem, a subsystem of a
fossil power plant, in which this approach is used for fault diagnosis and
prediction with good results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6676</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6676</id><created>2013-01-23</created><authors><author><keyname>Attias</keyname><forenames>Hagai</forenames></author></authors><title>Inferring Parameters and Structure of Latent Variable Models by
  Variational Bayes</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-21-30</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current methods for learning graphical models with latent variables and a
fixed structure estimate optimal values for the model parameters. Whereas this
approach usually produces overfitting and suboptimal generalization
performance, carrying out the Bayesian program of computing the full posterior
distributions over the parameters remains a difficult problem. Moreover,
learning the structure of models with latent variables, for which the Bayesian
approach is crucial, is yet a harder problem. In this paper I present the
Variational Bayes framework, which provides a solution to these problems. This
approach approximates full posterior distributions over model parameters and
structures, as well as latent variables, in an analytical manner without
resorting to sampling methods. Unlike in the Laplace approximation, these
posteriors are generally non-Gaussian and no Hessian needs to be computed. The
resulting algorithm generalizes the standard Expectation Maximization
algorithm, and its convergence is guaranteed. I demonstrate that this algorithm
can be applied to a large class of models in several domains, including
unsupervised clustering and blind source separation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6677</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6677</id><created>2013-01-23</created><authors><author><keyname>Azoury</keyname><forenames>Katy S.</forenames></author><author><keyname>Warmuth</keyname><forenames>Manfred K.</forenames></author></authors><title>Relative Loss Bounds for On-line Density Estimation with the Exponential
  Family of Distributions</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-31-40</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider on-line density estimation with a parameterized density from the
exponential family. The on-line algorithm receives one example at a time and
maintains a parameter that is essentially an average of the past examples.
After receiving an example the algorithm incurs a loss which is the negative
log-likelihood of the example w.r.t. the past parameter of the algorithm. An
off-line algorithm can choose the best parameter based on all the examples. We
prove bounds on the additional total loss of the on-line algorithm over the
total loss of the off-line algorithm. These relative loss bounds hold for an
arbitrary sequence of examples. The goal is to design algorithms with the best
possible relative loss bounds. We use a certain divergence to derive and
analyze the algorithms. This divergence is a relative entropy between two
exponential distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6678</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6678</id><created>2013-01-23</created><authors><author><keyname>Barry</keyname><forenames>Philip S.</forenames></author><author><keyname>Laskey</keyname><forenames>Kathryn Blackmond</forenames></author></authors><title>An Application of Uncertain Reasoning to Requirements Engineering</title><categories>cs.SE cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-41-48</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the use of Bayesian Networks to tackle one of the tougher
problems in requirements engineering, translating user requirements into system
requirements. The approach taken is to model domain knowledge as Bayesian
Network fragments that are glued together to form a complete view of the domain
specific system requirements. User requirements are introduced as evidence and
the propagation of belief is used to determine what are the appropriate system
requirements as indicated by user requirements. This concept has been
demonstrated in the development of a system specification and the results are
presented here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6679</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6679</id><created>2013-01-23</created><authors><author><keyname>Benferhat</keyname><forenames>Salem</forenames></author><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Garcia</keyname><forenames>Laurent</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>Possibilistic logic bases and possibilistic graphs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-57-64</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Possibilistic logic bases and possibilistic graphs are two different
frameworks of interest for representing knowledge. The former stratifies the
pieces of knowledge (expressed by logical formulas) according to their level of
certainty, while the latter exhibits relationships between variables. The two
types of representations are semantically equivalent when they lead to the same
possibility distribution (which rank-orders the possible interpretations). A
possibility distribution can be decomposed using a chain rule which may be
based on two different kinds of conditioning which exist in possibility theory
(one based on product in a numerical setting, one based on minimum operation in
a qualitative setting). These two types of conditioning induce two kinds of
possibilistic graphs. In both cases, a translation of these graphs into
possibilistic bases is provided. The converse translation from a possibilistic
knowledge base into a min-based graph is also described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6680</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6680</id><created>2013-01-23</created><authors><author><keyname>Boman</keyname><forenames>Magnus</forenames></author><author><keyname>Davidsson</keyname><forenames>Paul</forenames></author><author><keyname>Younes</keyname><forenames>Hakan L.</forenames></author></authors><title>Artificial Decision Making Under Uncertainty in Intelligent Buildings</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-65-70</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our hypothesis is that by equipping certain agents in a multi-agent system
controlling an intelligent building with automated decision support, two
important factors will be increased. The first is energy saving in the
building. The second is customer value---how the people in the building
experience the effects of the actions of the agents. We give evidence for the
truth of this hypothesis through experimental findings related to tools for
artificial decision making. A number of assumptions related to agent control,
through monitoring and delegation of tasks to other kinds of agents, of rooms
at a test site are relaxed. Each assumption controls at least one uncertainty
that complicates considerably the procedures for selecting actions part of each
such agent. We show that in realistic decision situations, room-controlling
agents can make bounded rational decisions even under dynamic real-time
constraints. This result can be, and has been, generalized to other domains
with even harsher time constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6681</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6681</id><created>2013-01-23</created><authors><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author><author><keyname>Brafman</keyname><forenames>Ronen I.</forenames></author><author><keyname>Hoos</keyname><forenames>Holger H.</forenames></author><author><keyname>Poole</keyname><forenames>David L.</forenames></author></authors><title>Reasoning With Conditional Ceteris Paribus Preference Statem</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-71-80</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many domains it is desirable to assess the preferences of users in a
qualitative rather than quantitative way. Such representations of qualitative
preference orderings form an importnat component of automated decision tools.
We propose a graphical representation of preferences that reflects conditional
dependence and independence of preference statements under a ceteris paribus
(all else being equal) interpretation. Such a representation is ofetn compact
and arguably natural. We describe several search algorithms for dominance
testing based on this representation; these algorithms are quite effective,
especially in specific network topologies, such as chain-and tree- structured
networks, as well as polytrees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6682</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6682</id><created>2013-01-23</created><authors><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author><author><keyname>Goldszmidt</keyname><forenames>Moises</forenames></author><author><keyname>Sabata</keyname><forenames>Bikash</forenames></author></authors><title>Continuous Value Function Approximation for Sequential Bidding Policies</title><categories>cs.AI cs.GT</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-81-90</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Market-based mechanisms such as auctions are being studied as an appropriate
means for resource allocation in distributed and mulitagent decision problems.
When agents value resources in combination rather than in isolation, they must
often deliberate about appropriate bidding strategies for a sequence of
auctions offering resources of interest. We briefly describe a discrete dynamic
programming model for constructing appropriate bidding policies for resources
exhibiting both complementarities and substitutability. We then introduce a
continuous approximation of this model, assuming that money (or the numeraire
good) is infinitely divisible. Though this has the potential to reduce the
computational cost of computing policies, value functions in the transformed
problem do not have a convenient closed form representation. We develop {em
grid-based} approximation for such value functions, representing value
functions using piecewise linear approximations. We show that these methods can
offer significant computational savings with relatively small cost in solution
quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6683</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6683</id><created>2013-01-23</created><authors><author><keyname>Boyen</keyname><forenames>Xavier</forenames></author><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Discovering the Hidden Structure of Complex Dynamic Systems</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-91-100</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic Bayesian networks provide a compact and natural representation for
complex dynamic systems. However, in many cases, there is no expert available
from whom a model can be elicited. Learning provides an alternative approach
for constructing models of dynamic systems. In this paper, we address some of
the crucial computational aspects of learning the structure of dynamic systems,
particularly those where some relevant variables are partially observed or even
entirely unknown. Our approach is based on the Structural Expectation
Maximization (SEM) algorithm. The main computational cost of the SEM algorithm
is the gathering of expected sufficient statistics. We propose a novel
approximation scheme that allows these sufficient statistics to be computed
efficiently. We also investigate the fundamental problem of discovering the
existence of hidden variables without exhaustive and expensive search. Our
approach is based on the observation that, in dynamic systems, ignoring a
hidden variable typically results in a violation of the Markov property. Thus,
our algorithm searches for such violations in the data, and introduces hidden
variables to explain them. We provide empirical results showing that the
algorithm is able to learn the dynamics of complex systems in a computationally
tractable way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6684</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6684</id><created>2013-01-23</created><authors><author><keyname>Cheng</keyname><forenames>Jie</forenames></author><author><keyname>Greiner</keyname><forenames>Russell</forenames></author></authors><title>Comparing Bayesian Network Classifiers</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-101-108</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we empirically evaluate algorithms for learning four types of
Bayesian network (BN) classifiers - Naive-Bayes, tree augmented Naive-Bayes, BN
augmented Naive-Bayes and general BNs, where the latter two are learned using
two variants of a conditional-independence (CI) based BN-learning algorithm.
Experimental results show the obtained classifiers, learned using the CI based
algorithms, are competitive with (or superior to) the best known classifiers,
based on both Bayesian networks and other formalisms; and that the
computational time for learning and using these classifiers is relatively
small. Moreover, these results also suggest a way to learn yet more effective
classifiers; we demonstrate empirically that this new algorithm does work as
expected. Collectively, these results argue that BN classifiers deserve more
attention in machine learning and data mining communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6685</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6685</id><created>2013-01-23</created><updated>2015-05-16</updated><authors><author><keyname>Chickering</keyname><forenames>David Maxwell</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>Fast Learning from Sparse Data</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1999-PG-109-115</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe two techniques that significantly improve the running time of
several standard machine-learning algorithms when data is sparse. The first
technique is an algorithm that effeciently extracts one-way and two-way
counts--either real or expected-- from discrete data. Extracting such counts is
a fundamental step in learning algorithms for constructing a variety of models
including decision trees, decision graphs, Bayesian networks, and naive-Bayes
clustering models. The second technique is an algorithm that efficiently
performs the E-step of the EM algorithm (i.e. inference) when applied to a
naive-Bayes clustering model. Using real-world data sets, we demonstrate a
dramatic decrease in running time for algorithms that incorporate these
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6686</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6686</id><created>2013-01-23</created><authors><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author><author><keyname>Yoo</keyname><forenames>Changwon</forenames></author></authors><title>Causal Discovery from a Mixture of Experimental and Observational Data</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-116-125</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a Bayesian method for combining an arbitrary mixture of
observational and experimental data in order to learn causal Bayesian networks.
Observational data are passively observed. Experimental data, such as that
produced by randomized controlled trials, result from the experimenter
manipulating one or more variables (typically randomly) and observing the
states of other variables. The paper presents a Bayesian method for learning
the causal structure and parameters of the underlying causal process that is
generating the data, given that (1) the data contains a mixture of
observational and experimental case records, and (2) the causal process is
modeled as a causal Bayesian network. This learning method was applied using as
input various mixtures of experimental and observational data that were
generated from the ALARM causal Bayesian network. In these experiments, the
absolute and relative quantities of experimental and observational data were
varied systematically. For each of these training datasets, the learning method
was applied to predict the causal structure and to estimate the causal
parameters that exist among randomly selected pairs of nodes in ALARM that are
not confounded. The paper reports how these structure predictions and parameter
estimates compare with the true causal structures and parameters as given by
the ALARM network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6687</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6687</id><created>2013-01-23</created><authors><author><keyname>Cussens</keyname><forenames>James</forenames></author></authors><title>Loglinear models for first-order probabilistic reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-126-133</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work on loglinear models in probabilistic constraint logic programming
is applied to first-order probabilistic reasoning. Probabilities are defined
directly on the proofs of atomic formulae, and by marginalisation on the atomic
formulae themselves. We use Stochastic Logic Programs (SLPs) composed of
labelled and unlabelled definite clauses to define the proof probabilities. We
have a conservative extension of first-order reasoning, so that, for example,
there is a one-one mapping between logical and random variables. We show how,
in this framework, Inductive Logic Programming (ILP) can be used to induce the
features of a loglinear model from data. We also compare the presented
framework with other approaches to first-order probabilistic reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6688</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6688</id><created>2013-01-23</created><authors><author><keyname>Dasgupta</keyname><forenames>Sanjoy</forenames></author></authors><title>Learning Polytrees</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-134-141</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the task of learning the maximum-likelihood polytree from data.
Our first result is a performance guarantee establishing that the optimal
branching (or Chow-Liu tree), which can be computed very easily, constitutes a
good approximation to the best polytree. We then show that it is not possible
to do very much better, since the learning problem is NP-hard even to
approximately solve within some constant factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6689</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6689</id><created>2013-01-23</created><authors><author><keyname>Dash</keyname><forenames>Denver</forenames></author><author><keyname>Druzdzel</keyname><forenames>Marek J.</forenames></author></authors><title>A Hybrid Anytime Algorithm for the Constructiion of Causal Models From
  Sparse Data</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-142-149</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a hybrid constraint-based/Bayesian algorithm for learning causal
networks in the presence of sparse data. The algorithm searches the space of
equivalence classes of models (essential graphs) using a heuristic based on
conventional constraint-based techniques. Each essential graph is then
converted into a directed acyclic graph and scored using a Bayesian scoring
metric. Two variants of the algorithm are developed and tested using data from
randomly generated networks of sizes from 15 to 45 nodes with data sizes
ranging from 250 to 2000 records. Both variations are compared to, and found to
consistently outperform two variations of greedy search with restarts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6690</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6690</id><created>2013-01-23</created><authors><author><keyname>Dearden</keyname><forenames>Richard</forenames></author><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Andre</keyname><forenames>David</forenames></author></authors><title>Model-Based Bayesian Exploration</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-150-159</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement learning systems are often concerned with balancing exploration
of untested actions against exploitation of actions that are known to be good.
The benefit of exploration can be estimated using the classical notion of Value
of Information - the expected improvement in future decision quality arising
from the information acquired by exploration. Estimating this quantity requires
an assessment of the agent's uncertainty about its current value estimates for
states. In this paper we investigate ways of representing and reasoning about
this uncertainty in algorithms where the system attempts to learn a model of
its environment. We explicitly represent uncertainty about the parameters of
the model and build probability distributions over Q-values based on these.
These distributions are used to compute a myopic approximation to the value of
information for each action and hence to select the action that best balances
exploration and exploitation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6691</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6691</id><created>2013-01-23</created><authors><author><keyname>Dekhtyar</keyname><forenames>Michael I.</forenames></author><author><keyname>Dekhtyar</keyname><forenames>Alex</forenames></author><author><keyname>Subrahmanian</keyname><forenames>V. S.</forenames></author></authors><title>Hybrid Probabilistic Programs: Algorithms and Complexity</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-160-169</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid Probabilistic Programs (HPPs) are logic programs that allow the
programmer to explicitly encode his knowledge of the dependencies between
events being described in the program. In this paper, we classify HPPs into
three classes called HPP_1,HPP_2 and HPP_r,r&gt;= 3. For these classes, we provide
three types of results for HPPs. First, we develop algorithms to compute the
set of all ground consequences of an HPP. Then we provide algorithms and
complexity results for the problems of entailment (&quot;Given an HPP P and a query
Q as input, is Q a logical consequence of P?&quot;) and consistency (&quot;Given an HPP P
as input, is P consistent?&quot;). Our results provide a fine characterization of
when polynomial algorithms exist for the above problems, and when these
problems become intractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6692</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6692</id><created>2013-01-23</created><authors><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Grabisch</keyname><forenames>Michel</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author><author><keyname>Smets</keyname><forenames>Philippe</forenames></author></authors><title>Assessing the value of a candidate. Comparing belief function and
  possibility theories</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-170-177</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of assessing the value of a candidate is viewed here as a
multiple combination problem. On the one hand a candidate can be evaluated
according to different criteria, and on the other hand several experts are
supposed to assess the value of candidates according to each criterion.
Criteria are not equally important, experts are not equally competent or
reliable. Moreover levels of satisfaction of criteria, or levels of confidence
are only assumed to take their values in qualitative scales which are just
linearly ordered. The problem is discussed within two frameworks, the
transferable belief model and the qualitative possibility theory. They
respectively offer a quantitative and a qualitative setting for handling the
problem, providing thus a way to compare the nature of the underlying
assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6693</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6693</id><created>2013-01-23</created><authors><author><keyname>Ezawa</keyname><forenames>Kazuo J.</forenames></author><author><keyname>Napiorkowski</keyname><forenames>Greg</forenames></author><author><keyname>Kossarski</keyname><forenames>Mariusz</forenames></author></authors><title>Evaluation of Distributed Intelligence on the Smart Card</title><categories>cs.CY cs.CR</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-178-187</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe challenges in the risk management of smart card based electronic
cash industry and describe a method to evaluate the effectiveness of
distributed intelligence on the smart card. More specifically, we discuss the
evaluation of distributed intelligence function called &quot;on-chip risk
management&quot; of the smart card for the global electronic cash payment
application using micro dynamic simulation. Handling of uncertainty related to
future economic environment, various potential counterfeit attack scenarios,
requires simulation of such environment to evaluate on-chip performance.
Creation of realistic simulation of electronic cash economy, transaction
environment, consumers, merchants, banks are challenge themselves. In addition,
we shows examples of detection capability of off-chip, host based counterfeit
detection systems based on the micro dynamic simulation model generated data
set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6694</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6694</id><created>2013-01-23</created><authors><author><keyname>Fargier</keyname><forenames>Helene</forenames></author><author><keyname>Perny</keyname><forenames>Patrice</forenames></author></authors><title>Qualitative Models for Decision Under Uncertainty without the
  Commensurability Assumption</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-188-195</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates a purely qualitative version of Savage's theory for
decision making under uncertainty. Until now, most representation theorems for
preference over acts rely on a numerical representation of utility and
uncertainty where utility and uncertainty are commensurate. Disrupting the
tradition, we relax this assumption and introduce a purely ordinal axiom
requiring that the Decision Maker (DM) preference between two acts only depends
on the relative position of their consequences for each state. Within this
qualitative framework, we determine the only possible form of the decision rule
and investigate some instances compatible with the transitivity of the strict
preference. Finally we propose a mild relaxation of our ordinality axiom,
leaving room for a new family of qualitative decision rules compatible with
transitivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6695</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6695</id><created>2013-01-23</created><authors><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Goldszmidt</keyname><forenames>Moises</forenames></author><author><keyname>Wyner</keyname><forenames>Abraham</forenames></author></authors><title>Data Analysis with Bayesian Networks: A Bootstrap Approach</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-196-205</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years there has been significant progress in algorithms and methods
for inducing Bayesian networks from data. However, in complex data analysis
problems, we need to go beyond being satisfied with inducing networks with high
scores. We need to provide confidence measures on features of these networks:
Is the existence of an edge between two nodes warranted? Is the Markov blanket
of a given node robust? Can we say something about the ordering of the
variables? We should be able to address these questions, even when the amount
of data is not enough to induce a high scoring network. In this paper we
propose Efron's Bootstrap as a computationally efficient approach for answering
these questions. In addition, we propose to use these confidence measures to
induce better structures from the data, and to detect the presence of latent
variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6696</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6696</id><created>2013-01-23</created><authors><author><keyname>Friedman</keyname><forenames>Nir</forenames></author><author><keyname>Nachman</keyname><forenames>Iftach</forenames></author><author><keyname>Pe'er</keyname><forenames>Dana</forenames></author></authors><title>Learning Bayesian Network Structure from Massive Datasets: The &quot;Sparse
  Candidate&quot; Algorithm</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-206-215</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning Bayesian networks is often cast as an optimization problem, where
the computational task is to find a structure that maximizes a statistically
motivated score. By and large, existing learning tools address this
optimization problem using standard heuristic search techniques. Since the
search space is extremely large, such search procedures can spend most of the
time examining candidates that are extremely unreasonable. This problem becomes
critical when we deal with data sets that are large either in the number of
instances, or the number of attributes. In this paper, we introduce an
algorithm that achieves faster learning by restricting the search space. This
iterative algorithm restricts the parents of each variable to belong to a small
subset of candidates. We then search for a network that satisfies these
constraints. The learned network is then used for selecting better candidates
for the next iteration. We evaluate this algorithm both on synthetic and
real-life data. Our results show that it is significantly faster than
alternative search procedures without loss of quality in the learned
structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6697</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6697</id><created>2013-01-23</created><updated>2015-05-16</updated><authors><author><keyname>Geiger</keyname><forenames>Dan</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>Parameter Priors for Directed Acyclic Graphical Models and the
  Characterization of Several Probability Distributions</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1999-PG-216-225</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the only parameter prior for complete Gaussian DAG models that
satisfies global parameter independence, complete model equivalence, and some
weak regularity assumptions, is the normal-Wishart distribution. Our analysis
is based on the following new characterization of the Wishart distribution: let
W be an n x n, n &gt;= 3, positive-definite symmetric matrix of random variables
and f(W) be a pdf of W. Then, f(W) is a Wishart distribution if and only if
W_{11}-W_{12}W_{22}^{-1}W_{12}' is independent of {W_{12}, W_{22}} for every
block partitioning W_{11}, W_{12}, W_{12}', W_{22} of W. Similar
characterizations of the normal and normal-Wishart distributions are provided
as well. We also show how to construct a prior for every DAG model over X from
the prior of a single regression model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6698</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6698</id><created>2013-01-23</created><authors><author><keyname>Geiger</keyname><forenames>Dan</forenames></author><author><keyname>Meek</keyname><forenames>Christopher</forenames></author></authors><title>Quantifier Elimination for Statistical Problems</title><categories>cs.AI cs.LO</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-226-235</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent improvement on Tarski's procedure for quantifier elimination in the
first order theory of real numbers makes it feasible to solve small instances
of the following problems completely automatically: 1. listing all equality and
inequality constraints implied by a graphical model with hidden variables. 2.
Comparing graphyical models with hidden variables (i.e., model equivalence,
inclusion, and overlap). 3. Answering questions about the identification of a
model or portion of a model, and about bounds on quantities derived from a
model. 4. Determing whether a given set of independence assertions. We discuss
the foundation of quantifier elimination and demonstrate its application to
these problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6699</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6699</id><created>2013-01-23</created><authors><author><keyname>Giang</keyname><forenames>Phan H.</forenames></author><author><keyname>Shenoy</keyname><forenames>Prakash P.</forenames></author></authors><title>On Transformations between Probability and Spohnian Disbelief Functions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-236-244</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the relationship between probability and Spohn's
theory for representation of uncertain beliefs. Using the intuitive idea that
the more probable a proposition is, the more believable it is, we study
transformations from probability to Sphonian disbelief and vice-versa. The
transformations described in this paper are different from those described in
the literature. In particular, the former satisfies the principles of ordinal
congruence while the latter does not. Such transformations between probability
and Spohn's calculi can contribute to (1) a clarification of the semantics of
nonprobabilistic degree of uncertain belief, and (2) to a construction of a
decision theory for such calculi. In practice, the transformations will allow a
meaningful combination of more than one calculus in different stages of using
an expert system such as knowledge acquisition, inference, and interpretation
of results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6700</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6700</id><created>2013-01-23</created><authors><author><keyname>Goldman</keyname><forenames>Robert P.</forenames></author><author><keyname>Geib</keyname><forenames>Christopher W.</forenames></author><author><keyname>Miller</keyname><forenames>Christopher A.</forenames></author></authors><title>A New Model of Plan Recognition</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-245-254</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new abductive, probabilistic theory of plan recognition. This
model differs from previous plan recognition theories in being centered around
a model of plan execution: most previous methods have been based on plans as
formal objects or on rules describing the recognition process. We show that our
new model accounts for phenomena omitted from most previous plan recognition
theories: notably the cumulative effect of a sequence of observations of
partially-ordered, interleaved plans and the effect of context on plan
adoption. The model also supports inferences about the evolution of plan
execution in situations where another agent intervenes in plan execution. This
facility provides support for using plan recognition to build systems that will
intelligently assist a user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6701</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6701</id><created>2013-01-23</created><authors><author><keyname>Gruyer</keyname><forenames>Dominique</forenames></author><author><keyname>Berge-Cherfaoui</keyname><forenames>Veronique</forenames></author></authors><title>Multi-objects association in perception of dynamical situation</title><categories>cs.AI cs.CV</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-255-262</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In current perception systems applied to the rebuilding of the environment
for intelligent vehicles, the part reserved to object association for the
tracking is increasingly significant. This allows firstly to follow the objects
temporal evolution and secondly to increase the reliability of environment
perception. We propose in this communication the development of a multi-objects
association algorithm with ambiguity removal entering into the design of such a
dynamic perception system for intelligent vehicles. This algorithm uses the
belief theory and data modelling with fuzzy mathematics in order to be able to
handle inaccurate as well as uncertain information due to imperfect sensors.
These theories also allow the fusion of numerical as well as symbolic data. We
develop in this article the problem of matching between known and perceived
objects. This makes it possible to update a dynamic environment map for a
vehicle. The belief theory will enable us to quantify the belief in the
association of each perceived object with each known object. Conflicts can
appear in the case of object appearance or disappearance, or in the case of a
confused situation or bad perception. These conflicts are removed or solved
using an assignment algorithm, giving a solution called the &quot; best &quot; and so
ensuring the tracking of some objects present in our environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6702</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6702</id><created>2013-01-23</created><authors><author><keyname>Ha</keyname><forenames>Vu A.</forenames></author><author><keyname>Haddawy</keyname><forenames>Peter</forenames></author></authors><title>A Hybrid Approach to Reasoning with Partially Elicited Preference Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-263-270</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical Decision Theory provides a normative framework for representing and
reasoning about complex preferences. Straightforward application of this theory
to automate decision making is difficult due to high elicitation cost. In
response to this problem, researchers have recently developed a number of
qualitative, logic-oriented approaches for representing and reasoning about
references. While effectively addressing some expressiveness issues, these
logics have not proven powerful enough for building practical automated
decision making systems. In this paper we present a hybrid approach to
preference elicitation and decision making that is grounded in classical
multi-attribute utility theory, but can make effective use of the expressive
power of qualitative approaches. Specifically, assuming a partially specified
multilinear utility function, we show how comparative statements about classes
of decision alternatives can be used to further constrain the utility function
and thus identify sup-optimal alternatives. This work demonstrates that
quantitative and qualitative approaches can be synergistically integrated to
provide effective and flexible decision support.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6703</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6703</id><created>2013-01-23</created><authors><author><keyname>Harmanec</keyname><forenames>David</forenames></author></authors><title>Faithful Approximations of Belief Functions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-271-278</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A conceptual foundation for approximation of belief functions is proposed and
investigated. It is based on the requirements of consistency and closeness. An
optimal approximation is studied. Unfortunately, the computation of the optimal
approximation turns out to be intractable. Hence, various heuristic methods are
proposed and experimantally evaluated both in terms of their accuracy and in
terms of the speed of computation. These methods are compared to the earlier
proposed approximations of belief functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6704</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6704</id><created>2013-01-23</created><authors><author><keyname>Hoey</keyname><forenames>Jesse</forenames></author><author><keyname>St-Aubin</keyname><forenames>Robert</forenames></author><author><keyname>Hu</keyname><forenames>Alan</forenames></author><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author></authors><title>SPUDD: Stochastic Planning using Decision Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-279-288</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov decisions processes (MDPs) are becoming increasing popular as models
of decision theoretic planning. While traditional dynamic programming methods
perform well for problems with small state spaces, structured methods are
needed for large problems. We propose and examine a value iteration algorithm
for MDPs that uses algebraic decision diagrams(ADDs) to represent value
functions and policies. An MDP is represented using Bayesian networks and ADDs
and dynamic programming is applied directly to these ADDs. We demonstrate our
method on large MDPs (up to 63 million states) and show that significant gains
can be had when compared to tree-structured representations (with up to a
thirty-fold reduction in the number of nodes required to represent optimal
value functions).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6705</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6705</id><created>2013-01-23</created><authors><author><keyname>Hofmann</keyname><forenames>Thomas</forenames></author></authors><title>Probabilistic Latent Semantic Analysis</title><categories>cs.LG cs.IR stat.ML</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-289-296</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic Latent Semantic Analysis is a novel statistical technique for
the analysis of two-mode and co-occurrence data, which has applications in
information retrieval and filtering, natural language processing, machine
learning from text, and in related areas. Compared to standard Latent Semantic
Analysis which stems from linear algebra and performs a Singular Value
Decomposition of co-occurrence tables, the proposed method is based on a
mixture decomposition derived from a latent class model. This results in a more
principled approach which has a solid foundation in statistics. In order to
avoid overfitting, we propose a widely applicable generalization of maximum
likelihood model fitting by tempered EM. Our approach yields substantial and
consistent improvements over Latent Semantic Analysis in a number of
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6706</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6706</id><created>2013-01-23</created><authors><author><keyname>Horsch</keyname><forenames>Michael C.</forenames></author><author><keyname>Poole</keyname><forenames>David L.</forenames></author></authors><title>Estimating the Value of Computation in Flexible Information Refinement</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-297-304</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We outline a method to estimate the value of computation for a flexible
algorithm using empirical data. To determine a reasonable trade-off between
cost and value, we build an empirical model of the value obtained through
computation, and apply this model to estimate the value of computation for
quite different problems. In particular, we investigate this trade-off for the
problem of constructing policies for decision problems represented as influence
diagrams. We show how two features of our anytime algorithm provide reasonable
estimates of the value of computation in this domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6707</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6707</id><created>2013-01-23</created><authors><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author><author><keyname>Jacobs</keyname><forenames>Andy</forenames></author><author><keyname>Hovel</keyname><forenames>David</forenames></author></authors><title>Attention-Sensitive Alerting</title><categories>cs.AI cs.HC</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-305-313</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce utility-directed procedures for mediating the flow of
potentially distracting alerts and communications to computer users. We present
models and inference procedures that balance the context-sensitive costs of
deferring alerts with the cost of interruption. We describe the challenge of
reasoning about such costs under uncertainty via an analysis of user activity
and the content of notifications. After introducing principles of
attention-sensitive alerting, we focus on the problem of guiding alerts about
email messages. We dwell on the problem of inferring the expected criticality
of email and discuss work on the Priorities system, centering on prioritizing
email by criticality and modulating the communication of notifications to users
about the presence and nature of incoming email.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6708</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6708</id><created>2013-01-23</created><authors><author><keyname>Kask</keyname><forenames>Kalev</forenames></author><author><keyname>Dechter</keyname><forenames>Rina</forenames></author></authors><title>Mini-Bucket Heuristics for Improved Search</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-314-323</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is a second in a series of two papers evaluating the power of a new
scheme that generates search heuristics mechanically. The heuristics are
extracted from an approximation scheme called mini-bucket elimination that was
recently introduced. The first paper introduced the idea and evaluated it
within Branch-and-Bound search. In the current paper the idea is further
extended and evaluated within Best-First search. The resulting algorithms are
compared on coding and medical diagnosis problems, using varying strength of
the mini-bucket heuristics.
  Our results demonstrate an effective search scheme that permits controlled
tradeoff between preprocessing (for heuristic generation) and search.
Best-first search is shown to outperform Branch-and-Bound, when supplied with
good heuristics, and sufficient memory space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6709</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6709</id><created>2013-01-23</created><authors><author><keyname>Koller</keyname><forenames>Daphne</forenames></author><author><keyname>Lerner</keyname><forenames>Uri</forenames></author><author><keyname>Anguelov</keyname><forenames>Dragomir</forenames></author></authors><title>A General Algorithm for Approximate Inference and its Application to
  Hybrid Bayes Nets</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-324-333</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The clique tree algorithm is the standard method for doing inference in
Bayesian networks. It works by manipulating clique potentials - distributions
over the variables in a clique. While this approach works well for many
networks, it is limited by the need to maintain an exact representation of the
clique potentials. This paper presents a new unified approach that combines
approximate inference and the clique tree algorithm, thereby circumventing this
limitation. Many known approximate inference algorithms can be viewed as
instances of this approach. The algorithm essentially does clique tree
propagation, using approximate inference to estimate the densities in each
clique. In many settings, the computation of the approximate clique potential
can be done easily using statistical importance sampling. Iterations are used
to gradually improve the quality of the estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6710</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6710</id><created>2013-01-23</created><authors><author><keyname>Kontkanen</keyname><forenames>Petri</forenames></author><author><keyname>Myllymaki</keyname><forenames>Petri</forenames></author><author><keyname>Silander</keyname><forenames>Tomi</forenames></author><author><keyname>Tirri</keyname><forenames>Henry</forenames></author></authors><title>On Supervised Selection of Bayesian Networks</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-334-342</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of possible models (e.g., Bayesian network structures) and a data
sample, in the unsupervised model selection problem the task is to choose the
most accurate model with respect to the domain joint probability distribution.
In contrast to this, in supervised model selection it is a priori known that
the chosen model will be used in the future for prediction tasks involving more
``focused' predictive distributions. Although focused predictive distributions
can be produced from the joint probability distribution by marginalization, in
practice the best model in the unsupervised sense does not necessarily perform
well in supervised domains. In particular, the standard marginal likelihood
score is a criterion for the unsupervised task, and, although frequently used
for supervised model selection also, does not perform well in such tasks. In
this paper we study the performance of the marginal likelihood score
empirically in supervised Bayesian network selection tasks by using a large
number of publicly available classification data sets, and compare the results
to those obtained by alternative model selection criteria, including empirical
crossvalidation methods, an approximation of a supervised marginal likelihood
measure, and a supervised version of Dawids prequential(predictive sequential)
principle.The results demonstrate that the marginal likelihood score does NOT
perform well FOR supervised model selection, WHILE the best results are
obtained BY using Dawids prequential r napproach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6711</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6711</id><created>2013-01-23</created><authors><author><keyname>Korb</keyname><forenames>Kevin B.</forenames></author><author><keyname>Nicholson</keyname><forenames>Ann</forenames></author><author><keyname>Jitnah</keyname><forenames>Nathalie</forenames></author></authors><title>Bayesian Poker</title><categories>cs.AI cs.GT</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-343-350</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Poker is ideal for testing automated reasoning under uncertainty. It
introduces uncertainty both by physical randomization and by incomplete
information about opponents hands.Another source OF uncertainty IS the limited
information available TO construct psychological models OF opponents, their
tendencies TO bluff, play conservatively, reveal weakness, etc. AND the
relation BETWEEN their hand strengths AND betting behaviour. ALL OF these
uncertainties must be assessed accurately AND combined effectively FOR ANY
reasonable LEVEL OF skill IN the game TO be achieved, since good decision
making IS highly sensitive TO those tasks.We describe our Bayesian Poker
Program(BPP), which uses a Bayesian network TO model the programs poker hand,
the opponents hand AND the opponents playing behaviour conditioned upon the
hand, and betting curves which govern play given a probability of winning. The
history of play with opponents is used to improve BPPs understanding OF their
behaviour.We compare BPP experimentally WITH : a simple RULE - based system; a
program which depends exclusively ON hand probabilities(i.e., without opponent
modeling); AND WITH human players.BPP has shown itself TO be an effective
player against ALL these opponents, barring the better humans.We also sketch
out SOME likely ways OF improving play.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6712</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6712</id><created>2013-01-23</created><authors><author><keyname>Kowalczyk</keyname><forenames>Ryszard</forenames></author></authors><title>On Quantified Linguistic Approximation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-351-358</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most fuzzy systems including fuzzy decision support and fuzzy control systems
provide out-puts in the form of fuzzy sets that represent the inferred
conclusions. Linguistic interpretation of such outputs often involves the use
of linguistic approximation that assigns a linguistic label to a fuzzy set
based on the predefined primary terms, linguistic modifiers and linguistic
connectives. More generally, linguistic approximation can be formalized in the
terms of the re-translation rules that correspond to the translation rules in
ex-plicitation (e.g. simple, modifier, composite, quantification and
qualification rules) in com-puting with words [Zadeh 1996]. However most
existing methods of linguistic approximation use the simple, modifier and
composite re-translation rules only. Although these methods can provide a
sufficient approximation of simple fuzzy sets the approximation of more complex
ones that are typical in many practical applications of fuzzy systems may be
less satisfactory. Therefore the question arises why not use in linguistic
ap-proximation also other re-translation rules corre-sponding to the
translation rules in explicitation to advantage. In particular linguistic
quantifica-tion may be desirable in situations where the conclusions
interpreted as quantified linguistic propositions can be more informative and
natu-ral. This paper presents some aspects of linguis-tic approximation in the
context of the re-translation rules and proposes an approach to linguistic
approximation with the use of quantifi-cation rules, i.e. quantified linguistic
approxima-tion. Two methods of the quantified linguistic approximation are
considered with the use of lin-guistic quantifiers based on the concepts of the
non-fuzzy and fuzzy cardinalities of fuzzy sets. A number of examples are
provided to illustrate the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6713</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6713</id><created>2013-01-23</created><authors><author><keyname>Kyburg</keyname><forenames>Henry E.</forenames><suffix>Jr.</suffix></author><author><keyname>Teng</keyname><forenames>Choh Man</forenames></author></authors><title>Choosing Among Interpretations of Probability</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-359-365</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is available an ever-increasing variety of procedures for managing
uncertainty. These methods are discussed in the literature of artificial
intelligence, as well as in the literature of philosophy of science. Heretofore
these methods have been evaluated by intuition, discussion, and the general
philosophical method of argument and counterexample. Almost any method of
uncertainty management will have the property that in the long run it will
deliver numbers approaching the relative frequency of the kinds of events at
issue. To find a measure that will provide a meaningful evaluation of these
treatments of uncertainty, we must look, not at the long run, but at the short
or intermediate run. Our project attempts to develop such a measure in terms of
short or intermediate length performance. We represent the effects of practical
choices by the outcomes of bets offered to agents characterized by two
uncertainty management approaches: the subjective Bayesian approach and the
Classical confidence interval approach. Experimental evaluation suggests that
the confidence interval approach can outperform the subjective approach in the
relatively short run.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6714</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6714</id><created>2013-01-23</created><authors><author><keyname>La Mura</keyname><forenames>Pierfrancesco</forenames></author><author><keyname>Shoham</keyname><forenames>Yoav</forenames></author></authors><title>Expected Utility Networks</title><categories>cs.GT cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-366-373</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new class of graphical representations, expected utility
networks (EUNs), and discuss some of its properties and potential applications
to artificial intelligence and economic theory. In EUNs not only probabilities,
but also utilities enjoy a modular representation. EUNs are undirected graphs
with two types of arc, representing probability and utility dependencies
respectively. The representation of utilities is based on a novel notion of
conditional utility independence, which we introduce and discuss in the context
of other existing proposals. Just as probabilistic inference involves the
computation of conditional probabilities, strategic inference involves the
computation of conditional expected utilities for alternative plans of action.
We define a new notion of conditional expected utility (EU) independence, and
show that in EUNs node separation with respect to the probability and utility
subgraphs implies conditional EU independence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6715</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6715</id><created>2013-01-23</created><authors><author><keyname>Lusena</keyname><forenames>Christopher</forenames></author><author><keyname>Li</keyname><forenames>Tong</forenames></author><author><keyname>Sittinger</keyname><forenames>Shelia</forenames></author><author><keyname>Wells</keyname><forenames>Chris</forenames></author><author><keyname>Goldsmith</keyname><forenames>Judy</forenames></author></authors><title>My Brain is Full: When More Memory Helps</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-374-381</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding good finite-horizon policies for POMDPs
under the expected reward metric. The policies considered are {em free
finite-memory policies with limited memory}; a policy is a mapping from the
space of observation-memory pairs to the space of action-memeory pairs (the
policy updates the memory as it goes), and the number of possible memory states
is a parameter of the input to the policy-finding algorithms. The algorithms
considered here are preliminary implementations of three search heuristics:
local search, simulated annealing, and genetic algorithms. We compare their
outcomes to each other and to the optimal policies for each instance. We
compare run times of each policy and of a dynamic programming algorithm for
POMDPs developed by Hansen that iteratively improves a finite-state controller
--- the previous state of the art for finite memory policies. The value of the
best policy can only improve as the amount of memory increases, up to the
amount needed for an optimal finite-memory policy. Our most surprising finding
is that more memory helps in another way: given more memory than is needed for
an optimal policy, the algorithms are more likely to converge to optimal-valued
policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6716</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6716</id><created>2013-01-23</created><authors><author><keyname>Madsen</keyname><forenames>Anders L.</forenames></author><author><keyname>Jensen</keyname><forenames>Finn Verner</forenames></author></authors><title>Lazy Evaluation of Symmetric Bayesian Decision Problems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-382-390</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving symmetric Bayesian decision problems is a computationally intensive
task to perform regardless of the algorithm used. In this paper we propose a
method for improving the efficiency of algorithms for solving Bayesian decision
problems. The method is based on the principle of lazy evaluation - a principle
recently shown to improve the efficiency of inference in Bayesian networks. The
basic idea is to maintain decompositions of potentials and to postpone
computations for as long as possible. The efficiency improvements obtained with
the lazy evaluation based method is emphasized through examples. Finally, the
lazy evaluation based method is compared with the hugin and valuation-based
systems architectures for solving symmetric Bayesian decision problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6717</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6717</id><created>2013-01-23</created><authors><author><keyname>Mahoney</keyname><forenames>Suzanne M.</forenames></author><author><keyname>Laskey</keyname><forenames>Kathryn Blackmond</forenames></author></authors><title>Representing and Combining Partially Specified CPTs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-391-400</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends previous work with network fragments and
situation-specific network construction. We formally define the asymmetry
network, an alternative representation for a conditional probability table. We
also present an object-oriented representation for partially specified
asymmetry networks. We show that the representation is parsimonious. We define
an algebra for the elements of the representation that allows us to 'factor'
any CPT and to soundly combine the partially specified asymmetry networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6718</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6718</id><created>2013-01-23</created><authors><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Singh</keyname><forenames>Satinder</forenames></author></authors><title>On the Complexity of Policy Iteration</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-401-408</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision-making problems in uncertain or stochastic domains are often
formulated as Markov decision processes (MDPs). Policy iteration (PI) is a
popular algorithm for searching over policy-space, the size of which is
exponential in the number of states. We are interested in bounds on the
complexity of PI that do not depend on the value of the discount factor. In
this paper we prove the first such non-trivial, worst-case, upper bounds on the
number of iterations required by PI to converge to the optimal policy. Our
analysis also sheds new light on the manner in which PI progresses through the
space of policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6719</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6719</id><created>2013-01-23</created><authors><author><keyname>McAllester</keyname><forenames>David A.</forenames></author><author><keyname>Singh</keyname><forenames>Satinder</forenames></author></authors><title>Approximate Planning for Factored POMDPs using Belief State
  Simplification</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-409-416</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are interested in the problem of planning for factored POMDPs. Building on
the recent results of Kearns, Mansour and Ng, we provide a planning algorithm
for factored POMDPs that exploits the accuracy-efficiency tradeoff in the
belief state simplification introduced by Boyen and Koller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6720</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6720</id><created>2013-01-23</created><authors><author><keyname>Meuleau</keyname><forenames>Nicolas</forenames></author><author><keyname>Kim</keyname><forenames>Kee-Eung</forenames></author><author><keyname>Kaelbling</keyname><forenames>Leslie Pack</forenames></author><author><keyname>Cassandra</keyname><forenames>Anthony R.</forenames></author></authors><title>Solving POMDPs by Searching the Space of Finite Policies</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-417-426</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving partially observable Markov decision processes (POMDPs) is highly
intractable in general, at least in part because the optimal policy may be
infinitely large. In this paper, we explore the problem of finding the optimal
policy from a restricted set of policies, represented as finite state automata
of a given size. This problem is also intractable, but we show that the
complexity can be greatly reduced when the POMDP and/or policy are further
constrained. We demonstrate good empirical results with a branch-and-bound
method for finding globally optimal deterministic policies, and a
gradient-ascent method for finding locally optimal stochastic policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6721</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6721</id><created>2013-01-23</created><authors><author><keyname>Meuleau</keyname><forenames>Nicolas</forenames></author><author><keyname>Peshkin</keyname><forenames>Leonid</forenames></author><author><keyname>Kim</keyname><forenames>Kee-Eung</forenames></author><author><keyname>Kaelbling</keyname><forenames>Leslie Pack</forenames></author></authors><title>Learning Finite-State Controllers for Partially Observable Environments</title><categories>cs.AI cs.SY</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-427-436</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reactive (memoryless) policies are sufficient in completely observable Markov
decision processes (MDPs), but some kind of memory is usually necessary for
optimal control of a partially observable MDP. Policies with finite memory can
be represented as finite-state automata. In this paper, we extend Baird and
Moore's VAPS algorithm to the problem of learning general finite-state
automata. Because it performs stochastic gradient descent, this algorithm can
be shown to converge to a locally optimal finite-state controller. We provide
the details of the algorithm and then consider the question of under what
conditions stochastic gradient descent will outperform exact gradient descent.
We conclude with empirical results comparing the performance of stochastic and
exact gradient descent, and showing the ability of our algorithm to extract the
useful information contained in the sequence of past observations to compensate
for the lack of observability at each time-step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6722</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6722</id><created>2013-01-23</created><authors><author><keyname>Mislevy</keyname><forenames>Robert</forenames></author><author><keyname>Almond</keyname><forenames>Russell</forenames></author><author><keyname>Yan</keyname><forenames>Duanli</forenames></author><author><keyname>Steinberg</keyname><forenames>Linda S.</forenames></author></authors><title>Bayes Nets in Educational Assessment: Where Do the Numbers Come From?</title><categories>cs.AI cs.CY stat.AP</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-437-446</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As observations and student models become complex, educational assessments
that exploit advances in technology and cognitive psychology can outstrip
familiar testing models and analytic methods. Within the Portal conceptual
framework for assessment design, Bayesian inference networks (BINs) record
beliefs about students' knowledge and skills, in light of what they say and do.
Joining evidence model BIN fragments- which contain observable variables and
pointers to student model variables - to the student model allows one to update
belief about knowledge and skills as observations arrive. Markov Chain Monte
Carlo (MCMC) techniques can estimate the required conditional probabilities
from empirical data, supplemented by expert judgment or substantive theory.
Details for the special cases of item response theory (IRT) and multivariate
latent class modeling are given, with a numerical example of the latter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6723</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6723</id><created>2013-01-23</created><authors><author><keyname>Monti</keyname><forenames>Stefano</forenames></author><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author></authors><title>A Bayesian Network Classifier that Combines a Finite Mixture Model and a
  Naive Bayes Model</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-447-456</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new Bayesian network model for classification that
combines the naive-Bayes (NB) classifier and the finite-mixture (FM)
classifier. The resulting classifier aims at relaxing the strong assumptions on
which the two component models are based, in an attempt to improve on their
classification performance, both in terms of accuracy and in terms of
calibration of the estimated probabilities. The proposed classifier is obtained
by superimposing a finite mixture model on the set of feature variables of a
naive Bayes model. We present experimental results that compare the predictive
performance on real datasets of the new classifier with the predictive
performance of the NB classifier and the FM classifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6724</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6724</id><created>2013-01-23</created><authors><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author></authors><title>A Variational Approximation for Bayesian Networks with Discrete and
  Continuous Latent Variables</title><categories>cs.AI cs.LG stat.ML</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-457-466</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to use a variational approximation to the logistic function to
perform approximate inference in Bayesian networks containing discrete nodes
with continuous parents. Essentially, we convert the logistic function to a
Gaussian, which facilitates exact inference, and then iteratively adjust the
variational parameters to improve the quality of the approximation. We
demonstrate experimentally that this approximation is faster and potentially
more accurate than sampling. We also introduce a simple new technique for
handling evidence, which allows us to handle arbitrary distributions on
observed nodes, as well as achieving a significant speedup in networks with
discrete variables of large cardinality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6725</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6725</id><created>2013-01-23</created><authors><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author><author><keyname>Weiss</keyname><forenames>Yair</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Loopy Belief Propagation for Approximate Inference: An Empirical Study</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-467-476</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, researchers have demonstrated that loopy belief propagation - the
use of Pearls polytree algorithm IN a Bayesian network WITH loops OF error-
correcting codes.The most dramatic instance OF this IS the near Shannon - limit
performance OF Turbo Codes codes whose decoding algorithm IS equivalent TO
loopy belief propagation IN a chain - structured Bayesian network. IN this
paper we ask : IS there something special about the error - correcting code
context, OR does loopy propagation WORK AS an approximate inference schemeIN a
more general setting? We compare the marginals computed using loopy propagation
TO the exact ones IN four Bayesian network architectures, including two real -
world networks : ALARM AND QMR.We find that the loopy beliefs often converge
AND WHEN they do, they give a good approximation TO the correct
marginals.However,ON the QMR network, the loopy beliefs oscillated AND had no
obvious relationship TO the correct posteriors. We present SOME initial
investigations INTO the cause OF these oscillations, AND show that SOME simple
methods OF preventing them lead TO the wrong results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6726</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6726</id><created>2013-01-23</created><authors><author><keyname>Myers</keyname><forenames>James W.</forenames></author><author><keyname>Laskey</keyname><forenames>Kathryn Blackmond</forenames></author><author><keyname>Levitt</keyname><forenames>Tod S.</forenames></author></authors><title>Learning Bayesian Networks from Incomplete Data with Stochastic Search
  Algorithms</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-476-485</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes stochastic search approaches, including a new stochastic
algorithm and an adaptive mutation operator, for learning Bayesian networks
from incomplete data. This problem is characterized by a huge solution space
with a highly multimodal landscape. State-of-the-art approaches all involve
using deterministic approaches such as the expectation-maximization algorithm.
These approaches are guaranteed to find local maxima, but do not explore the
landscape for other modes. Our approach evolves structure and the missing data.
We compare our stochastic algorithms and show they all produce accurate
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6727</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6727</id><created>2013-01-23</created><authors><author><keyname>Neil</keyname><forenames>Julian R.</forenames></author><author><keyname>Wallace</keyname><forenames>Chris S.</forenames></author><author><keyname>Korb</keyname><forenames>Kevin B.</forenames></author></authors><title>Learning Bayesian Networks with Restricted Causal Interactions</title><categories>cs.AI cs.LG stat.ML</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-486-493</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major problem for the learning of Bayesian networks (BNs) is the
exponential number of parameters needed for conditional probability tables.
Recent research reduces this complexity by modeling local structure in the
probability tables. We examine the use of log-linear local models. While
log-linear models in this context are not new (Whittaker, 1990; Buntine, 1991;
Neal, 1992; Heckerman and Meek, 1997), for structure learning they are
generally subsumed under a naive Bayes model. We describe an alternative
interpretation, and use a Minimum Message Length (MML) (Wallace, 1987) metric
for structure learning of networks exhibiting causal independence, which we
term first-order networks (FONs). We also investigate local model selection on
a node-by-node basis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6728</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6728</id><created>2013-01-23</created><authors><author><keyname>Nguyen</keyname><forenames>Hien</forenames></author><author><keyname>Haddawy</keyname><forenames>Peter</forenames></author></authors><title>The Decision-Theoretic Interactive Video Advisor</title><categories>cs.IR cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-494-501</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need to help people choose among large numbers of items and to filter
through large amounts of information has led to a flood of research in
construction of personal recommendation agents. One of the central issues in
constructing such agents is the representation and elicitation of user
preferences or interests. This topic has long been studied in Decision Theory,
but surprisingly little work in the area of recommender systems has made use of
formal decision-theoretic techniques. This paper describes DIVA, a
decision-theoretic agent for recommending movies that contains a number of
novel features. DIVA represents user preferences using pairwise comparisons
among items, rather than numeric ratings. It uses a novel similarity measure
based on the concept of the probability of conflict between two orderings of
items. The system has a rich representation of preference, distinguishing
between a user's general taste in movies and his immediate interests. It takes
an incremental approach to preference elicitation in which the user can provide
feedback if not satisfied with the recommendation list. We empirically evaluate
the performance of the system using the EachMovie collaborative filtering
database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6729</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6729</id><created>2013-01-23</created><authors><author><keyname>Nielsen</keyname><forenames>Thomas D.</forenames></author><author><keyname>Jensen</keyname><forenames>Finn Verner</forenames></author></authors><title>Welldefined Decision Scenarios</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-502-511</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence diagrams serve as a powerful tool for modelling symmetric decision
problems. When solving an influence diagram we determine a set of strategies
for the decisions involved. A strategy for a decision variable is in principle
a function over its past. However, some of the past may be irrelevant for the
decision, and for computational reasons it is important not to deal with
redundant variables in the strategies. We show that current methods (e.g. the
&quot;Decision Bayes-ball&quot; algorithm by Shachter UAI98) do not determine the
relevant past, and we present a complete algorithm.
  Actually, this paper takes a more general outset: When formulating a decision
scenario as an influence diagram, a linear temporal ordering of the decisions
variables is required. This constraint ensures that the decision scenario is
welldefined. However, the structure of a decision scenario often yields certain
decisions conditionally independent, and it is therefore unnecessary to impose
a linear temporal ordering on the decisions. In this paper we deal with partial
influence diagrams i.e. influence diagrams with only a partial temporal
ordering specified. We present a set of conditions which are necessary and
sufficient to ensure that a partial influence diagram is welldefined. These
conditions are used as a basis for the construction of an algorithm for
determining whether or not a partial influence diagram is welldefined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6730</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6730</id><created>2013-01-23</created><authors><author><keyname>Ortiz</keyname><forenames>Luis E.</forenames></author><author><keyname>Kaelbling</keyname><forenames>Leslie Pack</forenames></author></authors><title>Accelerating EM: An Empirical Study</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-512-521</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications require that we learn the parameters of a model from data.
EM is a method used to learn the parameters of probabilistic models for which
the data for some of the variables in the models is either missing or hidden.
There are instances in which this method is slow to converge. Therefore,
several accelerations have been proposed to improve the method. None of the
proposed acceleration methods are theoretically dominant and experimental
comparisons are lacking. In this paper, we present the different proposed
accelerations and try to compare them experimentally. From the results of the
experiments, we argue that some acceleration of EM is always possible, but that
which acceleration is superior depends on properties of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6731</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6731</id><created>2013-01-23</created><authors><author><keyname>Pavlovic</keyname><forenames>Vladimir</forenames></author><author><keyname>Frey</keyname><forenames>Brendan J.</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>Variational Learning in Mixed-State Dynamic Graphical Models</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-522-530</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real-valued stochastic time-series are locally linear (Gassian), but
globally non-linear. For example, the trajectory of a human hand gesture can be
viewed as a linear dynamic system driven by a nonlinear dynamic system that
represents muscle actions. We present a mixed-state dynamic graphical model in
which a hidden Markov model drives a linear dynamic system. This combination
allows us to model both the discrete and continuous causes of trajectories such
as human gestures. The number of computations needed for exact inference is
exponential in the sequence length, so we derive an approximate variational
inference technique that can also be used to learn the parameters of the
discrete and continuous models. We show how the mixed-state model and the
variational technique can be used to classify human hand gestures made with a
computer mouse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6732</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6732</id><created>2013-01-23</created><authors><author><keyname>Pennock</keyname><forenames>David M.</forenames></author><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author></authors><title>Graphical Representations of Consensus Belief</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-531-540</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphical models based on conditional independence support concise encodings
of the subjective belief of a single agent. A natural question is whether the
consensus belief of a group of agents can be represented with equal parsimony.
We prove, under relatively mild assumptions, that even if everyone agrees on a
common graph topology, no method of combining beliefs can maintain that
structure. Even weaker conditions rule out local aggregation within conditional
probability tables. On a more positive note, we show that if probabilities are
combined with the logarithmic opinion pool (LogOP), then commonly held Markov
independencies are maintained. This suggests a straightforward procedure for
constructing a consensus Markov network. We describe an algorithm for computing
the LogOP with time complexity comparable to that of exact Bayesian inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6733</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6733</id><created>2013-01-23</created><authors><author><keyname>Pfeffer</keyname><forenames>Avi</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author><author><keyname>Milch</keyname><forenames>Brian</forenames></author><author><keyname>Takusagawa</keyname><forenames>Ken T.</forenames></author></authors><title>SPOOK: A System for Probabilistic Object-Oriented Knowledge
  Representation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-541-550</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work, we pointed out the limitations of standard Bayesian
networks as a modeling framework for large, complex domains. We proposed a new,
richly structured modeling language, {em Object-oriented Bayesian Netorks},
that we argued would be able to deal with such domains. However, it turns out
that OOBNs are not expressive enough to model many interesting aspects of
complex domains: the existence of specific named objects, arbitrary relations
between objects, and uncertainty over domain structure. These aspects are
crucial in real-world domains such as battlefield awareness. In this paper, we
present SPOOK, an implemented system that addresses these limitations. SPOOK
implements a more expressive language that allows it to represent the
battlespace domain naturally and compactly. We present a new inference
algorithm that utilizes the model structure in a fundamental way, and show
empirically that it achieves orders of magnitude speedup over existing
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6734</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6734</id><created>2013-01-23</created><authors><author><keyname>Portinale</keyname><forenames>Luigi</forenames></author><author><keyname>Bobbio</keyname><forenames>Andrea</forenames></author></authors><title>Bayesian Networks for Dependability Analysis: an Application to Digital
  Control Reliability</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-551-558</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian Networks (BN) provide robust probabilistic methods of reasoning
under uncertainty, but despite their formal grounds are strictly based on the
notion of conditional dependence, not much attention has been paid so far to
their use in dependability analysis. The aim of this paper is to propose BN as
a suitable tool for dependability analysis, by challenging the formalism with
basic issues arising in dependability tasks. We will discuss how both modeling
and analysis issues can be naturally dealt with by BN. Moreover, we will show
how some limitations intrinsic to combinatorial dependability methods such as
Fault Trees can be overcome using BN. This will be pursued through the study of
a real-world example concerning the reliability analysis of a redundant digital
Programmable Logic Controller (PLC) with majority voting 2:3
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6735</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6735</id><created>2013-01-23</created><authors><author><keyname>Renooij</keyname><forenames>Silja</forenames></author><author><keyname>van der Gaag</keyname><forenames>Linda C.</forenames></author></authors><title>Enhancing QPNs for Trade-off Resolution</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-559-566</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Qualitative probabilistic networks have been introduced as qualitative
abstractions of Bayesian belief networks. One of the major drawbacks of these
qualitative networks is their coarse level of detail, which may lead to
unresolved trade-offs during inference. We present an enhanced formalism for
qualitative networks with a finer level of detail. An enhanced qualitative
probabilistic network differs from a regular qualitative network in that it
distinguishes between strong and weak influences. Enhanced qualitative
probabilistic networks are purely qualitative in nature, as regular qualitative
networks are, yet allow for efficiently resolving trade-offs during inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6736</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6736</id><created>2013-01-23</created><authors><author><keyname>Sabbadin</keyname><forenames>Regis</forenames></author></authors><title>A Possibilistic Model for Qualitative Sequential Decision Problems under
  Uncertainty in Partially Observable Environments</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-567-574</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we propose a qualitative (ordinal) counterpart for the
Partially Observable Markov Decision Processes model (POMDP) in which the
uncertainty, as well as the preferences of the agent, are modeled by
possibility distributions. This qualitative counterpart of the POMDP model
relies on a possibilistic theory of decision under uncertainty, recently
developed. One advantage of such a qualitative framework is its ability to
escape from the classical obstacle of stochastic POMDPs, in which even with a
finite state space, the obtained belief state space of the POMDP is infinite.
Instead, in the possibilistic framework even if exponentially larger than the
state space, the belief state space remains finite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6737</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6737</id><created>2013-01-23</created><authors><author><keyname>Schum</keyname><forenames>David A.</forenames></author></authors><title>Inference Networks and the Evaluation of Evidence: Alternative Analyses</title><categories>cs.AI stat.AP</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-575-584</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inference networks have a variety of important uses and are constructed by
persons having quite different standpoints. Discussed in this paper are three
different but complementary methods for generating and analyzing probabilistic
inference networks. The first method, though over eighty years old, is very
useful for knowledge representation in the task of constructing probabilistic
arguments. It is also useful as a heuristic device in generating new forms of
evidence. The other two methods are formally equivalent ways for combining
probabilities in the analysis of inference networks. The use of these three
methods is illustrated in an analysis of a mass of evidence in a celebrated
American law case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6738</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6738</id><created>2013-01-23</created><authors><author><keyname>Settimi</keyname><forenames>Raffaella</forenames></author><author><keyname>Smith</keyname><forenames>Jim Q.</forenames></author><author><keyname>Gargoum</keyname><forenames>A. S.</forenames></author></authors><title>Approximate Learning in Complex Dynamic Bayesian Networks</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-585-593</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we extend the work of Smith and Papamichail (1999) and present
fast approximate Bayesian algorithms for learning in complex scenarios where at
any time frame, the relationships between explanatory state space variables can
be described by a Bayesian network that evolve dynamically over time and the
observations taken are not necessarily Gaussian. It uses recent developments in
approximate Bayesian forecasting methods in combination with more familiar
Gaussian propagation algorithms on junction trees. The procedure for learning
state parameters from data is given explicitly for common sampling
distributions and the methodology is illustrated through a real application.
The efficiency of the dynamic approximation is explored by using the Hellinger
divergence measure and theoretical bounds for the efficacy of such a procedure
are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6739</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6739</id><created>2013-01-23</created><authors><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author></authors><title>Efficient Value of Information Computation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-594-601</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most useful sensitivity analysis techniques of decision analysis
is the computation of value of information (or clairvoyance), the difference in
value obtained by changing the decisions by which some of the uncertainties are
observed. In this paper, some simple but powerful extensions to previous
algorithms are introduced which allow an efficient value of information
calculation on the rooted cluster tree (or strong junction tree) used to solve
the original decision problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6740</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6740</id><created>2013-01-23</created><authors><author><keyname>Shatkay</keyname><forenames>Hagit</forenames></author></authors><title>Learning Hidden Markov Models with Geometrical Constraints</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-602-611</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hidden Markov models (HMMs) and partially observable Markov decision
processes (POMDPs) form a useful tool for modeling dynamical systems. They are
particularly useful for representing environments such as road networks and
office buildings, which are typical for robot navigation and planning. The work
presented here is concerned with acquiring such models. We demonstrate how
domain-specific information and constraints can be incorporated into the
statistical estimation process, greatly improving the learned models in terms
of the model quality, the number of iterations required for convergence and
robustness to reduction in the amount of available data. We present new
initialization heuristics which can be used even when the data suffers from
cumulative rotational error, new update rules for the model parameters, as an
instance of generalized EM, and a strategy for enforcing complete geometrical
consistency in the model. Experimental results demonstrate the effectiveness of
our approach for both simulated and real robot data, in traditionally
hard-to-learn environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6741</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6741</id><created>2013-01-23</created><authors><author><keyname>Smets</keyname><forenames>Philippe</forenames></author></authors><title>Practical Uses of Belief Functions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-612-621</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present examples where the use of belief functions provided sound and
elegant solutions to real life problems. These are essentially characterized by
?missing' information. The examples deal with 1) discriminant analysis using a
learning set where classes are only partially known; 2) an information
retrieval systems handling inter-documents relationships; 3) the combination of
data from sensors competent on partially overlapping frames; 4) the
determination of the number of sources in a multi-sensor environment by
studying the inter-sensors contradiction. The purpose of the paper is to report
on such applications where the use of belief functions provides a convenient
tool to handle ?messy' data problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6742</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6742</id><created>2013-01-23</created><authors><author><keyname>Takikawa</keyname><forenames>Masami</forenames></author><author><keyname>D'Ambrosio</keyname><forenames>Bruce</forenames></author></authors><title>Multiplicative Factorization of Noisy-Max</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-622-630</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The noisy-or and its generalization noisy-max have been utilized to reduce
the complexity of knowledge acquisition. In this paper, we present a new
representation of noisy-max that allows for efficient inference in general
Bayesian networks. Empirical studies show that our method is capable of
computing queries in well-known large medical networks, QMR-DT and CPCS, for
which no previous exact inference method has been shown to perform well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6743</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6743</id><created>2013-01-23</created><authors><author><keyname>van der Torre</keyname><forenames>Leendert</forenames></author><author><keyname>Tan</keyname><forenames>Yao-Hua</forenames></author></authors><title>An Update Semantics for Defeasible Obligations</title><categories>cs.AI cs.LO</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-631-638</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The deontic logic DUS is a Deontic Update Semantics for prescriptive
obligations based on the update semantics of Veltman. In DUS the definition of
logical validity of obligations is not based on static truth values but on
dynamic action transitions. In this paper prescriptive defeasible obligations
are formalized in update semantics and the diagnostic problem of defeasible
deontic logic is discussed. Assume a defeasible obligation `normally A ought to
be (done)' together withthe fact `A is not (done).' Is this an exception of the
normality claim, or is it a violation of the obligation? In this paper we
formalize the heuristic principle that it is a violation, unless there is a
more specific overriding obligation. The underlying motivation from legal
reasoning is that criminals should have as little opportunities as possible to
excuse themselves by claiming that their behavior was exceptional rather than
criminal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6744</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6744</id><created>2013-01-23</created><authors><author><keyname>Tresp</keyname><forenames>Volker</forenames></author><author><keyname>Haft</keyname><forenames>Michael</forenames></author><author><keyname>Hofmann</keyname><forenames>Reimar</forenames></author></authors><title>Mixture Approximations to Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-639-646</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structure and parameters in a Bayesian network uniquely specify the
probability distribution of the modeled domain. The locality of both structure
and probabilistic information are the great benefits of Bayesian networks and
require the modeler to only specify local information. On the other hand this
locality of information might prevent the modeler - and even more any other
person - from obtaining a general overview of the important relationships
within the domain. The goal of the work presented in this paper is to provide
an &quot;alternative&quot; view on the knowledge encoded in a Bayesian network which
might sometimes be very helpful for providing insights into the underlying
domain. The basic idea is to calculate a mixture approximation to the
probability distribution represented by the Bayesian network. The mixture
component densities can be thought of as representing typical scenarios implied
by the Bayesian model, providing intuition about the basic relationships. As an
additional benefit, performing inference in the approximate model is very
simple and intuitive and can provide additional insights. The computational
complexity for the calculation of the mixture approximations criticaly depends
on the measure which defines the distance between the probability distribution
represented by the Bayesian network and the approximate distribution. Both the
KL-divergence and the backward KL-divergence lead to inefficient algorithms.
Incidentally, the latter is used in recent work on mixtures of mean field
solutions to which the work presented here is closely related. We show,
however, that using a mean squared error cost function leads to update
equations which can be solved using the junction tree algorithm. We conclude
that the mean squared error cost function can be used for Bayesian networks in
which inference based on the junction tree is tractable. For large networks,
however, one may have to rely on mean field approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6745</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6745</id><created>2013-01-23</created><authors><author><keyname>van der Gaag</keyname><forenames>Linda C.</forenames></author><author><keyname>Renooij</keyname><forenames>Silja</forenames></author><author><keyname>Witteman</keyname><forenames>Cilia L. M.</forenames></author><author><keyname>Aleman</keyname><forenames>Berthe M. P.</forenames></author><author><keyname>Taal</keyname><forenames>Babs G.</forenames></author></authors><title>How to Elicit Many Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-647-654</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In building Bayesian belief networks, the elicitation of all probabilities
required can be a major obstacle. We learned the extent of this often-cited
observation in the construction of the probabilistic part of a complex
influence diagram in the field of cancer treatment. Based upon our negative
experiences with existing methods, we designed a new method for probability
elicitation from domain experts. The method combines various ideas, among which
are the ideas of transcribing probabilities and of using a scale with both
numerical and verbal anchors for marking assessments. In the construction of
the probabilistic part of our influence diagram, the method proved to allow for
the elicitation of many probabilities in little time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6746</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6746</id><created>2013-01-23</created><authors><author><keyname>Voorbraak</keyname><forenames>Frans</forenames></author></authors><title>Probabilistic Belief Change: Expansion, Conditioning and Constraining</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-655-662</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The AGM theory of belief revision has become an important paradigm for
investigating rational belief changes. Unfortunately, researchers working in
this paradigm have restricted much of their attention to rather simple
representations of belief states, namely logically closed sets of propositional
sentences. In our opinion, this has resulted in a too abstract categorisation
of belief change operations: expansion, revision, or contraction. Occasionally,
in the AGM paradigm, also probabilistic belief changes have been considered,
and it is widely accepted that the probabilistic version of expansion is
conditioning. However, we argue that it may be more correct to view
conditioning and expansion as two essentially different kinds of belief change,
and that what we call constraining is a better candidate for being considered
probabilistic expansion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6747</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6747</id><created>2013-01-23</created><authors><author><keyname>Welch</keyname><forenames>Robert L.</forenames></author><author><keyname>Smith</keyname><forenames>Clayton</forenames></author></authors><title>Bayesian Control for Concentrating Mixed Nuclear Waste</title><categories>cs.AI cs.SY</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-663-669</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A control algorithm for batch processing of mixed waste is proposed based on
conditional Gaussian Bayesian networks. The network is compiled during batch
staging for real-time response to sensor input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6748</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6748</id><created>2013-01-23</created><authors><author><keyname>Wong</keyname><forenames>Michael S. K. M.</forenames></author><author><keyname>Butz</keyname><forenames>C. J.</forenames></author></authors><title>Contextual Weak Independence in Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-670-679</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that the notion of (strong) conditional independence (CI) is
too restrictive to capture independencies that only hold in certain contexts.
This kind of contextual independency, called context-strong independence (CSI),
can be used to facilitate the acquisition, representation, and inference of
probabilistic knowledge. In this paper, we suggest the use of contextual weak
independence (CWI) in Bayesian networks. It should be emphasized that the
notion of CWI is a more general form of contextual independence than CSI.
Furthermore, if the contextual strong independence holds for all contexts, then
the notion of CSI becomes strong CI. On the other hand, if the weak contextual
independence holds for all contexts, then the notion of CWI becomes weak
independence (WI) nwhich is a more general noncontextual independency than
strong CI. More importantly, complete axiomatizations are studied for both the
class of WI and the class of CI and WI together. Finally, the interesting
property of WI being a necessary and sufficient condition for ensuring
consistency in granular probabilistic networks is shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6749</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6749</id><created>2013-01-23</created><authors><author><keyname>Xiang</keyname><forenames>Yanping</forenames></author><author><keyname>Jensen</keyname><forenames>Finn Verner</forenames></author></authors><title>Inference in Multiply Sectioned Bayesian Networks with Extended
  Shafer-Shenoy and Lazy Propagation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-680-687</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As Bayesian networks are applied to larger and more complex problem domains,
search for flexible modeling and more efficient inference methods is an ongoing
effort. Multiply sectioned Bayesian networks (MSBNs) extend the HUGIN inference
for Bayesian networks into a coherent framework for flexible modeling and
distributed inference.Lazy propagation extends the Shafer-Shenoy and HUGIN
inference methods with reduced space complexity. We apply the Shafer-Shenoy and
lazy propagation to inference in MSBNs. The combination of the MSBN framework
and lazy propagation provides a better framework for modeling and inference in
very large domains. It retains the modeling flexibility of MSBNs and reduces
the runtime space complexity, allowing exact inference in much larger domains
given the same computational resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6750</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6750</id><created>2013-01-23</created><authors><author><keyname>Xiang</keyname><forenames>Yanping</forenames></author><author><keyname>Poh</keyname><forenames>Kim-Leng</forenames></author></authors><title>Time-Critical Dynamic Decision Making</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-688-695</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent interests in dynamic decision modeling have led to the development of
several representation and inference methods. These methods however, have
limited application under time critical conditions where a trade-off between
model quality and computational tractability is essential. This paper presents
an approach to time-critical dynamic decision modeling. A knowledge
representation and modeling method called the time-critical dynamic influence
diagram is proposed. The formalism has two forms. The condensed form is used
for modeling and model abstraction, while the deployed form which can be
converted from the condensed form is used for inference purposes. The proposed
approach has the ability to represent space-temporal abstraction within the
model. A knowledge-based meta-reasoning approach is proposed for the purpose of
selecting the best abstracted model that provide the optimal trade-off between
model quality and model tractability. An outline of the knowledge-based model
construction algorithm is also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6751</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6751</id><created>2013-01-23</created><authors><author><keyname>Zhang</keyname><forenames>Nevin Lianwen</forenames></author><author><keyname>Lee</keyname><forenames>Stephen S.</forenames></author><author><keyname>Zhang</keyname><forenames>Weihong</forenames></author></authors><title>A Method for Speeding Up Value Iteration in Partially Observable Markov
  Decision Processes</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-696-703</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a technique for speeding up the convergence of value iteration for
partially observable Markov decisions processes (POMDPs). The underlying idea
is similar to that behind modified policy iteration for fully observable Markov
decision processes (MDPs). The technique can be easily incorporated into any
existing POMDP value iteration algorithms. Experiments have been conducted on
several test problems with one POMDP value iteration algorithm called
incremental pruning. We find that the technique can make incremental pruning
run several orders of magnitude faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6752</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6752</id><created>2013-01-28</created><authors><author><keyname>Tian</keyname><forenames>Yongge</forenames></author></authors><title>How to solve three fundamental linear matrix inequalities in the
  L\&quot;owner partial ordering</title><categories>math.OC cs.SY math.OA</categories><comments>37 pages</comments><msc-class>15A09, 15A24, 15A39, 15A45, 15B57, 90C11, 90C47</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows how to solve analytically the three fundamental linear
matrix inequalities
  $$
  AXB \succcurlyeq C \, (\succ C), \ \ AXA^*\succcurlyeq B \, (\succ B), \ \ AX
+(AX)^{*} \succcurlyeq B \, (\succ B)
  $$ in the L\&quot;owner partial ordering by using ranks, inertias and generalized
inverses of matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6770</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6770</id><created>2013-01-28</created><authors><author><keyname>Zhixiang</keyname><affiliation>Eddie</affiliation></author><author><keyname>Xu</keyname></author><author><keyname>Chen</keyname><forenames>Minmin</forenames></author><author><keyname>Weinberger</keyname><forenames>Kilian Q.</forenames></author><author><keyname>Sha</keyname><forenames>Fei</forenames></author></authors><title>An alternative text representation to TF-IDF and Bag-of-Words</title><categories>cs.IR cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In text mining, information retrieval, and machine learning, text documents
are commonly represented through variants of sparse Bag of Words (sBoW) vectors
(e.g. TF-IDF). Although simple and intuitive, sBoW style representations suffer
from their inherent over-sparsity and fail to capture word-level synonymy and
polysemy. Especially when labeled data is limited (e.g. in document
classification), or the text documents are short (e.g. emails or abstracts),
many features are rarely observed within the training corpus. This leads to
overfitting and reduced generalization accuracy. In this paper we propose Dense
Cohort of Terms (dCoT), an unsupervised algorithm to learn improved sBoW
document features. dCoT explicitly models absent words by removing and
reconstructing random sub-sets of words in the unlabeled corpus. With this
approach, dCoT learns to reconstruct frequent words from co-occurring
infrequent words and maps the high dimensional sparse sBoW vectors into a
low-dimensional dense representation. We show that the feature removal can be
marginalized out and that the reconstruction can be solved for in closed-form.
We demonstrate empirically, on several benchmark datasets, that dCoT features
significantly improve the classification accuracy across several document
classification tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6780</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6780</id><created>2013-01-28</created><authors><author><keyname>Zhao</keyname><forenames>Yuchen</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author></authors><title>On Graph Stream Clustering with Side Information</title><categories>cs.DB cs.SI</categories><comments>Full version of SIAM SDM 2013 paper</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Graph clustering becomes an important problem due to emerging applications
involving the web, social networks and bio-informatics. Recently, many such
applications generate data in the form of streams. Clustering massive, dynamic
graph streams is significantly challenging because of the complex structures of
graphs and computational difficulties of continuous data. Meanwhile, a large
volume of side information is associated with graphs, which can be of various
types. The examples include the properties of users in social network
activities, the meta attributes associated with web click graph streams and the
location information in mobile communication networks. Such attributes contain
extremely useful information and has the potential to improve the clustering
process, but are neglected by most recent graph stream mining techniques. In
this paper, we define a unified distance measure on both link structures and
side attributes for clustering. In addition, we propose a novel optimization
framework DMO, which can dynamically optimize the distance metric and make it
adapt to the newly received stream data. We further introduce a carefully
designed statistics SGS(C) which consume constant storage spaces with the
progression of streams. We demonstrate that the statistics maintained are
sufficient for the clustering process as well as the distance optimization and
can be scalable to massive graphs with side attributes. We will present
experiment results to show the advantages of the approach in graph stream
clustering with both links and side information over the baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6787</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6787</id><created>2013-01-28</created><authors><author><keyname>Mhiri</keyname><forenames>Mariem</forenames></author><author><keyname>Cheikhrouhou</keyname><forenames>Karim</forenames></author><author><keyname>Samet</keyname><forenames>Abdelaziz</forenames></author><author><keyname>M&#xe9;riaux</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author></authors><title>Energy-Efficient Spectrum Sharing in Relay-Assisted Cognitive Radio
  Systems</title><categories>cs.IT cs.GT math.IT</categories><comments>IEEE Proc. of the International Conference on Network Games, Control
  and Optimization (NetGCooP), Nov. 2012, Avignon, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work characterizes an important solution concept of a relevant spectrum
game. Two energy-efficient sources communicating with their respective
destination compete for an extra channel brought by a relay charging the used
bandwidth through a pricing mechanism. This game is shown to possess a unique
Nash bargaining solution, exploiting a time-sharing argument. This
Pareto-efficient solution can be implemented by using a distributed
optimization algorithm for which each transmitter uses a simple gradient-type
algorithm and alternately updates its spectrum sharing policy. Typical
numerical results show to what extent spectral efficiency can be improved in a
system involving selfish energy-efficient sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6789</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6789</id><created>2013-01-25</created><authors><author><keyname>Tripathy</keyname><forenames>B. K.</forenames></author><author><keyname>Acharjya</keyname><forenames>D. P.</forenames></author></authors><title>Approximation of Classification and Measures of Uncertainty in Rough Set
  on Two Universal Sets</title><categories>cs.AI</categories><comments>14 pages, International Journal of Advanced Science and Technology
  Vol. 40, March, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of rough set captures indiscernibility of elements in a set. But,
in many real life situations, an information system establishes the relation
between different universes. This gave the extension of rough set on single
universal set to rough set on two universal sets. In this paper, we introduce
approximation of classifications and measures of uncertainty basing upon rough
set on two universal sets employing the knowledge due to binary relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6791</identifier>
 <datestamp>2013-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6791</id><created>2013-01-28</created><updated>2013-10-11</updated><authors><author><keyname>Cai</keyname><forenames>Jian-Feng</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author></authors><title>Guarantees of Total Variation Minimization for Signal Recovery</title><categories>cs.IT cs.CV cs.LG math.IT</categories><comments>lower bounds added; version with Gaussian width, improved bounds;
  stability results added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider using total variation minimization to recover
signals whose gradients have a sparse support, from a small number of
measurements. We establish the proof for the performance guarantee of total
variation (TV) minimization in recovering \emph{one-dimensional} signal with
sparse gradient support. This partially answers the open problem of proving the
fidelity of total variation minimization in such a setting \cite{TVMulti}. In
particular, we have shown that the recoverable gradient sparsity can grow
linearly with the signal dimension when TV minimization is used. Recoverable
sparsity thresholds of TV minimization are explicitly computed for
1-dimensional signal by using the Grassmann angle framework. We also extend our
results to TV minimization for multidimensional signals. Stability of
recovering signal itself using 1-D TV minimization has also been established
through a property called &quot;almost Euclidean property for 1-dimensional TV
norm&quot;. We further give a lower bound on the number of random Gaussian
measurements for recovering 1-dimensional signal vectors with $N$ elements and
$K$-sparse gradients. Interestingly, the number of needed measurements is lower
bounded by $\Omega((NK)^{\frac{1}{2}})$, rather than the $O(K\log(N/K))$ bound
frequently appearing in recovering $K$-sparse signal vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6793</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6793</id><created>2013-01-28</created><authors><author><keyname>M&#xe9;riaux</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Varma</keyname><forenames>Vineeth</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author></authors><title>Mean Field Energy Games in Wireless Networks</title><categories>cs.IT cs.GT math.IT</categories><comments>IEEE Proc. of Asilomar Conf. on Signals, Systems, and Computers, Nov.
  2012, Pacific Grove, CA, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work tackles the problem of energy-efficient distributed power control
in wireless networks with a large number of transmitters. The problem is
modeled by a dynamic game. Each transmitter-receiver communication is
characterized by a state given by the available energy and/or the individual
channel state and whose evolution is governed by certain dynamics. Since
equilibrium analysis in such a (stochastic) game is generally difficult and
even impossible, the problem is approximated by exploiting the large system
assumption. Under an appropriate exchangeability assumption, the corresponding
mean field game is well defined and studied in detail for special cases. The
main contribution of this work is to show how mean field games can be applied
to the problem under investigation and provide illustrative numerical results.
Our results indicate that this approach can lead to significant gains in terms
of energy-efficiency at the resulting equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6798</identifier>
 <datestamp>2014-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6798</id><created>2013-01-28</created><updated>2014-06-09</updated><authors><author><keyname>Asadi</keyname><forenames>Meysam</forenames></author><author><keyname>Torghabeh</keyname><forenames>Ramezan Paravi</forenames></author><author><keyname>Santhanam</keyname><forenames>Narayana P.</forenames></author></authors><title>Stationary and Transition Probabilities in Slow Mixing, Long Memory
  Markov Processes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We observe a length-$n$ sample generated by an unknown,stationary ergodic
Markov process (\emph{model}) over a finite alphabet $\mathcal{A}$. Given any
string $\bf{w}$ of symbols from $\mathcal{A}$ we want estimates of the
conditional probability distribution of symbols following $\bf{w}$, as well as
the stationary probability of $\bf{w}$. Two distinct problems that complicate
estimation in this setting are (i) long memory, and (ii) \emph{slow mixing}
which could happen even with only one bit of memory.
  Any consistent estimator in this setting can only converge pointwise over the
class of all ergodic Markov models. Namely, given any estimator and any sample
size $n$, the underlying model could be such that the estimator performs poorly
on a sample of size $n$ with high probability. But can we look at a length-$n$
sample and identify \emph{if} an estimate is likely to be accurate?
  Since the memory is unknown \emph{a-priori}, a natural approach is to
estimate a potentially coarser model with memory $k_n=\mathcal{O}(\log n)$. As
$n$ grows, pointwise consistent estimates that hold eventually almost surely
(eas) are known so long as the scaling of $k_n$ is not superlogarithmic in $n$.
Here, rather than eas convergence results, we want the best answers possible
with a length-$n$ sample. Combining results in universal compression with
Aldous' coupling arguments, we obtain sufficient conditions on the length-$n$
sample (even for slow mixing models) to identify when naive (i) estimates of
the conditional probabilities and (ii) estimates related to the stationary
probabilities are accurate; and also bound the deviations of the naive
estimates from true values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6799</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6799</id><created>2013-01-28</created><updated>2013-02-16</updated><authors><author><keyname>B&#xe9;rard</keyname><forenames>B.</forenames></author><author><keyname>Mullins</keyname><forenames>J.</forenames></author><author><keyname>Sassolas</keyname><forenames>M.</forenames></author></authors><title>Quantifying Opacity</title><categories>cs.CR</categories><comments>To appear in Mathematical Structures in Computer Science, Cambridge
  University Press</comments><doi>10.1017/S0960129513000637</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opacity is a general language-theoretic framework in which several security
properties of a system can be expressed. Its parameters are a predicate, given
as a subset of runs of the system, and an observation function, from the set of
runs into a set of observables. The predicate describes secret information in
the system and, in the possibilistic setting, it is opaque if its membership
cannot be inferred from observation.
  In this paper, we propose several notions of quantitative opacity for
probabilistic systems, where the predicate and the observation function are
seen as random variables. Our aim is to measure (i) the probability of opacity
leakage relative to these random variables and (ii) the level of uncertainty
about membership of the predicate inferred from observation. We show how these
measures extend possibilistic opacity, we give algorithms to compute them for
regular secrets and observations, and we apply these computations on several
classical examples. We finally partially investigate the non-deterministic
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6802</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6802</id><created>2013-01-28</created><authors><author><keyname>Wang</keyname><forenames>Cheng</forenames></author></authors><title>Algorithms for Generating Large-scale Clustered Random Graphs</title><categories>physics.soc-ph cs.SI</categories><comments>31 pages,7 figures, and 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real social networks are often compared to random graphs in order to assess
whether their typological structure could be the result of random processes.
However, an Erd\H{o}s-R\'enyi random graph in large scale is often lack of
local structure beyond the dyadic level and as a result we need to generate the
clustered random graph instead of the simple random graph to compare the local
structure at the triadic level. In this paper a generalized version of
Gleeson's algorithm is advanced to generate a clustered random graph in
large-scale which persists the number of nodes |V|, the number of edges |E|,
and the global clustering coefficient C{\Delta} as in the real network. And it
also has advantages in randomness evaluation and computation time when
comparing with the existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6804</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6804</id><created>2013-01-28</created><authors><author><keyname>Ying</keyname><forenames>Mingsheng</forenames></author><author><keyname>Feng</keyname><forenames>Yuang</forenames></author><author><keyname>Yu</keyname><forenames>Nengkun</forenames></author></authors><title>Quantum Information-Flow Security: Noninterference and Access Control</title><categories>cs.CR quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum cryptography has been extensively studied in the last twenty years,
but information-flow security of quantum computing and communication systems
has been almost untouched in the previous research. Duo to the essential
difference between classical and quantum systems, formal methods developed for
classical systems, including probabilistic systems, cannot be directly applied
to quantum systems. This paper defines an automata model in which we can
rigorously reason about information-flow security of quantum systems. The model
is a quantum generalisation of Goguen and Meseguer's noninterference. The
unwinding proof technique for quantum noninterference is developed, and a
certain compositionality of security for quantum systems is established. The
proposed formalism is then used to prove security of access control in quantum
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6809</identifier>
 <datestamp>2014-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6809</id><created>2013-01-28</created><updated>2014-06-25</updated><authors><author><keyname>Tagliasacchi</keyname><forenames>Andrea</forenames></author></authors><title>Skeletal Representations and Applications</title><categories>cs.GR</categories><comments>42 pages, SFU Depth Exam</comments><report-no>SFU-CMPT TR 2012-55-1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When representing a solid object there are alternatives to the use of
traditional explicit (surface meshes) or implicit (zero crossing of implicit
functions) methods. Skeletal representations encode shape information in a
mixed fashion: they are composed of a set of explicit primitives, yet they are
able to efficiently encode the shape's volume as well as its topology. I will
discuss, in two dimensions, how symmetry can be used to reduce the
dimensionality of the data (from a 2D solid to a 1D curve), and how this
relates to the classical definition of skeletons by Medial Axis Transform.
While the medial axis of a 2D shape is composed of a set of curves, in 3D it
results in a set of sheets connected in a complex fashion. Because of this
complexity, medial skeletons are difficult to use in practical applications.
Curve skeletons address this problem by strictly requiring their geometry to be
one dimensional, resulting in an intuitive yet powerful shape representation.
In this report I will define both medial and curve skeletons and discuss their
mutual relationship. I will also present several algorithms for their
computation and a variety of scenarios where skeletons are employed, with a
special focus on geometry processing and shape analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6817</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6817</id><created>2013-01-28</created><authors><author><keyname>Govc</keyname><forenames>Dejan</forenames></author></authors><title>On the definition of homological critical value</title><categories>math.AT cs.CG math.GN</categories><comments>14 pages, 9 figures</comments><msc-class>55N99 (Primary) 54G20, 54E45, 68W30 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We point out that there is a problem with the definition of homological
critical value (as defined in the widely cited paper \cite{stability} by
Cohen-Steiner, Edelsbrunner and Harer). Under that definition, the critical
value lemma of \cite{stability} in fact fails. We provide several
counterexamples and a definition (due to Bubenik and Scott
\cite{categorification}) we feel should be preferred and under which the
critical value lemma does indeed hold. One of the counterexamples we have found
is a height function on a compact smooth manifold. In the end we prove that,
despite all this, a modified version of the critical value lemma remains valid
under the original definition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6822</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6822</id><created>2013-01-28</created><authors><author><keyname>Sweeney</keyname><forenames>Latanya</forenames></author></authors><title>Discrimination in Online Ad Delivery</title><categories>cs.IR cs.CY</categories><report-no>1071-1</report-no><acm-class>H.3.3; H.3.5; K.4.1; K.4.2; K.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Google search for a person's name, such as &quot;Trevon Jones&quot;, may yield a
personalized ad for public records about Trevon that may be neutral, such as
&quot;Looking for Trevon Jones?&quot;, or may be suggestive of an arrest record, such as
&quot;Trevon Jones, Arrested?&quot;. This writing investigates the delivery of these
kinds of ads by Google AdSense using a sample of racially associated names and
finds statistically significant discrimination in ad delivery based on searches
of 2184 racially associated personal names across two websites. First names,
assigned at birth to more black or white babies, are found predictive of race
(88% black, 96% white), and those assigned primarily to black babies, such as
DeShawn, Darnell and Jermaine, generated ads suggestive of an arrest in 81 to
86 percent of name searches on one website and 92 to 95 percent on the other,
while those assigned at birth primarily to whites, such as Geoffrey, Jill and
Emma, generated more neutral copy: the word &quot;arrest&quot; appeared in 23 to 29
percent of name searches on one site and 0 to 60 percent on the other. On the
more ad trafficked website, a black-identifying name was 25% more likely to get
an ad suggestive of an arrest record. A few names did not follow these
patterns. All ads return results for actual individuals and ads appear
regardless of whether the name has an arrest record in the company's database.
The company maintains Google received the same ad text for groups of last names
(not first names), raising questions as to whether Google's technology exposes
racial bias.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6836</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6836</id><created>2013-01-29</created><authors><author><keyname>Kwon</keyname><forenames>Keehang</forenames></author><author><keyname>Park</keyname><forenames>Kyunghwan</forenames></author><author><keyname>Park</keyname><forenames>Mi-Young</forenames></author></authors><title>Towards Interactive Object-Oriented Programming</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To represent interactive objects, we propose a choice-disjunctive declaration
statement of the form S R where S;R are the (procedure or field) declaration
statements within a class. This statement has the following semantics: request
the user to choose one between S and R when an object of this class is created.
This statement is useful for representing interactive objects that require
interactions with the user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6837</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6837</id><created>2013-01-29</created><updated>2014-04-30</updated><authors><author><keyname>Xie</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Chen</forenames></author><author><keyname>Zhang</keyname><forenames>Quan</forenames></author><author><keyname>Tang</keyname><forenames>Chaojing</forenames></author></authors><title>Preserving Privacy of Mobile Reader Holders in Server-less RFID
  Authentication and Searching Protocols</title><categories>cs.CR</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Along with the development of internet of things and pervasive computing,
researchers are increasingly focusing on server-less RFID authentication and
searching protocols, which utilize mobile RFID readers. However, revealing
privacy of mobile reader holders is a widely neglected problem in current
research. This paper concentrates on preserving privacy of mobile reader
holders in server-less RFID authentication and searching protocols. We propose
a detailed requirement as a principle for future protocol designs, and a scheme
to enhance most current protocols. We apply our scheme to two classical
protocols. The comparisons between the original and our enhanced protocols show
that our scheme is secure and effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6843</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6843</id><created>2013-01-29</created><authors><author><keyname>Dewan</keyname><forenames>Prateek</forenames></author><author><keyname>Gupta</keyname><forenames>Mayank</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>ChaMAILeon: Simplified email sharing like never before!</title><categories>cs.CY</categories><comments>6 pages, 4 images</comments><acm-class>H.1.2; H.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While passwords, by definition, are meant to be secret, recent trends in the
Internet usage have witnessed an increasing number of people sharing their
email passwords for both personal and professional purposes. As sharing
passwords increases the chances of your passwords being compromised, leading
websites like Google strongly advise their users not to share their passwords
with anyone. To cater to this conflict of usability versus security and
privacy, we introduce ChaMAILeon, an experimental service, which allows users
to share their email passwords while maintaining their privacy and not
compromising their security. In this report, we discuss the technical details
of the implementation of ChaMAILeon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6847</identifier>
 <datestamp>2013-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6847</id><created>2013-01-29</created><updated>2013-09-17</updated><authors><author><keyname>Li</keyname><forenames>Taiyong</forenames></author><author><keyname>Zhang</keyname><forenames>Zhilin</forenames></author></authors><title>Robust Face Recognition via Block Sparse Bayesian Learning</title><categories>cs.CV</categories><comments>Accepted by Mathematical Problems in Engineering in 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face recognition (FR) is an important task in pattern recognition and
computer vision. Sparse representation (SR) has been demonstrated to be a
powerful framework for FR. In general, an SR algorithm treats each face in a
training dataset as a basis function, and tries to find a sparse representation
of a test face under these basis functions. The sparse representation
coefficients then provide a recognition hint. Early SR algorithms are based on
a basic sparse model. Recently, it has been found that algorithms based on a
block sparse model can achieve better recognition rates. Based on this model,
in this study we use block sparse Bayesian learning (BSBL) to find a sparse
representation of a test face for recognition. BSBL is a recently proposed
framework, which has many advantages over existing block-sparse-model based
algorithms. Experimental results on the Extended Yale B, the AR and the CMU PIE
face databases show that using BSBL can achieve better recognition rates and
higher robustness than state-of-the-art algorithms in most cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6849</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6849</id><created>2013-01-29</created><updated>2013-03-13</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Ivanova</keyname><forenames>Inga A.</forenames></author></authors><title>Mutual Redundancies in Inter-human Communication Systems: Steps Towards
  a Calculus of Processing Meaning</title><categories>cs.DL</categories><comments>forthcoming in the Journal of the American Society for Information
  Science and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of inter-human communication requires a more complex framework than
Shannon's (1948) mathematical theory of communication because &quot;information&quot; is
defined in the latter case as meaningless uncertainty. Assuming that meaning
cannot be communicated, we extend Shannon's theory by defining mutual
redundancy as a positional counterpart of the relational communication of
information. Mutual redundancy indicates the surplus of meanings that can be
provided to the exchanges in reflexive communications. The information is
redundant because based on &quot;pure sets,&quot; that is, without subtraction of mutual
information in the overlaps. We show that in the three-dimensional case (e.g.,
of a Triple Helix of university-industry-government relations), mutual
redundancy is equal to mutual information (Rxyz = Txyz); but when the
dimensionality is even, the sign is different. We generalize to the measurement
in N dimensions and proceed to the interpretation. Using Luhmann's
social-systems theory and/or Giddens' structuration theory, mutual redundancy
can be provided with an interpretation in the sociological case: different
meaning-processing structures code and decode with other algorithms. A surplus
of (&quot;absent&quot;) options can then be generated that add to the redundancy.
Luhmann's &quot;functional (sub)systems&quot; of expectations or Giddens' &quot;rule-resource
sets&quot; are positioned mutually, but coupled operationally in events or
&quot;instantiated&quot; in actions. Shannon-type information is generated by the
mediation, but the &quot;structures&quot; are (re-)positioned towards one another as sets
of (potentially counterfactual) expectations. The structural differences among
the coding and decoding algorithms provide a source of additional options in
reflexive and anticipatory communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6870</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6870</id><created>2013-01-29</created><authors><author><keyname>Malhotra</keyname><forenames>Anshu</forenames></author><author><keyname>Totti</keyname><forenames>Luam</forenames></author><author><keyname>Meira</keyname><forenames>Wagner</forenames><suffix>Jr.</suffix></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author><author><keyname>Almeida</keyname><forenames>Virgilio</forenames></author></authors><title>Studying User Footprints in Different Online Social Networks</title><categories>cs.SI</categories><comments>The paper is already published in ASONAM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the growing popularity and usage of online social media services, people
now have accounts (some times several) on multiple and diverse services like
Facebook, LinkedIn, Twitter and YouTube. Publicly available information can be
used to create a digital footprint of any user using these social media
services. Generating such digital footprints can be very useful for
personalization, profile management, detecting malicious behavior of users. A
very important application of analyzing users' online digital footprints is to
protect users from potential privacy and security risks arising from the huge
publicly available user information. We extracted information about user
identities on different social networks through Social Graph API, FriendFeed,
and Profilactic; we collated our own dataset to create the digital footprints
of the users. We used username, display name, description, location, profile
image, and number of connections to generate the digital footprints of the
user. We applied context specific techniques (e.g. Jaro Winkler similarity,
Wordnet based ontologies) to measure the similarity of the user profiles on
different social networks. We specifically focused on Twitter and LinkedIn. In
this paper, we present the analysis and results from applying automated
classifiers for disambiguating profiles belonging to the same user from
different social networks. UserID and Name were found to be the most
discriminative features for disambiguating user profiles. Using the most
promising set of features and similarity metrics, we achieved accuracy,
precision and recall of 98%, 99%, and 96%, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6879</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6879</id><created>2013-01-29</created><updated>2013-07-15</updated><authors><author><keyname>Himpe</keyname><forenames>Christian</forenames></author><author><keyname>Ohlberger</keyname><forenames>Mario</forenames></author></authors><title>A Unified Software Framework for Empirical Gramians</title><categories>math.OC cs.MS cs.SY math.DS</categories><comments>Preprint</comments><msc-class>93c99</msc-class><acm-class>G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common approach in model reduction is balanced truncation, which is based
on gramian matrices classifiying certain attributes of states or parameters of
a given dynamic system. Initially restricted to linear systems, the empirical
gramians not only extended this concept to nonlinear systems, but also provide
a uniform computational method. This work introduces a unified software
framework supplying routines for six types of empirical gramians. The gramian
types will be discussed and applied in a model reduction framework for
multiple-input-multiple-output (MIMO) systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6880</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6880</id><created>2013-01-29</created><updated>2013-03-05</updated><authors><author><keyname>Roy</keyname><forenames>Shibdas</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Huntington</keyname><forenames>Elanor H.</forenames></author></authors><title>Adaptive Continuous Homodyne Phase Estimation Using Robust
  Fixed-Interval Smoothing</title><categories>quant-ph cs.SY math.OC</categories><comments>8 pages, 5 figures, Proceedings of the 2013 American Control
  Conference</comments><doi>10.1109/ACC.2013.6580312</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive homodyne estimation of a continuously evolving optical phase using
time-symmetric quantum smoothing has been demonstrated experimentally to
provide superior accuracy in the phase estimate compared to adaptive or
nonadaptive estimation using filtering alone. Here, we illustrate how the
mean-square error in the adaptive phase estimate may be further reduced below
the standard quantum limit for the stochastic noise process considered by using
a Rauch-Tung-Striebel smoother as the estimator, alongwith an optimal Kalman
filter in the feedback loop. Further, the estimation using smoothing can be
made robust to uncertainties in the underlying parameters of the noise process
modulating the system phase to be estimated. This has been done using a robust
fixed-interval smoother designed for uncertain systems satisfying a certain
integral quadratic constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6898</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6898</id><created>2013-01-29</created><updated>2013-09-02</updated><authors><author><keyname>Hellmuth</keyname><forenames>Marc</forenames></author><author><keyname>Ostermeier</keyname><forenames>Lydia</forenames></author><author><keyname>Stadler</keyname><forenames>Peter F.</forenames></author></authors><title>Square Property, Equitable Partitions, and Product-like Graphs</title><categories>cs.DM math.CO</categories><comments>20 pages, 6 figures</comments><doi>10.1016/j.disc.2013.12.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Equivalence relations on the edge set of a graph $G$ that satisfy restrictive
conditions on chordless squares play a crucial role in the theory of Cartesian
graph products and graph bundles. We show here that such relations in a natural
way induce equitable partitions on the vertex set of $G$, which in turn give
rise to quotient graphs that can have a rich product structure even if $G$
itself is prime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6899</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6899</id><created>2013-01-29</created><authors><author><keyname>Aggarwal</keyname><forenames>Anupama</forenames></author><author><keyname>Rajadesingan</keyname><forenames>Ashwin</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>PhishAri: Automatic Realtime Phishing Detection on Twitter</title><categories>cs.SI physics.soc-ph</categories><comments>Best Paper Award at APWG eCRS 2012, #phishing #Twitter
  #realtime-detection #usable #end-user-tool</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of online social media, phishers have started using social
networks like Twitter, Facebook, and Foursquare to spread phishing scams.
Twitter is an immensely popular micro-blogging network where people post short
messages of 140 characters called tweets. It has over 100 million active users
who post about 200 million tweets everyday. Phishers have started using Twitter
as a medium to spread phishing because of this vast information dissemination.
Further, it is difficult to detect phishing on Twitter unlike emails because of
the quick spread of phishing links in the network, short size of the content,
and use of URL obfuscation to shorten the URL. Our technique, PhishAri, detects
phishing on Twitter in realtime. We use Twitter specific features along with
URL features to detect whether a tweet posted with a URL is phishing or not.
Some of the Twitter specific features we use are tweet content and its
characteristics like length, hashtags, and mentions. Other Twitter features
used are the characteristics of the Twitter user posting the tweet such as age
of the account, number of tweets, and the follower-followee ratio. These
Twitter specific features coupled with URL based features prove to be a strong
mechanism to detect phishing tweets. We use machine learning classification
techniques and detect phishing tweets with an accuracy of 92.52%. We have
deployed our system for end-users by providing an easy to use Chrome browser
extension which works in realtime and classifies a tweet as phishing or safe.
We show that we are able to detect phishing tweets at zero hour with high
accuracy which is much faster than public blacklists and as well as Twitter's
own defense mechanism to detect malicious content. To the best of our
knowledge, this is the first realtime, comprehensive and usable system to
detect phishing on Twitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6900</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6900</id><created>2013-01-29</created><authors><author><keyname>Kaltenbrunner</keyname><forenames>Andreas</forenames></author><author><keyname>Arag&#xf3;n</keyname><forenames>Pablo</forenames></author><author><keyname>Laniado</keyname><forenames>David</forenames></author><author><keyname>Volkovich</keyname><forenames>Yana</forenames></author></authors><title>Not all paths lead to Rome: Analysing the network of sister cities</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work analyses the practice of sister city pairing. We investigate
structural properties of the resulting city and country networks and present
rankings of the most central nodes in these networks. We identify different
country clusters and find that the practice of sister city pairing is not
influenced by geographical proximity but results in highly assortative
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6905</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6905</id><created>2013-01-29</created><updated>2014-04-24</updated><authors><author><keyname>Kowalski</keyname><forenames>Robert</forenames></author><author><keyname>Sadri</keyname><forenames>Fariba</forenames></author></authors><title>Towards a Logic-Based Unifying Framework for Computing</title><categories>cs.LO cs.AI cs.DB cs.PL</categories><comments>An improved version of this paper will be published in the journal,
  New Generation Computing, with the title &quot;Reactive Computing as Model
  Generation&quot;. In the meanwhile, a copy of the revised paper can be found on
  http://www.doc.ic.ac.uk/~rak/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a logic-based, framework inspired by artificial
intelligence, but scaled down for practical database and programming
applications. Computation in the framework is viewed as the task of generating
a sequence of state transitions, with the purpose of making an agent's goals
all true. States are represented by sets of atomic sentences (or facts),
representing the values of program variables, tuples in a coordination
language, facts in relational databases, or Herbrand models.
  In the model-theoretic semantics, the entire sequence of states and events
are combined into a single model-theoretic structure, by associating timestamps
with facts and events. But in the operational semantics, facts are updated
destructively, without timestamps. We show that the model generated by
destructive updates is identical to the model generated by reasoning with facts
containing timestamps. We also extend the model with intentional predicates and
composite event predicates defined by logic programs containing conditions in
first-order logic, which query the current state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6916</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6916</id><created>2013-01-29</created><authors><author><keyname>Gripon</keyname><forenames>Vincent</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael</forenames></author></authors><title>Reconstructing a Graph from Path Traces</title><categories>cs.DS cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of inferring the structure of a network from
indirect observations. Each observation (a &quot;trace&quot;) is the unordered set of
nodes which are activated along a path through the network. Since a trace does
not convey information about the order of nodes within the path, there are many
feasible orders for each trace observed, and thus the problem of inferring the
network from traces is, in general, illposed. We propose and analyze an
algorithm which inserts edges by ordering each trace into a path according to
which pairs of nodes in the path co-occur most frequently in the observations.
When all traces involve exactly 3 nodes, we derive necessary and sufficient
conditions for the reconstruction algorithm to exactly recover the graph.
Finally, for a family of random graphs, we present expressions for
reconstruction error probabilities (false discoveries and missed detections).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6917</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6917</id><created>2013-01-29</created><updated>2013-04-20</updated><authors><author><keyname>Gripon</keyname><forenames>Vincent</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael</forenames></author></authors><title>Maximum Likelihood Associative Memories</title><categories>cs.IT cs.IR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Associative memories are structures that store data in such a way that it can
later be retrieved given only a part of its content -- a sort-of
error/erasure-resilience property. They are used in applications ranging from
caches and memory management in CPUs to database engines. In this work we study
associative memories built on the maximum likelihood principle. We derive
minimum residual error rates when the data stored comes from a uniform binary
source. Second, we determine the minimum amount of memory required to store the
same data. Finally, we bound the computational complexity for message
retrieval. We then compare these bounds with two existing associative memory
architectures: the celebrated Hopfield neural networks and a neural network
architecture introduced more recently by Gripon and Berrou.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6923</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6923</id><created>2013-01-29</created><authors><author><keyname>Ghozlan</keyname><forenames>Hassan</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author></authors><title>On Wiener Phase Noise Channels at High Signal-to-Noise Ratio</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a waveform channel where the transmitted signal is corrupted by
Wiener phase noise and additive white Gaussian noise (AWGN). A discrete-time
channel model that takes into account the effect of filtering on the phase
noise is developed. The model is based on a multi-sample receiver which, at
high Signal-to-Noise Ratio (SNR), achieves a rate that grows logarithmically
with the SNR if the number of samples per symbol grows with the square-root of
the SNR. Moreover, the pre-log factor is at least 1/2 in this case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6932</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6932</id><created>2013-01-29</created><authors><author><keyname>Jain</keyname><forenames>Paridhi</forenames></author><author><keyname>Rodrigues</keyname><forenames>Tiago</forenames></author><author><keyname>Magno</keyname><forenames>Gabriel</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author><author><keyname>Almeida</keyname><forenames>Virgilio</forenames></author></authors><title>Cross-Pollination of Information in Online Social Media: A Case Study on
  Popular Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>This report has been published in SocialCom PASSAT 2011 as a six page
  short paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Owing to the popularity of Online Social Media (OSM), Internet users share a
lot of information (including personal) on and across OSM services every day.
For example, it is common to find a YouTube video embedded in a blog post with
an option to share the link on Facebook. Users recommend, comment, and forward
information they receive from friends, contributing in spreading the
information in and across OSM services. We term this information diffusion
process from one OSM service to another as Cross-Pollination, and the network
formed by users who participate in Cross-Pollination and content produced in
the network as \emph{Cross-Pollinated network}. Research has been done about
information diffusion within one OSM service, but little is known about
Cross-Pollination. In this paper, we aim at filling this gap by studying how
information (video, photo, location) from three popular OSM services (YouTube,
Flickr and Foursquare) diffuses on Twitter, the most popular microblogging
service. Our results show that Cross-Pollinated networks follow temporal and
topological characteristics of the diffusion OSM (Twitter in our study).
Furthermore, popularity of information on source OSM (YouTube, Flickr and
Foursquare) does not imply its popularity on Twitter. Our results also show
that Cross-Pollination helps Twitter in terms of traffic generation and user
involvement, but only a small fraction of videos and photos gain a significant
number of views from Twitter. We believe this is the first research work which
explicitly characterizes the diffusion of information across different OSM
services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6938</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6938</id><created>2013-01-29</created><authors><author><keyname>Karasik</keyname><forenames>Roy</forenames><affiliation>Shitz</affiliation></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Robust Uplink Communications over Fading Channels with Variable Backhaul
  Connectivity</title><categories>cs.IT math.IT</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two mobile users communicate with a central decoder via two base stations.
Communication between the mobile users and the base stations takes place over a
Gaussian interference channel with constant channel gains or quasi-static
fading. Instead, the base stations are connected to the central decoder through
orthogonal finite-capacity links, whose connectivity is subject to random
fluctuations. There is only receive-side channel state information, and hence
the mobile users are unaware of the channel state and of the backhaul
connectivity state, while the base stations know the fading coefficients but
are uncertain about the backhaul links' state. The base stations are oblivious
to the mobile users' codebooks and employ compress-and-forward to relay
information to the central decoder. Upper and lower bounds are derived on
average achievable throughput with respect to the prior distribution of the
fading coefficients and of the backhaul links' states. The lower bounds are
obtained by proposing strategies that combine the broadcast coding approach and
layered distributed compression techniques. The upper bound is obtained by
assuming that all the nodes know the channel state. Numerical results confirm
the advantages of the proposed approach with respect to conventional non-robust
strategies in both scenarios with and without fading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6939</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6939</id><created>2013-01-29</created><updated>2013-01-30</updated><authors><author><keyname>Grefenstette</keyname><forenames>Edward</forenames></author><author><keyname>Dinu</keyname><forenames>Georgiana</forenames></author><author><keyname>Zhang</keyname><forenames>Yao-Zhong</forenames></author><author><keyname>Sadrzadeh</keyname><forenames>Mehrnoosh</forenames></author><author><keyname>Baroni</keyname><forenames>Marco</forenames></author></authors><title>Multi-Step Regression Learning for Compositional Distributional
  Semantics</title><categories>cs.CL cs.LG</categories><comments>10 pages + 1 page references, to be presented at the 10th
  International Conference on Computational Semantics (IWCS 2013)</comments><msc-class>68T50</msc-class><acm-class>G.1.3; H.3.1; I.2.7; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a model for compositional distributional semantics related to the
framework of Coecke et al. (2010), and emulating formal semantics by
representing functions as tensors and arguments as vectors. We introduce a new
learning method for tensors, generalising the approach of Baroni and Zamparelli
(2010). We evaluate it on two benchmark data sets, and find it to outperform
existing leading methods. We argue in our analysis that the nature of this
learning method also renders it suitable for solving more subtle problems
compositional distributional models might face.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6944</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6944</id><created>2013-01-29</created><authors><author><keyname>Christmann</keyname><forenames>Andreas</forenames></author><author><keyname>Hable</keyname><forenames>Robert</forenames></author></authors><title>On the Consistency of the Bootstrap Approach for Support Vector Machines
  and Related Kernel Based Methods</title><categories>stat.ML cs.LG</categories><comments>13 pages</comments><msc-class>62G08, 62G09, 62G20, 62G86</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that bootstrap approximations of support vector machines (SVMs)
based on a general convex and smooth loss function and on a general kernel are
consistent. This result is useful to approximate the unknown finite sample
distribution of SVMs by the bootstrap approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6946</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6946</id><created>2013-01-29</created><updated>2013-02-11</updated><authors><author><keyname>Taassori</keyname><forenames>Mehdi</forenames><affiliation>Member, IEEE</affiliation></author><author><keyname>Taassori</keyname><forenames>Meysam</forenames><affiliation>Member, IEEE</affiliation></author><author><keyname>Uysal</keyname><forenames>Sener</forenames><affiliation>Member, IEEE</affiliation></author></authors><title>MFLP: Most Frequent Least Power Encoding</title><categories>cs.OH</categories><comments>This paper has been withdrawn by the authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn by the authors. In this paper, we propose a new
low power coding technique by decreasing the number of switching activities on
the buses which use transition signaling to transmit data. This approach
dedicates the symbols with less ones to high probability data. MFLP unlike the
most low power encoding does not rely on spatial redundancy. Due to this
superiority, MFLP is unique in power decreasing in the Network on Chip (NoC).
Not only does this algorithm reduce the power consumption, but also it can
compress the data. It offers a tradeoff to designers to choose between
compression and power; that is, the more power consumption decrease we need,
the less compression we earn. This coding uses tree based infrastructure in
order to decrease the number of ones to reduce the switching activities, and
the power consumption consequently. The proposed algorithm constructs the tree
with this contribution that code words with less ones are allocated to more
frequent data. The experimental results for the outside and inside of the NoC
indicate that the proposed coding algorithm reduces the switching activity up
to 30 and 45%, the link power consumption up to 35 and 46% and the total power
dissipation up to 34.9 and 16% for the outside and inside of the NoC,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6952</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6952</id><created>2013-01-29</created><authors><author><keyname>Schwartz</keyname><forenames>Yannick</forenames><affiliation>INRIA Saclay - Ile de France, LNAO</affiliation></author><author><keyname>Barbot</keyname><forenames>Alexis</forenames><affiliation>LNAO</affiliation></author><author><keyname>Thyreau</keyname><forenames>Benjamin</forenames><affiliation>LNAO</affiliation></author><author><keyname>Frouin</keyname><forenames>Vincent</forenames><affiliation>LNAO</affiliation></author><author><keyname>Varoquaux</keyname><forenames>Ga&#xeb;l</forenames><affiliation>INRIA Saclay - Ile de France, LNAO</affiliation></author><author><keyname>Siram</keyname><forenames>Aditya</forenames><affiliation>LNAO</affiliation></author><author><keyname>Marcus</keyname><forenames>Daniel</forenames><affiliation>LNAO</affiliation></author><author><keyname>Poline</keyname><forenames>Jean-Baptiste</forenames><affiliation>LNAO</affiliation></author></authors><title>PyXNAT: XNAT in Python</title><categories>cs.DB cs.CV q-bio.QM</categories><proxy>ccsd</proxy><journal-ref>Frontiers in Neuroinformatics 6, 12 (2012) 1-14</journal-ref><doi>10.3389/fninf.2012.00012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As neuroimaging databases grow in size and complexity, the time researchers
spend investigating and managing the data increases to the expense of data
analysis. As a result, investigators rely more and more heavily on scripting
using high-level languages to automate data management and processing tasks.
For this, a structured and programmatic access to the data store is necessary.
Web services are a first step toward this goal. They however lack in
functionality and ease of use because they provide only low level interfaces to
databases. We introduce here PyXNAT, a Python module that interacts with The
Extensible Neuroimaging Archive Toolkit (XNAT) through native Python calls
across multiple operating systems. The choice of Python enables PyXNAT to
expose the XNAT Web Services and unify their features with a higher level and
more expressive language. PyXNAT provides XNAT users direct access to all the
scientific packages in Python. Finally PyXNAT aims to be efficient and easy to
use, both as a backend library to build XNAT clients and as an alternative
frontend from the command line.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6956</identifier>
 <datestamp>2013-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6956</id><created>2013-01-29</created><updated>2013-05-17</updated><authors><author><keyname>Jeon</keyname><forenames>Wonseok</forenames></author><author><keyname>Chung</keyname><forenames>Sae-Young</forenames></author></authors><title>The Capacity of Wireless Channels: A Physical Approach</title><categories>cs.IT math.IT</categories><comments>5 pages, to appear in proceedings of 2013 IEEE ISIT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the capacity of wireless channels is characterized based on
electromagnetic and antenna theories with only minimal assumptions. We assume
the transmitter can generate an arbitrary current distribution inside a
spherical region and the receive antennas are uniformly distributed on a bigger
sphere surrounding the transmitter. The capacity is shown to be $(\alpha P/N_0)
\log e$ [bits/sec] in the limit of large number of receive antennas, where $P$
is the transmit power constraint, $\alpha$ is the normalized density of the
receive antennas and $N_0$ is the noise power spectral density. Although this
result may look trivial, it is surprising in two ways. First, this result holds
regardless of the bandwidth (bandwidth can even be negligibly small). Second,
this result shows that the capacity is irrespective of the size of the region
containing the transmitter. This is against some previous results that claimed
the maximum degrees of freedom is proportional to the surface area containing
the transmitter normalized by the square of the wavelength. Our result has
important practical implications since it shows that even a compact antenna
array with negligible bandwidth and antenna spacing well below the wavelength
can provide a huge throughput as if the array was big enough so that the
antenna spacing is on the order of the wavelength.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6963</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6963</id><created>2013-01-29</created><updated>2013-01-31</updated><authors><author><keyname>Ariffin</keyname><forenames>Muhammad Rezal Kamel</forenames></author></authors><title>An asymmetric primitive based on the Bivariate Function Hard Problem</title><categories>cs.CR cs.IT math.IT</categories><msc-class>11T71, 94A60, 11D45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bivariate Function Hard Problem (BFHP) has been in existence implicitly
in almost all number theoretic based cryptosystems. This work defines the BFHP
in a more general setting and produces an efficient asymmetric cryptosystem.
The cryptosystem has a complexity order of O(n^2) for both encryption and
decryption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6972</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6972</id><created>2013-01-29</created><authors><author><keyname>McLaughlin</keyname><forenames>James</forenames></author><author><keyname>Clark</keyname><forenames>John A.</forenames></author></authors><title>Using evolutionary computation to create vectorial Boolean functions
  with low differential uniformity and high nonlinearity</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The two most important criteria for vectorial Boolean functions used as
S-boxes in block ciphers are differential uniformity and nonlinearity. Previous
work in this field has focused only on nonlinearity and a different criterion,
autocorrelation. In this paper, we describe the results of experiments in using
simulated annealing, memetic algorithms, and ant colony optimisation to create
vectorial Boolean functions with low differential uniformity.
  Keywords: Metaheuristics, simulated annealing, memetic algorithms, ant colony
optimization, cryptography, Boolean functions, vectorial Boolean functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6975</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6975</id><created>2013-01-29</created><updated>2013-06-20</updated><authors><author><keyname>Zahedi</keyname><forenames>Keyan</forenames></author><author><keyname>Ay</keyname><forenames>Nihat</forenames></author></authors><title>Quantifying Morphological Computation</title><categories>cs.AI cs.IT math.IT</categories><journal-ref>Entropy. 2013; 15(5):1887-1915</journal-ref><doi>10.3390/e15051887</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The field of embodied intelligence emphasises the importance of the
morphology and environment with respect to the behaviour of a cognitive system.
The contribution of the morphology to the behaviour, commonly known as
morphological computation, is well-recognised in this community. We believe
that the field would benefit from a formalisation of this concept as we would
like to ask how much the morphology and the environment contribute to an
embodied agent's behaviour, or how an embodied agent can maximise the
exploitation of its morphology within its environment. In this work we derive
two concepts of measuring morphological computation, and we discuss their
relation to the Information Bottleneck Method. The first concepts asks how much
the world contributes to the overall behaviour and the second concept asks how
much the agent's action contributes to a behaviour. Various measures are
derived from the concepts and validated in two experiments which highlight
their strengths and weaknesses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6980</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6980</id><created>2013-01-29</created><updated>2013-08-26</updated><authors><author><keyname>Kaufman</keyname><forenames>Brett</forenames></author><author><keyname>Lilleberg</keyname><forenames>Jorma</forenames></author><author><keyname>Aazhang</keyname><forenames>Behnaam</forenames></author></authors><title>Spectrum Sharing Scheme Between Cellular Users and Ad-hoc
  Device-to-Device Users</title><categories>cs.NI</categories><comments>11 pages, 11 figues, Published in IEEE Trans. Wireless Communications</comments><journal-ref>IEEE Trans. Wireless Communications, vol. 12, no. 3, pp.
  1038-1049, Mar. 2013</journal-ref><doi>10.1109/TWC.2012.011513.120063</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an attempt to utilize spectrum resources more efficiently, protocols
sharing licensed spectrum with unlicensed users are receiving increased
attention. From the perspective of cellular networks, spectrum underutilization
makes spatial reuse a feasible complement to existing standards. Interference
management is a major component in designing these schemes as it is critical
that licensed users maintain their expected quality of service. We develop a
distributed dynamic spectrum protocol in which ad-hoc device-to-device users
opportunistically access the spectrum actively in use by cellular users.
  First, channel gain estimates are used to set feasible transmit powers for
device-to-device users that keeps the interference they cause within the
allowed interference temperature. Then network information is distributed by
route discovery packets in a random access manner to help establish either a
single-hop or multi-hop route between two device-to-device users. We show that
network information in the discovery packet can decrease the failure rate of
the route discovery and reduce the number of necessary transmissions to find a
route. Using the found route, we show that two device-to-device users can
communicate with a low probability of outage while only minimally affecting the
cellular network, and can achieve significant power savings when communicating
directly with each other instead of utilizing the cellular base station.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.6999</identifier>
 <datestamp>2014-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.6999</id><created>2013-01-29</created><updated>2014-01-10</updated><authors><author><keyname>Schmidt</keyname><forenames>Kai-Uwe</forenames></author><author><keyname>Zhou</keyname><forenames>Yue</forenames></author></authors><title>Planar functions over fields of characteristic two</title><categories>math.CO cs.IT math.AG math.IT</categories><comments>23 pages, minor corrections and simplifications compared to the first
  version</comments><msc-class>11T06, 14H20 (Primary) 11T71, 05B10 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical planar functions are functions from a finite field to itself and
give rise to finite projective planes. They exist however only for fields of
odd characteristic. We study their natural counterparts in characteristic two,
which we also call planar functions. They again give rise to finite projective
planes, as recently shown by the second author. We give a characterisation of
planar functions in characteristic two in terms of codes over $\mathbb{Z}_4$.
We then specialise to planar monomial functions $f(x)=cx^t$ and present
constructions and partial results towards their classification. In particular,
we show that $t=1$ is the only odd exponent for which $f(x)=cx^t$ is planar
(for some nonzero $c$) over infinitely many fields. The proof techniques
involve methods from algebraic geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7002</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7002</id><created>2013-01-29</created><updated>2013-02-08</updated><authors><author><keyname>Ohlsson</keyname><forenames>Henrik</forenames></author><author><keyname>Yang</keyname><forenames>Allen Y.</forenames></author><author><keyname>Dong</keyname><forenames>Roy</forenames></author><author><keyname>Verhaegen</keyname><forenames>Michel</forenames></author><author><keyname>Sastry</keyname><forenames>S. Shankar</forenames></author></authors><title>Quadratic Basis Pursuit</title><categories>cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many compressive sensing problems today, the relationship between the
measurements and the unknowns could be nonlinear. Traditional treatment of such
nonlinear relationships have been to approximate the nonlinearity via a linear
model and the subsequent un-modeled dynamics as noise. The ability to more
accurately characterize nonlinear models has the potential to improve the
results in both existing compressive sensing applications and those where a
linear approximation does not suffice, e.g., phase retrieval. In this paper, we
extend the classical compressive sensing framework to a second-order Taylor
expansion of the nonlinearity. Using a lifting technique and a method we call
quadratic basis pursuit, we show that the sparse signal can be recovered
exactly when the sampling rate is sufficiently high. We further present
efficient numerical algorithms to recover sparse signals in second-order
nonlinear systems, which are considerably more difficult to solve than their
linear counterparts in sparse optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7006</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7006</id><created>2013-01-29</created><updated>2014-02-25</updated><authors><author><keyname>Crampes</keyname><forenames>Michel</forenames></author><author><keyname>Planti&#xe9;</keyname><forenames>Michel</forenames></author></authors><title>A Unified Community Detection, Visualization and Analysis method</title><categories>cs.SI physics.data-an physics.soc-ph stat.OT</categories><comments>Submitted to Advances in Complex Systems</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Community detection in social graphs has attracted researchers' interest for
a long time. With the widespread of social networks on the Internet it has
recently become an important research domain. Most contributions focus upon the
definition of algorithms for optimizing the so-called modularity function. In
the first place interest was limited to unipartite graph inputs and partitioned
community outputs. Recently bipartite graphs, directed graphs and overlapping
communities have been investigated. Few contributions embrace at the same time
the three types of nodes. In this paper we present a method which unifies
commmunity detection for the three types of nodes and at the same time merges
partitionned and overlapping communities. Moreover results are visualized in
such a way that they can be analyzed and semantically interpreted. For
validation we experiment this method on well known simple benchmarks. It is
then applied to real data in three cases. In two examples of photos sets with
tagged people we reveal social networks. A second type of application is of
particularly interest. After applying our method to Human Brain Tractography
Data provided by a team of neurologists, we produce clusters of white fibers in
accordance with other well known clustering methods. Moreover our approach for
visualizing overlapping clusters allows better understanding of the results by
the neurologist team. These last results open up the possibility of applying
community detection methods in other domains such as data analysis with
original enhanced performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7015</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7015</id><created>2013-01-29</created><updated>2013-03-01</updated><authors><author><keyname>Shen</keyname><forenames>Entong</forenames></author><author><keyname>Yu</keyname><forenames>Ting</forenames></author></authors><title>Mining Frequent Graph Patterns with Differential Privacy</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discovering frequent graph patterns in a graph database offers valuable
information in a variety of applications. However, if the graph dataset
contains sensitive data of individuals such as mobile phone-call graphs and
web-click graphs, releasing discovered frequent patterns may present a threat
to the privacy of individuals. {\em Differential privacy} has recently emerged
as the {\em de facto} standard for private data analysis due to its provable
privacy guarantee. In this paper we propose the first differentially private
algorithm for mining frequent graph patterns.
  We first show that previous techniques on differentially private discovery of
frequent {\em itemsets} cannot apply in mining frequent graph patterns due to
the inherent complexity of handling structural information in graphs. We then
address this challenge by proposing a Markov Chain Monte Carlo (MCMC) sampling
based algorithm. Unlike previous work on frequent itemset mining, our
techniques do not rely on the output of a non-private mining algorithm.
Instead, we observe that both frequent graph pattern mining and the guarantee
of differential privacy can be unified into an MCMC sampling framework. In
addition, we establish the privacy and utility guarantee of our algorithm and
propose an efficient neighboring pattern counting technique as well.
Experimental results show that the proposed algorithm is able to output
frequent patterns with good precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7023</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7023</id><created>2013-01-29</created><updated>2013-07-18</updated><authors><author><keyname>Baldassini</keyname><forenames>Leonardo</forenames></author><author><keyname>Johnson</keyname><forenames>Oliver</forenames></author><author><keyname>Aldridge</keyname><forenames>Matthew</forenames></author></authors><title>The Capacity of Adaptive Group Testing</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><journal-ref>Proceedings of the International Symposium on Information Theory
  (ISIT) 2013, pages 2676-2680</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define capacity for group testing problems and deduce bounds for the
capacity of a variety of noisy models, based on the capacity of equivalent
noisy communication channels. For noiseless adaptive group testing we prove an
information-theoretic lower bound which tightens a bound of Chan et al. This
can be combined with a performance analysis of a version of Hwang's adaptive
group testing algorithm, in order to deduce the capacity of noiseless and
erasure group testing models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7031</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7031</id><created>2013-01-29</created><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Adaptive Reduced-Rank Constrained Constant Modulus Beamforming
  Algorithms Based on Joint Iterative Optimization of Filters</title><categories>cs.IT math.IT</categories><comments>10 figures; IEEE Transactions on Signal Processing, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a robust reduced-rank scheme for adaptive beamforming
based on joint iterative optimization (JIO) of adaptive filters. The novel
scheme is designed according to the constant modulus (CM) criterion subject to
different constraints, and consists of a bank of full-rank adaptive filters
that forms the transformation matrix, and an adaptive reduced-rank filter that
operates at the output of the bank of filters to estimate the desired signal.
We describe the proposed scheme for both the direct-form processor (DFP) and
the generalized sidelobe canceller (GSC) structures. For each structure, we
derive stochastic gradient (SG) and recursive least squares (RLS) algorithms
for its adaptive implementation. The Gram-Schmidt (GS) technique is applied to
the adaptive algorithms for reformulating the transformation matrix and
improving performance. An automatic rank selection technique is developed and
employed to determine the most adequate rank for the derived algorithms. The
complexity and convexity analyses are carried out. Simulation results show that
the proposed algorithms outperform the existing full-rank and reduced-rank
methods in convergence and tracking performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7046</identifier>
 <datestamp>2013-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7046</id><created>2013-01-29</created><updated>2013-02-09</updated><authors><author><keyname>Oohama</keyname><forenames>Yasutada</forenames></author></authors><title>Outer Bound of the Capacity Region for Identification via Multiple
  Access Channels</title><categories>cs.IT math.IT</categories><comments>This arXiv paper is a complete version of the paper submitted to IEEE
  ISIT 2013. After submission we found some minor mistakes in Property 1 b),
  Theorems 1 and 2 in the paper. In this arXiv version we have corrected those
  mistakes. A mistake in Version 4 in the description of the region C'(X,Y|W)
  in Property 2-b) is corrected in In the present version. Typos in the
  previous version are corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the identification (ID) via multiple access
channels (MACs). In the general MAC the ID capacity region includes the
ordinary transmission (TR) capacity region. In this paper we discuss the
converse coding theorem. We estimate two types of error probabilities of
identification for rates outside capacity region, deriving some function which
serves as a lower bound of the sum of two error probabilities of
identification. This function has a property that it tends to zero as $n\to
\infty$ for noisy channels satisfying the strong converse property. Using this
property, we establish that the transmission capacity region is equal to the ID
capacity for the MAC satisfying the strong converse property. To derive the
result we introduce a new resolvability problem on the output from the MAC. We
further develop a new method of converting the direct coding theorem for the
above MAC resolvability problem into the converse coding theorem for the ID via
MACs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7047</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7047</id><created>2013-01-29</created><authors><author><keyname>Zhao</keyname><forenames>Yunpeng</forenames></author><author><keyname>Levina</keyname><forenames>Elizaveta</forenames></author><author><keyname>Zhu</keyname><forenames>Ji</forenames></author></authors><title>Link prediction for partially observed networks</title><categories>stat.ML cs.LG cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Link prediction is one of the fundamental problems in network analysis. In
many applications, notably in genetics, a partially observed network may not
contain any negative examples of absent edges, which creates a difficulty for
many existing supervised learning approaches. We develop a new method which
treats the observed network as a sample of the true network with different
sampling rates for positive and negative examples. We obtain a relative ranking
of potential links by their probabilities, utilizing information on node
covariates as well as on network topology. Empirically, the method performs
well under many settings, including when the observed network is sparse. We
apply the method to a protein-protein interaction network and a school
friendship network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7054</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7054</id><created>2013-01-29</created><authors><author><keyname>Gerami</keyname><forenames>Majid</forenames></author><author><keyname>Xiao</keyname><forenames>Ming</forenames></author></authors><title>Repair for Distributed Storage Systems with Erasure Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the repair problem of distributed storage systems in erasure
networks where the packets transmitted from surviving nodes to the new node
might be lost. The fundamental storage-bandwidth tradeoff is calculated by
multicasting analysis in erasure networks. The optimal tradeoff bound can be
asymptotically achieved when the number of transmission (packets) goes to
infinity. For a limited number of transmission, we study the probability of
successful regenerating. Then, we investigate two approaches of increasing the
probability of successful regenerating, namely, by connecting more surviving
nodes or by increasing the storage space of nodes. Using more nodes may pose
larger delay and in certain situation it might not be possible to connect to
more nodes too. We show that in addition to reducing repair bandwidth,
increasing storage space can also increase reliability for repair.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7064</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7064</id><created>2013-01-29</created><authors><author><keyname>Turk</keyname><forenames>Matthew J.</forenames></author></authors><title>How to Scale a Code in the Human Dimension</title><categories>astro-ph.IM cs.CY</categories><comments>Manuscript prepared from talk at Scientific Software Days 2012,
  December 2012. Slides and video of talk available at
  http://scisoftdays.org/meetings/2012/speakers_and_abstracts/#turk</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  As scientists' needs for computational techniques and tools grow, they cease
to be supportable by software developed in isolation. In many cases, these
needs are being met by communities of practice, where software is developed by
domain scientists to reach pragmatic goals and satisfy distinct and enumerable
scientific goals. We present techniques that have been successful in growing
and engaging communities of practice, specifically in the yt and Enzo
communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7090</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7090</id><created>2013-01-29</created><authors><author><keyname>Bonamy</keyname><forenames>Marthe</forenames></author><author><keyname>L&#xe9;v&#xea;que</keyname><forenames>Benjamin</forenames></author><author><keyname>Pinlou</keyname><forenames>Alexandre</forenames></author></authors><title>Graphs with maximum degree D at least 17 and maximum average degree less
  than 3 are list 2-distance (D+2)-colorable</title><categories>cs.DM math.CO</categories><comments>22 pages, 5 figures, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For graphs of bounded maximum average degree, we consider the problem of
2-distance coloring. This is the problem of coloring the vertices while
ensuring that two vertices that are adjacent or have a common neighbor receive
different colors. It is already known that planar graphs of girth at least 6
and of maximum degree D are list 2-distance (D+2)-colorable when D&gt;=24 (Borodin
and Ivanova (2009)) and 2-distance (D+2)-colorable when D&gt;=18 (Borodin and
Ivanova (2009)). We prove here that D&gt;=17 suffices in both cases. More
generally, we show that graphs with maximum average degree less than 3 and
D&gt;=17 are list 2-distance (D+2)-colorable. The proof can be transposed to list
injective (D+1)-coloring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7101</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7101</id><created>2013-01-29</created><authors><author><keyname>Haas</keyname><forenames>Zygmunt J.</forenames></author><author><keyname>Nikolov</keyname><forenames>Milen</forenames></author></authors><title>Towards Optimal Broadcast in Wireless Networks</title><categories>cs.NI</categories><comments>15 pages, 21 figures</comments><acm-class>C.2.1; C.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Broadcast is a fundamental operation in networks, especially in wireless
Mobile Ad Hoc NETworks (MANET). For example, some form of broadcasting is used
by all on-demand MANET routing protocols, when there is uncertainty as to the
location of the destination node, or for service discovery. Being such a basic
operation of the networking protocols, the importance of efficient broadcasting
has long been recognized by the networking community. Numerous papers proposed
increasingly more efficient implementation of broadcasting, while other studies
presented bounds on broadcast performance. In this work, we present a new
approach to efficient broadcast in networks with dynamic topologies, such as
MANET, and we introduce a new broadcasting algorithm for such networking
environments. We evaluate our algorithm, showing that its performance comes
remarkably close to the corresponding theoretical performance bounds, even in
the presence of packet loss due to, for example, MAC-layer collisions.
Furthermore, we compare the performance of the proposed algorithm with other
recently proposed schemes, including in various mobility settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7112</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7112</id><created>2013-01-29</created><authors><author><keyname>Ianovski</keyname><forenames>Egor</forenames></author></authors><title>Computable Component-wise Reducibility</title><categories>cs.FL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider equivalence relations and preorders complete for various levels
of the arithmetical hierarchy under computable, component-wise reducibility. We
show that implication in first order logic is a complete preorder for $\SI 1$,
the $\le^P_m$ relation on EXPTIME sets for $\SI 2$ and the embeddability of
computable subgroups of $(\QQ,+)$ for $\SI 3$. In all cases, the symmetric
fragment of the preorder is complete for equivalence relations on the same
level. We present a characterisation of $\PI 1$ equivalence relations which
allows us to establish that equality of polynomial time functions and inclusion
of polynomial time sets are complete for $\PI 1$ equivalence relations and
preorders respectively. We also show that this is the limit of the enquiry: for
$n\geq 2$ there are no $\PI n$ nor $\DE n$-complete equivalence relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7119</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7119</id><created>2013-01-29</created><updated>2015-04-23</updated><authors><author><keyname>Dieudonn&#xe9;</keyname><forenames>Yoann</forenames></author><author><keyname>Pelc</keyname><forenames>Andrzej</forenames></author><author><keyname>Villain</keyname><forenames>Vincent</forenames></author></authors><title>How to Meet Asynchronously at Polynomial Cost</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two mobile agents starting at different nodes of an unknown network have to
meet. This task is known in the literature as rendezvous. Each agent has a
different label which is a positive integer known to it, but unknown to the
other agent. Agents move in an asynchronous way: the speed of agents may vary
and is controlled by an adversary. The cost of a rendezvous algorithm is the
total number of edge traversals by both agents until their meeting. The only
previous deterministic algorithm solving this problem has cost exponential in
the size of the graph and in the larger label. In this paper we present a
deterministic rendezvous algorithm with cost polynomial in the size of the
graph and in the length of the smaller label. Hence we decrease the cost
exponentially in the size of the graph and doubly exponentially in the labels
of agents. As an application of our rendezvous algorithm we solve several
fundamental problems involving teams of unknown size larger than 1 of labeled
agents moving asynchronously in unknown networks. Among them are the following
problems: team size, in which every agent has to find the total number of
agents, leader election, in which all agents have to output the label of a
single agent, perfect renaming in which all agents have to adopt new different
labels from the set {1, . . . , k}, where k is the number of agents, and
gossiping, in which each agent has initially a piece of information (value) and
all agents have to output all the values. Using our rendezvous algorithm we
solve all these problems at cost polynomial in the size of the graph and in the
smallest length of all labels of participating agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7134</identifier>
 <datestamp>2013-08-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7134</id><created>2013-01-29</created><updated>2013-08-14</updated><authors><author><keyname>Guo</keyname><forenames>Peng</forenames></author><author><keyname>Chen</keyname><forenames>Wenming</forenames></author><author><keyname>Wang</keyname><forenames>Yi</forenames></author></authors><title>A general variable neighborhood search for single-machine total
  tardiness scheduling problem with step-deteriorating jobs</title><categories>math.OC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we study a single-machine scheduling problem of minimizing
the total tardiness for a set of independent jobs. The processing time of a job
is modeled as a step function of its starting time and a specific deteriorating
date. A mixed integer programming model was applied to the problem and
validated. Since the problem is known to be NP-hard, we proposed a heuristic
named simple weighted search procedure (SWSP) and a general variable
neighborhood search algorithm (GVNS).
  A perturbation procedure with 3-opt is embedded within the GVNS process in
order to explore broader spaces. Extensive numerical experiments are carried
out on some randomly generated test instances so as to investigate the
performance of the proposed algorithms. By comparing to the results of the
CPLEX optimization solver, the heuristic SWSP and the standard variable
neighborhood search, it is shown that the proposed GVNS algorithm can provide
better solutions within a reasonable running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7144</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7144</id><created>2013-01-30</created><authors><author><keyname>Roy</keyname><forenames>Shibdas</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Huntington</keyname><forenames>Elanor H.</forenames></author></authors><title>Robust Phase Estimation of Squeezed State</title><categories>quant-ph cs.SY math.OC</categories><comments>2 pages, 2 figures, Proceedings of CLEO:QELS 2013</comments><doi>10.1364/CLEO_QELS.2013.JTh2A.88</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal phase estimation of a phase-squeezed quantum state of light has been
recently shown to beat the coherent-state limit. Here, the estimation is made
robust to uncertainties in underlying parameters using a robust fixed-interval
smoother.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7153</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7153</id><created>2013-01-30</created><authors><author><keyname>McIver</keyname><forenames>Annabelle</forenames></author><author><keyname>Rabehaja</keyname><forenames>Tahiry</forenames></author><author><keyname>Struth</keyname><forenames>Georg</forenames></author></authors><title>Weak Concurrent Kleene Algebra with Application to Algebraic
  Verification</title><categories>cs.FL</categories><comments>17 pages</comments><msc-class>68Q70</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We propose a generalisation of concurrent Kleene algebra \cite{Hoa09} that
can take account of probabilistic effects in the presence of concurrency. The
algebra is proved sound with respect to a model of automata modulo a variant of
rooted $\eta$-simulation equivalence. Applicability is demonstrated by
algebraic treatments of two examples: algebraic may testing and Rabin's
solution to the choice coordination problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7154</identifier>
 <datestamp>2013-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7154</id><created>2013-01-30</created><updated>2013-06-12</updated><authors><author><keyname>Bringmann</keyname><forenames>Karl</forenames></author></authors><title>Bringing Order to Special Cases of Klee's Measure Problem</title><categories>cs.CG</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Klee's Measure Problem (KMP) asks for the volume of the union of n
axis-aligned boxes in d-space. Omitting logarithmic factors, the best algorithm
has runtime O*(n^{d/2}) [Overmars,Yap'91]. There are faster algorithms known
for several special cases: Cube-KMP (where all boxes are cubes), Unitcube-KMP
(where all boxes are cubes of equal side length), Hypervolume (where all boxes
share a vertex), and k-Grounded (where the projection onto the first k
dimensions is a Hypervolume instance).
  In this paper we bring some order to these special cases by providing
reductions among them. In addition to the trivial inclusions, we establish
Hypervolume as the easiest of these special cases, and show that the runtimes
of Unitcube-KMP and Cube-KMP are polynomially related. More importantly, we
show that any algorithm for one of the special cases with runtime T(n,d)
implies an algorithm for the general case with runtime T(n,2d), yielding the
first non-trivial relation between KMP and its special cases. This allows to
transfer W[1]-hardness of KMP to all special cases, proving that no n^{o(d)}
algorithm exists for any of the special cases under reasonable complexity
theoretic assumptions. Furthermore, assuming that there is no improved
algorithm for the general case of KMP (no algorithm with runtime O(n^{d/2 -
eps})) this reduction shows that there is no algorithm with runtime
O(n^{floor(d/2)/2 - eps}) for any of the special cases. Under the same
assumption we show a tight lower bound for a recent algorithm for 2-Grounded
[Yildiz,Suri'12].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7170</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7170</id><created>2013-01-30</created><authors><author><keyname>Samara</keyname><forenames>Ghassan</forenames></author></authors><title>Increasing Network Visibility Using Coded Repetition Beacon Piggybacking</title><categories>cs.NI</categories><comments>8 Pages</comments><journal-ref>World Applied Sciences Journal 13 (1); 100-108, 2011, ISSN
  1818-4952, \c{opyright}IDOSI Publications, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular Ad hoc Networks (VANET) is one of the most challenging research
areas in the field of Mobile Ad Hoc Networks. In this research, we propose a
new mechanism for increasing network visibility, by taking the information
gained from periodic safety messages (beacons), and inserting it into a
'neighbor' table. The table will be propagated to all neighbors giving a wider
vision for each vehicle belonging to the network. It will also decrease the
risk of collision at road junctions as each vehicle will have prior knowledge
oncoming vehicles before reaching the junction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7173</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7173</id><created>2013-01-30</created><authors><author><keyname>Jiang</keyname><forenames>Zhi-Qiang</forenames><affiliation>ECUST</affiliation></author><author><keyname>Xie</keyname><forenames>Wen-Jie</forenames><affiliation>ECUST</affiliation></author><author><keyname>Li</keyname><forenames>Ming-Xia</forenames><affiliation>ECUST</affiliation></author><author><keyname>Podobnik</keyname><forenames>Boris</forenames><affiliation>BU and ZSEM</affiliation></author><author><keyname>Zhou</keyname><forenames>Wei-Xing</forenames><affiliation>ECUST</affiliation></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames><affiliation>BU</affiliation></author></authors><title>Calling patterns in human communication dynamics</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>7 pages and 8 figues</comments><journal-ref>PNAS 110: 1600-1605 (2013)</journal-ref><doi>10.1073/pnas.1220433110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern technologies not only provide a variety of communication modes, e.g.,
texting, cellphone conversation, and online instant messaging, but they also
provide detailed electronic traces of these communications between individuals.
These electronic traces indicate that the interactions occur in temporal
bursts. Here, we study the inter-call durations of the 100,000 most-active
cellphone users of a Chinese mobile phone operator. We confirm that the
inter-call durations follow a power-law distribution with an exponential cutoff
at the population level but find differences when focusing on individual users.
We apply statistical tests at the individual level and find that the inter-call
durations follow a power-law distribution for only 3460 individuals (3.46%).
The inter-call durations for the majority (73.34%) follow a Weibull
distribution. We quantify individual users using three measures: out-degree,
percentage of outgoing calls, and communication diversity. We find that the
cellphone users with a power-law duration distribution fall into three
anomalous clusters: robot-based callers, telecom frauds, and telephone sales.
This information is of interest to both academics and practitioners, mobile
telecom operator in particular. In contrast, the individual users with a
Weibull duration distribution form the fourth cluster of ordinary cellphone
users. We also discover more information about the calling patterns of these
four clusters, e.g., the probability that a user will call the $c_r$-th most
contact and the probability distribution of burst sizes. Our findings may
enable a more detailed analysis of the huge body of data contained in the logs
of massive users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7178</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7178</id><created>2013-01-30</created><authors><author><keyname>Desgroseilliers</keyname><forenames>Marc</forenames></author><author><keyname>Leveque</keyname><forenames>Olivier</forenames></author><author><keyname>Preissmann</keyname><forenames>Emmanuel</forenames></author></authors><title>Spatial degrees of freedom of MIMO systems in Line-of-Sight Environment</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the efficiency of MIMO transmissions in a rich scattering environment
has been demonstrated, less is known about the situation where the fading
matrix coefficients come from a line-of-sight model. In this paper, we study in
detail how this line-of-sight assumption affects the performance of distributed
MIMO transmissions between far away clusters of nodes in a wireless network.
Our analysis pertains to the study of a new class of random matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7183</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7183</id><created>2013-01-30</created><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author><author><keyname>Wu</keyname><forenames>Yingjie</forenames></author><author><keyname>Zhu</keyname><forenames>Daxin</forenames></author></authors><title>A Dynamic Programming Solution to a Generalized LCS Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a generalized longest common subsequence problem,
the string-excluding constrained LCS problem. For the two input sequences $X$
and $Y$ of lengths $n$ and $m$, and a constraint string $P$ of length $r$, the
problem is to find a common subsequence $Z$ of $X$ and $Y$ excluding $P$ as a
substring and the length of $Z$ is maximized. The problem and its solution were
first proposed by Chen and Chao\cite{1}, but we found that their algorithm can
not solve the problem correctly. A new dynamic programming solution for the
STR-EC-LCS problem is then presented in this paper. The correctness of the new
algorithm is proved. The time complexity of the new algorithm is $O(nmr)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7189</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7189</id><created>2013-01-30</created><updated>2013-07-02</updated><authors><author><keyname>Pe&#xf1;a</keyname><forenames>Jose M.</forenames></author></authors><title>Approximate Counting of Graphical Models Via MCMC Revisited</title><categories>stat.ML cs.AI</categories><comments>In Proceedings of the 15th Conference of the Spanish Association for
  Artificial Intelligence (CAEPIA 2013). Lecture Notes in Artificial
  Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Pe\~na (2007), MCMC sampling is applied to approximately calculate the
ratio of essential graphs (EGs) to directed acyclic graphs (DAGs) for up to 20
nodes. In the present paper, we extend that work from 20 to 31 nodes. We also
extend that work by computing the approximate ratio of connected EGs to
connected DAGs, of connected EGs to EGs, and of connected DAGs to DAGs.
Furthermore, we prove that the latter ratio is asymptotically 1. We also
discuss the implications of these results for learning DAGs from data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7190</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7190</id><created>2013-01-30</created><authors><author><keyname>Frigerio</keyname><forenames>Marco</forenames></author><author><keyname>Buchli</keyname><forenames>Jonas</forenames></author><author><keyname>Caldwell</keyname><forenames>Darwin G.</forenames></author></authors><title>A Domain Specific Language for kinematic models and fast implementations
  of robot dynamics algorithms</title><categories>cs.RO cs.PL</categories><comments>Presented at DSLRob 2011 (arXiv:1212.3308)</comments><report-no>DSLRob/2011/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rigid body dynamics algorithms play a crucial role in several components of a
robot controller and simulations. Real time constraints in high frequency
control loops and time requirements of specific applications demand these
functions to be very efficient. Despite the availability of established
algorithms, their efficient implementation for a specific robot still is a
tedious and error-prone task. However, these components are simply necessary to
get high performance controllers.
  To achieve efficient yet well maintainable implementations of dynamics
algorithms we propose to use a domain specific language to describe the
kinematics/dynamics model of a robot. Since the algorithms are parameterized on
this model, executable code tailored for a specific robot can be generated,
thanks to the facilities available for \dsls. This approach allows the users to
deal only with the high level description of their robot and relieves them from
problematic hand-crafted development; resources and efforts can then be focused
on open research questions.
  Preliminary results about the generation of efficient code for inverse
dynamics will be presented as a proof of concept of this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7192</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7192</id><created>2013-01-30</created><authors><author><keyname>Schreiber</keyname><forenames>Michael</forenames></author></authors><title>Empirical Evidence for the Relevance of Fractional Scoring in the
  Calculation of Percentile Rank Scores</title><categories>cs.DL physics.soc-ph stat.AP</categories><comments>10 pages, 4 tables, accepted for publication in Journal of American
  Society for Information Science and Technology</comments><journal-ref>Journal of the American Society for Information Science and
  Technology, 64(4), 861-867 (2013)</journal-ref><doi>10.1002/asi.22774</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Fractional scoring has been proposed to avoid inconsistencies in the
attribution of publications to percentile rank classes. Uncertainties and
ambiguities in the evaluation of percentile ranks can be demonstrated most
easily with small datasets. But for larger datasets an often large number of
papers with the same citation count leads to the same uncertainties and
ambiguities which can be avoided by fractional scoring. This is demonstrated
for four different empirical datasets with several thousand publications each
which are assigned to 6 percentile rank classes. Only by utilizing fractional
scoring the total score of all papers exactly reproduces the theoretical value
in each case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7231</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7231</id><created>2013-01-30</created><authors><author><keyname>Shum</keyname><forenames>Lamling Venus</forenames></author><author><keyname>Gupta</keyname><forenames>Manik</forenames></author><author><keyname>Rajalakshmi</keyname><forenames>Pachamuthu</forenames></author></authors><title>Data Analysis on the High-Frequency Pollution Data Collected in India</title><categories>cs.OH</categories><comments>7 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fine grained 1Hz Carbon Monoxide pollution data were collected on a busy road
in Hyderabad, India. In this paper we report the findings from analysing the
experimental data, in which it was found that the data were log-normally
distributed and nonlinear. The dominant frequencies at peak hours were caused
by the pattern of traffic flow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7232</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7232</id><created>2013-01-30</created><authors><author><keyname>S&#xf8;rensen</keyname><forenames>Jesper H.</forenames></author><author><keyname>Stefanovi&#x107;</keyname><forenames>Cedomir</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Coded Splitting Tree Protocols</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach to multiple access control called coded
splitting tree protocol. The approach builds on the known tree splitting
protocols, code structure and successive interference cancellation (SIC).
Several instances of the tree splitting protocol are initiated, each instance
is terminated prematurely and subsequently iterated. The combined set of leaves
from all the tree instances can then be viewed as a graph code, which is
decodable using belief propagation. The main design problem is determining the
order of splitting, which enables successful decoding as early as possible.
Evaluations show that the proposed protocol provides considerable gains over
the standard tree splitting protocol applying SIC. The improvement comes at the
expense of an increased feedback and receiver complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7236</identifier>
 <datestamp>2013-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7236</id><created>2013-01-30</created><updated>2013-05-17</updated><authors><author><keyname>Yu</keyname><forenames>Jiun-Hung</forenames></author><author><keyname>Loeliger</keyname><forenames>Hans-Andrea</forenames></author></authors><title>Reverse Berlekamp-Massey Decoding</title><categories>cs.IT math.IT</categories><comments>Updated version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new algorithm for decoding Reed-Solomon codes (up to half the
minimum distance) and for computing inverses in $F[x]/m(x)$. The proposed
algorithm is similar in spirit and structure to the Berlekamp-Massey algorithm,
but it works naturally for general $m(x)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7245</identifier>
 <datestamp>2013-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7245</id><created>2013-01-30</created><authors><author><keyname>Kaufman</keyname><forenames>Brett</forenames></author><author><keyname>Lilleberg</keyname><forenames>Jorma</forenames></author><author><keyname>Aazhang</keyname><forenames>Behnaam</forenames></author></authors><title>Femtocell Architectures with Spectrum Sharing for Cellular Radio
  Networks</title><categories>cs.NI</categories><comments>9 pages, 8 figures, Accepted to the International Journal of Advances
  in Engineering Sciences and Applied Mathematics special issue on
  Multi-Terminal Information Theory</comments><journal-ref>International Journal of Advances in Engineering Sciences and
  Applied Mathematics, vol. 5, no. 1, pp. 66-75, Mar. 2013</journal-ref><doi>10.1007/s12572-013-0083-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Femtocells are an emerging technology aimed at providing gains to both
network operators and end-users. These gains come at a cost of increased
interference, specifically the cross network interference between the macrocell
and femtocell networks. This interference is one of the main performance
limiting factors in allowing an underlaid femtocell network to share the
spectrum with the cellular network. To manage this interference, we first
propose a femtocell architecture that orthog- onally partitions the network
bandwidth between the macrocell and femtocell networks. This scheme eliminates
the cross network interference thus giving the femtocells more freedom over
their use of the spectrum. Specifically, no interference constraint is imposed
by the cellular network allowing femto users to transmit at a constant power on
randomly selected channels. Although simple, this scheme is enough to give
gains up to 200% in sum rate.
  We then propose a second architecture where both networks share the bandwidth
simultaneously. A femtocell power control scheme that relies on minimal
coordination with the macrocell base station is used in conjunction with an
interference sensing channel assignment mechanism. These two schemes together
yield sum rate gains up to 200%. We then develop a technique for macro users to
join a nearby femtocell and share a common chan- nel with a femtocell user
through the use of successive interfer- ence cancellation. By adding this
mechanism to the power control and channel assignment schemes, we show sum rate
gains over 300% and up to 90% power savings for macrocell users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7250</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7250</id><created>2013-01-30</created><updated>2013-08-08</updated><authors><author><keyname>Bj&#xf6;rklund</keyname><forenames>Andreas</forenames></author><author><keyname>Husfeldt</keyname><forenames>Thore</forenames></author></authors><title>The Parity of Directed Hamiltonian Cycles</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deterministic algorithm that given any directed graph on n
vertices computes the parity of its number of Hamiltonian cycles in O(1.619^n)
time and polynomial space. For bipartite graphs, we give a 1.5^n poly(n)
expected time algorithm. Our algorithms are based on a new combinatorial
formula for the number of Hamiltonian cycles modulo a positive integer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7251</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7251</id><created>2013-01-30</created><authors><author><keyname>Alsinet</keyname><forenames>Teresa</forenames></author><author><keyname>Godo</keyname><forenames>Lluis</forenames></author><author><keyname>Sandri</keyname><forenames>Sandra</forenames></author></authors><title>On the Semantics and Automated Deduction for PLFC, a Logic of
  Possibilistic Uncertainty and Fuzziness</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1999)</comments><proxy>auai</proxy><report-no>UAI-P-1999-PG-3-12</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Possibilistic logic is a well-known graded logic of uncertainty suitable to
reason under incomplete information and partially inconsistent knowledge, which
is built upon classical first order logic. There exists for Possibilistic logic
a proof procedure based on a refutation complete resolution-style calculus.
Recently, a syntactical extension of first order Possibilistic logic (called
PLFC) dealing with fuzzy constants and fuzzily restricted quantifiers has been
proposed. Our aim is to present steps towards both the formalization of PLFC
itself and an automated deduction system for it by (i) providing a formal
semantics; (ii) defining a sound resolution-style calculus by refutation; and
(iii) describing a first-order proof procedure for PLFC clauses based on (ii)
and on a novel notion of most general substitution of two literals in a
resolution step. In contrast to standard Possibilistic logic semantics,
truth-evaluation of formulas with fuzzy constants are many-valued instead of
boolean, and consequently an extended notion of possibilistic uncertainty is
also needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7257</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7257</id><created>2013-01-30</created><authors><author><keyname>W&#xe4;hlisch</keyname><forenames>Matthias</forenames></author><author><keyname>Vorbach</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Keil</keyname><forenames>Christian</forenames></author><author><keyname>Sch&#xf6;nfelder</keyname><forenames>Jochen</forenames></author><author><keyname>Schmidt</keyname><forenames>Thomas C.</forenames></author><author><keyname>Schiller</keyname><forenames>Jochen H.</forenames></author></authors><title>Design, Implementation, and Operation of a Mobile Honeypot</title><categories>cs.CR cs.NI</categories><acm-class>C.2.6; C.2.0; C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile nodes, in particular smartphones are one of the most relevant devices
in the current Internet in terms of quantity and economic impact. There is the
common believe that those devices are of special interest for attackers due to
their limited resources and the serious data they store. On the other hand, the
mobile regime is a very lively network environment, which misses the (limited)
ground truth we have in commonly connected Internet nodes. In this paper we
argue for a simple long-term measurement infrastructure that allows for (1) the
analysis of unsolicited traffic to and from mobile devices and (2) fair
comparison with wired Internet access. We introduce the design and
implementation of a mobile honeypot, which is deployed on standard hardware for
more than 1.5 years. Two independent groups developed the same concept for the
system. We also present preliminary measurement results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7265</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7265</id><created>2013-01-30</created><authors><author><keyname>Gerami</keyname><forenames>Majid</forenames></author><author><keyname>Xiao</keyname><forenames>Ming</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Decentralized Minimum-Cost Repair for Distributed Storage Systems</title><categories>cs.IT cs.DC math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There have been emerging lots of applications for distributed storage systems
e.g., those in wireless sensor networks or cloud storage. Since storage nodes
in wireless sensor networks have limited battery, it is valuable to find a
repair scheme with optimal transmission costs (e.g., energy). The optimal-cost
repair has been recently investigated in a centralized way. However a
centralized control mechanism may not be available or is very expensive. For
the scenarios, it is interesting to study optimal-cost repair in a
decentralized setup. We formulate the optimal-cost repair as convex
optimization problems for the network with convex transmission costs. Then we
use primal and dual decomposition approaches to decouple the problem into
subproblems to be solved locally. Thus, each surviving node, collaborating with
other nodes, can minimize its transmission cost such that the global cost is
minimized. We further study the optimality and convergence of the algorithms.
Finally, we discuss the code construction and determine the field size for
finding feasible network codes in our approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7283</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7283</id><created>2013-01-30</created><authors><author><keyname>Hogg</keyname><forenames>J. D.</forenames></author><author><keyname>Scott</keyname><forenames>J. A.</forenames></author></authors><title>On the effects of scaling on the performance of Ipopt</title><categories>math.OC cs.MS math.NA</categories><report-no>RAL-P-2012-009</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The open-source nonlinear solver Ipopt (https://projects.coin-or.org/Ipopt)
is a widely-used software package for the solution of large-scale non-linear
optimization problems. At its heart, it employs a third-party linear solver to
solve a series of sparse symmetric indefinite systems. The speed, accuracy and
robustness of the chosen linear solver is critical to the overall performance
of Ipopt. In some instances, it can be beneficial to scale the linear system
before it is solved.
  In this paper, different scaling algorithms are employed within Ipopt with a
new linear solver HSL_MA97 from the HSL mathematical software library
(http://www.hsl.rl.ac.uk). An extensive collection of problems from the CUTEr
test set (http://www.cuter.rl.ac.uk) is used to illustrate the effects of
scaling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7314</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7314</id><created>2013-01-30</created><authors><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author></authors><title>Subexponential parameterized algorithm for computing the cutwidth of a
  semi-complete digraph</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cutwidth of a digraph is a width measure introduced by Chudnovsky, Fradkin,
and Seymour [4] in connection with development of a structural theory for
tournaments, or more generally, for semi-complete digraphs. In this paper we
provide an algorithm with running time 2^{O(\sqrt{k log k})} * n^{O(1)} that
tests whether the cutwidth of a given n-vertex semi-complete digraph is at most
k, improving upon the currently fastest algorithm of the second author [18]
that works in 2^{O(k)} * n^2 time. As a byproduct, we obtain a new algorithm
for Feedback Arc Set in tournaments (FAST) with running time 2^{c\sqrt{k}} *
n^{O(1)}, where c = 2\pi / \sqrt(3)*\ln(2) &lt;= 5.24, that is simpler than the
algorithms of Feige [9] and of Karpinski and Schudy[16], both also working in
2^{O(\sqrt{k})} * n^{O(1)} time. Our techniques can be applied also to other
layout problems on semi-complete digraphs. We show that the Optimal Linear
Arrangement problem, a close relative of Feedback Arc Set, can be solved in
2^{O(k^{1/3} \sqrt{\log k})} * n^{O(1)} time, where k is the target cost of the
ordering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7320</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7320</id><created>2013-01-30</created><authors><author><keyname>Bradonji&#x107;</keyname><forenames>Milan</forenames></author><author><keyname>Hagberg</keyname><forenames>Aric</forenames></author><author><keyname>Hengartner</keyname><forenames>Nicolas W.</forenames></author><author><keyname>Lemons</keyname><forenames>Nathan</forenames></author><author><keyname>Percus</keyname><forenames>Allon G.</forenames></author></authors><title>The phase transition in inhomogeneous random intersection graphs</title><categories>cs.DM math.CO math.PR</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the component evolution in inhomogeneous random intersection
graphs when the average degree is close to 1. As the average degree increases,
the size of the largest component in the random intersection graph goes through
a phase transition. We give bounds on the size of the largest components before
and after this transition. We also prove that the largest component after the
transition is unique. These results are similar to the phase transition in
Erd\H{o}s-R\'enyi random graphs; one notable difference is that the jump in the
size of the largest component varies in size depending on the parameters of the
random intersection graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7321</identifier>
 <datestamp>2013-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7321</id><created>2013-01-30</created><updated>2013-02-22</updated><authors><author><keyname>Kloos</keyname><forenames>Johannes</forenames></author><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author><author><keyname>Niksic</keyname><forenames>Filip</forenames></author><author><keyname>Piskac</keyname><forenames>Ruzica</forenames></author></authors><title>Incremental, Inductive Coverability</title><categories>cs.LO</categories><comments>Non-reviewed version, original version submitted to CAV 2013; this is
  a revised version, containing more experimental results and some corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an incremental, inductive (IC3) procedure to check coverability of
well-structured transition systems. Our procedure generalizes the IC3 procedure
for safety verification that has been successfully applied in finite-state
hardware verification to infinite-state well-structured transition systems. We
show that our procedure is sound, complete, and terminating for downward-finite
well-structured transition systems---where each state has a finite number of
states below it---a class that contains extensions of Petri nets, broadcast
protocols, and lossy channel systems.
  We have implemented our algorithm for checking coverability of Petri nets. We
describe how the algorithm can be efficiently implemented without the use of
SMT solvers. Our experiments on standard Petri net benchmarks show that IC3 is
competitive with state-of-the-art implementations for coverability based on
symbolic backward analysis or expand-enlarge-and-check algorithms both in time
taken and space usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7345</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7345</id><created>2013-01-30</created><updated>2013-04-29</updated><authors><author><keyname>Ghatak</keyname><forenames>Anirban</forenames></author></authors><title>Codes on Lattices for Random SAF Routing</title><categories>cs.IT math.IT</categories><comments>17 pages, 1 figure; some typos corrected, a table of numerical data
  added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a construction of constant weight codes based on the unique
decomposition of elements in lattices is presented. The conditions for unique
primary decomposition and unique irreducible decomposition in lattices are
discussed and connections with the decomposition of ideals in Noetherian
commutative rings established. In this context it is shown, drawing on the
definitive works of Dilworth, Ward and others, that, as opposed to Noetherian
commutative rings, the existence of unique irreducible decomposition in
lattices does not guarantee unique primary decomposition. The source alphabet
in our proposed construction is a set of uniquely decomposable elements
constructed from a chosen subset of irreducible or primary elements of the
appropriate lattice. The distance function between two lattice elements is
based on the symmetric distance between sets of constituent elements. It is
known that constructing such constant weight codes is equivalent to
constructing a Johnson graph with appropriate parameters. Some bounds on the
code sizes are also presented and a method to obtain codes of optimal size,
utilizing the Johnson graph description of the codes, is discussed. As an
application we show how these codes can be used for error and erasure
correction in random networks employing store-and-forward (SAF) routing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7351</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7351</id><created>2013-01-30</created><authors><author><keyname>Anderson</keyname><forenames>Ross</forenames></author><author><keyname>Brady</keyname><forenames>Robert</forenames></author></authors><title>Why quantum computing is hard - and quantum cryptography is not provably
  secure</title><categories>quant-ph cs.CR math-ph math.MP</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite high hopes for quantum computation in the 1990s, progress in the past
decade has been slow; we still cannot perform computation with more than about
three qubits and are no closer to solving problems of real interest than a
decade ago. Separately, recent experiments in fluid mechanics have demonstrated
the emergence of a full range of quantum phenomena from completely classical
motion. We present two specific hypotheses. First, Kuramoto theory may give a
basis for geometrical thinking about entanglement. Second, we consider a recent
soliton model of the electron, in which the quantum-mechanical wave function is
a phase modulation of a carrier wave. Both models are consistent with one
another and with observation. Both models suggest how entanglement and
decoherence may be related to device geometry. Both models predict that it will
be difficult to maintain phase coherence of more than three qubits in the
plane, or four qubits in a three-dimensional structure. The soliton model also
shows that the experimental work which appeared to demonstrate a violation of
Bell's inequalities might not actually do so; regardless of whether it is a
correct description of the world, it exposes a flaw in the logic of the Bell
tests. Thus the case for the security of EPR-based quantum cryptography has
just not been made. We propose experiments in quantum computation to test this.
Finally, we examine two possible interpretations of such soliton models: one is
consistent with the transactional interpretation of quantum mechanics, while
the other is an entirely classical model in which we do not have to abandon the
idea of a single world where action is local and causal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7356</identifier>
 <datestamp>2013-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7356</id><created>2013-01-30</created><authors><author><keyname>Behrend</keyname><forenames>Roger E.</forenames></author></authors><title>Fractional Perfect b-Matching Polytopes. I: General Theory</title><categories>math.CO cs.DM</categories><comments>37 pages</comments><msc-class>52B05, 05C50, 05C70, 52B11, 90C27, 90C35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fractional perfect b-matching polytope of an undirected graph G is the
polytope of all assignments of nonnegative real numbers to the edges of G such
that the sum of the numbers over all edges incident to any vertex v is a
prescribed nonnegative number b_v. General theorems which provide conditions
for nonemptiness, give a formula for the dimension, and characterize the
vertices, edges and face lattices of such polytopes are obtained. Many of these
results are expressed in terms of certain spanning subgraphs of G which are
associated with subsets or elements of the polytope. For example, it is shown
that an element u of the fractional perfect b-matching polytope of G is a
vertex of the polytope if and only if each component of the graph of u either
is acyclic or else contains exactly one cycle with that cycle having odd
length, where the graph of u is defined to be the spanning subgraph of G whose
edges are those at which u is positive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7358</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7358</id><created>2013-01-30</created><authors><author><keyname>Amgoud</keyname><forenames>Leila</forenames></author><author><keyname>Cayrol</keyname><forenames>Claudette</forenames></author></authors><title>On the Acceptability of Arguments in Preference-Based Argumentation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-1-7</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Argumentation is a promising model for reasoning with uncertain knowledge.
The key concept of acceptability enables to differentiate arguments and
counterarguments: The certainty of a proposition can then be evaluated through
the most acceptable arguments for that proposition. In this paper, we
investigate different complementary points of view: - an acceptability based on
the existence of direct counterarguments, - an acceptability based on the
existence of defenders. Pursuing previous work on preference-based
argumentation principles, we enforce both points of view by taking into account
preference orderings for comparing arguments. Our approach is illustrated in
the context of reasoning with stratified knowldge bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7359</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7359</id><created>2013-01-30</created><authors><author><keyname>Benferhat</keyname><forenames>Salem</forenames></author><author><keyname>Sossai</keyname><forenames>Claudio</forenames></author></authors><title>Merging Uncertain Knowledge Bases in a Possibilistic Logic Framework</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-8-15</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of merging uncertain information in the
framework of possibilistic logic. It presents several syntactic combination
rules to merge possibilistic knowledge bases, provided by different sources,
into a new possibilistic knowledge base. These combination rules are first
described at the meta-level outside the language of possibilistic logic. Next,
an extension of possibilistic logic, where the combination rules are inside the
language, is proposed. A proof system in a sequent form, which is sound and
complete with respect to the possibilistic logic semantics, is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.7360</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.7360</id><created>2013-01-30</created><authors><author><keyname>Bloemeke</keyname><forenames>Mark</forenames></author><author><keyname>Valtorta</keyname><forenames>Marco</forenames></author></authors><title>A Hybrid Algorithm to Compute Marginal and Joint Beliefs in Bayesian
  Networks and Its Complexity</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fourteenth Conference on Uncertainty in
  Artificial Intelligence (UAI1998)</comments><proxy>auai</proxy><report-no>UAI-P-1998-PG-16-23</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There exist two general forms of exact algorithms for updating probabilities
in Bayesian Networks. The first approach involves using a structure, usually a
clique tree, and performing local message based calculation to extract the
belief in each variable. The second general class of algorithm involves the use
of non-serial dynamic programming techniques to extract the belief in some
desired group of variables. In this paper we present a hybrid algorithm based
on the latter approach yet possessing the ability to retrieve the belief in all
single variables. The technique is advantageous in that it saves a NP-hard
computation step over using one algorithm of each type. Furthermore, this
technique re-enforces a conjecture of Jensen and Jensen [JJ94] in that it still
requires a single NP-hard step to set up the structure on which inference is
performed, as we show by confirming Li and D'Ambrosio's [LD94] conjectured
NP-hardness of OFP.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="40000" completeListSize="102538">1122234|41001</resumptionToken>
</ListRecords>
</OAI-PMH>
