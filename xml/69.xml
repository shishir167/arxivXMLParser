<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:38:19Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|68001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8594</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8594</id><created>2014-10-30</created><authors><author><keyname>Almarza</keyname><forenames>Javier</forenames></author><author><keyname>Figueira</keyname><forenames>Santiago</forenames></author></authors><title>Normality in non-integer bases and polynomial time randomness</title><categories>math.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that if $x\in[0,1]$ is polynomial time random (i.e. no polynomial
time computable martingale succeeds on the binary fractional expansion of $x$)
then $x$ is normal in any integer base greater than one. We show that if $x$ is
polynomial time random and $\beta&gt;1$ is Pisot, then $x$ is &quot;normal in base
$\beta$&quot;, in the sense that the sequence $(x\beta^n)_{n\in\mathbb{N}}$ is
uniformly distributed modulo one. We work with the notion of &quot;$P$-martingale&quot;,
a generalization of martingales to non-uniform distributions, and show that a
sequence over a finite alphabet is distributed according to an irreducible,
invariant Markov measure~$P$ if an only if no $P$-martingale whose betting
factors are computed by a deterministic finite automaton succeeds on it. This
is a generalization of Schnorr and Stimm's characterization of normal sequences
in integer bases. Our results use tools and techniques from symbolic dynamics,
together with automata theory and algorithmic randomness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8597</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8597</id><created>2014-10-30</created><updated>2015-05-19</updated><authors><author><keyname>Han</keyname><forenames>Qiuyi</forenames></author><author><keyname>Xu</keyname><forenames>Kevin S.</forenames></author><author><keyname>Airoldi</keyname><forenames>Edoardo M.</forenames></author></authors><title>Consistent estimation of dynamic and multi-layer block models</title><categories>stat.ME cs.SI math.ST physics.soc-ph stat.TH</categories><comments>To appear at ICML 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Significant progress has been made recently on theoretical analysis of
estimators for the stochastic block model (SBM). In this paper, we consider the
multi-graph SBM, which serves as a foundation for many application settings
including dynamic and multi-layer networks. We explore the asymptotic
properties of two estimators for the multi-graph SBM, namely spectral
clustering and the maximum-likelihood estimate (MLE), as the number of layers
of the multi-graph increases. We derive sufficient conditions for consistency
of both estimators and propose a variational approximation to the MLE that is
computationally feasible for large networks. We verify the sufficient
conditions via simulation and demonstrate that they are practical. In addition,
we apply the model to two real data sets: a dynamic social network and a
multi-layer social network with several types of relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8616</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8616</id><created>2014-10-30</created><authors><author><keyname>Chandra</keyname><forenames>Abhijit</forenames></author><author><keyname>Kar</keyname><forenames>Oliva</forenames></author></authors><title>Data Driven Prognosis: A multi-physics approach verified via balloon
  burst experiment</title><categories>cs.CE</categories><doi>10.1098/rspa.2014.0525</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multi-physics formulation for Data Driven Prognosis (DDP) is developed.
Unlike traditional predictive strategies that require controlled off-line
measurements or training for determination of constitutive parameters to derive
the transitional statistics, the proposed DDP algorithm relies solely on in
situ measurements. It utilizes a deterministic mechanics framework, but the
stochastic nature of the solution arises naturally from the underlying
assumptions regarding the order of the conservation potential as well as the
number of dimensions involved. The proposed DDP scheme is capable of predicting
onset of instabilities. Since the need for off-line testing (or training) is
obviated, it can be easily implemented for systems where such a priori testing
is difficult or even impossible to conduct. The prognosis capability is
demonstrated here via a balloon burst experiment where the instability is
predicted utilizing only on-line visual observations. The DDP scheme never
failed to predict the incipient failure, and no false positives were issued.
The DDP algorithm is applicable to others types of datasets. Time horizons of
DDP predictions can be adjusted by using memory over different time windows.
Thus, a big dataset can be parsed in time to make a range of predictions over
varying time horizons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8618</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8618</id><created>2014-10-30</created><updated>2015-05-21</updated><authors><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Zhang</keyname><forenames>Haixian</forenames></author><author><keyname>Mao</keyname><forenames>Hua</forenames></author><author><keyname>Sang</keyname><forenames>Yongsheng</forenames></author><author><keyname>Yi</keyname><forenames>Zhang</forenames></author></authors><title>Symmetric low-rank representation for subspace clustering</title><categories>cs.CV</categories><comments>13 pages</comments><doi>10.1016/j.neucom.2015.08.077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a symmetric low-rank representation (SLRR) method for subspace
clustering, which assumes that a data set is approximately drawn from the union
of multiple subspaces. The proposed technique can reveal the membership of
multiple subspaces through the self-expressiveness property of the data. In
particular, the SLRR method considers a collaborative representation combined
with low-rank matrix recovery techniques as a low-rank representation to learn
a symmetric low-rank representation, which preserves the subspace structures of
high-dimensional data. In contrast to performing iterative singular value
decomposition in some existing low-rank representation based algorithms, the
symmetric low-rank representation in the SLRR method can be calculated as a
closed form solution by solving the symmetric low-rank optimization problem. By
making use of the angular information of the principal directions of the
symmetric low-rank representation, an affinity graph matrix is constructed for
spectral clustering. Extensive experimental results show that it outperforms
state-of-the-art subspace clustering algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8620</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8620</id><created>2014-10-30</created><authors><author><keyname>Defazio</keyname><forenames>Aaron</forenames></author><author><keyname>Graepel</keyname><forenames>Thore</forenames></author></authors><title>A Comparison of learning algorithms on the Arcade Learning Environment</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Reinforcement learning agents have traditionally been evaluated on small toy
problems. With advances in computing power and the advent of the Arcade
Learning Environment, it is now possible to evaluate algorithms on diverse and
difficult problems within a consistent framework. We discuss some challenges
posed by the arcade learning environment which do not manifest in simpler
environments. We then provide a comparison of model-free, linear learning
algorithms on this challenging problem set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8623</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8623</id><created>2014-10-30</created><authors><author><keyname>Fenn</keyname><forenames>Shannon</forenames></author><author><keyname>Mendes</keyname><forenames>Alexandre</forenames></author><author><keyname>Budden</keyname><forenames>David</forenames></author></authors><title>Addressing the non-functional requirements of computer vision systems: A
  case study</title><categories>cs.CV cs.RO cs.SE</categories><comments>17 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer vision plays a major role in the robotics industry, where vision
data is frequently used for navigation and high-level decision making. Although
there is significant research in algorithms and functional requirements, there
is a comparative lack of emphasis on how best to map these abstract concepts
onto an appropriate software architecture.
  In this study, we distinguish between the functional and non-functional
requirements of a computer vision system. Using a RoboCup humanoid robot system
as a case study, we propose and develop a software architecture that fulfills
the latter criteria.
  The modifiability of the proposed architecture is demonstrated by detailing a
number of feature detection algorithms and emphasizing which aspects of the
underlying framework were modified to support their integration. To demonstrate
portability, we port our vision system (designed for an application-specific
DARwIn-OP humanoid robot) to a general-purpose, Raspberry Pi computer. We
evaluate performance on both platforms and compare them to a vision system
optimised for functional requirements only.
  The architecture and implementation presented in this study provide a highly
generalisable framework for computer vision system design that is of particular
benefit in research and development, competition and other environments in
which rapid system evolution is necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8633</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8633</id><created>2014-10-31</created><authors><author><keyname>Sediq</keyname><forenames>Akram Bin</forenames></author><author><keyname>Schoenen</keyname><forenames>Rainer</forenames></author><author><keyname>Yanikomeroglu</keyname><forenames>Halim</forenames></author><author><keyname>Senarath</keyname><forenames>Gamini</forenames></author></authors><title>Optimized Distributed Inter-cell Interference Coordination (ICIC) Scheme
  using Projected Subgradient and Network Flow Optimization</title><categories>cs.NI</categories><comments>To appear in IEEE Transactions on Communications (acceptance: 17
  October 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we tackle the problem of multi-cell resource scheduling, where
the objective is to maximize the weighted sum-rate through inter-cell
interference coordination (ICIC). The blanking method is used to mitigate the
inter-cell interference, where a resource is either used with a predetermined
transmit power or not used at all, i.e., blanked. This problem is known to be
strongly NP-hard, which means that it is not only hard to solve in polynomial
time, but it is also hard to find an approximation algorithm with guaranteed
optimality gap. In this work, we identify special scenarios where a
polynomial-time algorithm can be constructed to solve this problem with
theoretical guarantees. In particular, we define a dominant interference
environment, in which for each user the received power from each interferer is
significantly greater than the aggregate received power from all other weaker
interferers.
  We show that the strongly NP-hard problem can be tightly relaxed to a linear
programming problem in a dominant interference environment. Consequently, we
propose a polynomial-time distributed algorithm that is based on the
primal-decomposition, the projected-subgradient, and the network flow
optimization methods. In comparison with baseline schemes, simulation results
show that the proposed scheme achieves higher gains in aggregate throughput,
cell-edge throughput, and outage probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8635</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8635</id><created>2014-10-31</created><updated>2014-12-09</updated><authors><author><keyname>Lu</keyname><forenames>Xiao</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author><author><keyname>Kim</keyname><forenames>Dong In</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Wireless Charger Networking for Mobile Devices: Fundamentals, Standards,
  and Applications</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless charging is a technique of transmitting power through an air gap to
an electrical device for the purpose of energy replenishment. Recently, the
wireless charging technology has been significantly advanced in terms of
efficiency and functionality. This article first presents an overview and
fundamentals of wireless charging. We then provide the review of standards,
i.e., Qi and Alliance for Wireless Power (A4WP), and highlight on their
communication protocols. Next, we propose a novel concept of wireless charger
networking which allows chargers to be connected to facilitate information
collection and control. We demonstrate the application of the wireless charger
network in user-charger assignment, which clearly shows the benefit in terms of
reduced cost for users to identify the best chargers to replenish energy for
their mobile devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8660</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8660</id><created>2014-10-31</created><authors><author><keyname>Jiang</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Zhou</keyname><forenames>Sheng</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author></authors><title>Dynamic Channel Acquisition in MU-MIMO</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Trans. Commun</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiuser multiple-input-multiple-output (MU-MIMO) systems are known to be
hindered by dimensionality loss due to channel state information (CSI)
acquisition overhead. In this paper, we investigate user-scheduling in MU-MIMO
systems on account of CSI acquisition overhead, where a base station
dynamically acquires user channels to avoid choking the system with CSI
overhead. The genie-aided optimization problem (GAP) is first formulated to
maximize the Lyapunov-drift every scheduling step, incorporating user queue
information and taking channel fluctuations into consideration. The scheduling
scheme based on GAP, namely the GAP-rule, is proved to be throughput-optimal
but practically infeasible, and thus serves as a performance bound. In view of
the implementation overhead and delay unfairness of the GAP-rule, the T-frame
dynamic channel acquisition scheme and the power-law DCA scheme are further
proposed to mitigate the implementation overhead and delay unfairness,
respectively. Both schemes are based on the GAP-rule and proved
throughput-optimal. To make the schemes practically feasible, we then propose
the heuristic schemes, queue-based quantized-block-length user scheduling
scheme (QQS), T-frame QQS, and power-law QQS, which are the practical versions
of the aforementioned GAP-based schemes, respectively. The QQS-based schemes
substantially decrease the complexity, and also perform fairly close to the
optimum. Numerical results evaluate the proposed schemes under various system
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8663</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8663</id><created>2014-10-31</created><authors><author><keyname>Tan</keyname><forenames>Zihan</forenames></author><author><keyname>Zeng</keyname><forenames>Liwei</forenames></author><author><keyname>Li</keyname><forenames>Jian</forenames></author></authors><title>On the Inequalities of Projected Volumes and the Constructible Region</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the following geometry problem: given a $2^n-1$ dimensional vector
$\pi=\{\pi_S\}_{S\subseteq [n], S\ne \emptyset}$, is there an object
$T\subseteq\mathbb{R}^n$ such that $\log(\mathsf{vol}(T_S))= \pi_S$, for all
$S\subseteq [n]$, where $T_S$ is the projection of $T$ to the subspace spanned
by the axes in $S$? If $\pi$ does correspond to an object in $\mathbb{R}^n$, we
say that $\pi$ is {\em constructible}. We use $\Psi_n$ to denote the
constructible region, i.e., the set of all constructible vectors in
$\mathbb{R}^{2^n-1}$. In 1995, Bollob\'{a}s and Thomason showed that $\Psi_n$
is contained in a polyhedral cone, defined a class of so called uniform cover
inequalities. We propose a new set of natural inequalities, called
nonuniform-cover inequalities, which generalize the BT inequalities. We show
that any linear inequality that all points in $\Psi_n$ satisfy must be a
nonuniform-cover inequality. Based on this result and an example by
Bollob\'{a}s and Thomason, we show that constructible region $\Psi_n$ is not
even convex, and thus cannot be fully characterized by linear inequalities. We
further show that some subclasses of the nonuniform-cover inequalities are not
correct by various combinatorial constructions, which refutes a previous
conjecture about $\Psi_n$. Finally, we conclude with an interesting conjecture
regarding the convex hull of $\Psi_n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8664</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8664</id><created>2014-10-31</created><authors><author><keyname>Lin</keyname><forenames>Yishi</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author></authors><title>Algorithmic Design for Competitive Influence Maximization Problems</title><categories>cs.SI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given the popularity of the viral marketing campaign in online social
networks, finding an effective method to identify a set of most influential
nodes so to compete well with other viral marketing competitors is of upmost
importance. We propose a &quot;General Competitive Independent Cascade (GCIC)&quot; model
to describe the general influence propagation of two competing sources in the
same network. We formulate the &quot;Competitive Influence Maximization (CIM)&quot;
problem as follows: Under a prespecified influence propagation model and that
the competitor's seed set is known, how to find a seed set of $k$ nodes so as
to trigger the largest influence cascade? We propose a general algorithmic
framework TCIM for the CIM problem under the GCIC model. TCIM returns a
$(1-1/e-\epsilon)$-approximate solution with probability at least
$1-n^{-\ell}$, and has an efficient time complexity of $O(c(k+\ell)(m+n)\log
n/\epsilon^2)$, where $c$ depends on specific propagation model and may also
depend on $k$ and underlying network $G$. To the best of our knowledge, this is
the first general algorithmic framework that has both $(1-1/e-\epsilon)$
performance guarantee and practical efficiency. We conduct extensive
experiments on real-world datasets under three specific influence propagation
models, and show the efficiency and accuracy of our framework. In particular,
we achieve up to four orders of magnitude speedup as compared to the previous
state-of-the-art algorithms with the approximate guarantee.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8665</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8665</id><created>2014-10-31</created><authors><author><keyname>Mary</keyname><forenames>Sahaaya Arul</forenames></author><author><keyname>Gnanadurai</keyname><forenames>Jasmine Beulah</forenames></author></authors><title>A Novel Predictive and Non-Predictive Cooperative Model for Routing in
  Ad Hoc Networks</title><categories>cs.NI</categories><comments>6 Pages, 3 Figures</comments><journal-ref>International Journal of Computer Applications 102(15):32-37,
  September 2014</journal-ref><doi>10.5120/17894-8908</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adhoc networks are formed by intermediate nodes which agree to relay
traffic.The link between nodes is broken when a node rejects to relay traffic.
Various parameters like depreciation in the energy of a node, distance between
nodes and mobility of the nodes play a vital role in determining the nodes
rejection to relay traffic.The objective of this paper is to propose a novel
model that identifies the cooperative nodes forming stable routes at the route
discovery phase.The weight factor of the different parameters decides the
varied type of networks where the proposed model can be applied. Hence, an
Artificial Neural Network based nondeterministic generic predictive model is
proposed to identify the varied types of networks based on the weight
factor.This study has been substantiated by simulation using OMNET++ simulator.
We are sure that this paper will give a better solution to identify cooperative
nodes thereby improving the performance of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8668</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8668</id><created>2014-10-31</created><authors><author><keyname>K&#xfc;&#xe7;&#xfc;k</keyname><forenames>Dilek</forenames></author><author><keyname>Steinberger</keyname><forenames>Ralf</forenames></author></authors><title>Experiments to Improve Named Entity Recognition on Turkish Tweets</title><categories>cs.CL</categories><comments>appears in Proceedings of the EACL Workshop on Language Analysis for
  Social Media, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media texts are significant information sources for several
application areas including trend analysis, event monitoring, and opinion
mining. Unfortunately, existing solutions for tasks such as named entity
recognition that perform well on formal texts usually perform poorly when
applied to social media texts. In this paper, we report on experiments that
have the purpose of improving named entity recognition on Turkish tweets, using
two different annotated data sets. In these experiments, starting with a
baseline named entity recognition system, we adapt its recognition rules and
resources to better fit Twitter language by relaxing its capitalization
constraint and by diacritics-based expansion of its lexical resources, and we
employ a simplistic normalization scheme on tweets to observe the effects of
these on the overall named entity recognition performance on Turkish tweets.
The evaluation results of the system with these different settings are provided
with discussions of these results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8673</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8673</id><created>2014-10-31</created><authors><author><keyname>Guo</keyname><forenames>Meng</forenames></author><author><keyname>Tumova</keyname><forenames>Jana</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author></authors><title>Communication-Free Multi-Agent Control under Local Temporal Tasks and
  Relative-Distance Constraints</title><categories>cs.MA cs.SY math.OC</categories><comments>10 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a distributed control and coordination strategy for multi-agent
systems where each agent has a local task specified as a Linear Temporal Logic
(LTL) formula and at the same time is subject to relative-distance constraints
with its neighboring agents. The local tasks capture the temporal requirements
on individual agents' behaviors, while the relative-distance constraints impose
requirements on the collective motion of the whole team. The proposed solution
relies only on relative-state measurements among the neighboring agents without
the need for explicit information exchange. It is guaranteed that the local
tasks given as syntactically co-safe or general LTL formulas are fulfilled and
the relative-distance constraints are satisfied at all time. The approach is
demonstrated with computer simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8674</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8674</id><created>2014-10-31</created><authors><author><keyname>Zemanov&#xe1;</keyname><forenames>Alena</forenames></author><author><keyname>Zeman</keyname><forenames>Jan</forenames></author><author><keyname>&#x160;ejnoha</keyname><forenames>Michal</forenames></author></authors><title>Finite element model based on refined plate theories for laminated glass
  units</title><categories>cs.CE</categories><comments>22 pages, 10 figures, 3 tables</comments><journal-ref>Latin American Journal of Solids and Structures 15(6):1158--1180,
  2015</journal-ref><doi>10.1590/1679-78251676</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Laminated glass units exhibit complex response as a result of different
mechanical behavior and properties of glass and polymer foil. We aim to develop
a finite element model for elastic laminated glass plates based on the refined
plate theory by Mau. For a geometrically nonlinear description of the behavior
of units, each layer behaves according to the Reissner-Mindlin kinematics,
complemented with membrane effects and the von K\'{a}rm\'{a}n assumptions.
Nodal Lagrange multipliers enforce the compatibility of independent layers in
this approach. We have derived the discretized model by the energy-minimization
arguments, assuming that the unknown fields are approximated by bi-linear
functions at the element level, and solved the resulting system by the Newton
method with consistent linearization. We have demonstrated through verification
and validation examples that the proposed formulation is reliable and
accurately reproduces the behavior of laminated glass units. This study
represents a first step to the development of a comprehensive, mechanics-based
model for laminated glass systems that is suitable for implementation in common
engineering finite element solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8675</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8675</id><created>2014-10-31</created><authors><author><keyname>Oiwa</keyname><forenames>Hidekazu</forenames></author><author><keyname>Fujimaki</keyname><forenames>Ryohei</forenames></author></authors><title>Partition-wise Linear Models</title><categories>stat.ML cs.LG</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Region-specific linear models are widely used in practical applications
because of their non-linear but highly interpretable model representations. One
of the key challenges in their use is non-convexity in simultaneous
optimization of regions and region-specific models. This paper proposes novel
convex region-specific linear models, which we refer to as partition-wise
linear models. Our key ideas are 1) assigning linear models not to regions but
to partitions (region-specifiers) and representing region-specific linear
models by linear combinations of partition-specific models, and 2) optimizing
regions via partition selection from a large number of given partition
candidates by means of convex structured regularizations. In addition to
providing initialization-free globally-optimal solutions, our convex
formulation makes it possible to derive a generalization bound and to use such
advanced optimization techniques as proximal methods and decomposition of the
proximal maps for sparsity-inducing regularizations. Experimental results
demonstrate that our partition-wise linear models perform better than or are at
least competitive with state-of-the-art region-specific or locally linear
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8685</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8685</id><created>2014-10-31</created><authors><author><keyname>Campani</keyname><forenames>Marco</forenames></author><author><keyname>Vaglio</keyname><forenames>Ruggero</forenames></author></authors><title>A simple interpretation of the growth of scientific/technological
  research impact leading to hype-type evolution curves</title><categories>cs.DL physics.soc-ph</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The empirical and theoretical justification of Gartner hype curves is a very
relevant open question in the field of Technological Life Cycle analysis. The
scope of the present paper is to introduce a simple model describing the growth
of scientific/technological research impact, in the specific case where science
is the main source of a new idea driving a technological development, leading
to hype-type evolution curves. The main idea of the model is that, in a first
stage, the growth of the scientific interest of a new specific field (as can be
measured by publication numbers) basically follows the classical logistic
growth curve. At a second stage, starting at a later trigger time, the
technological development based on that scientific idea (as can be measured by
patent deposits) can be described as the integral (in a mathematical sense) of
the first curve, since technology is based on the overall accumulated
scientific knowledge. The model is tested through a bibliometric analysis of
the publication and patent deposit rate for Organic Light Emitting Diodes
(OLED) scientific research and technology, as well as for other emerging
technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8692</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8692</id><created>2014-10-31</created><updated>2015-03-06</updated><authors><author><keyname>Bergstra</keyname><forenames>Jan A.</forenames></author><author><keyname>Bethke</keyname><forenames>Inge</forenames></author></authors><title>Note on paraconsistency and reasoning about fractions</title><categories>cs.LO math.LO</categories><comments>6 pages</comments><report-no>report TCS1413, October 2014</report-no><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply a paraconsistent logic to reason about fractions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8694</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8694</id><created>2014-10-31</created><authors><author><keyname>Tahraoui</keyname><forenames>M. A.</forenames></author><author><keyname>Duchene</keyname><forenames>E.</forenames></author><author><keyname>Kheddouci</keyname><forenames>H.</forenames></author><author><keyname>Wozniak</keyname><forenames>M.</forenames></author></authors><title>Labeled embedding of (n,n-2)-graphs in their complements</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph packing generally deals with unlabeled graphs. In \cite{EHRT11}, the
authors have introduced a new variant of the graph packing problem, called the
\textit{labeled packing of a graph}. This problem has recently been studied on
trees \cite{TDK13} and cycles \cite{EHRT11}. In this note, we present a lower
bound on the labeled packing number of any $(n,n-2)$-graph into $K_n$. This
result improves the bound given by Wo\'zniak in \cite{W94}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8713</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8713</id><created>2014-10-31</created><authors><author><keyname>Kuriakose</keyname><forenames>Jeril</forenames></author><author><keyname>Joshi</keyname><forenames>Sandeep</forenames></author><author><keyname>George</keyname><forenames>V. I.</forenames></author></authors><title>Localization in Wireless Sensor Networks: A Survey</title><categories>cs.NI</categories><comments>3 papes, 3 figures, conference proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Localization is widely used in Wireless Sensor Networks (WSNs) to identify
the current location of the sensor odes. A WSN consist of thousands of nodes
that make the installation of GPS on each sensor node expensive and moreover
GPS may not provide exact localization results in an indoor environment.
Manually configuring location reference on each sensor node is also not
possible for dense network. This gives rise to a problem where the sensor nodes
must identify its current location without using any special hardware like GPS
and without the help of manual configuration. In this paper we review the
localization techniques used by wireless sensor nodes to identify their current
location.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8714</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8714</id><created>2014-10-31</created><updated>2015-10-09</updated><authors><author><keyname>Bocharova</keyname><forenames>Irina E.</forenames></author><author><keyname>F&#xe0;bregas</keyname><forenames>Albert Guill&#xe9;n i</forenames></author><author><keyname>Kudryashov</keyname><forenames>Boris D.</forenames></author><author><keyname>Martinez</keyname><forenames>Alfonso</forenames></author><author><keyname>Campo</keyname><forenames>Adri&#xe0; Tauste</forenames></author><author><keyname>Vazquez-Vilar</keyname><forenames>Gonzalo</forenames></author></authors><title>Multi-Class Source-Channel Coding</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><msc-class>94A05</msc-class><acm-class>E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an almost-lossless source-channel coding scheme in which
source messages are assigned to different classes and encoded with a channel
code that depends on the class index. The code performance is studied by means
of random-coding error exponents and validated by simulation of a
low-complexity implementation using existing source and channel codes. While
each class code can be seen as a concatenation of a source code and a channel
code, the overall performance improves on that of separate source-channel
coding and approaches that of joint source-channel coding when the number of
classes increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8720</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8720</id><created>2014-10-31</created><updated>2016-02-15</updated><authors><author><keyname>de Rezende</keyname><forenames>Pedro J.</forenames></author><author><keyname>de Souza</keyname><forenames>Cid C.</forenames></author><author><keyname>Friedrichs</keyname><forenames>Stephan</forenames></author><author><keyname>Hemmer</keyname><forenames>Michael</forenames></author><author><keyname>Kr&#xf6;ller</keyname><forenames>Alexander</forenames></author><author><keyname>Tozoni</keyname><forenames>Davi C.</forenames></author></authors><title>Engineering Art Galleries</title><categories>cs.CG</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Art Gallery Problem is one of the most well-known problems in
Computational Geometry, with a rich history in the study of algorithms,
complexity, and variants. Recently there has been a surge in experimental work
on the problem. In this survey, we describe this work, show the chronology of
developments, and compare current algorithms, including two unpublished
versions, in an exhaustive experiment. Furthermore, we show what core
algorithmic ingredients have led to recent successes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8725</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8725</id><created>2014-10-31</created><authors><author><keyname>Agnihotri</keyname><forenames>Samar</forenames></author></authors><title>Analog Network Coding in Nonlinear Chains</title><categories>cs.IT math.IT</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of characterizing the optimal rate achievable with analog network
coding (ANC) for a unicast communication over general wireless relay networks
is computationally hard. A relay node performing ANC scales and forwards its
input signals. The source-destination channel in such communication scenarios
is, in general, an intersymbol interference (ISI) channel which leads to the
single-letter characterization of the optimal rate in terms of an optimization
problem with nonconvex, non closed-form objective function and non-convex
constraints. For a special class of such networks, called layered networks, a
few key results and insights are however available.
  To gain insights into the nature of the optimal solution and to construct
low-complexity schemes to characterize the optimal rate for general wireless
relay networks, we need (1) network topologies that are regular enough to be
amenable for analysis, yet general enough to capture essential characteristics
of general wireless relay networks, and (2) schemes to approximate the
objective function in closed-form without significantly compromising the
performance. Towards these two goals, this work proposes (1) nonlinear chain
networks, and (2) two approximation schemes. We show that their combination
allows us to tightly characterize the optimal ANC rate with low computational
complexity for a much larger class of general wireless relay networks than
possible with existing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8747</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8747</id><created>2014-10-31</created><authors><author><keyname>Camelo</keyname><forenames>Pedro</forenames></author><author><keyname>Moura</keyname><forenames>Joao</forenames></author><author><keyname>Krippahl</keyname><forenames>Ludwig</forenames></author></authors><title>CONDENSER: A Graph-Based Approachfor Detecting Botnets</title><categories>cs.CR</categories><comments>BotConf 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Botnets represent a global problem and are responsible for causing large
financial and operational damage to their victims. They are implemented with
evasion in mind, and aim at hiding their architecture and authors, making them
difficult to detect in general. These kinds of networks are mainly used for
identity theft, virtual extortion, spam campaigns and malware dissemination.
Botnets have a great potential in warfare and terrorist activities, making it
of utmost importance to take action against. We present CONDENSER, a method for
identifying data generated by botnet activity. We start by selecting
appropriate the features from several data feeds, namely DNS non-existent
domain responses and live communication packages directed to command and
control servers that we previously sinkholed. By using machine learning
algorithms and a graph based representation of data, then allows one to
identify botnet activity, helps identifying anomalous traffic, quickly detect
new botnets and improve activities of tracking known botnets. Our main
contributions are threefold: first, the use of a machine learning classifier
for classifying domain names as being generated by domain generation algorithms
(DGA); second, a clustering algorithm using the set of selected features that
groups network communication with similar patterns; third, a graph based
knowledge representation framework where we store processed data, allowing us
to perform queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8749</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8749</id><created>2014-10-30</created><authors><author><keyname>Li</keyname><forenames>Jiwei</forenames></author><author><keyname>Wang</keyname><forenames>Xun</forenames></author><author><keyname>Hovy</keyname><forenames>Eduard</forenames></author></authors><title>What a Nasty day: Exploring Mood-Weather Relationship from Twitter</title><categories>cs.SI cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While it has long been believed in psychology that weather somehow influences
human's mood, the debates have been going on for decades about how they are
correlated. In this paper, we try to study this long-lasting topic by
harnessing a new source of data compared from traditional psychological
researches: Twitter. We analyze 2 years' twitter data collected by twitter API
which amounts to $10\%$ of all postings and try to reveal the correlations
between multiple dimensional structure of human mood with meteorological
effects. Some of our findings confirm existing hypotheses, while others
contradict them. We are hopeful that our approach, along with the new data
source, can shed on the long-going debates on weather-mood correlation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8750</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8750</id><created>2014-10-31</created><authors><author><keyname>Awasthi</keyname><forenames>Pranjal</forenames></author><author><keyname>Blum</keyname><forenames>Avrim</forenames></author><author><keyname>Sheffet</keyname><forenames>Or</forenames></author><author><keyname>Vijayaraghavan</keyname><forenames>Aravindan</forenames></author></authors><title>Learning Mixtures of Ranking Models</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work concerns learning probabilistic models for ranking data in a
heterogeneous population. The specific problem we study is learning the
parameters of a Mallows Mixture Model. Despite being widely studied, current
heuristics for this problem do not have theoretical guarantees and can get
stuck in bad local optima. We present the first polynomial time algorithm which
provably learns the parameters of a mixture of two Mallows models. A key
component of our algorithm is a novel use of tensor decomposition techniques to
learn the top-k prefix in both the rankings. Before this work, even the
question of identifiability in the case of a mixture of two Mallows models was
unresolved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8753</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8753</id><created>2014-10-31</created><updated>2015-02-27</updated><authors><author><keyname>Yakimenka</keyname><forenames>Yauhen</forenames></author><author><keyname>Skachek</keyname><forenames>Vitaly</forenames></author></authors><title>Refined Upper Bounds on Stopping Redundancy of Binary Linear Codes</title><categories>cs.IT math.IT</categories><comments>5 pages; ITW 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $l$-th stopping redundancy $\rho_l(\mathcal C)$ of the binary $[n, k, d]$
code $\mathcal C$, $1 \le l \le d$, is defined as the minimum number of rows in
the parity-check matrix of $\mathcal C$, such that the smallest stopping set is
of size at least $l$. The stopping redundancy $\rho(\mathcal C)$ is defined as
$\rho_d(\mathcal C)$. In this work, we improve on the probabilistic analysis of
stopping redundancy, proposed by Han, Siegel and Vardy, which yields the best
bounds known today. In our approach, we judiciously select the first few rows
in the parity-check matrix, and then continue with the probabilistic method. By
using similar techniques, we improve also on the best known bounds on
$\rho_l(\mathcal C)$, for $1 \le l \le d$. Our approach is compared to the
existing methods by numerical computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8772</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8772</id><created>2014-10-30</created><authors><author><keyname>Varghese</keyname><forenames>Anish</forenames></author><author><keyname>Edwards</keyname><forenames>Bob</forenames></author><author><keyname>Mitra</keyname><forenames>Gaurav</forenames></author><author><keyname>Rendell</keyname><forenames>Alistair P.</forenames></author></authors><title>Programming the Adapteva Epiphany 64-core Network-on-chip Coprocessor</title><categories>cs.AR cs.DC cs.MS</categories><comments>14 pages, submitted to IJHPCA Journal special edition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the construction of exascale computing systems energy efficiency and power
consumption are two of the major challenges. Low-power high performance
embedded systems are of increasing interest as building blocks for large scale
high- performance systems. However, extracting maximum performance out of such
systems presents many challenges. Various aspects from the hardware
architecture to the programming models used need to be explored. The Epiphany
architecture integrates low-power RISC cores on a 2D mesh network and promises
up to 70 GFLOPS/Watt of processing efficiency. However, with just 32 KB of
memory per eCore for storing both data and code, and only low level inter-core
communication support, programming the Epiphany system presents several
challenges. In this paper we evaluate the performance of the Epiphany system
for a variety of basic compute and communication operations. Guided by this
data we explore strategies for implementing scientific applications on memory
constrained low-powered devices such as the Epiphany. With future systems
expected to house thousands of cores in a single chip, the merits of such
architectures as a path to exascale is compared to other competing systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8774</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8774</id><created>2014-10-31</created><authors><author><keyname>Dabrowski</keyname><forenames>Konrad K.</forenames></author><author><keyname>de Werra</keyname><forenames>Dominique</forenames></author><author><keyname>Lozin</keyname><forenames>Vadim V.</forenames></author><author><keyname>Zamaraev</keyname><forenames>Viktor</forenames></author></authors><title>Combinatorics and algorithms for augmenting graphs</title><categories>cs.DM cs.DS math.CO</categories><comments>13 pages, 5 figures</comments><msc-class>68R10 (Primary), 05C85, 05C75, 05C69, 05C55 (Secondary)</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of augmenting graphs generalizes Berge's idea of augmenting
chains, which was used by Edmonds in his celebrated solution of the maximum
matching problem. This problem is a special case of the more general maximum
independent set (MIS) problem. Recently, the augmenting graph approach has been
successfully applied to solve MIS in various other special cases. However, our
knowledge of augmenting graphs is still very limited, and we do not even know
what the minimal infinite classes of augmenting graphs are. In the present
paper, we find an answer to this question and apply it to extend the area of
polynomial-time solvability of the maximum independent set problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8776</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8776</id><created>2014-10-31</created><authors><author><keyname>Gensollen</keyname><forenames>Nicolas</forenames></author><author><keyname>Gauthier</keyname><forenames>Vincent</forenames></author><author><keyname>Marot</keyname><forenames>Michel</forenames></author><author><keyname>Becker</keyname><forenames>Monique</forenames></author></authors><title>Coalition Formation Algorithm of Prosumers in a Smart Grid Environment</title><categories>cs.GT</categories><comments>6 pages, 4 figures, 1 table. submited to ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a smart grid environment, we study coalition formation of prosumers that
aim at entering the energy market. It is paramount for the grid operation that
the energy producers are able to sustain the grid demand in terms of stability
and minimum production requirement. We design an algorithm that seeks to form
coalitions that will meet both of these requirements: a minimum energy level
for the coalitions and a steady production level which leads to finding
uncorrelated sources of energy to form a coalition. We propose an algorithm
that uses graph tools such as correlation graphs or clique percolation to form
coalitions that meet such complex constraints. We validate the algorithm
against a random procedure and show that, it not only performs better in term
of social welfare for the power grid, but also that it is more robust against
unforeseen production variations due to changing weather conditions for
instance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8783</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8783</id><created>2014-10-31</created><authors><author><keyname>Khoufi</keyname><forenames>Nabil</forenames></author><author><keyname>Aloulou</keyname><forenames>Chafik</forenames></author><author><keyname>Belguith</keyname><forenames>Lamia Hadrich</forenames></author></authors><title>Supervised learning model for parsing Arabic language</title><categories>cs.CL cs.LG</categories><comments>8 pages,1 figure, Proceedings of the 10th International Workshop on
  Natural Language Processing and Cognitive Science (NLPCS 2013),2013,
  Marseille, France, pp129-136</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parsing the Arabic language is a difficult task given the specificities of
this language and given the scarcity of digital resources (grammars and
annotated corpora). In this paper, we suggest a method for Arabic parsing based
on supervised machine learning. We used the SVMs algorithm to select the
syntactic labels of the sentence. Furthermore, we evaluated our parser
following the cross validation method by using the Penn Arabic Treebank. The
obtained results are very encouraging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8794</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8794</id><created>2014-10-31</created><authors><author><keyname>Shah</keyname><forenames>Shahid Mehraj</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>Achieving Shannon Capacity Region as Secrecy Rate Region in a Multiple
  Access Wiretap Channel</title><categories>cs.IT cs.CR math.IT</categories><comments>6 pages, 1 figure, Submitted to IEEE WCNC 2015, New Orleans, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a two user multiple-access channel with an eavesdropper at the
receiving end. We use previously transmitted messages as a key in the next slot
till we achieve the capacity region of the usual Multiple Access Channel (MAC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8797</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8797</id><created>2014-10-31</created><updated>2015-02-09</updated><authors><author><keyname>Azimi-Abarghouyi</keyname><forenames>Seyed Mohammad</forenames></author><author><keyname>Nasiri-Kenari</keyname><forenames>Masoumeh</forenames></author><author><keyname>Maham</keyname><forenames>Behrouz</forenames></author></authors><title>Integer Forcing-and-Forward Transceiver Design for MIMO Multi-Pair
  Two-Way Relaying</title><categories>cs.IT math.IT</categories><comments>30 pages, 7 figures, Submitted to a IEEE journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new transmission scheme, named as Integer
Forcing-and-Forward (IFF), for communications among multi-pair multiple-antenna
users in which each pair exchanges their messages with the help of a single
multi antennas relay in the multiple-access and broadcast phases. The proposed
scheme utilizes Integer Forcing Linear Receiver (IFLR) at relay, which uses
equations, i.e., linear integer-combinations of messages, to harness the
intra-pair interference. Accordingly, we propose the design of mean squared
error (MSE) based transceiver, including precoder and projection matrices for
the relay and users, assuming that the perfect channel state information (CSI)
is available. In this regards, in the multiple-access phase, we introduce two
new MSE criteria for the related precoding and filter designs, i.e., the sum of
the equations MSE (Sum-Equation MSE) and the maximum of the equations MSE
(Max-Equation MSE), to exploit the equations in the relay. In addition, the
convergence of the proposed criteria is proven as well. Moreover, in the
broadcast phase, we use the two traditional MSE criteria, i.e. the sum of the
users' mean squred errors (Sum MSE) and the maximum of the users' mean squared
errors (Max MSE), to design the related precoding and filters for recovering
relay's equations by the users. Then, we consider a more practical scenario
with imperfect CSI. For this case, IFLR receiver is modified, and another
transceiver design is proposed, which take into account the effect of channels
estimation error. We evaluate the performance of our proposed strategy and
compare the results with the conventional amplify-and-forward (AF) and
denoise-and-forward (DF) strategies for the same scenario. The results indicate
the substantial superiority of the proposed strategy in terms of the outage
probability and the sum rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8802</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8802</id><created>2014-10-31</created><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author></authors><title>Low Rank Matrix Approximation in Linear Time</title><categories>cs.CG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  $\newcommand{\MatA}{\mathcal{M}}$ $\newcommand{\eps}{\varepsilon}$
$\newcommand{\NSize}{\mathsf{N}{}}$ $\newcommand{\MatB}{\mathcal{B}}$
$\newcommand{\Fnorm}[1]{\left\| {#1} \right\|_F}$
$\newcommand{\PrcOpt}[2]{\mu_{\mathrm{opt}}\pth{#1, #2}}$
$\newcommand{\pth}[1]{\left(#1\right)}$
  Given a matrix $\MatA$ with $n$ rows and $d$ columns, and fixed $k$ and
$\eps$, we present an algorithm that in linear time (i.e., $O(\NSize )$)
computes a $k$-rank matrix $\MatB$ with approximation error $\Fnorm{\MatA -
\MatB}^2 \leq (1+\eps) \PrcOpt{\MatA}{k}$, where $\NSize = n d$ is the input
size, and $\PrcOpt{\MatA}{k}$ is the minimum error of a $k$-rank approximation
to $\MatA$.
  This algorithm succeeds with constant probability, and to our knowledge it is
the first linear-time algorithm to achieve multiplicative approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8805</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8805</id><created>2014-10-31</created><updated>2014-11-05</updated><authors><author><keyname>Balmahoon</keyname><forenames>R.</forenames></author><author><keyname>Vinck</keyname><forenames>H.</forenames></author><author><keyname>Cheng</keyname><forenames>L.</forenames></author></authors><title>Correlated Source Coded Sequences with Compromised Channel and Source
  Symbols using Shannon's Cipher System</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1401.6264</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correlated sources are present in communication systems where protocols
ensure that there is some predetermined information for sources to transmit.
Here, two correlated sources across a channel with eavesdroppers are
investigated, and conditions for perfect secrecy when some channel information
and some source data symbols (the predetermined information) have been
wiretapped are determined. The adversary in this situation has access to more
information than if a link is wiretapped only and can thus determine more about
a particular source. This scenario caters for an application where the
eavesdropper has access to some preexisting information. We provide bounds for
the channel and key rates for this scenario. Further, we provide a method to
reduce the key lengths required for perfect secrecy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8808</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8808</id><created>2014-10-29</created><authors><author><keyname>Pareti</keyname><forenames>Paolo</forenames></author><author><keyname>Klein</keyname><forenames>Ewan</forenames></author><author><keyname>Barker</keyname><forenames>Adam</forenames></author></authors><title>A Semantic Web of Know-How: Linked Data for Community-Centric Tasks</title><categories>cs.AI cs.CL</categories><comments>6th International Workshop on Web Intelligence &amp; Communities (WIC14),
  Proceedings of the companion publication of the 23rd International Conference
  on World Wide Web (WWW 2014)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper proposes a novel framework for representing community know-how on
the Semantic Web. Procedural knowledge generated by web communities typically
takes the form of natural language instructions or videos and is largely
unstructured. The absence of semantic structure impedes the deployment of many
useful applications, in particular the ability to discover and integrate
know-how automatically. We discuss the characteristics of community know-how
and argue that existing knowledge representation frameworks fail to represent
it adequately. We present a novel framework for representing the semantic
structure of community know-how and demonstrate the feasibility of our approach
by providing a concrete implementation which includes a method for
automatically acquiring procedural knowledge for real-world tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8816</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8816</id><created>2014-10-31</created><updated>2016-01-19</updated><authors><author><keyname>Braun</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Pokutta</keyname><forenames>Sebastian</forenames></author><author><keyname>Zink</keyname><forenames>Daniel</forenames></author></authors><title>Affine reductions for LPs and SDPs</title><categories>cs.CC</categories><comments>Minor updates, correction of typos</comments><msc-class>68Q17, 90C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a reduction mechanism for LP and SDP formulations that degrades
approximation factors in a controlled fashion. Our reduction mechanism is a
minor restriction of classical reductions establishing inapproximability in the
context of PCP theorems. As a consequence we establish strong linear
programming inapproximability (for LPs with a polynomial number of constraints)
for many problems. In particular we obtain a $3/2-\varepsilon$
inapproximability for VertexCover answering an open question in
[arXiv:1309.0563] and we answer a weak version of our sparse graph conjecture
posed in [arXiv:1311.4001] showing an inapproximability factor of
$1/2+\varepsilon$ for bounded degree IndependentSet. In the case of SDPs, we
obtain inapproximability results for these problems relative to the
SDP-inapproximability of MaxCUT. Moreover, using our reduction framework we are
able to reproduce various results for CSPs from [arXiv:1309.0563] via simple
reductions from Max-2-XOR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8819</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8819</id><created>2014-10-31</created><updated>2015-06-23</updated><authors><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author><author><keyname>Sorge</keyname><forenames>Manuel</forenames></author></authors><title>On Kernelization and Approximation for the Vector Connectivity Problem</title><categories>cs.CC cs.DM</categories><comments>Non-constructive Kernelization argument, improved technical details
  of signatures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Vector Connectivity problem we are given an undirected graph
$G=(V,E)$, a demand function $\phi\colon V\to\{0,\ldots,d\}$, and an integer
$k$. The question is whether there exists a set $S$ of at most $k$ vertices
such that every vertex $v\in V\setminus S$ has at least $\phi(v)$
vertex-disjoint paths to $S$; this abstractly captures questions about placing
servers or warehouses relative to demands. The problem is \NP-hard already for
instances with $d=4$ (Cicalese et al., arXiv '14), admits a log-factor
approximation (Boros et al., Networks '14), and is fixed-parameter tractable in
terms of~$k$ (Lokshtanov, unpublished '14). We prove several results regarding
kernelization and approximation for Vector Connectivity and the variant Vector
$d$-Connectivity where the upper bound $d$ on demands is a fixed constant. For
Vector $d$-Connectivity we give a factor $d$-approximation algorithm and
construct a vertex-linear kernelization, i.e., an efficient reduction to an
equivalent instance with $f(d)k=O(k)$ vertices. For Vector Connectivity we have
a factor $\text{opt}$-approximation and we can show that it has no
kernelization to size polynomial in $k$ or even $k+d$ unless
$\mathsf{NP\subseteq coNP/poly}$, making $f(d)\operatorname{poly}(k)$ optimal
for Vector $d$-Connectivity. Finally, we provide a write-up for fixed-parameter
tractability of Vector Connectivity($k$) by giving an alternative FPT algorithm
based on matroid intersection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8837</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8837</id><created>2014-10-31</created><updated>2015-11-03</updated><authors><author><keyname>Kiah</keyname><forenames>Han Mao</forenames></author><author><keyname>Puleo</keyname><forenames>Gregory J.</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>Codes for DNA Storage Channels</title><categories>cs.IT math.CO math.IT</categories><comments>32 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of assembling a sequence based on a collection of its
substrings observed through a noisy channel. The mathematical basis of the
problem is the construction and design of sequences that may be discriminated
based on a collection of their substrings observed through a noisy channel. We
explain the connection between the sequence reconstruction problem and the
problem of DNA synthesis and sequencing, and introduce the notion of a DNA
storage channel. We analyze the number of sequence equivalence classes under
the channel mapping and propose new asymmetric coding techniques to combat the
effects of synthesis and sequencing noise. In our analysis, we make use of
restricted de Bruijn graphs and Ehrhart theory for rational polytopes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8844</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8844</id><created>2014-10-31</created><updated>2014-11-07</updated><authors><author><keyname>Madden</keyname><forenames>Paul</forenames></author><author><keyname>Valente</keyname><forenames>Eduardo G.</forenames><suffix>Jr</suffix></author></authors><title>DDTS: A Practical System Testing Framework for Scientific Software</title><categories>cs.SE</categories><comments>Fixed bad formatting on last page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many scientific-software projects test their codes inadequately, or not at
all. Despite its well-known benefits, adopting routine testing is often not
easy. Development teams may have doubts about establishing effective test
procedures, writing test software, or handling the ever-growing complexity of
test cases. They may need to run (and test) on restrictive HPC platforms. They
almost certainly face time and budget pressures that can keep testing
languishing near the bottom of their to-do lists. This paper presents DDTS, a
framework for building test suite applications, designed to fit
scientific-software projects' requirements. DDTS aims to simplify introduction
of rigorous testing, and to ease growing pains as needs mature. It decomposes
the testing problem into practical, intuitive phases; makes configuration and
extension easy; is portable and suitable to HPC platforms; and exploits
parallelism. DDTS is currently used for automated regression and developer
pre-commit testing for several scientific-software projects with disparate
testing requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8852</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8852</id><created>2014-10-31</created><updated>2015-01-18</updated><authors><author><keyname>Kochems</keyname><forenames>Jonathan</forenames></author><author><keyname>Ong</keyname><forenames>C. -H. Luke</forenames></author></authors><title>Decidable Models of Recursive Asynchronous Concurrency</title><categories>cs.LO cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asynchronously communicating pushdown systems (ACPS) that satisfy the
empty-stack constraint (a pushdown process may receive only when its stack is
empty) are a popular decidable model for recursive programs with asynchronous
atomic procedure calls. We study a relaxation of the empty-stack constraint for
ACPS that permits concurrency and communication actions at any stack height,
called the shaped stack constraint, thus enabling a larger class of concurrent
programs to be modelled. We establish a close connection between ACPS with
shaped stacks and a novel extension of Petri nets: Nets with Nested Coloured
Tokens (NNCTs). Tokens in NNCTs are of two types: simple and complex. Complex
tokens carry an arbitrary number of coloured tokens. The rules of NNCT can
synchronise complex and simple tokens, inject coloured tokens into a complex
token, and eject all tokens of a specified set of colours to predefined places.
We show that the coverability problem for NNCTs is Tower-complete. To our
knowledge, NNCT is the first extension of Petri nets, in the class of nets with
an infinite set of token types, that has primitive recursive coverability. This
result implies Tower-completeness of coverability for ACPS with shaped stacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8853</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8853</id><created>2014-10-31</created><updated>2015-02-09</updated><authors><author><keyname>Schneeloch</keyname><forenames>James</forenames></author><author><keyname>Knarr</keyname><forenames>Samuel H.</forenames></author><author><keyname>Howland</keyname><forenames>Gregory A.</forenames></author><author><keyname>Howell</keyname><forenames>John C.</forenames></author></authors><title>Demonstrating Continuous Variable EPR Steering in spite of Finite
  Experimental Capabilities using Fano Steering Bounds</title><categories>quant-ph cs.IT math.IT</categories><comments>7 pages, 2 figures</comments><journal-ref>J. Opt. Soc. Am. B. Vol. 32, No. 4, April 2015</journal-ref><doi>10.1364/JOSAB.32.0000A8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how one can demonstrate continuous-variable Einstein-Podolsky-Rosen
(EPR) steering without needing to characterize entire measurement probability
distributions. To do this, we develop a modified Fano inequality useful for
discrete measurements of continuous variables, and use it to bound the
conditional uncertainties in continuous-variable entropic EPR-steering
inequalities. With these bounds, we show how one can hedge against experimental
limitations including a finite detector size, dead space between pixels, and
any such factors that impose an incomplete sampling of the true measurement
probability distribution. Furthermore, we use experimental data from the
position and momentum statistics of entangled photon pairs in parametric
downconversion to show that this method is sufficiently sensitive for practical
use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8864</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8864</id><created>2014-10-31</created><authors><author><keyname>Park</keyname><forenames>Dohyung</forenames></author><author><keyname>Caramanis</keyname><forenames>Constantine</forenames></author><author><keyname>Sanghavi</keyname><forenames>Sujay</forenames></author></authors><title>Greedy Subspace Clustering</title><categories>stat.ML cs.IT cs.LG math.IT</categories><comments>To appear in NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of subspace clustering: given points that lie on or
near the union of many low-dimensional linear subspaces, recover the subspaces.
To this end, one first identifies sets of points close to the same subspace and
uses the sets to estimate the subspaces. As the geometric structure of the
clusters (linear subspaces) forbids proper performance of general distance
based approaches such as K-means, many model-specific methods have been
proposed. In this paper, we provide new simple and efficient algorithms for
this problem. Our statistical analysis shows that the algorithms are guaranteed
exact (perfect) clustering performance under certain conditions on the number
of points and the affinity between subspaces. These conditions are weaker than
those considered in the standard statistical literature. Experimental results
on synthetic data generated from the standard unions of subspaces model
demonstrate our theory. We also show that our algorithm performs competitively
against state-of-the-art algorithms on real-world applications such as motion
segmentation and face clustering, with much simpler implementation and lower
computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8865</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8865</id><created>2014-10-31</created><updated>2015-05-21</updated><authors><author><keyname>Kleineberg</keyname><forenames>Kaj-Kolja</forenames></author><author><keyname>Bogu&#xf1;&#xe1;</keyname><forenames>Mari&#xe1;n</forenames></author></authors><title>Digital Ecology: Coexistence and Domination among Interacting Networks</title><categories>physics.soc-ph cs.SI nlin.AO physics.data-an</categories><journal-ref>Scientific Reports 5, 10268, 2015</journal-ref><doi>10.1038/srep10268</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The overwhelming success of the web 2.0, with online social networks as key
actors, has induced a paradigm shift in the nature of human interactions. The
user-driven character of these services for the first time has allowed
researchers to quantify large-scale social patterns. However, the mechanisms
that determine the fate of networks at a system level are still poorly
understood. For instance, the simultaneous existence of numerous digital
services naturally raises the question under which conditions these services
can coexist. In analogy to population dynamics, the digital world is forming a
complex ecosystem of interacting networks whose fitnesses depend on their
ability to attract and maintain users' attention, which constitutes a limited
resource. In this paper, we introduce an ecological theory of the digital world
which exhibits a stable coexistence of several networks as well as the
domination of a single one, in contrast to the principle of competitive
exclusion. Interestingly, our model also predicts that the most probable
outcome is the coexistence of a moderate number of services, in agreement with
empirical observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1410.8869</identifier>
 <datestamp>2014-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1410.8869</id><created>2014-10-31</created><authors><author><keyname>Latif</keyname><forenames>Mohammad Ayub</forenames></author><author><keyname>Naveed</keyname><forenames>Muhammad</forenames></author><author><keyname>Zaidi</keyname><forenames>Faraz</forenames></author></authors><title>Resilience of Social Networks Under Different Attack Strategies</title><categories>cs.SI physics.soc-ph</categories><journal-ref>Social Informatics. Springer International Publishing, 2013. 16-29</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have seen the world become a closely connected society with the
emergence of different types of social networks. Online social networks have
provided a way to bridge long distances and establish numerous communication
channels which were not possible earlier. These networks exhibit interesting
behavior under intentional attacks and random failures where different
structural properties influence the resilience in different ways.
  In this paper, we perform two sets of experiments and draw conclusions from
the results pertaining to the resilience of social networks. The first
experiment performs a comparative analysis of four different classes of
networks namely small world networks, scale free networks, small world-scale
free networks and random networks with four semantically different social
networks under different attack strategies. The second experiment compares the
resilience of these semantically different social networks under different
attack strategies. Empirical analysis reveals interesting behavior of different
classes of networks with different attack strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0001</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0001</id><created>2014-10-30</created><authors><author><keyname>Chandra</keyname><forenames>Abhijit</forenames></author><author><keyname>Kar</keyname><forenames>Oliva</forenames></author><author><keyname>Wu</keyname><forenames>Kuan-Chuen</forenames></author><author><keyname>Hall</keyname><forenames>Michelle</forenames></author><author><keyname>Gillette</keyname><forenames>Jason</forenames></author></authors><title>Prognosis of Anterior Cruciate Ligament (ACL) Reconstruction: A Data
  Driven Approach</title><categories>cs.CE</categories><comments>arXiv admin note: substantial text overlap with arXiv:1410.8616</comments><doi>10.1098/rspa.2014.0526</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Individuals who suffer anterior cruciate ligament (ACL) injury are at higher
risk of developing knee osteoarthritis (OA) and almost 50% display symptoms 10
to 20 years post injury. Anterior cruciate ligament reconstruction (ACLR) often
does not protect against knee OA development. Accordingly, a multiscale
formulation for Data Driven Prognosis (DDP) of post ACLR is developed. Unlike
traditional predictive strategies that require controlled off-line measurements
or training for determination of constitutive parameters to derive the
transitional statistics, the proposed DDP algorithm relies solely on in situ
measurements. The proposed DDP scheme is capable of predicting onset of
instabilities. Since the need for off line testing (or training) is obviated,
it can be easily implemented for ACLR, where such controlled a priori testing
is almost impossible to conduct. The DDP algorithm facilitates hierarchical
handling of the large data set, and can assess the state of recovery in post
ACLR conditions based on data collected from stair ascent and descent exercises
of subjects. The DDP algorithm identifies inefficient knee varus motion and
knee rotation as primary difficulties experienced by some of the post ACLR
population. In such cases, levels of energy dissipation rate at the knee, and
its fluctuation may be used as measures for assessing progress after ACL
reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0007</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0007</id><created>2014-10-31</created><authors><author><keyname>Miller</keyname><forenames>John E.</forenames></author><author><keyname>Bloodgood</keyname><forenames>Michael</forenames></author><author><keyname>Torii</keyname><forenames>Manabu</forenames></author><author><keyname>Vijay-Shanker</keyname><forenames>K.</forenames></author></authors><title>Rapid Adaptation of POS Tagging for Domain Specific Uses</title><categories>cs.CL cs.LG stat.ML</categories><comments>2 pages, 2 tables; appeared in Proceedings of the HLT-NAACL BioNLP
  Workshop on Linking Natural Language and Biology, June 2006</comments><acm-class>I.2.7; I.2.6; I.5.1; I.5.4</acm-class><journal-ref>In Proceedings of the HLT-NAACL BioNLP Workshop on Linking Natural
  Language and Biology, pages 118-119, New York, New York, June 2006.
  Association for Computational Linguistics</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Part-of-speech (POS) tagging is a fundamental component for performing
natural language tasks such as parsing, information extraction, and question
answering. When POS taggers are trained in one domain and applied in
significantly different domains, their performance can degrade dramatically. We
present a methodology for rapid adaptation of POS taggers to new domains. Our
technique is unsupervised in that a manually annotated corpus for the new
domain is not necessary. We use suffix information gathered from large amounts
of raw text as well as orthographic information to increase the lexical
coverage. We present an experiment in the Biological domain where our POS
tagger achieves results comparable to POS taggers specifically trained to this
domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0022</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0022</id><created>2014-10-31</created><authors><author><keyname>Panaganti</keyname><forenames>Varun</forenames></author></authors><title>Generalized Adaptive Dictionary Learning via Domain Shift Minimization</title><categories>cs.CV</categories><comments>6 pages, 2 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual data driven dictionaries have been successfully employed for various
object recognition and classification tasks. However, the task becomes more
challenging if the training and test data are from contrasting domains. In this
paper, we propose a novel and generalized approach towards learning an adaptive
and common dictionary for multiple domains. Precisely, we project the data from
different domains onto a low dimensional space while preserving the intrinsic
structure of data from each domain. We also minimize the domain-shift among the
data from each pair of domains. Simultaneously, we learn a common adaptive
dictionary. Our algorithm can also be modified to learn class-specific
dictionaries which can be used for classification. We additionally propose a
discriminative manifold regularization which imposes the intrinsic structure of
class specific features onto the sparse coefficients. Experiments on image
classification show that our approach fares better compared to the existing
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0023</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0023</id><created>2014-10-31</created><authors><author><keyname>Le</keyname><forenames>Ya</forenames></author><author><keyname>Bax</keyname><forenames>Eric</forenames></author><author><keyname>Barbieri</keyname><forenames>Nicola</forenames></author><author><keyname>Soriano</keyname><forenames>David Garcia</forenames></author><author><keyname>Mehta</keyname><forenames>Jitesh</forenames></author><author><keyname>Li</keyname><forenames>James</forenames></author></authors><title>Validation of Network Reconciliation</title><categories>cs.LG stat.ML</categories><comments>Short version will be submitted to NIPS workshop on networks, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network reconciliation is the problem of identifying nodes in separate
networks that represent the same entity, for example matching nodes across
social networks that correspond to the same user. We introduce a technique to
compute probably approximately correct (PAC) bounds on precision and recall for
network reconciliation algorithms. The bounds require some verified matches,
but those matches may be used to develop the algorithms. The bounds do not
require knowledge of the network generation process, and they can supply
confidence levels for individual matches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0024</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0024</id><created>2014-10-30</created><authors><author><keyname>Pham</keyname><forenames>Vu</forenames></author><author><keyname>Ghaoui</keyname><forenames>Laurent El</forenames></author><author><keyname>Fernandez</keyname><forenames>Arturo</forenames></author></authors><title>Robust sketching for multiple square-root LASSO problems</title><categories>math.OC cs.LG cs.SY stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many learning tasks, such as cross-validation, parameter search, or
leave-one-out analysis, involve multiple instances of similar problems, each
instance sharing a large part of learning data with the others. We introduce a
robust framework for solving multiple square-root LASSO problems, based on a
sketch of the learning data that uses low-rank approximations. Our approach
allows a dramatic reduction in computational effort, in effect reducing the
number of observations from $m$ (the number of observations to start with) to
$k$ (the number of singular values retained in the low-rank model), while not
sacrificing---sometimes even improving---the statistical performance.
Theoretical analysis, as well as numerical experiments on both synthetic and
real data, illustrate the efficiency of the method in large scale applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0028</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0028</id><created>2014-10-31</created><authors><author><keyname>Soprunov</keyname><forenames>Ivan</forenames></author></authors><title>Lattice polytopes in coding theory</title><categories>cs.IT math.CO math.IT</categories><comments>11 pages, 3 figures</comments><journal-ref>J. Algebra Comb. Discrete Appl., 2(2) pp.85-94 (2015)</journal-ref><doi>10.13069/jacodesmath.75353</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we discuss combinatorial questions about lattice polytopes
motivated by recent results on minimum distance estimation for toric codes. We
also prove a new inductive bound for the minimum distance of generalized toric
codes. As an application, we give new formulas for the minimum distance of
generalized toric codes for special lattice point configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0048</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0048</id><created>2014-10-31</created><authors><author><keyname>Meira</keyname><forenames>Luis A. A.</forenames></author><author><keyname>de Lima</keyname><forenames>Rog&#xe9;rio H. B.</forenames></author></authors><title>Fusion Tree Sorting</title><categories>cs.DS</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sorting problem is one of the most relevant problems in computer science.
Within the scope of modern computer science it has been studied for more than
70 years. In spite of these facts, new sorting algorithms have been developed
in recent years. Among several types of sorting algorithms, some are quicker;
others are more economic in relation to space, whereas others insert a few
restrictions in relation to data input. This paper is aimed at explaining the
fusion tree data structure, which is responsible for the first sorting
algorithm with complexity time smaller than nlgn. The nlgn time complexity has
led to some confusion and generated the wrong belief in part of the community
of being the minimum possible for this type of problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0052</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0052</id><created>2014-10-31</created><authors><author><keyname>Sallaberry</keyname><forenames>Arnaud</forenames></author><author><keyname>Fu</keyname><forenames>Yang-Chih</forenames></author><author><keyname>Ho</keyname><forenames>Hwai-Chung</forenames></author><author><keyname>Ma</keyname><forenames>Kwan-Liu</forenames></author></authors><title>ContactTrees: A Technique for Studying Personal Network Data</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network visualization allows a quick glance at how nodes (or actors) are
connected by edges (or ties). A conventional network diagram of &quot;contact tree&quot;
maps out a root and branches that represent the structure of nodes and edges,
often without further specifying leaves or fruits that would have grown from
small branches. By furnishing such a network structure with leaves and fruits,
we reveal details about &quot;contacts&quot; in our ContactTrees that underline ties and
relationships. Our elegant design employs a bottom-up approach that resembles a
recent attempt to understand subjective well-being by means of a series of
emotions. Such a bottom-up approach to social-network studies decomposes each
tie into a series of interactions or contacts, which help deepen our
understanding of the complexity embedded in a network structure. Unlike
previous network visualizations, ContactTrees can highlight how relationships
form and change based upon interactions among actors, and how relationships and
networks vary by contact attributes. Based on a botanical tree metaphor, the
design is easy to construct and the resulting tree-like visualization can
display many properties at both tie and contact levels, a key ingredient
missing from conventional techniques of network visualization. We first
demonstrate ContactTrees using a dataset consisting of three waves of 3-month
contact diaries over the 2004-2012 period, then compare ContactTrees with
alternative tools and discuss how this tool can be applied to other types of
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0054</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0054</id><created>2014-10-31</created><authors><author><keyname>Diamant</keyname><forenames>Emanuel</forenames></author></authors><title>Cognitive image processing: the time is right to recognize that the
  world does not rest more on turtles and elephants</title><categories>cs.CY q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional image processing is a field of science and technology developed
to facilitate human-centered image management. But today, when huge volumes of
visual data inundate our surroundings (due to the explosive growth of
image-capturing devices, proliferation of Internet communication means and
video sharing services over the World Wide Web), human-centered handling of
Big-data flows is impossible anymore. Therefore, it has to be replaced with a
machine (computer) supported counterpart. Of course, such an artificial
counterpart must be equipped with some cognitive abilities, usually
characteristic for a human being. Indeed, in the past decade, a new computer
design trend - Cognitive Computer development - is become visible. Cognitive
image processing definitely will be one of its main duties. It must be
specially mentioned that this trend is a particular case of a much more general
movement - the transition from a &quot;computational data-processing paradigm&quot; to a
&quot;cognitive information-processing paradigm&quot;, which affects today many fields of
science, technology, and engineering. This transition is a blessed novelty, but
its success is hampered by the lack of a clear delimitation between the notion
of data and the notion of information. Elaborating the case of cognitive image
processing, the paper intends to clarify these important research issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0059</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0059</id><created>2014-10-31</created><updated>2014-11-04</updated><authors><author><keyname>Nikolova</keyname><forenames>E.</forenames></author><author><keyname>Stier-Moses</keyname><forenames>N.</forenames></author></authors><title>The Burden of Risk Aversion in Mean-Risk Selfish Routing</title><categories>cs.GT</categories><comments>21 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considering congestion games with uncertain delays, we compute the
inefficiency introduced in network routing by risk-averse agents. At
equilibrium, agents may select paths that do not minimize the expected latency
so as to obtain lower variability. A social planner, who is likely to be more
risk neutral than agents because it operates at a longer time-scale, quantifies
social cost with the total expected delay along routes. From that perspective,
agents may make suboptimal decisions that degrade long-term quality. We define
the {\em price of risk aversion} (PRA) as the worst-case ratio of the social
cost at a risk-averse Wardrop equilibrium to that where agents are
risk-neutral. For networks with general delay functions and a single
source-sink pair, we show that the PRA depends linearly on the agents' risk
tolerance and on the degree of variability present in the network. In contrast
to the {\em price of anarchy}, in general the PRA increases when the network
gets larger but it does not depend on the shape of the delay functions. To get
this result we rely on a combinatorial proof that employs alternating paths
that are reminiscent of those used in max-flow algorithms. For {\em
series-parallel} (SP) graphs, the PRA becomes independent of the network
topology and its size. As a result of independent interest, we prove that for
SP networks with deterministic delays, Wardrop equilibria {\em maximize} the
shortest-path objective among all feasible flows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0060</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0060</id><created>2014-10-31</created><authors><author><keyname>Cuff</keyname><forenames>Paul</forenames></author></authors><title>Secrecy in Cascade Networks</title><categories>cs.IT math.IT</categories><comments>ITW Sept. 2013, 5 pages, uses IEEEtran.cls</comments><msc-class>94A15</msc-class><acm-class>H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a cascade network where a sequence of nodes each send a message
to their downstream neighbor to enable coordination, the first node having
access to an information signal. An adversary also receives all of the
communication as well as additional side-information. The performance of the
system is measured by a payoff function evaluated on actions produced at each
of the nodes, including the adversary. The challenge is to effectively use a
secret key to infuse some level of privacy into the encoding, in order thwart
the adversary's attempt to reduce the payoff. We obtain information-theoretic
inner and outer bounds on performance, and give examples where they are tight.
From these bounds, we also derive the optimal equivocation for this setting as
a special case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0062</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0062</id><created>2014-10-31</created><authors><author><keyname>Shi</keyname><forenames>Feng</forenames></author><author><keyname>Chen</keyname><forenames>Jianer</forenames></author><author><keyname>Feng</keyname><forenames>Qilong</forenames></author><author><keyname>Ding</keyname><forenames>Xiaojun</forenames></author><author><keyname>Wang</keyname><forenames>Jianxin</forenames></author></authors><title>Algorithms for Maximum Agreement Forest of Multiple General Trees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Maximum Agreement Forest (Maf) problem is a well-studied problem in
evolutionary biology, which asks for a largest common subforest of a given
collection of phylogenetic trees with identical leaf label-set. However, the
previous work about the Maf problem are mainly on two binary phylogenetic trees
or two general (i.e., binary and non-binary) phylogenetic trees. In this paper,
we study the more general version of the problem: the Maf problem on multiple
general phylogenetic trees. We present a parameterized algorithm of running
time $O(3^k n^2m)$ and a 3-approximation algorithm for the Maf problem on
multiple rooted general phylogenetic trees, and a parameterized algorithm of
running time $O(4^k n^2m)$ and a 4-approximation algorithm for the Maf problem
on multiple unrooted general phylogenetic trees. We also implement the
parameterized algorithm and approximation algorithm for the Maf problem on
multiple rooted general phylogenetic trees, and test them on simulated data and
biological data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0064</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0064</id><created>2014-11-01</created><authors><author><keyname>Chu</keyname><forenames>Lingyang</forenames></author><author><keyname>Wang</keyname><forenames>Shuhui</forenames></author><author><keyname>Liu</keyname><forenames>Siyuan</forenames></author><author><keyname>Huang</keyname><forenames>Qingming</forenames></author><author><keyname>Pei</keyname><forenames>Jian</forenames></author></authors><title>ALID: Scalable Dominant Cluster Detection</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting dominant clusters is important in many analytic applications. The
state-of-the-art methods find dense subgraphs on the affinity graph as the
dominant clusters. However, the time and space complexity of those methods are
dominated by the construction of the affinity graph, which is quadratic with
respect to the number of data points, and thus impractical on large data sets.
To tackle the challenge, in this paper, we apply Evolutionary Game Theory (EGT)
and develop a scalable algorithm, Approximate Localized Infection Immunization
Dynamics (ALID). The major idea is to perform Localized Infection Immunization
Dynamics (LID) to find dense subgraph within local range of the affinity graph.
LID is further scaled up with guaranteed high efficiency and detection quality
by an estimated Region of Interest (ROI) and a carefully designed Candidate
Infective Vertex Search method (CIVS). ALID only constructs small local
affinity graphs and has a time complexity of O(C(a^*+ {\delta})n) and a space
complexity of O(a^*(a^*+ {\delta})), where a^* is the size of the largest
dominant cluster and C &lt;&lt; n and {\delta} &lt;&lt; n are small constants. We
demonstrate by extensive experiments on both synthetic data and real world data
that ALID achieves state-of-the-art detection quality with much lower time and
space cost on single machine. We also demonstrate the encouraging
parallelization performance of ALID by implementing the Parallel ALID (PALID)
on Apache Spark. PALID processes 50 million SIFT data points in 2.29 hours,
achieving a speedup ratio of 7.51 with 8 executors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0074</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0074</id><created>2014-11-01</created><authors><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author><author><keyname>Baras</keyname><forenames>John S.</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>Emergent Behaviors over Signed Random Dynamical Networks: State-Flipping
  Model</title><categories>cs.SI cs.SY</categories><comments>IEEE Transactions on Control of Network Systems, in press. arXiv
  admin note: substantial text overlap with arXiv:1309.5488</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent studies from social, biological, and engineering network systems have
drawn attention to the dynamics over signed networks, where each link is
associated with a positive/negative sign indicating trustful/mistrustful,
activator/inhibitor, or secure/malicious interactions. We study asymptotic
dynamical patterns that emerge among a set of nodes that interact in a
dynamically evolving signed random network. Node interactions take place at
random on a sequence of deterministic signed graphs. Each node receives
positive or negative recommendations from its neighbors depending on the sign
of the interaction arcs, and updates its state accordingly. Recommendations
along a positive arc follow the standard consensus update. As in the work by
Altafini, negative recommendations use an update where the sign of the neighbor
state is flipped. Nodes may weight positive and negative recommendations
differently, and random processes are introduced to model the time-varying
attention that nodes pay to these recommendations. Conditions for almost sure
convergence and divergence of the node states are established. We show that
under this so-called state-flipping model, all links contribute to a consensus
of the absolute values of the nodes, even under switching sign patterns and
dynamically changing environment. A no-survivor property is established,
indicating that every node state diverges almost surely if the maximum network
state diverges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0076</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0076</id><created>2014-11-01</created><authors><author><keyname>Shen</keyname><forenames>Juei-Chin</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled Ben</forenames></author></authors><title>User Capacity of Pilot-Contaminated TDD Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>Accepted by IEEE GLOBECOM 2014, 6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pilot contamination has been regarded as a main limiting factor of time
division duplexing (TDD) massive multiple-input-multiple-output (Massive MIMO)
systems, as it will make the signal-to-interference-plus-noise ratio (SINR)
saturated. However, how pilot contamination will limit the user capacity of
downlink Massive MIMO, i.e., the maximum number of admissible users, has not
been addressed. This paper provides an explicit expression of the Massive MIMO
user capacity in the pilot-contaminated regime where the number of users is
larger than the pilot sequence length. Furthermore, the scheme for achieving
the user capacity, i.e., the uplink pilot training sequence and downlink power
allocation, has been identified. By using this capacity-achieving scheme, the
SINR requirement of each user can be satisfied and energy-efficient
transmission is feasible in the large-antenna-size (LAS) regime. Comparison
with two non-capacity-achieving schemes highlights the superiority of our
proposed scheme in terms of achieving higher user capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0080</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0080</id><created>2014-11-01</created><updated>2014-11-24</updated><authors><author><keyname>Feng</keyname><forenames>Shu</forenames></author><author><keyname>Chen</keyname><forenames>Gu</forenames></author><author><keyname>Mao</keyname><forenames>Wang</forenames></author><author><keyname>Berber</keyname><forenames>Stevan</forenames></author><author><keyname>Xiaohu</keyname><forenames>You</forenames></author></authors><title>Probability density derivation and analysis of SINR in massive MIMO
  systems with MF beamformer</title><categories>cs.NI</categories><comments>12 pages, 6 figures. This paper has been submitted to IEEE
  Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In massive MIMO systems, the matched filter (MF) beamforming is attractive
technique due to its extremely low complexity of implementation compared to
those high-complexity decomposition-based beamforming techniques such as
zero-forcing, and minimum mean square error. A specific problem in applying
these techniques is how to qualify and quantify the relationship between the
transmitted signal, channel noise and interference. This paper presents
detailed procedure of deriving an approximate formula for probability density
function (PDF) of the signal-to-interference-and-noise ratio (SINR) at user
terminal when multiple antennas and MF beamformer are used at the base station.
It is shown how the derived density function of SINR can be used to calculate
the symbol error rate of massive MIMO downlink. It is confirmed by simulation
that the derived approximate expression for PDF is consistent with the
simulated PDF in medium-scale and large-scale MIMO systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0085</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0085</id><created>2014-11-01</created><authors><author><keyname>Kanaujia</keyname><forenames>Atul</forenames></author><author><keyname>Choe</keyname><forenames>Tae Eun</forenames></author><author><keyname>Deng</keyname><forenames>Hongli</forenames></author></authors><title>Complex Events Recognition under Uncertainty in a Sensor Network</title><categories>cs.CV</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated extraction of semantic information from a network of sensors for
cognitive analysis and human-like reasoning is a desired capability in future
ground surveillance systems. We tackle the problem of complex decision making
under uncertainty in network information environment, where lack of effective
visual processing tools, incomplete domain knowledge frequently cause
uncertainty in the visual primitives, leading to sub-optimal decisions. While
state-of-the-art vision techniques exist in detecting visual entities (humans,
vehicles and scene elements) in an image, a missing functionality is the
ability to merge the information to reveal meaningful information for high
level inference. In this work, we develop a probabilistic first order predicate
logic(FOPL) based reasoning system for recognizing complex events in
synchronized stream of videos, acquired from sensors with non-overlapping
fields of view. We adopt Markov Logic Network(MLN) as a tool to model
uncertainty in observations, and fuse information extracted from heterogeneous
data in a probabilistically consistent way. MLN overcomes strong dependence on
pure empirical learning by incorporating domain knowledge, in the form of
user-defined rules and confidences associated with them. This work demonstrates
that the MLN based decision control system can be made scalable to model
statistical relations between a variety of entities and over long video
sequences. Experiments with real-world data, under a variety of settings,
illustrate the mathematical soundness and wide-ranging applicability of our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0087</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0087</id><created>2014-11-01</created><authors><author><keyname>Ahmed</keyname><forenames>Kaoutar Ben</forenames></author><author><keyname>Bouhorma</keyname><forenames>Mohammed</forenames></author><author><keyname>Ahmed</keyname><forenames>Mohamed Ben</forenames></author></authors><title>Age of Big Data and Smart Cities: Privacy Trade-Off</title><categories>cs.CY</categories><comments>7 pages, 1 figure, &quot;Published with International Journal of
  Engineering Trends and Technology (IJETT)&quot;</comments><journal-ref>International Journal of Engineering Trends and Technology
  (IJETT), V16(6),298-304 Oct 2014. ISSN:2231-5381</journal-ref><doi>10.14445/22315381/IJETT-V16P258</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Data will soon become one of the most precious treasures we have ever had, 43
trillion gigabytes of data will be created by 2020 according to a study made by
Mckinsey Global Institute, it is estimated that 2.3 trillion gigabytes of data
is created each day and most companies in the US have 100.000 gigabytes of data
stored. Data is recorded, stored and analyzed to enable technology and services
that the world relies on every day, this technology is getting smarter and we
will be soon living in a world of smart services or what is called smart
cities. This article presents an overview of the topic pointing to its actual
status and forecasting the crucial roles it will play in the future, we will
define big data analytics and smart cities and talk about their potential
contributions in changing our way of living and finally we will discuss the
possible down side of this upcoming technologies and how it can fool us,
violate our privacy and turn us into puppets or technology slaves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0090</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0090</id><created>2014-11-01</created><updated>2015-09-28</updated><authors><author><keyname>Brengos</keyname><forenames>Tomasz</forenames></author><author><keyname>Miculan</keyname><forenames>Marino</forenames></author><author><keyname>Peressotti</keyname><forenames>Marco</forenames></author></authors><title>Behavioural equivalences for coalgebras with unobservable moves</title><categories>cs.LO</categories><acm-class>F.1.1</acm-class><doi>10.1016/j.jlamp.2015.09.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a general categorical framework for the definition of weak
behavioural equivalences, building on and extending recent results in the
field. This framework is based on parametrized saturation categories, i.e.
categories whose hom-sets are endowed with complete orders and a suitable
iteration operators; this structure allows us to provide the abstract
definitions of various (weak) behavioural equivalence. We show that the Kleisli
categories of many common monads are categories of this kind. This allows us to
readily instantiate the abstract definitions to a wide range of existing
systems (weighted LTS, Segala systems, calculi with names, etc.), recovering
the corresponding notions of weak behavioural equivalences. Moreover, we can
provide neatly new weak behavioural equivalences for more complex behaviours,
like those definable on topological spaces, measurable spaces, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0092</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0092</id><created>2014-11-01</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author></authors><title>Systems, Resilience, and Organization: Analogies and Points of Contact
  with Hierarchy Theory</title><categories>cs.OH</categories><comments>In submission as a chapter for inclusion in the forthcoming book
  &quot;Evolutionary Theory: A Hierarchical Perspective,&quot; an interdisciplinary
  collaborative project supported by the John Templeton Foundation Grant 39982
  (group leaders: Niles Eldredge and Telmo Pievani)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aim of this paper is to provide preliminary elements for discussion about the
implications of the Hierarchy Theory of Evolution on the design and evolution
of artificial systems and socio-technical organizations. In order to achieve
this goal, a number of analogies are drawn between the System of Leibniz; the
socio-technical architecture known as Fractal Social Organization; resilience
and related disciplines; and Hierarchy Theory. In so doing we hope to provide
elements for reflection and, hopefully, enrich the discussion on the above
topics with considerations pertaining to related fields and disciplines,
including computer science, management science, cybernetics, social systems,
and general systems theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0095</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0095</id><created>2014-11-01</created><authors><author><keyname>Chakrabarty</keyname><forenames>Deeparnab</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Kothari</keyname><forenames>Pravesh</forenames></author></authors><title>Provable Submodular Minimization using Wolfe's Algorithm</title><categories>cs.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Owing to several applications in large scale learning and vision problems,
fast submodular function minimization (SFM) has become a critical problem.
Theoretically, unconstrained SFM can be performed in polynomial time [IFF 2001,
IO 2009]. However, these algorithms are typically not practical. In 1976, Wolfe
proposed an algorithm to find the minimum Euclidean norm point in a polytope,
and in 1980, Fujishige showed how Wolfe's algorithm can be used for SFM. For
general submodular functions, this Fujishige-Wolfe minimum norm algorithm seems
to have the best empirical performance.
  Despite its good practical performance, very little is known about Wolfe's
minimum norm algorithm theoretically. To our knowledge, the only result is an
exponential time analysis due to Wolfe himself. In this paper we give a maiden
convergence analysis of Wolfe's algorithm. We prove that in $t$ iterations,
Wolfe's algorithm returns an $O(1/t)$-approximate solution to the min-norm
point on {\em any} polytope. We also prove a robust version of Fujishige's
theorem which shows that an $O(1/n^2)$-approximate solution to the min-norm
point on the base polytope implies {\em exact} submodular minimization. As a
corollary, we get the first pseudo-polynomial time guarantee for the
Fujishige-Wolfe minimum norm algorithm for unconstrained submodular function
minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0100</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0100</id><created>2014-11-01</created><authors><author><keyname>Elango</keyname><forenames>Bakthavachalam</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Shankar</keyname><forenames>Subramaniam</forenames></author></authors><title>Study of Citation Networks in Tribology Research</title><categories>cs.DL</categories><comments>Submitted to the journal Scientometrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CitNetExplorer has been used to study the citation networks among the
scientific publications on tribology during the 15 years period from 1998-2012.
Three data sets from Web of Science have been analyzed: (1) Core publications
of tribology research, (2) publications on nanotribology and (3) publications
of Bharat Bhushan (a top-contributor to nanotribology research). Based on this
study, some suggestions are made to improve the CitNetExplorer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0103</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0103</id><created>2014-11-01</created><authors><author><keyname>Khabbazibasmenj</keyname><forenames>Arash</forenames></author><author><keyname>Girnyk</keyname><forenames>Maksym A.</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author><author><keyname>Vehkaper&#xe4;</keyname><forenames>Mikko</forenames></author><author><keyname>Rasmussen</keyname><forenames>Lars K.</forenames></author></authors><title>On the Optimal Precoding for MIMO Gaussian Wire-Tap Channels</title><categories>cs.IT math.IT</categories><comments>Published in Proceedings of the Tenth International Symposium on
  Wireless Communication Systems (ISWCS 2013), Ilmenau, Germany, August 2013</comments><journal-ref>In Proc. 10th Int. Symp. Wireless Commun. Syst. (ISWCS), pp. 1-4,
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding secrecy rate of a multiple-input
multiple-output (MIMO) wire-tap channel. A transmitter, a legitimate receiver,
and an eavesdropper are all equipped with multiple antennas. The channel states
from the transmitter to the legitimate user and to the eavesdropper are assumed
to be known at the transmitter. In this contribution, we address the problem of
finding the optimal precoder/transmit covariance matrix maximizing the secrecy
rate of the given wiretap channel. The problem formulation is shown to be
equivalent to a difference of convex functions programming problem and an
efficient algorithm for addressing this problem is developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0114</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0114</id><created>2014-11-01</created><authors><author><keyname>Girnyk</keyname><forenames>Maksym A.</forenames></author><author><keyname>Gabry</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Vehkaper&#xe4;</keyname><forenames>Mikko</forenames></author><author><keyname>Rasmussen</keyname><forenames>Lars K.</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>On the Transmit Beamforming for MIMO Wiretap Channels: Large-System
  Analysis</title><categories>cs.IT math.IT</categories><comments>Published in Lecture Notes in Computer Science 8317, pp. 90-102,
  2014. (Proceedings of International Conference on Information-Theoretic
  Security (ICITS), Singapore, November 2013)</comments><journal-ref>Lecture Notes in Computer Science 8317, pp. 90-102, 2014</journal-ref><doi>10.1007/978-3-319-04268-8_6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the growth of wireless networks, security has become a fundamental issue
in wireless communications due to the broadcast nature of these networks. In
this work, we consider MIMO wiretap channels in a fast fading environment, for
which the overall performance is characterized by the ergodic MIMO secrecy
rate. Unfortunately, the direct solution to finding ergodic secrecy rates is
prohibitive due to the expectations in the rates expressions in this setting.
To overcome this difficulty, we invoke the large-system assumption, which
allows a deterministic approximation to the ergodic mutual information.
Leveraging results from random matrix theory, we are able to characterize the
achievable ergodic secrecy rates. Based on this characterization, we address
the problem of covariance optimization at the transmitter. Our numerical
results demonstrate a good match between the large-system approximation and the
actual simulated secrecy rates, as well as some interesting features of the
precoder optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0126</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0126</id><created>2014-11-01</created><authors><author><keyname>Raman</keyname><forenames>Gowtham Rangarajan</forenames></author></authors><title>Detection of texts in natural images</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A framework that makes use of Connected components and supervised Support
machine to recognise texts is proposed. The image is preprocessed and and edge
graph is calculated using a probabilistic framework to compensate for
photometric noise. Connected components over the resultant image is calculated,
which is bounded and then pruned using geometric constraints. Finally a Gabor
Feature based SVM is used to classify the presence of text in the candidates.
The proposed method was tested with ICDAR 10 dataset and few other images
available on the internet. It resulted in a recall and precision metric of 0.72
and 0.88 comfortably better than the benchmark Eiphstein's algorithm. The
proposed method recorded a 0.70 and 0.74 in natural images which is
significantly better than current methods on natural images. The proposed
method also scales almost linearly for high resolution, cluttered images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0129</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0129</id><created>2014-11-01</created><updated>2016-01-22</updated><authors><author><keyname>Vincent-Lamarre</keyname><forenames>Philippe</forenames></author><author><keyname>Mass&#xe9;</keyname><forenames>Alexandre Blondin</forenames></author><author><keyname>Lopes</keyname><forenames>Marcos</forenames></author><author><keyname>Lord</keyname><forenames>M&#xe9;lanie</forenames></author><author><keyname>Marcotte</keyname><forenames>Odile</forenames></author><author><keyname>Harnad</keyname><forenames>Stevan</forenames></author></authors><title>The Latent Structure of Dictionaries</title><categories>cs.CL cs.IR</categories><comments>38 pages, 10 figures, 2 tables, 73 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How many words (and which ones) are sufficient to define all other words?
When dictionaries are analyzed as directed graphs with links from defining
words to defined words, they reveal a latent structure. Recursively removing
all words that are reachable by definition but that do not define any further
words reduces the dictionary to a Kernel of about 10%. This is still not the
smallest number of words that can define all the rest. About 75% of the Kernel
turns out to be its Core, a Strongly Connected Subset of words with a
definitional path to and from any pair of its words and no word's definition
depending on a word outside the set. But the Core cannot define all the rest of
the dictionary. The 25% of the Kernel surrounding the Core consists of small
strongly connected subsets of words: the Satellites. The size of the smallest
set of words that can define all the rest (the graph's Minimum Feedback Vertex
Set or MinSet) is about 1% of the dictionary, 15% of the Kernel, and half-Core,
half-Satellite. But every dictionary has a huge number of MinSets. The Core
words are learned earlier, more frequent, and less concrete than the
Satellites, which in turn are learned earlier and more frequent but more
concrete than the rest of the Dictionary. In principle, only one MinSet's words
would need to be grounded through the sensorimotor capacity to recognize and
categorize their referents. In a dual-code sensorimotor-symbolic model of the
mental lexicon, the symbolic code could do all the rest via re-combinatory
definition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0130</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0130</id><created>2014-11-01</created><authors><author><keyname>Antal</keyname><forenames>Balint</forenames></author><author><keyname>Hajdu</keyname><forenames>Andras</forenames></author><author><keyname>Maros-Szabo</keyname><forenames>Zsuzsanna</forenames></author><author><keyname>Torok</keyname><forenames>Zsolt</forenames></author><author><keyname>Csutak</keyname><forenames>Adrienne</forenames></author><author><keyname>Peto</keyname><forenames>Tunde</forenames></author></authors><title>A Two-phase Decision Support Framework for the Automatic Screening of
  Digital Fundus Images</title><categories>cs.CV</categories><journal-ref>Journal of Computational Science, Elsevier, Volume 3, Issue 5,
  September 2012, Pages 262-268</journal-ref><doi>10.1016/j.jocs.2012.01.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give a brief review on the present status of automated
detection systems describe for the screening of diabetic retinopathy. We
further detail an enhanced detection procedure that consists of two steps.
First, a pre-screening algorithm is considered to classify the input digital
fundus images based on the severity of abnormalities. If an image is found to
be seriously abnormal, it will not be analysed further with robust lesion
detector algorithms. As a further improvement, we introduce a novel feature
extraction approach based on clinical observations. The second step of the
proposed method detects regions of interest with possible lesions on the images
that previously passed the pre-screening step. These regions will serve as
input to the specific lesion detectors for detailed analysis. This procedure
can increase the computational performance of a screening system. Experimental
results show that both two steps of the proposed approach are capable to
efficiently exclude a large amount of data from further processing, thus, to
decrease the computational burden of the automatic screening system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0132</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0132</id><created>2014-11-01</created><authors><author><keyname>Akbari</keyname><forenames>S.</forenames></author><author><keyname>Dalirrooyfard</keyname><forenames>M.</forenames></author><author><keyname>Ehsani</keyname><forenames>K.</forenames></author><author><keyname>Sherkati</keyname><forenames>R.</forenames></author></authors><title>A Note on Signed k-Submatching in Graphs</title><categories>cs.DM</categories><comments>4 pages</comments><msc-class>05C70, 05C78</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a graph of order $n$. For every $v\in V(G)$, let $E_G(v)$ denote
the set of all edges incident with $v$. A signed $k$-submatching of $G$ is a
function $f:E(G)\longrightarrow \{-1,1\}$, satisfying $f(E_G(v))\leq 1$ for at
least $k$ vertices, where $f(S)=\sum_{e\in S}f(e)$, for each $ S\subseteq
E(G)$. The maximum of the value of $f(E(G))$, taken over all signed
$k$-submatching $f$ of $G$, is called the signed $k$-submatching number and is
denoted by $\beta ^k_S(G)$. In this paper, we prove that for every graph $G$ of
order $n$ and for any positive integer $k \leq n$, $\beta ^k_S (G) \geq n-k -
\omega(G)$, where $w(G)$ is the number of components of $G$. This settles a
conjecture proposed by Wang. Also, we present a formula for the computation of
$\beta_S^n(G)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0143</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0143</id><created>2014-11-01</created><authors><author><keyname>Bermolen</keyname><forenames>Paola</forenames></author><author><keyname>Jonckheere</keyname><forenames>Matthieu</forenames></author><author><keyname>Larroca</keyname><forenames>Federico</forenames></author><author><keyname>Moyal</keyname><forenames>Pascal</forenames></author></authors><title>Estimating the Spatial Reuse with Configuration Models</title><categories>cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new methodology to estimate the spatial reuse of CSMA-like
scheduling. Instead of focusing on spatial configurations of users, we model
the interferences between users as a random graph. Using configuration models
for random graphs, we show how the properties of the medium access mechanism
are captured by some deterministic differential equations, when the size of the
graph gets large. Performance indicators such as the probability of connection
of a given node can then be efficiently computed from these equations. We also
perform simulations to illustrate the results on different types of random
graphs. Even on spatial structures, these estimates get very accurate as soon
as the variance of the interference is not negligible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0149</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0149</id><created>2014-11-01</created><updated>2015-03-03</updated><authors><author><keyname>Abraham</keyname><forenames>Ittai</forenames></author><author><keyname>Alonso</keyname><forenames>Omar</forenames></author><author><keyname>Kandylas</keyname><forenames>Vasilis</forenames></author><author><keyname>Patel</keyname><forenames>Rajesh</forenames></author><author><keyname>Shelford</keyname><forenames>Steven</forenames></author><author><keyname>Slivkins</keyname><forenames>Aleksandrs</forenames></author></authors><title>How Many Workers to Ask? Adaptive Exploration for Collecting High
  Quality Labels</title><categories>cs.AI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing has been part of the IR toolbox as a cheap and fast mechanism
to obtain labels for system development and evaluation. Successful deployment
of crowdsourcing at scale involves adjusting many variables, a very important
one being the number of workers needed per human intelligence task (HIT). We
consider the crowdsourcing task of learning the answer to simple
multiple-choice HITs, which are representative of many relevance experiments.
In order to provide statistically significant results, one often needs to ask
multiple workers to answer the same HIT. A stopping rule is an algorithm that,
given a HIT, decides for any given set of worker answers if the system should
stop and output an answer or iterate and ask one more worker. Knowing the
historic performance of a worker in the form of a quality score can be
beneficial in such a scenario. In this paper we investigate how to devise
better stopping rules given such quality scores. We also suggest adaptive
exploration as a promising approach for scalable and automatic creation of
ground truth. We conduct a data analysis on an industrial crowdsourcing
platform, and use the observations from this analysis to design new stopping
rules that use the workers' quality scores in a non-trivial manner. We then
perform a simulation based on a real-world workload, showing that our algorithm
performs better than the more naive approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0154</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0154</id><created>2014-11-01</created><updated>2015-03-06</updated><authors><author><keyname>Guidi</keyname><forenames>Ferruccio</forenames></author></authors><title>The Formal System $\lambda\delta$ Revised, Stage A: Extending the
  Applicability Condition</title><categories>cs.LO</categories><comments>36 pages, start of section 2.6 and of Apendix B updated, some typos
  corrected</comments><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The formal system $\lambda\delta$ is a typed lambda calculus derived from
$\Lambda_\infty$, aiming to support the foundations of Mathematics that require
an underlying theory of expressions (for example the Minimal Type Theory). The
system is developed in the context of the Hypertextual Electronic Library of
Mathematics as a machine-checked digital specification, that is not the formal
counterpart of previous informal material. The first version of the calculus
appeared in 2006 and proved unsatisfactory for some reasons. In this article we
present a revised version of the system and we prove three relevant desired
properties: the confluence of reduction, the strong normalization of an
extended form of reduction, known as the &quot;big tree&quot; theorem, and the
preservation of validity by reduction. To our knowledge, we are presenting here
the first fully machine-checked proof of the &quot;big tree&quot; theorem for a calculus
that includes $\Lambda_\infty$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0156</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0156</id><created>2014-11-01</created><authors><author><keyname>Cushing</keyname><forenames>William</forenames></author><author><keyname>Benton</keyname><forenames>J.</forenames></author><author><keyname>Eyerich</keyname><forenames>Patrick</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author></authors><title>Surrogate Search As a Way to Combat Harmful Effects of Ill-behaved
  Evaluation Functions</title><categories>cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1103.3687</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, several researchers have found that cost-based satisficing search
with A* often runs into problems. Although some &quot;work arounds&quot; have been
proposed to ameliorate the problem, there has been little concerted effort to
pinpoint its origin. In this paper, we argue that the origins of this problem
can be traced back to the fact that most planners that try to optimize cost
also use cost-based evaluation functions (i.e., f(n) is a cost estimate). We
show that cost-based evaluation functions become ill-behaved whenever there is
a wide variance in action costs; something that is all too common in planning
domains. The general solution to this malady is what we call a surrogatesearch,
where a surrogate evaluation function that doesn't directly track the cost
objective, and is resistant to cost-variance, is used. We will discuss some
compelling choices for surrogate evaluation functions that are based on size
rather that cost. Of particular practical interest is a cost-sensitive version
of size-based evaluation function -- where the heuristic estimates the size of
cheap paths, as it provides attractive quality vs. speed tradeoffs
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0158</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0158</id><created>2014-11-01</created><updated>2014-12-02</updated><authors><author><keyname>Klimann</keyname><forenames>Ines</forenames></author><author><keyname>Picantin</keyname><forenames>Matthieu</forenames></author><author><keyname>Savchuk</keyname><forenames>Dmytro</forenames></author></authors><title>Orbit automata as a new tool to attack the order problem in automaton
  groups</title><categories>math.GR cs.FL</categories><comments>19 pages, 9 figures; example with Bellaterra group added; references
  updated</comments><msc-class>20E08, 20F10, 20K15, 68Q70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new tool, called the orbit automaton, that describes the
action of an automaton group $G$ on the subtrees corresponding to the orbits of
$G$ on levels of the tree. The connection between $G$ and the groups generated
by the orbit automata is used to find elements of infinite order in certain
automaton groups for which other methods failed to work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0161</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0161</id><created>2014-11-01</created><authors><author><keyname>Honeine</keyname><forenames>Paul</forenames></author></authors><title>Entropy of Overcomplete Kernel Dictionaries</title><categories>cs.IT cs.CV cs.LG cs.NE math.IT stat.ML</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In signal analysis and synthesis, linear approximation theory considers a
linear decomposition of any given signal in a set of atoms, collected into a
so-called dictionary. Relevant sparse representations are obtained by relaxing
the orthogonality condition of the atoms, yielding overcomplete dictionaries
with an extended number of atoms. More generally than the linear decomposition,
overcomplete kernel dictionaries provide an elegant nonlinear extension by
defining the atoms through a mapping kernel function (e.g., the gaussian
kernel). Models based on such kernel dictionaries are used in neural networks,
gaussian processes and online learning with kernels.
  The quality of an overcomplete dictionary is evaluated with a diversity
measure the distance, the approximation, the coherence and the Babel measures.
In this paper, we develop a framework to examine overcomplete kernel
dictionaries with the entropy from information theory. Indeed, a higher value
of the entropy is associated to a further uniform spread of the atoms over the
space. For each of the aforementioned diversity measures, we derive lower
bounds on the entropy. Several definitions of the entropy are examined, with an
extensive analysis in both the input space and the mapped feature space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0168</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0168</id><created>2014-11-01</created><authors><author><keyname>Gelashvili</keyname><forenames>Rati</forenames></author><author><keyname>Ghaffari</keyname><forenames>Mohsen</forenames></author><author><keyname>Li</keyname><forenames>Jerry</forenames></author><author><keyname>Shavit</keyname><forenames>Nir</forenames></author></authors><title>On the Importance of Registers for Computability</title><categories>cs.DC cs.DS</categories><comments>12 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All consensus hierarchies in the literature assume that we have, in addition
to copies of a given object, an unbounded number of registers. But why do we
really need these registers?
  This paper considers what would happen if one attempts to solve consensus
using various objects but without any registers. We show that under a
reasonable assumption, objects like queues and stacks cannot emulate the
missing registers. We also show that, perhaps surprisingly, initialization,
shown to have no computational consequences when registers are readily
available, is crucial in determining the synchronization power of objects when
no registers are allowed. Finally, we show that without registers, the number
of available objects affects the level of consensus that can be solved.
  Our work thus raises the question of whether consensus hierarchies which
assume an unbounded number of registers truly capture synchronization power,
and begins a line of research aimed at better understanding the interaction
between read-write memory and the powerful synchronization operations available
on modern architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0169</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0169</id><created>2014-11-01</created><authors><author><keyname>Chan</keyname><forenames>Siu-On</forenames></author><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author><author><keyname>Sun</keyname><forenames>Xiaorui</forenames></author></authors><title>Near-Optimal Density Estimation in Near-Linear Time Using Variable-Width
  Histograms</title><categories>cs.LG cs.DS math.ST stat.TH</categories><comments>conference version appears in NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $p$ be an unknown and arbitrary probability distribution over $[0,1)$. We
consider the problem of {\em density estimation}, in which a learning algorithm
is given i.i.d. draws from $p$ and must (with high probability) output a
hypothesis distribution that is close to $p$. The main contribution of this
paper is a highly efficient density estimation algorithm for learning using a
variable-width histogram, i.e., a hypothesis distribution with a piecewise
constant probability density function.
  In more detail, for any $k$ and $\epsilon$, we give an algorithm that makes
$\tilde{O}(k/\epsilon^2)$ draws from $p$, runs in $\tilde{O}(k/\epsilon^2)$
time, and outputs a hypothesis distribution $h$ that is piecewise constant with
$O(k \log^2(1/\epsilon))$ pieces. With high probability the hypothesis $h$
satisfies $d_{\mathrm{TV}}(p,h) \leq C \cdot \mathrm{opt}_k(p) + \epsilon$,
where $d_{\mathrm{TV}}$ denotes the total variation distance (statistical
distance), $C$ is a universal constant, and $\mathrm{opt}_k(p)$ is the smallest
total variation distance between $p$ and any $k$-piecewise constant
distribution. The sample size and running time of our algorithm are optimal up
to logarithmic factors. The &quot;approximation factor&quot; $C$ in our result is
inherent in the problem, as we prove that no algorithm with sample size bounded
in terms of $k$ and $\epsilon$ can achieve $C&lt;2$ regardless of what kind of
hypothesis distribution it uses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0181</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0181</id><created>2014-11-01</created><authors><author><keyname>Razavi</keyname><forenames>Hamed</forenames></author><author><keyname>Bloch</keyname><forenames>Anthony M.</forenames></author><author><keyname>Chevallereau</keyname><forenames>Christine</forenames></author><author><keyname>Grizzle</keyname><forenames>J. W.</forenames></author></authors><title>Restricted Discrete Invariance and Self-Synchronization For Stable
  Walking of Bipedal Robots</title><categories>cs.RO cs.SY</categories><comments>Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Models of bipedal locomotion are hybrid, with a continuous component often
generated by a Lagrangian plus actuators, and a discrete component where leg
transfer takes place. The discrete component typically consists of a locally
embedded co-dimension one submanifold in the continuous state space of the
robot, called the switching surface, and a reset map that provides a new
initial condition when a solution of the continuous component intersects the
switching surface. The aim of this paper is to identify a low-dimensional
submanifold of the switching surface, which, when it can be rendered invariant
by the closed-loop dynamics, leads to asymptotically stable periodic gaits. The
paper begins this process by studying the well-known 3D Linear Inverted
Pendulum (LIP) model, where analytical results are much easier to obtain. A key
contribution here is the notion of \textit{self-synchronization}, which refers
to the periods of the pendular motions in the sagittal and frontal planes
tending to a common period. The notion of invariance resulting from the study
of the 3D LIP model is then extended to a 9-DOF 3D biped. A numerical study is
performed to illustrate that asymptotically stable walking may be obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0182</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0182</id><created>2014-11-01</created><updated>2015-04-01</updated><authors><author><keyname>Srinivasan</keyname><forenames>Akshay</forenames></author><author><keyname>Venkadesan</keyname><forenames>Madhusudhan</forenames></author></authors><title>Polynomial mechanics and optimal control</title><categories>cs.SY math.OC</categories><comments>Final version to ECC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new algorithm for trajectory optimization of mechanical
systems. Our method combines pseudo-spectral methods for function approximation
with variational discretization schemes that exactly preserve conserved
mechanical quantities such as momentum. We thus obtain a global discretization
of the Lagrange-d'Alembert variational principle using pseudo-spectral methods.
Our proposed scheme inherits the numerical convergence characteristics of
spectral methods, yet preserves momentum-conservation and symplecticity after
discretization. We compare this algorithm against two other established methods
for two examples of underactuated mechanical systems; minimum-effort swing-up
of a two-link and a three-link acrobot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0183</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0183</id><created>2014-11-01</created><updated>2014-11-04</updated><authors><author><keyname>Banerjee</keyname><forenames>Taposh</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Data-Efficient Quickest Outlying Sequence Detection in Sensor Networks</title><categories>math.ST cs.IT math.IT math.PR stat.TH</categories><comments>Submitted to IEEE Transactions on Signal Processing, Nov 2014. arXiv
  admin note: text overlap with arXiv:1408.4747</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sensor network is considered where at each sensor a sequence of random
variables is observed. At each time step, a processed version of the
observations is transmitted from the sensors to a common node called the fusion
center. At some unknown point in time the distribution of observations at an
unknown subset of the sensor nodes changes. The objective is to detect the
outlying sequences as quickly as possible, subject to constraints on the false
alarm rate, the cost of observations taken at each sensor, and the cost of
communication between the sensors and the fusion center. Minimax formulations
are proposed for the above problem and algorithms are proposed that are shown
to be asymptotically optimal for the proposed formulations, as the false alarm
rate goes to zero. It is also shown, via numerical studies, that the proposed
algorithms perform significantly better than those based on fractional
sampling, in which the classical algorithms from the literature are used and
the constraint on the cost of observations is met by using the outcome of a
sequence of biased coin tosses, independent of the observation process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0186</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0186</id><created>2014-11-01</created><updated>2014-12-16</updated><authors><author><keyname>Kjos-Hanssen</keyname><forenames>Bj&#xf8;rn</forenames><affiliation>University of Hawaii at Manoa</affiliation></author><author><keyname>Nguyen</keyname><forenames>Paul Kim Long V.</forenames><affiliation>University of Hawaii at Manoa</affiliation></author><author><keyname>Rute</keyname><forenames>Jason</forenames><affiliation>Pennsylvania State University</affiliation></author></authors><title>Algorithmic randomness for Doob's martingale convergence theorem in
  continuous time</title><categories>cs.LO math.LO math.PR</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (December
  18, 2014) lmcs:978</journal-ref><doi>10.2168/LMCS-10(4:12)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study Doob's martingale convergence theorem for computable continuous time
martingales on Brownian motion, in the context of algorithmic randomness. A
characterization of the class of sample points for which the theorem holds is
given. Such points are given the name of Doob random points. It is shown that a
point is Doob random if its tail is computably random in a certain sense.
Moreover, Doob randomness is strictly weaker than computable randomness and is
incomparable with Schnorr randomness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0187</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0187</id><created>2014-11-01</created><updated>2015-09-28</updated><authors><author><keyname>Yan</keyname><forenames>Yanfei</forenames></author><author><keyname>Liu</keyname><forenames>Ling</forenames></author><author><keyname>Ling</keyname><forenames>Cong</forenames></author><author><keyname>Wu</keyname><forenames>Xiaofu</forenames></author></authors><title>Construction of Capacity-Achieving Lattice Codes: Polar Lattices</title><categories>cs.IT math.IT</categories><comments>Lemma 3 and its proof updated</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new class of lattices constructed from polar
codes, namely polar lattices, to achieve the capacity
$\frac{1}{2}\log(1+\text{SNR})$ of the additive white Gaussian-noise (AWGN)
channel for any signal-to-noise ratio (SNR). Our construction follows the
multilevel approach of Forney et al., where on each level we construct a
capacity-achieving polar code. The component polar codes are naturally nested,
thereby fulfilling the requirement of the multilevel lattice construction. We
prove that polar lattices are AWGN-good, in the sense that the error
probability (for infinite lattice decoding) vanishes for any fixed
volume-to-noise ratio (VNR) greater than $2\pi e$. Furthermore, using the
technique of source polarization, we propose discrete Gaussian shaping over the
polar lattice to satisfy the power constraint. The proposed polar lattices
permit low-complexity multistage successive cancellation decoding. Both the
construction and shaping are explicit, and the overall complexity of encoding
and decoding is $O(N\log N)$ for any fixed target error probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0189</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0189</id><created>2014-11-01</created><authors><author><keyname>Chen</keyname><forenames>Xinquan</forenames></author></authors><title>Synchronization Clustering based on a Linearized Version of Vicsek model</title><categories>cs.LG cs.DB</categories><comments>37 pages, 9 figures, 3 tabels, 27 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a kind of effective synchronization clustering method
based on a linearized version of Vicsek model. This method can be represented
by an Effective Synchronization Clustering algorithm (ESynC), an Improved
version of ESynC algorithm (IESynC), a Shrinking Synchronization Clustering
algorithm based on another linear Vicsek model (SSynC), and an effective
Multi-level Synchronization Clustering algorithm (MSynC). After some analysis
and comparisions, we find that ESynC algorithm based on the Linearized version
of the Vicsek model has better synchronization effect than SynC algorithm based
on an extensive Kuramoto model and a similar synchronization clustering
algorithm based on the original Vicsek model. By simulated experiments of some
artificial data sets, we observe that ESynC algorithm, IESynC algorithm, and
SSynC algorithm can get better synchronization effect although it needs less
iterative times and less time than SynC algorithm. In some simulations, we also
observe that IESynC algorithm and SSynC algorithm can get some improvements in
time cost than ESynC algorithm. At last, it gives some research expectations to
popularize this algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0194</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0194</id><created>2014-11-01</created><updated>2015-04-02</updated><authors><author><keyname>Huang</keyname><forenames>Lingxiao</forenames></author><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Phillips</keyname><forenames>Jeff M.</forenames></author><author><keyname>Wang</keyname><forenames>Haitao</forenames></author></authors><title>$\epsilon$-Kernel Coresets for Stochastic Points</title><categories>cs.DS cs.CG</categories><comments>40 pages, 4 figures. Added a new simpler quant-kernel construction.
  Added a new notion fpow-kernel to handel fractional powers and new results
  for shape fitting problems. Fixed several typos and cleaned several places</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the dramatic growth in the number of application domains that generate
probabilistic, noisy and uncertain data, there has been an increasing interest
in designing algorithms for geometric or combinatorial optimization problems
over such data. In this paper, we initiate the study of constructing
$\epsilon$-kernel coresets for uncertain points. We consider uncertainty in the
existential model where each point's location is fixed but only occurs with a
certain probability, and the locational model where each point has a
probability distribution describing its location. An $\epsilon$-kernel coreset
approximates the width of a point set in any direction. We consider
approximating the expected width (an \expkernel), as well as the probability
distribution on the width (an \probkernel) for any direction. We show that
there exists a set of $O(1/\epsilon^{(d-1)/2})$ deterministic points which
approximate the expected width under the existential and locational models, and
we provide efficient algorithms for constructing such coresets. We show,
however, it is not always possible to find a subset of the original uncertain
points which provides such an approximation. However, if the existential
probability of each point is lower bounded by a constant, an exp-kernel\ (or an
fpow-kernel) is still possible. We also construct an quant-kernel coreset in
linear time. Finally, combining with known techniques, we show a few
applications to approximating the extent of uncertain functions, maintaining
extent measures for stochastic moving points and some shape fitting problems
under uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0198</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0198</id><created>2014-11-01</created><authors><author><keyname>Tang</keyname><forenames>Changbing</forenames></author><author><keyname>Li</keyname><forenames>Ang</forenames></author><author><keyname>Li</keyname><forenames>Xiang</forenames></author></authors><title>When reputation enforces evolutionary cooperation in unreliable MANETs</title><categories>cs.GT cs.NI</categories><comments>12pages, 11 figures</comments><msc-class>01A25 - 05Cxx</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In self-organized mobile ad hoc networks (MANETs), network functions rely on
cooperation of self-interested nodes, where a challenge is to enforce their
mutual cooperation. In this paper, we study cooperative packet forwarding in a
one-hop unreliable channel which results from loss of packets and noisy
observation of transmissions. We propose an indirect reciprocity framework
based on evolutionary game theory, and enforce cooperation of packet forwarding
strategies in both structured and unstructured MANETs. Furthermore, we analyze
the evolutionary dynamics of cooperative strategies, and derive the threshold
of benefit-to-cost ratio to guarantee the convergence of cooperation. The
numerical simulations verify that the proposed evolutionary game theoretic
solution enforces cooperation when the benefit-to-cost ratio of the altruistic
exceeds the critical condition. In addition, the network throughput performance
of our proposed strategy in structured MANETs is measured, which is in close
agreement with that of the full cooperative strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0217</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0217</id><created>2014-11-02</created><authors><author><keyname>Jovanovic</keyname><forenames>Raka</forenames></author><author><keyname>Kais</keyname><forenames>Sabre</forenames></author><author><keyname>Alharbi</keyname><forenames>Fahhad H.</forenames></author></authors><title>Cuckoo Search Inspired Hybridization of the Nelder-Mead Simplex
  Algorithm Applied to Optimization of Photovoltaic Cells</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new hybridization of the Cuckoo Search (CS) is developed and applied to
optimize multi-cell solar systems; namely multi-junction and split spectrum
cells. The new approach consists of combining the CS with the Nelder-Mead
method. More precisely, instead of using single solutions as nests for the CS,
we use the concept of a simplex which is used in the Nelder-Mead algorithm.
This makes it possible to use the flip operation introduces in the Nelder-Mead
algorithm instead of the Levy flight which is a standard part of the CS. In
this way, the hybridized algorithm becomes more robust and less sensitive to
parameter tuning which exists in CS. The goal of our work was to optimize the
performance of multi-cell solar systems. Although the underlying problem
consists of the minimization of a function of a relatively small number of
parameters, the difficulty comes from the fact that the evaluation of the
function is complex and only a small number of evaluations is possible. In our
test, we show that the new method has a better performance when compared to
similar but more compex hybridizations of Nelder-Mead algorithm using genetic
algorithms or particle swarm optimization on standard benchmark functions.
Finally, we show that the new method outperforms some standard meta-heuristics
for the problem of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0220</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0220</id><created>2014-11-02</created><authors><author><keyname>Zou</keyname><forenames>Yulong</forenames></author><author><keyname>Zhu</keyname><forenames>Jia</forenames></author><author><keyname>Wang</keyname><forenames>Gongpu</forenames></author><author><keyname>Shao</keyname><forenames>Hua</forenames></author></authors><title>Secrecy Outage Probability Analysis of Multi-User Multi-Eavesdropper
  Wireless Systems</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><journal-ref>IEEE/CIC International Conference on Communications in China,
  Shanghai, China, October 13-15, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore the physical-layer security of a multi-user
wireless system that consists of multiple users intending to transmit to a base
station (BS), while multiple eavesdroppers attempt to tap the user
transmissions. We examine the employment of multi-user scheduling for improving
the transmission security against eavesdropping and propose a multi-user
scheduling scheme, which only requires the channel state information (CSI) of
BS without the need of the passive eavesdroppers' CSI. We also consider the
round-robin scheduling for comparison purposes. The closed-form secrecy outage
probability expressions of the round-robin scheduling and proposed multi-user
scheduling are derived over Rayleigh fading channels. Numerical results
demonstrate that the proposed multi-user scheduling outperforms the round-robin
scheduling in terms of the secrecy outage probability. As the number of users
increases, the secrecy outage probability of round-robin scheduling keeps
unchanged. By contrast, the secrecy outage performance of the proposed
multi-user scheduling improves significantly with an increasing number of
users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0224</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0224</id><created>2014-11-02</created><authors><author><keyname>Zou</keyname><forenames>Yulong</forenames></author><author><keyname>Zhu</keyname><forenames>Jia</forenames></author><author><keyname>Zheng</keyname><forenames>Baoyu</forenames></author></authors><title>Outage Analysis of Multi-Relay Selection for Cognitive Radio with
  Imperfect Spectrum Sensing</title><categories>cs.IT math.IT</categories><comments>5 pages</comments><journal-ref>The 2nd IEEE Global Conference on Signal and Information
  Processing, Atlanta, USA, December 3-5, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we examine the outage performance of a cognitive relay
network, which is comprised of a secondary transmitter (ST), multiple
decode-and-forward (DF) relays and a secondary destination (SD). We propose a
multi-relay selection scheme for the cognitive relay network, where multiple
relays are selected and used to participate in forwarding the secondary
transmission from ST to SD. A closed-form expression of the outage probability
for the proposed multi-relay selection under imperfect spectrum sensing is
derived in Rayleigh fading environments. For comparison purposes, the
conventional direct transmission and the best-relay selection are also
considered as benchmarks. Numerical results show that as the spectrum sensing
performance improves with an increasing detection probability and/or a
decreasing false alarm probability, the outage probabilities of the proposed
multi-relay selection as well as the direct transmission and the best-relay
selection schemes all decrease accordingly. It is also demonstrated that the
proposed multi-relay selection significantly outperforms the conventional
approaches in terms of the outage probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0225</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0225</id><created>2014-11-02</created><authors><author><keyname>Nabi</keyname><forenames>Zubair</forenames></author></authors><title>Censorship is Futile</title><categories>cs.CY</categories><comments>First Monday, Volume 19, Number 11 - 3 November 2014</comments><doi>10.5210/fm.v19i11.</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The Internet has become the new battle ground between authoritarian regimes
and ordinary individuals who want unimpeded access to information. The immense
popularity of online activism and citizen journalism enabled by social media
has instigated state level players to partially or completely block access to
the Internet. In return, individuals and organizations have been employing
various anti-censorship tools to circumvent these restrictions. In this paper,
we claim that censorship is futile as not only has it been ineffective in
restricting access, it has also had the side-effect of popularising blocked
content. Using data from Alexa Web Rankings, Google Trends, and YouTube
Statistics, we quantify the ineffectiveness of state level censorship in
Pakistan and Turkey and highlight the emergence of the Streisand Effect. We
hope that our findings will, a) prove to governments and other players the
futility of their actions, and b) aid citizens around the world in using legal
measures to counteract censorship by showing its ineffectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0228</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0228</id><created>2014-11-02</created><updated>2014-11-29</updated><authors><author><keyname>Zou</keyname><forenames>Yulong</forenames></author><author><keyname>Champagne</keyname><forenames>Benoit</forenames></author><author><keyname>Zhu</keyname><forenames>Wei-Ping</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>Relay-Selection Improves the Security-Reliability Trade-off in Cognitive
  Radio Systems</title><categories>cs.IT math.IT</categories><comments>13 pages, IEEE Transactions on Communications, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a cognitive radio (CR) network consisting of a secondary
transmitter (ST), a secondary destination (SD) and multiple secondary relays
(SRs) in the presence of an eavesdropper. We rely on careful relay selection
for protecting the ST-SD transmission against the eavesdropper with the aid of
both single-relay and multi-relay selection. To be specific, only the &quot;best&quot; SR
is chosen in the single-relay selection for assisting the secondary
transmission, whereas the multi-relay selection invokes multiple SRs for
simultaneously forwarding the ST's transmission to the SD. We analyze both the
intercept probability and outage probability of the proposed single-relay and
multi-relay selection schemes for the secondary transmission relying on
realistic spectrum sensing. We also evaluate the performance of classic direct
transmission and artificial noise based methods for the purpose of comparison
with the proposed relay selection schemes. It is shown that as the intercept
probability requirement is relaxed, the outage performance of the direct
transmission, the artificial noise based and the relay selection schemes
improves, and vice versa. This implies a trade-off between the security and
reliability of the secondary transmission in the presence of eavesdropping
attacks, which is referred to as the security-reliability trade-off (SRT).
Furthermore, we demonstrate that the SRTs of the single-relay and multi-relay
selection schemes are generally better than that of classic direct
transmission. Moreover, as the number of SRs increases, the SRTs of the
proposed single-relay and multi-relay selection approaches significantly
improve. Finally, our numerical results show that as expected, the multi-relay
selection scheme achieves a better SRT performance than the single-relay
selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0237</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0237</id><created>2014-11-02</created><authors><author><keyname>Guri</keyname><forenames>Mordechai</forenames></author><author><keyname>Kedma</keyname><forenames>Gabi</forenames></author><author><keyname>Kachlon</keyname><forenames>Assaf</forenames></author><author><keyname>Elovici</keyname><forenames>Yuval</forenames></author></authors><title>AirHopper: Bridging the Air-Gap between Isolated Networks and Mobile
  Phones using Radio Frequencies</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information is the most critical asset of modern organizations, and
accordingly coveted by adversaries. When highly sensitive data is involved, an
organization may resort to air-gap isolation, in which there is no networking
connection between the inner network and the external world. While infiltrating
an air-gapped network has been proven feasible in recent years (e.g., Stuxnet),
data exfiltration from an air-gapped network is still considered to be one of
the most challenging phases of an advanced cyber-attack. In this paper we
present &quot;AirHopper&quot;, a bifurcated malware that bridges the air-gap between an
isolated network and nearby infected mobile phones using FM signals. While it
is known that software can intentionally create radio emissions from a video
display unit, this is the first time that mobile phones are considered in an
attack model as the intended receivers of maliciously crafted radio signals. We
examine the attack model and its limitations, and discuss implementation
considerations such as stealth and modulation methods. Finally, we evaluate
AirHopper and demonstrate how textual and binary data can be exfiltrated from
physically isolated computer to mobile phones at a distance of 1-7 meters, with
effective bandwidth of 13-60 Bps (Bytes per second).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0246</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0246</id><created>2014-11-02</created><authors><author><keyname>Jamali</keyname><forenames>Amin</forenames></author><author><keyname>Hemami</keyname><forenames>Seyed Mostafa Safavi</forenames></author><author><keyname>Berenjkoub</keyname><forenames>Mehdi</forenames></author><author><keyname>Saidi</keyname><forenames>Hossein</forenames></author></authors><title>An Adaptive MAC Protocol for Wireless LANs</title><categories>cs.NI</categories><comments>11 pages, 13 figures</comments><journal-ref>Journal of Communications and Networks, Vol. 16, No. 3, June 2014</journal-ref><doi>10.1109/JCN.2014.000052</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on contention-based Medium Access Control (MAC) protocols
used in Wireless Local Area Networks (WLANs). We propose a novel MAC protocol
called Adaptive Backoff Tuning MAC (ABTMAC) based on IEEE 802.11 DCF. In our
proposed MAC protocol, we utilize a fixed transmission attempt rate and each
node dynamically adjusts its backoff window size considering the current
network status. We determined the appropriate transmission attempt rate for
both cases where the Request-To-Send/Clear-To-Send (RTS/CTS) mechanism was and
was not employed. Robustness against performance degradation caused by the
difference between desired and actual values of the attempt rate parameter is
considered when setting it. The performance of the protocol is evaluated
analytically and through simulations. These results indicate that a wireless
network utilizing ABTMAC performs better than one using IEEE 802.11 DCF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0247</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0247</id><created>2014-11-02</created><authors><author><keyname>Lillicrap</keyname><forenames>Timothy P.</forenames></author><author><keyname>Cownden</keyname><forenames>Daniel</forenames></author><author><keyname>Tweed</keyname><forenames>Douglas B.</forenames></author><author><keyname>Akerman</keyname><forenames>Colin J.</forenames></author></authors><title>Random feedback weights support learning in deep neural networks</title><categories>q-bio.NC cs.NE</categories><comments>14 pages, 5 figures in main text; 13 pages appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The brain processes information through many layers of neurons. This deep
architecture is representationally powerful, but it complicates learning by
making it hard to identify the responsible neurons when a mistake is made. In
machine learning, the backpropagation algorithm assigns blame to a neuron by
computing exactly how it contributed to an error. To do this, it multiplies
error signals by matrices consisting of all the synaptic weights on the
neuron's axon and farther downstream. This operation requires a precisely
choreographed transport of synaptic weight information, which is thought to be
impossible in the brain. Here we present a surprisingly simple algorithm for
deep learning, which assigns blame by multiplying error signals by random
synaptic weights. We show that a network can learn to extract useful
information from signals sent through these random feedback connections. In
essence, the network learns to learn. We demonstrate that this new mechanism
performs as quickly and accurately as backpropagation on a variety of problems
and describe the principles which underlie its function. Our demonstration
provides a plausible basis for how a neuron can be adapted using error signals
generated at distal locations in the brain, and thus dispels long-held
assumptions about the algorithmic constraints on learning in neural circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0252</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0252</id><created>2014-11-02</created><authors><author><keyname>Xie</keyname><forenames>Xinqian</forenames></author><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Wang</keyname><forenames>Wenbo</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Channel Estimation for Two-Way Relay Networks in the Presence of
  Synchronization Errors</title><categories>cs.IT math.IT</categories><comments>14 pages, 9 figures</comments><doi>10.1109/TSP.2014.2360146</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates pilot-aided channel estimation for two-way relay
networks (TWRNs) in the presence of synchronization errors between the two
sources. The unpredictable synchronization error leads to time domain offset
and signal arriving order (SAO) ambiguity when two signals sent from two
sources are superimposed at the relay. A two-step channel estimation algorithm
is first proposed, in which the linear minimum mean-square-error (LMMSE)
estimator is used to obtain initial channel estimates based on pilot symbols
and a linear minimum error probability (LMEP) estimator is then developed to
update these estimates. Optimal training sequences and power allocation at the
relay are designed to further improve the performance for LMMSE based initial
channel estimation. To tackle the SAO ambiguity problem, the generalized
likelihood ratio testing (GLRT) method is applied and an upper bound on the SAO
detection error probability is derived. By using the SAO information, a scaled
LMEP estimation algorithm is proposed to compensate the performance degradation
caused by SAO detection error. Simulation results show that the proposed
estimation algorithms can effectively mitigate the negative effects caused by
asynchronous transmissions in TWRNs, thus significantly outperforming the
existing channel estimation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0257</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0257</id><created>2014-11-02</created><authors><author><keyname>Efentakis</keyname><forenames>Alexandros</forenames></author><author><keyname>Pfoser</keyname><forenames>Dieter</forenames></author><author><keyname>Vassiliou</keyname><forenames>Yannis</forenames></author></authors><title>SALT. A unified framework for all shortest-path query variants on road
  networks</title><categories>cs.DS</categories><acm-class>G.2.2; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although recent scientific output focuses on multiple shortest-path problem
definitions for road networks, none of the existing solutions does efficiently
answer all different types of SP queries. This work proposes SALT, a novel
framework that not only efficiently answers SP related queries but also
k-nearest neighbor queries not handled by previous approaches. Our solution
offers all the benefits needed for practical use-cases, including excellent
query performance and very short preprocessing times, thus making it also a
viable option for dynamic road networks, i.e., edge weights changing frequently
due to traffic updates. The proposed SALT framework is a deployable software
solution capturing a range of network-related query problems under one
&quot;algorithmic hood&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0264</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0264</id><created>2014-11-02</created><updated>2015-07-26</updated><authors><author><keyname>Razgon</keyname><forenames>Igor</forenames></author></authors><title>On the read-once property of branching programs and CNFs of bounded
  treewidth</title><categories>cs.CC cs.AI</categories><comments>Significantly simplified proof of the main combinatorial lemma.
  AROSRN replaced back by NROBP due to their equivalence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prove a space lower bound of $n^{\Omega(k)}$ for
non-deterministic (syntactic) read-once branching programs ({\sc nrobp}s) on
functions expressible as {\sc cnf}s with treewidth at most $k$ of their primal
graphs. This lower bound rules out the possibility of fixed-parameter space
complexity of {\sc nrobp}s parameterized by $k$.
  We use lower bound for {\sc nrobp}s to obtain a quasi-polynomial separation
between Free Binary Decision Diagrams and Decision Decomposable Negation Normal
Forms, essentially matching the existing upper bound introduced by Beame et al.
and thus proving the tightness of the latter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0275</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0275</id><created>2014-11-02</created><authors><author><keyname>Verstak</keyname><forenames>Alex</forenames></author><author><keyname>Acharya</keyname><forenames>Anurag</forenames></author><author><keyname>Suzuki</keyname><forenames>Helder</forenames></author><author><keyname>Henderson</keyname><forenames>Sean</forenames></author><author><keyname>Iakhiaev</keyname><forenames>Mikhail</forenames></author><author><keyname>Lin</keyname><forenames>Cliff Chiung Yu</forenames></author><author><keyname>Shetty</keyname><forenames>Namit</forenames></author></authors><title>On the Shoulders of Giants: The Growing Impact of Older Articles</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we examine the evolution of the impact of older scholarly
articles. We attempt to answer four questions. First, how often are older
articles cited and how has this changed over time. Second, how does the impact
of older articles vary across different research fields. Third, is the change
in the impact of older articles accelerating or slowing down. Fourth, are these
trends different for much older articles.
  To answer these questions, we studied citations from articles published in
1990-2013. We computed the fraction of citations to older articles from
articles published each year as the measure of impact. We considered articles
that were published at least 10 years before the citing article as older
articles. We computed these numbers for 261 subject categories and 9 broad
areas of research. Finally, we repeated the computation for two other
definitions of older articles, 15 years and older and 20 years and older.
  There are three conclusions from our study. First, the impact of older
articles has grown substantially over 1990-2013. In 2013, 36% of citations were
to articles that are at least 10 years old; this fraction has grown 28% since
1990. The fraction of older citations increased over 1990-2013 for 7 out of 9
broad areas and 231 out of 261 subject categories. Second, the increase over
the second half (2002-2013) was double the increase in the first half
(1990-2001).
  Third, the trend of a growing impact of older articles also holds for even
older articles. In 2013, 21% of citations were to articles &gt;= 15 years old with
an increase of 30% since 1990 and 13% of citations were to articles &gt;= 20 years
old with an increase of 36%.
  Now that finding and reading relevant older articles is about as easy as
finding and reading recently published articles, significant advances aren't
getting lost on the shelves and are influencing work worldwide for years after.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0279</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0279</id><created>2014-11-02</created><updated>2014-11-05</updated><authors><author><keyname>Abeliuk</keyname><forenames>Andres</forenames></author><author><keyname>Berbeglia</keyname><forenames>Gerardo</forenames></author><author><keyname>Cebrian</keyname><forenames>Manuel</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author></authors><title>Optimizing Expected Utility in a Multinomial Logit Model with Position
  Bias and Social Influence</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by applications in retail, online advertising, and cultural
markets, this paper studies how to find the optimal assortment and positioning
of products subject to a capacity constraint. We prove that the optimal
assortment and positioning can be found in polynomial time for a multinomial
logit model capturing utilities, position bias, and social influence. Moreover,
in a dynamic market, we show that the policy that applies the optimal
assortment and positioning and leverages social influence outperforms in
expectation any policy not using social influence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0281</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0281</id><created>2014-11-02</created><updated>2016-03-04</updated><authors><author><keyname>Chou</keyname><forenames>Remi A.</forenames></author><author><keyname>Bloch</keyname><forenames>Matthieu R.</forenames></author></authors><title>Polar Coding for the Broadcast Channel with Confidential Messages: A
  Random Binning Analogy</title><categories>cs.IT math.IT</categories><comments>20 pages, two-column, 6 figures, accepted to IEEE Transactions on
  Information Theory; parts of the results were presented at the 2015 IEEE
  Information Theory Workshop; minor change in title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a low-complexity polar coding scheme for the discrete memoryless
broadcast channel with confidential messages under strong secrecy and
randomness constraints. Our scheme extends previous work by using an optimal
rate of uniform randomness in the stochastic encoder, and avoiding assumptions
regarding the symmetry or degraded nature of the channels. The price paid for
these extensions is that the encoder and decoders are required to share a
secret seed of negligible size and to increase the block length through
chaining. We also highlight a close conceptual connection between the proposed
polar coding scheme and a random binning proof of the secrecy capacity region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0282</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0282</id><created>2014-11-02</created><authors><author><keyname>Soni</keyname><forenames>Akshay</forenames></author><author><keyname>Jain</keyname><forenames>Swayambhoo</forenames></author><author><keyname>Haupt</keyname><forenames>Jarvis</forenames></author><author><keyname>Gonella</keyname><forenames>Stefano</forenames></author></authors><title>Noisy Matrix Completion under Sparse Factor Models</title><categories>stat.ML cs.IT math.IT stat.AP</categories><comments>42 Pages, 7 Figures, Submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines a general class of noisy matrix completion tasks where
the goal is to estimate a matrix from observations obtained at a subset of its
entries, each of which is subject to random noise or corruption. Our specific
focus is on settings where the matrix to be estimated is well-approximated by a
product of two (a priori unknown) matrices, one of which is sparse. Such
structural models - referred to here as &quot;sparse factor models&quot; - have been
widely used, for example, in subspace clustering applications, as well as in
contemporary sparse modeling and dictionary learning tasks. Our main
theoretical contributions are estimation error bounds for sparsity-regularized
maximum likelihood estimators for problems of this form, which are applicable
to a number of different observation noise or corruption models. Several
specific implications are examined, including scenarios where observations are
corrupted by additive Gaussian noise or additive heavier-tailed (Laplace)
noise, Poisson-distributed observations, and highly-quantized (e.g., one-bit)
observations. We also propose a simple algorithmic approach based on the
alternating direction method of multipliers for these tasks, and provide
experimental evidence to support our error analyses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0283</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0283</id><created>2014-11-02</created><updated>2015-04-13</updated><authors><author><keyname>Ardeshiri</keyname><forenames>Tohid</forenames></author><author><keyname>Chen</keyname><forenames>Tianshi</forenames></author></authors><title>Maximum Entropy Property of Discrete-time Stable Spline Kernel</title><categories>cs.SY</categories><comments>accepted for IEEE International Conference on Acoustics, Speech and
  Signal Processing, Brisbane, Australia, April 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the maximum entropy property of the discrete-time first-order
stable spline kernel is studied. The advantages of studying this property in
discrete-time domain instead of continuous-time domain are outlined. One of
such advantages is that the differential entropy rate is well-defined for
discrete-time stochastic processes. By formulating the maximum entropy problem
for discrete-time stochastic processes we provide a simple and self-contained
proof to show what maximum entropy property the discrete-time first-order
stable spline kernel has.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0290</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0290</id><created>2014-11-02</created><authors><author><keyname>Petrosyan</keyname><forenames>Petros A.</forenames></author><author><keyname>Mkhitaryan</keyname><forenames>Sargis T.</forenames></author></authors><title>Interval cyclic edge-colorings of graphs</title><categories>math.CO cs.DM</categories><comments>23 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A proper edge-coloring of a graph $G$ with colors $1,\ldots,t$ is called an
\emph{interval cyclic $t$-coloring} if all colors are used, and the edges
incident to each vertex $v\in V(G)$ are colored by $d_{G}(v)$ consecutive
colors modulo $t$, where $d_{G}(v)$ is the degree of a vertex $v$ in $G$. A
graph $G$ is \emph{interval cyclically colorable} if it has an interval cyclic
$t$-coloring for some positive integer $t$. The set of all interval cyclically
colorable graphs is denoted by $\mathfrak{N}_{c}$. For a graph $G\in
\mathfrak{N}_{c}$, the least and the greatest values of $t$ for which it has an
interval cyclic $t$-coloring are denoted by $w_{c}(G)$ and $W_{c}(G)$,
respectively. In this paper we investigate some properties of interval cyclic
colorings. In particular, we prove that if $G$ is a triangle-free graph with at
least two vertices and $G\in \mathfrak{N}_{c}$, then $W_{c}(G)\leq \vert
V(G)\vert +\Delta(G)-2$. We also obtain bounds on $w_{c}(G)$ and $W_{c}(G)$ for
various classes of graphs. Finally, we give some methods for constructing of
interval cyclically non-colorable graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0292</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0292</id><created>2014-11-02</created><updated>2015-06-08</updated><authors><author><keyname>Kucukelbir</keyname><forenames>Alp</forenames></author><author><keyname>Blei</keyname><forenames>David M.</forenames></author></authors><title>Population Empirical Bayes</title><categories>stat.ML cs.LG</categories><comments>UAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian predictive inference analyzes a dataset to make predictions about
new observations. When a model does not match the data, predictive accuracy
suffers. We develop population empirical Bayes (POP-EB), a hierarchical
framework that explicitly models the empirical population distribution as part
of Bayesian analysis. We introduce a new concept, the latent dataset, as a
hierarchical variable and set the empirical population as its prior. This leads
to a new predictive density that mitigates model mismatch. We efficiently apply
this method to complex models by proposing a stochastic variational inference
algorithm, called bumping variational inference (BUMP-VI). We demonstrate
improved predictive accuracy over classical Bayesian inference in three models:
a linear regression model of health data, a Bayesian mixture model of natural
images, and a latent Dirichlet allocation topic model of scientific documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0294</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0294</id><created>2014-11-02</created><updated>2014-11-04</updated><authors><author><keyname>Grigorescu</keyname><forenames>Andrea</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Schaefer</keyname><forenames>Rafael F.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Capacity Region Continuity of the Compound Broadcast Channel with
  Confidential Messages</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The compound broadcast channel with confidential messages (BCC) generalizes
the BCC by modeling the uncertainty of the channel. For the compound BCC, it is
only known that the actual channel realization belongs to a pre-specified
uncertainty set of channels and that it is constant during the whole
transmission. For reliable and secure communication is necessary to operate at
a rate pair within the compound BCC capacity region. Therefore, the question
whether small variations of the uncertainty set lead to large losses of the
compound BCC capacity region is studied. It is shown that the compound BCC
model is robust, i.e., the capacity region depends continuously on the
uncertainty set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0296</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0296</id><created>2014-11-02</created><updated>2014-11-17</updated><authors><author><keyname>Feragen</keyname><forenames>Aasa</forenames></author><author><keyname>Lauze</keyname><forenames>Francois</forenames></author><author><keyname>Hauberg</keyname><forenames>S&#xf8;ren</forenames></author></authors><title>Geodesic Exponential Kernels: When Curvature and Linearity Conflict</title><categories>cs.LG cs.CV</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider kernel methods on general geodesic metric spaces and provide both
negative and positive results. First we show that the common Gaussian kernel
can only be generalized to a positive definite kernel on a geodesic metric
space if the space is flat. As a result, for data on a Riemannian manifold, the
geodesic Gaussian kernel is only positive definite if the Riemannian manifold
is Euclidean. This implies that any attempt to design geodesic Gaussian kernels
on curved Riemannian manifolds is futile. However, we show that for spaces with
conditionally negative definite distances the geodesic Laplacian kernel can be
generalized while retaining positive definiteness. This implies that geodesic
Laplacian kernels can be generalized to some curved spaces, including spheres
and hyperbolic spaces. Our theoretical results are verified empirically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0306</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0306</id><created>2014-11-02</created><updated>2015-11-08</updated><authors><author><keyname>Alaoui</keyname><forenames>Ahmed El</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author></authors><title>Fast Randomized Kernel Methods With Statistical Guarantees</title><categories>stat.ML cs.LG stat.CO</categories><comments>Improved presentation. Technical details fixed. A conference version
  of this paper appears in NIPS15 under the modified title &quot;Fast Randomized
  Kernel Ridge Regression with Statistical Guarantees&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One approach to improving the running time of kernel-based machine learning
methods is to build a small sketch of the input and use it in lieu of the full
kernel matrix in the machine learning task of interest. Here, we describe a
version of this approach that comes with running time guarantees as well as
improved guarantees on its statistical performance. By extending the notion of
\emph{statistical leverage scores} to the setting of kernel ridge regression,
our main statistical result is to identify an importance sampling distribution
that reduces the size of the sketch (i.e., the required number of columns to be
sampled) to the \emph{effective dimensionality} of the problem. This quantity
is often much smaller than previous bounds that depend on the \emph{maximal
degrees of freedom}. Our main algorithmic result is to present a fast algorithm
to compute approximations to these scores. This algorithm runs in time that is
linear in the number of samples---more precisely, the running time is
$O(np^2)$, where the parameter $p$ depends only on the trace of the kernel
matrix and the regularization parameter---and it can be applied to the matrix
of feature vectors, without having to form the full kernel matrix. This is
obtained via a variant of length-squared sampling that we adapt to the kernel
setting in a way that is of independent interest. Lastly, we provide empirical
results illustrating our theory, and we discuss how this new notion of the
statistical leverage of a data point captures in a fine way the difficulty of
the original statistical learning problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0319</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0319</id><created>2014-11-02</created><updated>2015-01-18</updated><authors><author><keyname>Elkayam</keyname><forenames>Nir</forenames></author><author><keyname>Feder</keyname><forenames>Meir</forenames></author></authors><title>Achievable and Converse bounds over a general channel and general
  decoding metric</title><categories>cs.IT math.IT</categories><comments>Accepted to ITW 2015, Added &quot;sphere packing&quot; proof of the
  meta-converse, Correct the mismatched converse to incoporate tie breaking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Achievable and converse bounds for general channels and mismatched decoding
are derived. The direct (achievable) bound is derived using random coding and
the analysis is tight up to factor 2. The converse is given in term of the
achievable bound and the factor between them is given. This gives performance
of the best rate-R code with possible mismatched decoding metric over a general
channel, up to the factor that is identified. In the matched case we show that
the converse equals the minimax meta-converse of Polyanskiy et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0326</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0326</id><created>2014-11-02</created><updated>2015-06-03</updated><authors><author><keyname>Florea</keyname><forenames>Corneliu</forenames></author><author><keyname>Vertan</keyname><forenames>Constantin</forenames></author><author><keyname>Florea</keyname><forenames>Laura</forenames></author></authors><title>High Dynamic Range Imaging by Perceptual Logarithmic Exposure Merging</title><categories>cs.CV</categories><comments>14 pages 8 figures. Accepted at AMCS journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we emphasize a similarity between the Logarithmic-Type Image
Processing (LTIP) model and the Naka-Rushton model of the Human Visual System
(HVS). LTIP is a derivation of the Logarithmic Image Processing (LIP), which
further replaces the logarithmic function with a ratio of polynomial functions.
Based on this similarity, we show that it is possible to present an unifying
framework for the High Dynamic Range (HDR) imaging problem, namely that
performing exposure merging under the LTIP model is equivalent to standard
irradiance map fusion. The resulting HDR algorithm is shown to provide high
quality in both subjective and objective evaluations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0336</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0336</id><created>2014-11-02</created><updated>2015-07-04</updated><authors><author><keyname>Elkotby</keyname><forenames>Hussain</forenames></author><author><keyname>Vu</keyname><forenames>Mai</forenames></author></authors><title>Uplink User-Assisted Relaying in Cellular Networks</title><categories>cs.IT math.IT</categories><comments>32 pages, 13 figures, revised version submitted to IEEE Transactions
  on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use stochastic geometry to analyze the performance of a partial
decode-and-forward (PDF) relaying scheme applied in a user-assisted relaying
setting, where an active user relays data through another idle user in uplink
cellular communication. We present the geometric model of a network deploying
user-assisted relaying and propose two geometric cooperation policies for fast
and slow fading channels. We analytically derive the cooperation probability
for both policies. This cooperation probability is further used in the
analytical derivation of the moments of inter-cell interference power caused by
system-wide deployment of this user-assisted PDF relaying. We then model the
inter-cell interference power statistics using the Gamma distribution by
matching the first two moments analytically derived. This cooperation and
interference analysis provides the theoretical basis for quantitatively
evaluating the performance impact of user-assisted relaying in cellular
networks. We then numerically evaluate the average transmission rate
performance and show that user-assisted relaying can significantly improve
per-user transmission rate despite of increased inter-cell interference. This
transmission rate gain is significant for active users near the cell edge and
further increases with higher idle user density, supporting user-assisted
relaying as a viable solution to crowded population areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0344</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0344</id><created>2014-11-02</created><updated>2015-06-20</updated><authors><author><keyname>Fotouhi</keyname><forenames>Babak</forenames></author><author><keyname>Momeni</keyname><forenames>Naghmeh</forenames></author></authors><title>Inter-layer Degree Correlations in Heterogeneously Growing Multiplex
  Networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><doi>10.1007/978-3-319-16112-9_16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multiplex network growth literature has been confined to homogeneous
growth hitherto, where the number of links that each new incoming node
establishes is the same across layers. This paper focuses on heterogeneous
growth. We first analyze the case of two preferentially growing layers and find
a closed-form expression for the inter-layer degree distribution, and
demonstrate that non-trivial inter-layer degree correlations emerge in the
steady state. Then we focus on the case of uniform growth. Surprisingly,
inter-layer correlations arise in the random case, too. Also, we observe that
the expression for the average layer-2 degree of nodes whose layer-1 degree is
k, is identical for the uniform and preferential schemes. Throughout,
theoretical predictions are corroborated using Monte Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0347</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0347</id><created>2014-11-02</created><authors><author><keyname>Pilanci</keyname><forenames>Mert</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>Iterative Hessian sketch: Fast and accurate solution approximation for
  constrained least-squares</title><categories>math.OC cs.IT cs.LG math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study randomized sketching methods for approximately solving least-squares
problem with a general convex constraint. The quality of a least-squares
approximation can be assessed in different ways: either in terms of the value
of the quadratic objective function (cost approximation), or in terms of some
distance measure between the approximate minimizer and the true minimizer
(solution approximation). Focusing on the latter criterion, our first main
result provides a general lower bound on any randomized method that sketches
both the data matrix and vector in a least-squares problem; as a surprising
consequence, the most widely used least-squares sketch is sub-optimal for
solution approximation. We then present a new method known as the iterative
Hessian sketch, and show that it can be used to obtain approximations to the
original least-squares problem using a projection dimension proportional to the
statistical complexity of the least-squares minimizer, and a logarithmic number
of iterations. We illustrate our general theory with simulations for both
unconstrained and constrained versions of least-squares, including
$\ell_1$-regularization and nuclear norm constraints. We also numerically
demonstrate the practicality of our approach in a real face expression
classification experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0349</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0349</id><created>2014-11-02</created><authors><author><keyname>Gurvich</keyname><forenames>Vladimir</forenames></author><author><keyname>Oudalov</keyname><forenames>Vladimir</forenames></author></authors><title>A four-person chess-like game without Nash equilibria in pure stationary
  strategies</title><categories>math.CO cs.GT</categories><comments>9 pages, 1 figure, 1 table</comments><msc-class>05C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short note we give an example of a four-person finite positional game
with perfect information that has no positions of chance and no Nash equilibria
in pure stationary strategies. The corresponding directed graph has only one
directed cycle and only five terminal positions.
  It remains open: (i) if the number $n$ of the players can be reduced from $4$
to $3$, (ii) if the number $p$ of the terminals can be reduced from $5$ to $4$,
and most important, (iii) whether it is possible to get a similar example in
which the outcome $c$ corresponding to all (possibly, more than one) directed
cycles is worse than every terminal for each player.
  Yet, it is known that (j) $n$ cannot be reduced to $2$, (jj) $p$ cannot be
reduced to $3$, and (jjj) there can be no similar example in which each player
makes a decision in a unique position.
  Keywords: stochastic, positional, chess-like, transition-free games with
perfect information and without moves of chance; Nash equilibrium, directed
cycles (dicycles), terminal position.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0352</identifier>
 <datestamp>2015-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0352</id><created>2014-11-02</created><updated>2015-05-29</updated><authors><author><keyname>Chevalier-Boisvert</keyname><forenames>Maxime</forenames></author><author><keyname>Feeley</keyname><forenames>Marc</forenames></author></authors><title>Simple and Effective Type Check Removal through Lazy Basic Block
  Versioning</title><categories>cs.PL</categories><acm-class>D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamically typed programming languages such as JavaScript and Python defer
type checking to run time. In order to maximize performance, dynamic language
VM implementations must attempt to eliminate redundant dynamic type checks.
However, type inference analyses are often costly and involve tradeoffs between
compilation time and resulting precision. This has lead to the creation of
increasingly complex multi-tiered VM architectures.
  This paper introduces lazy basic block versioning, a simple JIT compilation
technique which effectively removes redundant type checks from critical code
paths. This novel approach lazily generates type-specialized versions of basic
blocks on-the-fly while propagating context-dependent type information. This
does not require the use of costly program analyses, is not restricted by the
precision limitations of traditional type analyses and avoids the
implementation complexity of speculative optimization techniques.
  We have implemented intraprocedural lazy basic block versioning in a
JavaScript JIT compiler. This approach is compared with a classical flow-based
type analysis. Lazy basic block versioning performs as well or better on all
benchmarks. On average, 71% of type tests are eliminated, yielding speedups of
up to 50%. We also show that our implementation generates more efficient
machine code than TraceMonkey, a tracing JIT compiler for JavaScript, on
several benchmarks. The combination of implementation simplicity, low
algorithmic complexity and good run time performance makes basic block
versioning attractive for baseline JIT compilers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0359</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0359</id><created>2014-11-02</created><updated>2015-05-11</updated><authors><author><keyname>Coffrin</keyname><forenames>Carleton</forenames></author><author><keyname>Gordon</keyname><forenames>Dan</forenames></author><author><keyname>Scott</keyname><forenames>Paul</forenames></author></authors><title>NESTA, The NICTA Energy System Test Case Archive</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years the power systems research community has seen an explosion of
work applying operations research techniques to challenging power network
optimization problems. Regardless of the application under consideration, all
of these works rely on power system test cases for evaluation and validation.
However, many of the well established power system test cases were developed as
far back as the 1960s with the aim of testing AC power flow algorithms. It is
unclear if these power flow test cases are suitable for power system
optimization studies. This report surveys all of the publicly available AC
transmission system test cases, to the best of our knowledge, and assess their
suitability for optimization tasks. It finds that many of the traditional test
cases are missing key network operation constraints, such as line thermal
limits and generator capability curves. To incorporate these missing
constraints, data driven models are developed from a variety of publicly
available data sources. The resulting extended test cases form a compressive
archive, NESTA, for the evaluation and validation of power system optimization
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0370</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0370</id><created>2014-11-03</created><authors><author><keyname>Ananthapadmanabha</keyname><forenames>T V</forenames></author><author><keyname>Girish</keyname><forenames>K V Vijay</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>A G</forenames></author></authors><title>Detection of transitions between broad phonetic classes in a speech
  signal</title><categories>cs.SD</categories><comments>12 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detection of transitions between broad phonetic classes in a speech signal is
an important problem which has applications such as landmark detection and
segmentation. The proposed hierarchical method detects silence to non-silence
transitions, high amplitude (mostly sonorants) to low ampli- tude (mostly
fricatives/affricates/stop bursts) transitions and vice-versa. A subset of the
extremum (minimum or maximum) samples between every pair of successive
zero-crossings is selected above a second pass threshold, from each bandpass
filtered speech signal frame. Relative to the mid-point (reference) of a frame,
locations of the first and the last extrema lie on either side, if the speech
signal belongs to a homogeneous segment; else, both these locations lie on the
left or the right side of the reference, indicating a transition frame. When
tested on the entire TIMIT database, of the transitions detected, 93.6% are
within a tolerance of 20 ms from the hand labeled boundaries. Sonorant,
unvoiced non-sonorant and silence classes and their respective onsets are
detected with an accuracy of about 83.5% for the same tolerance. The results
are as good as, and in some respects better than the state-of-the-art methods
for similar tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0372</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0372</id><created>2014-11-03</created><updated>2014-12-20</updated><authors><author><keyname>Tang</keyname><forenames>Zhu</forenames></author><author><keyname>Feng</keyname><forenames>Zhenqian</forenames></author><author><keyname>Yu</keyname><forenames>Wanrong</forenames></author><author><keyname>Zhao</keyname><forenames>Baokang</forenames></author><author><keyname>Wu</keyname><forenames>Chunqing</forenames></author></authors><title>Link Reassignment based Snapshot Partition for Polar-orbit LEO Satellite
  Networks</title><categories>cs.NI</categories><comments>6 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Snapshot is a fundamental notion proposed for routing in mobile low earth
orbit (LEO) satellite networks which is characterized with predictable topology
dynamics. Its distribution has a great impact on the routing performance and
on-board storage. Originally, the snapshot distribution is invariable by using
the static snapshot partition method based on the mechanical steering antenna.
Utilizing nowadays advanced phased-array antenna technology, we proposed a
quasi-dynamic snapshot partition method based on the inter-satellite links
(ISLs) reassignment for the polar-orbit LEO satellite networks. By steering the
inter-satellite antennas when snapshot switches, we can reassign the ISLs and
add available ISLs for a better following snapshot. Results show that our
method can gain more stable routing performance by obtaining constant snapshot
duration, constant ISL number and lower end to end delay. Especially for
Iridium system, our method can gain not only longer merged snapshot duration,
half of the snapshot number, but also higher utilization ratio of inter-plane
ISLs. Potentially, our method could be very useful for the Iridium-next system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0390</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0390</id><created>2014-11-03</created><updated>2015-03-02</updated><authors><author><keyname>Barletta</keyname><forenames>Luca</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author></authors><title>Upper Bound on the Capacity of Discrete-Time Wiener Phase Noise Channels</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure. To be presented at IEEE Inf. Theory Workshop (ITW)
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A discrete-time Wiener phase noise channel with an integrate-and-dump
multi-sample receiver is studied. An upper bound to the capacity with an
average input power constraint is derived, and a high signal-to-noise ratio
(SNR) analysis is performed. If the oversampling factor grows as
$\text{SNR}^\alpha$ for $0\le \alpha \le 1$, then the capacity pre-log is at
most $(1+\alpha)/2$ at high SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0392</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0392</id><created>2014-11-03</created><authors><author><keyname>Rajabi</keyname><forenames>Roozbeh</forenames></author><author><keyname>Ghassemian</keyname><forenames>Hassan</forenames></author></authors><title>Sparsity Constrained Graph Regularized NMF for Spectral Unmixing of
  Hyperspectral Data</title><categories>cs.CV</categories><comments>10 pages, Journal</comments><doi>10.1007/s12524-014-0408-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral images contain mixed pixels due to low spatial resolution of
hyperspectral sensors. Mixed pixels are pixels containing more than one
distinct material called endmembers. The presence percentages of endmembers in
mixed pixels are called abundance fractions. Spectral unmixing problem refers
to decomposing these pixels into a set of endmembers and abundance fractions.
Due to nonnegativity constraint on abundance fractions, nonnegative matrix
factorization methods (NMF) have been widely used for solving spectral unmixing
problem. In this paper we have used graph regularized NMF (GNMF) method
combined with sparseness constraint to decompose mixed pixels in hyperspectral
imagery. This method preserves the geometrical structure of data while
representing it in low dimensional space. Adaptive regularization parameter
based on temperature schedule in simulated annealing method also has been used
in this paper for the sparseness term. Proposed algorithm is applied on
synthetic and real datasets. Synthetic data is generated based on endmembers
from USGS spectral library. AVIRIS Cuprite dataset is used as real dataset for
evaluation of proposed method. Results are quantified based on spectral angle
distance (SAD) and abundance angle distance (AAD) measures. Results in
comparison with other methods show that the proposed method can unmix data more
effectively. Specifically for the Cuprite dataset, performance of the proposed
method is approximately 10% better than the VCA and Sparse NMF in terms of root
mean square of SAD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0401</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0401</id><created>2014-11-03</created><updated>2015-01-20</updated><authors><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author></authors><title>Four-Dimensional Coded Modulation with Bit-wise Decoders for Future
  Optical Communications</title><categories>cs.IT math.IT physics.optics</categories><comments>This is an updated and expanded version of arXiv:1310.4149</comments><doi>10.1109/JLT.2015.2396118</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coded modulation (CM) is the combination of forward error correction (FEC)
and multilevel constellations. Coherent optical communication systems result in
a four-dimensional (4D) signal space, which naturally leads to 4D-CM
transceivers. A practically attractive design paradigm is to use a bit-wise
decoder, where the detection process is (suboptimally) separated into two
steps: soft-decision demapping followed by binary decoding. In this paper,
bit-wise decoders are studied from an information-theoretic viewpoint. 4D
constellations with up to 4096 constellation points are considered. Metrics to
predict the post-FEC bit-error rate (BER) of bit-wise decoders are analyzed.
The mutual information is shown to fail at predicting the post-FEC BER of
bit-wise decoders and the so-called generalized mutual information is shown to
be a much more robust metric. For the suboptimal scheme under consideration, it
is also shown that constellations that transmit and receive information in each
polarization and quadrature independently (e.g., PM-QPSK, PM-16QAM, and
PM-64QAM) outperform the best 4D constellations designed for uncoded
transmission. Theoretical gains are as high as 4 dB, which are then validated
via numerical simulations of low-density parity check codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0402</identifier>
 <datestamp>2015-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0402</id><created>2014-11-03</created><updated>2015-05-26</updated><authors><author><keyname>Felsner</keyname><forenames>Stefan</forenames></author><author><keyname>Micek</keyname><forenames>Piotr</forenames></author><author><keyname>Ueckerdt</keyname><forenames>Torsten</forenames></author></authors><title>On-line coloring between two lines</title><categories>math.CO cs.CG</categories><comments>grant support added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study on-line colorings of certain graphs given as intersection graphs of
objects &quot;between two lines&quot;, i.e., there is a pair of horizontal lines such
that each object of the representation is a connected set contained in the
strip between the lines and touches both. Some of the graph classes admitting
such a representation are permutation graphs (segments), interval graphs
(axis-aligned rectangles), trapezoid graphs (trapezoids) and cocomparability
graphs (simple curves). We present an on-line algorithm coloring graphs given
by convex sets between two lines that uses $O(\omega^3)$ colors on graphs with
maximum clique size $\omega$.
  In contrast intersection graphs of segments attached to a single line may
force any on-line coloring algorithm to use an arbitrary number of colors even
when $\omega=2$.
  The {\em left-of} relation makes the complement of intersection graphs of
objects between two lines into a poset. As an aside we discuss the relation of
the class $\mathcal{C}$ of posets obtained from convex sets between two lines
with some other classes of posets: all $2$-dimensional posets and all posets of
height $2$ are in $\mathcal{C}$ but there is a $3$-dimensional poset of height
$3$ that does not belong to $\mathcal{C}$.
  We also show that the on-line coloring problem for curves between two lines
is as hard as the on-line chain partition problem for arbitrary posets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0406</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0406</id><created>2014-11-03</created><updated>2015-07-28</updated><authors><author><keyname>Bhardwaj</keyname><forenames>Arjun</forenames></author><author><keyname>Sangeetha</keyname></author></authors><title>GC-SROIQ(C) : Expressive Constraint Modelling and Grounded
  Circumscription for SROIQ</title><categories>cs.AI</categories><comments>The algorithm mentioned in this version was found to be incorrect.
  Further, the existing two authors would not be collaborating anymore, and
  when ready, Arjun Bhardwaj may submit a paper on related topics, which would
  not qualify as a revision, but would be a new paper/submission in its own
  right</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developments in semantic web technologies have promoted ontological encoding
of knowledge from diverse domains. However, modelling many practical domains
requires more expressive representations schemes than what the standard
description logics(DLs) support. We extend the DL SROIQ with constraint
networks and grounded circumscription. Applications of constraint modelling
include embedding ontologies with temporal or spatial information, while
grounded circumscription allows defeasible inference and closed world
reasoning. This paper overcomes restrictions on existing constraint modelling
approaches by introducing expressive constructs. Grounded circumscription
allows concept and role minimization and is decidable for DL. We provide a
general and intuitive algorithm for the framework of grounded circumscription
that can be applied to a whole range of logics. We present the resulting logic:
GC-SROIQ(C), and describe a tableau decision procedure for it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0416</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0416</id><created>2014-11-03</created><updated>2015-11-06</updated><authors><author><keyname>Meyer</keyname><forenames>Sebastian</forenames></author><author><keyname>Held</keyname><forenames>Leonhard</forenames></author><author><keyname>H&#xf6;hle</keyname><forenames>Michael</forenames></author></authors><title>Spatio-Temporal Analysis of Epidemic Phenomena Using the R Package
  surveillance</title><categories>stat.CO cs.CE physics.data-an stat.AP</categories><comments>53 pages, 20 figures, package homepage:
  http://surveillance.r-forge.r-project.org/</comments><msc-class>62-04</msc-class><acm-class>G.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The availability of geocoded health data and the inherent temporal structure
of communicable diseases have led to an increased interest in statistical
models and software for spatio-temporal data with epidemic features. The open
source R package surveillance can handle various levels of aggregation at which
infective events have been recorded: individual-level time-stamped
geo-referenced data (case reports) in either continuous space or discrete
space, as well as counts aggregated by period and region. For each of these
data types, the surveillance package implements tools for visualization,
likelihoood inference and simulation from recently developed statistical
regression frameworks capturing endemic and epidemic dynamics. Altogether, this
paper is a guide to the spatio-temporal modeling of epidemic phenomena,
exemplified by analyses of public health surveillance data on measles and
invasive meningococcal disease.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0435</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0435</id><created>2014-11-03</created><updated>2015-01-26</updated><authors><author><keyname>Wunder</keyname><forenames>Gerhard</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Strohmer</keyname><forenames>Thomas</forenames></author><author><keyname>Jung</keyname><forenames>Peter</forenames></author></authors><title>Sparse Signal Processing Concepts for Efficient 5G System Design</title><categories>cs.IT math.IT</categories><comments>18 pages, 5 figures, accepted for publication in IEEE Access</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As it becomes increasingly apparent that 4G will not be able to meet the
emerging demands of future mobile communication systems, the question what
could make up a 5G system, what are the crucial challenges and what are the key
drivers is part of intensive, ongoing discussions. Partly due to the advent of
compressive sensing, methods that can optimally exploit sparsity in signals
have received tremendous attention in recent years. In this paper we will
describe a variety of scenarios in which signal sparsity arises naturally in 5G
wireless systems. Signal sparsity and the associated rich collection of tools
and algorithms will thus be a viable source for innovation in 5G wireless
system design. We will discribe applications of this sparse signal processing
paradigm in MIMO random access, cloud radio access networks, compressive
channel-source network coding, and embedded security. We will also emphasize
important open problem that may arise in 5G system design, for which sparsity
will potentially play a key role in their solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0440</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0440</id><created>2014-11-03</created><updated>2016-02-14</updated><authors><author><keyname>Corneli</keyname><forenames>Joseph</forenames></author><author><keyname>Jordanous</keyname><forenames>Anna</forenames></author><author><keyname>Guckelsberger</keyname><forenames>Christian</forenames></author><author><keyname>Pease</keyname><forenames>Alison</forenames></author><author><keyname>Colton</keyname><forenames>Simon</forenames></author></authors><title>Modelling serendipity in a computational context</title><categories>cs.AI</categories><comments>32 pp., submitted to Minds and Machines</comments><acm-class>I.2.11, D.2.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Most prior work that deals with serendipity in a computing context focuses on
computational discovery: we argue that serendipity also includes an important
invention aspect. Building on a survey that describes serendipitous discovery
and invention in science and technology, we advance a definition of serendipity
and an accompanying model that can be used evaluate the potential for
serendipity in computational systems. The model adapts existing recommendations
for evaluating computational creativity. It is applied in three case studies
that evaluate the serendipity of existing and hypothetical systems in the
context of evolutionary computing, automated programming, and recommender
systems. From this analysis, we extract recommendations for practitioners
working with computational serendipity, and outline future directions for
research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0442</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0442</id><created>2014-11-03</created><authors><author><keyname>Gubbi</keyname><forenames>Abdullah</forenames></author><author><keyname>Azeem</keyname><forenames>Mohammad Fazle</forenames></author><author><keyname>Kumari</keyname><forenames>M Sharmila</forenames></author></authors><title>Non Binary Local Gradient Contours for Face Recognition</title><categories>cs.CV</categories><comments>12 pages</comments><journal-ref>International Journal of Information Processing, 8(3), 63-74, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the features from the traditional Local Binary Patterns (LBP) and Local
Directional Patterns (LDP) are found to be ineffective for face recognition, we
have proposed a new approach derived on the basis of Information sets whereby
the loss of information that occurs during the binarization is eliminated. The
information sets expand the scope of fuzzy sets by connecting the attribute and
the corresponding membership function value as a product. Since face is having
smooth texture in a limited area, the extracted features must be highly
discernible. To limit the number of features, we consider only the non
overlapping windows. By the application of the information set theory we can
reduce the number of feature of an image. The derived features are shown to
work fairly well over eigenface, fisherface and LBP methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0443</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0443</id><created>2014-11-03</created><updated>2015-03-12</updated><authors><author><keyname>Ordentlich</keyname><forenames>Or</forenames></author><author><keyname>Shayevitz</keyname><forenames>Ofer</forenames></author></authors><title>Subset-Universal Lossy Compression</title><categories>cs.IT math.IT</categories><comments>To be presented at the 2015 IEEE Information Theory Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A lossy source code $\mathcal{C}$ with rate $R$ for a discrete memoryless
source $S$ is called subset-universal if for every $0&lt;R'&lt; R$, almost every
subset of $2^{nR'}$ of its codewords achieves average distortion close to the
source's distortion-rate function $D(R')$. In this paper we prove the
asymptotic existence of such codes. Moreover, we show the asymptotic existence
of a code that is subset-universal with respect to all sources with the same
alphabet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0446</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0446</id><created>2014-11-03</created><updated>2014-11-06</updated><authors><author><keyname>Ghanem</keyname><forenames>Samah A. M.</forenames></author></authors><title>Multiple Access Gaussian Channels with Arbitrary Inputs: Optimal
  Precoding and Power Allocation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive new closed-form expressions for the gradient of the
mutual information with respect to arbitrary parameters of the two-user
multiple access channel (MAC). The derived relations generalize the fundamental
relation between the derivative of the mutual information and the minimum mean
squared error (MMSE) to multiuser setups. We prove that the derivative of the
mutual information with respect to the signal to noise ratio (SNR) is equal to
the MMSE plus a covariance induced due to the interference, quantified by a
term with respect to the cross correlation of the multiuser input estimates,
the channels and the precoding matrices. We also derive new relations for the
gradient of the conditional and non-conditional mutual information with respect
to the MMSE. Capitalizing on the new fundamental relations, we investigate the
linear precoding and power allocation policies that maximize the mutual
information for the two-user MAC Gaussian channels with arbitrary input
distributions. We show that the optimal design of linear precoders may satisfy
a fixed-point equation as a function of the channel and the input constellation
under specific setups. We show also that the non-mutual interference in a
multiuser setup introduces a term to the gradient of the mutual information
which plays a fundamental role in the design of optimal transmission
strategies, particularly the optimal precoding and power allocation, and
explains the losses in the data rates. Therefore, we provide a novel
interpretation of the interference with respect to the channel, power, and
input estimates of the main user and the interferer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0457</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0457</id><created>2014-11-03</created><updated>2015-03-12</updated><authors><author><keyname>Powell</keyname><forenames>Thomas</forenames></author></authors><title>Parametrised bar recursion: A unifying framework for realizability
  interpretations of classical dependent choice</title><categories>cs.LO math.LO</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the last twenty years or so a wide range of realizability
interpretations of classical analysis have been developed. In many cases, these
are achieved by extending the base interpreting system of primitive recursive
functionals with some form of bar recursion, which realizes the negative
translation of either countable or countable dependent choice. In this work we
present the many variants of bar recursion used in this context as
instantiations of a general, parametrised recursor, and give a uniform proof
that under certain conditions this recursor realizes a corresponding family of
parametrised dependent choice principles. From this proof, the soundness of
most of the existing bar recursive realizability interpretations of choice,
including those based on the Berardi-Bezem-Coquand functional, modified
realizability and the more recent products of selection functions of Escard\'o
and Oliva, follows as a simple corollary. We achieve not only a uniform
framework in which familiar realizability interpretations of choice can be
compared, but show that these represent just simple instances of a large family
of potential interpretations of dependent choice principles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0473</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0473</id><created>2014-11-03</created><authors><author><keyname>Morales</keyname><forenames>A. J</forenames></author><author><keyname>Borondo</keyname><forenames>J.</forenames></author><author><keyname>Losada</keyname><forenames>J. C.</forenames></author><author><keyname>Benito</keyname><forenames>R. M.</forenames></author></authors><title>Efficiency of Human Activity on Information Spreading on Twitter</title><categories>physics.soc-ph cs.SI</categories><comments>29 pages, 10 figures</comments><journal-ref>Social Networks, Volume 39, Pages 1 11, 2014</journal-ref><doi>10.1016/j.socnet.2014.03.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the collective reaction to individual actions is key to
effectively spread information in social media. In this work we define
efficiency on Twitter, as the ratio between the emergent spreading process and
the activity employed by the user. We characterize this property by means of a
quantitative analysis of the structural and dynamical patterns emergent from
human interactions, and show it to be universal across several Twitter
conversations. We found that some influential users efficiently cause
remarkable collective reactions by each message sent, while the majority of
users must employ extremely larger efforts to reach similar effects. Next we
propose a model that reproduces the retweet cascades occurring on Twitter to
explain the emergent distribution of the user efficiency. The model shows that
the dynamical patterns of the conversations are strongly conditioned by the
topology of the underlying network. We conclude that the appearance of a small
fraction of extremely efficient users results from the heterogeneity of the
followers network and independently of the individual user behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0481</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0481</id><created>2014-11-03</created><authors><author><keyname>Bagheri</keyname><forenames>Hamid</forenames></author></authors><title>Synthesis from Formal Partial Abstractions</title><categories>cs.SE</categories><comments>Hamid Bagheri, &quot;Synthesis from Formal Partial Abstractions,&quot; PhD
  thesis, University of Virginia, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing complex software systems is costly, time-consuming and
error-prone. Model- driven development (MDD) promises to improve software
productivity, timeliness, quality and cost through the transformation of
abstract application models to code-level implementations. However, it remains
unreasonably difficult to build the modeling languages and translators required
for software synthesis. This difficulty, in turns, limits the applicability of
MDD, and makes it hard to achieve reliability in MDD tools. This dissertation
research seeks to reduce the cost, broaden the applicability, and increase the
quality of model-driven development systems by embedding modeling languages
within established formal languages and by using the analyzers provided with
such languages for synthesis purposes to reduce the need for hand coding of
translators. This dissertation, in particular, explores the proposed approach
using relational logic as expressed in Alloy as the general specification
language, and the Alloy Analyzer as the general-purpose analyzer. Synthesis is
thus driven by finite-domain constraint satisfaction. One important aspect of
this work is its focus on partial specifications of particular aspects of the
system, such as application architectures and target platforms, and synthesis
of partial code bases from such specifications. Contributions of this work
include novel insights, methods and tools for (1) synthesizing architectural
models from abstract application models; (2) synthesizing partial,
platform-specific application frameworks from application architectures; and
(3) synthesizing object-relational mapping tradeoff spaces and database schemas
for database-backed object-oriented applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0482</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0482</id><created>2014-11-03</created><authors><author><keyname>Darabi</keyname><forenames>Mohammadreza</forenames></author></authors><title>Reduction of CRB in Arbitrary Pre-designed Arrays Using Alter an Element
  Position</title><categories>cs.IT math.IT</categories><comments>14 pages, Submitted to IEEE Journal on Selected Areas in
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous estimation of range and angle of close emitters usually requires
a multidimensional search. This paper offers an algorithm to improve the
position of an element of any array designed on the basis of some certain or
random rules. In the proposed method one element moves on its original
direction, i.e., keeping the vertical distance to each source, to reach the
constellation with less CRB. The performance of this method has been
demonstrated through simulation and a comparison of the CRB with receptive
signals covariance matrix determinant has been made before and after the use of
this method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0490</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0490</id><created>2014-11-03</created><updated>2015-03-30</updated><authors><author><keyname>Cerone</keyname><forenames>Andrea</forenames><affiliation>IMDEA Software Institute</affiliation></author><author><keyname>Hennessy</keyname><forenames>Matthew</forenames><affiliation>Trinity College Dublin</affiliation></author><author><keyname>Merro</keyname><forenames>Massimo</forenames><affiliation>Universit&#xe0; degli Studi di Verona</affiliation></author></authors><title>Modelling MAC-Layer Communications in Wireless Systems</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 1 (March 31,
  2015) lmcs:1138</journal-ref><doi>10.2168/LMCS-11(1:18)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a timed process calculus for modelling wireless networks in which
individual stations broadcast and receive messages; moreover the broadcasts are
subject to collisions. Based on a reduction semantics for the calculus we
define a contextual equivalence to compare the external behaviour of such
wireless networks. Further, we construct an extensional LTS (labelled
transition system) which models the activities of stations that can be directly
observed by the external environment. Standard bisimulations in this LTS
provide a sound proof method for proving systems contextually equivalence. We
illustrate the usefulness of the proof methodology by a series of examples.
Finally we show that this proof method is also complete, for a large class of
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0541</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0541</id><created>2014-11-03</created><authors><author><keyname>Mirzasoleiman</keyname><forenames>Baharan</forenames></author><author><keyname>Karbasi</keyname><forenames>Amin</forenames></author><author><keyname>Sarkar</keyname><forenames>Rik</forenames></author><author><keyname>Krause</keyname><forenames>Andreas</forenames></author></authors><title>Distributed Submodular Maximization</title><categories>cs.LG cs.AI cs.DC cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many large-scale machine learning problems -- clustering, non-parametric
learning, kernel machines, etc. -- require selecting a small yet representative
subset from a large dataset. Such problems can often be reduced to maximizing a
submodular set function subject to various constraints. Classical approaches to
submodular optimization require centralized access to the full dataset, which
is impractical for truly large-scale problems. In this paper, we consider the
problem of submodular function maximization in a distributed fashion. We
develop a simple, two-stage protocol GreeDi, that is easily implemented using
MapReduce style computations. We theoretically analyze our approach, and show
that under certain natural conditions, performance close to the centralized
approach can be achieved. We begin with monotone submodular maximization
subject to a cardinality constraint, and then extend this approach to obtain
approximation guarantees for (not necessarily monotone) submodular maximization
subject to more general constraints including matroid or knapsack
constraints.In our extensive experiments, we demonstrate the effectiveness of
our approach on several applications, including sparse Gaussian process
inference and exemplar based clustering on tens of millions of examples using
Hadoop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0544</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0544</id><created>2014-11-03</created><updated>2014-11-20</updated><authors><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author><author><keyname>Lingas</keyname><forenames>Andrzej</forenames></author><author><keyname>Sledneu</keyname><forenames>Dzmitry</forenames></author></authors><title>A QPTAS for the Base of the Number of Triangulations of a Planar Point
  Set</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of triangulations of a planar n point set is known to be $c^n$,
where the base $c$ lies between $2.43$ and $30.$ The fastest known algorithm
for counting triangulations of a planar n point set runs in $O^*(2^n)$ time.
The fastest known arbitrarily close approximation algorithm for the base of the
number of triangulations of a planar n point set runs in time subexponential in
$n.$ We present the first quasi-polynomial approximation scheme for the base of
the number of triangulations of a planar point set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0547</identifier>
 <datestamp>2015-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0547</id><created>2014-11-03</created><updated>2015-05-22</updated><authors><author><keyname>Puleo</keyname><forenames>Gregory J.</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>Correlation Clustering with Constrained Cluster Sizes and Extended
  Weights Bounds</title><categories>cs.LG cs.DS</categories><comments>17 pages, simplified the last section and fixed some other minor
  errors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of correlation clustering on graphs with constraints
on both the cluster sizes and the positive and negative weights of edges. Our
contributions are twofold: First, we introduce the problem of correlation
clustering with bounded cluster sizes. Second, we extend the regime of weight
values for which the clustering may be performed with constant approximation
guarantees in polynomial time and apply the results to the bounded cluster size
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0556</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0556</id><created>2014-11-03</created><authors><author><keyname>Momeni</keyname><forenames>Naghmeh</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael G.</forenames></author></authors><title>Measuring the Generalized Friendship Paradox in Networks with
  Quality-dependent Connectivity</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The friendship paradox is a sociological phenomenon stating that most people
have fewer friends than their friends do. The generalized friendship paradox
refers to the same observation for attributes other than degree, and it has
been observed in Twitter and scientific collaboration networks. This paper
takes an analytical approach to model this phenomenon. We consider a
preferential attachment-like network growth mechanism governed by both node
degrees and `qualities'. We introduce measures to quantify paradoxes, and
contrast the results obtained in our model to those obtained for an
uncorrelated network, where the degrees and qualities of adjacent nodes are
uncorrelated. We shed light on the effect of the distribution of node qualities
on the friendship paradox. We consider both the mean and the median to measure
paradoxes, and compare the results obtained by using these two statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0557</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0557</id><created>2014-11-03</created><authors><author><keyname>Saltz</keyname><forenames>Matthew</forenames></author><author><keyname>Prat-P&#xe8;rez</keyname><forenames>Arnau</forenames></author><author><keyname>Dominguez-Sal</keyname><forenames>David</forenames></author></authors><title>Distributed Community Detection with the WCC Metric</title><categories>cs.SI physics.soc-ph</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection has become an extremely active area of research in recent
years, with researchers proposing various new metrics and algorithms to address
the problem. Recently, the Weighted Community Clustering (WCC) metric was
proposed as a novel way to judge the quality of a community partitioning based
on the distribution of triangles in the graph, and was demonstrated to yield
superior results over other commonly used metrics like modularity. The same
authors later presented a parallel algorithm for optimizing WCC on large
graphs. In this paper, we propose a new distributed, vertex-centric algorithm
for community detection using the WCC metric. Results are presented that
demonstrate the algorithm's performance and scalability on up to 32 worker
machines and real graphs of up to 1.8 billion vertices. The algorithm scales
best with the largest graphs, and to our knowledge, it is the first distributed
algorithm for optimizing the WCC metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0582</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0582</id><created>2014-11-03</created><authors><author><keyname>Vitale</keyname><forenames>Jonathan</forenames></author><author><keyname>Williams</keyname><forenames>Mary-Anne</forenames></author><author><keyname>Johnston</keyname><forenames>Benjamin</forenames></author><author><keyname>Boccignone</keyname><forenames>Giuseppe</forenames></author></authors><title>Affective Facial Expression Processing via Simulation: A Probabilistic
  Model</title><categories>cs.CV</categories><comments>Annual International Conference on Biologically Inspired Cognitive
  Architectures - BICA 2014</comments><doi>10.1016/j.bica.2014.11.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the mental state of other people is an important skill for
intelligent agents and robots to operate within social environments. However,
the mental processes involved in `mind-reading' are complex. One explanation of
such processes is Simulation Theory - it is supported by a large body of
neuropsychological research. Yet, determining the best computational model or
theory to use in simulation-style emotion detection, is far from being
understood.
  In this work, we use Simulation Theory and neuroscience findings on
Mirror-Neuron Systems as the basis for a novel computational model, as a way to
handle affective facial expressions. The model is based on a probabilistic
mapping of observations from multiple identities onto a single fixed identity
(`internal transcoding of external stimuli'), and then onto a latent space
(`phenomenological response'). Together with the proposed architecture we
present some promising preliminary results
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0583</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0583</id><created>2014-11-03</created><updated>2015-10-01</updated><authors><author><keyname>Hoffmann</keyname><forenames>Philipp H. W.</forenames></author></authors><title>A Hitchhiker's Guide to Automatic Differentiation</title><categories>math.NA cs.NA</categories><comments>39 pages, 10 figures</comments><msc-class>65-02, 65K99</msc-class><journal-ref>Numerical Algorithms (2015)</journal-ref><doi>10.1007/s11075-015-0067-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article provides an overview of some of the mathematical principles of
Automatic Differentiation (AD). In particular, we summarise different
descriptions of the Forward Mode of AD, like the matrix-vector product based
approach, the idea of lifting functions to the algebra of dual numbers, the
method of Taylor series expansion on dual numbers and the application of the
push-forward operator, and explain why they all reduce to the same actual chain
of computations. We further give a short mathematical description of some
methods of higher-order Forward AD and, at the end of this paper, briefly
describe the Reverse Mode of Automatic Differentiation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0588</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0588</id><created>2014-11-03</created><authors><author><keyname>Borisova</keyname><forenames>Nadezhda</forenames></author><author><keyname>Iliev</keyname><forenames>Grigor</forenames></author><author><keyname>Karashtranova</keyname><forenames>Elena</forenames></author></authors><title>On Detecting Noun-Adjective Agreement Errors in Bulgarian Language Using
  GATE</title><categories>cs.CL</categories><comments>8 pages</comments><journal-ref>FMNS2013 (2013) 180-187</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this article, we describe an approach for automatic detection of
noun-adjective agreement errors in Bulgarian texts by explaining the necessary
steps required to develop a simple Java-based language processing application.
For this purpose, we use the GATE language processing framework, which is
capable of analyzing texts in Bulgarian language and can be embedded in
software applications, accessed through a set of Java APIs. In our example
application we also demonstrate how to use the functionality of GATE to perform
regular expressions over annotations for detecting agreement errors in simple
noun phrases formed by two words - attributive adjective and a noun, where the
attributive adjective precedes the noun. The provided code samples can also be
used as a starting point for implementing natural language processing
functionalities in software applications related to language processing tasks
like detection, annotation and retrieval of word groups meeting a specific set
of criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0591</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0591</id><created>2014-11-03</created><authors><author><keyname>Fisher</keyname><forenames>Charles K.</forenames></author><author><keyname>Mehta</keyname><forenames>Pankaj</forenames></author></authors><title>Bayesian feature selection with strongly-regularizing priors maps to the
  Ising Model</title><categories>cond-mat.stat-mech cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying small subsets of features that are relevant for prediction and/or
classification tasks is a central problem in machine learning and statistics.
The feature selection task is especially important, and computationally
difficult, for modern datasets where the number of features can be comparable
to, or even exceed, the number of samples. Here, we show that feature selection
with Bayesian inference takes a universal form and reduces to calculating the
magnetizations of an Ising model, under some mild conditions. Our results
exploit the observation that the evidence takes a universal form for
strongly-regularizing priors --- priors that have a large effect on the
posterior probability even in the infinite data limit. We derive explicit
expressions for feature selection for generalized linear models, a large class
of statistical techniques that include linear and logistic regression. We
illustrate the power of our approach by analyzing feature selection in a
logistic regression-based classifier trained to distinguish between the letters
B and D in the notMNIST dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0593</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0593</id><created>2014-11-03</created><authors><author><keyname>Kufleitner</keyname><forenames>Manfred</forenames></author><author><keyname>W&#xe4;chter</keyname><forenames>Jan Philipp</forenames></author></authors><title>Two-Variable Ehrenfeucht-Fraisse Games over Omega-Terms</title><categories>cs.LO cs.FL</categories><acm-class>F.4.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fragments of first-order logic over words can often be characterized in terms
of finite monoids, and identities of omega-terms are an effective mechanism for
specifying classes of monoids. Huschenbett and the first author have shown how
to use infinite Ehrenfeucht-Fraisse games on linear orders for showing that
some given fragment satisfies an identity of omega-terms (STACS 2014). After
revisiting this result, we show that for two-variable logic one can use simpler
linear orders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0594</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0594</id><created>2014-11-03</created><updated>2016-01-11</updated><authors><author><keyname>Ghanem</keyname><forenames>Samah A. M.</forenames></author></authors><title>Multi-Cell Processing with Limited Cooperation: A Novel Framework to
  Timely Designs and Reduced CSI Feedback with General Inputs</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the optimal power allocation and optimal precoding for a
multi-cell-processing (MCP) framework with limited cooperation. In particular,
we consider two base stations(BSs) which maximize the achievable rate for two
users connecting to each BS and sharing channel state information (CSI). We
propose a two way channel estimation or prediction process. Such framework has
promising outcomes in terms of feedback reduction and acheivable rates moving
the system from one with unkown CSI at the transmitter to a system with
instantanous CSI at both sides of the communication. We derive new extentions
of the fundamental relation between the gradient of the mutual information and
the MMSE for the conditional and non-conditional mutual information.
Capitalizing on such relations, we provide the optimal power allocation and
optimal precoding designs with respect to the estimated channel and MMSE. The
designs introduced are optimal for multiple access (MAC) Gaussian coherent
time-varying fading channels with general inputs and can be specialized to
multiple input multiple output (MIMO) channels by decoding interference. The
impact of interference on the capacity is quantified by the gradient of the
mutual information with respect to the power, channel, and error covariance of
the interferer. We provide two novel distributed MCP algorithms that provide
the solutions for the optimal power allocation and optimal precoding for the UL
and DL with a two way channel estimation to keep track of the channel
variations over blocks of data transmission. Therefore, we provide a novel
solution that allows with limited cooperation: a significant reduction in the
CSI feedback from the receiver to the transmitter, and timely optimal designs
of the precoding and power allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0602</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0602</id><created>2014-11-03</created><authors><author><keyname>Schelter</keyname><forenames>Sebastian</forenames></author><author><keyname>Satuluri</keyname><forenames>Venu</forenames></author><author><keyname>Zadeh</keyname><forenames>Reza</forenames></author></authors><title>Factorbird - a Parameter Server Approach to Distributed Matrix
  Factorization</title><categories>cs.LG</categories><comments>10 pages. Submitted to the NIPS 2014 Workshop on Distributed Matrix
  Computations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Factorbird, a prototype of a parameter server approach for
factorizing large matrices with Stochastic Gradient Descent-based algorithms.
We designed Factorbird to meet the following desiderata: (a) scalability to
tall and wide matrices with dozens of billions of non-zeros, (b) extensibility
to different kinds of models and loss functions as long as they can be
optimized using Stochastic Gradient Descent (SGD), and (c) adaptability to both
batch and streaming scenarios. Factorbird uses a parameter server in order to
scale to models that exceed the memory of an individual machine, and employs
lock-free Hogwild!-style learning with a special partitioning scheme to
drastically reduce conflicting updates. We also discuss other aspects of the
design of our system such as how to efficiently grid search for hyperparameters
at scale. We present experiments of Factorbird on a matrix built from a subset
of Twitter's interaction graph, consisting of more than 38 billion non-zeros
and about 200 million rows and columns, which is to the best of our knowledge
the largest matrix on which factorization results have been reported in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0610</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0610</id><created>2014-11-03</created><authors><author><keyname>Bapst</keyname><forenames>Victor</forenames></author><author><keyname>Coja-Oghlan</keyname><forenames>Amin</forenames></author><author><keyname>Efthymiou</keyname><forenames>Charilaos</forenames></author></authors><title>Planting colourings silently</title><categories>cs.DM math.CO math.PR</categories><msc-class>05C80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $k\geq3$ be a fixed integer and let $Z_k(G)$ be the number of
$k$-colourings of the graph $G$. For certain values of the average degree, the
random variable $Z_k(G(n,m))$ is known to be concentrated in the sense that
$\frac1n(\ln Z_k(G(n,m))-\ln E[Z_k(G(n,m))])$ converges to $0$ in probability
[Achlioptas and Coja-Oghlan: FOCS 2008]. In the present paper we prove a
significantly stronger concentration result. Namely, we show that for a wide
range of average degrees, $\frac1\omega(\ln Z_k(G(n,m))-\ln E[Z_k(G(n,m))])$
converges to $0$ in probability for any diverging function
$\omega=\omega(n)\to\infty$. For $k$ exceeding a certain constant $k_0$ this
result covers all average degrees up to the so-called condensation phase
transition, and this is best possible. As an application, we show that the
experiment of choosing a $k$-colouring of the random graph $G(n,m)$ uniformly
at random is contiguous with respect to the so-called &quot;planted model&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0622</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0622</id><created>2014-10-19</created><authors><author><keyname>Rahmani</keyname><forenames>Mostafa</forenames></author><author><keyname>Atia</keyname><forenames>George</forenames></author></authors><title>A Subspace Method for Array Covariance Matrix Estimation</title><categories>cs.NA cs.IT math.IT stat.AP</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a subspace method for the estimation of an array
covariance matrix. It is shown that when the received signals are uncorrelated,
the true array covariance matrices lie in a specific subspace whose dimension
is typically much smaller than the dimension of the full space. Based on this
idea, a subspace based covariance matrix estimator is proposed. The estimator
is obtained as a solution to a semi-definite convex optimization problem. While
the optimization problem has no closed-form solution, a nearly optimal
closed-form solution is proposed making it easy to implement. In comparison to
the conventional approaches, the proposed method yields higher estimation
accuracy because it eliminates the estimation error which does not lie in the
subspace of the true covariance matrices. The numerical examples indicate that
the proposed covariance matrix estimator can significantly improve the
estimation quality of the covariance matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0628</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0628</id><created>2014-11-03</created><updated>2015-05-08</updated><authors><author><keyname>Sopin</keyname><forenames>Valerii</forenames></author></authors><title>NL is not equal to NP and NP is not equal to EXPTIME</title><categories>cs.CC cs.DS</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in Corollary 1. There exists mistake in Corollary 7, 11</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn by the author due to a crucial sign error in
Corollary 1. There exists mistake in Corollary 7, 11. We show that NL is not
equal to NP and NP is not equal to EXPTIME. In addition, P = NP if and only if
P = PSPACE. And BPP = P implies NP = PSPACE, NP = PH, NP = co-NP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0630</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0630</id><created>2014-11-03</created><authors><author><keyname>Allahverdyan</keyname><forenames>Armen E.</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author></authors><title>Active Inference for Binary Symmetric Hidden Markov Models</title><categories>stat.ML cond-mat.dis-nn cs.IT cs.LG math.IT</categories><comments>9 pages, 3 figures</comments><doi>10.1007/s10955-015-1321-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider active maximum a posteriori (MAP) inference problem for Hidden
Markov Models (HMM), where, given an initial MAP estimate of the hidden
sequence, we select to label certain states in the sequence to improve the
estimation accuracy of the remaining states. We develop an analytical approach
to this problem for the case of binary symmetric HMMs, and obtain a closed form
solution that relates the expected error reduction to model parameters under
the specified active inference scheme. We then use this solution to determine
most optimal active inference scheme in terms of error reduction, and examine
the relation of those schemes to heuristic principles of uncertainty reduction
and solution unicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0644</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0644</id><created>2014-11-03</created><authors><author><keyname>Gr&#xf8;nlund</keyname><forenames>Allan</forenames></author><author><keyname>Larsen</keyname><forenames>Kasper Green</forenames></author></authors><title>Towards Tight Lower Bounds for Range Reporting on the RAM</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the orthogonal range reporting problem, we are to preprocess a set of $n$
points with integer coordinates on a $U \times U$ grid. The goal is to support
reporting all $k$ points inside an axis-aligned query rectangle. This is one of
the most fundamental data structure problems in databases and computational
geometry. Despite the importance of the problem its complexity remains
unresolved in the word-RAM. On the upper bound side, three best tradeoffs
exists: (1.) Query time $O(\lg \lg n + k)$ with $O(nlg^{\varepsilon}n)$ words
of space for any constant $\varepsilon&gt;0$. (2.) Query time $O((1 + k) \lg \lg
n)$ with $O(n \lg \lg n)$ words of space. (3.) Query time
$O((1+k)\lg^{\varepsilon} n)$ with optimal $O(n)$ words of space. However, the
only known query time lower bound is $\Omega(\log \log n +k)$, even for linear
space data structures.
  All three current best upper bound tradeoffs are derived by reducing range
reporting to a ball-inheritance problem. Ball-inheritance is a problem that
essentially encapsulates all previous attempts at solving range reporting in
the word-RAM. In this paper we make progress towards closing the gap between
the upper and lower bounds for range reporting by proving cell probe lower
bounds for ball-inheritance. Our lower bounds are tight for a large range of
parameters, excluding any further progress for range reporting using the
ball-inheritance reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0650</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0650</id><created>2014-11-03</created><authors><author><keyname>Ding</keyname><forenames>Jian</forenames></author><author><keyname>Sly</keyname><forenames>Allan</forenames></author><author><keyname>Sun</keyname><forenames>Nike</forenames></author></authors><title>Proof of the satisfiability conjecture for large k</title><categories>math.PR cs.DM math-ph math.MP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish the satisfiability threshold for random k-SAT for all k &gt;= k_0.
That is, there exists a limiting density alpha_s(k) such that a random k-SAT
formula of clause density alpha is with high probability satisfiable for alpha
&lt; alpha_s(k), and unsatisfiable for alpha &gt; alpha_s(k). The satisfiability
threshold alpha_s(k) is given explicitly by the one-step replica symmetry
breaking prediction from statistical physics. We believe that our methods may
apply to a range of random CSPs in the 1RSB universality class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0652</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0652</id><created>2014-11-03</created><authors><author><keyname>JafariAsbagh</keyname><forenames>Mohsen</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Varol</keyname><forenames>Onur</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author></authors><title>Clustering memes in social media streams</title><categories>cs.SI cs.CY cs.LG physics.soc-ph</categories><comments>25 pages, 8 figures, accepted on Social Network Analysis and Mining
  (SNAM). The final publication is available at Springer via
  http://dx.doi.org/10.1007/s13278-014-0237-x</comments><journal-ref>Social Network Analysis and Mining, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of clustering content in social media has pervasive applications,
including the identification of discussion topics, event detection, and content
recommendation. Here we describe a streaming framework for online detection and
clustering of memes in social media, specifically Twitter. A pre-clustering
procedure, namely protomeme detection, first isolates atomic tokens of
information carried by the tweets. Protomemes are thereafter aggregated, based
on multiple similarity measures, to obtain memes as cohesive groups of tweets
reflecting actual concepts or topics of discussion. The clustering algorithm
takes into account various dimensions of the data and metadata, including
natural language, the social network, and the patterns of information
diffusion. As a result, our system can build clusters of semantically,
structurally, and topically related tweets. The clustering process is based on
a variant of Online K-means that incorporates a memory mechanism, used to
&quot;forget&quot; old memes and replace them over time with the new ones. The evaluation
of our framework is carried out by using a dataset of Twitter trending topics.
Over a one-week period, we systematically determined whether our algorithm was
able to recover the trending hashtags. We show that the proposed method
outperforms baseline algorithms that only use content features, as well as a
state-of-the-art event detection method that assumes full knowledge of the
underlying follower network. We finally show that our online learning framework
is flexible, due to its independence of the adopted clustering algorithm, and
best suited to work in a streaming scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0654</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0654</id><created>2014-10-16</created><authors><author><keyname>Gonzalez-Granadillo</keyname><forenames>Gustavo</forenames><affiliation>Institut Mines-T&#xe9;l&#xe9;com, T&#xe9;l&#xe9;com SudParis, France</affiliation></author><author><keyname>Ponchel</keyname><forenames>Christophe</forenames><affiliation>Cassidian CyberSecurity, France</affiliation></author><author><keyname>Blanc</keyname><forenames>Gregory</forenames><affiliation>Institut Mines-T&#xe9;l&#xe9;com, T&#xe9;l&#xe9;com SudParis, France</affiliation></author><author><keyname>Debar</keyname><forenames>Herv&#xe9;</forenames><affiliation>Institut Mines-T&#xe9;l&#xe9;com, T&#xe9;l&#xe9;com SudParis, France</affiliation></author></authors><title>Combining Technical and Financial Impacts for Countermeasure Selection</title><categories>cs.CR cs.CY</categories><comments>In Proceedings AIDP 2014, arXiv:1410.3226</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 165, 2014, pp. 1-14</journal-ref><doi>10.4204/EPTCS.165.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research in information security has generally focused on providing a
comprehensive interpretation of threats, vulnerabilities, and attacks, in
particular to evaluate their danger and prioritize responses accordingly. Most
of the current approaches propose advanced techniques to detect intrusions and
complex attacks but few of these approaches propose well defined methodologies
to react against a given attack. In this paper, we propose a novel and
systematic method to select security countermeasures from a pool of candidates,
by ranking them based on the technical and financial impact associated to each
alternative. The method includes industrial evaluation and simulations of the
impact associated to a given security measure which allows to compute the
return on response investment for different candidates. A simple case study is
proposed at the end of the paper to show the applicability of the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0659</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0659</id><created>2014-11-03</created><updated>2015-10-29</updated><authors><author><keyname>Chistikov</keyname><forenames>Dmitry</forenames></author><author><keyname>Dimitrova</keyname><forenames>Rayna</forenames></author><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author></authors><title>Approximate Counting in SMT and Value Estimation for Probabilistic
  Programs</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  #SMT, or model counting for logical theories, is a well-known hard problem
that generalizes such tasks as counting the number of satisfying assignments to
a Boolean formula and computing the volume of a polytope. In the realm of
satisfiability modulo theories (SMT) there is a growing need for model counting
solvers, coming from several application domains (quantitative information
flow, static analysis of probabilistic programs). In this paper, we show a
reduction from an approximate version of #SMT to SMT.
  We focus on the theories of integer arithmetic and linear real arithmetic. We
propose model counting algorithms that provide approximate solutions with
formal bounds on the approximation error. They run in polynomial time and make
a polynomial number of queries to the SMT solver for the underlying theory,
exploiting &quot;for free&quot; the sophisticated heuristics implemented within modern
SMT solvers. We have implemented the algorithms and used them to solve the
value problem for a model of loop-free probabilistic programs with
nondeterminism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0698</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0698</id><created>2014-11-03</created><updated>2015-10-13</updated><authors><author><keyname>Fremont</keyname><forenames>Daniel J.</forenames></author><author><keyname>Donz&#xe9;</keyname><forenames>Alexandre</forenames></author><author><keyname>Seshia</keyname><forenames>Sanjit A.</forenames></author><author><keyname>Wessel</keyname><forenames>David</forenames></author></authors><title>Control Improvisation</title><categories>cs.FL</categories><comments>16 pages; full version of an FSTTCS 2015 paper</comments><acm-class>F.4.3; G.3; F.2.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We formalize and analyze a new automata-theoretic problem termed control
improvisation. Given an automaton, the problem is to produce an improviser, a
probabilistic algorithm that randomly generates words in its language, subject
to two additional constraints: the satisfaction of an admissibility predicate,
and the exhibition of a specified amount of randomness. Control improvisation
has multiple applications, including, for example, generating musical
improvisations that satisfy rhythmic and melodic constraints, where
admissibility is determined by some bounded divergence from a reference melody.
We analyze the complexity of the control improvisation problem, giving cases
where it is efficiently solvable and cases where it is #P-hard or undecidable.
We also show how symbolic techniques based on Boolean satisfiability (SAT)
solvers can be used to approximately solve some of the intractable cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0710</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0710</id><created>2014-11-03</created><authors><author><keyname>Stourm</keyname><forenames>Valeria</forenames></author><author><keyname>Bax</keyname><forenames>Eric</forenames></author></authors><title>Pigovian Taxes Can Increase Platform Competitiveness: The Case of Online
  Display Advertising</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A media platform's policy on obtrusive ads mediates an effectiveness-nuisance
tradeoff. Allowing more obtrusive advertising can increase the effectiveness of
ads, so the platform can elicit more short-term revenue from advertisers, but
the nuisance to viewers can decrease their engagement over time, which
decreases the platform's opportunity for future revenue. To optimize long-term
revenue, a platform can use a combination of advertiser bids and ad impact on
user experience to price and allocate ad space.
  We study the conditions for advertisers, viewers, and the platform to
simultaneously benefit from using ad impact on user experience as a criterion
for ad selection and pricing. It is important for advertisers to benefit,
because media platforms compete with one another for advertisers. Our results
show that platforms with more advertisers competing for ad space are more
likely to generate increased profits for themselves and their advertisers by
introducing ad impact on user experience as a factor in their auction
mechanisms. As a result, doing so can be a successful strategy in competition
against other platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0715</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0715</id><created>2014-11-03</created><authors><author><keyname>Fotouhi</keyname><forenames>Babak</forenames></author></authors><title>Beyond the Steady-State: Analytical Study of Network Growth at Arbitrary
  Times, for Arbitrary Initial Conditions</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In studying network growth, the conventional approach is to devise a growth
mechanism, quantify the evolution of a statistic or distribution (such as the
degree distribution), and then solve the equations in the steady state (the
infinite-size limit). Consequently, empirical studies also seek to verify the
steady-state prediction in real data. The caveat concomitant with confining the
analysis to this time regime is that no real system has infinite size; most
real growing networks are far from the steady state. This underlines the
importance of finite-size analysis. In this paper, we consider the
shifted-linear preferential attachment as an illustrative example of
arbitrary-time network growth analysis. We obtain the degree distribution for
arbitrary initial conditions at arbitrary times. We corroborate our theoretical
predictions with Monte Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0722</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0722</id><created>2014-11-03</created><authors><author><keyname>Fran&#xe7;a</keyname><forenames>Urbano</forenames></author><author><keyname>Sayama</keyname><forenames>Hiroki</forenames></author><author><keyname>McSwiggen</keyname><forenames>Colin</forenames></author><author><keyname>Daneshvar</keyname><forenames>Roozbeh</forenames></author><author><keyname>Bar-Yam</keyname><forenames>Yaneer</forenames></author></authors><title>Visualizing the &quot;Heartbeat&quot; of a City with Tweets</title><categories>physics.soc-ph cs.CY cs.SI</categories><comments>11 pages, 6 figures</comments><report-no>New England Complex Systems Institute Report 2014-11-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Describing the dynamics of a city is a crucial step to both understanding the
human activity in urban environments and to planning and designing cities
accordingly. Here we describe the collective dynamics of New York City and
surrounding areas as seen through the lens of Twitter usage. In particular, we
observe and quantify the patterns that emerge naturally from the hourly
activities in different areas of New York City, and discuss how they can be
used to understand the urban areas. Using a dataset that includes more than 6
million geolocated Twitter messages we construct a movie of the geographic
density of tweets. We observe the diurnal &quot;heartbeat&quot; of the NYC area. The
largest scale dynamics are the waking and sleeping cycle and commuting from
residential communities to office areas in Manhattan. Hourly dynamics reflect
the interplay of commuting, work and leisure, including whether people are
preoccupied with other activities or actively using Twitter. Differences
between weekday and weekend dynamics point to changes in when people wake and
sleep, and engage in social activities. We show that by measuring the average
distances to the heart of the city one can quantify the weekly differences and
the shift in behavior during weekends. We also identify locations and times of
high Twitter activity that occur because of specific activities. These include
early morning high levels of traffic as people arrive and wait at air
transportation hubs, and on Sunday at the Meadowlands Sports Complex and Statue
of Liberty. We analyze the role of particular individuals where they have large
impacts on overall Twitter activity. Our analysis points to the opportunity to
develop insight into both geographic social dynamics and attention through
social media analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0724</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0724</id><created>2014-11-03</created><updated>2015-02-17</updated><authors><author><keyname>Firer</keyname><forenames>Marcelo</forenames></author><author><keyname>Pinheiro</keyname><forenames>Jerry Anderson</forenames></author></authors><title>Bounds for complexity of syndrome decoding for poset metrics</title><categories>cs.IT math.IT</categories><comments>Submitted to ITW 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we show how to decompose a linear code relatively to any given
poset metric. We prove that the complexity of syndrome decoding is determined
by a maximal (primary) such decomposition and then show that a refinement of a
partial order leads to a refinement of the primary decomposition. Using this
and considering already known results about hierarchical posets, we can
establish upper and lower bounds for the complexity of syndrome decoding
relatively to a poset metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0728</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0728</id><created>2014-11-03</created><authors><author><keyname>Kalathil</keyname><forenames>Dileep</forenames></author><author><keyname>Borkar</keyname><forenames>Vivek</forenames></author><author><keyname>Jain</keyname><forenames>Rahul</forenames></author></authors><title>A Learning Scheme for Approachability in MDPs and Stackelberg Stochastic
  Games</title><categories>cs.LG cs.GT cs.SY math.OC</categories><comments>18 Pages, Submitted to Mathematics of Operations Research</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of approachability was introduced by Blackwell in the context of
vector-valued repeated games. The famous approachability theorem prescribes a
strategy for approachability, i.e., for `steering' the average vector-cost of a
given player towards a given target set, irrespective of the strategies of the
other players. In this paper, motivated from the multi-objective
optimization/decision making problems in dynamically changing environments, we
address the approachability problem in Markov Decision Processes (MDPs) and
Stackelberg stochastic games with vector-valued cost functions. We make two
main contributions. Firstly, we give simple and computationally tractable
strategy for approachability for MDPs and Stackelberg stochastic games.
Secondly, we give reinforcement learning algorithms to learn the approachable
strategy when the transition kernel is unknown. We also show that the
conditions that we give for approachability are both necessary and sufficient
for convex sets and thus a complete characterization. We also give sufficient
conditions for non-convex sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0729</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0729</id><created>2014-11-03</created><updated>2014-11-29</updated><authors><author><keyname>Chitambar</keyname><forenames>Eric</forenames></author><author><keyname>Hsieh</keyname><forenames>Min-Hsiu</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>The Private and Public Correlation Cost of Three Random Variables with
  Collaboration</title><categories>quant-ph cs.IT math.IT</categories><comments>Cleaned up a few details in the achievability proof</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of generating arbitrary three-party
correlations from a combination of public and secret correlations. Two parties
-- called Alice and Bob -- share perfectly correlated bits that are secret from
a collaborating third party, Charlie. At the same time, all three parties have
access to a separate source of correlated bits, and their goal is to convert
these two resources into multiple copies of some given tripartite distribution
$P_{XYZ}$. We obtain a single-letter characterization of the trade-off between
public and private bits that are needed to achieve this task. The rate of
private bits is shown to generalize Wyner's classic notion of common
information held between a pair of random variables. The problem we consider is
also closely related to the task of secrecy formation in which $P_{XYZ}$ is
generated using public communication and local randomness but with Charlie
functioning as an adversary instead of a collaborator. We describe in detail
the differences between the collaborative and adversarial scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0735</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0735</id><created>2014-11-03</created><updated>2015-04-28</updated><authors><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author><author><keyname>Tyagi</keyname><forenames>Himanshu</forenames></author><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author></authors><title>Secret Key Agreement: General Capacity and Second-Order Asymptotics</title><categories>cs.IT cs.CR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the problem of secret key agreement using interactive public
communication for two parties and propose a new secret key agreement protocol.
The protocol attains the secret key capacity for general observations and
attains the second-order asymptotic term in the maximum length of a secret key
for independent and identically distributed observations. In contrast to the
previously suggested secret key agreement protocols, the proposed protocol uses
interactive communication. In fact, the standard one-way communication protocol
used prior to this work fails to attain the asymptotic results above. Our
converse proofs rely on a recently established upper bound for secret key
lengths. Both our lower and upper bounds are derived in a single-shot setup and
the asymptotic results are obtained as corollaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0740</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0740</id><created>2014-11-03</created><updated>2014-11-17</updated><authors><author><keyname>Baghaie</keyname><forenames>Ahmadreza</forenames></author><author><keyname>D'souza</keyname><forenames>Roshan M.</forenames></author><author><keyname>Yu</keyname><forenames>Zeyun</forenames></author></authors><title>State-of-the-Art in Retinal Optical Coherence Tomography Image Analysis</title><categories>cs.CV</categories><comments>Added references, corrected typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical Coherence Tomography (OCT) is one of the most emerging imaging
modalities that has been used widely in the field of biomedical imaging. From
its emergence in 1990's, plenty of hardware and software improvements have been
made. Its applications range from ophthalmology to dermatology to coronary
imaging etc. Here, the focus is on applications of OCT in ophthalmology and
retinal imaging. OCT is able to non-invasively produce cross-sectional volume
images of the tissues which are further used for analysis of the tissue
structure and its properties. Due to the underlying physics, OCT images usually
suffer from a granular pattern, called speckle noise, which restricts the
process of interpretation, hence requiring specialized noise reduction
techniques to remove the noise while preserving image details. Also, given the
fact that OCT images are in the $\mu m$ -level, further analysis in needed to
distinguish between the different structures in the imaged volume. Therefore
the use of different segmentation techniques are of high importance. The
movement of the tissue under imaging or the progression of disease in the
tissue also imposes further implications both on the quality and the proper
interpretation of the acquired images. Thus, use of image registration
techniques can be very helpful. In this work, an overview of such image
analysis techniques will be given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0756</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0756</id><created>2014-11-03</created><authors><author><keyname>Zhang</keyname><forenames>Yan</forenames></author><author><keyname>Zhu</keyname><forenames>Zhaohui</forenames></author><author><keyname>Zhang</keyname><forenames>Jinjin</forenames></author></authors><title>Greatest solutions of equations in $\text{CLL}_R$ and its application</title><categories>cs.LO</categories><comments>13 pages. arXiv admin note: text overlap with arXiv:1301.3350</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the process calculus $\text{CLL}_R$ furtherly.
  First, we prove that for any equation $X=_{RS} t_X$ such that $X$ is strongly
guarded in $t_X$, $\langle X|X=t_X \rangle$ is the largest solution w.r.t
$\sqsubseteq_{RS}$.
  Second, we encode a fragment of action-based CTL in $\text{CLL}_R$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0763</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0763</id><created>2014-11-03</created><authors><author><keyname>Yang</keyname><forenames>Xu</forenames></author><author><keyname>Qiao</keyname><forenames>Hong</forenames></author><author><keyname>Liu</keyname><forenames>Zhi-Yong</forenames></author></authors><title>A Weighted Common Subgraph Matching Algorithm</title><categories>cs.DS cs.CV</categories><comments>6 pages, 5 figures, the second round revision in IEEE TNNLS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a weighted common subgraph (WCS) matching algorithm to find the
most similar subgraphs in two labeled weighted graphs. WCS matching, as a
natural generalization of the equal-sized graph matching or subgraph matching,
finds wide applications in many computer vision and machine learning tasks. In
this paper, the WCS matching is first formulated as a combinatorial
optimization problem over the set of partial permutation matrices. Then it is
approximately solved by a recently proposed combinatorial optimization
framework - Graduated NonConvexity and Concavity Procedure (GNCCP).
Experimental comparisons on both synthetic graphs and real world images
validate its robustness against noise level, problem size, outlier number, and
edge density.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0778</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0778</id><created>2014-11-03</created><authors><author><keyname>Huang</keyname><forenames>Xiaolei</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Liu</keyname><forenames>Tianli</forenames></author><author><keyname>Chiu</keyname><forenames>David</forenames></author><author><keyname>Zhu</keyname><forenames>Tingshao</forenames></author><author><keyname>Li</keyname><forenames>Xin</forenames></author></authors><title>Detecting Suicidal Ideation in Chinese Microblogs with Psychological
  Lexicons</title><categories>cs.CL</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Suicide is among the leading causes of death in China. However, technical
approaches toward preventing suicide are challenging and remaining under
development. Recently, several actual suicidal cases were preceded by users who
posted microblogs with suicidal ideation to Sina Weibo, a Chinese social media
network akin to Twitter. It would therefore be desirable to detect suicidal
ideations from microblogs in real-time, and immediately alert appropriate
support groups, which may lead to successful prevention. In this paper, we
propose a real-time suicidal ideation detection system deployed over Weibo,
using machine learning and known psychological techniques. Currently, we have
identified 53 known suicidal cases who posted suicide notes on Weibo prior to
their deaths.We explore linguistic features of these known cases using a
psychological lexicon dictionary, and train an effective suicidal Weibo post
detection model. 6714 tagged posts and several classifiers are used to verify
the model. By combining both machine learning and psychological knowledge, SVM
classifier has the best performance of different classifiers, yielding an
F-measure of 68:3%, a Precision of 78:9%, and a Recall of 60:3%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0782</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0782</id><created>2014-11-03</created><authors><author><keyname>Shin</keyname><forenames>Seung Woo</forenames></author><author><keyname>Thachuk</keyname><forenames>Chris</forenames></author><author><keyname>Winfree</keyname><forenames>Erik</forenames></author></authors><title>Verifying Chemical Reaction Network Implementations: A Pathway
  Decomposition Approach</title><categories>cs.CE cs.ET cs.LO q-bio.MN</categories><comments>21 pages, 4 figures. Appeared in VEMDP 2014, an affiliated workshop
  of CAV 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emerging fields of genetic engineering, synthetic biology, DNA computing,
DNA nanotechnology, and molecular programming herald the birth of a new
information technology that acquires information by directly sensing molecules
within a chemical environment, stores information in molecules such as DNA,
RNA, and proteins, processes that information by means of chemical and
biochemical transformations, and uses that information to direct the
manipulation of matter at the nanometer scale. To scale up beyond current
proof-of-principle demonstrations, new methods for managing the complexity of
designed molecular systems will need to be developed. Here we focus on the
challenge of verifying the correctness of molecular implementations of abstract
chemical reaction networks, where operation in a well-mixed &quot;soup&quot; of molecules
is stochastic, asynchronous, concurrent, and often involves multiple
intermediate steps in the implementation, parallel pathways, and side
reactions. This problem relates to the verification of Petri Nets, but existing
approaches are not sufficient for certain situations that commonly arise in
molecular implementations, such as what we call &quot;delayed choice.&quot; We formulate
a new theory of pathway decomposition that provides an elegant formal basis for
comparing chemical reaction network implementations, and we present an
algorithm that computes this basis. We further show how pathway decomposition
can be combined with weak bisimulation to handle a wider class that includes
all currently known enzyme-free DNA implementation techniques. We anticipate
that our notion of logical equivalence between chemical reaction network
implementations will be valuable for other molecular implementations such as
biochemical enzyme systems, and perhaps even more broadly in concurrency
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0784</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0784</id><created>2014-11-04</created><authors><author><keyname>Basurto</keyname><forenames>Rogelio</forenames></author><author><keyname>Le&#xf3;n</keyname><forenames>Paulina A.</forenames></author><author><keyname>Mart&#xed;nez</keyname><forenames>Genaro J.</forenames></author><author><keyname>Seck-Tuoh-Mora</keyname><forenames>Juan C.</forenames></author></authors><title>Logic gates and complex dynamics in a hexagonal cellular automaton: the
  Spiral rule</title><categories>nlin.CG cs.ET</categories><comments>20 pages, 14 figures, 6 tables</comments><journal-ref>Journal of Cellular Automata, 8(1-2), p. 53-71, 2013. URL:
  http://www.oldcitypublishing.com/journals/jca-home/jca-issue-contents/jca-volume-8-number-1-2-2013/jca-8-1-2-p-53-71/</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous works, hexagonal cellular automata (CA) have been studied as a
variation of the famous Game of Life CA, mainly for spiral phenomena
simulations; where the most interesting constructions are related to the
Belousov-Zhabotinsky reaction. In this paper, we analyse a special kind of
hexagonal CA, {\it Spiral rule}. Such automaton shows a non-trivial complex
behaviour related to discrete models of reaction-diffusion chemical media,
dominated by spiral guns which easily emerge from random initial conditions.
The computing capabilities of this automaton are shown by means of logic gates.
These are defined by collisions between mobile localizations. Also, an extended
classification of complex self-localisation patterns is presented, including
some self-organised patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0791</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0791</id><created>2014-11-04</created><authors><author><keyname>Liu</keyname><forenames>Xiao</forenames></author><author><keyname>Han</keyname><forenames>Congying</forenames></author><author><keyname>Guo</keyname><forenames>Tiande</forenames></author></authors><title>A Robust Point Sets Matching Method</title><categories>cs.CV</categories><comments>9 pages, 3 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Point sets matching method is very important in computer vision, feature
extraction, fingerprint matching, motion estimation and so on. This paper
proposes a robust point sets matching method. We present an iterative algorithm
that is robust to noise case. Firstly, we calculate all transformations between
two points. Then similarity matrix are computed to measure the possibility that
two transformation are both true. We iteratively update the matching score
matrix by using the similarity matrix. By using matching algorithm on graph, we
obtain the matching result. Experimental results obtained by our approach show
robustness to outlier and jitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0802</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0802</id><created>2014-11-04</created><authors><author><keyname>Ma</keyname><forenames>Lu</forenames></author><author><keyname>Ghafarianzadeh</keyname><forenames>Mahsa</forenames></author><author><keyname>Coleman</keyname><forenames>Dave</forenames></author><author><keyname>Correll</keyname><forenames>Nikolaus</forenames></author><author><keyname>Sibley</keyname><forenames>Gabe</forenames></author></authors><title>Simultaneous Localization, Mapping, and Manipulation for Unsupervised
  Object Discovery</title><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an unsupervised framework for simultaneous appearance-based object
discovery, detection, tracking and reconstruction using RGBD cameras and a
robot manipulator. The system performs dense 3D simultaneous localization and
mapping concurrently with unsupervised object discovery. Putative objects that
are spatially and visually coherent are manipulated by the robot to gain
additional motion-cues. The robot uses appearance alone, followed by structure
and motion cues, to jointly discover, verify, learn and improve models of
objects. Induced motion segmentation reinforces learned models which are
represented implicitly as 2D and 3D level sets to capture both shape and
appearance. We compare three different approaches for appearance-based object
discovery and find that a novel form of spatio-temporal super-pixels gives the
highest quality candidate object models in terms of precision and recall. Live
experiments with a Baxter robot demonstrate a holistic pipeline capable of
automatic discovery, verification, detection, tracking and reconstruction of
unknown objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0813</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0813</id><created>2014-11-04</created><authors><author><keyname>Bhardwaj</keyname><forenames>Arjun</forenames></author><author><keyname>Narayanaswamy</keyname><forenames>N. S.</forenames></author></authors><title>An Intuitive Procedure for Converting PDA to CFG, by Construction of
  Single State PDA</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present here the proof for an alternative procedure to convert a Push Down
Automata (PDA) into a Context Free Grammar (CFG). The procedure involves
intermediate conversion to a single state PDA. In view of the authors, this
conversion is conceptually intuitive and can serve as a teaching aid for the
relevant topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0814</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0814</id><created>2014-11-04</created><authors><author><keyname>Liu</keyname><forenames>Yiguang</forenames></author></authors><title>A random algorithm for low-rank decomposition of large-scale matrices
  with missing entries</title><categories>cs.NA cs.CV</categories><doi>10.1109/TIP.2015.2458176</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Random SubMatrix method (RSM) is proposed to calculate the low-rank
decomposition of large-scale matrices with known entry percentage \rho. RSM is
very fast as the floating-point operations (flops) required are compared
favorably with the state-of-the-art algorithms. Meanwhile RSM is very
memory-saving. With known entries homogeneously distributed in the given
matrix, sub-matrices formed by known entries are randomly selected. According
to the just proved theorem that subspace related to smaller singular values is
less perturbed by noise, the null vectors or the right singular vectors
associated with the minor singular values are calculated for each submatrix.
The vectors are the null vectors of the corresponding submatrix in the ground
truth of the given large-scale matrix. If enough sub-matrices are randomly
chosen, the low-rank decomposition is estimated. The experimental results on
random synthetical matrices with sizes such as 131072X1024 and on real data
sets indicate that RSM is much faster and memory-saving, and, meanwhile, has
considerable high precision achieving or approximating to the best.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0818</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0818</id><created>2014-11-04</created><authors><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author><author><keyname>Gingl</keyname><forenames>Zoltan</forenames></author><author><keyname>Mingesz</keyname><forenames>Robert</forenames></author><author><keyname>Vadai</keyname><forenames>Gergely</forenames></author><author><keyname>Smulko</keyname><forenames>Janusz</forenames></author><author><keyname>Granqvist</keyname><forenames>Claes-Goran</forenames></author></authors><title>Analysis of an attenuator artifact in an experimental attack by
  Gunn-Allison-Abbott against the Kirchhoff-law-Johnson-noise (KLJN) secure key
  exchange system</title><categories>cs.CR</categories><comments>Accepted for publication in Fluctuation and Noise Letters, on
  November 3, 2014</comments><journal-ref>Fluct. Noise Lett. 14 (2015) 1550011</journal-ref><doi>10.1142/S021947751550011X</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A recent paper by Gunn-Allison-Abbott (GAA) [L.J. Gunn et al., Scientific
Reports 4 (2014) 6461] argued that the Kirchhoff-law-Johnson-noise (KLJN)
secure key exchange system could experience a severe information leak. Here we
refute their results and demonstrate that GAA's arguments ensue from a serious
design flaw in their system. Specifically, an attenuator broke the single
Kirchhoff-loop into two coupled loops, which is an incorrect operation since
the single loop is essential for the security in the KLJN system, and hence
GAA's asserted information leak is trivial. Another consequence is that a fully
defended KLJN system would not be able to function due to its built-in
current-comparison defense against active (invasive) attacks. In this paper we
crack GAA's scheme via an elementary current comparison attack which yields
negligible error probability for Eve even without averaging over the
correlation time of the noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0821</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0821</id><created>2014-11-04</created><authors><author><keyname>Feige</keyname><forenames>Uriel</forenames></author></authors><title>NP-hardness of hypercube 2-segmentation</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hypercube 2-segmentation problem is a certain biclustering problem that
was previously claimed to be NP-hard, but for which there does not appear to be
a publicly available proof of NP-hardness. This manuscript provides such a
proof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0825</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0825</id><created>2014-11-04</created><updated>2014-12-19</updated><authors><author><keyname>Fang</keyname><forenames>Song</forenames></author></authors><title>Limitations of state estimation: absolute lower bound of minimum
  variance estimation/filtering, Gaussianity-whiteness measure (joint
  Shannon-Wiener entropy), and Gaussianing-whitening filter (maximum
  Gaussianity-whiteness measure principle)</title><categories>cs.SY cs.IT math.IT math.OC</categories><comments>This paper has been withdrawn by the author due to personal reasons</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at obtaining performance limitations of state estimation in
terms of variance minimization (minimum variance estimation and filtering)
using information theory. Two new notions, negentropy rate and
Gaussianity-whiteness measure (joint Shannon-Wiener entropy), are proposed to
facilitate the analysis. Topics such as Gaussianing-whitening filter (the
maximum Gaussianity-whiteness measure principle) are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0827</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0827</id><created>2014-11-04</created><authors><author><keyname>Lami</keyname><forenames>Gabriele</forenames></author><author><keyname>Cristoforetti</keyname><forenames>Marco</forenames></author><author><keyname>Jurman</keyname><forenames>Giuseppe</forenames></author><author><keyname>Furlanello</keyname><forenames>Cesare</forenames></author><author><keyname>Furlanello</keyname><forenames>Tommaso</forenames></author></authors><title>Entropy Dynamics of Community Alignment in the Italian Parliament
  Time-Dependent Network</title><categories>cs.SI math.DS physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex institutions are typically characterized by meso-scale structures
which are fundamental for the successful coordination of multiple agents. Here
we introduce a framework to study the temporal dynamics of the node-community
relationship based on the concept of community alignment, a measure derived
from the modularity matrix that defines the alignment of a node with respect to
the core of its community. The framework is applied to the 16th legislature of
the Italian Parliament to study the dynamic relationship in voting behavior
between Members of the Parliament (MPs) and their political parties. As a novel
contribution, we introduce two entropy-based measures that capture politically
interesting dynamics: the group alignment entropy (over a single snapshot), and
the node alignment entropy (over multiple snapshots). We show that significant
meso-scale changes in the time-dependent network structures can be detected by
a combination of the two measures. We observe a steady growth of the group
alignment entropy after a major internal conflict in the ruling majority and a
different distribution of nodes alignment entropy after the government
transition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0835</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0835</id><created>2014-11-04</created><authors><author><keyname>Randour</keyname><forenames>Mickael</forenames></author><author><keyname>Raskin</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Sankur</keyname><forenames>Ocan</forenames></author></authors><title>Variations on the Stochastic Shortest Path Problem</title><categories>cs.LO cs.FL cs.GT math.OC</categories><comments>Invited paper for VMCAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this invited contribution, we revisit the stochastic shortest path
problem, and show how recent results allow one to improve over the classical
solutions: we present algorithms to synthesize strategies with multiple
guarantees on the distribution of the length of paths reaching a given target,
rather than simply minimizing its expected value. The concepts and algorithms
that we propose here are applications of more general results that have been
obtained recently for Markov decision processes and that are described in a
series of recent papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0849</identifier>
 <datestamp>2014-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0849</id><created>2014-11-04</created><updated>2014-12-18</updated><authors><author><keyname>B&#xe4;uerle</keyname><forenames>Nicole</forenames></author><author><keyname>Gilitschenski</keyname><forenames>Igor</forenames></author><author><keyname>Hanebeck</keyname><forenames>Uwe D.</forenames></author></authors><title>Exact and Approximate Hidden Markov Chain Filters Based on Discrete
  Observations</title><categories>math.PR cs.SY q-fin.MF</categories><comments>Minor changes</comments><msc-class>62M20, 93E11</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a Hidden Markov Model (HMM) where the integrated continuous-time
Markov chain can be observed at discrete time points perturbed by a Brownian
motion. The aim is to derive a filter for the underlying continuous-time Markov
chain. The recursion formula for the discrete-time filter is easy to derive,
however involves densities which are very hard to obtain. In this paper we
derive exact formulas for the necessary densities in the case the state space
of the HMM consists of two elements only. This is done by relating the
underlying integrated continuous-time Markov chain to the so-called asymmetric
telegraph process and by using recent results on this process. In case the
state space consists of more than two elements we present three different ways
to approximate the densities for the filter. The first approach is based on the
continuous filter problem. The second approach is to derive a PDE for the
densities and solve it numerically and the third approach is a crude discrete
time approximation of the Markov chain. All three approaches are compared in a
numerical study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0851</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0851</id><created>2014-11-04</created><updated>2015-03-01</updated><authors><author><keyname>Haack</keyname><forenames>Christian</forenames><affiliation>aicas GmbH</affiliation></author><author><keyname>Huisman</keyname><forenames>Marieke</forenames><affiliation>University of Twente</affiliation></author><author><keyname>Hurlin</keyname><forenames>Cl&#xe9;ment</forenames><affiliation>Prove &amp; Run</affiliation></author><author><keyname>Amighi</keyname><forenames>Afshin</forenames><affiliation>University of Twente</affiliation></author></authors><title>Permission-Based Separation Logic for Multithreaded Java Programs</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 1 (February
  27, 2015) lmcs:998</journal-ref><doi>10.2168/LMCS-11(1:2)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a program logic for reasoning about multithreaded
Java-like programs with dynamic thread creation, thread joining and reentrant
object monitors. The logic is based on concurrent separation logic. It is the
first detailed adaptation of concurrent separation logic to a multithreaded
Java-like language. The program logic associates a unique static access
permission with each heap location, ensuring exclusive write accesses and
ruling out data races. Concurrent reads are supported through fractional
permissions. Permissions can be transferred between threads upon thread
starting, thread joining, initial monitor entrancies and final monitor exits.
In order to distinguish between initial monitor entrancies and monitor
reentrancies, auxiliary variables keep track of multisets of currently held
monitors. Data abstraction and behavioral subtyping are facilitated through
abstract predicates, which are also used to represent monitor invariants,
preconditions for thread starting and postconditions for thread joining.
Value-parametrized types allow to conveniently capture common strong global
invariants, like static object ownership relations. The program logic is
presented for a model language with Java-like classes and interfaces, the
soundness of the program logic is proven, and a number of illustrative examples
are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0860</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0860</id><created>2014-11-04</created><authors><author><keyname>Xu</keyname><forenames>Miao</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>CUR Algorithm for Partially Observed Matrices</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CUR matrix decomposition computes the low rank approximation of a given
matrix by using the actual rows and columns of the matrix. It has been a very
useful tool for handling large matrices. One limitation with the existing
algorithms for CUR matrix decomposition is that they need an access to the {\it
full} matrix, a requirement that can be difficult to fulfill in many real world
applications. In this work, we alleviate this limitation by developing a CUR
decomposition algorithm for partially observed matrices. In particular, the
proposed algorithm computes the low rank approximation of the target matrix
based on (i) the randomly sampled rows and columns, and (ii) a subset of
observed entries that are randomly sampled from the matrix. Our analysis shows
the relative error bound, measured by spectral norm, for the proposed algorithm
when the target matrix is of full rank. We also show that only $O(n r\ln r)$
observed entries are needed by the proposed algorithm to perfectly recover a
rank $r$ matrix of size $n\times n$, which improves the sample complexity of
the existing algorithms for matrix completion. Empirical studies on both
synthetic and real-world datasets verify our theoretical claims and demonstrate
the effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0861</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0861</id><created>2014-11-04</created><authors><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Huang</keyname><forenames>Xiaolei</forenames></author><author><keyname>Liu</keyname><forenames>Tianli</forenames></author><author><keyname>Chen</keyname><forenames>Zhenxiang</forenames></author><author><keyname>Zhu</keyname><forenames>Tingshao</forenames></author></authors><title>Using Linguistic Features to Estimate Suicide Probability of Chinese
  Microblog Users</title><categories>cs.SI cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If people with high risk of suicide can be identified through social media
like microblog, it is possible to implement an active intervention system to
save their lives. Based on this motivation, the current study administered the
Suicide Probability Scale(SPS) to 1041 weibo users at Sina Weibo, which is a
leading microblog service provider in China. Two NLP (Natural Language
Processing) methods, the Chinese edition of Linguistic Inquiry and Word Count
(LIWC) lexicon and Latent Dirichlet Allocation (LDA), are used to extract
linguistic features from the Sina Weibo data. We trained predicting models by
machine learning algorithm based on these two types of features, to estimate
suicide probability based on linguistic features. The experiment results
indicate that LDA can find topics that relate to suicide probability, and
improve the performance of prediction. Our study adds value in prediction of
suicidal probability of social network users with their behaviors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0863</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0863</id><created>2014-11-04</created><authors><author><keyname>Desai</keyname><forenames>Madhav</forenames></author></authors><title>Inner Loop Optimizations in Mapping Single Threaded Programs to Hardware</title><categories>cs.AR</categories><comments>8 pages double column</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of mapping high-level algorithms to hardware, we consider the
basic problem of generating an efficient hardware implementation of a single
threaded program, in particular, that of an inner loop. We describe a
control-flow mechanism which provides dynamic loop-pipelining capability in
hardware, so that multiple iterations of an arbitrary inner loop can be made
simultaneously active in the generated hardware, We study the impact of this
loop-pipelining scheme in conjunction with source-level loop-unrolling. In
particular, we apply this technique to some common loop kernels: regular
kernels such as the fast-fourier transform and matrix multiplication, as well
as an example of an inner loop whose body has branching. The resulting
resulting hardware descriptions are synthesized to an FPGA target, and then
characterized for performance and resource utilization. We observe that the use
of dynamic loop-pipelining mechanism alone typically results in a significant
improvements in the performance of the hardware. If the loop is statically
unrolled and if loop-pipelining is applied to the unrolled program, then the
performance improvement is still substantial. When dynamic loop pipelining is
used in conjunction with static loop unrolling, the improvement in performance
ranges from 6X to 20X (in terms of number of clock cycles needed for the
computation) across the loop kernels that we have studied. These optimizations
do have a hardware overhead, but, in spite of this, we observe that the joint
use of these loop optimizations not only improves performance, but also the
performance/cost ratio of the resulting hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0871</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0871</id><created>2014-11-04</created><authors><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>Wollan</keyname><forenames>Paul</forenames></author></authors><title>An exact characterization of tractable demand patterns for maximum
  disjoint path problems</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the following general disjoint paths problem: given a supply graph
$G$, a set $T\subseteq V(G)$ of terminals, a demand graph $H$ on the vertices
$T$, and an integer $k$, the task is to find a set of $k$ pairwise
vertex-disjoint valid paths, where we say that a path of the supply graph $G$
is valid if its endpoints are in $T$ and adjacent in the demand graph $H$. For
a class $\mathcal{H}$ of graphs, we denote by $\mathcal{H}$-Maximum Disjoint
Paths the restriction of this problem when the demand graph $H$ is assumed to
be a member of $\mathcal{H}$. We study the fixed-parameter tractability of this
family of problems, parameterized by $k$. Our main result is a complete
characterization of the fixed-parameter tractable cases of
$\mathcal{H}$-Maximum Disjoint Paths for every hereditary class $\mathcal{H}$
of graphs: it turns out that complexity depends on the existence of large
induced matchings and large induced skew bicliques in the demand graph $H$ (a
skew biclique is a bipartite graph on vertices $a_1$, $\dots$, $a_n$, $b_1$,
$\dots$, $b_n$ with $a_i$ and $b_j$ being adjacent if and only if $i\le j$).
Specifically, we prove the following classification for every hereditary class
$\mathcal{H}$.
  1. If $\mathcal{H}$ does not contain every matching and does not contain
every skew biclique, then $\mathcal{H}$-Maximum Disjoint Paths is FPT.
  2. If $\mathcal{H}$ does not contain every matching, but contains every skew
biclique, then $\mathcal{H}$-Maximum Disjoint Paths is W[1]-hard, admits an FPT
approximation, and the valid paths satisfy an analog of the Erd\H{o}s-P\'osa
property.
  3. If $\mathcal{H}$ contains every matching, then $\mathcal{H}$-Maximum
Disjoint Paths is W[1]-hard and the valid paths do not satisfy the analog of
the Erd\H{o}s-P\'osa property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0882</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0882</id><created>2014-11-04</created><updated>2014-11-19</updated><authors><author><keyname>Ibrahim</keyname><forenames>Sharif</forenames></author><author><keyname>Krishnamoorthy</keyname><forenames>Bala</forenames></author><author><keyname>Vixie</keyname><forenames>Kevin R.</forenames></author></authors><title>Flat Norm Decomposition of Integral Currents</title><categories>math.DG cs.CG math.AT</categories><comments>17 pages, corrections to section on least area problem</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currents represent generalized surfaces studied in geometric measure theory.
They range from relatively tame integral currents that represent oriented
manifolds with integer multiplicities and finite volume as well as boundary, to
arbitrary elements of the dual space of differential forms. The flat norm
provides a natural distance in the space of currents, and works by decomposing
a $d$-dimensional current into $d$- and (the boundary of) $(d+1)$-dimensional
pieces. A natural question about currents is the following. If the input is an
integral current, can its flat norm decomposition be integral as well? The
answer is not known in general, except in the case of $d$-currents that are
boundaries of $(d+1)$-currents in $\mathbb{R}^{d+1}$. Following correspondences
between the flat norm and $L^1$ total variation ($L^1$TV) of functionals, the
answer is affirmative in this case. On the other hand, for the discretization
of the flat norm on a finite simplicial complex, the analogous statement
remains true even when the inputs are not boundaries. This result is implied by
the boundary matrix of the simplicial complex being totally unimodular -- a
result distinct from the $L^1$TV approach. We develop an analysis framework
that extends the result in the simplicial setting to that for $d$-currents in
$\mathbb{R}^{d+1}$, provided a suitable triangulation result holds. Following
results of Shewchuk on triangulating planar straight line graphs while bounding
both the size and location of small angles, our framework shows that the
discrete result implies the continuous result for the case of $1$-currents in
$\mathbb{R}^2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0895</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0895</id><created>2014-11-04</created><authors><author><keyname>Lu</keyname><forenames>Liang</forenames></author><author><keyname>Renals</keyname><forenames>Steve</forenames></author></authors><title>Tied Probabilistic Linear Discriminant Analysis for Speech Recognition</title><categories>cs.CL cs.AI</categories><doi>10.1109/LSP.2014.2313410</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acoustic models using probabilistic linear discriminant analysis (PLDA)
capture the correlations within feature vectors using subspaces which do not
vastly expand the model. This allows high dimensional and correlated feature
spaces to be used, without requiring the estimation of multiple high dimension
covariance matrices. In this letter we extend the recently presented PLDA
mixture model for speech recognition through a tied PLDA approach, which is
better able to control the model size to avoid overfitting. We carried out
experiments using the Switchboard corpus, with both mel frequency cepstral
coefficient features and bottleneck feature derived from a deep neural network.
Reductions in word error rate were obtained by using tied PLDA, compared with
the PLDA mixture model, subspace Gaussian mixture models, and deep neural
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0906</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0906</id><created>2014-11-04</created><updated>2015-01-27</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>de Nooy</keyname><forenames>Wouter</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Prathap</keyname><forenames>Gangan</forenames></author></authors><title>The &quot;Tournaments&quot; Metaphor in Citation Impact Studies: Power-Weakness
  Ratios (PWR) as a Journal Indicator</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ramanujacharyulu's (1964) Power-Weakness Ratio (PWR) measures impact by
recursively multiplying the citation matrix by itself until convergence is
reached in both the cited and citing dimensions; the quotient of these values
is defined as PWR, whereby &quot;cited&quot; is considered as power and &quot;citing&quot; as
weakness. In this study, PWR is explained in relation to other size-independent
recursive metrics such as Pinski &amp; Narin's (1976) influence weights,
Bergstrom's (2007) Eigenfactor, Brin &amp; Page's (2001) PageRank, and Kleinberg's
(1999) Hubs-and-Authorities from the HITS algorithm. A test using the set of 83
journals in &quot;information and library science&quot; (according to the Web-of-Science
categorization) converged, but did not provide interpretable results. Further
decomposition of this set into homogeneous sub-graphs shows that--like most
other journal indicators--PWR can perhaps be used within homogeneous sets, but
not across citation communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0912</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0912</id><created>2014-11-04</created><authors><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>Akgun</keyname><forenames>Ozgur</forenames></author><author><keyname>Miguel</keyname><forenames>Ian</forenames></author><author><keyname>Thai</keyname><forenames>Long</forenames></author><author><keyname>Barker</keyname><forenames>Adam</forenames></author></authors><title>Cloud Benchmarking for Performance</title><categories>cs.DC cs.PF</categories><comments>6 pages, 6th IEEE International Conference on Cloud Computing
  Technology and Science (IEEE CloudCom) 2014, Singapore</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How can applications be deployed on the cloud to achieve maximum performance?
This question has become significant and challenging with the availability of a
wide variety of Virtual Machines (VMs) with different performance capabilities
in the cloud. The above question is addressed by proposing a six step
benchmarking methodology in which a user provides a set of four weights that
indicate how important each of the following groups: memory, processor,
computation and storage are to the application that needs to be executed on the
cloud. The weights along with cloud benchmarking data are used to generate a
ranking of VMs that can maximise performance of the application. The rankings
are validated through an empirical analysis using two case study applications;
the first is a financial risk application and the second is a molecular
dynamics simulation, which are both representative of workloads that can
benefit from execution on the cloud. Both case studies validate the feasibility
of the methodology and highlight that maximum performance can be achieved on
the cloud by selecting the top ranked VMs produced by the methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0921</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0921</id><created>2014-11-04</created><updated>2015-03-02</updated><authors><author><keyname>Glantz</keyname><forenames>Roland</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author><author><keyname>Noe</keyname><forenames>Alexander</forenames></author></authors><title>Algorithms for Mapping Parallel Processes onto Grid and Torus
  Architectures</title><categories>cs.DS</categories><comments>Accepted at PDP-2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Static mapping is the assignment of parallel processes to the processing
elements (PEs) of a parallel system, where the assignment does not change
during the application's lifetime. In our scenario we model an application's
computations and their dependencies by an application graph. This graph is
first partitioned into (nearly) equally sized blocks. These blocks need to
communicate at block boundaries. To assign the processes to PEs, our goal is to
compute a communication-efficient bijective mapping between the blocks and the
PEs.
  This approach of partitioning followed by bijective mapping has many degrees
of freedom. Thus, users and developers of parallel applications need to know
more about which choices work for which application graphs and which parallel
architectures. To this end, we not only develop new mapping algorithms (derived
from known greedy methods). We also perform extensive experiments involving
different classes of application graphs (meshes and complex networks),
architectures of parallel computers (grids and tori), as well as different
partitioners and mapping algorithms. Surprisingly, the quality of the
partitions, unless very poor, has little influence on the quality of the
mapping.
  More importantly, one of our new mapping algorithms always yields the best
results in terms of the quality measure maximum congestion when the application
graphs are complex networks. In case of meshes as application graphs, this
mapping algorithm always leads in terms of maximum congestion AND maximum
dilation, another common quality measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0928</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0928</id><created>2014-11-04</created><authors><author><keyname>Bensman</keyname><forenames>Stephen J.</forenames></author><author><keyname>Daugherty</keyname><forenames>Alice</forenames></author><author><keyname>Smolinsky</keyname><forenames>Lawrence J.</forenames></author><author><keyname>Sage</keyname><forenames>Daniel S.</forenames></author><author><keyname>Katz</keyname><forenames>J. Sylvan</forenames></author></authors><title>Power-law distributions, the h-index, and Google Scholar (GS) citations:
  a test of their relationship with economics Nobelists</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents proof that Google Scholar (GS) can construct documentary
sets relevant for evaluating researchers' works. Nobelists in economics were
the researchers under analysis, and two types of tests of the GS cites to their
works were performed: distributional and semantic. Distributional tests found
that the GS cites to the laureates' works conformed to the power-law model with
an asymptote or &quot;tail&quot; conterminous with their h-index demarcating their core
oeuvre, validating both GS and the h-index. Semantic tests revealed that their
works highest in GS cites were on topics for which they were awarded the prize.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0944</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0944</id><created>2014-11-04</created><authors><author><keyname>Freixas</keyname><forenames>Josep</forenames></author><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author></authors><title>The cost of getting local monotonicity</title><categories>cs.GT math.OC</categories><comments>26 pages, 2 figures, 1 table</comments><msc-class>91A12, 91A80, 91B12</msc-class><acm-class>G.1.6</acm-class><doi>10.1016/j.ejor.2015.11.030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manfred Holler introduced the Public Good index as a proposal to divide a
public good among players. In its unnormalized version, i.e., the raw measure,
it counts the number of times that a player belongs to a minimal winning
coalition. Unlike the Banzhaf index, it does not count the remaining winning
coalitions in which the player is crucial. Holler noticed that his index does
not satisfy local monotonicity, a fact that can be seen either as a major
drawback or as an advantage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0948</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0948</id><created>2014-11-04</created><updated>2015-09-15</updated><authors><author><keyname>Acan</keyname><forenames>Huseyin</forenames></author><author><keyname>Collevecchio</keyname><forenames>Andrea</forenames></author><author><keyname>Mehrabian</keyname><forenames>Abbas</forenames></author><author><keyname>Wormald</keyname><forenames>Nick</forenames></author></authors><title>On the push&amp;pull protocol for rumour spreading</title><categories>cs.DC math.PR</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The asynchronous push&amp;pull protocol, a randomized distributed algorithm for
spreading a rumour in a graph $G$, works as follows. Independent Poisson clocks
of rate 1 are associated with the vertices of $G$. Initially, one vertex of $G$
knows the rumour. Whenever the clock of a vertex $x$ rings, it calls a random
neighbour $y$: if $x$ knows the rumour and $y$ does not, then $x$ tells $y$ the
rumour (a push operation), and if $x$ does not know the rumour and $y$ knows
it, $y$ tells $x$ the rumour (a pull operation). The average spread time of $G$
is the expected time it takes for all vertices to know the rumour, and the
guaranteed spread time of $G$ is the smallest time $t$ such that with
probability at least $1-1/n$, after time $t$ all vertices know the rumour. The
synchronous variant of this protocol, in which each clock rings precisely at
times $1,2,\dots$, has been studied extensively. We prove the following results
for any $n$-vertex graph: In either version, the average spread time is at most
linear even if only the pull operation is used, and the guaranteed spread time
is within a logarithmic factor of the average spread time, so it is $O(n\log
n)$. In the asynchronous version, both the average and guaranteed spread times
are $\Omega(\log n)$. We give examples of graphs illustrating that these bounds
are best possible up to constant factors. We also prove theoretical
relationships between the guaranteed spread times in the two versions. Firstly,
in all graphs the guaranteed spread time in the asynchronous version is within
an $O(\log n)$ factor of that in the synchronous version, and this is tight.
Next, we find examples of graphs whose asynchronous spread times are
logarithmic, but the synchronous versions are polynomially large. Finally, we
show for any graph that the ratio of the synchronous spread time to the
asynchronous spread time is $O(n^{2/3})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0958</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0958</id><created>2014-11-04</created><updated>2015-10-12</updated><authors><author><keyname>Chehreghani</keyname><forenames>Morteza Haghir</forenames></author><author><keyname>Chehreghani</keyname><forenames>Mostafa Haghir</forenames></author></authors><title>Modeling Transitivity in Complex Networks</title><categories>cs.SI physics.soc-ph</categories><comments>16 pages, 4 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important source of high clustering coefficient in real-world networks is
transitivity. However, existing approaches for modeling transitivity suffer
from at least one of the following problems: i) they produce graphs from a
specific class like bipartite graphs, ii) they do not give an analytical
argument for the high clustering coefficient of the model, and iii) their
clustering coefficient is still significantly lower than real-world networks.
In this paper, we propose a new model for complex networks which is based on
adding transitivity to scale-free models. We theoretically analyze the model
and provide analytical arguments for its different properties. In particular,
we calculate a lower bound on the clustering coefficient of the model which is
independent of the network size, as seen in real-world networks. More than
theoretical analysis, the main properties of the model are evaluated
empirically and it is shown that the model can precisely simulate real-world
networks from different domains with and different specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0960</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0960</id><created>2014-11-04</created><updated>2015-01-14</updated><authors><author><keyname>Berndt</keyname><forenames>Sebastian</forenames></author><author><keyname>Jansen</keyname><forenames>Klaus</forenames></author><author><keyname>Klein</keyname><forenames>Kim-Manuel</forenames></author></authors><title>Fully Dynamic Bin Packing Revisited</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the fully dynamic bin packing problem, where items arrive and
depart in an online fashion and repacking of previously packed items is
allowed. The goal is, of course, to minimize both the number of bins used as
well as the amount of repacking. A recently introduced way of measuring the
repacking costs at each timestep is the migration factor, defined as the total
size of repacked items divided by the size of an arriving or departing item.
Concerning the trade-off between number of bins and migration factor, if we
wish to achieve an asymptotic competitive ration of $1 + \epsilon$ for the
number of bins, a relatively simple argument proves a lower bound of
$\Omega(\frac{1}{\epsilon})$ for the migration factor. We establish a nearly
matching upper bound of $O(\frac{1}{\epsilon}^4 \log \frac{1}{\epsilon})$ using
a new dynamic rounding technique and new ideas to handle small items in a
dynamic setting such that no amortization is needed. The running time of our
algorithm is polynomial in the number of items $n$ and in $\frac{1}{\epsilon}$.
The previous best trade-off was for an asymptotic competitive ratio of
$\frac{5}{4}$ for the bins (rather than $1+\epsilon$) and needed an amortized
number of $O(\log n)$ repackings (while in our scheme the number of repackings
is independent of $n$ and non-amortized).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0967</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0967</id><created>2014-11-02</created><authors><author><keyname>Jovanovic</keyname><forenames>Raka</forenames></author><author><keyname>Tuba</keyname><forenames>Milan</forenames></author><author><keyname>Voss</keyname><forenames>Stefan</forenames></author></authors><title>A Multi-Heuristic Approach for Solving the Pre-Marshalling Problem</title><categories>cs.AI cs.DM</categories><doi>10.1007/s10100-015-0410-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimizing the number of reshuffling operations at maritime container
terminals incorporates the Pre-Marshalling Problem (PMP) as an important
problem. Based on an analysis of existing solution approaches we develop new
heuristics utilizing specific properties of problem instances of the PMP. We
show that the heuristic performance is highly dependent on these properties. We
introduce a new method that exploits a greedy heuristic of four stages, where
for each of these stages several different heuristics may be applied. Instead
of using randomization to improve the performance of the heuristic, we
repetitively generate a number of solutions by using a combination of different
heuristics for each stage. In doing so, only a small number of solutions is
generated for which we intend that they do not have undesirable properties,
contrary to the case when simple randomization is used. Our experiments show
that such a deterministic algorithm significantly outperforms the original
nondeterministic method when the quality of found solutions is observed, with a
much lower number of generated solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0968</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0968</id><created>2014-11-04</created><authors><author><keyname>Dhuli</keyname><forenames>Sateeshkrishna</forenames></author><author><keyname>Gaurav</keyname><forenames>Kumar</forenames></author><author><keyname>Singh</keyname><forenames>Y. N.</forenames></author></authors><title>Convergence Analysis for Regular Wireless Consensus Networks</title><categories>cs.DC cs.SY</categories><comments>10 pages, 19 figures</comments><doi>10.1109/JSEN.2015.2420952</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Average consensus algorithms can be implemented over wireless sensor networks
(WSN), where global statistics can be computed using communications among
sensor nodes locally. Simple execution, robustness to global topology changes
due to frequent node failures and underlying distributed philosophy has made
consensus algorithms more suitable to WSNs. Since these algorithms are
iterative in nature, their performance is characterized by convergence speed.
We study the convergence of the average consensus algorithms for WSNs using
regular graphs. We obtained the analytical expressions for optimal consensus
and convergence parameters which decides the convergence time for r-nearest
neighbor cycle and torus networks. We have also derived the generalized
expression for optimal consensus and convergence parameters for m-dimensional
r-nearest neighbor torus networks. The obtained analytical results agree with
the simulation results and shown the effect of network dimension, number of
nodes and transmission radius on convergence time. This work provides the basic
analytical tools for managing and controlling the performance of average
consensus algorithm in the finite sized practical networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0969</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0969</id><created>2014-11-04</created><authors><author><keyname>Klus</keyname><forenames>Stefan</forenames></author><author><keyname>Sahai</keyname><forenames>Tuhin</forenames></author></authors><title>A Spectral Assignment Approach for the Graph Isomorphism Problem</title><categories>cs.DM</categories><msc-class>05C60, 05C50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a heuristic for the graph isomorphism problem that
is based on the eigendecomposition of the adjacency matrices. It is well known,
that the eigenvalues of the adjacency matrices of isomorphic graphs need to be
identical. However, two graphs $ G_A $ and $ G_B $ can be isospectral but
non-isomorphic. If the graphs possess repeated eigenvalues, which typically
correspond to graph symmetries, finding isomorphisms is much harder. By
repeatedly perturbing the adjacency matrices, it is possible to break
symmetries of the graphs without changing the isomorphism and to assign
vertices of $ G_A $ to vertices of $ G_B $, provided that an admissible
assignment exists. This heuristic approach can be used to construct a
permutation which transforms $ G_A $ into $ G_B $ if the graphs are isomorphic,
or to show that no isomorphism exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0972</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0972</id><created>2014-11-04</created><authors><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author><author><keyname>Becker</keyname><forenames>Stephen</forenames></author><author><keyname>Schmidt</keyname><forenames>Mark</forenames></author></authors><title>Convex Optimization for Big Data</title><categories>math.OC cs.LG stat.ML</categories><comments>23 pages, 4 figurs, 8 algorithms</comments><journal-ref>IEEE Signal Processing Magazine, Vol. 31(5), pages 32--43, 2014</journal-ref><doi>10.1109/MSP.2014.2329397</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article reviews recent advances in convex optimization algorithms for
Big Data, which aim to reduce the computational, storage, and communications
bottlenecks. We provide an overview of this emerging field, describe
contemporary approximation techniques like first-order methods and
randomization for scalability, and survey the important role of parallel and
distributed computation. The new Big Data algorithms are based on surprisingly
simple principles and attain staggering accelerations even on classical
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0973</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0973</id><created>2014-11-04</created><updated>2014-11-13</updated><authors><author><keyname>Hescott</keyname><forenames>Benjamin</forenames></author><author><keyname>Malchik</keyname><forenames>Caleb</forenames></author><author><keyname>Winslow</keyname><forenames>Andrew</forenames></author></authors><title>More Tight Bounds for Active Self-Assembly Using an Insertion Primitive</title><categories>cs.DS cs.ET cs.FL</categories><comments>A subset of the results appear in arXiv:1401.0359 and the proceedings
  of ESA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove several limits on the behavior of a model of self-assembling
particles introduced by Dabby and Chen (SODA 2013), called insertion systems,
where monomers insert themselves into the middle of a growing linear polymer.
First, we prove that the expressive power of these systems is equal to
context-free grammars, answering a question posed by Dabby and Chen.
  Second, we give tight bounds on the maximum length and minimum expected time
of constructed polymers in systems of three increasingly restricted classes. We
prove that systems of $k$ monomer types can deterministically construct
polymers of length $n = 2^{\Theta(k^{3/2})}$ in $O(\log^{5/3}(n))$ expected
time. We also prove that if non-deterministic construction of a finite number
of polymers is permitted, then the expected construction time can be reduced to
$O(\log^{3/2}(n))$ at the trade-off of decreasing the length to
$2^{\Theta(k)}$. If the system is allowed to construct an infinite number of
polymers, then constructing polymers of unbounded length in $O(\log{n})$
expected time is possible. We follow these positive results with a set of lower
bounds proving that these are the best possible polymer lengths and expected
construction times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0976</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0976</id><created>2014-11-04</created><updated>2015-04-17</updated><authors><author><keyname>Gyori</keyname><forenames>Benjamin M.</forenames></author><author><keyname>Paulin</keyname><forenames>Daniel</forenames></author><author><keyname>Palaniappan</keyname><forenames>Sucheendra K.</forenames></author></authors><title>Probabilistic verification of partially observable dynamical systems</title><categories>cs.SY cs.LO q-bio.QM</categories><comments>21 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The construction and formal verification of dynamical models is important in
engineering, biology and other disciplines. We focus on non-linear models
containing a set of parameters governing their dynamics. The value of these
parameters is often unknown and not directly observable through measurements,
which are themselves noisy. When treating parameters as random variables, one
can constrain their distribution by conditioning on observations and thereby
constructing a posterior probability distribution. We aim to perform model
verification with respect to this posterior. The main difficulty in performing
verification on a model under the posterior distribution is that in general, it
is difficult to obtain \emph{independent} samples from the posterior,
especially for non-linear dynamical models. Standard statistical model checking
methods require independent realizations of the system and are therefore not
applicable in this context.
  We propose a Markov chain Monte Carlo based statistical model checking
framework, which produces a sequence of dependent random realizations of the
model dynamics over the parameter posterior. Using this sequence of samples, we
use statistical hypothesis tests to verify whether the model satisfies a
bounded temporal logic property with a certain probability. We use sample size
bounds tailored to the setting of dependent samples for fixed sample size and
sequential tests. We apply our method to a case-study from the domain of
systems biology, to a model of the JAK-STAT biochemical pathway. The pathway is
modeled as a system of non-linear ODEs containing a set of unknown parameters.
Noisy, indirect observations of the system state are available from an
experiment. The results show that the proposed method enables probabilistic
verification with respect to the parameter posterior with specified error
bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0997</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0997</id><created>2014-11-04</created><authors><author><keyname>Eckman</keyname><forenames>Chad</forenames></author><author><keyname>Lindgren</keyname><forenames>Jonathan A.</forenames></author><author><keyname>Pearse</keyname><forenames>Erin P. J.</forenames></author><author><keyname>Sacco</keyname><forenames>David J.</forenames></author><author><keyname>Zhang</keyname><forenames>Zachariah</forenames></author></authors><title>Iterated geometric harmonics for data imputation and reconstruction of
  missing data</title><categories>cs.LG stat.ML</categories><comments>13 pages, 9 figures</comments><msc-class>Primary: 68Q32, 60J20, 42-04, 15-04, 62M20. Secondary: 68T10, 68T30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The method of geometric harmonics is adapted to the situation of incomplete
data by means of the iterated geometric harmonics (IGH) scheme. The method is
tested on natural and synthetic data sets with 50--500 data points and
dimensionality of 400--10,000. Experiments suggest that the algorithm converges
to a near optimal solution within 4--6 iterations, at runtimes of less than 30
minutes on a medium-grade desktop computer. The imputation of missing data
values is applied to collections of damaged images (suffering from data
annihilation rates of up to 70\%) which are reconstructed with a surprising
degree of accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.0998</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.0998</id><created>2014-11-04</created><updated>2014-11-05</updated><authors><author><keyname>Hsu</keyname><forenames>Justin</forenames></author><author><keyname>Huang</keyname><forenames>Zhiyi</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Wu</keyname><forenames>Zhiwei Steven</forenames></author></authors><title>Jointly Private Convex Programming</title><categories>cs.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an extremely general method for approximately
solving a large family of convex programs where the solution can be divided
between different agents, subject to joint differential privacy. This class
includes multi-commodity flow problems, general allocation problems, and
multi-dimensional knapsack problems, among other examples. The accuracy of our
algorithm depends on the \emph{number} of constraints that bind between
individuals, but crucially, is \emph{nearly independent} of the number of
primal variables and hence the number of agents who make up the problem. As the
number of agents in a problem grows, the error we introduce often becomes
negligible.
  We also consider the setting where agents are strategic and have preferences
over their part of the solution. For any convex program in this class that
maximizes \emph{social welfare}, there is a generic reduction that makes the
corresponding optimization \emph{approximately dominant strategy truthful} by
charging agents prices for resources as a function of the approximately optimal
dual variables, which are themselves computed under differential privacy. Our
results substantially expand the class of problems that are known to be
solvable under both privacy and incentive constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1000</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1000</id><created>2014-11-04</created><authors><author><keyname>Gustavson</keyname><forenames>Richard</forenames></author><author><keyname>Kondratieva</keyname><forenames>Marina</forenames></author><author><keyname>Ovchinnikov</keyname><forenames>Alexey</forenames></author></authors><title>New effective differential Nullstellensatz</title><categories>math.AC cs.SC math.AG math.CA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show new upper and lower bounds for the effective differential
Nullstellensatz for differential fields of characteristic zero with several
commuting derivations. Seidenberg was the first to address this problem in
1956, without giving a complete solution. The first explicit bounds appeared in
2009 in a paper by Golubitsky, Kondratieva, Szanto, and Ovchinnikov, with the
upper bound expressed in terms of the Ackermann function. D'Alfonso, Jeronimo,
and Solern\'o, using novel ideas, obtained in 2014 a new bound if restricted to
the case of one derivation and constant coefficients. To obtain the bound in
the present paper without this restriction, we extend this approach and use the
new methods of Freitag and Le\'on S\'anchez and of Pierce from 2014, which
represent a model-theoretic approach to differential algebraic geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1001</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1001</id><created>2014-11-04</created><updated>2015-02-15</updated><authors><author><keyname>Alistarh</keyname><forenames>Dan</forenames></author><author><keyname>Gelashvili</keyname><forenames>Rati</forenames></author><author><keyname>Vladu</keyname><forenames>Adrian</forenames></author></authors><title>How to Elect a Leader Faster than a Tournament</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of electing a leader from among $n$ contenders is one of the
fundamental questions in distributed computing. In its simplest formulation,
the task is as follows: given $n$ processors, all participants must eventually
return a win or lose indication, such that a single contender may win. Despite
a considerable amount of work on leader election, the following question is
still open: can we elect a leader in an asynchronous fault-prone system faster
than just running a $\Theta(\log n)$-time tournament, against a strong adaptive
adversary?
  In this paper, we answer this question in the affirmative, improving on a
decades-old upper bound. We introduce two new algorithmic ideas to reduce the
time complexity of electing a leader to $O(\log^* n)$, using $O(n^2)$
point-to-point messages. A non-trivial application of our algorithm is a new
upper bound for the tight renaming problem, assigning $n$ items to the $n$
participants in expected $O(\log^2 n)$ time and $O(n^2)$ messages. We
complement our results with lower bound of $\Omega(n^2)$ messages for solving
these two problems, closing the question of their message complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1006</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1006</id><created>2014-11-04</created><updated>2014-11-05</updated><authors><author><keyname>Dadashkarimi</keyname><forenames>Javid</forenames></author><author><keyname>Shakery</keyname><forenames>Azadeh</forenames></author><author><keyname>Faili</keyname><forenames>Heshaam</forenames></author></authors><title>A Probabilistic Translation Method for Dictionary-based Cross-lingual
  Information Retrieval in Agglutinative Languages</title><categories>cs.IR cs.CL</categories><comments>The 3rd conference of Computational Linguistic, Sharif University of
  Technology, November 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Translation ambiguity, out of vocabulary words and missing some translations
in bilingual dictionaries make dictionary-based Cross-language Information
Retrieval (CLIR) a challenging task. Moreover, in agglutinative languages which
do not have reliable stemmers, missing various lexical formations in bilingual
dictionaries degrades CLIR performance. This paper aims to introduce a
probabilistic translation model to solve the ambiguity problem, and also to
provide most likely formations of a dictionary candidate. We propose Minimum
Edit Support Candidates (MESC) method that exploits a monolingual corpus and a
bilingual dictionary to translate users' native language queries to documents'
language. Our experiments show that the proposed method outperforms
state-of-the-art dictionary-based English-Persian CLIR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1044</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1044</id><created>2014-11-04</created><updated>2015-04-22</updated><authors><author><keyname>Bu&#x161;i&#x107;</keyname><forenames>Ana</forenames></author><author><keyname>Meyn</keyname><forenames>Sean</forenames></author></authors><title>Optimization of Dynamic Matching Models</title><categories>cs.DM math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a dynamic matching model with random arrivals. In prior work,
authors have proposed policies that are stabilizing, and also policies that are
approximately finite-horizon optimal. This paper considers the infinite-horizon
average-cost optimal control problem.
  A relaxation of the stochastic control problem is proposed, which is found to
be a special case of an inventory model, as treated in the classical theory of
Clark and Scarf. The optimal policy for the relaxation admits a closed-form
expression. Based on the policy for this relaxation, a new matching policy is
proposed. For a parameterized family of models in which the network load
approaches capacity, this policy is shown to be approximately optimal, with
bounded regret, even though the average cost grows without bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1045</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1045</id><created>2014-11-04</created><updated>2015-04-09</updated><authors><author><keyname>K&#xfc;mmerer</keyname><forenames>Matthias</forenames></author><author><keyname>Theis</keyname><forenames>Lucas</forenames></author><author><keyname>Bethge</keyname><forenames>Matthias</forenames></author></authors><title>Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on
  ImageNet</title><categories>cs.CV q-bio.NC stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent results suggest that state-of-the-art saliency models perform far from
optimal in predicting fixations. This lack in performance has been attributed
to an inability to model the influence of high-level image features such as
objects. Recent seminal advances in applying deep neural networks to tasks like
object recognition suggests that they are able to capture this kind of
structure. However, the enormous amount of training data necessary to train
these networks makes them difficult to apply directly to saliency prediction.
We present a novel way of reusing existing neural networks that have been
pretrained on the task of object recognition in models of fixation prediction.
Using the well-known network of Krizhevsky et al. (2012), we come up with a new
saliency model that significantly outperforms all state-of-the-art models on
the MIT Saliency Benchmark. We show that the structure of this network allows
new insights in the psychophysics of fixation selection and potentially their
neural implementation. To train our network, we build on recent work on the
modeling of saliency as point processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1076</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1076</id><created>2014-11-04</created><authors><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Richard</keyname><forenames>Emile</forenames></author></authors><title>A statistical model for tensor PCA</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>Neural Information Processing Systems (NIPS) 2014 (slightly expanded:
  30 pages, 6 figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Principal Component Analysis problem for large tensors of
arbitrary order $k$ under a single-spike (or rank-one plus noise) model. On the
one hand, we use information theory, and recent results in probability theory,
to establish necessary and sufficient conditions under which the principal
component can be estimated using unbounded computational resources. It turns
out that this is possible as soon as the signal-to-noise ratio $\beta$ becomes
larger than $C\sqrt{k\log k}$ (and in particular $\beta$ can remain bounded as
the problem dimensions increase).
  On the other hand, we analyze several polynomial-time estimation algorithms,
based on tensor unfolding, power iteration and message passing ideas from
graphical models. We show that, unless the signal-to-noise ratio diverges in
the system dimensions, none of these approaches succeeds. This is possibly
related to a fundamental limitation of computationally tractable estimators for
this problem.
  We discuss various initializations for tensor power iteration, and show that
a tractable initialization based on the spectrum of the matricized tensor
outperforms significantly baseline methods, statistically and computationally.
Finally, we consider the case in which additional side information is available
about the unknown signal. We characterize the amount of side information that
allows the iterative algorithms to converge to a good estimate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1080</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1080</id><created>2014-11-02</created><authors><author><keyname>Jovanovic</keyname><forenames>Raka</forenames></author><author><keyname>Bousselham</keyname><forenames>Abdelkader</forenames></author><author><keyname>Voss</keyname><forenames>Stefan</forenames></author></authors><title>A Heuristic Method for Solving the Problem of Partitioning Graphs with
  Supply and Demand</title><categories>cs.AI</categories><doi>10.1007/s10479-015-1930-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a greedy algorithm for solving the problem of the
maximum partitioning of graphs with supply and demand (MPGSD). The goal of the
method is to solve the MPGSD for large graphs in a reasonable time limit. This
is done by using a two stage greedy algorithm, with two corresponding types of
heuristics. The solutions acquired in this way are improved by applying a
computationally inexpensive, hill climbing like, greedy correction procedure.
In our numeric experiments we analyze different heuristic functions for each
stage of the greedy algorithm, and show that their performance is highly
dependent on the properties of the specific instance. Our tests show that by
exploring a relatively small number of solutions generated by combining
different heuristic functions, and applying the proposed correction procedure
we can find solutions within only a few percent of the optimal ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1086</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1086</id><created>2014-11-04</created><authors><author><keyname>Ballardini</keyname><forenames>Augusto Luis</forenames></author><author><keyname>Fontana</keyname><forenames>Simone</forenames></author><author><keyname>Furlan</keyname><forenames>Axel</forenames></author><author><keyname>Sorrenti</keyname><forenames>Domenico G.</forenames></author></authors><title>ira_laser_tools: a ROS LaserScan manipulation toolbox</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Laser scanners are sensors of widespread use in robotic applications. Under
the Robot Operating System (ROS) the information generated by laser scanners
can be conveyed by either LaserScan messages or in the form of PointClouds.
Many publicly available algorithms (mapping, localization, navigation, etc.)
rely on LaserScan messages, yet a tool for handling multiple lasers, merging
their measurements, or to generate generic LaserScan messages from PointClouds,
is not available. This report describes two tools, in the form of ROS nodes,
which we release as open source under the BSD license, which allow to either
merge multiple single-plane laser scans or to generate virtual laser scans from
a point cloud. A short tutorial, along with the main advantages and limitations
of these tools are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1087</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1087</id><created>2014-11-04</created><authors><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Netrapalli</keyname><forenames>Praneeth</forenames></author></authors><title>Fast Exact Matrix Completion with Finite Samples</title><categories>cs.NA cs.DS cs.IT cs.LG math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix completion is the problem of recovering a low rank matrix by observing
a small fraction of its entries. A series of recent works [KOM12,JNS13,HW14]
have proposed fast non-convex optimization based iterative algorithms to solve
this problem. However, the sample complexity in all these results is
sub-optimal in its dependence on the rank, condition number and the desired
accuracy.
  In this paper, we present a fast iterative algorithm that solves the matrix
completion problem by observing $O(nr^5 \log^3 n)$ entries, which is
independent of the condition number and the desired accuracy. The run time of
our algorithm is $O(nr^7\log^3 n\log 1/\epsilon)$ which is near linear in the
dimension of the matrix. To the best of our knowledge, this is the first near
linear time algorithm for exact matrix completion with finite sample complexity
(i.e. independent of $\epsilon$).
  Our algorithm is based on a well known projected gradient descent method,
where the projection is onto the (non-convex) set of low rank matrices. There
are two key ideas in our result: 1) our argument is based on a $\ell_{\infty}$
norm potential function (as opposed to the spectral norm) and provides a novel
way to obtain perturbation bounds for it. 2) we prove and use a natural
extension of the Davis-Kahan theorem to obtain perturbation bounds on the best
low rank approximation of matrices with good eigen-gap. Both of these ideas may
be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1088</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1088</id><created>2014-11-04</created><authors><author><keyname>Gillenwater</keyname><forenames>Jennifer</forenames></author><author><keyname>Kulesza</keyname><forenames>Alex</forenames></author><author><keyname>Fox</keyname><forenames>Emily</forenames></author><author><keyname>Taskar</keyname><forenames>Ben</forenames></author></authors><title>Expectation-Maximization for Learning Determinantal Point Processes</title><categories>stat.ML cs.LG</categories><journal-ref>Neural Information Processing Systems (NIPS), 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A determinantal point process (DPP) is a probabilistic model of set diversity
compactly parameterized by a positive semi-definite kernel matrix. To fit a DPP
to a given task, we would like to learn the entries of its kernel matrix by
maximizing the log-likelihood of the available data. However, log-likelihood is
non-convex in the entries of the kernel matrix, and this learning problem is
conjectured to be NP-hard. Thus, previous work has instead focused on more
restricted convex learning settings: learning only a single weight for each row
of the kernel matrix, or learning weights for a linear combination of DPPs with
fixed kernel matrices. In this work we propose a novel algorithm for learning
the full kernel matrix. By changing the kernel parameterization from matrix
entries to eigenvalues and eigenvectors, and then lower-bounding the likelihood
in the manner of expectation-maximization algorithms, we obtain an effective
optimization procedure. We test our method on a real-world product
recommendation task, and achieve relative gains of up to 16.5% in test
log-likelihood compared to the naive approach of maximizing likelihood by
projected gradient ascent on the entries of the kernel matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1091</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1091</id><created>2014-11-04</created><authors><author><keyname>Long</keyname><forenames>Jonathan</forenames></author><author><keyname>Zhang</keyname><forenames>Ning</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>Do Convnets Learn Correspondence?</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural nets (convnets) trained from massive labeled datasets
have substantially improved the state-of-the-art in image classification and
object detection. However, visual understanding requires establishing
correspondence on a finer level than object category. Given their large pooling
regions and training from whole-image labels, it is not clear that convnets
derive their success from an accurate correspondence model which could be used
for precise localization. In this paper, we study the effectiveness of convnet
activation features for tasks requiring correspondence. We present evidence
that convnet features localize at a much finer scale than their receptive field
sizes, that they can be used to perform intraclass alignment as well as
conventional hand-engineered features, and that they outperform conventional
features in keypoint prediction on objects from PASCAL VOC 2011.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1098</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1098</id><created>2014-11-04</created><authors><author><keyname>Valles-Catala</keyname><forenames>Toni</forenames></author><author><keyname>Massucci</keyname><forenames>Francesco A.</forenames></author><author><keyname>Guimera</keyname><forenames>Roger</forenames></author><author><keyname>Sales-Pardo</keyname><forenames>Marta</forenames></author></authors><title>Multilayer stochastic block models reveal the multilayer structure of
  complex networks</title><categories>physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech cs.SI physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In complex systems, the network of interactions we observe between system's
components is the aggregate of the interactions that occur through different
mechanisms or layers. Recent studies reveal that the existence of multiple
interaction layers can have a dramatic impact in the dynamical processes
occurring on these systems. However, these studies assume that the interactions
between systems components in each one of the layers are known, while typically
for real-world systems we do not have that information. Here, we address the
issue of uncovering the different interaction layers from aggregate data by
introducing multilayer stochastic block models (SBMs), a generalization of
single-layer SBMs that considers different mechanisms of layer aggregation.
First, we find the complete probabilistic solution to the problem of finding
the optimal multilayer SBM for a given aggregate observed network. Because this
solution is computationally intractable, we propose an approximation that
enables us to verify that multilayer SBMs are more predictive of network
structure in real-world complex systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1101</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1101</id><created>2014-11-04</created><authors><author><keyname>Pfeiffer</keyname><forenames>Ryan</forenames></author></authors><title>The Decrits Consensus Algorithm: Decentralized Agreement without Proof
  of Work</title><categories>cs.CR</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decrits is a cryptocurrency in development that makes use of a novel
consensus algorithm that does not require proof-of-work. This paper describes
how the Decrits Consensus Algorithm (DCA) is as trustless as a proof-of-work
algorithm while offering superior transaction security at virtually no cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1102</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1102</id><created>2014-11-04</created><authors><author><keyname>Paikan</keyname><forenames>Ali</forenames></author><author><keyname>Tikhanoff</keyname><forenames>Vadim</forenames></author><author><keyname>Metta</keyname><forenames>Giorgio</forenames></author><author><keyname>Natale</keyname><forenames>Lorenzo</forenames></author></authors><title>Enhancing software module reusability using port plug-ins: an experiment
  with the iCub robot</title><categories>cs.RO cs.SE</categories><comments>Published on the Proceedings of IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systematically developing high--quality reusable software components is a
difficult task and requires careful design to find a proper balance between
potential reuse, functionalities and ease of implementation. Extendibility is
an important property for software which helps to reduce cost of development
and significantly boosts its reusability. This work introduces an approach to
enhance components reusability by extending their functionalities using
plug-ins at the level of the connection points (ports). Application--dependent
functionalities such as data monitoring and arbitration can be implemented
using a conventional scripting language and plugged into the ports of
components. The main advantage of our approach is that it avoids to introduce
application--dependent modifications to existing components, thus reducing
development time and fostering the development of simpler and therefore more
reusable components. Another advantage of our approach is that it reduces
communication and deployment overheads as extra functionalities can be added
without introducing additional modules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1108</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1108</id><created>2014-11-04</created><authors><author><keyname>Antanas</keyname><forenames>Laura</forenames></author><author><keyname>Moreno</keyname><forenames>Plinio</forenames></author><author><keyname>Neumann</keyname><forenames>Marion</forenames></author><author><keyname>de Figueiredo</keyname><forenames>Rui Pimentel</forenames></author><author><keyname>Kersting</keyname><forenames>Kristian</forenames></author><author><keyname>Santos-Victor</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>De Raedt</keyname><forenames>Luc</forenames></author></authors><title>High-level Reasoning and Low-level Learning for Grasping: A
  Probabilistic Logic Pipeline</title><categories>cs.RO</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  While grasps must satisfy the grasping stability criteria, good grasps depend
on the specific manipulation scenario: the object, its properties and
functionalities, as well as the task and grasp constraints. In this paper, we
consider such information for robot grasping by leveraging manifolds and
symbolic object parts. Specifically, we introduce a new probabilistic logic
module to first semantically reason about pre-grasp configurations with respect
to the intended tasks. Further, a mapping is learned from part-related visual
features to good grasping points. The probabilistic logic module makes use of
object-task affordances and object/task ontologies to encode rules that
generalize over similar object parts and object/task categories. The use of
probabilistic logic for task-dependent grasping contrasts with current
approaches that usually learn direct mappings from visual perceptions to
task-dependent grasping points. We show the benefits of the full probabilistic
logic pipeline experimentally and on a real robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1112</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1112</id><created>2014-11-04</created><authors><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author></authors><title>Learning of Agent Capability Models with Applications in Multi-agent
  Planning</title><categories>cs.AI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One important challenge for a set of agents to achieve more efficient
collaboration is for these agents to maintain proper models of each other. An
important aspect of these models of other agents is that they are often partial
and incomplete. Thus far, there are two common representations of agent models:
MDP based and action based, which are both based on action modeling. In many
applications, agent models may not have been given, and hence must be learnt.
While it may seem convenient to use either MDP based or action based models for
learning, in this paper, we introduce a new representation based on capability
models, which has several unique advantages. First, we show that learning
capability models can be performed efficiently online via Bayesian learning,
and the learning process is robust to high degrees of incompleteness in plan
execution traces (e.g., with only start and end states). While high degrees of
incompleteness in plan execution traces presents learning challenges for MDP
based and action based models, capability models can still learn to {\em
abstract} useful information out of these traces. As a result, capability
models are useful in applications in which such incompleteness is common, e.g.,
robot learning human model from observations and interactions. Furthermore,
when used in multi-agent planning (with each agent modeled separately),
capability models provide flexible abstraction of actions. The limitation,
however, is that the synthesized plan is incomplete and abstract.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1119</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1119</id><created>2014-11-04</created><updated>2014-11-11</updated><authors><author><keyname>Liu</keyname><forenames>Xianghang</forenames></author><author><keyname>Domke</keyname><forenames>Justin</forenames></author></authors><title>Projecting Markov Random Field Parameters for Fast Mixing</title><categories>cs.LG stat.ML</categories><comments>Neural Information Processing Systems 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov chain Monte Carlo (MCMC) algorithms are simple and extremely powerful
techniques to sample from almost arbitrary distributions. The flaw in practice
is that it can take a large and/or unknown amount of time to converge to the
stationary distribution. This paper gives sufficient conditions to guarantee
that univariate Gibbs sampling on Markov Random Fields (MRFs) will be fast
mixing, in a precise sense. Further, an algorithm is given to project onto this
set of fast-mixing parameters in the Euclidean norm. Following recent work, we
give an example use of this to project in various divergence measures,
comparing univariate marginals obtained by sampling after projection to common
variational methods and Gibbs sampling on the original parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1122</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1122</id><created>2014-11-04</created><authors><author><keyname>Furcy</keyname><forenames>David</forenames></author><author><keyname>Micka</keyname><forenames>Samuel</forenames></author><author><keyname>Summers</keyname><forenames>Scott M.</forenames></author></authors><title>Optimal program-size complexity for self-assembly at temperature 1 in 3D</title><categories>cs.CG cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Working in a three-dimensional variant of Winfree's abstract Tile Assembly
Model, we show that, for all $N \in \mathbb{N}$, there is a tile set that
uniquely self-assembles into an $N \times N$ square shape at temperature 1 with
optimal program-size complexity of $O(\log N / \log \log N)$ (the program-size
complexity, also known as tile complexity, of a shape is the minimum number of
unique tile types required to uniquely self-assemble it). Moreover, our
construction is &quot;just barely&quot; 3D in the sense that it works even when the
placement of tiles is restricted to the $z = 0$ and $z = 1$ planes. This result
affirmatively answers an open question from Cook, Fu, Schweller (SODA 2011). To
achieve this result, we develop a general 3D temperature 1 optimal encoding
construction, reminiscent of the 2D temperature 2 optimal encoding construction
of Soloveichik and Winfree (SICOMP 2007), and perhaps of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1124</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1124</id><created>2014-11-04</created><authors><author><keyname>Allen-Zhu</keyname><forenames>Zeyuan</forenames></author><author><keyname>Orecchia</keyname><forenames>Lorenzo</forenames></author></authors><title>Nearly-Linear Time Packing and Covering LP Solver with Faster
  Convergence Rate Than $O(1/\varepsilon^2)$</title><categories>cs.DS cs.NA math.NA math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Packing and covering linear programs (LP) are an important class of problems
that bridges computer science, operation research, and optimization. Efficient
algorithms for solving such LPs have received significant attention in the past
20 years [LN93, PST95, BBR97, You01, Nem04, BI04, BBR04, Nes05, AK08, AHK12,
KY13, You14, AO15]. Unfortunately, all known nearly-linear time algorithms for
producing $(1+\varepsilon)$-approximate solutions to packing and covering LPs
have a running time dependence that is at least proportional to
$\varepsilon^{-2}$. This is also known as an $O(1/\sqrt{T})$ convergence rate
and is particularly poor in many applications.
  In this paper, we leverage insights from optimization theory to break this
longstanding barrier. Our algorithms solve the packing LP in time $\tilde{O}(N
\varepsilon^{-1})$ and the covering LP in time $\tilde{O}(N
\varepsilon^{-1.5})$. At high level, they can be described as linear couplings
of several first-order descent steps. This is the first application of our
linear coupling technique (see [AO14]) to problems that are not amenable to
blackbox applications known iterative algorithms in convex optimization. Our
work also introduces a sequence of new techniques, including the stochastic and
the non-symmetric execution of gradient truncation operations, which may be of
independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1125</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1125</id><created>2014-11-04</created><authors><author><keyname>Xu</keyname><forenames>S.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Poor</keyname><forenames>H. V.</forenames></author></authors><title>Distributed Low-Rank Estimation Based on Joint Iterative Optimization in
  Wireless Sensor Networks</title><categories>cs.IT cs.LG math.IT</categories><comments>5 figures, 6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel distributed reduced--rank scheme and an adaptive
algorithm for distributed estimation in wireless sensor networks. The proposed
distributed scheme is based on a transformation that performs dimensionality
reduction at each agent of the network followed by a reduced-dimension
parameter vector. A distributed reduced-rank joint iterative estimation
algorithm is developed, which has the ability to achieve significantly reduced
communication overhead and improved performance when compared with existing
techniques. Simulation results illustrate the advantages of the proposed
strategy in terms of convergence rate and mean square error performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1126</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1126</id><created>2014-11-04</created><authors><author><keyname>Fang</keyname><forenames>Rui</forenames></author><author><keyname>Huang</keyname><forenames>Zequn</forenames></author><author><keyname>Rossi</keyname><forenames>Louis F.</forenames></author><author><keyname>Shen</keyname><forenames>Chien-Chung</forenames></author></authors><title>Probabilistic Modeling of IEEE 802.11 Distributed Coordination Functions</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and analyze a new Markov model of the IEEE 802.11 Distributed
Coordination Function (DCF) for wireless networks. The new model is derived
from a detailed DCF description where transition probabilities are determined
by precise estimates of collision probabilities based on network topology and
node states. For steady state calculations, we approximate joint probabilities
from marginal probabilities using product approximations. To assess the quality
of the model, we compare detailed equilibrium node states with results from
realistic simulations of wireless networks. We find very close correspondence
between the model and the simulations in a variety of representative network
topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1127</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1127</id><created>2014-11-04</created><authors><author><keyname>Christiano</keyname><forenames>Paul</forenames></author></authors><title>Provably Manipulation-Resistant Reputation Systems</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a community of users who must make periodic decisions about
whether to interact with one another. We propose a protocol which allows honest
users to reliably interact with each other, while limiting the damage done by
each malicious or incompetent user. The worst-case cost per user is sublinear
in the average number of interactions per user and is independent of the number
of users. Our guarantee holds simultaneously for every group of honest users.
For example, multiple groups of users with incompatible tastes or preferences
can coexist.
  As a motivating example, we consider a game where players have periodic
opportunities to do one another favors but minimal ability to determine when a
favor was done. In this setting, our protocol achieves nearly optimal
collective welfare while remaining resistant to exploitation.
  Our results also apply to a collaborative filtering setting where users must
make periodic decisions about whether to interact with resources such as movies
or restaurants. In this setting, we guarantee that any set of honest users
achieves a payoff nearly as good as if they had identified the optimal set of
items in advance and then chosen to interact only with resources from that set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1129</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1129</id><created>2014-11-04</created><authors><author><keyname>Wu</keyname><forenames>Zhaohui</forenames></author><author><keyname>Yuan</keyname><forenames>Dayu</forenames></author><author><keyname>Treeratpituk</keyname><forenames>Pucktada</forenames></author><author><keyname>Giles</keyname><forenames>C. Lee</forenames></author></authors><title>Science and Ethnicity: How Ethnicities Shape the Evolution of Computer
  Science Research Community</title><categories>cs.DL cs.SI</categories><comments>11 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Globalization and the world wide web has resulted in academia and science
being an international and multicultural community forged by researchers and
scientists with different ethnicities. How ethnicity shapes the evolution of
membership, status and interactions of the scientific community, however, is
not well understood. This is due to the difficulty of ethnicity identification
at the large scale. We use name ethnicity classification as an indicator of
ethnicity. Based on automatic name ethnicity classification of 1.7+ million
authors gathered from Web, the name ethnicity of computer science scholars is
investigated by population size, publication contribution and collaboration
strength. By showing the evolution of name ethnicity from 1936 to 2010, we
discover that ethnicity diversity has increased significantly over time and
that different research communities in certain publication venues have
different ethnicity compositions. We notice a clear rise in the number of Asian
name ethnicities in papers. Their fraction of publication contribution
increases from approximately 10% to near 50% from 1970 to 2010. We also find
that name ethnicity acts as a homophily factor on coauthor networks, shaping
the formation of coauthorship as well as evolution of research communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1132</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1132</id><created>2014-11-04</created><updated>2015-06-19</updated><authors><author><keyname>Arabshahi</keyname><forenames>Forough</forenames></author><author><keyname>Huang</keyname><forenames>Furong</forenames></author><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Butts</keyname><forenames>Carter T.</forenames></author><author><keyname>Fitshugh</keyname><forenames>Sean M.</forenames></author></authors><title>Are you going to the party: depends, who else is coming? [Learning
  hidden group dynamics via conditional latent tree models]</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scalable probabilistic modeling and prediction in high dimensional
multivariate time-series is a challenging problem, particularly for systems
with hidden sources of dependence and/or homogeneity. Examples of such problems
include dynamic social networks with co-evolving nodes and edges and dynamic
student learning in online courses. Here, we address these problems through the
discovery of hierarchical latent groups. We introduce a family of Conditional
Latent Tree Models (CLTM), in which tree-structured latent variables
incorporate the unknown groups. The latent tree itself is conditioned on
observed covariates such as seasonality, historical activity, and node
attributes. We propose a statistically efficient framework for learning both
the hierarchical tree structure and the parameters of the CLTM. We demonstrate
competitive performance in multiple real world datasets from different domains.
These include a dataset on students' attempts at answering questions in a
psychology MOOC, Twitter users participating in an emergency management
discussion and interacting with one another, and windsurfers interacting on a
beach in Southern California. In addition, our modeling framework provides
valuable and interpretable information about the hidden group structures and
their effect on the evolution of the time series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1134</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1134</id><created>2014-11-04</created><updated>2015-02-10</updated><authors><author><keyname>De Sa</keyname><forenames>Christopher</forenames></author><author><keyname>Olukotun</keyname><forenames>Kunle</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author></authors><title>Global Convergence of Stochastic Gradient Descent for Some Non-convex
  Matrix Problems</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic gradient descent (SGD) on a low-rank factorization is commonly
employed to speed up matrix problems including matrix completion, subspace
tracking, and SDP relaxation. In this paper, we exhibit a step size scheme for
SGD on a low-rank least-squares problem, and we prove that, under broad
sampling conditions, our method converges globally from a random starting point
within $O(\epsilon^{-1} n \log n)$ steps with constant probability for
constant-rank problems. Our modification of SGD relates it to stochastic power
iteration. We also show experiments to illustrate the runtime and convergence
of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1139</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1139</id><created>2014-11-04</created><authors><author><keyname>Firer</keyname><forenames>Marcelo</forenames></author><author><keyname>Panek</keyname><forenames>Luciano</forenames></author><author><keyname>Pinheiro</keyname><forenames>Jerry Anderson</forenames></author></authors><title>Coding and Decoding Schemes for MSE and Image Transmission</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we explore possibilities for coding and decoding tailor-made for
mean squared error evaluation of error in contexts such as image transmission.
To do so, we introduce a loss function that expresses the overall performance
of a coding and decoding scheme for discrete channels and that exchanges the
usual goal of minimizing the error probability to that of minimizing the
expected loss. In this environment we explore the possibilities of using
ordered decoders to create a message-wise unequal error protection (UEP), where
the most valuable information is protected by placing in its proximity
information words that differ by a small valued error. We give explicit
examples, using scale-of-gray images, including small-scale performance
analysis and visual simulations for the BSMC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1147</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1147</id><created>2014-11-04</created><updated>2014-11-10</updated><authors><author><keyname>Ammar</keyname><forenames>Waleed</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author><author><keyname>Smith</keyname><forenames>Noah A.</forenames></author></authors><title>Conditional Random Field Autoencoders for Unsupervised Structured
  Prediction</title><categories>cs.LG cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a framework for unsupervised learning of structured predictors
with overlapping, global features. Each input's latent representation is
predicted conditional on the observable data using a feature-rich conditional
random field. Then a reconstruction of the input is (re)generated, conditional
on the latent structure, using models for which maximum likelihood estimation
has a closed-form. Our autoencoder formulation enables efficient learning
without making unrealistic independence assumptions or restricting the kinds of
features that can be used. We illustrate insightful connections to traditional
autoencoders, posterior regularization and multi-view learning. We show
competitive results with instantiations of the model for two canonical NLP
tasks: part-of-speech induction and bitext word alignment, and show that
training our model can be substantially more efficient than comparable
feature-rich baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1152</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1152</id><created>2014-11-05</created><updated>2015-08-17</updated><authors><author><keyname>Esponda</keyname><forenames>Ignacio</forenames></author><author><keyname>Pouzo</keyname><forenames>Demian</forenames></author></authors><title>Berk-Nash Equilibrium: A Framework for Modeling Agents with Misspecified
  Models</title><categories>q-fin.EC cs.GT</categories><comments>90 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an equilibrium framework that relaxes the standard assumption that
people have a correctly-specified view of their environment. Each player is
characterized by a (possibly misspecified) subjective model, which describes
the set of feasible beliefs over payoff-relevant consequences as a function of
actions. We introduce the notion of a Berk-Nash equilibrium: Each player
follows a strategy that is optimal given her belief, and her belief is
restricted to be the best fit among the set of beliefs she considers possible.
The notion of best fit is formalized in terms of minimizing the
Kullback-Leibler divergence, which is endogenous and depends on the equilibrium
strategy profile. Standard solution concepts such as Nash equilibrium and
self-confirming equilibrium constitute special cases where players have
correctly-specified models. We provide a learning foundation for Berk-Nash
equilibrium by extending and combining results from the statistics literature
on misspecified learning and the economics literature on learning in games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1154</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1154</id><created>2014-11-05</created><authors><author><keyname>Sharma</keyname><forenames>Chayanika</forenames></author><author><keyname>Sabharwal</keyname><forenames>Sangeeta</forenames></author><author><keyname>Sibal</keyname><forenames>Ritu</forenames></author></authors><title>A Survey on Software Testing Techniques using Genetic Algorithm</title><categories>cs.SE</categories><comments>13 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The overall aim of the software industry is to ensure delivery of high
quality software to the end user. To ensure high quality software, it is
required to test software. Testing ensures that software meets user
specifications and requirements. However, the field of software testing has a
number of underlying issues like effective generation of test cases,
prioritisation of test cases etc which need to be tackled. These issues demand
on effort, time and cost of the testing. Different techniques and methodologies
have been proposed for taking care of these issues. Use of evolutionary
algorithms for automatic test generation has been an area of interest for many
researchers. Genetic Algorithm (GA) is one such form of evolutionary
algorithms. In this research paper, we present a survey of GA approach for
addressing the various issues encountered during software testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1158</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1158</id><created>2014-11-05</created><authors><author><keyname>Cesa-Bianchi</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>On the Complexity of Learning with Kernels</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A well-recognized limitation of kernel learning is the requirement to handle
a kernel matrix, whose size is quadratic in the number of training examples.
Many methods have been proposed to reduce this computational cost, mostly by
using a subset of the kernel matrix entries, or some form of low-rank matrix
approximation, or a random projection method. In this paper, we study lower
bounds on the error attainable by such methods as a function of the number of
entries observed in the kernel matrix or the rank of an approximate kernel
matrix. We show that there are kernel learning problems where no such method
will lead to non-trivial computational savings. Our results also quantify how
the problem difficulty depends on parameters such as the nature of the loss
function, the regularization parameter, the norm of the desired predictor, and
the kernel matrix rank. Our results also suggest cases where more efficient
kernel learning might be possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1161</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1161</id><created>2014-11-05</created><authors><author><keyname>Shi</keyname><forenames>Long</forenames></author><author><keyname>Liew</keyname><forenames>Soung Chang</forenames></author><author><keyname>Lu</keyname><forenames>Lu</forenames></author></authors><title>On the Subtleties of q-PAM Linear Physical-Layer Network Coding</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates various subtleties of applying linear physical-layer
network coding (PNC) with q-level pulse amplitude modulation (q-PAM) in two-way
relay channels (TWRC). A critical issue is how the PNC system performs when the
received powers from the two users at the relay are imbalanced. In particular,
how would the PNC system perform under slight power imbalance that is
inevitable in practice, even when power control is applied? To answer these
questions, this paper presents a comprehensive analysis of q-PAM PNC. Our
contributions are as follows: 1) We give a systematic way to obtain the
analytical relationship between the minimum distance of the signal
constellation induced by the superimposed signals of the two users (a key
performance determining factor) and the channel-gain ratio of the two users,
for all q. In particular, we show how the minimum distance changes in a
piecewise linear fashion as the channel-gain ratio varies. 2) We show that the
performance of q-PAM PNC is highly sensitive to imbalanced received powers from
the two users at the relay, even when the power imbalance is slight (e.g., the
residual power imbalance in a power-controlled system). This sensitivity
problem is exacerbated as q increases, calling into question the robustness of
high-order modulated PNC. 3) We propose an asynchronized PNC system in which
the symbol arrival times of the two users at the relay are deliberately made to
be asynchronous. We show that such asynchronized PNC, when operated with a
belief propagation (BP) decoder, can remove the sensitivity problem, allowing a
robust high-order modulated PNC system to be built.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1170</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1170</id><created>2014-11-05</created><updated>2014-12-01</updated><authors><author><keyname>Goh</keyname><forenames>Ong Sing</forenames></author><author><keyname>Fung</keyname><forenames>Lance</forenames></author></authors><title>An Intelligent Personal Robot Assistant</title><categories>cs.RO cs.AI cs.HC</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error</comments><report-no>TJ211.G63 2004</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent development in developing humanoid robot poses new challenges to
human-machine interaction communication. A major challenge is to develop robots
that can behave like and interact with human in the most natural way possible.
This paper proposes a system to develop a robot that can receive command, and
talk to people in natural language. In addition, the robot can also be
&quot;trained&quot; to become an expert in sepcific areas to provide expert advice to
human-beings. Most important of all, the robot can display emotions through
facial expression, speech and gesture so that the interaction process will
become more comprehensive and compelling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1171</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1171</id><created>2014-11-05</created><authors><author><keyname>Zeng</keyname><forenames>Rui</forenames></author><author><keyname>Wu</keyname><forenames>Jiasong</forenames></author><author><keyname>Shao</keyname><forenames>Zhuhong</forenames></author><author><keyname>Senhadji</keyname><forenames>Lotfi</forenames></author><author><keyname>Shu</keyname><forenames>Huazhong</forenames></author></authors><title>Multilinear Principal Component Analysis Network for Tensor Object
  Classification</title><categories>cs.CV</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently proposed principal component analysis network (PCANet) has been
proved high performance for visual content classification. In this letter, we
develop a tensorial extension of PCANet, namely, multilinear principal analysis
component network (MPCANet), for tensor object classification. Compared to
PCANet, the proposed MPCANet uses the spatial structure and the relationship
between each dimension of tensor objects much more efficiently. Experiments
were conducted on different visual content datasets including UCF sports action
video sequences database and UCF11 database. The experimental results have
revealed that the proposed MPCANet achieves higher classification accuracy than
PCANet for tensor object classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1172</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1172</id><created>2014-11-05</created><authors><author><keyname>Zeng</keyname><forenames>Rui</forenames></author><author><keyname>Wu</keyname><forenames>Jiasong</forenames></author><author><keyname>Senhadji</keyname><forenames>Lotfi</forenames></author><author><keyname>Shu</keyname><forenames>Huazhong</forenames></author></authors><title>Tensor object classification via multilinear discriminant analysis
  network</title><categories>cs.CV</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a multilinear discriminant analysis network (MLDANet) for
the recognition of multidimensional objects, known as tensor objects. The
MLDANet is a variation of linear discriminant analysis network (LDANet) and
principal component analysis network (PCANet), both of which are the recently
proposed deep learning algorithms. The MLDANet consists of three parts: 1) The
encoder learned by MLDA from tensor data. 2) Features maps ob-tained from
decoder. 3) The use of binary hashing and histogram for feature pooling. A
learning algorithm for MLDANet is described. Evaluations on UCF11 database
indicate that the proposed MLDANet outperforms the PCANet, LDANet, MPCA + LDA,
and MLDA in terms of classification for tensor objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1209</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1209</id><created>2014-11-05</created><authors><author><keyname>Rihani</keyname><forenames>Hamza</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Dementiev</keyname><forenames>Roman</forenames></author></authors><title>MultiQueues: Simpler, Faster, and Better Relaxed Concurrent Priority
  Queues</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Priority queues with parallel access are an attractive data structure for
applications like prioritized online scheduling, discrete event simulation, or
branch-and-bound. However, a classical priority queue constitutes a severe
bottleneck in this context, leading to very small throughput. Hence, there has
been significant interest in concurrent priority queues with a somewhat relaxed
semantics where deleted elements only need to be close to the minimum. In this
paper we present a very simple approach based on multiple sequential priority
queues. It turns out to outperform previous more complicated data structures
while at the same time improving the quality of the returned elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1215</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1215</id><created>2014-11-05</created><authors><author><keyname>Saleem</keyname><forenames>Muhammed Asif</forenames></author><author><keyname>Varghese</keyname><forenames>Blesson</forenames></author><author><keyname>Barker</keyname><forenames>Adam</forenames></author></authors><title>BigExcel: A Web-Based Framework for Exploring Big Data in Social
  Sciences</title><categories>cs.DC</categories><comments>8 pages</comments><journal-ref>Workshop of Big Humanities Data at the IEEE International
  Conference on Big Data (IEEE BigData) 2014, Washington D. C., USA</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper argues that there are three fundamental challenges that need to be
overcome in order to foster the adoption of big data technologies in
non-computer science related disciplines: addressing issues of accessibility of
such technologies for non-computer scientists, supporting the ad hoc
exploration of large data sets with minimal effort and the availability of
lightweight web-based frameworks for quick and easy analytics. In this paper,
we address the above three challenges through the development of 'BigExcel', a
three tier web-based framework for exploring big data to facilitate the
management of user interactions with large data sets, the construction of
queries to explore the data set and the management of the infrastructure. The
feasibility of BigExcel is demonstrated through two Yahoo Sandbox datasets. The
first dataset is the Yahoo Buzz Score data set we use for quantitatively
predicting trending technologies and the second is the Yahoo n-gram corpus we
use for qualitatively inferring the coverage of important events. A
demonstration of the BigExcel framework and source code is available at
http://bigdata.cs.st-andrews.ac.uk/projects/bigexcel-exploring-big-data-for-social-sciences/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1217</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1217</id><created>2014-11-05</created><authors><author><keyname>Wu</keyname><forenames>Junfeng</forenames></author><author><keyname>Shi</keyname><forenames>Guodong</forenames></author><author><keyname>Anderson</keyname><forenames>Brian D. O.</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author></authors><title>Kalman Filtering over Gilbert-Elliott Channels: Stability Conditions and
  the Critical Curve</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the stability of Kalman filtering over
Gilbert-Elliott channels where random packet drop follows a time-homogeneous
two-state Markov chain whose state transition is determined by a pair of
failure and recovery rates. First of all, we establish a relaxed condition
guaranteeing peak-covariance stability described by an inequality in terms of
the spectral radius of the system matrix and transition probabilities of the
Markov chain. We further show that that condition can be interpreted using a
linear matrix inequality feasibility problem. Next, we prove that the
peak-covariance stability implies mean-square stability, if the system matrix
has no defective eigenvalues on the unit circle. This connection between the
two stability notions holds for any random packet drop process. We prove that
there exists a critical curve in the failure-recovery rate plane, below which
the Kalman filter is mean-square stable and no longer mean-square stable above,
via a coupling method in stochastic processes. Finally, a lower bound for this
critical failure rate is obtained making use of the relationship we establish
between the two stability criteria, based on an approximate relaxation of the
system matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1220</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1220</id><created>2014-11-05</created><authors><author><keyname>Dimond</keyname><forenames>Jonathan</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author></authors><title>Faster Exact Search using Document Clustering</title><categories>cs.IR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how full-text search based on inverted indices can be accelerated by
clustering the documents without losing results (SeCluD -- SEarch with
CLUstered Documents). We develop a fast multilevel clustering algorithm that
explicitly uses query cost for conjunctive queries as an objective function.
Depending on the inputs we get up to four times faster than non-clustered
search. The resulting clusters are also useful for data compression and for
distributing the work over many machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1243</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1243</id><created>2014-11-05</created><authors><author><keyname>Kampakis</keyname><forenames>Stylianos</forenames></author><author><keyname>Adamides</keyname><forenames>Andreas</forenames></author></authors><title>Using Twitter to predict football outcomes</title><categories>stat.ML cs.CL cs.SI</categories><acm-class>I.2.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter has been proven to be a notable source for predictive modelling on
various domains such as the stock market, the dissemination of diseases or
sports outcomes. However, such a study has not been conducted in football
(soccer) so far. The purpose of this research was to study whether data mined
from Twitter can be used for this purpose. We built a set of predictive models
for the outcome of football games of the English Premier League for a 3 month
period based on tweets and we studied whether these models can overcome
predictive models which use only historical data and simple football
statistics. Moreover, combined models are constructed using both Twitter and
historical data. The final results indicate that data mined from Twitter can
indeed be a useful source for predicting games in the Premier League. The final
Twitter-based model performs significantly better than chance when measured by
Cohen's kappa and is comparable to the model that uses simple statistics and
historical data. Combining both models raises the performance higher than it
was achieved by each individual model. Thereby, this study provides evidence
that Twitter derived features can indeed provide useful information for the
prediction of football (soccer) outcomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1263</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1263</id><created>2014-11-05</created><authors><author><keyname>Halldorsson</keyname><forenames>Magnus M.</forenames></author><author><keyname>Tonoyan</keyname><forenames>Tigran</forenames></author></authors><title>How Well Can Graphs Represent Wireless Interference?</title><categories>cs.NI cs.DS</categories><comments>26 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Efficient use of a wireless network requires that transmissions be grouped
into feasible sets, where feasibility means that each transmission can be
successfully decoded in spite of the interference caused by simultaneous
transmissions. Feasibility is most closely modeled by a
signal-to-interference-plus-noise (SINR) formula, which unfortunately is
conceptually complicated, being an asymmetric, cumulative, many-to-one
relationship. We re-examine how well graphs can capture wireless receptions as
encoded in SINR relationships, placing them in a framework in order to
understand the limits of such modelling. We seek for each wireless instance a
pair of graphs that provide upper and lower bounds on the feasibility relation,
while aiming to minimize the gap between the two graphs. The cost of a graph
formulation is the worst gap over all instances, and the price of (graph)
abstraction is the smallest cost of a graph formulation. We propose a family of
conflict graphs that is parameterized by a non-decreasing sub-linear function,
and show that with a judicious choice of functions, the graphs can capture
feasibility with a cost of $O(\log^* \Delta)$, where $\Delta$ is the ratio
between the longest and the shortest link length. This holds on the plane and
more generally in doubling metrics. We use this to give greatly improved
$O(\log^* \Delta)$-approximation for fundamental link scheduling problems with
arbitrary power control. We explore the limits of graph representations and
find that our upper bound is tight: the price of graph abstraction is
$\Omega(\log^* \Delta)$. We also give strong impossibility results for general
metrics, and for approximations in terms of the number of links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1267</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1267</id><created>2014-11-05</created><authors><author><keyname>Ananthapadmanabha</keyname><forenames>T. V.</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>A. G.</forenames></author><author><keyname>Balachandran</keyname><forenames>Pradeep</forenames></author></authors><title>An Interesting Property of LPCs for Sonorant Vs Fricative Discrimination</title><categories>cs.SD</categories><comments>5 pages including references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear prediction (LP) technique estimates an optimum all-pole filter of a
given order for a frame of speech signal. The coefficients of the all-pole
filter, 1/A(z) are referred to as LP coefficients (LPCs). The gain of the
inverse of the all-pole filter, A(z) at z = 1, i.e, at frequency = 0, A(1)
corresponds to the sum of LPCs, which has the property of being lower (higher)
than a threshold for the sonorants (fricatives). When the inverse-tan of A(1),
denoted as T(1), is used a feature and tested on the sonorant and fricative
frames of the entire TIMIT database, an accuracy of 99.07% is obtained. Hence,
we refer to T(1) as sonorant-fricative discrimination index (SFDI). This
property has also been tested for its robustness for additive white noise and
on the telephone quality speech of the NTIMIT database. These results are
comparable to, or in some respects, better than the state-of-the-art methods
proposed for a similar task. Such a property may be used for segmenting a
speech signal or for non-uniform frame-rate analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1279</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1279</id><created>2014-11-03</created><authors><author><keyname>Yun</keyname><forenames>Se-Young</forenames></author><author><keyname>Lelarge</keyname><forenames>Marc</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author></authors><title>Streaming, Memory Limited Algorithms for Community Detection</title><categories>cs.SI cs.DS</categories><comments>NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider sparse networks consisting of a finite number of
non-overlapping communities, i.e. disjoint clusters, so that there is higher
density within clusters than across clusters. Both the intra- and inter-cluster
edge densities vanish when the size of the graph grows large, making the
cluster reconstruction problem nosier and hence difficult to solve. We are
interested in scenarios where the network size is very large, so that the
adjacency matrix of the graph is hard to manipulate and store. The data stream
model in which columns of the adjacency matrix are revealed sequentially
constitutes a natural framework in this setting. For this model, we develop two
novel clustering algorithms that extract the clusters asymptotically
accurately. The first algorithm is {\it offline}, as it needs to store and keep
the assignments of nodes to clusters, and requires a memory that scales
linearly with the network size. The second algorithm is {\it online}, as it may
classify a node when the corresponding column is revealed and then discard this
information. This algorithm requires a memory growing sub-linearly with the
network size. To construct these efficient streaming memory-limited clustering
algorithms, we first address the problem of clustering with partial
information, where only a small proportion of the columns of the adjacency
matrix is observed and develop, for this setting, a new spectral algorithm
which is of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1280</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1280</id><created>2014-11-05</created><authors><author><keyname>Gorbil</keyname><forenames>Gokce</forenames></author><author><keyname>Abdelrahman</keyname><forenames>Omer H.</forenames></author><author><keyname>Pavloski</keyname><forenames>Mihajlo</forenames></author><author><keyname>Gelenbe</keyname><forenames>Erol</forenames></author></authors><title>Storms in Mobile Networks</title><categories>cs.NI cs.CR</categories><comments>Submitted to the IEEE TETC special issue on &quot;Emerging topics in Cyber
  Security&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile networks are vulnerable to signalling attacks and storms that are
caused by traffic patterns that overload the control plane, and differ from
distributed denial of service (DDoS) attacks in the Internet since they
directly attack the control plane, and also reserve wireless bandwidth without
actually using it. Such attacks can result from malware and mobile botnets, as
well as from poorly designed applications, and can cause service outages in 3G
and 4G networks which have been experienced by mobile operators. Since the
radio resource control (RRC) protocol in 3G and 4G networks is particularly
susceptible to such attacks, we analyze their effect with a mathematical model
that helps to predict the congestion that is caused by an attack. A detailed
simulation model of a mobile network is used to better understand the temporal
dynamics of user behavior and signalling in the network and to show how RRC
based signalling attacks and storms cause significant problems in the control
plane and the user plane of the network. Our analysis also serves to identify
how storms can be detected, and to propose how system parameters can be chosen
to mitigate their effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1282</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1282</id><created>2014-11-05</created><updated>2016-02-09</updated><authors><author><keyname>Gemmetto</keyname><forenames>Valerio</forenames></author><author><keyname>Squartini</keyname><forenames>Tiziano</forenames></author><author><keyname>Picciolo</keyname><forenames>Francesco</forenames></author><author><keyname>Ruzzenenti</keyname><forenames>Franco</forenames></author><author><keyname>Garlaschelli</keyname><forenames>Diego</forenames></author></authors><title>Multiplexity and multireciprocity in directed multiplexes</title><categories>physics.soc-ph cs.SI</categories><comments>16 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multi-layer networks with directed links, introducing measures of
dependency between different layers requires more than a straightforward
extension of the multiplexity measures that have been developed for undirected
multiplexes. In particular, one should take into account the effects of
reciprocity, i.e. the tendency of pairs of vertices to establish mutual
connections. In single-layer networks, reciprocity is a crucial structural
property affecting several dynamical processes. Here we extend it to
multiplexes and introduce the notion of multireciprocity, defined as the
tendency of links in one layer to be reciprocated by links in a different
layer. While ordinary reciprocity reduces to a scalar quantity,
multireciprocity requires a square matrix generated by all the possible pairs
of layers. We introduce multireciprocity metrics valid for both binary and
weighted multiplexes and provide an empirical analysis of the World Trade
Multiplex (WTM), representing the import-export relationships between world
countries in different products. We show that several pairs of layers exhibit
strong multiplexity, an effect which is however largely encoded into the degree
or strength sequences of individual layers. We also find that most pairs of
commodities are characterised by positive multireciprocity, and that such
values are significantly lower than the usual reciprocity measured on the
aggregated network. Moreover, layers with low (high) internal reciprocity are
embedded within groups of layers with low (high) mutual multireciprocity. We
finally identify robust empirical patterns showing that joint multi-layer
connection probabilities can be reconstructed from marginal ones via the
multireciprocity matrix. Therefore the latter can bridge the gap between
single-layer properties and truly multiplex information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1284</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1284</id><created>2014-11-05</created><authors><author><keyname>Li</keyname><forenames>Wenling</forenames></author><author><keyname>Jia</keyname><forenames>Yingmin</forenames></author></authors><title>Kullback-Leibler divergence for interacting multiple model estimation
  with random matrices</title><categories>cs.SY</categories><comments>16 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of interacting multiple model (IMM) estimation
for jump Markov linear systems with unknown measurement noise covariance. The
system state and the unknown covariance are jointly estimated in the framework
of Bayesian estimation, where the unknown covariance is modeled as a random
matrix according to an inverse-Wishart distribution. For the IMM estimation
with random matrices, one difficulty encountered is the combination of a set of
weighted inverse-Wishart distributions. Instead of using the moment matching
approach, this difficulty is overcome by minimizing the weighted
Kullback-Leibler divergence for inverse-Wishart distributions. It is shown that
a closed form solution can be derived for the optimization problem and the
resulting solution coincides with an inverse-Wishart distribution. Simulation
results show that the proposed filter performs better than the previous work
using the moment matching approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1293</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1293</id><created>2014-11-05</created><updated>2015-03-07</updated><authors><author><keyname>Tang</keyname><forenames>Yu-Hang</forenames></author><author><keyname>Kudo</keyname><forenames>Shuhei</forenames></author><author><keyname>Bian</keyname><forenames>Xin</forenames></author><author><keyname>Li</keyname><forenames>Zhen</forenames></author><author><keyname>Karniadakis</keyname><forenames>George E.</forenames></author></authors><title>Multiscale Universal Interface: A Concurrent Framework for Coupling
  Heterogeneous Solvers</title><categories>physics.comp-ph cs.CE cs.DC physics.flu-dyn</categories><comments>The library source code is freely available under the GPLv3 license
  at http://www.cfm.brown.edu/repo/release/MUI/</comments><doi>10.1016/j.jcp.2015.05.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concurrently coupled numerical simulations using heterogeneous solvers are
powerful tools for modeling multiscale phenomena. However, major modifications
to existing codes are often required to enable such simulations, posing
significant difficulties in practice. In this paper we present a C++ library,
i.e. the Multiscale Universal Interface (MUI), which is capable of facilitating
the coupling effort for a wide range of multiscale simulations. The library
adopts a header-only form with minimal external dependency and hence can be
easily dropped into existing codes. A data sampler concept is introduced,
combined with a hybrid dynamic/static typing mechanism, to create an easily
customizable framework for solver-independent data interpretation. The library
integrates MPI MPMD support and an asynchronous communication protocol to
handle inter-solver information exchange irrespective of the solvers' own MPI
awareness. Template metaprogramming is heavily employed to simultaneously
improve runtime performance and code flexibility. We validated the library by
solving three different multiscale problems, which also serve to demonstrate
the flexibility of the framework in handling heterogeneous models and solvers.
In the first example, a Couette flow was simulated using two concurrently
coupled Smoothed Particle Hydrodynamics (SPH) simulations of different spatial
resolutions. In the second example, we coupled the deterministic SPH method
with the stochastic Dissipative Particle Dynamics (DPD) method to study the
effect of surface grafting on the hydrodynamics properties on the surface. In
the third example, we consider conjugate heat transfer between a solid domain
and a fluid domain by coupling the particle-based energy-conserving DPD (eDPD)
method with the Finite Element Method (FEM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1297</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1297</id><created>2014-11-05</created><authors><author><keyname>Pereira</keyname><forenames>Osvaldo</forenames></author><author><keyname>Torre</keyname><forenames>Esley</forenames></author><author><keyname>Garc&#xe9;s</keyname><forenames>Yasel</forenames></author><author><keyname>Rodr&#xed;guez</keyname><forenames>Roberto</forenames></author></authors><title>Edge Detection based on Kernel Density Estimation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Edges of an image are considered a crucial type of information. These can be
extracted by applying edge detectors with different methodology. Edge detection
is a vital step in computer vision tasks, because it is an essential issue for
pattern recognition and visual interpretation. In this paper, we propose a new
method for edge detection in images, based on the estimation by kernel of the
probability density function. In our algorithm, pixels in the image with
minimum value of density function are labeled as edges. The boundary between
two homogeneous regions is defined in two domains: the spatial/lattice domain
and the range/color domain. Extensive experimental evaluations proved that our
edge detection method is significantly a competitive algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1303</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1303</id><created>2014-11-04</created><updated>2015-05-21</updated><authors><author><keyname>Dumitrescu</keyname><forenames>Adrian</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>Csaba D.</forenames></author></authors><title>Convex polygons in geometric triangulations</title><categories>math.MG cs.DM math.CO</categories><comments>19 pages, 5 figures, extended abstract to be presented at WADS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the maximum number of convex polygons in a triangulation of $n$
points in the plane is $O(1.5029^n)$. This improves an earlier bound of
$O(1.6181^n)$ established by van Kreveld, L\&quot;offler, and Pach (2012) and almost
matches the current best lower bound of $\Omega(1.5028^n)$ due to the same
authors. Given a planar straight-line graph $G$ with $n$ vertices, we show how
to compute efficiently the number of convex polygons in $G$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1307</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1307</id><created>2014-11-05</created><authors><author><keyname>Thramboulidis</keyname><forenames>Kleanthis</forenames></author></authors><title>An Open Distributed Architecture for Flexible Hybrid Assembly Systems: A
  Model Driven Engineering Approach</title><categories>cs.SE</categories><comments>9 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assembly systems constitute one of the most important fields in today
industry. In this paper we propose an open distributed architecture for the
engineering of evolvable flexible hybrid assembly systems. The proposed
architecture is based on the model driven development paradigm. Models are used
to represent structure and behavior and a domain specific engineering tool is
defined to facilitate the assembly system engineer in the engineering process
of the assembly system. Specific meta models are defined to capture domain
knowledge to guide the engineer in the construction of the models required to
construct the assembly system. This work is a specialization of our previous
work that defined a SOA based framework for embedded industrial automation
systems. It adapts and extends, in the assembly systems domain, the 3+1
SysML-view model architecture defined for the engineering of mechatronics
Manufacturing systems. The proposed architecture can be used to develop a
framework for evolvable flexible and reconfigurable assembly systems that would
exploit the benefits the Cyber Physical paradigm utilizing web technologies,
the IoT, the Cloud computing and Big Data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1316</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1316</id><created>2014-11-05</created><updated>2014-11-06</updated><authors><author><keyname>Buckley</keyname><forenames>David</forenames></author><author><keyname>Chen</keyname><forenames>Ke</forenames></author><author><keyname>Knowles</keyname><forenames>Joshua</forenames></author></authors><title>Rapid Skill Capture in a First-Person Shooter</title><categories>cs.HC cs.LG</categories><comments>16 pages, 28 figures, journal paper submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various aspects of computer game design, including adaptive elements of game
levels, characteristics of 'bot' behavior, and player matching in multiplayer
games, would ideally be sensitive to a player's skill level. Yet, while
difficulty and player learning have been explored in the context of games,
there has been little work analyzing skill per se, and how it pertains to a
player's input. To this end, we present a data set of 476 game logs from over
40 players of a first-person shooter game (Red Eclipse) as a basis of a case
study. We then analyze different metrics of skill and show that some of these
can be predicted using only a few seconds of keyboard and mouse input. We argue
that the techniques used here are useful for adapting games to match players'
skill levels rapidly, perhaps more rapidly than solutions based on performance
averaging such as TrueSkill.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1319</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1319</id><created>2014-11-05</created><authors><author><keyname>Miller</keyname><forenames>Avery</forenames></author><author><keyname>Pelc</keyname><forenames>Andrzej</forenames></author></authors><title>Election vs. Selection: Two Ways of Finding the Largest Node in a Graph</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding the node with the largest label in a network, modeled as an
undirected connected graph, is one of the fundamental problems in distributed
computing. This is the way in which $\textit{leader election}$ is usually
solved. We consider two distinct tasks in which the largest-labeled node is
found deterministically. In $\textit{selection}$, this node must output 1 and
all other nodes must output 0. In $\textit{election}$, the other nodes must
additionally learn the largest label. Our aim is to compare the difficulty of
these two tasks executed under stringent running time constraints. The measure
of difficulty is the amount of information that nodes of the network must
initially possess in order to solve the given task in an imposed amount of
time. Following the standard framework of $\textit{algorithms with advice}$,
this information (a single binary string) is provided to all nodes at the start
by an oracle knowing the entire graph. The length of this string is called the
$\textit{size of advice}$. Consider the class of $n$-node graphs with any
diameter $diam \leq D$. If time is larger than $diam$, then both tasks can be
solved without advice. For the task of $\textit{election}$, we show that if
time is smaller than $diam$, then the optimal size of advice is $\Theta(\log
n)$, and if time is exactly $diam$, then the optimal size of advice is
$\Theta(\log D)$. For the task of $\textit{selection}$, the situation changes
dramatically, even within the class of rings. Indeed, for the class of rings,
we show that, if time is $O(diam^{\epsilon})$, for any $\epsilon &lt;1$, then the
optimal size of advice is $\Theta(\log D)$, and, if time is $\Theta(diam)$ (and
at most $diam$) then this optimal size is $\Theta(\log \log D)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1323</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1323</id><created>2014-11-05</created><updated>2015-06-30</updated><authors><author><keyname>Chen</keyname><forenames>Yongxin</forenames></author><author><keyname>Georgiou</keyname><forenames>Tryphon</forenames></author><author><keyname>Pavon</keyname><forenames>Michele</forenames></author></authors><title>Fast cooling for a system of stochastic oscillators</title><categories>math-ph cs.SY math.MP</categories><comments>21 pages, 2 figures</comments><msc-class>93E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study feedback control of coupled nonlinear stochastic oscillators in a
force field. We first consider the problem of asymptotically driving the system
to a desired {\em steady state} corresponding to reduced thermal noise. Among
the feedback controls achieving the desired asymptotic transfer, we find that
the most efficient one {from an energy point of view} is characterized by {\em
time-reversibility}. We also extend the theory of Schr\&quot;{o}dinger bridges to
this model, thereby steering the system in {\em finite time} and with minimum
effort to a target steady-state distribution. The system can then be maintained
in this state through the optimal steady-state feedback control. The solution,
in the finite-horizon case, involves a space-time harmonic function $\varphi$,
and $-\log\varphi$ plays the role of an artificial, time-varying potential in
which the desired evolution occurs. This framework appears extremely general
and flexible and can be viewed as a considerable generalization of existing
active control strategies such as macromolecular cooling. In the case of a
quadratic potential, the results assume a form particularly attractive from the
algorithmic viewpoint as the optimal control can be computed via deterministic
matricial differential equations. An example involving inertial particles
illustrates both transient and steady state optimal feedback control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1328</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1328</id><created>2014-11-05</created><authors><author><keyname>Tanovic</keyname><forenames>Omer</forenames></author><author><keyname>Megretski</keyname><forenames>Alexandre</forenames></author><author><keyname>Li</keyname><forenames>Yan</forenames></author><author><keyname>Stojanovic</keyname><forenames>Vladimir M.</forenames></author><author><keyname>Osqui</keyname><forenames>Mitra</forenames></author></authors><title>Discrete-Time Models Resulting From Dynamic Continuous-Time
  Perturbations In Phase-Amplitude Modulation-Demodulation Schemes</title><categories>cs.SY math.OC</categories><comments>Submitted to the 14th European Control Conference (ECC15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider discrete-time (DT) systems S in which a DT input is first
tranformed to a continuous-time (CT) format by phase-amplitude modulation, then
modified by a non-linear CT dynamical transformation F, and finally converted
back to DT output using an ideal de-modulation scheme. Assuming that F belongs
to a special class of CT Volterra series models with fixed degree and memory
depth, we provide a complete characterization of S as a series connection of a
DT Volterra series model of fixed degree and memory depth, and an LTI system
with special properties. The result suggests a new, non-obvious, analytically
motivated structure of digital compensation of analog nonlinear distortions
(for example, those caused by power amplifiers) in digital communication
systems. Results from a MATLAB simulation are used to demonstrate effectiveness
of the new compensation scheme, as compared to the standard Volterra series
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1339</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1339</id><created>2014-11-05</created><updated>2014-11-06</updated><authors><author><keyname>Jain</keyname><forenames>Siddharth</forenames></author><author><keyname>Bansal</keyname><forenames>R. K.</forenames></author></authors><title>On Match Lengths, Zero Entropy and Large Deviations - with Application
  to Sliding Window Lempel-Ziv Algorithm</title><categories>cs.IT math.IT</categories><comments>accepted to appear in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Sliding Window Lempel-Ziv (SWLZ) algorithm that makes use of recurrence
times and match lengths has been studied from various perspectives in
information theory literature. In this paper, we undertake a finer study of
these quantities under two different scenarios, i) \emph{zero entropy} sources
that are characterized by strong long-term memory, and ii) the processes with
weak memory as described through various mixing conditions.
  For zero entropy sources, a general statement on match length is obtained. It
is used in the proof of almost sure optimality of Fixed Shift Variant of
Lempel-Ziv (FSLZ) and SWLZ algorithms given in literature. Through an example
of stationary and ergodic processes generated by an irrational rotation we
establish that for a window of size $n_w$, a compression ratio given by
$O(\frac{\log n_w}{{n_w}^a})$ where $a$ depends on $n_w$ and approaches 1 as
$n_w \rightarrow \infty$, is obtained under the application of FSLZ and SWLZ
algorithms. Also, we give a general expression for the compression ratio for a
class of stationary and ergodic processes with zero entropy.
  Next, we extend the study of Ornstein and Weiss on the asymptotic behavior of
the \emph{normalized} version of recurrence times and establish the \emph{large
deviation property} (LDP) for a class of mixing processes. Also, an estimator
of entropy based on recurrence times is proposed for which large deviation
principle is proved for sources satisfying similar mixing conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1356</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1356</id><created>2014-09-24</created><authors><author><keyname>Maeno</keyname><forenames>Yoshiharu</forenames></author><author><keyname>Nishiguchi</keyname><forenames>Kenji</forenames></author><author><keyname>Morinaga</keyname><forenames>Satoshi</forenames></author><author><keyname>Matsushima</keyname><forenames>Hirokazu</forenames></author></authors><title>Impact of credit default swaps on financial contagion</title><categories>q-fin.RM cs.SI</categories><comments>presented at the IEEE Computational Intelligence for Financial
  Engineering and Economics, London, March 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It had been believed in the conventional practice that the risk of a bank
going bankrupt is lessened in a straightforward manner by transferring the risk
of loan defaults. But the failure of American International Group in 2008 posed
a more complex aspect of financial contagion. This study presents an extension
of the asset network systemic risk model (ANWSER) to investigate whether credit
default swaps mitigate or intensify the severity of financial contagion. A
protection buyer bank transfers the risk of every possible debtor bank default
to protection seller banks. The empirical distribution of the number of bank
bankruptcies is obtained with the extended model. Systemic capital buffer ratio
is calculated from the distribution. The ratio quantifies the effective loss
absorbency capability of the entire financial system to force back financial
contagion. The key finding is that the leverage ratio is a good estimate of a
systemic capital buffer ratio as the backstop of a financial system. The risk
transfer from small and medium banks to big banks in an interbank network does
not mitigate the severity of financial contagion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1361</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1361</id><created>2014-11-05</created><updated>2014-11-12</updated><authors><author><keyname>Robinson-Garcia</keyname><forenames>Nicolas</forenames></author><author><keyname>Jimenez-Contreras</keyname><forenames>Evaristo</forenames></author><author><keyname>Fuente-Gutierrez</keyname><forenames>Enrique</forenames></author><author><keyname>Torres-Salinas</keyname><forenames>Daniel</forenames></author></authors><title>Bibliometric Indicators for Publishers: Data processing, indicators and
  interpretation</title><categories>cs.DL</categories><comments>v.2 link to website and typos corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here we describe the Bibliometric Indicators for Publishers Project, an
initiative undertaken by EC3Metrics SL for the analysis and development of
indicators based on books and book chapters. Its goal is to study and analyze
the publication and citation patterns of books and book chapters considering
academic publishers as the unit of analysis. It aims at developing new
methodologies and indicators that can better capture and define the research
impact of publishers. It is an on-going project in which data sources and
indicators are tested. We consider academic publishers as an analogy of
journals, focusing on them as the unit of analysis. In this working paper we
present the http://bipublishers.es website where all findings derived from the
project are displayed. We describe the data retrieval and normalization process
and we show the main results. A total 482,470 records have been retrieved and
processed, identifying 342 publishers from which 254 have been analyzed. Then
six indicators have been calculated for each publisher for four fields and 38
disciplines and displayed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1367</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1367</id><created>2014-11-03</created><authors><author><keyname>Camps</keyname><forenames>Rosa</forenames></author><author><keyname>Mora</keyname><forenames>Xavier</forenames></author><author><keyname>Saumell</keyname><forenames>Laia</forenames></author></authors><title>Choosing by means of approval-preferential voting. The revised approval
  choice</title><categories>cs.GT</categories><msc-class>91B12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We look at procedures for making a collective choice through
approval-preferential voting. A new procedure of this kind is proposed that is
in the spirit of Condorcet's last ideas on elections, namely making sure that a
good choice is made rather than aiming at the best choice but not being so sure
about it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1372</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1372</id><created>2014-11-05</created><authors><author><keyname>Keivan</keyname><forenames>Nima</forenames></author><author><keyname>Sibley</keyname><forenames>Gabe</forenames></author></authors><title>Online SLAM with Any-time Self-calibration and Automatic Change
  Detection</title><categories>cs.CV cs.RO</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A framework for online simultaneous localization, mapping and
self-calibration is presented which can detect and handle significant change in
the calibration parameters. Estimates are computed in constant-time by
factoring the problem and focusing on segments of the trajectory that are most
informative for the purposes of calibration. A novel technique is presented to
detect the probability that a significant change is present in the calibration
parameters. The system is then able to re-calibrate. Maximum likelihood
trajectory and map estimates are computed using an asynchronous and adaptive
optimization. The system requires no prior information and is able to
initialize without any special motions or routines, or in the case where
observability over calibration parameters is delayed. The system is
experimentally validated to calibrate camera intrinsic parameters for a
nonlinear camera model on a monocular dataset featuring a significant zoom
event partway through, and achieves high accuracy despite unknown initial
calibration parameters. Self-calibration and re-calibration parameters are
shown to closely match estimates computed using a calibration target. The
accuracy of the system is demonstrated with SLAM results that achieve sub-1%
distance-travel error even in the presence of significant re-calibration
events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1373</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1373</id><created>2014-11-05</created><updated>2015-11-17</updated><authors><author><keyname>Hibbard</keyname><forenames>Bill</forenames></author></authors><title>Ethical Artificial Intelligence</title><categories>cs.AI</categories><comments>minor edit: remove page break between Figure 10.2 and its caption</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This book-length article combines several peer reviewed papers and new
material to analyze the issues of ethical artificial intelligence (AI). The
behavior of future AI systems can be described by mathematical equations, which
are adapted to analyze possible unintended AI behaviors and ways that AI
designs can avoid them. This article makes the case for utility-maximizing
agents and for avoiding infinite sets in agent definitions. It shows how to
avoid agent self-delusion using model-based utility functions and how to avoid
agents that corrupt their reward generators (sometimes called &quot;perverse
instantiation&quot;) using utility functions that evaluate outcomes at one point in
time from the perspective of humans at a different point in time. It argues
that agents can avoid unintended instrumental actions (sometimes called &quot;basic
AI drives&quot; or &quot;instrumental goals&quot;) by accurately learning human values. This
article defines a self-modeling agent framework and shows how it can avoid
problems of resource limits, being predicted by other agents, and inconsistency
between the agent's utility function and its definition (one version of this
problem is sometimes called &quot;motivated value selection&quot;). This article also
discusses how future AI will differ from current AI, the politics of AI, and
the ultimate use of AI to help understand the nature of the universe and our
place in it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1379</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1379</id><created>2014-11-05</created><authors><author><keyname>Tzamos</keyname><forenames>Christos</forenames></author><author><keyname>Wilkens</keyname><forenames>Christopher A.</forenames></author></authors><title>The Value of Knowing Your Enemy</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many auction settings implicitly or explicitly require that bidders are
treated equally ex-ante. This may be because discrimination is philosophically
or legally impermissible, or because it is practically difficult to implement
or impossible to enforce. We study so-called {\em anonymous} auctions to
understand the revenue tradeoffs and to develop simple anonymous auctions that
are approximately optimal.
  We consider digital goods settings and show that the optimal anonymous,
dominant strategy incentive compatible auction has an intuitive structure ---
imagine that bidders are randomly permuted before the auction, then infer a
posterior belief about bidder i's valuation from the values of other bidders
and set a posted price that maximizes revenue given this posterior.
  We prove that no anonymous mechanism can guarantee an approximation better
than O(n) to the optimal revenue in the worst case (or O(log n) for regular
distributions) and that even posted price mechanisms match those guarantees.
Understanding that the real power of anonymous mechanisms comes when the
auctioneer can infer the bidder identities accurately, we show a tight O(k)
approximation guarantee when each bidder can be confused with at most k &quot;higher
types&quot;. Moreover, we introduce a simple mechanism based on n target prices that
is asymptotically optimal and build on this mechanism to extend our results to
m-unit auctions and sponsored search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1380</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1380</id><created>2014-11-05</created><authors><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Sidorenko</keyname><forenames>Pavel</forenames></author><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author><author><keyname>Barel</keyname><forenames>Shaby</forenames></author><author><keyname>Cohen</keyname><forenames>Oren</forenames></author></authors><title>Sparse Phase Retrieval from Short-Time Fourier Measurements</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Signal Processing Letters</comments><doi>10.1109/LSP.2014.2364225</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classical 1D phase retrieval problem. In order to overcome
the difficulties associated with phase retrieval from measurements of the
Fourier magnitude, we treat recovery from the magnitude of the short-time
Fourier transform (STFT). We first show that the redundancy offered by the STFT
enables unique recovery for arbitrary nonvanishing inputs, under mild
conditions. An efficient algorithm for recovery of a sparse input from the STFT
magnitude is then suggested, based on an adaptation of the recently proposed
GESPAR algorithm. We demonstrate through simulations that using the STFT leads
to improved performance over recovery from the oversampled Fourier magnitude
with the same number of measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1381</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1381</id><created>2014-11-05</created><authors><author><keyname>Chawla</keyname><forenames>Shuchi</forenames></author><author><keyname>Devanur</keyname><forenames>Nikhil R.</forenames></author><author><keyname>Karlin</keyname><forenames>Anna</forenames></author><author><keyname>Sivan</keyname><forenames>Balasubramanian</forenames></author></authors><title>How to sell an app: pay-per-play or buy-it-now?</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider pricing in settings where a consumer discovers his value for a
good only as he uses it, and the value evolves with each use. We explore simple
and natural pricing strategies for a seller in this setting, under the
assumption that the seller knows the distribution from which the consumer's
initial value is drawn, as well as the stochastic process that governs the
evolution of the value with each use.
  We consider the differences between up-front or &quot;buy-it-now&quot; pricing (BIN),
and &quot;pay-per-play&quot; (PPP) pricing, where the consumer is charged per use. Our
results show that PPP pricing can be a very effective mechanism for price
discrimination, and thereby can increase seller revenue. But it can also be
advantageous to the buyers, as a way of mitigating risk. Indeed, this
mitigation of risk can yield a larger pool of buyers. We also show that the
practice of offering free trials is largely beneficial.
  We consider two different stochastic processes for how the buyer's value
evolves: In the first, the key random variable is how long the consumer remains
interested in the product. In the second process, the consumer's value evolves
according to a random walk or Brownian motion with reflection at 1, and
absorption at 0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1387</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1387</id><created>2014-11-04</created><updated>2015-09-07</updated><authors><author><keyname>Srivastava</keyname><forenames>Siddharth</forenames></author><author><keyname>Gupta</keyname><forenames>Ramji</forenames></author><author><keyname>Rai</keyname><forenames>Astha</forenames></author><author><keyname>Cheema</keyname><forenames>A. S.</forenames></author></authors><title>Electronic Health Records and Cloud based Generic Medical Equipment
  Interface</title><categories>cs.CY</categories><comments>National Conference on Medical Informatics 2014 (AIIMS, New Delhi)</comments><acm-class>J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Now-a-days Health Care industry is well equipped with Medical Equipments to
provide accurate and timely reports of investigation and examination results.
Medical Equipments available in market are made for specific tests suited for a
particular laboratory leading to a wide variety of devices. The result viewing
experience on console of these devices is not only cumborsome for medical staff
but inefficient. Therefore, Medical Equipment Interfaces act as backbone of any
Hospital Management Information System assisting in better management and
delivery of test results. It also acts as a mode to collect data for further
research and analysis. These equipments communicate via a fixed data format but
compatibility among these formats is a major issue being faced in modern and
legacy medical equipments. In this paper, we present a case study of designing
and implementing a cloud based Generic Medical Equipment Interface(GMEI) along
with the state of the art in such systems. This solution removes the burden of
reentry of patient details into the Electronic Health Record(EHR) and thrives
for accelerating EMR initiative in the country
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1395</identifier>
 <datestamp>2014-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1395</id><created>2014-10-23</created><authors><author><keyname>Bera</keyname><forenames>Sahadev</forenames></author><author><keyname>Bhowmick</keyname><forenames>Partha</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Bhargab B.</forenames></author></authors><title>On Covering a Solid Sphere with Concentric Spheres in ${\mathbb Z}^3$</title><categories>cs.CG cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a digital sphere, constructed by the circular sweep of a digital
semicircle (generatrix) around its diameter, consists of some holes
(absentee-voxels), which appear on its spherical surface of revolution. This
incompleteness calls for a proper characterization of the absentee-voxels whose
restoration will yield a complete spherical surface without any holes. In this
paper, we present a characterization of such absentee-voxels using certain
techniques of digital geometry and show that their count varies quadratically
with the radius of the semicircular generatrix. Next, we design an algorithm to
fill these absentee-voxels so as to generate a spherical surface of revolution,
which is more realistic from the viewpoint of visual perception. We further
show that covering a solid sphere by a set of complete spheres also results in
an asymptotically larger count of absentees, which is cubic in the radius of
the sphere. The characterization and generation of complete solid spheres
without any holes can also be accomplished in a similar fashion. We furnish
test results to substantiate our theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1397</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1397</id><created>2014-11-05</created><updated>2015-01-04</updated><authors><author><keyname>Chung</keyname><forenames>Kai-Min</forenames></author><author><keyname>Wu</keyname><forenames>Xiaodi</forenames></author><author><keyname>Yuen</keyname><forenames>Henry</forenames></author></authors><title>Strong parallel repetition for free entangled games, with any number of
  players</title><categories>quant-ph cs.CC</categories><comments>This manuscript has been withdrawn due to an error in Lemma 6.1, and
  has been replaced by arXiv:1501.0033</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a strong parallel repetition theorem for the entangled value of
multi-player, one-round free games (games where the inputs come from a product
distribution). Our result is the first parallel repetition theorem for
entangled games involving more than two players. Furthermore, our theorem
applies to games where the players are allowed to output (possibly entangled)
quantum states as answers.
  More specifically, let $G$ be a $k$-player free game, with entangled value
$\mathrm{val}^*(G) = 1 - \epsilon$. We show that the entangled value of the
$n$-fold repetition of $G$, $\mathrm{val}^*(G^{\otimes n})$, is at most $(1 -
\epsilon)^{\Omega(n/k^2)}$. In the traditional setting of $k=2$ players, our
parallel repetition theorem is optimal in terms of its dependence on $\epsilon$
and $n$. For an arbitrary number of players, our result is nearly optimal: for
all $k$, we exhibit a $k$-player free game $G$ and $n &gt; 1$ such that
$\mathrm{val}^*(G^{\otimes n}) \geq \mathrm{val}^*(G)^{n/k}$. Hence, exponent
of the repeated game value cannot be improved beyond $\Omega(n/k)$.
  Our parallel repetition theorem improves on the prior results of [Jain, et
al. 2014] and [Chailloux, Scarpa 2014] in a number of ways: (1) our theorem
applies to a larger class of games (arbitrary number of players, quantum
outputs); (2) we demonstrate that strong parallel repetition holds for the
entangled value of free games: i.e., the base of the repeated game value is $1
- \epsilon$, rather than $1 - \epsilon^2$; and (3) there is no dependence of
the repeated game value on the input and output alphabets of $G$. In contrast,
it is known that the repeated game value of classical free games must depend on
the output size. Thus our results demonstrate a seperation between the behavior
of entangled games and classical games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1398</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1398</id><created>2014-11-04</created><updated>2015-01-30</updated><authors><author><keyname>Haynes</keyname><forenames>Nicholas D.</forenames></author><author><keyname>Soriano</keyname><forenames>Miguel C.</forenames></author><author><keyname>Rosin</keyname><forenames>David P.</forenames></author><author><keyname>Fischer</keyname><forenames>Ingo</forenames></author><author><keyname>Gauthier</keyname><forenames>Daniel J.</forenames></author></authors><title>Reservoir computing with a single time-delay autonomous Boolean node</title><categories>cs.ET nlin.AO</categories><comments>5 pages, 5 figures</comments><journal-ref>Physical Review E 91, 020801(R)(1-5) (2015)</journal-ref><doi>10.1103/PhysRevE.91.020801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate reservoir computing with a physical system using a single
autonomous Boolean logic element with time-delay feedback. The system generates
a chaotic transient with a window of consistency lasting between 30 and 300 ns,
which we show is sufficient for reservoir computing. We then characterize the
dependence of computational performance on system parameters to find the best
operating point of the reservoir. When the best parameters are chosen, the
reservoir is able to classify short input patterns with performance that
decreases over time. In particular, we show that four distinct input patterns
can be classified for 70 ns, even though the inputs are only provided to the
reservoir for 7.5 ns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1401</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1401</id><created>2014-11-05</created><authors><author><keyname>Hoppensteadt</keyname><forenames>Frank</forenames></author></authors><title>Spin Wave Neuroanalog of von Neumann's Microwave Computer</title><categories>cs.ET</categories><msc-class>35Q41, 94C10, 82C32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frequency and phase of neural activity play important roles in the behaving
brain. The emerging understanding of these roles has been informed by the
design of analog devices that have been important to neuroscience, among them
the neuroanalog computer developed by O. Schmitt in the 1930's. In the 1950's,
J. von Neumann, in a search for high performance computing using microwaves,
invented a logic machine based on similar devices, that can perform logic
functions including binary arithmetic. Described here is a novel embodiment of
his machine using nano-magnetics. The embodiment is based on properties of
ferromagnetic thin films that are governed by a nonlinear Schrodinger equation
for magnetization in a film. Electrical currents through point contacts on a
film create spin torque nano oscillators (STNO) that define the oscillator
elements of the system. These oscillators may communicate through directed
graphs of electrical connections or by radiation in the form of spin waves. It
is shown here how to construct a logic machine using STNO, that this machine
can perform several computations simultaneously using multiplexing of inputs,
that this system can evaluate iterated logic functions, and that spin waves can
communicate frequency, phase and binary information. Neural tissue and the
Schmitt, von Neumann and STNO devices share a common bifurcation structure,
although these systems operate on vastly different space and time scales. This
suggests that neural circuits may be capable of computational functionality
described here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1420</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1420</id><created>2014-11-05</created><updated>2015-11-03</updated><authors><author><keyname>Belkin</keyname><forenames>Mikhail</forenames></author><author><keyname>Rademacher</keyname><forenames>Luis</forenames></author><author><keyname>Voss</keyname><forenames>James</forenames></author></authors><title>Basis Learning as an Algorithmic Primitive</title><categories>cs.LG</categories><comments>56 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of important problems in theoretical computer science and machine
learning can be interpreted as recovering a certain basis. These include
certain tensor decompositions, Independent Component Analysis (ICA), spectral
clustering and Gaussian mixture learning. Each of these problems reduces to an
instance of our general model, which we call a &quot;Basis Encoding Function&quot; (BEF).
We show that learning a basis within this model can then be provably and
efficiently achieved using a first order iteration algorithm (gradient
iteration). Our algorithm goes beyond tensor methods, providing a
function-based generalization for a number of existing methods including the
classical matrix power method, the tensor power iteration as well as
cumulant-based FastICA. Our framework also unifies the unusual phenomenon
observed in these domains that they can be solved using efficient non-convex
optimization. Specifically, we describe a class of BEFs such that their local
maxima on the unit sphere are in one-to-one correspondence with the basis
elements. This description relies on a certain &quot;hidden convexity&quot; property of
these functions.
  We provide a complete theoretical analysis of gradient iteration even when
the BEF is perturbed. We show convergence and complexity bounds polynomial in
dimension and other relevant parameters, such as perturbation size. Our
perturbation results can be considered as a non-linear version of the classical
Davis-Kahan theorem for perturbations of eigenvectors of symmetric matrices. In
addition we show that our algorithm exhibits fast (superlinear) convergence and
relate the speed of convergence to the properties of the BEF. Moreover, the
gradient iteration algorithm can be easily and efficiently implemented in
practice. Finally we apply our framework by providing the first provable
algorithm for recovery in a general perturbed ICA model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1432</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1432</id><created>2014-11-05</created><authors><author><keyname>Ngo</keyname><forenames>A. Q. T.</forenames></author><author><keyname>Bastian</keyname><forenames>P.</forenames></author><author><keyname>Ippisch</keyname><forenames>O.</forenames></author></authors><title>Numerical solution of steady-state groundwater flow and solute transport
  problems: Discontinuous Galerkin based methods compared to the Streamline
  Diffusion approach</title><categories>math.NA cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we consider the simulation of subsurface flow and solute
transport processes in the stationary limit. In the convection-dominant case,
the numerical solution of the transport problem may exhibit non-physical
diffusion and under- and overshoots. For an interior penalty discontinuous
Galerkin (DG) discretization, we present a $h$-adaptive refinement strategy
and, alternatively, a new efficient approach for reducing numerical under- and
overshoots using a diffusive $L^2$-projection. Furthermore, we illustrate an
efficient way of solving the linear system arising from the DG discretization.
In $2$-D and $3$-D examples, we compare the DG-based methods to the streamline
diffusion approach with respect to computing time and their ability to resolve
steep fronts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1434</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1434</id><created>2014-11-05</created><updated>2014-12-05</updated><authors><author><keyname>Shanmugam</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Tandon</keyname><forenames>Rashish</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Ravikumar</keyname><forenames>Pradeep</forenames></author></authors><title>On the Information Theoretic Limits of Learning Ising Models</title><categories>cs.LG</categories><comments>21 pages; to appear in NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a general framework for computing lower-bounds on the sample
complexity of recovering the underlying graphs of Ising models, given i.i.d
samples. While there have been recent results for specific graph classes, these
involve fairly extensive technical arguments that are specialized to each
specific graph class. In contrast, we isolate two key graph-structural
ingredients that can then be used to specify sample complexity lower-bounds.
Presence of these structural properties makes the graph class hard to learn. We
derive corollaries of our main result that not only recover existing recent
results, but also provide lower bounds for novel graph classes not considered
previously. We also extend our framework to the random graph setting and derive
corollaries for Erd\H{o}s-R\'{e}nyi graphs in a certain dense setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1442</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1442</id><created>2014-11-05</created><authors><author><keyname>Wang</keyname><forenames>Wei</forenames></author></authors><title>Optical Character Recognition, Using K-Nearest Neighbors</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The problem of optical character recognition, OCR, has been widely discussed
in the literature. Having a hand-written text, the program aims at recognizing
the text. Even though there are several approaches to this issue, it is still
an open problem. In this paper we would like to propose an approach that uses
K-nearest neighbors algorithm, and has the accuracy of more than 90%. The
training and run time is also very short.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1446</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1446</id><created>2014-11-05</created><authors><author><keyname>Wang</keyname><forenames>Wei</forenames></author></authors><title>Electrocardiography Separation of Mother and Baby</title><categories>cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Extraction of Electrocardiography (ECG or EKG) signals of mother and baby is
a challenging task, because one single device is used and it receives a mixture
of multiple heart beats. In this paper, we would like to design a filter to
separate the signals from each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1455</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1455</id><created>2014-11-05</created><updated>2015-04-05</updated><authors><author><keyname>Rahman</keyname><forenames>Md Farhadur</forenames></author><author><keyname>Liu</keyname><forenames>Weimo</forenames></author><author><keyname>Thirumuruganathan</keyname><forenames>Saravanan</forenames></author><author><keyname>Zhang</keyname><forenames>Nan</forenames></author><author><keyname>Das</keyname><forenames>Gautam</forenames></author></authors><title>Rank-Based Inference over Web Databases</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, there has been much research in Ranked Retrieval model in
structured databases, especially those in web databases. With this model, a
search query returns top-k tuples according to not just exact matches of
selection conditions, but a suitable ranking function. This paper studies a
novel problem on the privacy implications of database ranking. The motivation
is a novel yet serious privacy leakage we found on real-world web databases
which is caused by the ranking function design. Many such databases feature
private attributes - e.g., a social network allows users to specify certain
attributes as only visible to him/herself, but not to others. While these
websites generally respect the privacy settings by not directly displaying
private attribute values in search query answers, many of them nevertheless
take into account such private attributes in the ranking function design. The
conventional belief might be that tuple ranks alone are not enough to reveal
the private attribute values. Our investigation, however, shows that this is
not the case in reality.
  To address the problem, we introduce a taxonomy of the problem space with two
dimensions, (1) the type of query interface and (2) the capability of
adversaries. For each subspace, we develop a novel technique which either
guarantees the successful inference of private attributes, or does so for a
significant portion of real-world tuples. We demonstrate the effectiveness and
efficiency of our techniques through theoretical analysis, extensive
experiments over real-world datasets, as well as successful online attacks over
websites with tens to hundreds of millions of users - e.g., Amazon Goodreads
and Renren.com.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1460</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1460</id><created>2014-11-05</created><authors><author><keyname>Green</keyname><forenames>Oded</forenames></author><author><keyname>Dukhan</keyname><forenames>Marat</forenames></author><author><keyname>Vuduc</keyname><forenames>Richard</forenames></author></authors><title>Branch-Avoiding Graph Algorithms</title><categories>cs.DC cs.PF</categories><acm-class>C.0; C.4; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper quantifies the impact of branches and branch mispredictions on the
single-core performance for two classes of graph problems. Specifically, we
consider classical algorithms for computing connected components and
breadth-first search (BFS). We show that branch mispredictions are costly and
can reduce performance by as much as 30%-50%. This insight suggests that one
should seek graph algorithms and implementations that avoid branches.
  As a proof-of-concept, we devise such implementations for both the classic
top-down algorithm for BFS and the Shiloach-Vishkin algorithm for connected
components. We evaluate these implementations on current x86 and ARM-based
processors to show the efficacy of the approach. Our results suggest how both
compiler writers and architects might exploit this insight to improve graph
processing systems more broadly and create better systems for such problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1467</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1467</id><created>2014-11-05</created><updated>2015-12-28</updated><authors><author><keyname>Han</keyname><forenames>Yanjun</forenames></author><author><keyname>Jiao</keyname><forenames>Jiantao</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Minimax Estimation of Discrete Distributions under $\ell_1$ Loss</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Information Theory, Vol. 61, No. 11, pp
  6343-6354, Nov. 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the problem of discrete distribution estimation under $\ell_1$
loss. We provide non-asymptotic upper and lower bounds on the maximum risk of
the empirical distribution (the maximum likelihood estimator), and the minimax
risk in regimes where the alphabet size $S$ may grow with the number of
observations $n$. We show that among distributions with bounded entropy $H$,
the asymptotic maximum risk for the empirical distribution is $2H/\ln n$, while
the asymptotic minimax risk is $H/\ln n$. Moreover, Moreover, we show that a
hard-thresholding estimator oblivious to the unknown upper bound $H$, is
asymptotically minimax. However, if we constrain the estimates to lie in the
simplex of probability distributions, then the asymptotic minimax risk is again
$2H/\ln n$. We draw connections between our work and the literature on density
estimation, entropy estimation, total variation distance ($\ell_1$ divergence)
estimation, joint distribution estimation in stochastic processes, normal mean
estimation, and adaptive estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1483</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1483</id><created>2014-11-05</created><authors><author><keyname>Xie</keyname><forenames>Xinqian</forenames></author><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Training Design and Channel Estimation in Uplink Cloud Radio Access
  Networks</title><categories>cs.IT math.IT</categories><comments>11 pages, 3 figures, submitted to IEEE SPL</comments><doi>10.1109/LSP.2014.2380776</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To decrease the training overhead and improve the channel estimation accuracy
in uplink cloud radio access networks (C-RANs), a superimposed-segment training
design is proposed. The core idea of the proposal is that each mobile station
superimposes a periodic training sequence on the data signal, and each remote
radio heads prepends a separate pilot to the received signal before forwarding
it to the centralized base band unit pool. Moreover, a complex-exponential
basis-expansion-model based channel estimation algorithm to maximize a
posteriori probability is developed, where the basis-expansion-model
coefficients of access links (ALs) and the channel fading of wireless backhaul
links are first obtained, after which the time-domain channel samples of ALs
are restored in terms of maximizing the average effective signal-to-noise ratio
(AESNR). Simulation results show that the proposed channel estimation algorithm
can effectively decrease the estimation mean square error and increase the
AESNR in C-RANs, thus significantly outperforming the existing solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1488</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1488</id><created>2014-11-05</created><updated>2015-09-14</updated><authors><author><keyname>Anandkumar</keyname><forenames>Anima</forenames></author><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Janzamin</keyname><forenames>Majid</forenames></author></authors><title>Analyzing Tensor Power Method Dynamics in Overcomplete Regime</title><categories>cs.LG stat.ML</categories><comments>38 pages; analysis of noise added to the previous version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel analysis of the dynamics of tensor power iterations in the
overcomplete regime where the tensor CP rank is larger than the input
dimension. Finding the CP decomposition of an overcomplete tensor is NP-hard in
general. We consider the case where the tensor components are randomly drawn,
and show that the simple power iteration recovers the components with bounded
error under mild initialization conditions. We apply our analysis to
unsupervised learning of latent variable models, such as multi-view mixture
models and spherical Gaussian mixtures. Given the third order moment tensor, we
learn the parameters using tensor power iterations. We prove it can correctly
learn the model parameters when the number of hidden components $k$ is much
larger than the data dimension $d$, up to $k = o(d^{1.5})$. We initialize the
power iterations with data samples and prove its success under mild conditions
on the signal-to-noise ratio of the samples. Our analysis significantly expands
the class of latent variable models where spectral methods are applicable. Our
analysis also deals with noise in the input tensor leading to sample complexity
result in the application to learning latent variable models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1490</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1490</id><created>2014-11-05</created><updated>2014-12-04</updated><authors><author><keyname>Balcan</keyname><forenames>Maria-Florina</forenames></author><author><keyname>Blum</keyname><forenames>Avrim</forenames></author><author><keyname>Vempala</keyname><forenames>Santosh</forenames></author></authors><title>Efficient Representations for Life-Long Learning and Autoencoding</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been a long-standing goal in machine learning, as well as in AI more
generally, to develop life-long learning systems that learn many different
tasks over time, and reuse insights from tasks learned, &quot;learning to learn&quot; as
they do so. In this work we pose and provide efficient algorithms for several
natural theoretical formulations of this goal. Specifically, we consider the
problem of learning many different target functions over time, that share
certain commonalities that are initially unknown to the learning algorithm. Our
aim is to learn new internal representations as the algorithm learns new target
functions, that capture this commonality and allow subsequent learning tasks to
be solved more efficiently and from less data. We develop efficient algorithms
for two very different kinds of commonalities that target functions might
share: one based on learning common low-dimensional and unions of
low-dimensional subspaces and one based on learning nonlinear Boolean
combinations of features. Our algorithms for learning Boolean feature
combinations additionally have a dual interpretation, and can be viewed as
giving an efficient procedure for constructing near-optimal sparse Boolean
autoencoders under a natural &quot;anchor-set&quot; assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1497</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1497</id><created>2014-11-05</created><authors><author><keyname>Chen</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Wang</keyname><forenames>Dongming</forenames></author></authors><title>The Spaces of Data, Information, and Knowledge</title><categories>cs.AI</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the data space $D$ of any given data set $X$ and explain how
functions and relations are defined over $D$. From $D$ and for a specific
domain $\Delta$ we construct the information space $I$ of $X$ by interpreting
variables, functions, and explicit relations over $D$ in $\Delta$ and by
including other relations that $D$ implies under the interpretation in
$\Delta$. Then from $I$ we build up the knowledge space $K$ of $X$ as the
product of two spaces $K_T$ and $K_P$, where $K_T$ is obtained from $I$ by
using the induction principle to generalize propositional relations to
quantified relations, the deduction principle to generate new relations, and
standard mechanisms to validate relations and $K_P$ is the space of
specifications of methods with operational instructions which are valid in
$K_T$. Through our construction of the three topological spaces the following
key observation is made clear: the retrieval of information from the given data
set for $\Delta$ consists essentially in mining domain objects and relations,
and the discovery of knowledge from the retrieved information consists
essentially in applying the induction and deduction principles to generate
propositions, synthesizing and modeling the information to generate
specifications of methods with operational instructions, and validating the
propositions and specifications. Based on this observation, efficient
approaches may be designed to discover profound knowledge automatically from
simple data, as demonstrated by the result of our study in the case of
geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1503</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1503</id><created>2014-11-06</created><authors><author><keyname>Pan</keyname><forenames>Ran</forenames></author></authors><title>Tensor Transpose and Its Properties</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensor transpose is a higher order generalization of matrix transpose. In
this paper, we use permutations and symmetry group to define? the tensor
transpose. Then we discuss the classification and composition of tensor
transposes. Properties of tensor transpose are studied in relation to tensor
multiplication, tensor eigenvalues, tensor decompositions and tensor rank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1507</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1507</id><created>2014-11-06</created><authors><author><keyname>Ishii</keyname><forenames>Daisuke</forenames></author><author><keyname>Yoshizoe</keyname><forenames>Kazuki</forenames></author><author><keyname>Suzumura</keyname><forenames>Toyotaro</forenames></author></authors><title>Scalable Parallel Numerical CSP Solver</title><categories>cs.AI cs.DC</categories><comments>The final publication is available at Springer</comments><doi>10.1007/978-3-319-10428-7_30</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a parallel solver for numerical constraint satisfaction problems
(NCSPs) that can scale on a number of cores. Our proposed method runs worker
solvers on the available cores and simultaneously the workers cooperate for the
search space distribution and balancing. In the experiments, we attained up to
119-fold speedup using 256 cores of a parallel computer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1509</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1509</id><created>2014-11-06</created><authors><author><keyname>Chen</keyname><forenames>Zetao</forenames></author><author><keyname>Lam</keyname><forenames>Obadiah</forenames></author><author><keyname>Jacobson</keyname><forenames>Adam</forenames></author><author><keyname>Milford</keyname><forenames>Michael</forenames></author></authors><title>Convolutional Neural Network-based Place Recognition</title><categories>cs.CV cs.LG cs.NE</categories><comments>8 pages, 11 figures, this paper has been accepted by 2014
  Australasian Conference on Robotics and Automation (ACRA 2014) to be held in
  University of Melbourne, Dec 2~4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently Convolutional Neural Networks (CNNs) have been shown to achieve
state-of-the-art performance on various classification tasks. In this paper, we
present for the first time a place recognition technique based on CNN models,
by combining the powerful features learnt by CNNs with a spatial and sequential
filter. Applying the system to a 70 km benchmark place recognition dataset we
achieve a 75% increase in recall at 100% precision, significantly outperforming
all previous state of the art techniques. We also conduct a comprehensive
performance comparison of the utility of features from all 21 layers for place
recognition, both for the benchmark dataset and for a second dataset with more
significant viewpoint changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1519</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1519</id><created>2014-11-06</created><authors><author><keyname>Mulzer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Nguyen</keyname><forenames>Huy L.</forenames></author><author><keyname>Seiferth</keyname><forenames>Paul</forenames></author><author><keyname>Stein</keyname><forenames>Yannik</forenames></author></authors><title>Approximate k-flat Nearest Neighbor Search</title><categories>cs.CG cs.DS</categories><comments>22 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $k$ be a nonnegative integer. In the approximate $k$-flat nearest
neighbor ($k$-ANN) problem, we are given a set $P \subset \mathbb{R}^d$ of $n$
points in $d$-dimensional space and a fixed approximation factor $c &gt; 1$. Our
goal is to preprocess $P$ so that we can efficiently answer approximate
$k$-flat nearest neighbor queries: given a $k$-flat $F$, find a point in $P$
whose distance to $F$ is within a factor $c$ of the distance between $F$ and
the closest point in $P$. The case $k = 0$ corresponds to the well-studied
approximate nearest neighbor problem, for which a plethora of results are
known, both in low and high dimensions. The case $k = 1$ is called approximate
line nearest neighbor. In this case, we are aware of only one provably
efficient data structure, due to Andoni, Indyk, Krauthgamer, and Nguyen. For $k
\geq 2$, we know of no previous results.
  We present the first efficient data structure that can handle approximate
nearest neighbor queries for arbitrary $k$. We use a data structure for
$0$-ANN-queries as a black box, and the performance depends on the parameters
of the $0$-ANN solution: suppose we have an $0$-ANN structure with query time
$O(n^{\rho})$ and space requirement $O(n^{1+\sigma})$, for $\rho, \sigma &gt; 0$.
Then we can answer $k$-ANN queries in time $O(n^{k/(k + 1 - \rho) + t})$ and
space $O(n^{1+\sigma k/(k + 1 - \rho)} + n\log^{O(1/t)} n)$. Here, $t &gt; 0$ is
an arbitrary constant and the $O$-notation hides exponential factors in $k$,
$1/t$, and $c$ and polynomials in $d$. Our new data structures also give an
improvement in the space requirement over the previous result for $1$-ANN: we
can achieve near-linear space and sublinear query time, a further step towards
practical applications where space constitutes the bottleneck.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1531</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1531</id><created>2014-11-06</created><authors><author><keyname>Nam</keyname><forenames>Junyoung</forenames></author></authors><title>A Codebook-Based Limited Feedback System for Large-Scale MIMO</title><categories>cs.IT math.IT</categories><comments>9 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider limited feedback systems for FDD large-scale
(massive) MIMO. A new codebook-based framework for multiuser (MU) MIMO downlink
systems is introduced and then compared with an ideal non-codebook based
system. We are particularly interested in the less-known finite-rate feedback
regime where the number $M$ of transmit antennas and the number of users are of
the same order of magnitude and $M$ is large but finite, which is a typical
scenario of large-scale MIMO. We provide new findings in this regime and
identify some benefits of the new framework in terms of scheduling gain and
downlink dedicated pilot overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1537</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1537</id><created>2014-11-06</created><updated>2014-11-07</updated><authors><author><keyname>Gong</keyname><forenames>Boqing</forenames></author><author><keyname>Chao</keyname><forenames>Wei-lun</forenames></author><author><keyname>Grauman</keyname><forenames>Kristen</forenames></author><author><keyname>Sha</keyname><forenames>Fei</forenames></author></authors><title>Large-Margin Determinantal Point Processes</title><categories>stat.ML cs.CV cs.LG</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determinantal point processes (DPPs) offer a powerful approach to modeling
diversity in many applications where the goal is to select a diverse subset. We
study the problem of learning the parameters (the kernel matrix) of a DPP from
labeled training data. We make two contributions. First, we show how to
reparameterize a DPP's kernel matrix with multiple kernel functions, thus
enhancing modeling flexibility. Second, we propose a novel parameter estimation
technique based on the principle of large margin separation. In contrast to the
state-of-the-art method of maximum likelihood estimation, our large-margin loss
function explicitly models errors in selecting the target subsets, and it can
be customized to trade off different types of errors (precision vs. recall).
Extensive empirical studies validate our contributions, including applications
on challenging document and video summarization, where flexibility in modeling
the kernel matrix and balancing different errors is indispensable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1544</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1544</id><created>2014-11-06</created><authors><author><keyname>Di Fabio</keyname><forenames>Barbara</forenames></author><author><keyname>Landi</keyname><forenames>Claudia</forenames></author></authors><title>The edit distance for Reeb graphs of surfaces</title><categories>cs.CG</categories><comments>An extended abstract of this work appeared in: E. Barcucci et al.
  (Eds.): DGCI 2014, LNCS 8668, pp. 202-213, 2014. c</comments><msc-class>Primary 05C10, 68T10, Secondary 54C30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reeb graphs are structural descriptors that capture shape properties of a
topological space from the perspective of a chosen function. In this work we
define a combinatorial metric for Reeb graphs of orientable surfaces in terms
of the cost necessary to transform one graph into another by edit operations.
The main contributions of this paper are the stability property and the
optimality of this edit distance. More precisely, the stability result states
that changes in the functions, measured by the maximum norm, imply not greater
changes in the corresponding Reeb graphs, measured by the edit distance. The
optimality result states that our edit distance discriminates Reeb graphs
better than any other metric for Reeb graphs of surfaces satisfying the
stability property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1546</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1546</id><created>2014-11-06</created><authors><author><keyname>Adcock</keyname><forenames>Aaron B.</forenames></author><author><keyname>Sullivan</keyname><forenames>Blair D.</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author></authors><title>Tree decompositions and social graphs</title><categories>cs.DS cs.SI physics.soc-ph stat.AP</categories><comments>v1 has 47 pages, 22 figures, 8 tables, 102 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has established that large informatics graphs such as social and
information networks have non-trivial tree-like structure when viewed at
moderate size scales. Here, we present results from the first detailed
empirical evaluation of the use of tree decomposition (TD) heuristics for
structure identification and extraction in social graphs. Although TDs have
historically been used in structural graph theory and scientific computing, we
show that---even with existing TD heuristics developed for those very different
areas---TD methods can identify interesting structure in a wide range of
realistic informatics graphs. Among other things, we show that TD methods can
identify structures that correlate strongly with the core-periphery structure
of realistic networks, even when using simple greedy heuristics; we show that
the peripheral bags of these TDs correlate well with low-conductance
communities (when they exist) found using local spectral computations; and we
show that several types of large-scale &quot;ground-truth&quot; communities, defined by
demographic metadata on the nodes of the network, are well-localized in the
large-scale and/or peripheral structures of the TDs. Our detailed empirical
results for different TD heuristics on toy and synthetic networks help to
establish a baseline to understand better the behavior of the heuristics on
more complex real-world networks; and our results here suggest future
directions for the development of improved TD heuristics that are more
appropriate for improved structure identification in realistic networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1567</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1567</id><created>2014-11-06</created><authors><author><keyname>Holtkamp</keyname><forenames>Hauke</forenames></author><author><keyname>Dietl</keyname><forenames>Guido</forenames></author><author><keyname>Haas</keyname><forenames>Harald</forenames></author></authors><title>Distributed DTX Alignment with Memory</title><categories>cs.IT math.IT</categories><comments>13 pages, In 2014 IEEE International Conference on Communications
  (ICC), Page(s): 3481 - 3486</comments><doi>10.1109/ICC.2014.6883860</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper addresses the assignment of transmission and sleep time slots
between interfering transmitters with the objective of minimal power
consumption. In particular, we address the constructive alignment of
Discontinuous Transmission (DTX) time slots under link rate constraints. Due to
the complexity of the combinatorial optimization problem at hand, we resort to
heuristic assignment strategies. We derive four time slot alignment solutions
(sequential alignment, random alignment, p-persistent ranking and DTX alignment
with memory) and identify trade-offs. One solution, DTX alignment with memory,
addresses trade-offs of the other three by maintaining memory of past alignment
and channel quality to buffer short term changes in channel quality. All
strategies are found to exhibit similar convergence behavior, but different
power consumption and retransmission probabilities. DTX alignment with memory
is shown to achieve up to 40% savings in power consumption and more than 20%
lower retransmission probability than the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1569</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1569</id><created>2014-11-06</created><authors><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author></authors><title>Large System Analysis for Amplify &amp; Forward SIMO Multiple Access Channel
  with Ill-conditioned Second Hop</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relaying has been extensively studied during the last decades and has found
numerous applications in wireless communications. The simplest relaying method,
namely amplify and forward, has shown potential in MIMO multiple access
systems, when Gaussian fading channels are assumed for both hops. However, in
some cases ill conditioned channels may appear on the second hop. For example,
this impairment could affect cooperative BS systems with microwave link
backhauling, which involve strong line of sight channels with insufficient
scattering. In this paper, we consider a large system analysis of such as model
focusing on both optimal joint decoding and joint MMSE filtering receivers.
Analytical methods based on free probability are presented for calculating the
ergodic throughput, the MMSE error and the average SINR. Furthermore, the
performance degradation of the system throughput is evaluated considering
second hop impairments such as ill-conditioning and rank deficiency, while
high- and low-SNR limits are calculated for the considered performance metrics.
Finally, the cooperative BS system is compared to a conventional channel
resource division strategy and suitable operating points are proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1571</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1571</id><created>2014-11-06</created><authors><author><keyname>Holtkamp</keyname><forenames>Hauke</forenames></author><author><keyname>Auer</keyname><forenames>Gunther</forenames></author><author><keyname>Giannini</keyname><forenames>Vito</forenames></author><author><keyname>Haas</keyname><forenames>Harald</forenames></author></authors><title>A Parameterized Base Station Power Model</title><categories>cs.IT math.IT</categories><comments>9 pages</comments><journal-ref>Communications Letters, IEEE, Volume: 17 , Issue: 11, Publication
  Year: 2013 , Page(s): 2033 - 2035 Communications Letters, IEEE, Volume: 17 ,
  Issue: 11</journal-ref><doi>10.1109/LCOMM.2013.091213.131042</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Power models are needed to assess the power consumption of cellular Base
Station (BS) on an abstract level. Currently available models are either too
simplified to cover necessary aspects or overly complex. We provide a
parameterized linear power model which covers the individual aspects of a BS
which are relevant for a power consumption analysis, especially the
transmission bandwidth and the number of radio chains. Details reflecting the
underlying architecture are abstracted in favor of simplicity and
applicability. We identify current power-saving techniques of cellular networks
for which this model can be used. Furthermore, the parameter set of typical
commercial BS is provided and compared to the underlying complex model. The
complex model is well approximated while only using a fraction of the input
parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1580</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1580</id><created>2014-11-06</created><authors><author><keyname>Liu</keyname><forenames>Shuiyin</forenames></author><author><keyname>Hong</keyname><forenames>Yi</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>Guaranteeing Positive Secrecy Capacity with Finite-Rate Feedback using
  Artificial Noise</title><categories>cs.IT math.IT</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the impact of finite-rate feedback on the capacity of fading channels
has been extensively studied in the literature, not much attention has been
paid to this problem under secrecy constraint. In this work, we study the
ergodic secret capacity of a multiple-input multiple-output
multiple-antenna-eavesdropper (MIMOME) wiretap channel with quantized channel
state information (CSI) at the transmitter and perfect CSI at the legitimate
receiver, under the assumption that only the statistics of eavesdropper CSI is
known at the transmitter. We refine the analysis of the random vector
quantization (RVQ) based artificial noise (AN) scheme in [1], where a heuristic
upper bound on the secrecy rate loss, when compared to the perfect CSI case,
was given. We propose a lower bound on the ergodic secrecy capacity. We show
that the lower bound and the secrecy capacity with perfect CSI coincide
asymptotically as the number of feedback bits and the AN power go to infinity.
For practical applications, we propose a very efficient quantization codebook
construction method for the two transmit antennas case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1582</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1582</id><created>2014-11-06</created><authors><author><keyname>Arnon-Friedman</keyname><forenames>Rotem</forenames></author><author><keyname>Renner</keyname><forenames>Renato</forenames></author><author><keyname>Vidick</keyname><forenames>Thomas</forenames></author></authors><title>Non-signalling parallel repetition using de Finetti reductions</title><categories>quant-ph cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of multiplayer games, the parallel repetition problem can be
phrased as follows: given a game $G$ with optimal winning probability
$1-\alpha$ and its repeated version $G^n$ (in which $n$ games are played
together, in parallel), can the players use strategies that are substantially
better than ones in which each game is played independently? This question is
relevant in physics for the study of correlations and plays an important role
in computer science in the context of complexity and cryptography. In this work
the case of multiplayer non-signalling games is considered, i.e., the only
restriction on the players is that they are not allowed to communicate during
the game. For complete-support games (games where all possible combinations of
questions have non-zero probability to be asked) with any number of players we
prove a threshold theorem stating that the probability that non-signalling
players win more than a fraction $1-\alpha+\beta$ of the $n$ games is
exponentially small in $n\beta^2$, for every $0\leq \beta \leq \alpha$. For
games with incomplete support we derive a similar statement, for a slightly
modified form of repetition. The result is proved using a new technique, based
on a recent de Finetti theorem, which allows us to avoid central technical
difficulties that arise in standard proofs of parallel repetition theorems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1607</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1607</id><created>2014-11-06</created><updated>2015-07-19</updated><authors><author><keyname>Bezanson</keyname><forenames>Jeff</forenames></author><author><keyname>Edelman</keyname><forenames>Alan</forenames></author><author><keyname>Karpinski</keyname><forenames>Stefan</forenames></author><author><keyname>Shah</keyname><forenames>Viral B.</forenames></author></authors><title>Julia: A Fresh Approach to Numerical Computing</title><categories>cs.MS</categories><comments>37 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bridging cultures that have often been distant, Julia combines expertise from
the diverse fields of computer science and computational science to create a
new approach to numerical computing. Julia is designed to be easy and fast.
Julia questions notions generally held as &quot;laws of nature&quot; by practitioners of
numerical computing:
  1. High-level dynamic programs have to be slow.
  2. One must prototype in one language and then rewrite in another language
for speed or deployment, and
  3. There are parts of a system for the programmer, and other parts best left
untouched as they are built by the experts.
  We introduce the Julia programming language and its design --- a dance
between specialization and abstraction. Specialization allows for custom
treatment. Multiple dispatch, a technique from computer science, picks the
right algorithm for the right circumstance. Abstraction, what good computation
is really about, recognizes what remains the same after differences are
stripped away. Abstractions in mathematics are captured as code through another
technique from computer science, generic programming.
  Julia shows that one can have machine performance without sacrificing human
convenience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1608</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1608</id><created>2014-11-06</created><authors><author><keyname>P&#xe4;&#xe4;kk&#xf6;nen</keyname><forenames>Joonas</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author><author><keyname>Tirkkonen</keyname><forenames>Olav</forenames></author></authors><title>Device-to-Device Data Storage with Regenerating Codes</title><categories>cs.NI</categories><comments>6 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching data files directly on mobile user devices combined with
device-to-device (D2D) communications has recently been suggested to improve
the capacity of wireless net6works. We investigate the performance of
regenerating codes in terms of the total energy consumption of a cellular
network. We show that regenerating codes can offer large performance gains. It
turns out that using redundancy against storage node failures is only
beneficial if the popularity of the data is between certain thresholds. As our
major contribution, we investigate under which circumstances regenerating codes
with multiple redundant data fragments outdo uncoded caching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1611</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1611</id><created>2014-11-06</created><updated>2016-03-03</updated><authors><author><keyname>Farkas</keyname><forenames>B&#xe1;lint</forenames></author><author><keyname>Wegner</keyname><forenames>Sven-Ake</forenames></author></authors><title>Variations on Barbalat's Lemma</title><categories>math.OC cs.SY</categories><comments>5 pages</comments><msc-class>26A06 (Primary), 26A12, 26A16, 26A42, 46E39 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is not hard to prove that a uniformly continuous real function, whose
integral up to infinity exists, vanishes at infinity, and it is probably little
known that this statement runs under the name &quot;Barbalat's Lemma.&quot; In fact, the
latter name is frequently used in control theory, where the lemma is used to
obtain Lyapunov-like stability theorems for non-linear and non-autonomous
systems. Barbalat's Lemma is qualitative in the sense that it asserts that a
function has certain properties, here convergence to zero. Such qualitative
statements can typically be proved by &quot;soft analysis&quot;, such as indirect proofs.
Indeed, in the original 1959 paper by Barbalat, the lemma was proved by
contradiction and this proof prevails in the control theory textbooks. In this
short note we first give a direct, &quot;hard analyis&quot; proof of the lemma, yielding
quantitative results, i.e. rates of convergence to zero. This proof allows also
for immediate generalizations. Finally, we unify three different versions which
recently appeared and discuss their relation to the original lemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1619</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1619</id><created>2014-11-06</created><authors><author><keyname>Bonacina</keyname><forenames>Ilario</forenames></author><author><keyname>Galesi</keyname><forenames>Nicola</forenames></author><author><keyname>Huynh</keyname><forenames>Tony</forenames></author><author><keyname>Wollan</keyname><forenames>Paul</forenames></author></authors><title>Space proof complexity for random $3$-CNFs via a $(2-\epsilon)$-Hall's
  Theorem</title><categories>cs.CC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the space complexity of refuting $3$-CNFs in Resolution and
algebraic systems. No lower bound for refuting any family of $3$-CNFs was
previously known for the total space in resolution or for the monomial space in
algebraic systems. We prove that every Polynomial Calculus with Resolution
refutation of a random $3$-CNF $\phi$ in $n$ variables requires, with high
probability, $\Omega(n/\log n)$ distinct monomials to be kept simultaneously in
memory. The same construction also proves that every Resolution refutation
$\phi$ requires, with high probability, $\Omega(n/\log n)$ clauses each of
width $\Omega(n/\log n)$ to be kept at the same time in memory. This gives a
$\Omega(n^2/\log^2 n)$ lower bound for the total space needed in Resolution to
refute $\phi$.
  The main technical innovation is a variant of Hall's theorem. We show that in
bipartite graphs $G$ with bipartition $(L,R)$ and left-degree at most 3, $L$
can be covered by certain families of disjoint paths, called $(2,4)$-matchings,
provided that $L$ expands in $R$ by a factor of $(2-\epsilon)$, for $\epsilon &lt;
\frac{1}{23}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1623</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1623</id><created>2014-11-06</created><authors><author><keyname>Sigtia</keyname><forenames>Siddharth</forenames></author><author><keyname>Benetos</keyname><forenames>Emmanouil</forenames></author><author><keyname>Boulanger-Lewandowski</keyname><forenames>Nicolas</forenames></author><author><keyname>Weyde</keyname><forenames>Tillman</forenames></author><author><keyname>Garcez</keyname><forenames>Artur S. d'Avila</forenames></author><author><keyname>Dixon</keyname><forenames>Simon</forenames></author></authors><title>A Hybrid Recurrent Neural Network For Music Transcription</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of incorporating higher-level symbolic score-like
information into Automatic Music Transcription (AMT) systems to improve their
performance. We use recurrent neural networks (RNNs) and their variants as
music language models (MLMs) and present a generative architecture for
combining these models with predictions from a frame level acoustic classifier.
We also compare different neural network architectures for acoustic modeling.
The proposed model computes a distribution over possible output sequences given
the acoustic input signal and we present an algorithm for performing a global
search for good candidate transcriptions. The performance of the proposed model
is evaluated on piano music from the MAPS dataset and we observe that the
proposed model consistently outperforms existing transcription methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1629</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1629</id><created>2014-11-06</created><updated>2015-10-16</updated><authors><author><keyname>Davis</keyname><forenames>Ernest</forenames></author></authors><title>The Limitations of Standardized Science Tests as Benchmarks for
  Artificial Intelligence Research: Position Paper</title><categories>cs.AI</categories><comments>24 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this position paper, I argue that standardized tests for elementary
science such as SAT or Regents tests are not very good benchmarks for measuring
the progress of artificial intelligence systems in understanding basic science.
The primary problem is that these tests are designed to test aspects of
knowledge and ability that are challenging for people; the aspects that are
challenging for AI systems are very different. In particular, standardized
tests do not test knowledge that is obvious for people; none of this knowledge
can be assumed in AI systems. Individual standardized tests also have specific
features that are not necessarily appropriate for an AI benchmark. I analyze
the Physics subject SAT in some detail and the New York State Regents Science
test more briefly. I also argue that the apparent advantages offered by using
standardized tests are mostly either minor or illusory. The one major real
advantage is that the significance is easily explained to the public; but I
argue that even this is a somewhat mixed blessing. I conclude by arguing that,
first, more appropriate collections of exam style problems could be assembled,
and second, that there are better kinds of benchmarks than exam-style problems.
In an appendix I present a collection of sample exam-style problems that test
kinds of knowledge missing from the standardized tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1635</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1635</id><created>2014-11-06</created><authors><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author><author><keyname>Scharnhorst</keyname><forenames>Andrea</forenames></author></authors><title>Scientometrics and Information Retrieval - weak-links revitalized</title><categories>cs.IR cs.DL</categories><comments>8 pages, 1 figure, editorial for a special issue to appear in
  Scientometrics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This special issue brings together eight papers from experts of communities
which often have been perceived as different once: bibliometrics,
scientometrics and informetrics on the one side and information retrieval on
the other. The idea of this special issue started at the workshop &quot;Combining
Bibliometrics and Information Retrieval&quot; held at the 14th International
Conference of Scientometrics and Informetrics, Vienna, July 14-19, 2013. Our
motivation as guest editors started from the observation that main discourses
in both fields are different, that communities are only partly overlapping and
from the belief that a knowledge transfer would be profitable for both sides.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1638</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1638</id><created>2014-11-05</created><authors><author><keyname>Steinerberger</keyname><forenames>Stefan</forenames></author></authors><title>A filtering technique for Markov chains with applications to spectral
  embedding</title><categories>cs.DM</categories><comments>9 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral methods have proven to be a highly effective tool in understanding
the intrinsic geometry of a high-dimensional data set $\left\{x_i
\right\}_{i=1}^{n} \subset \mathbb{R}^d$. The key ingredient is the
construction of a Markov chain on the set, where transition probabilities
depend on the distance between elements, for example where for every $1 \leq j
\leq n$ the probability of going from $x_j$ to $x_i$ is proportional to $$
p_{ij} \sim \exp \left( -\frac{1}{\varepsilon}\|x_i
-x_j\|^2_{\ell^2(\mathbb{R}^d)}\right) \qquad
\mbox{where}~\varepsilon&gt;0~\mbox{is a free parameter}.$$ We propose a method
which increases the self-consistency of such Markov chains before spectral
methods are applied. Instead of directly using a Markov transition matrix $P$,
we set $p_{ii} = 0$ and rescale, thereby obtaining a transition matrix $P^*$
modeling a non-lazy random walk. We then create a new transition matrix $Q =
(q_{ij})_{i,j=1}^{n}$ by demanding that for fixed $j$ the quantity $q_{ij}$ be
proportional to $$ q_{ij} \sim \min((P^*)_{ij}, ((P^*)^2)_{ij}, \dots,
((P^*)^k)_{ij}) \qquad \mbox{where usually}~ k=2.$$ We consider several
classical data sets, show that this simple method can increase the efficiency
of spectral methods and prove that it can correct randomly introduced errors in
the kernel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1646</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1646</id><created>2014-11-06</created><authors><author><keyname>Gisbrecht</keyname><forenames>Andrej</forenames></author><author><keyname>Schleif</keyname><forenames>Frank-Michael</forenames></author></authors><title>Metric and non-metric proximity transformations at linear costs</title><categories>cs.DS</categories><msc-class>46C20, 68T05, 68W25</msc-class><acm-class>G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain specific (dis-)similarity or proximity measures used e.g. in alignment
algorithms of sequence data, are popular to analyze complex data objects and to
cover domain specific data properties. Without an underlying vector space these
data are given as pairwise (dis-)similarities only. The few available methods
for such data focus widely on similarities and do not scale to large data sets.
Kernel methods are very effective for metric similarity matrices, also at large
scale, but costly transformations are necessary starting with non-metric (dis-)
similarities. We propose an integrative combination of Nystroem approximation,
potential double centering and eigenvalue correction to obtain valid kernel
matrices at linear costs in the number of samples. By the proposed approach
effective kernel approaches, become accessible. Experiments with several larger
(dis-)similarity data sets show that the proposed method achieves much better
runtime performance than the standard strategy while keeping competitive model
accuracy. The main contribution is an efficient and accurate technique, to
convert (potentially non-metric) large scale dissimilarity matrices into
approximated positive semi-definite kernel matrices at linear costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1652</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1652</id><created>2014-11-06</created><updated>2014-11-24</updated><authors><author><keyname>Goldberg</keyname><forenames>Felix</forenames></author></authors><title>Chip-firing may be much faster than you think</title><categories>math.CO cs.DM cs.GT</categories><msc-class>05C57, 91A50, 91A43, 05C50, 68Q80, 15A09</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new bound (Theorem \ref{thm:main}) for the duration of the chip-firing game
with $N$ chips on a $n$-vertex graph is obtained, by a careful analysis of the
pseudo-inverse of the discrete Laplacian matrix of the graph. This new bound is
expressed in terms of the entries of the pseudo-inverse.
  It is shown (Section 5) to be always better than the classic bound due to
Bj{\&quot;o}rner, Lov\'{a}sz and Shor. In some cases the improvement is dramatic.
  For instance: for strongly regular graphs the classic and the new bounds
reduce to $O(nN)$ and $O(n+N)$, respectively. For dense regular graphs -
$d=(\frac{1}{2}+\epsilon)n$ - the classic and the new bounds reduce to $O(N)$
and $O(n)$, respectively.
  This is a snapshot of a work in progress, so further results in this vein are
in the works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1668</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1668</id><created>2014-10-26</created><authors><author><keyname>Bera</keyname><forenames>Sahadev</forenames></author><author><keyname>Pal</keyname><forenames>Shyamosree</forenames></author><author><keyname>Bhowmick</keyname><forenames>Partha</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Bhargab B.</forenames></author></authors><title>On Chord and Sagitta in ${\mathbb Z}^2$: An Analysis towards Fast and
  Robust Circular Arc Detection</title><categories>cs.CG cs.CV</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although chord and sagitta, when considered in tandem, may reflect many
underlying geometric properties of circles on the Euclidean plane, their
implications on the digital plane are not yet well-understood. In this paper,
we explore some of their fundamental properties on the digital plane that have
a strong bearing on the unsupervised detection of circles and circular arcs in
a digital image. We show that although the chord-and-sagitta properties of a
real circle do not readily migrate to the digital plane, they can indeed be
used for the analysis in the discrete domain based on certain bounds on their
deviations, which are derived from the real domain. In particular, we derive an
upper bound on the circumferential angular deviation of a point in the context
of chord property, and an upper bound on the relative error in radius
estimation with regard to the sagitta property. Using these two bounds, we
design a novel algorithm for the detection and parameterization of circles and
circular arcs, which does not require any heuristic initialization or manual
tuning. The chord property is deployed for the detection of circular arcs,
whereas the sagitta property is used to estimate their centers and radii.
Finally, to improve the accuracy of estimation, the notion of restricted Hough
transform is used. Experimental results demonstrate superior efficiency and
robustness of the proposed methodology compared to existing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1680</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1680</id><created>2014-11-06</created><authors><author><keyname>Fooladivanda</keyname><forenames>D.</forenames></author><author><keyname>Mancini</keyname><forenames>G.</forenames></author><author><keyname>Garg</keyname><forenames>S.</forenames></author><author><keyname>Rosenberg</keyname><forenames>C.</forenames></author></authors><title>State of Charge Evolution Equations for Flywheels</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mathematical state-of-charge evolution equation is present for Flywheel
Energy Storage Systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1705</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1705</id><created>2014-11-05</created><authors><author><keyname>Xue</keyname><forenames>Yuanyi</forenames></author><author><keyname>Erkin</keyname><forenames>Beril</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author></authors><title>A Novel No-reference Video Quality Metric for Evaluating Temporal
  Jerkiness due to Frame Freezing</title><categories>cs.MM</categories><doi>10.1109/TMM.2014.2368272</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a novel no-reference (NR) video quality metric that
evaluates the impact of frame freezing due to either packet loss or late
arrival. Our metric uses a trained neural network acting on features that are
chosen to capture the impact of frame freezing on the perceived quality. The
considered features include the number of freezes, freeze duration statistics,
inter-freeze distance statistics, frame difference before and after the freeze,
normal frame difference, and the ratio of them. We use the neural network to
find the mapping between features and subjective test scores. We optimize the
network structure and the feature selection through a cross validation
procedure, using training samples extracted from both VQEG and LIVE video
databases. The resulting feature set and network structure yields accurate
quality prediction for both the training data containing 54 test videos and a
separate testing dataset including 14 videos, with Pearson Correlation
Coefficients greater than 0.9 and 0.8 for the training set and the testing set,
respectively. Our proposed metric has low complexity and could be utilized in a
system with realtime processing constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1723</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1723</id><created>2014-11-06</created><updated>2015-11-30</updated><authors><author><keyname>Doyle</keyname><forenames>C.</forenames></author><author><keyname>Sreenivasan</keyname><forenames>S.</forenames></author><author><keyname>Szymanski</keyname><forenames>B. K.</forenames></author><author><keyname>Korniss</keyname><forenames>G.</forenames></author></authors><title>Social consensus and tipping points with opinion inertia</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>final version, reflecting changes in response to referees' comments</comments><journal-ref>Physica A 443, 316-323 (2016)</journal-ref><doi>10.1016/j.physa.2015.09.081</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When opinions, behaviors or ideas diffuse within a population, some are
invariably stickier than others. The stickier the opinion, behavior or idea,
the greater is an individual's inertia to replace it with an alternative. Here
we study the effect of stickiness of opinions in a two-opinion model, where
individuals change their opinion only after a certain number of consecutive
encounters with the alternative opinion. Assuming that one opinion has a fixed
stickiness, we investigate how the critical size of the competing opinion
required to tip over the entire population varies as a function of the
competing opinion's stickiness. We analyze this scenario for the case of a
complete-graph topology through simulations, and through a semi-analytical
approach which yields an upper bound for the critical minority size. We present
analogous simulation results for the case of the Erd\H{o}s-R\'enyi random
network. Finally, we investigate the coarsening properties of sticky opinion
spreading on two-dimensional lattices, and show that the presence of stickiness
gives rise to an effective surface tension that causes the coarsening behavior
to become curvature-driven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1733</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1733</id><created>2014-11-06</created><authors><author><keyname>Portnoi</keyname><forenames>Marcos</forenames></author><author><keyname>Shen</keyname><forenames>Chien-Chung</forenames></author></authors><title>Wireless-Delimited Secure Zones with Encrypted Attribute-Based Broadcast
  for Safe Firearms</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents an application of the highly expressive Attribute-Based
Encryption to implement wireless-delimited Secure Zones for firearms. Within
these zones, radio-transmitted local policies based on attributes of the
consumer and the firearm are received by embedded hardware in the firearms,
which then advises the consumer about safe operations. The Secure Zones utilize
Attribute-Based Encryption to encode the policies and consumer or user
attributes, and providing privacy and security through it cryptography. We
describe a holistic approach to evolving the firearm to a cyber-physical system
to aid in augmenting safety. We introduce a conceptual model for a firearm
equipped with sensors and a context-aware software agent. Based on the
information from the sensors, the agent can access the context and inform the
consumer of potential unsafe operations. To support Secure Zones and the
cyber-physical firearm model, we propose a Key Infrastructure Scheme for key
generation, distribution, and management, and a Context-Aware Software Agent
Framework for Firearms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1743</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1743</id><created>2014-11-06</created><authors><author><keyname>D'Orsogna</keyname><forenames>Maria R.</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Statistical physics of crime: A review</title><categories>physics.soc-ph cs.SI nlin.AO q-bio.PE</categories><comments>15 two-column pages, 11 figures; accepted for publication in Physics
  of Life Reviews</comments><journal-ref>Phys. Life Rev. 12 (2015) 1-21</journal-ref><doi>10.1016/j.plrev.2014.11.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Containing the spreading of crime in urban societies remains a major
challenge. Empirical evidence suggests that, left unchecked, crimes may be
recurrent and proliferate. On the other hand, eradicating a culture of crime
may be difficult, especially under extreme social circumstances that impair the
creation of a shared sense of social responsibility. Although our understanding
of the mechanisms that drive the emergence and diffusion of crime is still
incomplete, recent research highlights applied mathematics and methods of
statistical physics as valuable theoretical resources that may help us better
understand criminal activity. We review different approaches aimed at modeling
and improving our understanding of crime, focusing on the nucleation of crime
hotspots using partial differential equations, self-exciting point process and
agent-based modeling, adversarial evolutionary games, and the network science
behind the formation of gangs and large-scale organized crime. We emphasize
that statistical physics of crime can relevantly inform the design of
successful crime prevention strategies, as well as improve the accuracy of
expectations about how different policing interventions should impact malicious
human activity deviating from social norms. We also outline possible directions
for future research, related to the effects of social and coevolving networks
and to the hierarchical growth of criminal structures due to self-organization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1745</identifier>
 <datestamp>2014-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1745</id><created>2014-11-06</created><authors><author><keyname>Cifuentes</keyname><forenames>Diego</forenames></author><author><keyname>Parrilo</keyname><forenames>Pablo</forenames></author></authors><title>Exploiting chordal structure in polynomial ideals: a Gr\&quot;obner bases
  approach</title><categories>cs.SC math.AC math.AG math.OC</categories><comments>39 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chordal structure and bounded treewidth allow for efficient computation in
numerical linear algebra, graphical models, constraint satisfaction and many
other areas. In this paper, we begin the study of how to exploit chordal
structure in computational algebraic geometry, and in particular, for solving
polynomial systems. The structure of a system of polynomial equations can be
described in terms of a graph. By carefully exploiting the properties of this
graph (in particular, its chordal completions), more efficient algorithms can
be developed. To this end, we develop a new technique, which we refer to as
chordal elimination, that relies on elimination theory and Gr\&quot;obner bases. By
maintaining graph structure throughout the process, chordal elimination can
outperform standard Gr\&quot;obner basis algorithms in many cases. The reason is
that all computations are done on &quot;smaller&quot; rings, of size equal to the
treewidth of the graph. In particular, for a restricted class of ideals, the
computational complexity is linear in the number of variables. Chordal
structure arises in many relevant applications. We demonstrate the suitability
of our methods in examples from graph colorings, cryptography, sensor
localization and differential equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1751</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1751</id><created>2014-11-06</created><updated>2015-06-24</updated><authors><author><keyname>Meir</keyname><forenames>Reshef</forenames></author><author><keyname>Parkes</keyname><forenames>David</forenames></author></authors><title>Playing the Wrong Game: Smoothness Bounds for Congestion Games with
  Behavioral Biases</title><categories>cs.GT cs.MA</categories><comments>Full version of a paper presented at NetEcon'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many situations a player may act so as to maximize a perceived utility
that is not exactly her utility function, but rather some other, biased,
utility. Examples of such biased utility functions are common in behavioral
economics, and include risk attitudes, altruism, present-bias and so on. When
analyzing a game, one may ask how inefficiency, measured by the Price of
Anarchy (PoA) is affected by the perceived utilities. The smoothness method
[Roughgarden and Tardos'04, Roughgarden'09] naturally extends to games with
such perceived utilities or costs, regardless of the game or the behavioral
bias. We show that such biased-smoothness is broadly applicable in the context
of nonatomic congestion games. First, we show that on series-parallel networks
we can use smoothness to yield PoA bounds even for diverse populations with
different biases. Second, we identify various classes of cost functions and
biases that are smooth, thereby substantially improving some recent results
from the literature. Finally we show some initial implications for atomic
congestion games. %
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1752</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1752</id><created>2014-11-06</created><authors><author><keyname>Prasad</keyname><forenames>Adarsh</forenames></author><author><keyname>Jegelka</keyname><forenames>Stefanie</forenames></author><author><keyname>Batra</keyname><forenames>Dhruv</forenames></author></authors><title>Submodular meets Structured: Finding Diverse Subsets in
  Exponentially-Large Structured Item Sets</title><categories>cs.LG cs.AI cs.CV cs.IR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To cope with the high level of ambiguity faced in domains such as Computer
Vision or Natural Language processing, robust prediction methods often search
for a diverse set of high-quality candidate solutions or proposals. In
structured prediction problems, this becomes a daunting task, as the solution
space (image labelings, sentence parses, etc.) is exponentially large. We study
greedy algorithms for finding a diverse subset of solutions in
structured-output spaces by drawing new connections between submodular
functions over combinatorial item sets and High-Order Potentials (HOPs) studied
for graphical models. Specifically, we show via examples that when marginal
gains of submodular diversity functions allow structured representations, this
enables efficient (sub-linear time) approximate maximization by reducing the
greedy augmentation step to inference in a factor graph with appropriately
constructed HOPs. We discuss benefits, tradeoffs, and show that our
constructions lead to significantly better proposals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1784</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1784</id><created>2014-11-06</created><authors><author><keyname>Mirza</keyname><forenames>Mehdi</forenames></author><author><keyname>Osindero</keyname><forenames>Simon</forenames></author></authors><title>Conditional Generative Adversarial Nets</title><categories>cs.LG cs.AI cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative Adversarial Nets [8] were recently introduced as a novel way to
train generative models. In this work we introduce the conditional version of
generative adversarial nets, which can be constructed by simply feeding the
data, y, we wish to condition on to both the generator and discriminator. We
show that this model can generate MNIST digits conditioned on class labels. We
also illustrate how this model could be used to learn a multi-modal model, and
provide preliminary examples of an application to image tagging in which we
demonstrate how this approach can generate descriptive tags which are not part
of training labels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1792</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1792</id><created>2014-11-06</created><authors><author><keyname>Yosinski</keyname><forenames>Jason</forenames></author><author><keyname>Clune</keyname><forenames>Jeff</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author><author><keyname>Lipson</keyname><forenames>Hod</forenames></author></authors><title>How transferable are features in deep neural networks?</title><categories>cs.LG cs.NE</categories><comments>To appear in Advances in Neural Information Processing Systems 27
  (NIPS 2014)</comments><journal-ref>Advances in Neural Information Processing Systems 27, pages
  3320-3328. Dec. 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many deep neural networks trained on natural images exhibit a curious
phenomenon in common: on the first layer they learn features similar to Gabor
filters and color blobs. Such first-layer features appear not to be specific to
a particular dataset or task, but general in that they are applicable to many
datasets and tasks. Features must eventually transition from general to
specific by the last layer of the network, but this transition has not been
studied extensively. In this paper we experimentally quantify the generality
versus specificity of neurons in each layer of a deep convolutional neural
network and report a few surprising results. Transferability is negatively
affected by two distinct issues: (1) the specialization of higher layer neurons
to their original task at the expense of performance on the target task, which
was expected, and (2) optimization difficulties related to splitting networks
between co-adapted neurons, which was not expected. In an example network
trained on ImageNet, we demonstrate that either of these two issues may
dominate, depending on whether features are transferred from the bottom,
middle, or top of the network. We also document that the transferability of
features decreases as the distance between the base task and target task
increases, but that transferring features even from distant tasks can be better
than using random features. A final surprising result is that initializing a
network with transferred features from almost any number of layers can produce
a boost to generalization that lingers even after fine-tuning to the target
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1801</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1801</id><created>2014-11-06</created><authors><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author></authors><title>Space-Time Encoded MISO Broadcast Channel with Outdated CSIT: An Error
  Rate and Diversity Performance Analysis</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studies of the MISO Broadcast Channel (BC) with delayed Channel State
Information at the Transmitter (CSIT) have so far focused on the sum-rate and
Degrees-of-Freedom (DoF) region analysis. In this paper, we investigate for the
first time the error rate performance at finite SNR and the
diversity-multiplexing tradeoff (DMT) at infinite SNR of a space-time encoded
transmission over a two-user MISO BC with delayed CSIT. We consider the
so-called MAT protocol obtained by Maddah-Ali and Tse, which was shown to
provide 33% DoF enhancement over TDMA. While the asymptotic DMT analysis shows
that MAT is always preferable to TDMA, the Pairwise Error Probability analysis
at finite SNR shows that MAT is in fact not always a better alternative to
TDMA. Benefits can be obtained over TDMA only at very high rate or once
concatenated with a full-rate full-diversity space-time code. The analysis is
also extended to spatially correlated channels and the influence of transmit
correlation matrices and user pairing strategies on the performance are
discussed. Relying on statistical CSIT, signal constellations are further
optimized to improve the error rate performance of MAT and make it insensitive
to user orthogonality. Finally, other transmission strategies relying on
delayed CSIT are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1804</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1804</id><created>2014-11-06</created><updated>2014-12-02</updated><authors><author><keyname>Liang</keyname><forenames>Dawen</forenames></author><author><keyname>Hoffman</keyname><forenames>Matthew D.</forenames></author></authors><title>Beta Process Non-negative Matrix Factorization with Stochastic
  Structured Mean-Field Variational Inference</title><categories>stat.ML cs.LG</categories><comments>6 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Beta process is the standard nonparametric Bayesian prior for latent factor
model. In this paper, we derive a structured mean-field variational inference
algorithm for a beta process non-negative matrix factorization (NMF) model with
Poisson likelihood. Unlike the linear Gaussian model, which is well-studied in
the nonparametric Bayesian literature, NMF model with beta process prior does
not enjoy the conjugacy. We leverage the recently developed stochastic
structured mean-field variational inference to relax the conjugacy constraint
and restore the dependencies among the latent variables in the approximating
variational distribution. Preliminary results on both synthetic and real
examples demonstrate that the proposed inference algorithm can reasonably
recover the hidden structure of the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1810</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1810</id><created>2014-11-06</created><updated>2015-10-30</updated><authors><author><keyname>Mandt</keyname><forenames>Stephan</forenames></author><author><keyname>McInerney</keyname><forenames>James</forenames></author><author><keyname>Abrol</keyname><forenames>Farhan</forenames></author><author><keyname>Ranganath</keyname><forenames>Rajesh</forenames></author><author><keyname>Blei</keyname><forenames>David</forenames></author></authors><title>Multicanonical Stochastic Variational Inference</title><categories>stat.ML cs.LG</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic variational inference (SVI) enables approximate posterior
inference with large data sets for otherwise intractable models, but like all
variational inference algorithms it suffers from local optima. Deterministic
annealing, which we formulate here for the generic class of conditionally
conjugate exponential family models, uses a temperature parameter that
deterministically deforms the objective, and reduce this parameter over the
course of the optimization to recover the original variational set-up. A
well-known drawback in annealing approaches is the choice of the annealing
schedule. We therefore introduce multicanonical variational inference (MVI), a
variational algorithm that operates at several annealing temperatures
simultaneously. This algorithm gives us adaptive annealing schedules. Compared
to the traditional SVI algorithm, both approaches find improved predictive
likelihoods on held-out data, with MVI being close to the best-tuned annealing
schedule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1821</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1821</id><created>2014-11-06</created><authors><author><keyname>Uniyal</keyname><forenames>Deepak</forenames></author><author><keyname>Raychoudhury</keyname><forenames>Vaskar</forenames></author></authors><title>Pervasive Healthcare-A Comprehensive Survey of Tools and Techniques</title><categories>cs.CY</categories><comments>48 pages, 2 figures, 14 tables</comments><acm-class>A.1; K.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pervasive healthcare is an emerging technology that aims to provide
round-the-clock monitoring of several vital signs of patients using various
health sensors, specialized communication protocols, and intelligent
context-aware applications. Pervasive healthcare applications proactively
contact the caregiver provided any abnormality arises in the health condition
of a monitored patient. It has been a boon to the patients suffering from
different diseases and requiring continuous monitoring and care, such as,
disabled individuals, elderly and weak persons living alone, children of
different ages, and adults who are susceptible to near-fatal falls or sudden
increases in blood pressure, heart rates, stress level, etc. Existing surveys
on pervasive healthcare cover generic techniques or a particular application,
like fall detection. In this paper, we carry out a comprehensive coverage of
several common disorders addressed by pervasive healthcare in recent years. We
roughly classify different diseases by age groups of patients and then discuss
various hardware and software tools and techniques to detect or treat them. We
have also included detailed tabular classification of a large selection of
significant research articles in pervasive healthcare.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1822</identifier>
 <datestamp>2014-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1822</id><created>2014-11-06</created><updated>2014-12-09</updated><authors><author><keyname>Raut</keyname><forenames>Manoj K.</forenames></author><author><keyname>Gupta</keyname><forenames>Manish K.</forenames></author></authors><title>On Octonary Codes and their Covering Radii</title><categories>cs.IT math.IT</categories><comments>16 pages, some errors fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces new reduction and torsion codes for an octonary code
and determines their basic properties. These could be useful for the
classification of self-orthogonal and self dual codes over $\mathbb{Z}_8$. We
also focus our attention on covering radius problem of octonary codes. In
particular, we determine lower and upper bounds of the covering radius of
several classes of Repetition codes, Simplex codes of Type $\alpha$ and Type
$\beta$ and their duals, MacDonald codes, and Reed-Muller codes over
$\mathbb{Z}_8$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1829</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1829</id><created>2014-11-07</created><authors><author><keyname>Song</keyname><forenames>Tianyu</forenames></author><author><keyname>Kam</keyname><forenames>Pooi-Yuen</forenames></author></authors><title>Efficient Direct Detection of M-PAM Sequences with Implicit CSI
  Acquisition for The FSO System</title><categories>cs.IT math.IT</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compared to on-off keying (OOK), M-ary pulse amplitude modulation (M-PAM,
M&gt;2) is more spectrally efficient. However, to detect M-PAM signals reliably,
the requirement of accurate channel state information is more stringent.
Previously, for OOK systems, we have developed a receiver that requires few
pilot symbols and can jointly detect the data sequence and estimate the unknown
channel gain implicitly. In this paper, using the same approach, we extend our
previous work and derive a generalized receiver for M-PAM systems. A
Viterbi-type trellis-search algorithm coupled with a selective-store strategy
is adopted, resulting in a low implementation complexity and a low memory
requirement. Therefore, the receiver is efficient in terms of energy, spectra,
implementation complexity and memory. Using theoretical analysis, we show that
its error performance approaches that of maximum likelihood detection with
perfect knowledge of the channel gain, as the observation window length
increases. Also, simulation results are presented to justify the theoretical
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1830</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1830</id><created>2014-11-07</created><updated>2015-01-29</updated><authors><author><keyname>Fasy</keyname><forenames>Brittany Terese</forenames></author><author><keyname>Kim</keyname><forenames>Jisu</forenames></author><author><keyname>Lecci</keyname><forenames>Fabrizio</forenames></author><author><keyname>Maria</keyname><forenames>Cl&#xe9;ment</forenames></author></authors><title>Introduction to the R package TDA</title><categories>cs.MS cs.CG stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a short tutorial and introduction to using the R package TDA,
which provides some tools for Topological Data Analysis. In particular, it
includes implementations of functions that, given some data, provide
topological information about the underlying space, such as the distance
function, the distance to a measure, the kNN density estimator, the kernel
density estimator, and the kernel distance. The salient topological features of
the sublevel sets (or superlevel sets) of these functions can be quantified
with persistent homology. We provide an R interface for the efficient
algorithms of the C++ libraries GUDHI, Dionysus and PHAT, including a function
for the persistent homology of the Rips filtration, and one for the persistent
homology of sublevel sets (or superlevel sets) of arbitrary functions evaluated
over a grid of points. The significance of the features in the resulting
persistence diagrams can be analyzed with functions that implement recently
developed statistical methods. The R package TDA also includes the
implementation of an algorithm for density clustering, which allows us to
identify the spatial organization of the probability mass associated to a
density function and visualize it by means of a dendrogram, the cluster tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1841</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1841</id><created>2014-11-07</created><updated>2015-05-19</updated><authors><author><keyname>Esmaeilzadeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Sadeghi</keyname><forenames>Parastoo</forenames></author><author><keyname>Aboutorab</keyname><forenames>Neda</forenames></author></authors><title>Random Linear Network Coding for Wireless Layered Video Broadcast:
  General Design Methods for Adaptive Feedback-free Transmission</title><categories>cs.IT cs.NI math.IT</categories><comments>15 pages, 12 figures, 3 tables, Under 2nd round of review, IEEE
  Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of broadcasting layered video streams over
heterogeneous single-hop wireless networks using feedback-free random linear
network coding (RLNC). We combine RLNC with unequal error protection (UEP) and
our main purpose is twofold. First, to systematically investigate the benefits
of UEP+RLNC layered approach in servicing users with different reception
capabilities. Second, to study the effect of not using feedback, by comparing
feedback-free schemes with idealistic full-feedback schemes. To these ends, we
study `expected percentage of decoded frames' as a key content-independent
performance metric and propose a general framework for calculation of this
metric, which can highlight the effect of key system, video and channel
parameters. We study the effect of number of layers and propose a scheme that
selects the optimum number of layers adaptively to achieve the highest
performance. Assessing the proposed schemes with real H.264 test streams, the
trade-offs among the users' performances are discussed and the gain of adaptive
selection of number of layers to improve the trade-offs is shown. Furthermore,
it is observed that the performance gap between the proposed feedback-free
scheme and the idealistic scheme is very small and the adaptive selection of
number of video layers further closes the gap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1876</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1876</id><created>2014-11-07</created><authors><author><keyname>Kumar</keyname><forenames>Puneet</forenames></author><author><keyname>Kumar</keyname><forenames>Dharminder</forenames></author><author><keyname>Kumar</keyname><forenames>Narendra</forenames></author></authors><title>E-Governance in India: Definitions, Challenges and Solutions</title><categories>cs.CY</categories><journal-ref>International Journal of Computer Applications 101(16):6-8,
  September 2014</journal-ref><doi>10.5120/17769-7408</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Government of India is transcending from traditional modus operandi of
governance towards technological involvement in the process of governance.
Currently, the Government of India is in the transition phase and seamlessly
unleashing the power of ICT in governance. The government is spending an
enormous amount of finances in deployment of e-governance, but, are these
efforts are going in the appropriate direction and leads towards intended
results? What do the people percept from the concept of e-governance? What is
the global perspective about perception of e-governance? What are the major
challenges confronting the deployment of e-governance? In this attempt the
authors have made an attempt to riposte aforesaid issues. Moreover, the authors
have also suggested some plausible suggestions which may help in successful and
sustainable deployment of e-governance in India.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1879</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1879</id><created>2014-11-07</created><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>De Carufel</keyname><forenames>Jean-Lou</forenames></author><author><keyname>Grimm</keyname><forenames>Carsten</forenames></author><author><keyname>Maheshwari</keyname><forenames>Anil</forenames></author><author><keyname>Smid</keyname><forenames>Michiel</forenames></author></authors><title>Optimal Data Structures for Farthest-Point Queries in Cactus Networks</title><categories>cs.DS cs.CC</categories><doi>10.7155/jgaa.00345</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the continuum of points on the edges of a network, i.e., a
connected, undirected graph with positive edge weights. We measure the distance
between these points in terms of the weighted shortest path distance, called
the network distance. Within this metric space, we study farthest points and
farthest distances. We introduce optimal data structures supporting queries for
the farthest distance and the farthest points on trees, cycles, uni-cyclic
networks, and cactus networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1897</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1897</id><created>2014-11-07</created><authors><author><keyname>Islam</keyname><forenames>Md Baharul</forenames></author><author><keyname>Ahmed</keyname><forenames>Arif</forenames></author><author><keyname>Islam</keyname><forenames>Md Kabirul</forenames></author><author><keyname>Shamsuddin</keyname><forenames>Abu Kalam</forenames></author></authors><title>Child Education Through Animation: An Experimental Study</title><categories>cs.CY cs.MM</categories><journal-ref>International Journal of Computer Graphics and Animation, Vol. 4,
  No. 4, October 2014 pg 43-52</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Teachers have tried to teach their students by introducing text books along
with verbal instructions in traditional education system. However, teaching and
learning methods could be changed for developing Information and Communication
Technology. It's time to adapt students with interactive learning system so
that they can improve their learning, catching, and memorizing capabilities. It
is indispensable to create high quality and realistic leaning environment for
students. Visual learning can be easier to understand and deal with their
learning. We developed visual learning materials in the form of video for
students of primary level using different multimedia application tools. The
objective of this paper is to examine the impact of students abilities to
acquire new knowledge or skills through visual learning materials and blended
leaning that is integration of visual learning materials with teachers
instructions. We visited a primary school in Dhaka city for this study and
conducted teaching with three different groups of students, (i) teacher taught
students by traditional system on same materials and marked level of students
ability to adapt by a set of questions, (ii) another group was taught with only
visual learning material and assessment was done with 15 questionnaires, (iii)
the third group was taught with the video of solar system combined with
teachers instructions and assessed with the same questionnaires. This
integration of visual materials with verbal instructions is a blended approach
of learning. The interactive blended approach greatly promoted students ability
of acquisition of knowledge and skills. Students response and perception were
very positive towards the blended technique than the other two methods. This
interactive blending leaning system may be an appropriate method especially for
school children.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1898</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1898</id><created>2014-11-07</created><authors><author><keyname>Kumar</keyname><forenames>M. Ravichandra</forenames></author><author><keyname>Teja</keyname><forenames>B. Ravi</forenames></author></authors><title>A Novel Uncertainty Parameter SR (Signal To Residual Spectrum Ratio)
  Evaluation Approach For Speech Enhancement</title><categories>cs.SD</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Usually, hearing impaired people use hearing aids which are implemented with
speech enhancement algorithms. Estimation of speech and estimation of nose are
the components in single channel speech enhancement system. The main objective
of any speech enhancement algorithm is estimation of noise power spectrum for
non stationary environment. VAD (Voice Activity Detector) is used to identify
speech pauses and during these pauses only estimation of noise. MMSE (Minimum
Mean Square Error) speech enhancement algorithm did not enhance the
intelligibility, quality and listener fatigues are the perceptual aspects of
speech. Novel evaluation approach SR (Signal to Residual spectrum ratio) based
on uncertainty parameter introduced for the benefits of hearing impaired people
in non stationary environments to control distortions. By estimation and
updating of noise based on division of original pure signal into three parts
such as pure speech, quasi speech and non speech frames based on multiple
threshold conditions. Different values of SR and LLR demonstrate the amount of
attenuation and amplification distortions. The proposed method will compared
with any one method WAT(Weighted Average Technique) Hence by using parameters
SR (signal to residual spectrum ratio) and LLR (log like hood ratio), MMSE
(Minim Mean Square Error) in terms of segmented SNR and LLR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1899</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1899</id><created>2014-11-07</created><updated>2014-11-24</updated><authors><author><keyname>Pritychenko</keyname><forenames>B.</forenames></author></authors><title>Intriguing Trends in Nuclear Physics Articles Authorship</title><categories>cs.DL nucl-ex physics.soc-ph</categories><comments>6 pages, 3 figures</comments><report-no>Brookhaven National Laboratory Report BNL-107072-2014-IR</report-no><journal-ref>Scientometrics, Volume 105, Issue 3, 1781 (2015)</journal-ref><doi>10.1007/s11192-015-1605-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increase in authorship of nuclear physics publications has been
investigated using the large statistical samples. This has been accomplished
with nuclear data mining of nuclear science references (NSR) and experimental
nuclear reaction (EXFOR) databases. The results of this study will be discussed
and conclusions will be given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1906</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1906</id><created>2014-11-07</created><authors><author><keyname>Mousas</keyname><forenames>Christos</forenames></author><author><keyname>Newbury</keyname><forenames>Paul</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Christos-Nikolaos</forenames></author></authors><title>Footprint-Driven Locomotion Composition</title><categories>cs.GR</categories><journal-ref>International Journal of Computer Graphics &amp; Animation (IJCGA)
  Vol.4, No.4, pp. 27-42, October 2014</journal-ref><doi>10.5121/ijcga.2014.4403</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most efficient ways of generating goal-directed walking motions is
synthesising the final motion based on footprints. Nevertheless, current
implementations have not examined the generation of continuous motion based on
footprints, where different behaviours can be generated automatically.
Therefore, in this paper a flexible approach for footprint-driven locomotion
composition is presented. The presented solution is based on the ability to
generate footprint-driven locomotion, with flexible features like jumping,
running, and stair stepping. In addition, the presented system examines the
ability of generating the desired motion of the character based on predefined
footprint patterns that determine which behaviour should be performed. Finally,
it is examined the generation of transition patterns based on the velocity of
the root and the number of footsteps required to achieve the target behaviour
smoothly and naturally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1907</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1907</id><created>2014-11-07</created><authors><author><keyname>Khalili</keyname><forenames>Ali</forenames></author><author><keyname>Natale</keyname><forenames>Lorenzo</forenames></author><author><keyname>Tacchella</keyname><forenames>Armando</forenames></author></authors><title>Reverse Engineering of Middleware for Verification of Robot Control
  Architectures</title><categories>cs.RO cs.FL cs.SE</categories><comments>14 pages, 4 figures. The final version of the article is published in
  Proc. of &quot;Simulation, Modeling, and Programming for Autonomous Robots&quot;,
  SIMPAR 2014 (published by Springer)</comments><doi>10.1007/978-3-319-11900-7_27</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of automating the verification of distributed control
software relying on publish-subscribe middleware. In this scenario, the main
challenge is that software correctness depends intrinsically on correct usage
of middleware components, but structured models of such components might not be
available for analysis, e.g., because they are too large and complex to be
described precisely in a cost-effective way. To overcome this problem, we
propose to identify abstract models of middleware as finite-state automata, and
then to perform verification on the combined middleware and control software
models. Both steps are carried out in a computer-assisted way using
state-of-the-art techniques in automata-based identification and verification.
Our main contribution is to show that the combination of identification and
verification is feasible and useful when considering typical issues that arise
in the implementation of distributed control software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1919</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1919</id><created>2014-11-07</created><updated>2015-03-09</updated><authors><author><keyname>Duan</keyname><forenames>Ran</forenames></author><author><keyname>Pettie</keyname><forenames>Seth</forenames></author><author><keyname>Su</keyname><forenames>Hsin-Hao</forenames></author></authors><title>Scaling Algorithms for Weighted Matching in General Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new scaling algorithm for maximum (or minimum) weight perfect
matching on general, edge weighted graphs. Our algorithm runs in
$O(m\sqrt{n}\log(nN))$ time, which matches the running time of the best {\em
bipartite} weighted matching algorithm (Gabow and Tarjan, 1989) and is within a
$\log(nN)$ factor of the best cardinality matching algorithm (Micali and
Vazirani, 1980). Here $m,n,$ and $N$ bound the number of edges, vertices, and
magnitude of any edge weight. In terms of running time our algorithm is just
slightly faster than the previous best (Gabow and Tarjan, 1991), by a
$\sqrt{\alpha(m,n)\log n}$ factor. However, it is dramatically simpler to
describe and analyze.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1920</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1920</id><created>2014-11-07</created><authors><author><keyname>Marinho</keyname><forenames>M. L. M.</forenames></author><author><keyname>Sampaio</keyname><forenames>S. C. B.</forenames></author><author><keyname>Lima</keyname><forenames>T. L. A.</forenames></author><author><keyname>Moura</keyname><forenames>H. P.</forenames></author></authors><title>A Guide To Deal With Uncertainties In Software Project Management</title><categories>cs.SE</categories><comments>International Journal of Computer Science &amp; Information Technology
  Vol 6 No 5 October 2014 20 pages</comments><acm-class>K.6.1; K.6.m</acm-class><doi>10.5121/ijcsit.2014.6501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various project management approaches do not consider the impact that
uncertainties have on the project. The identified threats by uncertainty in a
projec day-to-day are real and immediate and the expectations in a project are
often high. The project manager faces a dilemma: decisions must be made in the
present about future situations which are inherently uncertain. The use of
uncertainty management in project can be a determining factor for the project
success. This paper presents a systematic review about uncertainties management
in software projects and a guide is proposed based on the review. It aims to
present the best practices to manage uncertainties in software projects in a
structured way including techniques and strategies to uncertainties
containment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1922</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1922</id><created>2014-11-07</created><authors><author><keyname>Luna</keyname><forenames>Alexandre J. H. de O.</forenames></author><author><keyname>Kruchten</keyname><forenames>Philippe</forenames></author><author><keyname>Pedrosa</keyname><forenames>Marcello L. G. do E.</forenames></author><author><keyname>Neto</keyname><forenames>Humberto R. de Almeida</forenames></author><author><keyname>de Moura</keyname><forenames>Hermano P.</forenames></author></authors><title>State of the Art of Agile Governance: A Systematic Review</title><categories>cs.SE</categories><comments>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 6, No 5, October 2014 20 pages, 8 Figures, 7 Tables</comments><acm-class>K.6; K.6.1; K.6.4; D.2; D.2.9</acm-class><doi>10.5121/ijcsit.2014.6510</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context: Agility at the business level requires Information Technology (IT)
environment flexible and customizable, as well as effective and responsive
governance in order to deliver value faster, better, and cheaper to the
business. Objective: To understand better this context, our paper seeks to
investigate how the domain of agile governance has evolved, as well as to
derive implications for research and practice. Method: We conducted a
systematic review about the state of art of the agile governance up to and
including 2013. Our search strategy identified 1992 studies in 10 databases, of
which 167 had the potential to answer our research questions. Results: We
organized the studies into four major groups: software engineering, enterprise,
manufacturing and multidisciplinary; classifying them into 16 emerging
categories. As a result, the review provides a convergent definition for agile
governance, six meta- principles, and a map of findings organized by topic and
classified by relevance and convergence. Conclusion: The found evidence lead us
to believe that agile governance is a relatively new, wide and
multidisciplinary area focused on organizational performance and
competitiveness that needs to be more intensively studied. Finally, we made
improvements and additions to the methodological approach for systematic
reviews and qualitative studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1924</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1924</id><created>2014-11-07</created><authors><author><keyname>Wilson-Nunn</keyname><forenames>Daniel</forenames></author><author><keyname>Zenil</keyname><forenames>Hector</forenames></author></authors><title>On the Complexity and Behaviour of Cryptocurrencies Compared to Other
  Markets</title><categories>q-fin.ST cs.IT math.IT</categories><comments>16 pages, 11 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the behaviour of Bitcoin has interesting similarities to stock
and precious metal markets, such as gold and silver. We report that whilst
Litecoin, the second largest cryptocurrency, closely follows Bitcoin's
behaviour, it does not show all the reported properties of Bitcoin. Agreements
between apparently disparate complexity measures have been found, and it is
shown that statistical, information-theoretic, algorithmic and fractal measures
have different but interesting capabilities of clustering families of markets
by type. The report is particularly interesting because of the range and novel
use of some measures of complexity to characterize price behaviour, because of
the IRS designation of Bitcoin as an investment property and not a currency,
and the announcement of the Canadian government's own electronic currency
MintChip.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1931</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1931</id><created>2014-11-07</created><authors><author><keyname>Ramane</keyname><forenames>Muralikrishnan</forenames></author><author><keyname>Krishnamoorthy</keyname><forenames>Sharmila</forenames></author><author><keyname>Gowtham</keyname><forenames>Sasikala</forenames></author></authors><title>An Experimental Evaluation of Performance of A Hadoop Cluster on Replica
  Management</title><categories>cs.DC</categories><comments>October 2014, Volume 4, Number 5, PAGE 88-97</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hadoop is an open source implementation of the MapReduce Framework in the
realm of distributed processing. A Hadoop cluster is a unique type of
computational cluster designed for storing and analyzing large data sets across
cluster of workstations. To handle massive scale data, Hadoop exploits the
Hadoop Distributed File System termed as HDFS. The HDFS similar to most
distributed file systems share a familiar problem on data sharing and
availability among compute nodes, often which leads to decrease in performance.
This paper is an experimental evaluation of Hadoop's computing performance
which is made by designing a rack aware cluster that utilizes the Hadoop's
default block placement policy to improve data availability. Additionally, an
adaptive data replication scheme that relies on access count prediction using
Langrange's interpolation is adapted to fit the scenario. To prove, experiments
were conducted on a rack aware cluster setup which significantly reduced the
task completion time, but once the volume of the data being processed increases
there is a considerable cutback in computational speeds due to update cost.
Further the threshold level for balance between the update cost and replication
factor is identified and presented graphically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1933</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1933</id><created>2014-11-07</created><authors><author><keyname>Ramane</keyname><forenames>Muralikrishnan</forenames></author><author><keyname>Vasudevan</keyname><forenames>Balaji</forenames></author><author><keyname>Allaphan</keyname><forenames>Sathappan</forenames></author></authors><title>A Provenance-Policy Based Access Control Model For Data Usage Validation
  In Cloud</title><categories>cs.CR</categories><comments>October 2014, Volume 3, Number 5, Page. 1-9</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an organization specifically as virtual as cloud there is need for access
control systems to constrain users direct or backhanded action that could lead
to breach of security. In cloud, apart from owner access to confidential data
the third party auditing and accounting is done which could stir up further
data leaks. To control such data leaks and integrity, in past several security
policies based on role, identity and user attributes were proposed and found
ineffective since they depend on static policies which do not monitor data
access and its origin. Provenance on the other hand tracks data usage and its
origin which proves the authenticity of data. To employ provenance in a real
time system like cloud, the service provider needs to store metadata on the
subject of data alteration which is universally called as the Provenance
Information. This paper presents a provenance-policy based access control model
which is designed and integrated with the system that not only makes data
auditable but also incorporates accountability for data alteration events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1945</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1945</id><created>2014-11-07</created><authors><author><keyname>Sampaio</keyname><forenames>S. C. B.</forenames></author><author><keyname>Marinho</keyname><forenames>M. L. M.</forenames></author><author><keyname>Moura</keyname><forenames>H. P.</forenames></author></authors><title>Systematic Review on Project Actuality</title><categories>cs.SE</categories><comments>13 pages in International Journal of Computer Science &amp; Information
  Technology Vol 6 No 5 October 2014</comments><acm-class>K.6.1; D.2</acm-class><doi>10.5121/ijcsit.2014.6504</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays much is written about how to manage projects, but too little on what
really happens in project actuality. Project Actuality came out in the
Rethinking Project Management (RPM) agenda in 2006 and it aims at understanding
what really happens at project context. To be able to understand project
actuality phenomenon, we first need to get a better comprehension on its
definition and discover how to observe it and analyse it. This paper presents
the results of the systematic review conducted to collect evidence on Project
Actuality. The research focused on four search engines, in publications from
1994 to 2013. Among others, the study concludes that project actuality has been
analysed by several methods and techniques, mostly on large organization and
public sectors, in Northern Europe. The most common definitions, techniques,
and tips were identified as well as the intent of transforming the results in
knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1951</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1951</id><created>2014-11-07</created><authors><author><keyname>P&#xf6;ter</keyname><forenames>Manuel</forenames></author></authors><title>Pheet meets C++11</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pheet is a C++ task-scheduling framework that allows for easy customization
of internal data-structures. The implementation was started before the C++11
standard was committed and therefore did not use the new standardized memory
model but compiler/platform specific intrinsics for atomic memory operations.
This not only makes the implementation harder to port to other compilers or
architectures but also suffers from the fact that prior C++ versions did not
specify any memory model.
  In this report I discuss the porting of one of the internal Pheet data
structures to the new memory model and provide reasoning about the correctness
based on the semantics of the memory consistency model. Using two benchmarks
from the Pheet benchmark suite I compare the performance of the original
against the new implementation which shows a significant speedup under certain
conditions on one of the two test machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1953</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1953</id><created>2014-11-07</created><authors><author><keyname>Gutierrez</keyname><forenames>Juan Manuel Parrilla</forenames></author><author><keyname>Hinkley</keyname><forenames>Trevor</forenames></author><author><keyname>Taylor</keyname><forenames>James</forenames></author><author><keyname>Yanev</keyname><forenames>Kliment</forenames></author><author><keyname>Cronin</keyname><forenames>Leroy</forenames></author></authors><title>Hardware and Software manual for Evolution of Oil Droplets in a
  Chemo-Robotic Platform</title><categories>cs.RO cs.AI</categories><comments>42 pages, 25 figures, list of printed parts</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This manual outlines a fully automated liquid handling robot to enable
physically-embodied evolution within a chemical oil-droplet system. The robot
is based upon the REPRAP3D printer system and makes the droplets by mixing
chemicals and then placing them in a petri dish after which they are recorded
using a camera and the behaviour of the droplets analysed using image
recognition software. This manual accompanies the open access publication
published in Nature Communications DOI: 10.1038/ncomms6571.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1955</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1955</id><created>2014-11-07</created><authors><author><keyname>Aman</keyname><forenames>Valeria</forenames></author></authors><title>Is there any measurable benefit in publishing preprints in the arXiv
  section Quantitative Biology?</title><categories>cs.DL physics.soc-ph q-bio.OT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A public preprint server such as arXiv allows authors to publish their
manuscripts before submitting them to journals for peer review. It offers the
chance to establish priority by making the results available upon completion.
This article presents the arXiv section Quantitative Biology and investigates
the advantages of preprint publications in terms of reception, which can be
measured by means of citations. This paper focuses on the publication and
citation delay, citation counts and the authors publishing their e-prints on
arXiv. Moreover, the paper discusses the benefit for scientists as well as
publishers. The results that are based on 12 selected journals show that
submitting preprints to arXiv has become more common in the past few years, but
the number of papers submitted to Quantitative Biology is still small and
represents only a fraction of the total research output in biology. An immense
advantage of arXiv is to overcome the long publication delay resulting from
peer review. Although preprints are visible prior to the officially published
articles, a significant citation advantage was only found for the Journal of
Theoretical Biology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1958</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1958</id><created>2014-11-07</created><updated>2015-03-20</updated><authors><author><keyname>Cao</keyname><forenames>Jiajun</forenames></author><author><keyname>Simonin</keyname><forenames>Matthieu</forenames></author><author><keyname>Cooperman</keyname><forenames>Gene</forenames></author><author><keyname>Morin</keyname><forenames>Christine</forenames></author></authors><title>Checkpointing as a Service in Heterogeneous Cloud Environments</title><categories>cs.DC</categories><comments>20 pages, 11 figures, appears in CCGrid, 2015</comments><acm-class>H.3.4; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A non-invasive, cloud-agnostic approach is demonstrated for extending
existing cloud platforms to include checkpoint-restart capability. Most cloud
platforms currently rely on each application to provide its own fault
tolerance. A uniform mechanism within the cloud itself serves two purposes: (a)
direct support for long-running jobs, which would otherwise require a custom
fault-tolerant mechanism for each application; and (b) the administrative
capability to manage an over-subscribed cloud by temporarily swapping out jobs
when higher priority jobs arrive. An advantage of this uniform approach is that
it also supports parallel and distributed computations, over both TCP and
InfiniBand, thus allowing traditional HPC applications to take advantage of an
existing cloud infrastructure. Additionally, an integrated health-monitoring
mechanism detects when long-running jobs either fail or incur exceptionally low
performance, perhaps due to resource starvation, and proactively suspends the
job. The cloud-agnostic feature is demonstrated by applying the implementation
to two very different cloud platforms: Snooze and OpenStack. The use of a
cloud-agnostic architecture also enables, for the first time, migration of
applications from one cloud platform to another.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1971</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1971</id><created>2014-10-29</created><updated>2014-11-25</updated><authors><author><keyname>Zhou</keyname><forenames>Xiangyang</forenames></author><author><keyname>Zhang</keyname><forenames>Jiaxin</forenames></author><author><keyname>Kulis</keyname><forenames>Brian</forenames></author></authors><title>Power-Law Graph Cuts</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms based on spectral graph cut objectives such as normalized cuts,
ratio cuts and ratio association have become popular in recent years because
they are widely applicable and simple to implement via standard eigenvector
computations. Despite strong performance for a number of clustering tasks,
spectral graph cut algorithms still suffer from several limitations: first,
they require the number of clusters to be known in advance, but this
information is often unknown a priori; second, they tend to produce clusters
with uniform sizes. In some cases, the true clusters exhibit a known size
distribution; in image segmentation, for instance, human-segmented images tend
to yield segment sizes that follow a power-law distribution. In this paper, we
propose a general framework of power-law graph cut algorithms that produce
clusters whose sizes are power-law distributed, and also does not fix the
number of clusters upfront. To achieve our goals, we treat the Pitman-Yor
exchangeable partition probability function (EPPF) as a regularizer to graph
cut objectives. Because the resulting objectives cannot be solved by relaxing
via eigenvectors, we derive a simple iterative algorithm to locally optimize
the objectives. Moreover, we show that our proposed algorithm can be viewed as
performing MAP inference on a particular Pitman-Yor mixture model. Our
experiments on various data sets show the effectiveness of our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1972</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1972</id><created>2014-11-06</created><authors><author><keyname>Pan</keyname><forenames>Victor Y.</forenames></author></authors><title>Better Late Than Never: Filling a Void in the History of Fast Matrix
  Multiplication and Tensor Decompositions</title><categories>cs.NA math.NA</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multilinear and tensor decompositions are a popular tool in linear and
multilinear algebra and have a wide range of important applications to modern
computing. Our paper of 1972 presented the first nontrivial application of such
decompositions to fundamental matrix computations and was also a landmark in
the history of the acceleration of matrix multiplication. Published in 1972 in
Russian, it has never been translated into English. It has been very rarely
cited in the Western literature on matrix multiplication and never in the works
on multilinear and tensor decompositions. This motivates us to present its
translation into English, together with our brief comments on its impact on the
two fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1973</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1973</id><created>2014-11-07</created><authors><author><keyname>Vujanic</keyname><forenames>Robin</forenames></author><author><keyname>Esfahani</keyname><forenames>Peyman Mohajerin</forenames></author><author><keyname>Goulart</keyname><forenames>Paul</forenames></author><author><keyname>Mariethoz</keyname><forenames>Sebastien</forenames></author><author><keyname>Morari</keyname><forenames>Manfred</forenames></author></authors><title>A Decomposition Method for Large Scale MILPs, with Performance
  Guarantees and a Power System Application</title><categories>math.OC cs.SY</categories><comments>23 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lagrangian duality in mixed integer optimization is a useful framework for
problems decomposition and for producing tight lower bounds to the optimal
objective, but in contrast to the convex counterpart, it is generally unable to
produce optimal solutions directly. In fact, solutions recovered from the dual
may be not only suboptimal, but even infeasible. In this paper we concentrate
on large scale mixed--integer programs with a specific structure that is of
practical interest, as it appears in a variety of application domains such as
power systems or supply chain management. We propose a solution method for
these structures, in which the primal problem is modified in a certain way,
guaranteeing that the solutions produced by the corresponding dual are feasible
for the original unmodified primal problem. The modification is simple to
implement and the method is amenable to distributed computations. We also
demonstrate that the quality of the solutions recovered using our procedure
improves as the problem size increases, making it particularly useful for large
scale instances for which commercial solvers are inadequate. We illustrate the
efficacy of our method with extensive experimentations on a problem stemming
from power systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1977</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1977</id><created>2014-11-07</created><authors><author><keyname>Schweitzer</keyname><forenames>Pascal</forenames></author></authors><title>Towards an Isomorphism Dichotomy for Hereditary Graph Classes</title><categories>cs.DM cs.CC cs.DS math.CO</categories><comments>37 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we resolve the complexity of the isomorphism problem on all but
finitely many of the graph classes characterized by two forbidden induced
subgraphs. To this end we develop new techniques applicable for the structural
and algorithmic analysis of graphs. First, we develop a methodology to show
isomorphism completeness of the isomorphism problem on graph classes by
providing a general framework unifying various reduction techniques. Second, we
generalize the concept of the modular decomposition to colored graphs, allowing
for non-standard decompositions. We show that, given a suitable decomposition
functor, the graph isomorphism problem reduces to checking isomorphism of
colored prime graphs. Third, we extend the techniques of bounded color valence
and hypergraph isomorphism on hypergraphs of bounded color size as follows. We
say a colored graph has generalized color valence at most k if, after removing
all vertices in color classes of size at most k, for each color class C every
vertex has at most k neighbors in C or at most k non-neighbors in C. We show
that isomorphism of graphs of bounded generalized color valence can be solved
in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1990</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1990</id><created>2014-11-07</created><updated>2015-03-03</updated><authors><author><keyname>Halabi</keyname><forenames>Marwa El</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author></authors><title>A totally unimodular view of structured sparsity</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a simple framework for structured sparse recovery based
on convex optimization. We show that many structured sparsity models can be
naturally represented by linear matrix inequalities on the support of the
unknown parameters, where the constraint matrix has a totally unimodular (TU)
structure. For such structured models, tight convex relaxations can be obtained
in polynomial time via linear programming. Our modeling framework unifies the
prevalent structured sparsity norms in the literature, introduces new
interesting ones, and renders their tightness and tractability arguments
transparent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1995</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1995</id><created>2014-11-07</created><updated>2015-02-19</updated><authors><author><keyname>Bova</keyname><forenames>Simone</forenames></author><author><keyname>Capelli</keyname><forenames>Florent</forenames></author><author><keyname>Mengel</keyname><forenames>Stefan</forenames></author><author><keyname>Slivovsky</keyname><forenames>Friedrich</forenames></author></authors><title>A Strongly Exponential Separation of DNNFs from CNF Formulas</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decomposable Negation Normal Forms (DNNFs) are Boolean circuits in negation
normal form where the subcircuits leading into each AND gate are defined on
disjoint sets of variables. We prove a strongly exponential lower bound on the
size of DNNFs for a class of CNF formulas built from expander graphs. As a
corollary, we obtain a strongly exponential separation between DNNFs and CNF
formulas in prime implicates form. This settles an open problem in the area of
knowledge compilation (Darwiche and Marquis, 2002).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1996</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1996</id><created>2014-11-07</created><updated>2014-12-04</updated><authors><author><keyname>Mryglod</keyname><forenames>Olesya</forenames></author><author><keyname>Kenna</keyname><forenames>Ralph</forenames></author><author><keyname>Holovatch</keyname><forenames>Yurij</forenames></author><author><keyname>Berche</keyname><forenames>Bertrand</forenames></author></authors><title>Predicting results of the Research Excellence Framework using
  departmental $h$-Index</title><categories>cs.DL physics.soc-ph</categories><comments>13 pages, 3 figures, accepted for publication in Scientometrics</comments><journal-ref>Sientometrics 102 (2015) 2165-2180</journal-ref><doi>10.1007/s11192-014-1512-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare estimates for past institutional research performances coming from
two bibliometric indicators to the results of the UK's Research Assessment
Exercise which last took place in 2008. We demonstrate that a version of the
departmental h-index is better correlated with the actual results of that
peer-review exercise than a competing metric known as the normalised
citation-based indicator. We then determine the corresponding h-indices for
2008-2013, the period examined in the UK's Research Excellence Framework (REF)
2014. We place herewith the resulting predictions on the arXiv in advance of
the REF results being published (December 2014). These may be considered as
unbiased predictions of relative performances in that exercise. We will revisit
this paper after the REF results are available and comment on the reliability
or otherwise of these bibliometrics as compared with peer review.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1998</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1998</id><created>2014-10-24</created><authors><author><keyname>Hossain</keyname><forenames>M. M. Aftab</forenames></author><author><keyname>J&#xe4;ntti</keyname><forenames>Riku</forenames></author><author><keyname>Cavdar</keyname><forenames>Cicek</forenames></author></authors><title>Dimensioning of PA for massive MIMO system with load adaptive number of
  antennas</title><categories>cs.NI cs.PF</categories><comments>7 pages, 8 figures, to be published in Globecom GBA workshop 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper takes into consideration the non-ideal efficiency characteristics
of realistic power amplifiers (PAs) along with the daily traffic profile in
order to investigate the impact of PA dimensioning on the energy efficiency
(EE) of load adaptive massive MIMO system. A multicellular system has been
considered where each base station (BS) is equipped with a large number of
antennas to serve many single antenna users. For a given number of users in a
cell, the optimum number of active antennas maximizing EE has been derived
where total BS downlink power is assumed to be fixed. Under the same
assumption, the PAs have been dimensioned in a way that maximizes network EE
not only for a single time snapshot but over twenty four hours of operation
while considering dynamic efficiency characteristics of the PAs. In order to
incorporate this daily load profile, each BS has been modeled as an M/G/m/m
state dependent queue under the assumption that the network is dimensioned to
serve a maximum number of users at a time corresponding to 100% cell traffic
load. This load adaptive system along with the optimized PA dimensioning
achieves 30% higher energy efficiency compared to a base line system where the
BSs always run with a fixed number of active antennas which are most energy
efficient while serving 100% traffic load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.1999</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.1999</id><created>2014-11-07</created><authors><author><keyname>Ishkewy</keyname><forenames>Hossam</forenames></author><author><keyname>Harb</keyname><forenames>Hany</forenames></author><author><keyname>Farahat</keyname><forenames>Hassan</forenames></author></authors><title>Azhary: An Arabic Lexical Ontology</title><categories>cs.AI cs.CL</categories><comments>appears in International Journal of Web &amp; Semantic Technology
  (IJWesT) Vol.5, No.4, October 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arabic language is the most spoken languages in the Semitic languages group,
and one of the most common languages in the world spoken by more than 422
million. It is also of paramount importance to Muslims, it is a sacred language
of the Islamic Holly Book (Quran) and prayer (and other acts of worship) in
Islam is performed only by mastering some of Arabic words. Arabic is also a
major ritual language of a number of Christian churches in the Arab world and
it is also used in writing several intellectual and religious Jewish books in
the Middle Ages. Despite this, there is no semantic Arabic lexicon which
researchers can depend on. In this paper we introduce Azhary as a lexical
ontology for the Arabic language. It groups Arabic words into sets of synonyms
called synsets, and records a number of relationships between words such as
synonym, antonym, hypernym, hyponym, meronym, holonym and association
relations. The ontology contains 26,195 words organized in 13,328 synsets. It
has been developed and contrasted against AWN which is the most common
available Arabic lexical ontology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2003</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2003</id><created>2014-11-07</created><updated>2015-03-05</updated><authors><author><keyname>Gao</keyname><forenames>Shuyang</forenames></author><author><keyname>Steeg</keyname><forenames>Greg Ver</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author></authors><title>Efficient Estimation of Mutual Information for Strongly Dependent
  Variables</title><categories>cs.IT math.IT physics.data-an stat.ML</categories><comments>13 pages, to appear in International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate that a popular class of nonparametric mutual information (MI)
estimators based on k-nearest-neighbor graphs requires number of samples that
scales exponentially with the true MI. Consequently, accurate estimation of MI
between two strongly dependent variables is possible only for prohibitively
large sample size. This important yet overlooked shortcoming of the existing
estimators is due to their implicit reliance on local uniformity of the
underlying joint distribution. We introduce a new estimator that is robust to
local non-uniformity, works well with limited data, and is able to capture
relationship strengths over many orders of magnitude. We demonstrate the
superior performance of the proposed estimator on both synthetic and real-world
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2014</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2014</id><created>2014-11-07</created><authors><author><keyname>Pinals</keyname><forenames>Lisa</forenames></author><author><keyname>Vu</keyname><forenames>Mai</forenames></author></authors><title>Link-State Based Decode-Forward Schemes for Two-way Relaying</title><categories>cs.IT math.IT</categories><comments>To be presented at Globecom 2014, Emerging Technologies for 5G
  Wireless Cellular Networks (Wi5G)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze a composite decode-and-forward scheme for the
two-way relay channel with a direct link. During transmission, our scheme
combines both block Markov coding and an independent coding scheme similar to
network coding at the relay. The main contribution of this work is to examine
how link state impacts the allocation of power between these two distinct
techniques, which in turn governs the necessity of each technique in achieving
the largest transmission rate region. We analytically determine the link-state
regimes and associated relaying techniques. Our results illustrate an
interesting trend: when the user-to-relay link is marginally stronger than the
direct link, it is optimal to use only independent coding. In this case, the
relay need not use full power. However, for larger user-to-relay link gains,
the relay must supplement independent coding with block Markov coding to
achieve the largest rate region. These link-state regimes are important for the
application of two-way relaying in 5G networks, such as in D2D mode or
relay-aided transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2021</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2021</id><created>2014-11-07</created><updated>2015-11-17</updated><authors><author><keyname>Peng</keyname><forenames>Richard</forenames></author><author><keyname>Sun</keyname><forenames>He</forenames></author><author><keyname>Zanetti</keyname><forenames>Luca</forenames></author></authors><title>Partitioning Well-Clustered Graphs: Spectral Clustering Works!</title><categories>cs.DS cs.LG</categories><comments>28 pages. A preliminary version of this paper appeared in the 28th
  Annual Conference on Learning Theory (COLT 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study variants of the widely used spectral clustering that
partitions a graph into k clusters by (1) embedding the vertices of a graph
into a low-dimensional space using the bottom eigenvectors of the Laplacian
matrix, and (2) grouping the embedded points into k clusters via k-means
algorithms. We show that, for a wide class of graphs, spectral clustering gives
a good approximation of the optimal clustering. While this approach was
proposed in the early 1990s and has comprehensive applications, prior to our
work similar results were known only for graphs generated from stochastic
models.
  We also give a nearly-linear time algorithm for partitioning well-clustered
graphs based on heat kernel embeddings and approximate nearest neighbor data
structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2022</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2022</id><created>2014-11-07</created><authors><author><keyname>Kosolobov</keyname><forenames>Dmitry</forenames></author></authors><title>Online Square Detection</title><categories>cs.DS</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The online square detection problem is to detect the first occurrence of a
square in a string whose characters are provided as input one at a time. Recall
that a square is a string that is a concatenation of two identical strings. In
this paper we present an algorithm solving this problem in $O(n\log\sigma)$
time and linear space on ordered alphabet, where $\sigma$ is the number of
different letters in the input string. Our solution is relatively simple and
does not require much memory unlike the previously known online algorithm with
the same working time. Also we present an algorithm working in $O(n\log n)$
time and linear space on unordered alphabet, though this solution does not
outperform the previously known result with the same time bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2031</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2031</id><created>2014-11-07</created><authors><author><keyname>Hanisch</keyname><forenames>Robert J.</forenames></author><author><keyname>Allen</keyname><forenames>Alice</forenames></author><author><keyname>Berriman</keyname><forenames>G. Bruce</forenames></author><author><keyname>DuPrie</keyname><forenames>Kimberly</forenames></author><author><keyname>Mink</keyname><forenames>Jessica</forenames></author><author><keyname>Nemiroff</keyname><forenames>Robert J.</forenames></author><author><keyname>Schmidt</keyname><forenames>Judy</forenames></author><author><keyname>Shamir</keyname><forenames>Lior</forenames></author><author><keyname>Shortridge</keyname><forenames>Keith</forenames></author><author><keyname>Taylor</keyname><forenames>Mark</forenames></author><author><keyname>Teuben</keyname><forenames>Peter J.</forenames></author><author><keyname>Wallin</keyname><forenames>John</forenames></author></authors><title>Astrophysics Source Code Library Enhancements</title><categories>astro-ph.IM cs.DL</categories><comments>4 pages; to be published in ADASS XXIV Proceedings. ASCL can be
  accessed at http://ascl.net/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Astrophysics Source Code Library (ASCL; ascl.net) is a free online
registry of codes used in astronomy research; it currently contains over 900
codes and is indexed by ADS. The ASCL has recently moved a new infrastructure
into production. The new site provides a true database for the code entries and
integrates the WordPress news and information pages and the discussion forum
into one site. Previous capabilities are retained and permalinks to ascl.net
continue to work. This improvement offers more functionality and flexibility
than the previous site, is easier to maintain, and offers new possibilities for
collaboration. This presentation covers these recent changes to the ASCL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2036</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2036</id><created>2014-11-07</created><updated>2015-04-26</updated><authors><author><keyname>Babaioff</keyname><forenames>Moshe</forenames></author><author><keyname>Leme</keyname><forenames>Renato Paes</forenames></author><author><keyname>Sivan</keyname><forenames>Balasubramanian</forenames></author></authors><title>Price Competition, Fluctuations, and Welfare Guarantees</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In various markets where sellers compete in price, price oscillations are
observed rather than convergence to equilibrium. Such fluctuations have been
empirically observed in the retail market for gasoline, in airline pricing and
in the online sale of consumer goods. Motivated by this, we study a model of
price competition in which an equilibrium rarely exists. We seek to analyze the
welfare, despite the nonexistence of an equilibrium, and present welfare
guarantees as a function of the market power of the sellers.
  We first study best response dynamics in markets with sellers that provide a
homogeneous good, and show that except for a modest number of initial rounds,
the welfare is guaranteed to be high. We consider two variations: in the first
the sellers have full information about the valuation of the buyer. Here we
show that if there are $n$ items available across all sellers and $n_{\max}$ is
the maximum number of items controlled by any given seller, the ratio of the
optimal welfare to the achieved welfare will be at most
$\log(\frac{n}{n-n_{\max}+1})+1$. As the market power of the largest seller
diminishes, the welfare becomes closer to optimal. In the second variation we
consider an extended model where sellers have uncertainty about the buyer's
valuation. Here we similarly show that the welfare improves as the market power
of the largest seller decreases, yet with a worse ratio of
$\frac{n}{n-n_{\max}+1}$. The exponential gap in welfare between the two
variations quantifies the value of accurately learning the buyer valuation.
  Finally, we show that extending our results to heterogeneous goods in general
is not possible. Even for the simple class of $k$-additive valuations, there
exists a setting where the welfare approximates the optimal welfare within any
non-zero factor only for $O(1/s)$ fraction of the time, where $s$ is the number
of sellers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2045</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2045</id><created>2014-11-07</created><authors><author><keyname>Moon</keyname><forenames>Kevin R.</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Multivariate f-Divergence Estimation With Confidence</title><categories>cs.IT math.IT stat.ML</categories><comments>20 pages, 1 figure. Accepted to NIPS 2014 (supplementary material is
  included in the appendices)</comments><journal-ref>K.R. Moon and A.O. Hero III, &quot;Multivariate f-Divergence Estimation
  With Confidence,&quot; In Advances in Neural Information Processing Systems, pp.
  2420-2428, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of f-divergence estimation is important in the fields of machine
learning, information theory, and statistics. While several nonparametric
divergence estimators exist, relatively few have known convergence properties.
In particular, even for those estimators whose MSE convergence rates are known,
the asymptotic distributions are unknown. We establish the asymptotic normality
of a recently proposed ensemble estimator of f-divergence between two
distributions from a finite number of samples. This estimator has MSE
convergence rate of O(1/T), is simple to implement, and performs well in high
dimensions. This theory enables us to perform divergence-based inference tasks
such as testing equality of pairs of distributions based on empirical samples.
We experimentally validate our theoretical results and, as an illustration, use
them to empirically bound the best achievable classification error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2047</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2047</id><created>2014-11-07</created><authors><author><keyname>Romansky</keyname><forenames>Stephen</forenames></author></authors><title>Investigation of the relationship between code change set n-grams and
  change in energy consumption</title><categories>cs.PF cs.SE</categories><comments>17 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The amount of software running on mobile devices is constantly growing as
consumers and industry purchase more battery powered devices. On the other
hand, tools that provide developers with feed- back on how their software
changes a?ect battery life are not widely available. This work employs Green
Mining, the study of the rela- tionship between energy consumption and software
changesets, and n-gram language models to evaluate if source code changeset
perplex- ity correlates with change in energy consumption. A correlation be-
tween perplexity and change in energy consumption would permit the development
of a tool that predicts the impact a code changeset may have on a software
applications energy consumption. The case study results show that there is weak
to no correlation between cross en- tropy and change in energy consumption.
Therefore, future areas of investigation are proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2057</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2057</id><created>2014-11-07</created><authors><author><keyname>Banerjee</keyname><forenames>Siddhartha</forenames></author><author><keyname>Sanghavi</keyname><forenames>Sujay</forenames></author><author><keyname>Shakkottai</keyname><forenames>Sanjay</forenames></author></authors><title>Online Collaborative-Filtering on Graphs</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common phenomena in modern recommendation systems is the use of feedback
from one user to infer the `value' of an item to other users. This results in
an exploration vs. exploitation trade-off, in which items of possibly low value
have to be presented to users in order to ascertain their value. Existing
approaches to solving this problem focus on the case where the number of items
are small, or admit some underlying structure -- it is unclear, however, if
good recommendation is possible when dealing with content-rich settings with
unstructured content.
  We consider this problem under a simple natural model, wherein the number of
items and the number of item-views are of the same order, and an `access-graph'
constrains which user is allowed to see which item. Our main insight is that
the presence of the access-graph in fact makes good recommendation possible --
however this requires the exploration policy to be designed to take advantage
of the access-graph. Our results demonstrate the importance of `serendipity' in
exploration, and how higher graph-expansion translates to a higher quality of
recommendations; it also suggests a reason why in some settings, simple
policies like Twitter's `Latest-First' policy achieve a good performance.
  From a technical perspective, our model presents a way to study
exploration-exploitation tradeoffs in settings where the number of `trials' and
`strategies' are large (potentially infinite), and more importantly, of the
same order. Our algorithms admit competitive-ratio guarantees which hold for
the worst-case user, under both finite-population and infinite-horizon
settings, and are parametrized in terms of properties of the underlying graph.
Conversely, we also demonstrate that improperly-designed policies can be highly
sub-optimal, and that in many settings, our results are order-wise optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2059</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2059</id><created>2014-11-07</created><authors><author><keyname>Mart&#xed;nez</keyname><forenames>Conrado</forenames></author><author><keyname>Nebel</keyname><forenames>Markus E.</forenames></author><author><keyname>Wild</keyname><forenames>Sebastian</forenames></author></authors><title>Analysis of Branch Misses in Quicksort</title><categories>cs.DS math.PR</categories><comments>to be presented at ANALCO 2015</comments><doi>10.1137/1.9781611973761.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of algorithms mostly relies on counting classic elementary
operations like additions, multiplications, comparisons, swaps etc. This
approach is often sufficient to quantify an algorithm's efficiency. In some
cases, however, features of modern processor architectures like pipelined
execution and memory hierarchies have significant impact on running time and
need to be taken into account to get a reliable picture. One such example is
Quicksort: It has been demonstrated experimentally that under certain
conditions on the hardware the classically optimal balanced choice of the pivot
as median of a sample gets harmful. The reason lies in mispredicted branches
whose rollback costs become dominating.
  In this paper, we give the first precise analytical investigation of the
influence of pipelining and the resulting branch mispredictions on the
efficiency of (classic) Quicksort and Yaroslavskiy's dual-pivot Quicksort as
implemented in Oracle's Java 7 library. For the latter it is still not fully
understood why experiments prove it 10% faster than a highly engineered
implementation of a classic single-pivot version. For different branch
prediction strategies, we give precise asymptotics for the expected number of
branch misses caused by the aforementioned Quicksort variants when their pivots
are chosen from a sample of the input. We conclude that the difference in
branch misses is too small to explain the superiority of the dual-pivot
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2066</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2066</id><created>2014-11-07</created><updated>2016-01-19</updated><authors><author><keyname>Szabo</keyname><forenames>Zoltan</forenames></author><author><keyname>Sriperumbudur</keyname><forenames>Bharath</forenames></author><author><keyname>Poczos</keyname><forenames>Barnabas</forenames></author><author><keyname>Gretton</keyname><forenames>Arthur</forenames></author></authors><title>Learning Theory for Distribution Regression</title><categories>math.ST cs.LG math.FA stat.ML stat.TH</categories><comments>The paper has been significantly shortened and simplified, minimax
  optimality has been proved. Code: https://bitbucket.org/szzoli/ite/. arXiv
  admin note: text overlap with arXiv:1402.1754</comments><msc-class>62G08, 46E22, 47B32</msc-class><acm-class>G.3; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on the distribution regression problem: regressing to vector-valued
outputs from probability measures. Many important machine learning and
statistical tasks fit into this framework, including multi-instance learning,
and point estimation problems without analytical solution (such as
hyperparameter or entropy estimation). Despite the large number of available
heuristics in the literature, the inherent two-stage sampled nature of the
problem makes the theoretical analysis quite challenging, since in practice
only samples from sampled distributions are observable, and the estimates have
to rely on similarities computed between sets of points. To the best of our
knowledge, the only existing technique with consistency guarantees for
distribution regression requires kernel density estimation as an intermediate
step (which often performs poorly in practice), and the domain of the
distributions to be compact Euclidean. In this paper, we study a simple,
analytically computable, ridge regression-based alternative to distribution
regression, where we embed the distributions to a reproducing kernel Hilbert
space, and learn the regressor from the embeddings to the outputs. Our main
contribution is to prove that this scheme is consistent in the two-stage
sampled setup under mild conditions (on separable topological domains enriched
with kernels): we present an exact computational-statistical efficiency
tradeoff analysis showing that the studied estimator is able to match the
one-stage sampled minimax optimal rate. This result answers a 16-year-old open
question, establishing the consistency of the classical set kernel [Haussler,
1999; Gaertner et. al, 2002] in regression. We also cover consistency for more
recent kernels on distributions, including those due to [Christmann and
Steinwart, 2010].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2069</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2069</id><created>2014-11-07</created><authors><author><keyname>Bianchi</keyname><forenames>S.</forenames></author><author><keyname>Escalante</keyname><forenames>M.</forenames></author><author><keyname>Nasini</keyname><forenames>G.</forenames></author><author><keyname>Tun&#xe7;el</keyname><forenames>L.</forenames></author></authors><title>Lov\'asz-Schrijver SDP-operator, near-perfect graphs and near-bipartite
  graphs</title><categories>cs.DM math.OC</categories><comments>20 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Lov\'asz-Schrijver lift-and-project operator ($LS_+$) based on
the cone of symmetric, positive semidefinite matrices, applied to the
fractional stable set polytope of graphs. The problem of obtaining a
combinatorial characterization of graphs for which the $LS_+$-operator
generates the stable set polytope in one step has been open since 1990. We call
these graphs $LS_+$-perfect. In the current contribution, we pursue a full
combinatorial characterization of $LS_+$-perfect graphs and make progress
towards such a characterization by establishing a new, close relationship among
$LS_+$-perfect graphs, near-bipartite graphs and a newly introduced concept of
full-support-perfect graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2075</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2075</id><created>2014-11-07</created><authors><author><keyname>Islam</keyname><forenames>Md. Baharul</forenames></author><author><keyname>Islam</keyname><forenames>Md. Kabirul</forenames></author><author><keyname>Ahmed</keyname><forenames>Arif</forenames></author><author><keyname>Shamsuddin</keyname><forenames>Abu Kalam</forenames></author></authors><title>Interactive Digital Learning Materials for Kindergarten Students in
  Bangladesh</title><categories>cs.CY cs.MM</categories><comments>2nd SMART Conference 2013, International Journal of Trends in
  Computer Science, Volume 2, Issue 11, 2013</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The pedagogy of teaching and learning has changed with the proliferation of
communication technology and it is necessary to develop interactive learning
materials for children that may improve their learning, catching, and
memorizing capabilities. Perhaps, one of the most important innovations in the
age of technology is multimedia and its application. It is imperative to create
high quality and realistic learning environment for children. Interactive
learning materials can be easier to understand and deal with their first
learning. We developed some interactive learning materials in the form of a
video for Playgroup using multimedia application tools. This study investigated
the impact of students' abilities to acquire new knowledge or skills through
interactive learning materials. We visited one kindergartens (Nursery schools),
interviewed class teachers about their teaching methods and level of students'
ability of recognizing English alphabets, pictures, etc. The course teachers
were provided interactive learning materials to show their playgroups for a
number of sessions. The video included English alphabets with related words and
pictures, and motivational fun. We noticed that almost all children were very
interested to interact with their leaning video. The students were assessed
individually and asked to recognize the alphabets, and pictures. The students
adapted with their first alphabets very quickly. However, there were individual
differences in their cognitive development. This interactive multimedia can be
an alternative to traditional pedagogy for teaching playgroups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2079</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2079</id><created>2014-11-07</created><authors><author><keyname>Chen</keyname><forenames>Ning</forenames></author><author><keyname>Gravin</keyname><forenames>Nick</forenames></author><author><keyname>Lu</keyname><forenames>Pinyan</forenames></author></authors><title>Competitive analysis via benchmark decomposition</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a uniform approach for the design and analysis of prior-free
competitive auctions and online auctions. Our philosophy is to view the
benchmark function as a variable parameter of the model and study a broad class
of functions instead of a individual target benchmark. We consider a multitude
of well-studied auction settings, and improve upon a few previous results.
  (1) Multi-unit auctions. Given a $\beta$-competitive unlimited supply
auction, the best previously known multi-unit auction is $2\beta$-competitive.
We design a $(1+\beta)$-competitive auction reducing the ratio from $4.84$ to
$3.24$. These results carry over to matroid and position auctions.
  (2) General downward-closed environments. We design a $6.5$-competitive
auction improving upon the ratio of $7.5$. Our auction is noticeably simpler
than the previous best one.
  (3) Unlimited supply online auctions. Our analysis yields an auction with a
competitive ratio of $4.12$, which significantly narrows the margin of
$[4,4.84]$ previously known for this problem.
  A particularly important tool in our analysis is a simple decomposition
lemma, which allows us to bound the competitive ratio against a sum of
benchmark functions. We use this lemma in a &quot;divide and conquer&quot; fashion by
dividing the target benchmark into the sum of simpler functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2081</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2081</id><created>2014-11-08</created><authors><author><keyname>Hussain</keyname><forenames>Sadiq</forenames></author><author><keyname>Hazarika</keyname><forenames>G. C.</forenames></author></authors><title>Educational data mining using jmp</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Educational Data Mining is a growing trend in case of higher education. The
quality of the Educational Institute may be enhanced through discovering hidden
knowledge from the student databases/ data warehouses. Present paper is
designed to carry out a comparative study with the TDC (Three Year Degree)
Course students of different colleges affiliated to Dibrugarh University. The
study is conducted with major subject wise, gender wise and category/caste
wise. The experimental results may be visualized with Scatterplot3D, Bubble
Plot, Fit Y by X, Run Chart, Control Chart etc. of the SAS JMP Software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2084</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2084</id><created>2014-11-08</created><authors><author><keyname>Madan</keyname><forenames>Mamta</forenames></author><author><keyname>Mathur</keyname><forenames>Mohit</forenames></author></authors><title>Cloud Network Management Model A Novel Approach to Manage Cloud Traffic</title><categories>cs.NI</categories><comments>International Journal on Cloud Computing: Services and Architecture
  (IJCCSA),10/2014</comments><doi>10.5121/ijccsa.2014.4502</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud is in the air. More and More companies and personals are connecting to
cloud with so many variety of offering provided by the companies. The cloud
services are based on Internet i.e. TCP IP. The paper discusses limitations of
one of the main existing network management protocol i.e. Simple Network
Management Protocol (SNMP) with respect to the current network conditions. The
network traffic is growing at a high speed. When we talk about the networked
environment of cloud, the monitoring tool should be capable of handling the
traffic tribulations efficiently and represent a correct scenario of the
network condition. The proposed Model Cloud Network Management Model provides a
comprehensive solution to manage the growing traffic in cloud and trying to
improve communication of manager and agents as in SNMP (the traditional TCP IP
network management protocol). Firstly CNMM concentrates on reduction of packet
exchange between manager and agent. Secondly it eliminates the counter problems
exist in SNMP by having periodic updates from agent without querying by the
manager. For better management we are including managers using virtualized
technology. CNMM is a proposed model with efficient communication, secure
packet delivery and reduced traffic. Though the proposed model supposed to
manage the cloud traffic in a better and efficient way, the model is still a
theoretical study, its implementation and results are yet to discover. The
model however is the first step towards development of supported algorithms and
protocol. Our further study will concentrate on development of supported
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2088</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2088</id><created>2014-11-08</created><authors><author><keyname>Ghorbani</keyname><forenames>Ali</forenames></author><author><keyname>Ghorbani</keyname><forenames>Ghazaleh</forenames></author></authors><title>Energy Efficient Full Adder Cell Design With Using Carbon Nanotube Field
  Effect Transistors In 32 Nanometer Technology</title><categories>cs.AR</categories><comments>8 pages, 6 figures, International Journal of VLSI design &amp;
  Communication Systems (VLSICS) Vol.5, No.5, October 2014</comments><doi>10.5121/vlsic.2014.5501</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Full Adder is one of the critical parts of logical and arithmetic units. So,
presenting a low power full adder cell reduces the power consumption of the
entire circuit. Also, using Nano-scale transistors, because of their unique
characteristics will save energy consumption and decrease the chip area. In
this paper we presented a low power full adder cell by using carbon nanotube
field effect transistors (CNTFETs). Simulation results were carried out using
HSPICE based on the CNTFET model in 32 nanometer technology in Different values
of temperature and VDD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2090</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2090</id><created>2014-11-08</created><authors><author><keyname>S.</keyname><forenames>Nagaraja</forenames></author><author><keyname>J.</keyname><forenames>Prabhakar C.</forenames></author><author><keyname>U</keyname><forenames>Praveen Kumar P.</forenames></author></authors><title>Parallax Effect Free Mosaicing of Underwater Video Sequence Based on
  Texture Features</title><categories>cs.CV</categories><comments>13 pages, 7 figures, Signal &amp; Image Processing : An International
  Journal (SIPIJ), Vol.5, No.5, October 2014</comments><doi>10.5121/sipij.2014.5502</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present feature-based technique for construction of mosaic
image from underwater video sequence, which suffers from parallax distortion
due to propagation properties of light in the underwater environment. The most
of the available mosaic tools and underwater image mosaicing techniques yields
final result with some artifacts such as blurring, ghosting and seam due to
presence of parallax in the input images. The removal of parallax from input
images may not reduce its effects instead it must be corrected in successive
steps of mosaicing. Thus, our approach minimizes the parallax effects by
adopting an efficient local alignment technique after global registration. We
extract texture features using Centre Symmetric Local Binary Pattern (CS-LBP)
descriptor in order to find feature correspondences, which are used further for
estimation of homography through RANSAC. In order to increase the accuracy of
global registration, we perform preprocessing such as colour alignment between
two selected frames based on colour distribution adjustment. Because of
existence of 100% overlap in consecutive frames of underwater video, we select
frames with minimum overlap based on mutual offset in order to reduce the
computation cost during mosaicing. Our approach minimizes the parallax effects
considerably in final mosaic constructed using our own underwater video
sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2093</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2093</id><created>2014-11-08</created><authors><author><keyname>Kohli</keyname><forenames>Shruti</forenames></author><author><keyname>Gupta</keyname><forenames>Ankit</forenames></author></authors><title>Critical Regression Analysis of Real Time Industrial Web Data Set Using
  Data Mining Tool</title><categories>cs.CY</categories><comments>International Journal of Computer Applications Conference
  Proceedings,2013 4th International IT Summit Confluence 2013 - The Next
  Generation Information Technology Summit</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In todays fast pacing, highly competing,volatile and challenging world,
companies highly rely on data analysis obtained from both offline as well as
online way to make their future strategy, to sustain in the market. This paper
reviews the regression technique analysis on a real time web data to analyse
different attributes of interest and to predict possible growth factors for the
company, so as to enable the company to make possible strategic decisions for
the growth of the company.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2102</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2102</id><created>2014-11-08</created><updated>2014-12-19</updated><authors><author><keyname>Elayoubi</keyname><forenames>Salah Eddine</forenames></author><author><keyname>Fricker</keyname><forenames>Christine</forenames></author><author><keyname>Guillemin</keyname><forenames>Fabrice</forenames></author><author><keyname>Robert</keyname><forenames>Philippe</forenames></author><author><keyname>Sericola</keyname><forenames>Bruno</forenames></author></authors><title>Impatience in mobile networks and its application to data pricing</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider in this paper an import Quality of Experience (QoE) indicator in
mobile networks that is reneging of users due to impatience. We specifically
consider a cell under heavy load conditions and compute the reneging
probability by using a fluid limit analysis. By solving the fixed point
equation, we obtain a new QoE perturbation metric quantifying the impact of
reneging on the performance of the system. This metric is then used to devise a
new pricing scheme accounting of reneging. We specifically propose several
flavors of this pricing around the idea of having a flat rate for accessing the
network and an elastic price related to the level of QoE perturbation induced
by communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2105</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2105</id><created>2014-11-08</created><authors><author><keyname>Berkemer</keyname><forenames>Sarah</forenames></author><author><keyname>Chaves</keyname><forenames>Ricardo</forenames></author><author><keyname>Fritz</keyname><forenames>Adrian</forenames></author><author><keyname>Hellmuth</keyname><forenames>Marc</forenames></author><author><keyname>Hernandez-Rosales</keyname><forenames>Maribel</forenames></author><author><keyname>Stadler</keyname><forenames>Peter F.</forenames></author></authors><title>Spiders can be recognized by counting their legs</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spiders are arthropods that can be distinguished from their closest
relatives, the insects, by counting their legs. Spiders have 8, insects just 6.
Spider graphs are a very restricted class of graphs that naturally appear in
the context of cograph editing. The vertex set of a spider (or its complement)
is naturally partitioned into a clique (the body), an independent set (the
legs), and a rest (serving as the head). Here we show that spiders can be
recognized directly from their degree sequences through the number of their
legs (vertices with degree 1). Furthermore, we completely characterize the
degree sequences of spiders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2111</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2111</id><created>2014-11-08</created><authors><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author><author><keyname>Lebedev</keyname><forenames>Vladimir</forenames></author><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author></authors><title>Cascading of Fluctuations in Interdependent Energy Infrastructures:
  Gas-Grid Coupling</title><categories>physics.soc-ph cs.SY</categories><comments>13 pages, 8 figures</comments><report-no>LANL LA-UR-14-26971</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The revolution of hydraulic fracturing has dramatically increased the supply
and lowered the cost of natural gas in the United States driving an expansion
of natural gas-fired generation capacity in many electrical grids. Unrelated to
the natural gas expansion, lower capital costs and renewable portfolio
standards are driving an expansion of intermittent renewable generation
capacity such as wind and photovoltaic generation. These two changes may
potentially combine to create new threats to the reliability of these
interdependent energy infrastructures. Natural gas-fired generators are often
used to balance the fluctuating output of wind generation. However, the
time-varying output of these generators results in time-varying natural gas
burn rates that impact the pressure in interstate transmission pipelines.
Fluctuating pressure impacts the reliability of natural gas deliveries to those
same generators and the safety of pipeline operations. We adopt a partial
differential equation model of natural gas pipelines and use this model to
explore the effect of intermittent wind generation on the fluctuations of
pressure in natural gas pipelines. The mean square pressure fluctuations are
found to grow linearly in time with points of maximum deviation occurring at
the locations of flow reversals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2132</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2132</id><created>2014-11-08</created><authors><author><keyname>Grispos</keyname><forenames>George</forenames></author><author><keyname>Glisson</keyname><forenames>William Bradley</forenames></author><author><keyname>Pardue</keyname><forenames>J. Harold</forenames></author><author><keyname>Dickson</keyname><forenames>Mike</forenames></author></authors><title>Identifying User Behavior from Residual Data in Cloud-based Synchronized
  Apps</title><categories>cs.CR cs.CY</categories><comments>Please cite this paper as: G. Grispos, W.B. Glisson, J.H. Pardue and
  M. Dickson (2014). Identifying User Behavior from Residual Data in
  Cloud-based Synchronized Apps. Conference on Information Systems Applied
  Research (CONISAR 2014), 6-9 November 2014, Baltimore Maryland, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the distinction between personal and organizational device usage continues
to blur, the combination of applications that interact increases the need to
investigate potential security issues. Although security and forensic
researchers have been able to recover a variety of artifacts, empirical
research has not examined a suite of application artifacts from the perspective
of high-level pattern identification. This research presents a preliminary
investigation into the idea that residual artifacts generated by cloud-based
synchronized applications can be used to identify broad user behavior patterns.
To accomplish this, the researchers conducted a single-case, pretest-posttest,
quasi experiment using a smartphone device and a suite of Google mobile
applications. The contribution of this paper is two-fold. First, it provides a
proof of concept of the extent to which residual data from cloud-based
synchronized applications can be used to broadly identify user behavior
patterns from device data patterns. Second, it highlights the need for security
controls to prevent and manage information flow between BYOD mobile devices and
cloud synchronization services.
  Keywords: Residual Data, Cloud, Apps, Digital Forensics, BYOD
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2139</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2139</id><created>2014-11-08</created><authors><author><keyname>Xiao</keyname><forenames>Yuanzhang</forenames></author><author><keyname>D&#xf6;rfler</keyname><forenames>Florian</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Incentive Design in Peer Review: Rating and Repeated Endogenous Matching</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer review (e.g., grading assignments in Massive Open Online Courses
(MOOCs), academic paper review) is an effective and scalable method to evaluate
the products (e.g., assignments, papers) of a large number of agents when the
number of dedicated reviewing experts (e.g., teaching assistants, editors) is
limited. Peer review poses two key challenges: 1) identifying the reviewers'
intrinsic capabilities (i.e., adverse selection) and 2) incentivizing the
reviewers to exert high effort (i.e., moral hazard). Some works in mechanism
design address pure adverse selection using one-shot matching rules, and pure
moral hazard was addressed in repeated games with exogenously given and fixed
matching rules. However, in peer review systems exhibiting both adverse
selection and moral hazard, one-shot or exogenous matching rules do not link
agents' current behavior with future matches and future payoffs, and as we
prove, will induce myopic behavior (i.e., exerting the lowest effort) resulting
in the lowest review quality.
  In this paper, we propose for the first time a solution that simultaneously
solves adverse selection and moral hazard. Our solution exploits the repeated
interactions of agents, utilizes ratings to summarize agents' past review
quality, and designs matching rules that endogenously depend on agents'
ratings. Our proposed matching rules are easy to implement and require no
knowledge about agents' private information (e.g., their benefit and cost
functions). Yet, they are effective in guiding the system to an equilibrium
where the agents are incentivized to exert high effort and receive ratings that
precisely reflect their review quality. Using several illustrative examples, we
quantify the significant performance gains obtained by our proposed mechanism
as compared to existing one-shot or exogenous matching rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2140</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2140</id><created>2014-11-08</created><authors><author><keyname>Kim</keyname><forenames>Haider Al</forenames></author><author><keyname>Barua</keyname><forenames>Shouman</forenames></author><author><keyname>Ghosal</keyname><forenames>Pantha</forenames></author><author><keyname>Sandrasegaran</keyname><forenames>Kumbesan</forenames></author></authors><title>Macro with Pico Cells (HetNets) System Behavior Using Well-known
  scheduling Algorithms</title><categories>cs.NI</categories><comments>14 pages, 11 Figures, International Journal of Wireless &amp; Mobile
  Networks (IJWMN) Vol. 6, No. 5, October 2014</comments><doi>10.5121/ijwmn.2014.6509</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper demonstrates the concept of using Heterogeneous networks (HetNets)
to improve Long Term Evolution (LTE) system by introducing the LTE Advance
(LTE-A). The type of HetNets that has been chosen for this study is Macro with
Pico cells. Comparing the system performance with and without Pico cells has
clearly illustrated using three well-known scheduling algorithms (Proportional
Fair PF, Maximum Largest Weighted Delay First MLWDF and
Exponential/Proportional Fair EXP/PF). The system is judged based on
throughput, Packet Loss Ratio PLR, delay and fairness.A simulation platform
called LTE-Sim has been used to collect the data and produce the paper outcomes
and graphs. The results prove that adding Pico cells enhances the overall
system performance. From the simulation outcomes, the overall system
performance is as follows: throughput is duplicated or tripled based on the
number of users, the PLR is almost quartered, the delay is nearly reduced ten
times (PF case) and changed to be a half (MLWDF/EXP cases), and the fairness
stays closer to value of 1. It is considered an efficient and cost effective
way to increase the throughput, coverage and reduce the latency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2141</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2141</id><created>2014-11-08</created><authors><author><keyname>Baghaie</keyname><forenames>Ahmadreza</forenames></author><author><keyname>Yu</keyname><forenames>Zeyun</forenames></author><author><keyname>D'souza</keyname><forenames>Roshan M.</forenames></author></authors><title>Fast Mesh-Based Medical Image Registration</title><categories>cs.CV</categories><comments>Accepted manuscript for ISVC'2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a fast triangular mesh based registration method is proposed.
Having Template and Reference images as inputs, the template image is
triangulated using a content adaptive mesh generation algorithm. Considering
the pixel values at mesh nodes, interpolated using spline interpolation method
for both of the images, the energy functional needed for image registration is
minimized. The minimization process was achieved using a mesh based
discretization of the distance measure and regularization term which resulted
in a sparse system of linear equations, which due to the smaller size in
comparison to the pixel-wise registration method, can be solved directly. Mean
Squared Di?erence (MSD) is used as a metric for evaluating the results. Using
the mesh based technique, higher speed was achieved compared to pixel-based
curvature registration technique with fast DCT solver. The implementation was
done in MATLAB without any speci?c optimization. Higher speeds can be achieved
using C/C++ implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2153</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2153</id><created>2014-11-08</created><authors><author><keyname>Cirillo</keyname><forenames>Simone</forenames></author><author><keyname>Lloyd</keyname><forenames>Stefan</forenames></author><author><keyname>Nordin</keyname><forenames>Peter</forenames></author></authors><title>Evolving intraday foreign exchange trading strategies utilizing multiple
  instruments price series</title><categories>cs.NE q-fin.TR</categories><comments>15 pages, 10 figures, 9 tables</comments><acm-class>I.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Genetic Programming architecture for the generation of foreign
exchange trading strategies. The system's principal features are the evolution
of free-form strategies which do not rely on any prior models and the
utilization of price series from multiple instruments as input data. This
latter feature constitutes an innovation with respect to previous works
documented in literature. In this article we utilize Open, High, Low, Close bar
data at a 5 minutes frequency for the AUD.USD, EUR.USD, GBP.USD and USD.JPY
currency pairs. We will test the implementation analyzing the in-sample and
out-of-sample performance of strategies for trading the USD.JPY obtained across
multiple algorithm runs. We will also evaluate the differences between
strategies selected according to two different criteria: one relies on the
fitness obtained on the training set only, the second one makes use of an
additional validation dataset. Strategy activity and trade accuracy are
remarkably stable between in and out of sample results. From a profitability
aspect, the two criteria both result in strategies successful on out-of-sample
data but exhibiting different characteristics. The overall best performing
out-of-sample strategy achieves a yearly return of 19%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2156</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2156</id><created>2014-11-08</created><authors><author><keyname>Mohssen</keyname><forenames>Nesma</forenames></author><author><keyname>Momtaz</keyname><forenames>Rana</forenames></author><author><keyname>Aly</keyname><forenames>Heba</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author></authors><title>It's the Human that Matters: Accurate User Orientation Estimation for
  Mobile Computing Applications</title><categories>cs.CY cs.HC</categories><comments>Accepted for publication in the 11th International Conference on
  Mobile and Ubiquitous Systems: Computing, Networking and Services
  (Mobiquitous 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ubiquity of Internet-connected and sensor-equipped portable devices sparked a
new set of mobile computing applications that leverage the proliferating
sensing capabilities of smart-phones. For many of these applications, accurate
estimation of the user heading, as compared to the phone heading, is of
paramount importance. This is of special importance for many crowd-sensing
applications, where the phone can be carried in arbitrary positions and
orientations relative to the user body. Current state-of-the-art focus mainly
on estimating the phone orientation, require the phone to be placed in a
particular position, require user intervention, and/or do not work accurately
indoors; which limits their ubiquitous usability in different applications. In
this paper we present Humaine, a novel system to reliably and accurately
estimate the user orientation relative to the Earth coordinate system.
  Humaine requires no prior-configuration nor user intervention and works
accurately indoors and outdoors for arbitrary cell phone positions and
orientations relative to the user body. The system applies statistical analysis
techniques to the inertial sensors widely available on today's cell phones to
estimate both the phone and user orientation. Implementation of the system on
different Android devices with 170 experiments performed at different indoor
and outdoor testbeds shows that Humaine significantly outperforms the
state-of-the-art in diverse scenarios, achieving a median accuracy of
$15^\circ$ averaged over a wide variety of phone positions. This is $558\%$
better than the-state-of-the-art. The accuracy is bounded by the error in the
inertial sensors readings and can be enhanced with more accurate sensors and
sensor fusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2158</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2158</id><created>2014-11-08</created><updated>2016-03-02</updated><authors><author><keyname>Binkiewicz</keyname><forenames>Norbert</forenames></author><author><keyname>Vogelstein</keyname><forenames>Joshua T.</forenames></author><author><keyname>Rohe</keyname><forenames>Karl</forenames></author></authors><title>Covariate-assisted spectral clustering</title><categories>stat.ML cs.LG math.ST stat.ME stat.TH</categories><comments>30 pages, updated acknowledgments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biological and social systems consist of myriad interacting units. The
interactions can be represented in the form of a graph or network. Measurements
of these graphs can reveal the underlying structure of these interactions,
which provides insight into the systems that generated the graphs. Moreover, in
applications such as connectomics, social networks, and genomics, graph data
are accompanied by contextualizing measures on each node. We utilize these node
covariates to help uncover latent communities in a graph, using a modification
of spectral clustering. Statistical guarantees are provided under a joint
mixture model that we call the node-contextualized stochastic blockmodel,
including a bound on the mis-clustering rate. For most simulated conditions,
covariate-assisted spectral clustering yields results superior to regularized
spectral clustering without node covariates and to an adaptation of canonical
correlation analysis. We apply our clustering method to large brain graphs
derived from diffusion MRI data, using the node locations or neurological
region membership as covariates. In both cases, covariate-assisted spectral
clustering yields clusters that are easier to interpret neurologically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2160</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2160</id><created>2014-11-08</created><authors><author><keyname>Aguilera</keyname><forenames>Marcos K.</forenames></author><author><keyname>Leners</keyname><forenames>Joshua B.</forenames></author><author><keyname>Kotla</keyname><forenames>Ramakrishna</forenames></author><author><keyname>Walfish</keyname><forenames>Michael</forenames></author></authors><title>Yesquel: scalable SQL storage for Web applications</title><categories>cs.DC</categories><acm-class>C.2.4; H.2.4; H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on a brief history of the storage systems for Web applications, we
motivate the need for a new storage system. We then describe the architecture
of such a system, called Yesquel. Yesquel supports the SQL query language and
offers performance similar to NOSQL storage systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2169</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2169</id><created>2014-11-08</created><authors><author><keyname>Brevik</keyname><forenames>John O.</forenames></author><author><keyname>O'Sullivan</keyname><forenames>Michael E.</forenames></author></authors><title>The Sum-Product Algorithm for Degree-2 Check Nodes and Trapping Sets</title><categories>cs.IT math.IT</categories><comments>Unpublished, 26 pages</comments><msc-class>94B05, 94B35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sum-product algorithm for decoding of binary codes is analyzed for
bipartite graphs in which the check nodes all have degree $2$. The algorithm
simplifies dramatically and may be expressed using linear algebra. Exact
results about the convergence of the algorithm are derived and applied to
trapping sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2173</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2173</id><created>2014-11-08</created><authors><author><keyname>Martinez</keyname><forenames>Julieta</forenames></author><author><keyname>Hoos</keyname><forenames>Holger H.</forenames></author><author><keyname>Little</keyname><forenames>James J.</forenames></author></authors><title>Stacked Quantizers for Compositional Vector Compression</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Babenko and Lempitsky introduced Additive Quantization (AQ), a
generalization of Product Quantization (PQ) where a non-independent set of
codebooks is used to compress vectors into small binary codes. Unfortunately,
under this scheme encoding cannot be done independently in each codebook, and
optimal encoding is an NP-hard problem. In this paper, we observe that PQ and
AQ are both compositional quantizers that lie on the extremes of the codebook
dependence-independence assumption, and explore an intermediate approach that
exploits a hierarchical structure in the codebooks. This results in a method
that achieves quantization error on par with or lower than AQ, while being
several orders of magnitude faster. We perform a complexity analysis of PQ, AQ
and our method, and evaluate our approach on standard benchmarks of SIFT and
GIST descriptors, as well as on new datasets of features obtained from
state-of-the-art convolutional neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2180</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2180</id><created>2014-11-08</created><updated>2014-12-09</updated><authors><author><keyname>Riddell</keyname><forenames>Allen B.</forenames></author></authors><title>Public Domain Rank: Identifying Notable Individuals with the Wisdom of
  the Crowd</title><categories>cs.DL</categories><doi>10.1145/2788993.2789850</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Identifying literary, scientific, and technical works of enduring interest is
challenging. Few are able to name significant works across more than a handful
of domains or languages. This paper introduces an automatic method for
identifying authors of notable works throughout history. Notability is defined
using the record of which works volunteers have made available in public domain
digital editions. A significant benefit of this bottom-up approach is that it
also provides a novel and reproducible index of notability for all individuals
with Wikipedia pages. The method promises to supplement the work of cultural
organizations and institutions seeking to publicize the availability of notable
works and prioritize works for preservation and digitization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2186</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2186</id><created>2014-11-08</created><authors><author><keyname>Gao</keyname><forenames>Lianli</forenames></author><author><keyname>Bruenig</keyname><forenames>Michael</forenames></author><author><keyname>Hunter</keyname><forenames>Jane</forenames></author></authors><title>Estimating Fire Weather Indices via Semantic Reasoning over Wireless
  Sensor Network Data Streams</title><categories>cs.CY cs.AI</categories><comments>20pages, 12 figures</comments><msc-class>68Uxx</msc-class><journal-ref>Estimating Fire Weather Indices via Semantic Reasoning of Wireless
  Sensor Network Streams, International Journal of Web and Semantic Technology,
  Vol.5, pp.1-20 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wildfires are frequent, devastating events in Australia that regularly cause
significant loss of life and widespread property damage. Fire weather indices
are a widely-adopted method for measuring fire danger and they play a
significant role in issuing bushfire warnings and in anticipating demand for
bushfire management resources. Existing systems that calculate fire weather
indices are limited due to low spatial and temporal resolution. Localized
wireless sensor networks, on the other hand, gather continuous sensor data
measuring variables such as air temperature, relative humidity, rainfall and
wind speed at high resolutions. However, using wireless sensor networks to
estimate fire weather indices is a challenge due to data quality issues, lack
of standard data formats and lack of agreement on thresholds and methods for
calculating fire weather indices. Within the scope of this paper, we propose a
standardized approach to calculating Fire Weather Indices (a.k.a. fire danger
ratings) and overcome a number of the challenges by applying Semantic Web
Technologies to the processing of data streams from a wireless sensor network
deployed in the Springbrook region of South East Queensland. This paper
describes the underlying ontologies, the semantic reasoning and the Semantic
Fire Weather Index (SFWI) system that we have developed to enable domain
experts to specify and adapt rules for calculating Fire Weather Indices. We
also describe the Web-based mapping interface that we have developed, that
enables users to improve their understanding of how fire weather indices vary
over time within a particular region.Finally, we discuss our evaluation results
that indicate that the proposed system outperforms state-of-the-art techniques
in terms of accuracy, precision and query performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2188</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2188</id><created>2014-11-08</created><authors><author><keyname>Gao</keyname><forenames>Lianli</forenames></author><author><keyname>Bruenig</keyname><forenames>Michael</forenames></author><author><keyname>Hunter</keyname><forenames>Jane</forenames></author></authors><title>Semantic-based Detection of Segment Outliers and Unusual Events for
  Wireless Sensor Networks</title><categories>cs.OH</categories><comments>Dynamic Time Warping, Data Quality, Wireless Sensor Networks, Outlier
  Detection, Unusual Event Detection, Semantic Web</comments><msc-class>68Uxx</msc-class><journal-ref>Semantic based Detection of Segment Outliers and Unusual Events
  for Wireless Sensor Networks, International Conference on Information
  Quality, Little Rock, United States, pp.127-144 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Environmental scientists have increasingly been deploying wireless sensor
networks to capture valuable data that measures and records precise information
about our environment. One of the major challenges associated with wireless
sensor networks is the quality of the data and more specifically the detection
of segment outliers and unusual events. Most previous research has focused on
detecting outliers that are errors that are caused by unreliable sensors and
sensor nodes. However, there is an urgent need for the development of new tools
capable of identifying, tagging and visualizing erroneous segment outliers and
unusual events from sensor data streams. In this paper, we present a SOUE
Detector (Segment Outlier and Unusual Event-Detector) system for wireless
sensor networks that combines statistical analyses using Dynamic Time Warping
(DTW) with domain expert knowledge (captured via an ontology and semantic
inferencing rules). The resulting Web portal enables scientist to efficiently
search across a collection of wireless sensor data streams and identify,
retrieve and display segment outliers (both erroneous and genuine) within the
data streams. In this paper, we firstly describe the detection algorithms, the
implementation details and the functionality of the SOUE Detector system.
Secondly we evaluate our approach using data that comprises sensor observations
collected from a sensor network deployed in the Springbrook National Park in
Queensland, Australia. The experimental results show that the SOUE-Detector can
efficiently detect segment outliers and unusual events with high levels of
precision and recall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2190</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2190</id><created>2014-11-08</created><authors><author><keyname>Kanaya</keyname><forenames>Ichiroh</forenames></author><author><keyname>Imura</keyname><forenames>Masataka</forenames></author><author><keyname>Kanazawa</keyname><forenames>Mayuko</forenames></author></authors><title>Interactive Art To Go</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional artworks like paintings, photographs, or films can be reproduced
by conventional media like printing or video. This makes visitors of museums
possible to purchase postcards, posters, books, and DVDs of pictures and/or
movies shown at the exhibition. However, newly developing arts so called
interactive art, or new media art, has not been able to be reproduced due to
limitation of functionalities of the conventional media. In this article, the
authors report a novel approach of sharing such interactive art outside the
exhibition, so that the visitors of the museum can take a copy to home, and
even share it with non-visitors. The authors build up their new
projector-and-camera (ProCam) based interactive artwork for exhibition at
Museum of Contemporary Art Tokyo (MOT) by using Apple's iPhone. The exactly
same software driving this artwork was downloadable from Apple's App Store --
thus all visitors or even non-visitors could enjoy the same experience at home
or wherever they like.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2195</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2195</id><created>2014-11-08</created><authors><author><keyname>Liew</keyname><forenames>Li Ching</forenames></author><author><keyname>Goh</keyname><forenames>Ong Sing</forenames></author></authors><title>GeoTravel: Harvesting Ambient Geographic Footprints from GPS
  Trajectories</title><categories>cs.CY</categories><report-no>GeoTravel/PSM/FTMK/UTeM/2014</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study is about harvesting point of interest from GPS trajectories.
Trajectories are the paths that moving objects move by follow through space in
a function of time while GPS trajectories generally are point-sequences with
geographic coordinates, time stamp, speed and heading. User can get information
from GPS enable device. For example, user can acquire present location, search
the information around them and design driving routes to a destination and thus
design travel itineraries. By sharing GPS logs among each other, people are
able to find some places that attract them from other people's travel route.
Analysis on the GPS logs can get the point of interest that is popular. By
present the point of interest, user can choose travel place easily and the
travel itineraries is plan based on the user preferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2199</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2199</id><created>2014-11-09</created><authors><author><keyname>Gomaa</keyname><forenames>Ahmad</forenames></author><author><keyname>Elezabi</keyname><forenames>Ayman</forenames></author><author><keyname>Eissa</keyname><forenames>Mohamed</forenames></author></authors><title>A Subspace Method for I/Q Imbalance Estimation in Low-IF Receivers</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In low-intermediate-frequency (low-IF) receivers, I/Q imbalance (IQI) causes
interference on the desired signal from the blocker signal transmitted over the
image frequencies. Conventional approaches for data-aided IQI estimation in
zero-IF receivers are not applicable to low-IF receivers. In zero-IF receivers,
IQI induces self-interference where the pilots of the image interference are
completely identified. However, in low IF receivers, the image interference
originates from a foreign signal whose training sequence timing and structure
are neither known nor synchronized with the desired signal. We develop a
data-aided subspace method for the estimation of IQI parameters in low-IF
receivers in the presence of unknown fading channel. Our approach does not
require the knowledge of the interference statistics, channel model nor noise
statistics. Simulation results demonstrate the superiority of our approach over
other blind IQI compensation approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2208</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2208</id><created>2014-11-09</created><authors><author><keyname>Badawy</keyname><forenames>Ahmed</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author><author><keyname>Elfouly</keyname><forenames>Tarek</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author><author><keyname>Trinchero</keyname><forenames>Daniele</forenames></author><author><keyname>Chiasserini</keyname><forenames>Carla-Fabiana</forenames></author></authors><title>Secret Key Generation Based on AoA Estimation for Low SNR Conditions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of physical layer security, a physical layer characteristic is
used as a common source of randomness to generate the secret key. Therefore an
accurate estimation of this characteristic is the core for reliable secret key
generation. Estimation of almost all the existing physical layer characteristic
suffer dramatically at low signal to noise (SNR) levels. In this paper, we
propose a novel secret key generation algorithm that is based on the estimated
angle of arrival (AoA) between the two legitimate nodes. Our algorithm has an
outstanding performance at very low SNR levels. Our algorithm can exploit
either the Azimuth AoA to generate the secret key or both the Azimuth and
Elevation angles to generate the secret key. Exploiting a second common source
of randomness adds an extra degree of freedom to the performance of our
algorithm. We compare the performance of our algorithm to the algorithm that
uses the most commonly used characteristics of the physical layer which are
channel amplitude and phase. We show that our algorithm has a very low bit
mismatch rate (BMR) at very low SNR when both channel amplitude and phase based
algorithm fail to achieve an acceptable BMR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2212</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2212</id><created>2014-11-09</created><authors><author><keyname>Masoudi</keyname><forenames>Mehdi</forenames></author><author><keyname>Mazaheri</keyname><forenames>Milad</forenames></author><author><keyname>Rezaei</keyname><forenames>Aliakbar</forenames></author><author><keyname>Navi</keyname><forenames>Keivan</forenames></author></authors><title>Designing high-speed, low-power full adder cells based on carbon
  nanotube technology</title><categories>cs.AR</categories><comments>13 Pages, 13 Figures, 2 Tables</comments><doi>10.5121/vlsic.2014.5503</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents novel high speed and low power full adder cells based
on carbon nanotube field effect transistor (CNFET). Four full adder cells are
proposed in this article. First one (named CN9P4G) and second one (CN9P8GBUFF)
utilizes 13 and 17 CNFETs respectively. Third design that we named CN10PFS uses
only 10 transistors and is full swing. Finally, CN8P10G uses 18 transistors and
divided into two modules, causing Sum and Cout signals are produced in a
parallel manner. All inputs have been used straight, without inverting. These
designs also used the special feature of CNFET that is controlling the
threshold voltage by adjusting the diameters of CNFETs to achieve the best
performance and right voltage levels. All simulation performed using Synopsys
HSPICE software and the proposed designs are compared to other classical and
modern CMOS and CNFET-based full adder cells in terms of delay, power
consumption and power delay product.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2214</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2214</id><created>2014-11-09</created><authors><author><keyname>Saleh</keyname><forenames>Babak</forenames></author><author><keyname>Farhadi</keyname><forenames>Ali</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author></authors><title>Abnormal Object Recognition: A Comprehensive Study</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When describing images, humans tend not to talk about the obvious, but rather
mention what they find interesting. We argue that abnormalities and deviations
from typicalities are among the most important components that form what is
worth mentioning. In this paper we introduce the abnormality detection as a
recognition problem and show how to model typicalities and, consequently,
meaningful deviations from prototypical properties of categories. Our model can
recognize abnormalities and report the main reasons of any recognized
abnormality. We introduce the abnormality detection dataset and show
interesting results on how to reason about abnormalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2222</identifier>
 <datestamp>2015-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2222</id><created>2014-11-09</created><updated>2015-07-14</updated><authors><author><keyname>Karanjkar</keyname><forenames>Neha V.</forenames></author><author><keyname>Desai</keyname><forenames>Madhav P.</forenames></author></authors><title>Optimization of Discrete-parameter Multiprocessor Systems using a Novel
  Ergodic Interpolation Technique</title><categories>cs.DC cs.PF</categories><comments>A short version of this paper will be published in the proceedings of
  IEEE MASCOTS 2015 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern multi-core systems have a large number of design parameters, most of
which are discrete-valued, and this number is likely to keep increasing as chip
complexity rises. Further, the accurate evaluation of a potential design choice
is computationally expensive because it requires detailed cycle-accurate system
simulation. If the discrete parameter space can be embedded into a larger
continuous parameter space, then continuous space techniques can, in principle,
be applied to the system optimization problem. Such continuous space techniques
often scale well with the number of parameters.
  We propose a novel technique for embedding the discrete parameter space into
an extended continuous space so that continuous space techniques can be applied
to the embedded problem using cycle accurate simulation for evaluating the
objective function. This embedding is implemented using simulation-based
ergodic interpolation, which, unlike spatial interpolation, produces the
interpolated value within a single simulation run irrespective of the number of
parameters. We have implemented this interpolation scheme in a cycle-based
system simulator. In a characterization study, we observe that the interpolated
performance curves are continuous, piece-wise smooth, and have low statistical
error. We use the ergodic interpolation-based approach to solve a large
multi-core design optimization problem with 31 design parameters. Our results
indicate that continuous space optimization using ergodic interpolation-based
embedding can be a viable approach for large multi-core design optimization
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2223</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2223</id><created>2014-11-09</created><authors><author><keyname>Jang</keyname><forenames>Yong-Chull</forenames><affiliation>SWME Collaboration</affiliation></author><author><keyname>Jeong</keyname><forenames>Hwancheol</forenames><affiliation>SWME Collaboration</affiliation></author><author><keyname>Kim</keyname><forenames>Jangho</forenames><affiliation>SWME Collaboration</affiliation></author><author><keyname>Lee</keyname><forenames>Weonjong</forenames><affiliation>SWME Collaboration</affiliation></author><author><keyname>Pak</keyname><forenames>Jeonghwan</forenames><affiliation>SWME Collaboration</affiliation></author><author><keyname>Chung</keyname><forenames>Yuree</forenames><affiliation>SWME Collaboration</affiliation></author></authors><title>Code Optimization on Kepler GPUs and Xeon Phi</title><categories>hep-lat cs.DC</categories><comments>7 pages, 4 figures, Lattice 2014 proceeding</comments><journal-ref>PoS (LATTICE 2014) 035</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kepler GTX Titan Black and Kepler Tesla K40 are still the best GPUs for high
performance computing, although Maxwell GPUs such as GTX 980 are available in
the market. Hence, we measure the performance of our lattice QCD codes using
the Kepler GPUs. We also upgrade our code to use the latest CPS (Columbia
Physics System) library along with the most recent QUDA (QCD CUDA) library for
lattice QCD. These new libraries improve the performance of our conjugate
gradient (CG) inverter so that it runs twice faster than before. We also
investigate the performance of Xeon Phi 7120P coprocessor. It has similar
computing power with the Kepler GPUs in principle. However, its performance for
our CG code is significantly inferior to that of the GTX Titan Black GPUs at
present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2239</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2239</id><created>2014-11-09</created><authors><author><keyname>Medhat</keyname><forenames>Ramy</forenames></author><author><keyname>Joshi</keyname><forenames>Yogi</forenames></author><author><keyname>Bonakdarpour</keyname><forenames>Borzoo</forenames></author><author><keyname>Fischmeister</keyname><forenames>Sebastian</forenames></author></authors><title>Accelerated Runtime Verification of LTL Specifications with Counting
  Semantics</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Runtime verification is an effective automated method for specification-based
offline testing and analysis as well as online monitoring of complex systems.
The specification language is often a variant of regular expressions or a
popular temporal logic, such as LTL. This paper presents a novel and efficient
parallel algorithm for verifying a more expressive version of LTL
specifications that incorporates counting semantics, where nested quantifiers
can be subject to numerical constraints. Such constraints are useful in
evaluating thresholds (e.g., expected uptime of a web server). The significance
of this extension is that it enables us to reason about the correctness of a
large class of systems, such as web servers, OS kernels, and network behavior,
where properties are required to be instantiated for parameterized requests,
kernel objects, network nodes, etc. Our algorithm uses the popular {\em
MapReduce} architecture to split a program trace into variable-based clusters
at run time. Each cluster is then mapped to its respective monitor instances,
verified, and reduced collectively on a multi-core CPU or the GPU. Our
algorithm is fully implemented and we report very encouraging experimental
results, where the monitoring overhead is negligible on real-world data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2242</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2242</id><created>2014-11-09</created><authors><author><keyname>Avin</keyname><forenames>Chen</forenames></author><author><keyname>Lotker</keyname><forenames>Zvi</forenames></author><author><keyname>Peleg</keyname><forenames>David</forenames></author><author><keyname>Pignolet</keyname><forenames>Yvonne Anne</forenames></author><author><keyname>Turkel</keyname><forenames>Itzik</forenames></author></authors><title>Core-Periphery in Networks: An Axiomatic Approach</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent evidence shows that in many societies worldwide the relative sizes of
the economic and social elites are continuously shrinking. Is this a natural
social phenomenon? What are the forces that shape this process? We try to
address these questions by studying a Core-Periphery social structure composed
of a social elite, namely, a relatively small but well-connected and highly
influential group of powerful individuals, and the rest of society, the
periphery. Herein, we present a novel axiom-based model for the forces
governing the mutual influences between the elite and the periphery. Assuming a
simple set of axioms, capturing the elite's dominance, robustness, compactness
and density, we are able to draw strong conclusions about the elite-periphery
structure. In particular, we show that a balance of powers between elite and
periphery and an elite size that is sub-linear in the network size are
universal properties of elites in social networks that satisfy our axioms. We
note that the latter is in controversy to the common belief that the elite size
converges to a linear fraction of society (most recently claimed to be 1%). We
accompany these findings with a large scale empirical study on about 100
real-world networks, which supports our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2250</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2250</id><created>2014-11-09</created><authors><author><keyname>Arandjelovic</keyname><forenames>Ognjen</forenames></author><author><keyname>Pham</keyname><forenames>Duc-Son</forenames></author><author><keyname>Venkatesh</keyname><forenames>Svetha</forenames></author></authors><title>Two maximum entropy based algorithms for running quantile estimation in
  non-stationary data streams</title><categories>cs.DS</categories><comments>IEEE Transactions on Circuits and Systems for Video Technology, 2014.
  arXiv admin note: text overlap with arXiv:1409.7289</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need to estimate a particular quantile of a distribution is an important
problem which frequently arises in many computer vision and signal processing
applications. For example, our work was motivated by the requirements of many
semi-automatic surveillance analytics systems which detect abnormalities in
close-circuit television (CCTV) footage using statistical models of low-level
motion features. In this paper we specifically address the problem of
estimating the running quantile of a data stream when the memory for storing
observations is limited. We make several major contributions: (i) we highlight
the limitations of approaches previously described in the literature which make
them unsuitable for non-stationary streams, (ii) we describe a novel principle
for the utilization of the available storage space, (iii) we introduce two
novel algorithms which exploit the proposed principle in different ways, and
(iv) we present a comprehensive evaluation and analysis of the proposed
algorithms and the existing methods in the literature on both synthetic data
sets and three large real-world streams acquired in the course of operation of
an existing commercial surveillance system. Our findings convincingly
demonstrate that both of the proposed methods are highly successful and vastly
outperform the existing alternatives. We show that the better of the two
algorithms (data-aligned histogram) exhibits far superior performance in
comparison with the previously described methods, achieving more than 10 times
lower estimate errors on real-world data, even when its available working
memory is an order of magnitude smaller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2262</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2262</id><created>2014-11-09</created><updated>2015-08-14</updated><authors><author><keyname>Elbassioni</keyname><forenames>Khaled</forenames></author></authors><title>A Polynomial Delay Algorithm for Generating Connected Induced Subgraphs
  of a Given Cardinality</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a polynomial delay algorithm, that for any graph $G$ and positive
integer $k$, enumerates all connected induced subgraphs of $G$ of order $k$.
Our algorithm enumerates each subgraph in at most
$O((k\min\{(n-k),k\Delta\})^2(\Delta+\log k))$ and uses linear space $O(n+m)$,
where $n$ and $m$ are respectively the number of vertices and edges of $G$ and
$\Delta$ is the maximum degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2275</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2275</id><created>2014-11-09</created><authors><author><keyname>Elbassioni</keyname><forenames>Khaled M.</forenames></author></authors><title>On Finding Minimal Infrequent Elements in Multi-dimensional Data Defined
  over Partially Ordered Sets</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider databases in which each attribute takes values from a partially
ordered set (poset). This allows one to model a number of interesting scenarios
arising in different applications, including quantitative databases,
taxonomies, and databases in which each attribute is an interval representing
the duration of a certain event occurring over time. A natural problem that
arises in such circumstances is the following: given a database $\mathcal{D}$
and a threshold value $t$, find all collections of &quot;generalizations&quot; of
attributes which are &quot;supported&quot; by less than $t$ transactions from
$\mathcal{D}$. We call such collections infrequent elements. Due to
monotonicity, we can reduce the output size by considering only \emph{minimal}
infrequent elements. We study the complexity of finding all minimal infrequent
elements for some interesting classes of posets. We show how this problem can
be applied to mining association rules in different types of databases, and to
finding &quot;sparse regions&quot; or &quot;holes&quot; in quantitative data or in databases
recording the time intervals during which a re-occurring event appears over
time. Our main focus will be on these applications rather than on the
correctness or analysis of the given algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2276</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2276</id><created>2014-11-09</created><authors><author><keyname>Hoffmann</keyname><forenames>Matej</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Vincent C.</forenames></author></authors><title>Trade-Offs in Exploiting Body Morphology for Control: from Simple Bodies
  and Model-Based Control to Complex Bodies with Model-Free Distributed Control
  Schemes</title><categories>cs.RO cs.NE cs.SY</categories><journal-ref>Helmut Hauser; Rudolf M. F\&quot;uchslin &amp; Rolf Pfeifer, ed., 'E-book
  on Opinions and Outlooks on Morphological Computation', 2014, pp. 185--194</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Tailoring the design of robot bodies for control purposes is implicitly
performed by engineers, however, a methodology or set of tools is largely
absent and optimization of morphology (shape, material properties of robot
bodies, etc.) is lagging behind the development of controllers. This has become
even more prominent with the advent of compliant, deformable or &quot;soft&quot; bodies.
These carry substantial potential regarding their exploitation for
control---sometimes referred to as &quot;morphological computation&quot; in the sense of
offloading computation needed for control to the body. Here, we will argue in
favor of a dynamical systems rather than computational perspective on the
problem. Then, we will look at the pros and cons of simple vs. complex bodies,
critically reviewing the attractive notion of &quot;soft&quot; bodies automatically
taking over control tasks. We will address another key dimension of the design
space---whether model-based control should be used and to what extent it is
feasible to develop faithful models for different morphologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2286</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2286</id><created>2014-11-09</created><authors><author><keyname>Elango</keyname><forenames>Venmugil</forenames></author><author><keyname>Rastello</keyname><forenames>Fabrice</forenames></author><author><keyname>Pouchet</keyname><forenames>Louis-Noel</forenames></author><author><keyname>Ramanujam</keyname><forenames>J.</forenames></author><author><keyname>Sadayappan</keyname><forenames>P.</forenames></author></authors><title>On Characterizing the Data Access Complexity of Programs</title><categories>cs.CC</categories><acm-class>F.2; D.2.8</acm-class><doi>10.1145/2676726.2677010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Technology trends will cause data movement to account for the majority of
energy expenditure and execution time on emerging computers. Therefore,
computational complexity will no longer be a sufficient metric for comparing
algorithms, and a fundamental characterization of data access complexity will
be increasingly important. The problem of developing lower bounds for data
access complexity has been modeled using the formalism of Hong &amp; Kung's
red/blue pebble game for computational directed acyclic graphs (CDAGs).
However, previously developed approaches to lower bounds analysis for the
red/blue pebble game are very limited in effectiveness when applied to CDAGs of
real programs, with computations comprised of multiple sub-computations with
differing DAG structure. We address this problem by developing an approach for
effectively composing lower bounds based on graph decomposition. We also
develop a static analysis algorithm to derive the asymptotic data-access lower
bounds of programs, as a function of the problem size and cache size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2305</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2305</id><created>2014-11-09</created><authors><author><keyname>Zheng</keyname><forenames>Xun</forenames></author><author><keyname>Kim</keyname><forenames>Jin Kyu</forenames></author><author><keyname>Ho</keyname><forenames>Qirong</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Model-Parallel Inference for Big Topic Models</title><categories>cs.DC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In real world industrial applications of topic modeling, the ability to
capture gigantic conceptual space by learning an ultra-high dimensional topical
representation, i.e., the so-called &quot;big model&quot;, is becoming the next
desideratum after enthusiasms on &quot;big data&quot;, especially for fine-grained
downstream tasks such as online advertising, where good performances are
usually achieved by regression-based predictors built on millions if not
billions of input features. The conventional data-parallel approach for
training gigantic topic models turns out to be rather inefficient in utilizing
the power of parallelism, due to the heavy dependency on a centralized image of
&quot;model&quot;. Big model size also poses another challenge on the storage, where
available model size is bounded by the smallest RAM of nodes. To address these
issues, we explore another type of parallelism, namely model-parallelism, which
enables training of disjoint blocks of a big topic model in parallel. By
integrating data-parallelism with model-parallelism, we show that dependencies
between distributed elements can be handled seamlessly, achieving not only
faster convergence but also an ability to tackle significantly bigger model
size. We describe an architecture for model-parallel inference of LDA, and
present a variant of collapsed Gibbs sampling algorithm tailored for it.
Experimental results demonstrate the ability of this system to handle topic
modeling with unprecedented amount of 200 billion model variables only on a
low-end cluster with very limited computational resources and bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2311</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2311</id><created>2014-11-09</created><authors><author><keyname>Soto</keyname><forenames>Jos&#xe9; A.</forenames></author><author><keyname>Telha</keyname><forenames>Claudio</forenames></author></authors><title>Independent sets and hitting sets of bicolored rectangular families</title><categories>cs.CG cs.DS</categories><comments>36 pages, A preliminary version of this work appeared in IPCO 2011
  under the name &quot;Jump Number of Two-Directional Orthogonal Ray Graphs&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A bicolored rectangular family BRF is a collection of all axis-parallel
rectangles contained in a given region Z of the plane formed by selecting a
bottom-left corner from a set A and an upper-right corner from a set B. We
prove that the maximum independent set and the minimum hitting set of a BRF
have the same cardinality and devise polynomial time algorithms to compute
both. As a direct consequence, we obtain the first polynomial time algorithm to
compute minimum biclique covers, maximum cross-free matchings and jump numbers
in a class of bipartite graphs that significantly extends convex bipartite
graphs and interval bigraphs. We also establish several connections between our
work and other seemingly unrelated problems. Furthermore, when the bicolored
rectangular family is weighted, we show that the problem of finding the maximum
weight of an independent set is NP-hard, and provide efficient algorithms to
solve it on certain subclasses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2315</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2315</id><created>2014-11-09</created><authors><author><keyname>Chung</keyname><forenames>Kai-Min</forenames></author><author><keyname>Li</keyname><forenames>Xin</forenames></author><author><keyname>Wu</keyname><forenames>Xiaodi</forenames></author></authors><title>Multi-Source Randomness Extractors Against Quantum Side Information, and
  their Applications</title><categories>quant-ph cs.CC</categories><comments>52 pages; comments are welcome!</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of constructing multi-source extractors in the quantum
setting, which extract almost uniform random bits against quantum side
information collected from several initially independent classical random
sources. This is a natural generalization of seeded randomness extraction
against quantum side information and classical independent source extraction.
With new challenges such as potential entanglement in the side information, it
is not a prior clear under what conditions do quantum multi-source extractors
exist; the only previous work is [KK12], where the classical inner-product
two-source extractors of [CG88] and [DEOR04] are shown to be quantum secure in
the restricted Independent Adversary (IA) Model and entangled Bounded Storage
(BS) Model.
  In this paper we propose a new model called General Entangled (GE) Adversary
Model, which allows arbitrary entanglement in the side information and subsumes
both the IA model and the BS model. We proceed to show how to construct
GE-secure quantum multi-source extractors. To that end, we propose another
model called One-sided Adversary (OA) Model, which is weaker than all the above
models. Somewhat surprisingly, we establish equivalence between strong
OA-security and strong GE-security. As a result, all classical multi-source
extractors can either directly work, or be modified to work in the GE model at
the cost of one extra random source. Thus, our constructions essentially match
the best known constructions of classical multi-source extractors.
  We also apply our techniques to two important problems in cryptography and
distributed computing --- privacy amplification and network extractor. We show
that as long as the sources have certain amounts of conditional min-entropy in
our GE model (even with entangled quantum side information), we can design very
efficient privacy amplification protocols and network extractors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2316</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2316</id><created>2014-11-09</created><updated>2014-11-19</updated><authors><author><keyname>Fernandez</keyname><forenames>Joseph A.</forenames></author><author><keyname>Boddeti</keyname><forenames>Vishnu Naresh</forenames></author><author><keyname>Rodriguez</keyname><forenames>Andres</forenames></author><author><keyname>Kumar</keyname><forenames>B. V. K. Vijaya</forenames></author></authors><title>Zero-Aliasing Correlation Filters for Object Recognition</title><categories>cs.CV stat.ML</categories><comments>14 pages, to appear in IEEE Transactions on Pattern Analysis and
  Machine Intelligence (PAMI)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correlation filters (CFs) are a class of classifiers that are attractive for
object localization and tracking applications. Traditionally, CFs have been
designed in the frequency domain using the discrete Fourier transform (DFT),
where correlation is efficiently implemented. However, existing CF designs do
not account for the fact that the multiplication of two DFTs in the frequency
domain corresponds to a circular correlation in the time/spatial domain.
Because this was previously unaccounted for, prior CF designs are not truly
optimal, as their optimization criteria do not accurately quantify their
optimization intention. In this paper, we introduce new zero-aliasing
constraints that completely eliminate this aliasing problem by ensuring that
the optimization criterion for a given CF corresponds to a linear correlation
rather than a circular correlation. This means that previous CF designs can be
significantly improved by this reformulation. We demonstrate the benefits of
this new CF design approach with several important CFs. We present experimental
results on diverse data sets and present solutions to the computational
challenges associated with computing these CFs. Code for the CFs described in
this paper and their respective zero-aliasing versions is available at
http://vishnu.boddeti.net/projects/correlation-filters.html
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2328</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2328</id><created>2014-11-10</created><authors><author><keyname>Wang</keyname><forenames>Xun</forenames></author></authors><title>Modeling Word Relatedness in Latent Dirichlet Allocation</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard LDA model suffers the problem that the topic assignment of each word
is independent and word correlation hence is neglected. To address this
problem, in this paper, we propose a model called Word Related Latent Dirichlet
Allocation (WR-LDA) by incorporating word correlation into LDA topic models.
This leads to new capabilities that standard LDA model does not have such as
estimating infrequently occurring words or multi-language topic modeling.
Experimental results demonstrate the effectiveness of our model compared with
standard LDA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2331</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2331</id><created>2014-11-10</created><authors><author><keyname>Yamada</keyname><forenames>Makoto</forenames></author><author><keyname>Saha</keyname><forenames>Avishek</forenames></author><author><keyname>Ouyang</keyname><forenames>Hua</forenames></author><author><keyname>Yin</keyname><forenames>Dawei</forenames></author><author><keyname>Chang</keyname><forenames>Yi</forenames></author></authors><title>N$^3$LARS: Minimum Redundancy Maximum Relevance Feature Selection for
  Large and High-dimensional Data</title><categories>stat.ML cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1202.0515</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a feature selection method that finds non-redundant features from
a large and high-dimensional data in nonlinear way. Specifically, we propose a
nonlinear extension of the non-negative least-angle regression (LARS) called
N${}^3$LARS, where the similarity between input and output is measured through
the normalized version of the Hilbert-Schmidt Independence Criterion (HSIC). An
advantage of N${}^3$LARS is that it can easily incorporate with map-reduce
frameworks such as Hadoop and Spark. Thus, with the help of distributed
computing, a set of features can be efficiently selected from a large and
high-dimensional data. Moreover, N${}^3$LARS is a convex method and can find a
global optimum solution. The effectiveness of the proposed method is first
demonstrated through feature selection experiments for classification and
regression with small and high-dimensional datasets. Finally, we evaluate our
proposed method over a large and high-dimensional biology dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2335</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2335</id><created>2014-11-10</created><authors><author><keyname>Kumar</keyname><forenames>Kriti</forenames></author><author><keyname>Varghese</keyname><forenames>Ashley</forenames></author><author><keyname>Reddy</keyname><forenames>Pavan K</forenames></author><author><keyname>Narendra</keyname><forenames>N</forenames></author><author><keyname>Swamy</keyname><forenames>Prashanth</forenames></author><author><keyname>Chandra</keyname><forenames>M Girish</forenames></author><author><keyname>Balamuralidhar</keyname><forenames>P</forenames></author></authors><title>An Improved Tracking using IMU and Vision Fusion for Mobile Augmented
  Reality Applications</title><categories>cs.CV</categories><journal-ref>International Journal of Multimedia and its Applications, ISSN:
  0975-5578 (online); 0975-5934 (print), October 2014, Volume 6, Number 5</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Augmented Reality (MAR) is becoming an important cyber-physical system
application given the ubiquitous availability of mobile phones. With the need
to operate in unprepared environments, accurate and robust registration and
tracking has become an important research problem to solve. In fact, when MAR
is used for tele-interactive applications involving large distances, say from
an accident site to insurance office, tracking at both the ends is desirable
and further it is essential to appropriately fuse inertial and vision sensors
data. In this paper, we present results and discuss some insights gained in
marker-less tracking during the development of a prototype pertaining to an
example use case related to breakdown or damage assessment of a vehicle. The
novelty of this paper is in bringing together different components and modules
with appropriate enhancements towards a complete working system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2337</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2337</id><created>2014-11-10</created><authors><author><keyname>Fang</keyname><forenames>Chen</forenames></author><author><keyname>Rockmore</keyname><forenames>Daniel N.</forenames></author></authors><title>Multi-Task Metric Learning on Network Data</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-task learning (MTL) improves prediction performance in different
contexts by learning models jointly on multiple different, but related tasks.
Network data, which are a priori data with a rich relational structure, provide
an important context for applying MTL. In particular, the explicit relational
structure implies that network data is not i.i.d. data. Network data also often
comes with significant metadata (i.e., attributes) associated with each entity
(node). Moreover, due to the diversity and variation in network data (e.g.,
multi-relational links or multi-category entities), various tasks can be
performed and often a rich correlation exists between them. Learning algorithms
should exploit all of these additional sources of information for better
performance. In this work we take a metric-learning point of view for the MTL
problem in the network context. Our approach builds on structure preserving
metric learning (SPML). In particular SPML learns a Mahalanobis distance metric
for node attributes using network structure as supervision, so that the learned
distance function encodes the structure and can be used to predict link
patterns from attributes. SPML is described for single-task learning on single
network. Herein, we propose a multi-task version of SPML, abbreviated as
MT-SPML, which is able to learn across multiple related tasks on multiple
networks via shared intermediate parametrization. MT-SPML learns a specific
metric for each task and a common metric for all tasks. The task correlation is
carried through the common metric and the individual metrics encode task
specific information. When combined together, they are structure-preserving
with respect to individual tasks. MT-SPML works on general networks, thus is
suitable for a wide variety of problems. In experiments, we challenge MT-SPML
on two real-word problems, where MT-SPML achieves significant improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2344</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2344</id><created>2014-11-10</created><authors><author><keyname>Bhattacharyya</keyname><forenames>Arnab</forenames></author><author><keyname>Nair</keyname><forenames>Vineet</forenames></author></authors><title>An explicit sparse recovery scheme in the L1-norm</title><categories>cs.DS cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the approximate sparse recovery problem: given Ax, where A is a
known m-by-n dimensional matrix and x is an unknown (approximately) sparse
n-dimensional vector, recover an approximation to x. The goal is to design the
matrix A such that m is small and recovery is efficient. Moreover, it is often
desirable for A to have other nice properties, such as explicitness, sparsity,
and discreteness.
  In this work, we show that we can use spectral expander graphs to explicitly
design binary matrices A for which the column sparsity is optimal and for which
there is an efficient recovery algorithm (l1-minimization). In order to recover
x that is close to {\delta}n-sparse (where {\delta} is a constant), we design
an explicit binary matrix A that has m = O(sqrt{{\delta}} log(1/{\delta}) * n)
rows and has O(log(1/{\delta})) ones in each column. Previous such
constructions were based on unbalanced bipartite graphs with high vertex
expansion, for which we currently do not have explicit constructions. In
particular, ours is the first explicit non-trivial construction of a
measurement matrix A such that Ax can be computed in O(n log(1/{\delta})) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2345</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2345</id><created>2014-11-10</created><authors><author><keyname>Stanciu</keyname><forenames>Mihai I.</forenames></author><author><keyname>Azou</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>R&#x103;doi</keyname><forenames>Emanuel</forenames></author><author><keyname>&#x15e;erb&#x103;nescu</keyname><forenames>Alexandru</forenames></author></authors><title>A Statistical Analysis of Multipath Interference for Impulse Radio UWB
  Systems</title><categories>cs.NI</categories><comments>17 pages, 9 figures; submitted to the Journal of the Franklin
  Institute on Sept. 24, 2013</comments><journal-ref>Journal of the Franklin Institute Volume 352, Issue 12, December
  2015</journal-ref><doi>10.1016/j.jfranklin.2015.09.020</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a statistical characterization of the multipath
interference in an Impulse Radio (IR)-UWB system, considering the standardized
IEEE 802.15.4a channel model. In such systems, the chip length has to be
carefully tuned as all the propagation paths located beyond this limit can
cause interframe/intersymbol interferences (IFI/ISI). Our approach aims at
computing the probability density function (PDF) of the power of all multipath
components with delays larger than the chip time, so as to prevent such
interferences. Exact analytical expressions are derived first for the
probability that the chip length falls into a particular cluster of the
multipath propagation model and for the statistics of the number of paths
spread over several contiguous clusters. A power delay profile (PDP)
approximation is then used to evaluate the total interference power as the
problem appears to be mathematically intractable. Using the proposed
closed-form expressions, and assuming minimal prior information on the channel
state, a rapid update of the chip time value is enabled so as to control the
signal to interference plus noise ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2348</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2348</id><created>2014-11-10</created><authors><author><keyname>Meidl</keyname><forenames>Wilfried</forenames></author><author><keyname>Niederreiter</keyname><forenames>Harald</forenames></author></authors><title>Multisequences with high joint nonlinear complexity</title><categories>cs.IT math.IT</categories><msc-class>94A55, 65C10, 11K45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the new concept of joint nonlinear complexity for multisequences
over finite fields and we analyze the joint nonlinear complexity of two
families of explicit inversive multisequences. We also establish a
probabilistic result on the behavior of the joint nonlinear complexity of
random multisequences over a fixed finite field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2351</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2351</id><created>2014-11-10</created><updated>2014-11-11</updated><authors><author><keyname>Martens</keyname><forenames>Wim</forenames></author><author><keyname>Neven</keyname><forenames>Frank</forenames></author><author><keyname>Vansummeren</keyname><forenames>Stijn</forenames></author></authors><title>SCULPT: A Schema Language for Tabular Data on the Web</title><categories>cs.DB</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the recent working effort towards a recommendation by the World
Wide Web Consortium (W3C) for tabular data and metadata on the Web, we present
in this paper a concept for a schema language for tabular web data called
SCULPT. The language consists of rules constraining and defining the structure
of regions in the table. These regions are defined through the novel formalism
of region selection expressions. We present a formal model for SCULPT and
obtain a linear time combined complexity evaluation algorithm. In addition, we
consider weak and strong streaming evaluation for SCULPT and present a fragment
for each of these streaming variants. Finally, we discuss several extensions of
SCULPT including alternative semantics, types, complex content, and explore
region selection expressions as a basis for a transformation language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2356</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2356</id><created>2014-11-10</created><authors><author><keyname>Derakhshandeh</keyname><forenames>Zahra</forenames></author><author><keyname>Gmyr</keyname><forenames>Robert</forenames></author><author><keyname>Richa</keyname><forenames>Andrea W.</forenames></author><author><keyname>Scheideler</keyname><forenames>Christian</forenames></author><author><keyname>Strothmann</keyname><forenames>Thim</forenames></author><author><keyname>Tzur-David</keyname><forenames>Shimrit</forenames></author></authors><title>Infinite Object Coating in the Amoebot Model</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The term programmable matter refers to matter which has the ability to change
its physical properties (shape, density, moduli, conductivity, optical
properties, etc.) in a programmable fashion, based upon user input or
autonomous sensing. This has many applications like smart materials, autonomous
monitoring and repair, and minimal invasive surgery. While programmable matter
might have been considered pure science fiction more than two decades ago, in
recent years a large amount of research has been conducted in this field. Often
programmable matter is envisioned as a very large number of small locally
interacting computational particles. We propose the Amoebot model, a new model
which builds upon this vision of programmable matter. Inspired by the behavior
of amoeba, the Amoebot model offers a versatile framework to model
self-organizing particles and facilitates rigorous algorithmic research in the
area of programmable matter. We present an algorithm for the problem of coating
an infinite object under this model, and prove the correctness of the algorithm
and that it is work-optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2364</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2364</id><created>2014-11-10</created><authors><author><keyname>Zambianchi</keyname><forenames>Vincenzo</forenames></author><author><keyname>Paolini</keyname><forenames>Enrico</forenames></author><author><keyname>Dardari</keyname><forenames>Davide</forenames></author></authors><title>Capacity Achieving Peak Power Limited Probability Measures: Sufficient
  Conditions for Finite Discreteness</title><categories>cs.IT math.IT</categories><comments>23 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of capacity achieving (optimal) input probability measures has
been widely investigated for several channel models with constrained inputs. So
far, no outstanding generalizations have been derived. This paper does a
forward step in this direction, by introducing a set of new requirements, for
the class of real scalar conditional output probability measures, under which
the optimal input probability measure is shown to be discrete with a finite
number of probability mass points, when peak power limited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2367</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2367</id><created>2014-11-10</created><authors><author><keyname>Liu</keyname><forenames>Shuiyin</forenames></author><author><keyname>Hong</keyname><forenames>Yi</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>Unshared Secret Key Cryptography: Finite Constellation Inputs and Ideal
  Secrecy Outage</title><categories>cs.CR</categories><comments>7 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:1410.5021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Unshared Secret Key Cryptography (USK), recently proposed by the authors,
guarantees Shannon's ideal secrecy and perfect secrecy for MIMO wiretap
channels, without requiring secret key exchange. However, the requirement of
infinite constellation inputs limits its applicability to practical systems. In
this paper, we propose a practical USK scheme using finite constellation
inputs. The new scheme is based on a cooperative jamming technique, and is
valid for the case where the eavesdropper has more antennas than the
transmitter. We show that Shannon's ideal secrecy can be achieved with an
arbitrarily small outage probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2370</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2370</id><created>2014-11-10</created><authors><author><keyname>Luo</keyname><forenames>Wuqiong</forenames></author><author><keyname>Tay</keyname><forenames>Wee Peng</forenames></author><author><keyname>Leng</keyname><forenames>Mei</forenames></author></authors><title>On the Universality of Jordan Centers for Estimating Infection Sources
  in Tree Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding the infection sources in a network when we only know the network
topology and infected nodes, but not the rates of infection, is a challenging
combinatorial problem, and it is even more difficult in practice where the
underlying infection spreading model is usually unknown a priori. In this
paper, we are interested in finding a source estimator that is applicable to
various spreading models, including the Susceptible-Infected (SI),
Susceptible-Infected-Recovered (SIR), Susceptible-Infected-Recovered-Infected
(SIRI), and Susceptible-Infected-Susceptible (SIS) models. We show that under
the SI, SIR and SIRI spreading models and with mild technical assumptions, the
Jordan center is the infection source associated with the most likely infection
path in a tree network with a single infection source. This conclusion applies
for a wide range of spreading parameters, while it holds for regular trees
under the SIS model with homogeneous infection and recovery rates. Since the
Jordan center does not depend on the infection, recovery and reinfection rates,
it can be regarded as a universal source estimator. We also consider the case
where there are k&gt;1 infection sources, generalize the Jordan center definition
to a k-Jordan center set, and show that this is an optimal infection source set
estimator in a tree network for the SI model. Simulation results on various
general synthetic networks and real world networks suggest that Jordan
center-based estimators consistently outperform the distance, closeness, and
betweenness centrality based heuristics, even if the network is not a tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2373</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2373</id><created>2014-11-10</created><authors><author><keyname>d'Orey</keyname><forenames>Pedro M.</forenames></author><author><keyname>Maslekar</keyname><forenames>Nitin</forenames></author><author><keyname>de la Iglesia</keyname><forenames>Idoia</forenames></author><author><keyname>Zahariev</keyname><forenames>Nikola K.</forenames></author></authors><title>NAVI: Neighbor Aware Virtual Infrastructure for Information
  Dissemination in Vehicular Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular Networks enable a vast number of innovative applications, which
rely on the efficient exchange of information between vehicles. However,
efficient and reliable data dissemination is a particularly challenging task in
the context of vehicular networks due to the underlying properties of these
networks, limited availability of network infrastructure and variable
penetration rates for distinct communication technologies. This paper presents
a novel system and mechanism for information dissemination based on virtual
infrastructure selection in combination with multiple communication
technologies. The system has been evaluated using a simulation framework,
involving network simulation in conjugation with realistic vehicular mobility
traces. The presented simulation results show the feasibility of the proposed
mechanism to achieve maximum message penetration with reduced overhead.
Compared with a cellular-based only solution, our mechanism shows that the
judicious vehicle selection can lead to improved network utilization through
the offload of traffic to the short-range communication network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2374</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2374</id><created>2014-11-10</created><updated>2015-10-21</updated><authors><author><keyname>Liu</keyname><forenames>Kuan</forenames></author><author><keyname>Bellet</keyname><forenames>Aur&#xe9;lien</forenames></author><author><keyname>Sha</keyname><forenames>Fei</forenames></author></authors><title>Similarity Learning for High-Dimensional Sparse Data</title><categories>cs.LG stat.ML</categories><comments>14 pages. Proceedings of the 18th International Conference on
  Artificial Intelligence and Statistics (AISTATS 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A good measure of similarity between data points is crucial to many tasks in
machine learning. Similarity and metric learning methods learn such measures
automatically from data, but they do not scale well respect to the
dimensionality of the data. In this paper, we propose a method that can learn
efficiently similarity measure from high-dimensional sparse data. The core idea
is to parameterize the similarity measure as a convex combination of rank-one
matrices with specific sparsity structures. The parameters are then optimized
with an approximate Frank-Wolfe procedure to maximally satisfy relative
similarity constraints on the training data. Our algorithm greedily
incorporates one pair of features at a time into the similarity measure,
providing an efficient way to control the number of active features and thus
reduce overfitting. It enjoys very appealing convergence guarantees and its
time and memory complexity depends on the sparsity of the data instead of the
dimension of the feature space. Our experiments on real-world high-dimensional
datasets demonstrate its potential for classification, dimensionality reduction
and data exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2377</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2377</id><created>2014-11-10</created><authors><author><keyname>Kouya</keyname><forenames>Tomonori</forenames></author></authors><title>A Highly Efficient Implementation of Multiple Precision Sparse
  Matrix-Vector Multiplication and Its Application to Product-type Krylov
  Subspace Methods</title><categories>math.NA cs.NA</categories><journal-ref>International Journal of Numerical Methods and Applications,
  Vol.7, Issue 2, 2012, Pages 107 - 119</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We evaluate the performance of the Krylov subspace method by using highly
efficient multiple precision sparse matrix-vector multiplication (SpMV).
BNCpack is our multiple precision numerical computation library based on
MPFR/GMP, which is one of the most efficient arbitrary precision floating-point
arithmetic libraries. However, it does not include functions that can
manipulate multiple precision sparse matrices. Therefore, by using benchmark
tests, we show that SpMV implemented in these functions can be more efficient.
Finally, we also show that product-type Krylov subspace methods such as BiCG
and GPBiCG in which we have embedded SpMV, can efficiently solve large-scale
linear systems of equations provided in the UF sparse matrix collections in a
memory-restricted computing environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2378</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2378</id><created>2014-11-10</created><authors><author><keyname>Reyes</keyname><forenames>Eduardo Hermo</forenames></author><author><keyname>Joosten</keyname><forenames>Joost J.</forenames></author></authors><title>The Selfish Algorithm</title><categories>cs.CC nlin.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The principle of Generalized Natural Selection (GNS) states that in nature,
computational processes of high computational sophistication are more likely to
maintain/abide than processes of lower computational sophistication provided
that sufficiently many resources are around to sustain the processes. In this
paper we give a concrete set-up how to test GNS in a weak sense. In particular,
we work in the setting of Cellular Automata and see how GNS can manifest itself
in this setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2384</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2384</id><created>2014-11-10</created><updated>2015-05-05</updated><authors><author><keyname>Mercure</keyname><forenames>J. -F.</forenames></author><author><keyname>Lam</keyname><forenames>A.</forenames></author></authors><title>The effectiveness of policy on consumer choices for private road
  passenger transport emissions reductions in six major economies</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 5 figures, 2 tables + 8 pages Supplementary Information
  included, to appear in this final form in Environmental Research Letters</comments><journal-ref>Environmental Research Letters 10 (2015) 064008</journal-ref><doi>10.1088/1748-9326/10/6/064008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The effectiveness of fiscal policy to influence vehicle purchases for
emissions reductions in private passenger road transport depends on its ability
to incentivise consumers to make choices oriented towards lower emissions
vehicles. However, car purchase choices are known to be strongly socially
determined, and this sector is highly diverse due to significant socio-economic
differences between consumer groups. Here, we present a comprehensive dataset
and analysis of the structure of the 2012 private passenger vehicle fleet-years
in six major economies across the World (UK, USA, China, India, Japan and
Brazil) in terms of price, engine size and emissions distributions. We argue
that choices and aggregate elasticities of substitution can be predicted using
this data, enabling to evaluate the effectiveness of potential fiscal and
technological change policies on fleet-year emissions reductions. We provide
tools to do so based on the distributive structure of prices and emissions in
segments of a diverse market, both for conventional as well as unconventional
engine technologies. We find that markets differ significantly between nations,
and that correlations between engine sizes, emissions and prices exist strongly
in some markets and not strongly in others. We furthermore find that markets
for unconventional engine technologies have patchy coverages of varying levels.
These findings are interpreted in terms of policy strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2392</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2392</id><created>2014-11-10</created><authors><author><keyname>Zabolotnyi</keyname><forenames>Rostyslav</forenames></author><author><keyname>Leitner</keyname><forenames>Philipp</forenames></author><author><keyname>Hummer</keyname><forenames>Waldemar</forenames></author><author><keyname>Dustdar</keyname><forenames>Schahram</forenames></author></authors><title>JCloudScale: Closing the Gap Between IaaS and PaaS</title><categories>cs.SE cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Infrastructure-as-a-Service (IaaS) model of cloud computing is a
promising approach towards building elastically scaling systems. Unfortunately,
building such applications today is a complex, repetitive and error-prone
endeavor, as IaaS does not provide any abstraction on top of naked virtual
machines. Hence, all functionality related to elasticity needs to be
implemented anew for each application. In this paper, we present JCloudScale, a
Java-based middleware that supports building elastic applications on top of a
public or private IaaS cloud. JCloudScale allows to easily bring applications
to the cloud, with minimal changes to the application code. We discuss the
general architecture of the middleware as well as its technical features, and
evaluate our system with regard to both, user acceptance (based on a user
study) and performance overhead. Our results indicate that JCloudScale indeed
allowed many participants to build IaaS applications more efficiently,
comparable to the convenience features provided by industrial
Platform-as-a-Service (PaaS) solutions. However, unlike PaaS, using JCloudScale
does not lead to a loss of control and vendor lock-in for the developer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2394</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2394</id><created>2014-11-10</created><authors><author><keyname>Budaghyan</keyname><forenames>Lilya</forenames></author><author><keyname>Kholosha</keyname><forenames>Alexander</forenames></author><author><keyname>Carlet</keyname><forenames>Claude</forenames></author><author><keyname>Helleseth</keyname><forenames>Tor</forenames></author></authors><title>Univariate Niho Bent Functions from o-Polynomials</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discover that any univariate Niho bent function is a sum of
functions having the form of Leander-Kholosha bent functions with extra
coefficients of the power terms. This allows immediately, knowing the terms of
an o-polynomial, to obtain the powers of the additive terms in the polynomial
representing corresponding bent function. However, the coefficients are
calculated ambiguously. The explicit form is given for the bent functions
obtained from quadratic and cubic o-polynomials. We also calculate the
algebraic degree of any bent function in the Leander-Kholosha class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2404</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2404</id><created>2014-11-10</created><authors><author><keyname>Larsen</keyname><forenames>Kasper Green</forenames></author><author><keyname>Nelson</keyname><forenames>Jelani</forenames></author></authors><title>The Johnson-Lindenstrauss lemma is optimal for linear dimensionality
  reduction</title><categories>cs.IT cs.CG cs.DS math.FA math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For any $n&gt;1$ and $0&lt;\varepsilon&lt;1/2$, we show the existence of an
$n^{O(1)}$-point subset $X$ of $\mathbb{R}^n$ such that any linear map from
$(X,\ell_2)$ to $\ell_2^m$ with distortion at most $1+\varepsilon$ must have $m
= \Omega(\min\{n, \varepsilon^{-2}\log n\})$. Our lower bound matches the upper
bounds provided by the identity matrix and the Johnson-Lindenstrauss lemma,
improving the previous lower bound of Alon by a $\log(1/\varepsilon)$ factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2405</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2405</id><created>2014-11-10</created><authors><author><keyname>Sabetsarvestani</keyname><forenames>Zahra</forenames></author><author><keyname>Amindavar</keyname><forenames>Hamidreza</forenames></author></authors><title>Sparse Estimation with Generalized Beta Mixture and the Horseshoe Prior</title><categories>cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the use of the Generalized Beta Mixture (GBM) and Horseshoe
distributions as priors in the Bayesian Compressive Sensing framework is
proposed. The distributions are considered in a two-layer hierarchical model,
making the corresponding inference problem amenable to Expectation Maximization
(EM). We present an explicit, algebraic EM-update rule for the models, yielding
two fast and experimentally validated algorithms for signal recovery.
Experimental results show that our algorithms outperform state-of-the-art
methods on a wide range of sparsity levels and amplitudes in terms of
reconstruction accuracy, convergence rate and sparsity. The largest improvement
can be observed for sparse signals with high amplitudes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2406</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2406</id><created>2014-11-10</created><authors><author><keyname>Khlevnoy</keyname><forenames>Vladimir A.</forenames></author><author><keyname>Shchurov</keyname><forenames>Andrey A.</forenames></author></authors><title>A Formal Approach to Distributed System Security Test Generation</title><categories>cs.CR cs.DC</categories><comments>7 pages, 6 figures, 3 tables, Published with International Journal of
  Computer Trends and Technology (IJCTT). arXiv admin note: text overlap with
  arXiv:1410.1747</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V16(3), 2014 pg 121-127</journal-ref><doi>10.14445/22312803/IJCTT-V16P130</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deployment of distributed systems sets high requirements for procedures for
the security testing of these systems. This work introduces: (1) a list of
typical threats based on standards and actual practices; (2) an extended
six-layered model for test generation mission on the basis of technical
specifications and end-user requirements. Based on the list of typical threats
and the multilayer model, we describe a formal approach to the automated design
and generation of security mechanisms checklists for complex distributed
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2408</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2408</id><created>2014-11-10</created><authors><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Klein</keyname><forenames>Cornel</forenames></author></authors><title>Automata Describing Object Behavior</title><categories>cs.SE</categories><comments>22 pages, 5 figures</comments><journal-ref>Specification of Behavioral Semantics in Object-Oriented
  Information Modeling. Haim Kilov W. Harvey (ed.). p. 265-286. Kluwer Academic
  Publishers. 1996</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relating formal re nement techniques with commercial object oriented software
development methods is important to achieve enhancement of the power and
exibility of these software development methods and tools. We will present an
automata model together with a denotational and an operational semantics to
describe the behavior of objects. Based on the given semantics we de ne a set
of powerful re nement rules and discuss their applicability in software
engineering practice especially with the use of inheritance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2410</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2410</id><created>2014-11-10</created><authors><author><keyname>Broy</keyname><forenames>Manfred</forenames></author><author><keyname>Huber</keyname><forenames>Franz</forenames></author><author><keyname>Paech</keyname><forenames>Barbara</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Spies</keyname><forenames>Katharina</forenames></author></authors><title>Software and System Modeling Based on a Unified Formal Semantics</title><categories>cs.SE</categories><comments>26 pages, 4 figures; Requirements Targeting Software and Systems
  Engineering. International Workshop RTSE'97. Manfred Broy, Bernhard Rumpe
  (Eds.). Bernried, Germany, October 1997. LNCS 1526, Springer Verlag</comments><doi>10.1007/10692867_2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling and documentation are two essential ingredients for the engineering
discipline of software development. During the last twenty years a wide variety
of description and modeling techniques as well as document formats has been
proposed. However, often these are not integrated into a coherent methodology
with well-defined dependencies between the models and documentations. This
hampers focused software development as well as the provision of powerful
tool-support. In this paper we present the main issues and outline solutions in
the direction of a unified, formal basis for software and system modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2414</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2414</id><created>2014-11-10</created><authors><author><keyname>Philipps</keyname><forenames>Jan</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Refinement of Pipe-and-Filter Architectures</title><categories>cs.SE</categories><comments>20 pages, 4 figures; FM'99 - Formal Methods, Proceedings of the World
  Congress on Formal Methods in the Development of Computing System. LNCS 1708,
  pages 96-115 J. M. Wing, J. Woodcock, J. Davies (eds.) Springer, 1999</comments><doi>10.1007/3-540-48119-2_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software and hardware architectures are prone to modifications. We
demonstrate how a mathematically founded powerful refinement calculus for a
class of architectures, namely pipe and filter architectures, can be used to
modify a system in a provably correct way. The calculus consists of basic rules
to add and to remove filters (components) and pipes (channels) to a system. A
networking example demonstrates some of the features of our calculus. The
calculus is simple, flexible and compositional. Thus it allows us to build more
complex and specific rules that e.g. embed models of existing architectures or
define design patterns as transformation rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2417</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2417</id><created>2014-11-10</created><updated>2016-01-07</updated><authors><author><keyname>Bidokhti</keyname><forenames>Shirin Saeedi</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author></authors><title>Capacity Results for Multicasting Nested Message Sets over Combination
  Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of multicasting two nested messages is studied over a class of
networks known as combination networks. A source multicasts two messages, a
common and a private message, to several receivers. A subset of the receivers
(called the public receivers) only demand the common message and the rest of
the receivers (called the private receivers) demand both the common and the
private message. Three encoding schemes are discussed that employ linear
superposition coding and their optimality is proved in special cases. The
standard linear superposition scheme is shown to be optimal for networks with
two public receivers and any number of private receivers. When the number of
public receivers increases, this scheme stops being optimal. Two improvements
are discussed: one using pre-encoding at the source, and one using a block
Markov encoding scheme. The rate-regions that are achieved by the two schemes
are characterized in terms of feasibility problems. Both inner-bounds are shown
to be the capacity region for networks with three (or fewer) public and any
number of private receivers. Although the inner bounds are not comparable in
general, it is shown through an example that the region achieved by the block
Markov encoding scheme may strictly include the region achieved by the
pre-encoding/linear superposition scheme. Optimality results are founded on the
general framework of Balister and Bollob\'as (2012) for sub-modularity of the
entropy function. An equivalent graphical representation is introduced and a
lemma is proved that might be of independent interest.
  Motivated by the connections between combination networks and broadcast
channels, a new block Markov encoding scheme is proposed for broadcast channels
with two nested messages. The rate-region that is obtained includes the
previously known rate-regions. It remains open whether this inclusion is
strict.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2419</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2419</id><created>2014-11-10</created><authors><author><keyname>Hejda</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Steiner</keyname><forenames>Wolfgang</forenames></author></authors><title>Beta-expansions of rational numbers in quadratic Pisot bases</title><categories>math.DS cs.DM math.CO math.NT</categories><comments>12 pages, 3 figures, 2 tables</comments><msc-class>11A63 (11R06 37B10)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study rational numbers with purely periodic R\'enyi $\beta$-expansions.
For bases $\beta$ satisfying $\beta^2=a\beta+b$ with $b$ dividing $a$, we give
a necessary and sufficient condition for $\gamma(\beta)=1$, i.e., that all
rational numbers $p/q\in[0,1)$ with $\gcd(q,b)=1$ have a purely periodic
$\beta$-expansion. A simple algorithm for determining the value of
$\gamma(\beta)$ for all quadratic Pisot numbers $\beta$ is described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2427</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2427</id><created>2014-11-10</created><authors><author><keyname>Vieira</keyname><forenames>T. M.</forenames></author><author><keyname>Viswanathan</keyname><forenames>G. M.</forenames></author><author><keyname>da Silva</keyname><forenames>L. R.</forenames></author></authors><title>How to efficiently destroy a network with limited information</title><categories>physics.soc-ph cs.SI</categories><comments>10 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the general problem of how best to attack and destroy a network by
node removal, given limited or no prior information about the edges. We
consider a family of strategies in which nodes are randomly chosen, but not
removed. Instead, a random acquaintance (i.e., a first neighbour) of the chosen
node is removed from the network. By assigning an informal cost to the
information about the network structure, we show using cost-benefit analysis
that acquaintance removal is the optimal strategy to destroy networks
efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2429</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2429</id><created>2014-11-10</created><updated>2016-01-19</updated><authors><author><keyname>Leitner</keyname><forenames>Philipp</forenames></author><author><keyname>Cito</keyname><forenames>Juergen</forenames></author></authors><title>Patterns in the Chaos - a Study of Performance Variation and
  Predictability in Public IaaS Clouds</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Benchmarking the performance of public cloud providers is a common research
topic. Previous research has already extensively evaluated the performance of
different cloud platforms for different use cases, and under different
constraints and experiment setups. In this paper, we present a principled,
large-scale literature review to collect and codify existing research regarding
the predictability of performance in public Infrastructure-as-a-Service (IaaS)
clouds. We formulate 15 hypotheses relating to the nature of performance
variations in IaaS systems, to the factors of influence of performance
variations, and how to compare different instance types. In a second step, we
conduct extensive real-life experimentation on Amazon EC2 and Google Compute
Engine to empirically validate those hypotheses. At the time of our research,
performance in EC2 was substantially less predictable than in GCE. Further, we
show that hardware heterogeneity is in practice less prevalent than anticipated
by earlier research, while multi-tenancy has a dramatic impact on performance
and predictability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2435</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2435</id><created>2014-11-10</created><authors><author><keyname>Zhou</keyname><forenames>Yifan</forenames></author><author><keyname>Zhao</keyname><forenames>Zhifeng</forenames></author><author><keyname>Ying</keyname><forenames>Qianlan</forenames></author><author><keyname>Li</keyname><forenames>Rongpeng</forenames></author><author><keyname>Zhou</keyname><forenames>Xuan</forenames></author><author><keyname>Chen</keyname><forenames>Xianfu</forenames></author><author><keyname>Zhang</keyname><forenames>Honggang</forenames></author></authors><title>Large-scale Spatial Distribution Identification of Base Stations in
  Cellular Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of cellular system significantly depends on its network
topology, where the spatial deployment of base stations (BSs) plays a key role
in the downlink scenario. Moreover, cellular networks are undergoing a
heterogeneous evolution, which introduces unplanned deployment of smaller BSs,
thus complicating the performance evaluation even further. In this paper, based
on large amount of real BS locations data, we present a comprehensive analysis
on the spatial modeling of cellular network structure. Unlike the related
works, we divide the BSs into different subsets according to geographical
factor (e.g. urban or rural) and functional type (e.g. macrocells or
microcells), and perform detailed spatial analysis to each subset. After
examining the accuracy of Poisson point process (PPP) in BS locations modeling,
we take into account the Gibbs point processes as well as Neyman-Scott point
processes and compare their accuracy in view of large-scale modeling test.
Finally, we declare the inaccuracy of the PPP model, and reveal the general
clustering nature of BSs deployment, which distinctly violates the traditional
assumption. This paper carries out a first large-scale identification regarding
available literatures, and provides more realistic and more general results to
contribute to the performance analysis for the forthcoming heterogeneous
cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2438</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2438</id><created>2014-11-10</created><updated>2014-12-10</updated><authors><author><keyname>Amiri</keyname><forenames>Saeed Akhoondian</forenames></author><author><keyname>Kreutzer</keyname><forenames>Stephan</forenames></author><author><keyname>Rabinovich</keyname><forenames>Roman</forenames></author></authors><title>DAG-width is PSPACE-complete</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Berwanger et al. show that for every graph $G$ of size $n$ and DAG-width $k$
there is a DAG decomposition of width $k$ and size $n^{O(k)}$. This gives a
polynomial time algorithm for determining the DAG-width of a graph for any
fixed $k$. However, if the DAG-width of the graphs from a class is not bounded,
such algorithms become exponential. This raises the question whether we can
always find a DAG decomposition of size polynomial in $n$ as it is the case for
tree width and all generalisations of tree width similar to DAG-width.
  We show that there is an infinite class of graphs such that every DAG
decomposition of optimal width has size super-polynomial in $n$ and, moreover,
there is no polynomial size DAG decomposition which would approximate an
optimal decomposition up to an additive constant.
  In the second part we use our construction to prove that deciding whether the
DAG-width of a given graph is at most a given constant is PSPACE-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2443</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2443</id><created>2014-11-10</created><authors><author><keyname>Hsu</keyname><forenames>Bo-Kai</forenames></author><author><keyname>Lee</keyname><forenames>Chia-Han</forenames></author><author><keyname>Yeh</keyname><forenames>Ping-Cheng</forenames></author></authors><title>On Timing Synchronization for Quantity-based Modulation in Additive
  Inverse Gaussian Channel with Drift</title><categories>cs.ET</categories><comments>8 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Diffusion-based Molecular Communications, the channel between Transmitter
Nano-machine (TN) and Receiver Nano-machine (RN) can be modeled by Additive
Inverse Gaussian Channel, that is the first hitting time of messenger molecule
released from TN and captured by RN follows Inverse Gaussian distribution. In
this channel, a quantity-based modulation embedding message on the different
quantity levels of messenger molecules relies on a time-slotted system between
TN and RN. Accordingly, their clocks need to synchronize with each other. In
this paper, we discuss the approaches to make RN estimate its timing offset
between TN efficiently by the arrival times of molecules. We propose many
methods such as Maximum Likelihood Estimation (MLE), Unbiased Linear Estimation
(ULE), Iterative ULE, and Decision Feedback (DF). The numerical results shows
the comparison of them. We evaluate these methods by not only the Mean Square
Error, but also the computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2458</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2458</id><created>2014-11-10</created><authors><author><keyname>Luna</keyname><forenames>Alexandre J. H. de O.</forenames></author><author><keyname>Costa</keyname><forenames>Cleyverson P.</forenames></author><author><keyname>de Moura</keyname><forenames>Hermano P.</forenames></author><author><keyname>Novaes</keyname><forenames>Magdala A.</forenames></author><author><keyname>Nascimento</keyname><forenames>Cesar A. D. C. do</forenames></author></authors><title>Agile governance in Information and Communication Technologies: shifting
  paradigms</title><categories>cs.CY cs.SE</categories><comments>Journal of Information Systems and Technology Management (JISTEM),
  Vol 7, No 2, August 2010</comments><acm-class>K.6; K.6.1; K.6.4; D.2; D.2.9</acm-class><doi>10.4301/S1807-17752010000200004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the basis of the Agile Governance in Information and
Communication Technology (ICT), which is based on Agile Software Engineering
Methodologies principles and values. Its development was done through a
systematic review process, supported by Bibliometrics and Scientometrics
methods and techniques, where the Critical Success Factors (CSF) of ICT
Governance projects and the principles of the Agile Manifesto were analyzed.
Next, through an inductive approach, focused on the convergence between the
concepts involved, it was analyzed how agile principles could help to minimize
the gap between ICT and business. Evidences of their occurrence were taken
through a Conceptual Survey Research. As a result, the foundations and concepts
of Agile Governance in ICT were defined and, finally, the development of a
reference model was proposed as a future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2463</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2463</id><created>2014-11-10</created><authors><author><keyname>Zheng</keyname><forenames>Mengfan</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Chen</keyname><forenames>Wen</forenames></author></authors><title>Polar Coding for Secure Transmission in MISO Fading Wiretap Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a polar coding scheme for secure communication over
the multiple-input, single-output, single-antenna eavesdropper (MISOSE) fading
channel. We consider the case of block fading channels with known eavesdropper
channel state information (CSI) and the case of fading channels with known
eavesdropper channel distribution information (CDI). We use the artificial
noise assisted secure precoding method to maximize the secrecy capacity in the
first case, and to overcome the unawareness of the eavesdropper CSI in the
second case. We show that our proposed scheme can provide both reliable and
secure communication over the MISOSE channel with low encoding and decoding
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2469</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2469</id><created>2014-10-01</created><authors><author><keyname>Jasim</keyname><forenames>Omer K.</forenames></author><author><keyname>Abbas</keyname><forenames>Safia</forenames></author><author><keyname>El-Horbaty</keyname><forenames>El-Sayed M.</forenames></author><author><keyname>Salem</keyname><forenames>Abdel-Badeeh M.</forenames></author></authors><title>A New Trend of Pseudo Random Number Generation using QKD</title><categories>cs.CR</categories><comments>5 pages, 5 figures, 1 table, International Journal of Computer
  Applications, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random Numbers determine the security level of cryptographic applications as
they are used to generate padding schemes in the encryption/decryption process
as well as used to generate cryptographic keys. This paper utilizes the QKD to
generate a random quantum bit rely on BB84 protocol, using the NIST and DIEHARD
randomness test algorithms to test and evaluate the randomness rates for key
generation. The results show that the bits generated using QKD are truly
random, which in turn, overcomes the distance limitation (associated with QKD)
issue, its well-known challenges with the sending/ receiving data process
between different communication parties
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2484</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2484</id><created>2014-11-10</created><updated>2015-02-01</updated><authors><author><keyname>Beale</keyname><forenames>Paul D.</forenames></author></authors><title>A new class of scalable parallel pseudorandom number generators based on
  Pohlig-Hellman exponentiation ciphers</title><categories>physics.comp-ph cs.CR</categories><comments>12 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallel supercomputer-based Monte Carlo applications depend on pseudorandom
number generators that produce independent pseudorandom streams across many
separate processes. We propose a new scalable class of parallel pseudorandom
number generators based on Pohlig--Hellman exponentiation ciphers. The method
generates uniformly distributed floating point pseudorandom streams by
encrypting simple sequences of integer \textit{messages} into
\textit{ciphertexts} by exponentiation modulo prime numbers. The advantages of
the method are: the method is trivially parallelizable by parameterization with
each pseudorandom number generator derived from an independent prime modulus,
the method is fully scalable on massively parallel computing clusters due to
the large number of primes available for each implementation, the seeding and
initialization of the independent streams is simple, the method requires only a
few integer multiply--mod operations per pseudorandom number, the state of each
instance is defined by only a few integer values, the period of each instance
is different, and the method passes a battery of intrastream and interstream
correlation tests using up to $10^{13}$ pseudorandom numbers per test. The
32-bit implementation we propose has millions of possible instances, all with
periods greater than $10^{18}$. A 64-bit implementation depends on 128-bit
arithmetic, but would have more than $10^{15}$ possible instances and periods
greater than $10^{37}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2498</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2498</id><created>2014-11-10</created><authors><author><keyname>Belmega</keyname><forenames>E. Veronica</forenames></author><author><keyname>Sankar</keyname><forenames>Lalitha</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Enabling Data Exchange in Interactive State Estimation under Privacy
  Constraints</title><categories>cs.IT cs.GT math.IT</categories><comments>submitted to IEEE Journal of Selected Topics in Signal Processing</comments><doi>10.1109/JSTSP.2015.2427775</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data collecting agents in large networks, such as the electric power system,
need to share information (measurements) for estimating the system state in a
distributed manner. However, privacy concerns may limit or prevent this
exchange leading to a tradeoff between state estimation fidelity and privacy
(referred to as competitive privacy). This paper builds upon a recent
information-theoretic result (using mutual information to measure privacy and
mean-squared error to measure fidelity) that quantifies the region of
achievable distortion-leakage tuples in a two-agent network. The objective of
this paper is to study centralized and decentralized mechanisms that can enable
and sustain non-trivial data exchanges among the agents. A centralized
mechanism determines the data sharing policies that optimize a network-wide
objective function combining the fidelities and leakages at both agents. Using
common-goal games and best-response analysis, the optimal policies allow for
distributed implementation. In contrast, in the decentralized setting, repeated
discounted games are shown to naturally enable data exchange without any
central control nor economic incentives. The effect of repetition is modeled by
a time-averaged payoff function at each agent which combines its fidelity and
leakage at each interaction stage. For both approaches, it is shown that
non-trivial data exchange can be sustained for specific fidelity ranges even
when privacy is a limiting factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2499</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2499</id><created>2014-11-10</created><authors><author><keyname>Delhibabu</keyname><forenames>Radhakrishnan</forenames></author></authors><title>Comparative Study of View Update Algorithms in Rational Choice Theory</title><categories>cs.AI cs.DB cs.LO</categories><comments>http://link.springer.com/article/10.1007/s10489-014-0580-7. arXiv
  admin note: substantial text overlap with arXiv:1407.3512, arXiv:1301.5154</comments><doi>10.1007/s10489-014-0580-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamics of belief and knowledge is one of the major components of any
autonomous system that should be able to incorporate new pieces of information.
We show that knowledge base dynamics has interesting connection with kernel
change via hitting set and abduction. The approach extends and integrates
standard techniques for efficient query answering and integrity checking. The
generation of hitting set is carried out through a hyper tableaux calculus and
magic set that is focused on the goal of minimality. Many different view update
algorithms have been proposed in the literature to address this problem. The
present paper provides a comparative study of view update algorithms in
rational approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2503</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2503</id><created>2014-11-10</created><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames></author><author><keyname>Orfila</keyname><forenames>Jean-Baptiste</forenames></author></authors><title>Generating S-Boxes from Semi-fields Pseudo-extensions</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Specific vectorial boolean functions, such as S-Boxes or APN functions have
many applications, for instance in symmetric ciphers. In cryptography they must
satisfy some criteria (balancedness, high nonlinearity, high algebraic degree,
avalanche, or transparency) to provide best possible resistance against
attacks. Functions satisfying most criteria are however difficult to find.
Indeed, random generation does not work and the S-Boxes used in the AES or
Camellia ciphers are actually variations around a single function, the inverse
function in F_2^n. Would the latter function have an unforeseen weakness (for
instance if more practical algebraic attacks are developped), it would be
desirable to have some replacement candidates. For that matter, we propose to
weaken a little bit the algebraic part of the design of S-Boxes and use finite
semifields instead of finite fields to build such S-Boxes. Since it is not even
known how many semifields there are of order 256, we propose to build S-Boxes
and APN functions via semifields pseudo-extensions of the form S_{2^4}^2, where
S_{2^4} is any semifield of order 16 . Then, we mimic in this structure the use
of functions applied on a finite fields, such as the inverse or the cube. We
report here the construction of 12781 non equivalent S-Boxes with with maximal
nonlinearity, differential invariants, degrees and bit interdependency, and
2684 APN functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2513</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2513</id><created>2014-11-10</created><authors><author><keyname>Chee</keyname><forenames>Yeow Meng</forenames></author><author><keyname>Kiah</keyname><forenames>Han Mao</forenames></author><author><keyname>Zhang</keyname><forenames>Hui</forenames></author><author><keyname>Zhang</keyname><forenames>Xiande</forenames></author></authors><title>Constructions of Optimal and Near-Optimal Multiply Constant-Weight Codes</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiply constant-weight codes (MCWCs) have been recently studied to improve
the reliability of certain physically unclonable function response. In this
paper, we give combinatorial constructions for MCWCs which yield several new
infinite families of optimal MCWCs. Furthermore, we demonstrate that the
Johnson type upper bounds of MCWCs are asymptotically tight for fixed weights
and distances. Finally, we provide bounds and constructions of two dimensional
MCWCs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2516</identifier>
 <datestamp>2015-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2516</id><created>2014-11-10</created><updated>2015-05-13</updated><authors><author><keyname>Stefanoni</keyname><forenames>Giorgio</forenames></author><author><keyname>Motik</keyname><forenames>Boris</forenames></author></authors><title>Answering Conjunctive Queries over $\mathcal{EL}$ Knowledge Bases with
  Transitive and Reflexive Roles</title><categories>cs.AI cs.DB cs.LO</categories><comments>Extended version of a paper to appear on AAAI-15. In this version of
  the report, we fixed a few typos; all the results are unchanged</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Answering conjunctive queries (CQs) over $\mathcal{EL}$ knowledge bases (KBs)
with complex role inclusions is PSPACE-hard and in PSPACE in certain cases;
however, if complex role inclusions are restricted to role transitivity, the
tight upper complexity bound has so far been unknown. Furthermore, the existing
algorithms cannot handle reflexive roles, and they are not practicable.
Finally, the problem is tractable for acyclic CQs and $\mathcal{ELH}$, and
NP-complete for unrestricted CQs and $\mathcal{ELHO}$ KBs. In this paper we
complete the complexity landscape of CQ answering for several important cases.
In particular, we present a practicable NP algorithm for answering CQs over
$\mathcal{ELHO}^s$ KBs---a logic containing all of OWL 2 EL, but with complex
role inclusions restricted to role transitivity. Our preliminary evaluation
suggests that the algorithm can be suitable for practical use. Moreover, we
show that, even for a restricted class of so-called arborescent acyclic
queries, CQ answering over $\mathcal{EL}$ KBs becomes NP-hard in the presence
of either transitive or reflexive roles. Finally, we show that answering
arborescent CQs over $\mathcal{ELHO}$ KBs is tractable, whereas answering
acyclic CQs is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2528</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2528</id><created>2014-11-10</created><authors><author><keyname>Lin</keyname><forenames>Jianbiao</forenames></author><author><keyname>Zhong</keyname><forenames>Yukun</forenames></author><author><keyname>Lin</keyname><forenames>Xiaowei</forenames></author><author><keyname>Lin</keyname><forenames>Hui</forenames></author><author><keyname>Zeng</keyname><forenames>Qiang</forenames></author></authors><title>Hybrid Ant Colony Algorithm Clonal Selection in the Application of the
  Cloud's Resource Scheduling</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, thinking over characteristics of ant colony optimization
Algorithm, taking into account the characteristics of cloud computing, combined
with clonal selection algorithm (CSA) global optimum advantage of the
convergence of the clonal selection algorithm (CSA) into every ACO iteration,
speeding up the convergence rate, and the introduction of reverse mutation
strategy, ant colony optimization algorithm avoids local optimum. Depth study
of the cloud environment ant colony clonal selection algorithm resource
scheduling policy, clonal selection algorithm converges to solve optimization
problems when sufficient condition for global optimal solution based on clonal
selection algorithm for various applications such as BCA and CLONALG algorithm,
using these sufficient condition to meet and simulation platform CloudSim
achieve a simulation by extending the cloud. Experimental results show that
this task can be shortened fusion algorithm running time cloud environment,
improve resource utilization. Demonstrate the effectiveness of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2529</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2529</id><created>2014-11-10</created><updated>2014-11-11</updated><authors><author><keyname>Moghadam</keyname><forenames>Nima Najari</forenames></author><author><keyname>Farhadi</keyname><forenames>Hamed</forenames></author><author><keyname>Zetterberg</keyname><forenames>Per</forenames></author><author><keyname>Khormuji</keyname><forenames>Majid Nasiri</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>Interference Alignment: Practical Challenges and Test-bed Implementation</title><categories>cs.IT math.IT</categories><comments>Book Chapter accepted for publication in the book entitled:
  Contemporary Issues in Wireless Communications, ISBN: 978-953-51-4101-3,
  Khatib, M. (Ed.), to be published by INTECH Publishers. Expected month of
  publication: November 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data traffic over wireless communication networks has experienced a
tremendous growth in the last decade, and it is predicted to exponentially
increase in the next decades. Enabling future wireless networks to fulfill this
expectation is a challenging task both due to the scarcity of radio resources
(e.g. spectrum and energy), and also the inherent characteristics of the
wireless transmission medium. Wireless transmission is in general subject to
two phenomena: fading and interference. The elegant interference alignment
concept reveals that with proper transmission signalling design, different
interference signals can in fact be aligned together, such that more radio
resources can be assigned to the desired transmission. Although interference
alignment can achieve a larger data rate compared to orthogonal transmission
strategies, several challenges should be addressed to enable the deployment of
this technique in future wireless networks For instance, to perform
interference alignment, normally, global channel state information (CSI) is
required to be perfectly known at all terminals. Clearly, acquiring such
channel knowledge is a challenging problem in practice and proper channel
training and channel state feedback techniques need to be deployed. In
addition, since the channels are time-varying proper adaptive transmission is
needed. This chapter review recent advances in practical aspects of
interference alignment. It also presents recent test-bed implementations of
signal processing algorithms for the realization of interference alignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2536</identifier>
 <datestamp>2015-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2536</id><created>2014-11-10</created><updated>2015-04-01</updated><authors><author><keyname>Tan</keyname><forenames>Li</forenames></author><author><keyname>Chen</keyname><forenames>Zizhong</forenames></author></authors><title>Algorithmic Energy Saving for Parallel Cholesky, LU, and QR
  Factorizations</title><categories>cs.DC</categories><comments>35 pages, incorporating two short papers (one to be replaced) with
  some updates</comments><msc-class>68M14, 68M20, 68N25, 68W10, 68W15</msc-class><acm-class>D.3.4; D.4.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The pressing demands of improving energy efficiency for high performance
scientific computing have motivated a large body of software-controlled hard-
ware solutions using Dynamic Voltage and Frequency Scaling (DVFS) that
strategically switch processors to low-power states, when the peak processor
performance is not necessary. Although OS level solutions have demonstrated the
effectiveness of saving energy in a black-box fashion, for applications with
variable execution characteristics, the optimal energy efficiency can be
blundered away due to defective prediction mechanism and untapped load
imbalance. In this paper, we propose TX, a library level race-to-halt DVFS
scheduling approach that analyzes Task Dependency Set of each task in parallel
Cholesky, LU, and QR factorizations to achieve substantial energy savings OS
level solutions cannot fulfill. Partially giving up the generality of OS level
solutions per requiring library level source modification, TX lever- ages
algorithmic characteristics of the applications to gain greater energy savings.
Experimental results on two power-aware clusters indicate that TX can save up
to 17.8% more energy than state-of-the-art OS level solutions with negligible
3.5% on average performance loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2539</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2539</id><created>2014-11-10</created><authors><author><keyname>Kiros</keyname><forenames>Ryan</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author><author><keyname>Zemel</keyname><forenames>Richard S.</forenames></author></authors><title>Unifying Visual-Semantic Embeddings with Multimodal Neural Language
  Models</title><categories>cs.LG cs.CL cs.CV</categories><comments>13 pages. NIPS 2014 deep learning workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by recent advances in multimodal learning and machine translation,
we introduce an encoder-decoder pipeline that learns (a): a multimodal joint
embedding space with images and text and (b): a novel language model for
decoding distributed representations from our space. Our pipeline effectively
unifies joint image-text embedding models with multimodal neural language
models. We introduce the structure-content neural language model that
disentangles the structure of a sentence to its content, conditioned on
representations produced by the encoder. The encoder allows one to rank images
and sentences while the decoder can generate novel descriptions from scratch.
Using LSTM to encode sentences, we match the state-of-the-art performance on
Flickr8K and Flickr30K without using object detections. We also set new best
results when using the 19-layer Oxford convolutional network. Furthermore we
show that with linear encoders, the learned embedding space captures multimodal
regularities in terms of vector space arithmetic e.g. *image of a blue car* -
&quot;blue&quot; + &quot;red&quot; is near images of red cars. Sample captions generated for 800
images are made available for comparison.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2547</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2547</id><created>2014-11-05</created><authors><author><keyname>Kong</keyname><forenames>Yong</forenames></author></authors><title>Statistical distributions of sequencing by synthesis with probabilistic
  nucleotide incorporation</title><categories>q-bio.GN cs.DM math.CO stat.CO</categories><comments>25 pages, 2 figures</comments><msc-class>05A15, 60C05, 92D99</msc-class><journal-ref>Journal of Computational Biology. June 2009, 16(6): 817-827</journal-ref><doi>10.1089/cmb.2008.0215</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequencing by synthesis is used in many next-generation DNA sequencing
technologies. Some of the technologies, especially those exploring the
principle of single-molecule sequencing, allow incomplete nucleotide
incorporation in each cycle. We derive statistical distributions for sequencing
by synthesis by taking into account the possibility that nucleotide
incorporation may not be complete in each flow cycle. The statistical
distributions are expressed in terms of nucleotide probabilities of the target
sequences and the nucleotide incorporation probabilities for each nucleotide.
We give exact distributions both for fixed number of flow cycles and for fixed
sequence length. Explicit formulas are derived for the mean and variance of
these distributions. The results are generalizations of our previous work for
pyrosequencing. Incomplete nucleotide incorporation leads to significant change
in the mean and variance of the distributions, but still they can be
approximated by normal distributions with the same mean and variance. The
results are also generalized to handle sequence context dependent
incorporation. The statistical distributions will be useful for instrument and
software development for sequencing by synthesis platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2548</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2548</id><created>2014-11-05</created><authors><author><keyname>Kong</keyname><forenames>Yong</forenames></author></authors><title>Length distribution of sequencing by synthesis: fixed flow cycle model</title><categories>q-bio.GN cs.DM math.CO stat.CO</categories><comments>27 pages, 5 figures</comments><msc-class>05A15, 60C05, 92D99</msc-class><journal-ref>Journal of mathematical biology 67 (2), 389-410, 2013</journal-ref><doi>10.1007/s00285-012-0556-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequencing by synthesis is the underlying technology for many next-generation
DNA sequencing platforms. We developed a new model, the fixed flow cycle model,
to derive the distributions of sequence length for a given number of flow
cycles under the general conditions where the nucleotide incorporation is
probabilistic and may be incomplete, as in some single-molecule sequencing
technologies. Unlike the previous model, the new model yields the probability
distribution for the sequence length. Explicit closed form formulas are derived
for the mean and variance of the distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2549</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2549</id><created>2014-11-05</created><authors><author><keyname>Kong</keyname><forenames>Yong</forenames></author></authors><title>Distributions of positive signals in pyrosequencing</title><categories>q-bio.GN cs.DM math.CO stat.CO</categories><comments>19 pages, 2 figures</comments><msc-class>05A15 60C05 92D99</msc-class><journal-ref>Journal of mathematical biology 69 (1), 39-54, 2014</journal-ref><doi>10.1007/s00285-013-0691-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pyrosequencing is one of the important next-generation sequencing
technologies. We derive the distribution of the number of positive signals in
pyrograms of this sequencing technology as a function of flow cycle numbers and
nucleotide probabilities of the target sequences. As for the distribution of
sequence length, we also derive the distribution of positive signals for the
fixed flow cycle model. Explicit formulas are derived for the mean and variance
of the distributions. A simple result for the mean of the distribution is that
the mean number of positive signals in a pyrogram is approximately twice the
number of flow cycles, regardless of nucleotide probabilities. The statistical
distributions will be useful for instrument and software development for
pyrosequencing and other related platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2553</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2553</id><created>2014-11-10</created><authors><author><keyname>Morse</keyname><forenames>Jeremy</forenames></author></authors><title>Measuring the impact of input data on energy consumption of software</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The amount of energy consumed during the execution of software, and the
ability to predict future consumption, is an important factor in the design of
embedded electronic systems. In this technical report I examine factors in the
execution of software that can affect energy consumption. Taking a simple
embedded software benchmark I measure to what extent input data can affect
energy consumption, and propose a method for reflecting this in software energy
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2565</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2565</id><created>2014-11-10</created><authors><author><keyname>Zhu</keyname><forenames>Ru</forenames></author></authors><title>Grace: a Cross-platform Micromagnetic Simulator On Graphics Processing
  Units</title><categories>cs.CE physics.comp-ph</categories><comments>9 pages, 5 figures</comments><acm-class>I.6.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A micromagnetic simulator running on graphics processing unit (GPU) is
presented. It achieves significant performance boost as compared to previous
central processing unit (CPU) simulators, up to two orders of magnitude for
large input problems. Different from GPU implementations of other research
groups, this simulator is developed with C++ Accelerated Massive Parallelism
(C++ AMP) and is hardware platform compatible. It runs on GPU from venders
include NVidia, AMD and Intel, which paved the way for fast micromagnetic
simulation on both high-end workstations with dedicated graphics cards and
low-end personal computers with integrated graphics card. A copy of the
simulator software is publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2577</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2577</id><created>2014-11-10</created><updated>2015-04-20</updated><authors><author><keyname>Andoni</keyname><forenames>Alexandr</forenames></author><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author><author><keyname>Razenshteyn</keyname><forenames>Ilya</forenames></author></authors><title>Sketching and Embedding are Equivalent for Norms</title><categories>cs.DS cs.CC math.FA</categories><comments>28 pages, an extended abstract is to appear in the proceedings of the
  47th ACM Symposium on Theory of Computing (STOC 2015); changes in v2: added
  quantitative bounds for the main results, preliminaries section with
  necessary definitions and facts has been added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An outstanding open question (Question #5, http://sublinear.info) asks to
characterize metric spaces in which distances can be estimated using efficient
sketches. Specifically, we say that a sketching algorithm is efficient if it
achieves constant approximation using constant sketch size. A well-known result
of Indyk (J. ACM, 2006) implies that a metric that admits a constant-distortion
embedding into $\ell_p$ for $p\in(0,2]$ also admits an efficient sketching
scheme. But is the converse true, i.e., is embedding into $\ell_p$ the only way
to achieve efficient sketching?
  We address these questions for the important special case of normed spaces,
by providing an almost complete characterization of sketching in terms of
embeddings. In particular, we prove that a finite-dimensional normed space
allows efficient sketches if and only if it embeds (linearly) into
$\ell_{1-\varepsilon}$ with constant distortion. We further prove that for
norms that are closed under sum-product, efficient sketching is equivalent to
embedding into $\ell_1$ with constant distortion. Examples of such norms
include the Earth Mover's Distance (specifically its norm variant, called
Kantorovich-Rubinstein norm), and the trace norm (a.k.a. Schatten $1$-norm or
the nuclear norm). Using known non-embeddability theorems for these norms by
Naor and Schechtman (SICOMP, 2007) and by Pisier (Compositio. Math., 1978), we
then conclude that these spaces do not admit efficient sketches either, making
progress towards answering another open question (Question #7,
http://sublinear.info).
  Finally, we observe that resolving whether &quot;sketching is equivalent to
embedding into $\ell_1$ for general norms&quot; (i.e., without the above
restriction) is equivalent to resolving a well-known open problem in Functional
Analysis posed by Kwapien in 1969.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2581</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2581</id><created>2014-11-10</created><authors><author><keyname>Ranganath</keyname><forenames>Rajesh</forenames></author><author><keyname>Tang</keyname><forenames>Linpeng</forenames></author><author><keyname>Charlin</keyname><forenames>Laurent</forenames></author><author><keyname>Blei</keyname><forenames>David M.</forenames></author></authors><title>Deep Exponential Families</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe \textit{deep exponential families} (DEFs), a class of latent
variable models that are inspired by the hidden structures used in deep neural
networks. DEFs capture a hierarchy of dependencies between latent variables,
and are easily generalized to many settings through exponential families. We
perform inference using recent &quot;black box&quot; variational inference techniques. We
then evaluate various DEFs on text and combine multiple DEFs into a model for
pairwise recommendation data. In an extensive study, we show that going beyond
one layer improves predictions for DEFs. We demonstrate that DEFs find
interesting exploratory structure in large data sets, and give better
predictive performance than state-of-the-art models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2584</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2584</id><created>2014-11-09</created><authors><author><keyname>Costarelli</keyname><forenames>Danilo</forenames></author><author><keyname>Cluni</keyname><forenames>Federico</forenames></author><author><keyname>Minotti</keyname><forenames>Anna Maria</forenames></author><author><keyname>Vinti</keyname><forenames>Gianluca</forenames></author></authors><title>Applications of sampling Kantorovich operators to thermographic images
  for seismic engineering</title><categories>cs.CV math.NA</categories><comments>16 pages, 5 figures, 2 tables</comments><msc-class>41A35, 46E30, 47A58, 47B38, 94A12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present some applications of the multivariate sampling
Kantorovich operators $S_w$ to seismic engineering. The mathematical theory of
these operators, both in the space of continuous functions and in Orlicz
spaces, show how it is possible to approximate/reconstruct multivariate
signals, such as images. In particular, to obtain applications for
thermographic images a mathematical algorithm is developed using MATLAB and
matrix calculus. The setting of Orlicz spaces is important since allow us to
reconstruct not necessarily continuous signals by means of $S_w$. The
reconstruction of thermographic images of buildings by our sampling Kantorovich
algorithm allow us to obtain models for the simulation of the behavior of
structures under seismic action. We analyze a real world case study in term of
structural analysis and we compare the behavior of the building under seismic
action using various models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2635</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2635</id><created>2014-11-10</created><authors><author><keyname>Maurer</keyname><forenames>Andreas</forenames></author></authors><title>A chain rule for the expected suprema of Gaussian processes</title><categories>cs.LG</categories><journal-ref>Lecture Notes in Computer Science Volume 8776, 2014, pp 245-259</journal-ref><doi>10.1007/978-3-319-11662-4_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The expected supremum of a Gaussian process indexed by the image of an index
set under a function class is bounded in terms of separate properties of the
index set and the function class. The bound is relevant to the estimation of
nonlinear transformations or the analysis of learning algorithms whenever
hypotheses are chosen from composite classes, as is the case for multi-layer
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2636</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2636</id><created>2014-11-10</created><authors><author><keyname>Dawid</keyname><forenames>A. P.</forenames></author><author><keyname>Murtas</keyname><forenames>R.</forenames></author><author><keyname>Musio</keyname><forenames>M.</forenames></author></authors><title>Bounding the Probability of Causation in Mediation Analysis</title><categories>math.ST cs.AI stat.ME stat.TH</categories><comments>9 pages, 1 figure, 3 tables</comments><msc-class>62A99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given empirical evidence for the dependence of an outcome variable on an
exposure variable, we can typically only provide bounds for the &quot;probability of
causation&quot; in the case of an individual who has developed the outcome after
being exposed. We show how these bounds can be adapted or improved if further
information becomes available. In addition to reviewing existing work on this
topic, we provide a new analysis for the case where a mediating variable can be
observed. In particular we show how the probability of causation can be bounded
when there is no direct effect and no confounding.
  Keywords: Causal inference, Mediation Analysis, Probability of Causation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2645</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2645</id><created>2014-11-10</created><authors><author><keyname>Ferrer-i-Cancho</keyname><forenames>Ramon</forenames></author></authors><title>Non-crossing dependencies: least effort, not grammar</title><categories>cs.CL cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of null hypotheses (in a statistical sense) is common in hard
sciences but not in theoretical linguistics. Here the null hypothesis that the
low frequency of syntactic dependency crossings is expected by an arbitrary
ordering of words is rejected. It is shown that this would require star
dependency structures, which are both unrealistic and too restrictive. The
hypothesis of the limited resources of the human brain is revisited. Stronger
null hypotheses taking into account actual dependency lengths for the
likelihood of crossings are presented. Those hypotheses suggests that crossings
are likely to reduce when dependencies are shortened. A hypothesis based on
pressure to reduce dependency lengths is more parsimonious than a principle of
minimization of crossings or a grammatical ban that is totally dissociated from
the general and non-linguistic principle of economy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2647</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2647</id><created>2014-11-10</created><updated>2015-12-10</updated><authors><author><keyname>Lee</keyname><forenames>Christina E.</forenames></author><author><keyname>Ozdaglar</keyname><forenames>Asuman</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author></authors><title>Asynchronous Approximation of a Single Component of the Solution to a
  Linear System</title><categories>cs.DS</categories><report-no>MIT LIDS Report 3172</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a distributed asynchronous algorithm for approximating a single
component of the solution to a system of linear equations $Ax = b$, where $A$
is a positive definite real matrix, and $b \in \mathbb{R}^n$. This is
equivalent to solving for $x_i$ in $x = Gx + z$ for some $G$ and $z$ such that
the spectral radius of $G$ is less than 1. Our algorithm relies on the Neumann
series characterization of the component $x_i$, and is based on residual
updates. We analyze our algorithm within the context of a cloud computation
model, in which the computation is split into small update tasks performed by
small processors with shared access to a distributed file system. We prove a
robust asymptotic convergence result when $\rho(\tilde{G}) &lt; 1$, regardless of
the precise order and frequency in which the update tasks are performed, where
$\tilde{G}_{ij} = |G_{ij}|$. We provide convergence rate bounds which depend on
the order of update tasks performed, analyzing both deterministic update rules
via counting weighted random walks, as well as probabilistic update rules via
concentration bounds. The probabilistic analysis requires analyzing the product
of random matrices which are drawn from distributions that are time and path
dependent. We specifically consider the setting where $n$ is large, yet $G$ is
sparse, e.g., each row has at most $d$ nonzero entries. This is motivated by
applications in which $G$ is derived from the edge structure of an underlying
graph. Our results prove that if the local neighborhood of the graph does not
grow too quickly as a function of $n$, our algorithm can provide significant
reduction in computation cost as opposed to any algorithm which computes the
global solution vector $x$. Our algorithm obtains an $\epsilon \|x\|_2$
additive approximation for $x_i$ in constant time with respect to the size of
the matrix when $d = O(1)$ and $1/(1-\|G\|_2) = O(1)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2649</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2649</id><created>2014-11-10</created><updated>2015-02-27</updated><authors><author><keyname>Richter</keyname><forenames>Philipp</forenames></author><author><keyname>Allman</keyname><forenames>Mark</forenames></author><author><keyname>Bush</keyname><forenames>Randy</forenames></author><author><keyname>Paxson</keyname><forenames>Vern</forenames></author></authors><title>A Primer on IPv4 Scarcity</title><categories>cs.NI</categories><comments>ACM CCR 45(2), 11 pages, editorial contribution</comments><acm-class>C.2.3; C.2.2</acm-class><journal-ref>ACM Computer Communication Review 45(2), April 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the ongoing exhaustion of free address pools at the registries serving
the global demand for IPv4 address space, scarcity has become reality. Networks
in need of address space can no longer get more address allocations from their
respective registries.
  In this work we frame the fundamentals of the IPv4 address exhaustion
phenomena and connected issues. We elaborate on how the current ecosystem of
IPv4 address space has evolved since the standardization of IPv4, leading to
the rather complex and opaque scenario we face today. We outline the evolution
in address space management as well as address space use patterns, identifying
key factors of the scarcity issues. We characterize the possible solution space
to overcome these issues and open the perspective of address blocks as virtual
resources, which involves issues such as differentiation between address
blocks, the need for resource certification, and issues arising when
transferring address space between networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2664</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2664</id><created>2014-11-10</created><updated>2016-03-02</updated><authors><author><keyname>Dwork</keyname><forenames>Cynthia</forenames></author><author><keyname>Feldman</keyname><forenames>Vitaly</forenames></author><author><keyname>Hardt</keyname><forenames>Moritz</forenames></author><author><keyname>Pitassi</keyname><forenames>Toniann</forenames></author><author><keyname>Reingold</keyname><forenames>Omer</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author></authors><title>Preserving Statistical Validity in Adaptive Data Analysis</title><categories>cs.LG cs.DS</categories><comments>Updated related work with recent developments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A great deal of effort has been devoted to reducing the risk of spurious
scientific discoveries, from the use of sophisticated validation techniques, to
deep statistical methods for controlling the false discovery rate in multiple
hypothesis testing. However, there is a fundamental disconnect between the
theoretical results and the practice of data analysis: the theory of
statistical inference assumes a fixed collection of hypotheses to be tested, or
learning algorithms to be applied, selected non-adaptively before the data are
gathered, whereas in practice data is shared and reused with hypotheses and new
analyses being generated on the basis of data exploration and the outcomes of
previous analyses.
  In this work we initiate a principled study of how to guarantee the validity
of statistical inference in adaptive data analysis. As an instance of this
problem, we propose and investigate the question of estimating the expectations
of $m$ adaptively chosen functions on an unknown distribution given $n$ random
samples.
  We show that, surprisingly, there is a way to estimate an exponential in $n$
number of expectations accurately even if the functions are chosen adaptively.
This gives an exponential improvement over standard empirical estimators that
are limited to a linear number of estimates. Our result follows from a general
technique that counter-intuitively involves actively perturbing and
coordinating the estimates, using techniques developed for privacy
preservation. We give additional applications of this technique to our
question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2668</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2668</id><created>2014-11-05</created><authors><author><keyname>Kong</keyname><forenames>Yong</forenames></author></authors><title>Statistical distributions of pyrosequencing</title><categories>q-bio.GN cs.DM math.CO stat.CO</categories><comments>26 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1411.2547, arXiv:1411.2549</comments><msc-class>05A15, 60C05, 92D99</msc-class><journal-ref>Journal of Computational Biology 16 (1), 31-42, 2009</journal-ref><doi>10.1089/cmb.2008.0106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pyrosequencing is emerging as one of the important next-generation sequencing
technologies. We derive the statistical distributions of this technique in
terms of nucleotide probabilities of the target sequences. We give exact
distributions both for fixed number of flow cycles and for fixed sequence
length. Explicit formulas are derived for the mean and variance of these
distributions. In both cases, the distributions can be approximated accurately
by normal distributions with the same mean and variance. The statistical
distributions will be useful for instrument and software development for
pyrosequencing platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2669</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2669</id><created>2014-11-10</created><authors><author><keyname>Ghosal</keyname><forenames>Pantha</forenames></author><author><keyname>Barua</keyname><forenames>Shouman</forenames></author><author><keyname>Subramanian</keyname><forenames>Ramprasad</forenames></author><author><keyname>Xing</keyname><forenames>Shiqi</forenames></author><author><keyname>Sandrasegaran</keyname><forenames>Kumbesan</forenames></author></authors><title>A novel approach for mobility management inf lte femtocells</title><categories>cs.NI</categories><comments>13 Pages, 16 Figures, Online Journal(IJWMN:
  http://airccse.org/journal/jwmn/6514ijwmn04.pdf)</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks(IJWMN), vol.6,
  no.5, pp. 45-58, October 2014. Available
  at:http://airccse.org/journal/jwmn/6514ijwmn04.pdf</journal-ref><doi>10.5121/ijwmn.2014.6504</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LTE is an emerging wireless data communication technology to provide
broadband ubiquitous Internet access. Femtocells are included in 3GPP since
Release 8 to enhance the indoor network coverage and capacity. The main
challenge of mobility management in hierarchical LTE structure is to guarantee
efficient handover to or from/to/between Femtocells. This paper focuses, on
different types of Handover and comparison performance between different
decision algorithms. Furthermore, a speed based Handover algorithm for
macro-femto scenario is proposed with simulation results
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2671</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2671</id><created>2014-11-10</created><authors><author><keyname>Anwar</keyname><forenames>A.</forenames></author><author><keyname>Mahmood</keyname><forenames>A. N.</forenames></author></authors><title>Vulnerabilities of Smart Grid State Estimation against False Data
  Injection Attack</title><categories>cs.CR cs.SY</categories><comments>Renewable Energy Integration, Green Energy and Technology, Springer,
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, Information Security has become a notable issue in the
energy sector. After the invention of The Stuxnet worm in 2010, data integrity,
privacy and confidentiality has received significant importance in the
real-time operation of the control centres. New methods and frameworks are
being developed to protect the National Critical Infrastructures like energy
sector. In the recent literatures, it has been shown that the key real-time
operational tools (e.g., State Estimator) of any Energy Management System (EMS)
are vulnerable to Cyber Attacks. In this chapter, one such cyber attack named
False Data Injection Attack is discussed. A literature review with a case study
is considered to explain the characteristics and significance of such data
integrity attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2674</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2674</id><created>2014-11-10</created><updated>2015-01-27</updated><authors><author><keyname>Guo</keyname><forenames>Fangjian</forenames></author><author><keyname>Blundell</keyname><forenames>Charles</forenames></author><author><keyname>Wallach</keyname><forenames>Hanna</forenames></author><author><keyname>Heller</keyname><forenames>Katherine</forenames></author></authors><title>The Bayesian Echo Chamber: Modeling Social Influence via Linguistic
  Accommodation</title><categories>stat.ML cs.CL cs.LG cs.SI</categories><comments>14 pages, 7 figures, to appear in AISTATS 2015. Fixed minor
  formatting issues</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the Bayesian Echo Chamber, a new Bayesian generative model for
social interaction data. By modeling the evolution of people's language usage
over time, this model discovers latent influence relationships between them.
Unlike previous work on inferring influence, which has primarily focused on
simple temporal dynamics evidenced via turn-taking behavior, our model captures
more nuanced influence relationships, evidenced via linguistic accommodation
patterns in interaction content. The model, which is based on a discrete analog
of the multivariate Hawkes process, permits a fully Bayesian inference
algorithm. We validate our model's ability to discover latent influence
patterns using transcripts of arguments heard by the US Supreme Court and the
movie &quot;12 Angry Men.&quot; We showcase our model's capabilities by using it to infer
latent influence patterns from Federal Open Market Committee meeting
transcripts, demonstrating state-of-the-art performance at uncovering social
dynamics in group discussions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2679</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2679</id><created>2014-11-10</created><authors><author><keyname>Li</keyname><forenames>Jiwei</forenames></author><author><keyname>Ritter</keyname><forenames>Alan</forenames></author><author><keyname>Jurafsky</keyname><forenames>Dan</forenames></author></authors><title>Inferring User Preferences by Probabilistic Logical Reasoning over
  Social Networks</title><categories>cs.SI cs.AI cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a framework for inferring the latent attitudes or preferences of
users by performing probabilistic first-order logical reasoning over the social
network graph. Our method answers questions about Twitter users like {\em Does
this user like sushi?} or {\em Is this user a New York Knicks fan?} by building
a probabilistic model that reasons over user attributes (the user's location or
gender) and the social network (the user's friends and spouse), via inferences
like homophily (I am more likely to like sushi if spouse or friends like sushi,
I am more likely to like the Knicks if I live in New York). The algorithm uses
distant supervision, semi-supervised data harvesting and vector space models to
extract user attributes (e.g. spouse, education, location) and preferences
(likes and dislikes) from text. The extracted propositions are then fed into a
probabilistic reasoner (we investigate both Markov Logic and Probabilistic Soft
Logic). Our experiments show that probabilistic logical reasoning significantly
improves the performance on attribute and relation extraction, and also
achieves an F-score of 0.791 at predicting a users likes or dislikes,
significantly better than two strong baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2680</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2680</id><created>2014-11-10</created><authors><author><keyname>Akiba</keyname><forenames>Takuya</forenames></author><author><keyname>Iwata</keyname><forenames>Yoichi</forenames></author></authors><title>Branch-and-Reduce Exponential/FPT Algorithms in Practice: A Case Study
  of Vertex Cover</title><categories>cs.DS</categories><comments>To appear in ALENEX 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the gap between theory and practice for exact branching
algorithms. In theory, branch-and-reduce algorithms currently have the best
time complexity for numerous important problems. On the other hand, in
practice, state-of-the-art methods are based on different approaches, and the
empirical efficiency of such theoretical algorithms have seldom been
investigated probably because they are seemingly inefficient because of the
plethora of complex reduction rules. In this paper, we design a
branch-and-reduce algorithm for the vertex cover problem using the techniques
developed for theoretical algorithms and compare its practical performance with
other state-of-the-art empirical methods. The results indicate that
branch-and-reduce algorithms are actually quite practical and competitive with
other state-of-the-art approaches for several kinds of instances, thus showing
the practical impact of theoretical research on branching algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2683</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2683</id><created>2014-11-10</created><authors><author><keyname>Mesbah</keyname><forenames>Ali</forenames></author><author><keyname>Streif</keyname><forenames>Stefan</forenames></author></authors><title>A Probabilistic Approach to Robust Optimal Experiment Design with Chance
  Constraints</title><categories>cs.SY math.OC</categories><comments>Submitted to ADCHEM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate estimation of parameters is paramount in developing high-fidelity
models for complex dynamical systems. Model-based optimal experiment design
(OED) approaches enable systematic design of dynamic experiments to generate
input-output data sets with high information content for parameter estimation.
Standard OED approaches however face two challenges: (i) experiment design
under incomplete system information due to unknown true parameters, which
usually requires many iterations of OED; (ii) incapability of systematically
accounting for the inherent uncertainties of complex systems, which can lead to
diminished effectiveness of the designed optimal excitation signal as well as
violation of system constraints. This paper presents a robust OED approach for
nonlinear systems with arbitrarily-shaped time-invariant probabilistic
uncertainties. Polynomial chaos is used for efficient uncertainty propagation.
The distinct feature of the robust OED approach is the inclusion of chance
constraints to ensure constraint satisfaction in a stochastic setting. The
presented approach is demonstrated by optimal experimental design for the
JAK-STAT5 signaling pathway that regulates various cellular processes in a
biological cell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2684</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2684</id><created>2014-11-10</created><updated>2014-11-12</updated><authors><author><keyname>Penunuri</keyname><forenames>F.</forenames></author><author><keyname>Carvente</keyname><forenames>O.</forenames></author><author><keyname>Zambrano-Arjona</keyname><forenames>M. A.</forenames></author><author><keyname>Cruz-Villar</keyname><forenames>Carlos A.</forenames></author></authors><title>Dual Algorithms</title><categories>cs.CE</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cubic spline interpolation method, the Runge--Kutta method, and the
Newton-Raphson method are extended to dual versions (developed in the context
of dual numbers). This extension allows the calculation of the derivatives of
complicated compositions of functions which are not necessarily defined by a
closed form expression. The code for the algorithms has been written in Fortran
and some examples are presented. Among them, we use the dual Newton--Raphson
method to obtain the derivatives of the output angle in the RRRCR spatial
mechanism; we use the dual normal cubic spline interpolation algorithm to
obtain the thermal diffusivity using photothermal techniques; and we use the
dual Runge--Kutta method to obtain the derivatives of functions depending on
the solution of the Duffing equation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2689</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2689</id><created>2014-11-10</created><updated>2015-12-10</updated><authors><author><keyname>Raichel</keyname><forenames>Benjamin</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author></authors><title>Avoiding the Global Sort: A Faster Contour Tree Algorithm</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the classical problem of computing the \emph{contour tree} of a
scalar field $f:\mathbb{M} \to \mathbb{R}$, where $\mathbb{M}$ is a
triangulated simplicial mesh in $\mathbb{R}^d$. The contour tree is a
fundamental topological structure that tracks the evolution of level sets of
$f$ and has numerous applications in data analysis and visualization.
  All existing algorithms begin with a global sort of at least all critical
values of $f$, which can require (roughly) $\Omega(n\log n)$ time. Existing
lower bounds show that there are pathological instances where this sort is
required. We present the first algorithm whose time complexity depends on the
contour tree structure, and avoids the global sort for non-pathological inputs.
If $C$ denotes the set of critical points in $\mathbb{M}$, the running time is
roughly $O(\sum_{v \in C} \log \ell_v)$, where $\ell_v$ is the depth of $v$ in
the contour tree. This matches all existing upper bounds, but is a significant
improvement when the contour tree is short and fat. Specifically, our approach
ensures that any comparison made is between nodes in the same descending path
in the contour tree, allowing us to argue strong optimality properties of our
algorithm.
  Our algorithm requires several novel ideas: partitioning $\mathbb{M}$ in
well-behaved portions, a local growing procedure to iteratively build contour
trees, and the use of heavy path decompositions for the time complexity
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2699</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2699</id><created>2014-11-10</created><updated>2015-10-27</updated><authors><author><keyname>Kamenetsky</keyname><forenames>Dmitry</forenames></author><author><keyname>Cooke</keyname><forenames>Tristrom</forenames></author></authors><title>Tiling rectangles with holey polyominoes</title><categories>cs.CG</categories><comments>24 pages. Updated references and added Tom Sirgedas' proof</comments><msc-class>05-02</msc-class><acm-class>G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new type of polyominoes that can have transparent squares
(holes). We show how these polyominoes can tile rectangles and we categorise
them according to their tiling ability. We were able to categorise all but 6
polyominoes with 5 or fewer visible squares.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2702</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2702</id><created>2014-11-11</created><updated>2015-08-30</updated><authors><author><keyname>Mane</keyname><forenames>Pramod</forenames></author><author><keyname>Sarma</keyname><forenames>Monalisa</forenames></author><author><keyname>Samanta</keyname><forenames>Debasis</forenames></author><author><keyname>Ahuja</keyname><forenames>Kapil</forenames></author></authors><title>Social Cloud: Concept, Current Trends and Future Scope</title><categories>cs.CY cs.SI</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, various kinds of distributed resource sharing setups have
been proposed by taking social relationships into consideration. These
dissimilar resource sharing setups are tagged as Social Cloud. These setups
have appeared in various distributed computing forms such as community cloud,
grid, volunteer computing and network services. Such setups are discrete in
nature, and hence, do not conceptualize the totality of the Social Cloud
concept. In fact, it is difficult to conceptualize Social Cloud without a
general framework. There are three main objectives of this work. First, to
present a general framework of Social Cloud. Second, to report various Social
Cloud setups with corresponding architectural prototypes and current trends.
Third, to discuss research challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2714</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2714</id><created>2014-11-11</created><authors><author><keyname>Tan</keyname><forenames>Peng Hui</forenames></author><author><keyname>Joung</keyname><forenames>Jingon</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author></authors><title>Opportunistic Multicast Scheduling for Unicast Transmission in MIMO-OFDM
  System</title><categories>cs.NI</categories><comments>6 pages, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a multicast scheduling scheme to exploit content reuse when there
is asynchronicity in user requests. A unicast transmission setup is used for
content delivery, while multicast transmission is employed opportunistically to
reduce wireless resource usage. We then develop a multicast scheduling scheme
for the downlink multiple-input multiple output orthogonal-frequency division
multiplexing system in IEEE 802.11 wireless local area network (WLAN). At each
time slot, the scheduler serves the users by either unicast or multicast
transmission. Out-sequence data received by a user is stored in user's cache
for future use.Multicast precoding and user selection for multicast grouping
are also considered and compliance with the IEEE 802.11 WLAN transmission
protocol. The scheduling scheme is based on the Lyapunov optimization
technique, which aims to maximize system rate. The resulting scheme has low
complexity and requires no prior statistical information on the channels and
queues. Furthermore, in the absence of channel error, the proposed scheme
restricts the worst case of frame dropping deadline, which is useful for
delivering real-time traffic. Simulation results show that our proposed
algorithm outperforms existing techniques by 17 % to 35 % in term of user
capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2718</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2718</id><created>2014-11-11</created><updated>2014-11-17</updated><authors><author><keyname>Boucher</keyname><forenames>Christina</forenames></author><author><keyname>Bowe</keyname><forenames>Alex</forenames></author><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author><author><keyname>Sadakane</keyname><forenames>Kunihiko</forenames></author></authors><title>Variable-Order de Bruijn Graphs</title><categories>cs.DS q-bio.GN q-bio.QM</categories><comments>Conference submission, 10 pages, +minor corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The de Bruijn graph $G_K$ of a set of strings $S$ is a key data structure in
genome assembly that represents overlaps between all the $K$-length substrings
of $S$. Construction and navigation of the graph is a space and time bottleneck
in practice and the main hurdle for assembling large, eukaryote genomes. This
problem is compounded by the fact that state-of-the-art assemblers do not build
the de Bruijn graph for a single order (value of $K$) but for multiple values
of $K$. More precisely, they build $d$ de Bruijn graphs, each with a specific
order, i.e., $G_{K_1}, G_{K_2}, ..., G_{K_d}$. Although, this paradigm
increases the quality of the assembly produced, it increases the memory by a
factor of $d$ in most cases. In this paper, we show how to augment a succinct
de Bruijn graph representation by Bowe et al. (Proc. WABI, 2012) to support new
operations that let us change order on the fly, effectively representing all de
Bruijn graphs of order up to some maximum $K$ in a single data structure. Our
experiments show our variable-order de Bruijn graph only modestly increases
space usage, construction time, and navigation time compared to a single order
graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2720</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2720</id><created>2014-11-11</created><authors><author><keyname>Ben-Aryeh</keyname><forenames>Yacob</forenames></author></authors><title>Quantum and classical correlations in Bell three and four qubits,
  related to Hilbert-Schmidt decomposition</title><categories>quant-ph cs.IT math.IT</categories><comments>16 pages. arXiv admin note: substantial text overlap with
  arXiv:1403.2524</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present work studies quantum and classical correlations in three qubits
and four qubits general Bell states, produced by operating with braid operators
on the computational basis of states. The analogies between the general three
qubits and four qubits Bell states and that of two qubits Bell states are
discussed. The general Bell states are shown to be maximal entangled, i.e.,
with quantum correlations which are lost by tracing these states over one
qubit, remaining only with classical correlations, as shown by HS
decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2721</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2721</id><created>2014-11-11</created><updated>2015-08-31</updated><authors><author><keyname>Xiang</keyname><forenames>Ju</forenames></author><author><keyname>Tang</keyname><forenames>Yan-Ni</forenames></author><author><keyname>Gao</keyname><forenames>Yuan-Yuan</forenames></author><author><keyname>Zhang</keyname><forenames>Yan</forenames></author><author><keyname>Deng</keyname><forenames>Ke</forenames></author><author><keyname>Xu</keyname><forenames>Xiao-Ke</forenames></author><author><keyname>Hu</keyname><forenames>Ke</forenames></author></authors><title>Multi-resolution community detection based on generalized self-loop
  rescaling strategy</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>11 pages, 6 figures</comments><journal-ref>Physica A: Statistical Mechanics and its Applications, Volume 432,
  15 August 2015, Pages 127-139</journal-ref><doi>10.1016/j.physa.2015.03.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection is of considerable importance for analyzing the structure
and function of complex networks. Many real-world networks may possess
community structures at multiple scales, and recently, various multi-resolution
methods were proposed to identify the community structures at different scales.
In this paper, we present a type of multi-resolution methods by using the
generalized self-loop rescaling strategy. The self-loop rescaling strategy
provides one uniform ansatz for the design of multi-resolution community
detection methods. Many quality functions for community detection can be
unified in the framework of the self-loop rescaling. The resulting
multi-resolution quality functions can be optimized directly using the existing
modularity-optimization algorithms. Several derived multi-resolution methods
are applied to the analysis of community structures in several synthetic and
real-world networks. The results show that these methods can find the
pre-defined substructures in synthetic networks and real splits observed in
real-world networks. Finally, we give a discussion on the methods themselves
and their relationship. We hope that the study in the paper can be helpful for
the understanding of the multi-resolution methods and provide useful insight
into designing new community detection methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2724</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2724</id><created>2014-11-11</created><authors><author><keyname>Zhou</keyname><forenames>Zheng</forenames></author><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Zhao</keyname><forenames>Zhongyuan</forenames></author><author><keyname>Li</keyname><forenames>Yong</forenames></author></authors><title>Joint Power Splitting and Antenna Selection in Energy Harvesting Relay
  Channels</title><categories>cs.IT math.IT</categories><comments>11 pages, 3 figures, submitted to IEEE SPL</comments><doi>10.1109/LSP.2014.2369748</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The simultaneous wireless transfer of information and power with the help of
a relay equipped with multiple antennas is considered in this letter, where a
harvest-and-forward strategy is proposed. In particular, the relay harvests
energy and obtains information from the source with the radio-frequent signals
by jointly using the antenna selection (AS) and power splitting (PS)
techniques, and then the processed information is amplified and forwarded to
the destination relying on the harvested energy. This letter jointly optimizes
AS and PS to maximize the achievable rate for the proposed strategy.
Considering the joint optimization is according to the non-convex problem, a
two-stage procedure is proposed to determine the optimal ratio of received
signal power split for energy harvesting, and the optimized antenna set engaged
in information forwarding. Simulation results confirm the accuracy of the
two-stage procedure, and demonstrate that the proposed harvest-and-forward
strategy outperforms the conventional amplify-and-forward (AF) relaying and the
direct transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2738</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2738</id><created>2014-11-11</created><updated>2016-01-30</updated><authors><author><keyname>Rong</keyname><forenames>Xin</forenames></author></authors><title>word2vec Parameter Learning Explained</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The word2vec model and application by Mikolov et al. have attracted a great
amount of attention in recent two years. The vector representations of words
learned by word2vec models have been shown to carry semantic meanings and are
useful in various NLP tasks. As an increasing number of researchers would like
to experiment with word2vec or similar techniques, I notice that there lacks a
material that comprehensively explains the parameter learning process of word
embedding models in details, thus preventing researchers that are non-experts
in neural networks from understanding the working mechanism of such models.
  This note provides detailed derivations and explanations of the parameter
update equations of the word2vec models, including the original continuous
bag-of-word (CBOW) and skip-gram (SG) models, as well as advanced optimization
techniques, including hierarchical softmax and negative sampling. Intuitive
interpretations of the gradient equations are also provided alongside
mathematical derivations.
  In the appendix, a review on the basics of neuron networks and
backpropagation is provided. I also created an interactive demo, wevi, to
facilitate the intuitive understanding of the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2744</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2744</id><created>2014-11-11</created><updated>2015-04-20</updated><authors><author><keyname>Koldovsky</keyname><forenames>Zbynek</forenames></author><author><keyname>Malek</keyname><forenames>Jiri</forenames></author><author><keyname>Gannot</keyname><forenames>Sharon</forenames></author></authors><title>Spatial Source Subtraction Based on Incomplete Measurements of Relative
  Transfer Function</title><categories>cs.SD</categories><comments>IEEE Trans. on Speech, Audio and Language Processing, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relative impulse responses between microphones are usually long and dense due
to the reverberant acoustic environment. Estimating them from short and noisy
recordings poses a long-standing challenge of audio signal processing. In this
paper we apply a novel strategy based on ideas of Compressed Sensing. Relative
transfer function (RTF) corresponding to the relative impulse response can
often be estimated accurately from noisy data but only for certain frequencies.
This means that often only an incomplete measurement of the RTF is available. A
complete RTF estimate can be obtained through finding its sparsest
representation in the time-domain: that is, through computing the sparsest
among the corresponding relative impulse responses. Based on this approach, we
propose to estimate the RTF from noisy data in three steps. First, the RTF is
estimated using any conventional method such as the non-stationarity-based
estimator by Gannot et al. or through Blind Source Separation. Second,
frequencies are determined for which the RTF estimate appears to be accurate.
Third, the RTF is reconstructed through solving a weighted $\ell_1$ convex
program, which we propose to solve via a computationally efficient variant of
the SpaRSA (Sparse Reconstruction by Separable Approximation) algorithm. An
extensive experimental study with real-world recordings has been conducted. It
has been shown that the proposed method is capable of improving many
conventional estimators used as the first step in most situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2746</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2746</id><created>2014-11-11</created><authors><author><keyname>Jakovetic</keyname><forenames>Dusan</forenames></author><author><keyname>Minja</keyname><forenames>Aleksandar</forenames></author><author><keyname>Bajovic</keyname><forenames>Dragana</forenames></author><author><keyname>Vukobratovic</keyname><forenames>Dejan</forenames></author></authors><title>Distributed Storage Allocations for Neighborhood-based Data Access</title><categories>cs.IT math.IT</categories><comments>submitted to a conference on Oct 31, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a neighborhood-based data access model for distributed coded
storage allocation. Storage nodes are connected in a generic network and data
is accessed locally: a user accesses a randomly chosen storage node, which
subsequently queries its neighborhood to recover the data object. We aim at
finding an optimal allocation that minimizes the overall storage budget while
ensuring recovery with probability one. We show that the problem reduces to
finding the fractional dominating set of the underlying network. Furthermore,
we develop a fully distributed algorithm where each storage node communicates
only with its neighborhood in order to find its optimal storage allocation. The
proposed algorithm is based upon the recently proposed proximal center
method--an efficient dual decomposition based on accelerated dual gradient
method. We show that our algorithm achieves a $(1+\epsilon)$-approximation
ratio in $O(d_{\mathrm{max}}^{3/2}/\epsilon)$ iterations and per-node
communications, where $d_{\mathrm{max}}$ is the maximal degree across nodes.
Simulations demonstrate the effectiveness of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2749</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2749</id><created>2014-11-11</created><updated>2015-07-22</updated><authors><author><keyname>Kuhn</keyname><forenames>Tobias</forenames></author><author><keyname>Chichester</keyname><forenames>Christine</forenames></author><author><keyname>Krauthammer</keyname><forenames>Michael</forenames></author><author><keyname>Dumontier</keyname><forenames>Michel</forenames></author></authors><title>Publishing without Publishers: a Decentralized Approach to
  Dissemination, Retrieval, and Archiving of Data</title><categories>cs.DL</categories><comments>In Proceedings of the 14th International Semantic Web Conference
  (ISWC) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Making available and archiving scientific results is for the most part still
considered the task of classical publishing companies, despite the fact that
classical forms of publishing centered around printed narrative articles no
longer seem well-suited in the digital age. In particular, there exist
currently no efficient, reliable, and agreed-upon methods for publishing
scientific datasets, which have become increasingly important for science. Here
we propose to design scientific data publishing as a Web-based bottom-up
process, without top-down control of central authorities such as publishing
companies. Based on a novel combination of existing concepts and technologies,
we present a server network to decentrally store and archive data in the form
of nanopublications, an RDF-based format to represent scientific data. We show
how this approach allows researchers to publish, retrieve, verify, and
recombine datasets of nanopublications in a reliable and trustworthy manner,
and we argue that this architecture could be used for the Semantic Web in
general. Evaluation of the current small network shows that this system is
efficient and reliable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2785</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2785</id><created>2014-11-11</created><authors><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Gonz&#xe1;lez-Nova</keyname><forenames>Javier I.</forenames></author><author><keyname>Ladra</keyname><forenames>Susana</forenames></author><author><keyname>Navarro</keyname><forenames>Gonzalo</forenames></author><author><keyname>Seco</keyname><forenames>Diego</forenames></author></authors><title>Faster Compressed Quadtrees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-world point sets tend to be clustered, so using a machine word for each
point is wasteful. In this paper we first bound the number of nodes in the
quadtree for a point set in terms of the points' clustering. We then describe a
quadtree data structure that uses $\mathcal{O} (1)$ bits per node and supports
faster queries than previous structures with this property. Finally, we present
experimental evidence that our structure is practical.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2791</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2791</id><created>2014-11-11</created><authors><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Gao</keyname><forenames>Xinyu</forenames></author><author><keyname>Su</keyname><forenames>Xin</forenames></author><author><keyname>Han</keyname><forenames>Shuangfeng</forenames></author><author><keyname>I</keyname><forenames>Chih-Lin</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>Low-Complexity Soft-Output Signal Detection Based on Gauss-Seidel Method
  for Uplink Multi-User Large-Scale MIMO Systems</title><categories>cs.IT math.IT</categories><comments>This paper has been accepted for publication by IEEE Transactions on
  Vehicular Technology. MATLAB code can be provided via request to reduplicate
  the results in this paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For uplink large-scale MIMO systems, minimum mean square error (MMSE)
algorithm is near-optimal but involves matrix inversion with high complexity.
In this paper, we propose to exploit the Gauss-Seidel (GS) method to
iteratively realize the MMSE algorithm without the complicated matrix
inversion. To further accelerate the convergence rate and reduce the
complexity, we propose a diagonal-approximate initial solution to the GS
method, which is much closer to the final solution than the traditional
zero-vector initial solution. We also propose a approximated method to compute
log-likelihood ratios (LLRs) for soft channel decoding with a negligible
performance loss. The analysis shows that the proposed GS-based algorithm can
reduce the computational complexity from O(K^3) to O(K^2), where K is the
number of users. Simulation results verify that the proposed algorithm
outperforms the recently proposed Neumann series approximation algorithm, and
achieves the near-optimal performance of the classical MMSE algorithm with a
small number of iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2795</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2795</id><created>2014-11-11</created><authors><author><keyname>Chaudhary</keyname><forenames>Nitesh Kumar</forenames></author></authors><title>Speaker Identification From Youtube Obtained Data</title><categories>cs.SD cs.LG</categories><comments>7 pages, 5 figures, 1 Table, Signal &amp; Image Processing : An
  International Journal (SIPIJ) Vol.5, No.5, October 2014</comments><msc-class>68T10 (Primary), 62H30, 68T50 (Secondary)</msc-class><doi>10.5121/sipij.2014.5503</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An efficient, and intuitive algorithm is presented for the identification of
speakers from a long dataset (like YouTube long discussion, Cocktail party
recorded audio or video).The goal of automatic speaker identification is to
identify the number of different speakers and prepare a model for that speaker
by extraction, characterization and speaker-specific information contained in
the speech signal. It has many diverse application specially in the field of
Surveillance, Immigrations at Airport, cyber security, transcription in
multi-source of similar sound source, where it is difficult to assign
transcription arbitrary. The most commonly speech parametrization used in
speaker verification, K-mean, cepstral analysis, is detailed. Gaussian mixture
modeling, which is the speaker modeling technique is then explained. Gaussian
mixture models (GMM), perhaps the most robust machine learning algorithm has
been introduced examine and judge carefully speaker identification in text
independent. The application or employment of Gaussian mixture models for
monitoring &amp; Analysing speaker identity is encouraged by the familiarity,
awareness, or understanding gained through experience that Gaussian spectrum
depict the characteristics of speaker's spectral conformational pattern and
remarkable ability of GMM to construct capricious densities after that we
illustrate 'Expectation maximization' an iterative algorithm which takes some
arbitrary value in initial estimation and carry on the iterative process until
the convergence of value is observed,so by doing various number of experiments
we are able to obtain 79 ~ 82% of identification rate using Vector quantization
and 85 ~ 92.6% of identification rate using GMM modeling by Expectation
maximization parameter estimation depending on variation of parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2800</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2800</id><created>2014-11-11</created><updated>2014-11-18</updated><authors><author><keyname>Cerutti</keyname><forenames>Federico</forenames></author><author><keyname>Tachmazidis</keyname><forenames>Ilias</forenames></author><author><keyname>Vallati</keyname><forenames>Mauro</forenames></author><author><keyname>Batsakis</keyname><forenames>Sotirios</forenames></author><author><keyname>Giacomin</keyname><forenames>Massimiliano</forenames></author><author><keyname>Antoniou</keyname><forenames>Grigoris</forenames></author></authors><title>Exploiting Parallelism for Hard Problems in Abstract Argumentation:
  Technical Report</title><categories>cs.AI cs.DC</categories><comments>Technical report of an accepted AAAI-2015 Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abstract argumentation framework (\AFname) is a unifying framework able to
encompass a variety of nonmonotonic reasoning approaches, logic programming and
computational argumentation. Yet, efficient approaches for most of the decision
and enumeration problems associated to \AFname s are missing, thus potentially
limiting the efficacy of argumentation-based approaches in real domains. In
this paper, we present an algorithm for enumerating the preferred extensions of
abstract argumentation frameworks which exploits parallel computation. To this
purpose, the SCC-recursive semantics definition schema is adopted, where
extensions are defined at the level of specific sub-frameworks. The algorithm
shows significant performance improvements in large frameworks, in terms of
number of solutions found and speedup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2816</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2816</id><created>2014-11-11</created><updated>2015-04-24</updated><authors><author><keyname>Khramtcova</keyname><forenames>Elena</forenames></author><author><keyname>Papadopoulou</keyname><forenames>Evanthia</forenames></author></authors><title>Linear-Time Algorithms for the Farthest-Segment Voronoi Diagram and
  Related Tree Structures</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present linear-time algorithms to construct tree-like Voronoi diagrams
with disconnected regions, after the sequence of their faces along an enclosing
boundary (or at infinity) is known. We focus on Voronoi diagrams of line
segments including the farthest-segment Voronoi diagram, the order-(k+1)
subdivision within an order-k Voronoi region, and deleting a site from a
nearest-neighbor segment Voronoi diagram. Although tree-structured, these
diagrams illustrate properties surprisingly different from their counterparts
for points. The sequence of their faces along the relevant boundary forms a
Davenport-Schinzel sequence of order &gt;=2. Once this sequence is known, we show
how to compute the corresponding Voronoi diagram in linear time, expected or
deterministic, augmenting the existing frameworks for points in convex position
with the ability to handle non-point sites and multiple Voronoi faces. Our
techniques contribute towards the question of linear-time construction
algorithms for Voronoi diagrams whose graph structure is a tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2821</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2821</id><created>2014-11-11</created><authors><author><keyname>Afshar</keyname><forenames>Saeed</forenames></author><author><keyname>George</keyname><forenames>Libin</forenames></author><author><keyname>Tapson</keyname><forenames>Jonathan</forenames></author><author><keyname>van Schaik</keyname><forenames>Andre</forenames></author><author><keyname>de Chazal</keyname><forenames>Philip</forenames></author><author><keyname>Hamilton</keyname><forenames>Tara Julia</forenames></author></authors><title>Turn Down that Noise: Synaptic Encoding of Afferent SNR in a Single
  Spiking Neuron</title><categories>cs.NE q-bio.NC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We have added a simplified neuromorphic model of Spike Time Dependent
Plasticity (STDP) to the Synapto-dendritic Kernel Adapting Neuron (SKAN). The
resulting neuron model is the first to show synaptic encoding of afferent
signal to noise ratio in addition to the unsupervised learning of spatio
temporal spike patterns. The neuron model is particularly suitable for
implementation in digital neuromorphic hardware as it does not use any complex
mathematical operations and uses a novel approach to achieve synaptic
homeostasis. The neurons noise compensation properties are characterized and
tested on noise corrupted zeros digits of the MNIST handwritten dataset.
Results show the simultaneously learning common patterns in its input data
while dynamically weighing individual afferent channels based on their signal
to noise ratio. Despite its simplicity the interesting behaviors of the neuron
model and the resulting computational power may offer insights into biological
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2832</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2832</id><created>2014-11-11</created><updated>2015-04-30</updated><authors><author><keyname>Barrett</keyname><forenames>Adam B.</forenames></author></authors><title>Exploration of synergistic and redundant information sharing in static
  and dynamical Gaussian systems</title><categories>cs.IT math.IT q-bio.NC</categories><comments>29 pages, 4 figures, in press Physical Review E, minor revisions to
  original version in response to peer review</comments><journal-ref>Phys. Rev. E 91, 052802 (2015)</journal-ref><doi>10.1103/PhysRevE.91.052802</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To fully characterize the information that two `source' variables carry about
a third `target' variable, one must decompose the total information into
redundant, unique and synergistic components, i.e. obtain a partial information
decomposition (PID). However Shannon's theory of information does not provide
formulae to fully determine these quantities. Several recent studies have begun
addressing this. Some possible definitions for PID quantities have been
proposed, and some analyses have been carried out on systems composed of
discrete variables. Here we present the first in-depth analysis of PIDs on
Gaussian systems, both static and dynamical. We show that, for a broad class of
Gaussian systems, previously proposed PID formulae imply that: (i) redundancy
reduces to the minimum information provided by either source variable, and
hence is independent of correlation between sources; (ii) synergy is the extra
information contributed by the weaker source when the stronger source is known,
and can either increase or decrease with correlation between sources. We find
that Gaussian systems frequently exhibit net synergy, i.e. the information
carried jointly by both sources is greater than the sum of informations carried
by each source individually. Drawing from several explicit examples, we discuss
the implications of these findings for measures of information transfer and
information-based measures of complexity, both generally and within a
neuroscience setting. Importantly, by providing independent formulae for
synergy and redundancy applicable to continuous time-series data, we open up a
new approach to characterizing and quantifying information sharing amongst
complex system variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2837</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2837</id><created>2014-11-11</created><authors><author><keyname>Redondi</keyname><forenames>Alessandro</forenames></author><author><keyname>Buranapanichkit</keyname><forenames>Dujdow</forenames></author><author><keyname>Cesana</keyname><forenames>Matteo</forenames></author><author><keyname>Tagliasacchi</keyname><forenames>Marco</forenames></author><author><keyname>Andreopoulos</keyname><forenames>Yiannis</forenames></author></authors><title>Energy Consumption Of Visual Sensor Networks: Impact Of Spatio-Temporal
  Coverage</title><categories>cs.NI</categories><comments>to appear in IEEE Transactions on Circuits and Systems for Video
  Technology, 2014</comments><doi>10.1109/TCSVT.2014.2329378</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless visual sensor networks (VSNs) are expected to play a major role in
future IEEE 802.15.4 personal area networks (PAN) under recently-established
collision-free medium access control (MAC) protocols, such as the IEEE
802.15.4e-2012 MAC. In such environments, the VSN energy consumption is
affected by the number of camera sensors deployed (spatial coverage), as well
as the number of captured video frames out of which each node processes and
transmits data (temporal coverage). In this paper, we explore this aspect for
uniformly-formed VSNs, i.e., networks comprising identical wireless visual
sensor nodes connected to a collection node via a balanced cluster-tree
topology, with each node producing independent identically-distributed
bitstream sizes after processing the video frames captured within each network
activation interval. We derive analytic results for the energy-optimal
spatio-temporal coverage parameters of such VSNs under a-priori known bounds
for the number of frames to process per sensor and the number of nodes to
deploy within each tier of the VSN. Our results are parametric to the
probability density function characterizing the bitstream size produced by each
node and the energy consumption rates of the system of interest. Experimental
results reveal that our analytic results are always within 7% of the energy
consumption measurements for a wide range of settings. In addition, results
obtained via a multimedia subsystem show that the optimal spatio-temporal
settings derived by the proposed framework allow for substantial reduction of
energy consumption in comparison to ad-hoc settings. As such, our analytic
modeling is useful for early-stage studies of possible VSN deployments under
collision-free MAC protocols prior to costly and time-consuming experiments in
the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2842</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2842</id><created>2014-11-11</created><authors><author><keyname>Englert</keyname><forenames>Matthias</forenames></author><author><keyname>Siebert</keyname><forenames>Sandra</forenames></author><author><keyname>Ziegler</keyname><forenames>Martin</forenames></author></authors><title>Logical Limitations to Machine Ethics with Consequences to Lethal
  Autonomous Weapons</title><categories>cs.CY cs.AI</categories><acm-class>K.4.1; I.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lethal Autonomous Weapons promise to revolutionize warfare -- and raise a
multitude of ethical and legal questions. It has thus been suggested to program
values and principles of conduct (such as the Geneva Conventions) into the
machines' control, thereby rendering them both physically and morally superior
to human combatants.
  We employ mathematical logic and theoretical computer science to explore
fundamental limitations to the moral behaviour of intelligent machines in a
series of &quot;Gedankenexperiments&quot;: Refining and sharpening variants of the
Trolley Problem leads us to construct an (admittedly artificial but) fully
deterministic situation where a robot is presented with two choices: one
morally clearly preferable over the other -- yet, based on the undecidability
of the Halting problem, it provably cannot decide algorithmically which one.
Our considerations have surprising implications to the question of
responsibility and liability for an autonomous system's actions and lead to
specific technical recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2844</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2844</id><created>2014-11-11</created><updated>2015-03-26</updated><authors><author><keyname>Singer</keyname><forenames>Philipp</forenames></author><author><keyname>Helic</keyname><forenames>Denis</forenames></author><author><keyname>Hotho</keyname><forenames>Andreas</forenames></author><author><keyname>Strohmaier</keyname><forenames>Markus</forenames></author></authors><title>HypTrails: A Bayesian Approach for Comparing Hypotheses About Human
  Trails on the Web</title><categories>cs.SI physics.data-an physics.soc-ph</categories><comments>Published in the proceedings of WWW'15</comments><acm-class>H.5.3</acm-class><doi>10.1145/2736277.2741080</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When users interact with the Web today, they leave sequential digital trails
on a massive scale. Examples of such human trails include Web navigation,
sequences of online restaurant reviews, or online music play lists.
Understanding the factors that drive the production of these trails can be
useful for e.g., improving underlying network structures, predicting user
clicks or enhancing recommendations. In this work, we present a general
approach called HypTrails for comparing a set of hypotheses about human trails
on the Web, where hypotheses represent beliefs about transitions between
states. Our approach utilizes Markov chain models with Bayesian inference. The
main idea is to incorporate hypotheses as informative Dirichlet priors and to
leverage the sensitivity of Bayes factors on the prior for comparing hypotheses
with each other. For eliciting Dirichlet priors from hypotheses, we present an
adaption of the so-called (trial) roulette method. We demonstrate the general
mechanics and applicability of HypTrails by performing experiments with (i)
synthetic trails for which we control the mechanisms that have produced them
and (ii) empirical trails stemming from different domains including website
navigation, business reviews and online music played. Our work expands the
repertoire of methods available for studying human trails on the Web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2846</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2846</id><created>2014-11-11</created><authors><author><keyname>Emiris</keyname><forenames>Ioannis</forenames></author><author><keyname>Kalinka</keyname><forenames>Tatjana</forenames></author><author><keyname>Konaxis</keyname><forenames>Christos</forenames></author></authors><title>Sparse implicitization by interpolation: Geometric computations using
  matrix representations</title><categories>math.AG cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the computation of a superset of the implicit support,
implicitization of a parametrically given hyper-surface is reduced to computing
the nullspace of a numeric matrix. Our approach exploits the sparseness of the
given parametric equations and of the implicit polynomial. In this work, we
study how this interpolation matrix can be used to reduce some key geometric
predicates on the hyper-surface to simple numerical operations on the matrix,
namely membership and sidedness for given query points. We illustrate our
results with examples based on our Maple implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2852</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2852</id><created>2014-11-11</created><authors><author><keyname>Malik</keyname><forenames>Aqsa</forenames></author><author><keyname>Qadir</keyname><forenames>Junaid</forenames></author><author><keyname>Ahmad</keyname><forenames>Basharat</forenames></author><author><keyname>Yau</keyname><forenames>Kok-Lim Alvin</forenames></author><author><keyname>Ullah</keyname><forenames>Ubaid</forenames></author></authors><title>QoS in IEEE 802.11-based Wireless Networks: A Contemporary Survey</title><categories>cs.NI</categories><doi>10.1016/j.jnca.2015.04.016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Apart from mobile cellular networks, IEEE 802.11-based wireless local area
networks (WLANs) represent the most widely deployed wireless networking
technology. With the migration of critical applications onto data networks, and
the emergence of multimedia applications such as digital audio/video and
multimedia games, the success of IEEE 802.11 depends critically on its ability
to provide quality of service (QoS). A lot of research has focused on equipping
IEEE 802.11 WLANs with features to support QoS. In this survey, we provide an
overview of these techniques. We discuss the QoS features incorporated by the
IEEE 802.11 standard at both physical (PHY) and media access control (MAC)
layers, as well as other higher-layer proposals. We also focus on how the new
architectural developments of software-defined networking (SDN) and cloud
networking can be used to facilitate QoS provisioning in IEEE 802.11-based
networks. We conclude this paper by identifying some open research issues for
future consideration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2855</identifier>
 <datestamp>2015-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2855</id><created>2014-11-11</created><updated>2015-04-01</updated><authors><author><keyname>Razniewski</keyname><forenames>Simon</forenames></author></authors><title>Query-driven Data Completeness Management (PhD Thesis)</title><categories>cs.DB</categories><comments>Change to previous version: Fixed the date on the title page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge about data completeness is essentially in data-supported decision
making. In this thesis we present a framework for metadata-based assessment of
database completeness. We discuss how to express information about data
completeness and how to use such information to draw conclusions about the
completeness of query answers. In particular, we introduce formalisms for
stating completeness for parts of relational databases. We then present
techniques for drawing inferences between such statements and statements about
the completeness of query answers, and show how the techniques can be extended
to databases that contain null values. We show that the framework for
relational databases can be transferred to RDF data, and that a similar
framework can also be applied to spatial data. We also discuss how completeness
information can be verified over processes, and introduce a data-aware process
model that allows this verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2858</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2858</id><created>2014-11-11</created><updated>2015-07-05</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Can Technology Life-Cycles Be Indicated by Diversity in Patent
  Classifications? The crucial role of variety</title><categories>cs.DL cs.CY</categories><comments>accepted for publication in Scientometrics (July 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a previous study of patent classifications in nine material technologies
for photovoltaic cells, Leydesdorff et al. (2015) reported cyclical patterns in
the longitudinal development of Rao-Stirling diversity. We suggested that these
cyclical patterns can be used to indicate technological life-cycles. Upon
decomposition, however, the cycles are exclusively due to increases and
decreases in the variety of the classifications, and not to disparity or
technological distance, measured as (1 - cosine). A single frequency component
can accordingly be shown in the periodogram. Furthermore, the cyclical patterns
are associated with the numbers of inventors in the respective technologies.
Sometimes increased variety leads to a boost in the number of inventors, but in
early phases--when the technology is still under constructio--it can also be
the other way round. Since the development of the cycles thus seems independent
of technological distances among the patents, the visualization in terms of
patent maps can be considered as addressing an analytically different set of
research questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2860</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2860</id><created>2014-11-11</created><authors><author><keyname>Anam</keyname><forenames>Mohammad Ashraful</forenames></author><author><keyname>Whatmough</keyname><forenames>Paul N.</forenames></author><author><keyname>Andreopoulos</keyname><forenames>Yiannis</forenames></author></authors><title>Precision-Energy-Throughput Scaling Of Generic Matrix Multiplication and
  Convolution Kernels Via Linear Projections</title><categories>cs.MM cs.MS</categories><journal-ref>IEEE Transactions on Circuits and Systems for Video Technology,
  vol. 24, no. 11, pp. 1860-1873, Nov. 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generic matrix multiplication (GEMM) and one-dimensional
convolution/cross-correlation (CONV) kernels often constitute the bulk of the
compute- and memory-intensive processing within image/audio recognition and
matching systems. We propose a novel method to scale the energy and processing
throughput of GEMM and CONV kernels for such error-tolerant multimedia
applications by adjusting the precision of computation. Our technique employs
linear projections to the input matrix or signal data during the top-level GEMM
and CONV blocking and reordering. The GEMM and CONV kernel processing then uses
the projected inputs and the results are accumulated to form the final outputs.
Throughput and energy scaling takes place by changing the number of projections
computed by each kernel, which in turn produces approximate results, i.e.
changes the precision of the performed computation. Results derived from a
voltage- and frequency-scaled ARM Cortex A15 processor running face recognition
and music matching algorithms demonstrate that the proposed approach allows for
280%~440% increase of processing throughput and 75%~80% decrease of energy
consumption against optimized GEMM and CONV kernels without any impact in the
obtained recognition or matching accuracy. Even higher gains can be obtained if
one is willing to tolerate some reduction in the accuracy of the recognition
and matching applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2861</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2861</id><created>2014-11-11</created><updated>2015-05-03</updated><authors><author><keyname>Liang</keyname><forenames>Xiaodan</forenames></author><author><keyname>Liu</keyname><forenames>Si</forenames></author><author><keyname>Wei</keyname><forenames>Yunchao</forenames></author><author><keyname>Liu</keyname><forenames>Luoqi</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Computational Baby Learning</title><categories>cs.CV</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intuitive observations show that a baby may inherently possess the capability
of recognizing a new visual concept (e.g., chair, dog) by learning from only
very few positive instances taught by parent(s) or others, and this recognition
capability can be gradually further improved by exploring and/or interacting
with the real instances in the physical world. Inspired by these observations,
we propose a computational model for slightly-supervised object detection,
based on prior knowledge modelling, exemplar learning and learning with video
contexts. The prior knowledge is modeled with a pre-trained Convolutional
Neural Network (CNN). When very few instances of a new concept are given, an
initial concept detector is built by exemplar learning over the deep features
from the pre-trained CNN. Simulating the baby's interaction with physical
world, the well-designed tracking solution is then used to discover more
diverse instances from the massive online unlabeled videos. Once a positive
instance is detected/identified with high score in each video, more variable
instances possibly from different view-angles and/or different distances are
tracked and accumulated. Then the concept detector can be fine-tuned based on
these new instances. This process can be repeated again and again till we
obtain a very mature concept detector. Extensive experiments on Pascal
VOC-07/10/12 object detection datasets well demonstrate the effectiveness of
our framework. It can beat the state-of-the-art full-training based
performances by learning from very few samples for each object category, along
with about 20,000 unlabeled videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2862</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2862</id><created>2014-11-11</created><authors><author><keyname>Buranapanichkit</keyname><forenames>Dujdow</forenames></author><author><keyname>Deligiannis</keyname><forenames>Nikos</forenames></author><author><keyname>Andreopoulos</keyname><forenames>Yiannis</forenames></author></authors><title>Convergence of Desynchronization Primitives in Wireless Sensor Networks:
  A Stochastic Modeling Approach</title><categories>cs.NI</categories><comments>to appear, IEEE Transactions on Signal Processing, 2015</comments><doi>10.1109/TSP.2014.2369003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Desynchronization approaches in wireless sensor networks converge to
time-division multiple access (TDMA) of the shared medium without requiring
clock synchronization amongst the wireless sensors, or indeed the presence of a
central (coordinator) node. All such methods are based on the principle of
reactive listening of periodic &quot;fire&quot; or &quot;pulse&quot; broadcasts: each node updates
the time of its fire message broadcasts based on received fire messages from
some of the remaining nodes sharing the given spectrum. In this paper, we
present a novel framework to estimate the required iterations for convergence
to fair TDMA scheduling. Our estimates are fundamentally different from
previous conjectures or bounds found in the literature as, for the first time,
convergence to TDMA is defined in a stochastic sense. Our analytic results
apply to the Desync algorithm and to pulse-coupled oscillator algorithms with
inhibitory coupling. The experimental evaluation via iMote2 TinyOS nodes (based
on the IEEE 802.15.4 standard) as well as via computer simulations demonstrates
that, for the vast majority of settings, our stochastic model is within one
standard deviation from the experimentally-observed convergence iterations. The
proposed estimates are thus shown to characterize the desynchronization
convergence iterations significantly better than existing conjectures or
bounds. Therefore, they contribute towards the analytic understanding of how a
desynchronization-based system is expected to evolve from random initial
conditions to the desynchronized steady state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2864</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2864</id><created>2014-11-11</created><updated>2015-01-24</updated><authors><author><keyname>Totu</keyname><forenames>Luminita Cristiana</forenames><affiliation>Automation and Control, Aalborg University</affiliation></author><author><keyname>Wisniewski</keyname><forenames>Rafael</forenames><affiliation>Automation and Control, Aalborg University</affiliation></author><author><keyname>Leth</keyname><forenames>John</forenames><affiliation>Automation and Control, Aalborg University</affiliation></author></authors><title>Modeling Populations of Thermostatic Loads with Switching Rate Actuation</title><categories>cs.SY</categories><comments>In Proceedings HAS 2014, arXiv:1501.05405</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 174, 2015, pp. 13-21</journal-ref><doi>10.4204/EPTCS.174.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We model thermostatic devices using a stochastic hybrid description, and
introduce an external actuation mechanism that creates random switch events in
the discrete dynamics. We then conjecture the form of the Fokker-Planck
equation and successfully verify it numerically using Monte Carlo simulations.
The actuation mechanism and subsequent modeling result are relevant for power
system operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2865</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2865</id><created>2014-11-11</created><updated>2014-12-01</updated><authors><author><keyname>Kumar</keyname><forenames>Rahul</forenames></author><author><keyname>Kumar</keyname><forenames>Ajay</forenames></author></authors><title>Metamorphosis of Fuzzy Regular Expressions to Fuzzy Automata using the
  Follow Automata</title><categories>cs.FL</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To deal with system uncertainty, finite automata have been generalized into
fuzzy automata. Stamenkovic and Ciric proposed an approach using the position
automata for the construction of fuzzy automata from fuzzy regular expressions.
There exist multifarious methodologies for the construction of finite automata
from regular expressions known as Thompson construction, Antimirov partial
derivatives, Glushkov automata and follow automata etc. In this paper, we
propose an approach for the conversion of fuzzy regular expressions into fuzzy
automata using the concept of follow automata. The number of states of the
obtained Fuzzy automata using the proposed approach is lesser than the extant
approaches in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2867</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2867</id><created>2014-11-11</created><authors><author><keyname>Ozturk</keyname><forenames>Oczan</forenames></author><author><keyname>Mazumdar</keyname><forenames>Ravi R.</forenames></author><author><keyname>Likhanov</keyname><forenames>Nikolay B.</forenames></author></authors><title>Buffer occupancy asymptotics in rate proportional sharing networks with
  heterogeneous long-tailed inputs</title><categories>cs.PF</categories><msc-class>Primary 60J25, Secondary 68M20, 90B18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a network of rate proportional processor sharing
servers in which sessions with long-tailed duration arrive as Poisson
processes. In particular, we assume that a session of type $n$ transmits at a
rate $r_n$ bits per unit time and lasts for a random time $\tau_n$ with a
generalized Pareto distribution given by $P \{\tau_n &gt; x\} \sim \alpha_n
x^{-(1+\beta_n)}$ for large $x$, where $\alpha_n, \beta_n &gt; 0$. The weights are
taken to be the rates of the flows. The network is assumed to be loop-free with
respect to source-destination routes. We characterize the order $O-$asymptotics
of the complementary buffer occupancy distribution at each node in terms of the
input characteristics of the sessions. In particular, we show that the
distributions obey a power law whose exponent can be calculated via solving a
fixed point and deterministic knapsack problem. The paper concludes with some
canonical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2873</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2873</id><created>2014-11-11</created><authors><author><keyname>Borradaile</keyname><forenames>Glencora</forenames></author><author><keyname>Kennedy</keyname><forenames>W. Sean</forenames></author><author><keyname>Wilfong</keyname><forenames>Gordon</forenames></author><author><keyname>Zhang</keyname><forenames>Lisa</forenames></author></authors><title>Improving Robustness of Next-Hop Routing</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A weakness of next-hop routing is that following a link or router failure
there may be no routes between some source-destination pairs, or packets may
get stuck in a routing loop as the protocol operates to establish new routes.
In this article, we address these weaknesses by describing mechanisms to choose
alternate next hops.
  Our first contribution is to model the scenario as the following {\sc tree
augmentation} problem. Consider a mixed graph where some edges are directed and
some undirected. The directed edges form a spanning tree pointing towards the
common destination node. Each directed edge represents the unique next hop in
the routing protocol. Our goal is to direct the undirected edges so that the
resulting graph remains acyclic and the number of nodes with outdegree two or
more is maximized. These nodes represent those with alternative next hops in
their routing paths.
  We show that {\sc tree augmentation} is NP-hard in general and present a
simple $\frac{1}{2}$-approximation algorithm. We also study 3 special cases. We
give exact polynomial-time algorithms for when the input spanning tree consists
of exactly 2 directed paths or when the input graph has bounded treewidth. For
planar graphs, we present a polynomial-time approximation scheme when the input
tree is a breadth-first search tree. To the best of our knowledge, {\sc tree
augmentation} has not been previously studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2874</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2874</id><created>2014-11-09</created><updated>2015-01-22</updated><authors><author><keyname>Ho</keyname><forenames>Hsi-Ming</forenames></author><author><keyname>Ouaknine</keyname><forenames>Joel</forenames></author></authors><title>The Cyclic-Routing UAV Problem is PSPACE-Complete</title><categories>cs.LO cs.FL</categories><comments>19 pages. Full version of the FoSSaCS'15 paper with the same title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a finite set of targets, with each target assigned a relative
deadline, and each pair of targets assigned a fixed transit flight time. Given
a flock of identical UAVs, can one ensure that every target is repeatedly
visited by some UAV at intervals of duration at most the target's relative
deadline? The Cyclic-Routing UAV Problem (CR-UAV) is the question of whether
this task has a solution.
  This problem can straightforwardly be solved in PSPACE by modelling it as a
network of timed automata. The special case of there being a single UAV is
claimed to be NP-complete in the literature. In this paper, we show that the
CR-UAV Problem is in fact PSPACE-complete even in the single-UAV case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2878</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2878</id><created>2014-11-11</created><authors><author><keyname>Halfaker</keyname><forenames>Aaron</forenames></author><author><keyname>Keyes</keyname><forenames>Oliver</forenames></author><author><keyname>Kluver</keyname><forenames>Daniel</forenames></author><author><keyname>Thebault-Spieker</keyname><forenames>Jacob</forenames></author><author><keyname>Nguyen</keyname><forenames>Tien</forenames></author><author><keyname>Shores</keyname><forenames>Kenneth</forenames></author><author><keyname>Uduwage</keyname><forenames>Anuradha</forenames></author><author><keyname>Warncke-Wang</keyname><forenames>Morten</forenames></author></authors><title>User Session Identification Based on Strong Regularities in
  Inter-activity Time</title><categories>cs.HC cs.SI</categories><comments>9 pages, 5 figures, 1 table</comments><acm-class>H.1.1</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Session identification is a common strategy used to develop metrics for web
analytics and behavioral analyses of user-facing systems. Past work has argued
that session identification strategies based on an inactivity threshold is
inherently arbitrary or advocated that thresholds be set at about 30 minutes.
In this work, we demonstrate a strong regularity in the temporal rhythms of
user initiated events across several different domains of online activity
(incl. video gaming, search, page views and volunteer contributions). We
describe a methodology for identifying clusters of user activity and argue that
regularity with which these activity clusters appear implies a good
rule-of-thumb inactivity threshold of about 1 hour. We conclude with
implications that these temporal rhythms may have for system design based on
our observations and theories of goal-directed human activity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2880</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2880</id><created>2014-11-07</created><updated>2014-12-07</updated><authors><author><keyname>Cao</keyname><forenames>Jianxiong</forenames></author><author><keyname>Chen</keyname><forenames>YangQuan</forenames></author><author><keyname>Li</keyname><forenames>Changpin</forenames></author></authors><title>Multi-UAV-based Optimal Crop-dusting of Anomalously Diffusing
  Infestation of Crops</title><categories>cs.SY</categories><comments>21pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a UAV-based optimal crop-dusting method to control
anomalously diffusing infestation of crops. Two anomalous diffusion models are
considered, which are, respectively, time-fractional order diffusion equation
and space-fractional order diffusion equation. Our problem formulation is
motivated by real-time pest management by using networked unmanned cropdusters
where the pest spreading is modeled as a fractional diffusion equation. We
attempt to solve the optimal dynamic location of actuators by using Centroidal
Voronoi Tessellations. A new simulation platform (FO-DiffMAS-2D) for
measurement scheduling and controls in fractional order distributed parameter
systems is also introduced in this paper. Simulation results are presented to
show the effectiveness of the proposed method as well as the role of fractional
order in the overall control performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2883</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2883</id><created>2014-10-28</created><updated>2015-09-13</updated><authors><author><keyname>Jain</keyname><forenames>Namita</forenames></author><author><keyname>Murthy</keyname><forenames>C. A.</forenames></author></authors><title>A new estimate of mutual information based measure of dependence between
  two variables: properties and fast implementation</title><categories>cs.IT cs.LG math.IT</categories><comments>International Journal of Machine Learning and Cybernetics, Springer
  Berlin Heidelberg, 10-Sep-2015</comments><doi>10.1007/s13042-015-0418-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes a new method to estimate an existing mutual information
based dependence measure using histogram density estimates. Finding a suitable
bin length for histogram is an open problem. We propose a new way of computing
the bin length for histogram using a function of maximum separation between
points. The chosen bin length leads to consistent density estimates for
histogram method. The values of density thus obtained are used to calculate an
estimate of an existing dependence measure. The proposed estimate is named as
Mutual Information Based Dependence Index (MIDI). Some important properties of
MIDI have also been stated. The performance of the proposed method has been
compared to generally accepted measures like Distance Correlation (dcor),
Maximal Information Coefficient (MINE) in terms of accuracy and computational
complexity with the help of several artificial data sets with different amounts
of noise. The proposed method is able to detect many types of relationships
between variables, without making any assumption about the functional form of
the relationship. The power statistics of proposed method illustrate their
effectiveness in detecting non linear relationship. Thus, it is able to achieve
generality without a high rate of false positive cases. MIDI is found to work
better on a real life data set than competing methods. The proposed method is
found to overcome some of the limitations which occur with dcor and MINE.
Computationally, MIDI is found to be better than dcor and MINE, in terms of
time and memory, making it suitable for large data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2885</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2885</id><created>2014-11-09</created><authors><author><keyname>Mariusz</keyname><forenames>Frydrych</forenames></author><author><keyname>Wojciech</keyname><forenames>Horzelski</forenames></author></authors><title>Generator kodow liniowych o skonczonych charakterystykach</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a method to generate some families of linear codes over
finite fields of characteristics greater than two in the widest class due to
the size of Grassmann manifold, i.e. when the dimension is equal to
codimension. Our method applies some simple embedding of projective line into
the Grassman manifold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2890</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2890</id><created>2014-11-11</created><authors><author><keyname>Feghhi</keyname><forenames>Mahmood Mohassel</forenames></author><author><keyname>Abbasfar</keyname><forenames>Aliazam</forenames></author><author><keyname>Mirmohseni</keyname><forenames>Mahtab</forenames></author></authors><title>Performance Analysis for Energy Harvesting Communication Protocols with
  Fixed Rate Transmission</title><categories>cs.IT cs.ET cs.PF math.IT</categories><comments>29 pages, 12 figures, Accepted for publication in IET Communications,
  August 2014</comments><doi>10.1049/iet-com.2014.0281</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy Harvesting (EH) has emerged as a promising technique for Green
Communications and it is a novel technique to prolong the lifetime of the
wireless networks with replenishable nodes. In this paper, we consider the
energy shortage analysis of fixed rate transmission in communication systems
with energy harvesting nodes. First, we study the finite-horizon transmission
and provide the general formula for the energy shortage probability. We also
give some examples as benchmarks. Then, we continue to derive a closed-form
expression for infinite-horizon transmission, which is a lower bound for the
energy shortage probability of any finite-horizon transmission. These results
are proposed for both Additive White Gaussian Noise (AWGN) and fading channels.
Moreover, we show that even under \emph{random energy arrival}, one can
transmit at a fixed rate equal to capacity in the AWGN channels with negligible
aggregate shortage time. We achieve this result using our practical
transmission schemes, proposed for finite-horizon. Also, comprehensive
numerical simulations are performed in AWGN and fading channels with no Channel
State Information (CSI) available at the transmitter, which corroborate our
theoretical findings. Furthermore, we improve the performance of our
transmission schemes in the fading channel with no CSI at the transmitter by
optimizing the transmission initiation threshold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2893</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2893</id><created>2014-11-11</created><authors><author><keyname>Anagnostopoulos</keyname><forenames>Aris</forenames></author><author><keyname>Bessi</keyname><forenames>Alessandro</forenames></author><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author><author><keyname>Del Vicario</keyname><forenames>Michela</forenames></author><author><keyname>Petroni</keyname><forenames>Fabio</forenames></author><author><keyname>Scala</keyname><forenames>Antonio</forenames></author><author><keyname>Zollo</keyname><forenames>Fabiana</forenames></author><author><keyname>Quattrociocchi</keyname><forenames>Walter</forenames></author></authors><title>Viral Misinformation: The Role of Homophily and Polarization</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Misinformation, Virality, Attention Patterns</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spreading of unsubstantiated rumors on online social networks (OSN)
either unintentionally or intentionally (e.g., for political reasons or even
trolling) can have serious consequences such as in the recent case of rumors
about Ebola causing disruption to health-care workers. Here we show that
indicators aimed at quantifying information consumption patterns might provide
important insights about the virality of false claims. In particular, we
address the driving forces behind the popularity of contents by analyzing a
sample of 1.2M Facebook Italian users consuming different (and opposite) types
of information (science and conspiracy news). We show that users' engagement
across different contents correlates with the number of friends having similar
consumption patterns (homophily), indicating the area in the social network
where certain types of contents are more likely to spread. Then, we test
diffusion patterns on an external sample of $4,709$ intentional satirical false
claims showing that neither the presence of hubs (structural properties) nor
the most active users (influencers) are prevalent in viral phenomena. Instead,
we found out that in an environment where misinformation is pervasive, users'
aggregation around shared beliefs may make the usual exposure to conspiracy
stories (polarization) a determinant for the virality of false information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2897</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2897</id><created>2014-11-11</created><authors><author><keyname>Ismkhan</keyname><forenames>Hassan</forenames></author></authors><title>Accelerating the ANT Colony Optimization By Smart ANTs, Using Genetic
  Operator</title><categories>cs.NE</categories><comments>International Journal on Computational Science &amp; Applications,
  Volume: 4 - volume NO: 2 - Issue: April 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper research review Ant colony optimization (ACO) and Genetic
Algorithm (GA), both are two powerful meta-heuristics. This paper explains some
major defects of these two algorithm at first then proposes a new model for ACO
in which, artificial ants use a quick genetic operator and accelerate their
actions in selecting next state. Experimental results show that proposed hybrid
algorithm is effective and its performance including speed and accuracy beats
other version.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2901</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2901</id><created>2014-11-11</created><authors><author><keyname>Schuh</keyname><forenames>Bernd R.</forenames></author></authors><title>Easy/Hard Transition in k-SAT</title><categories>cs.CC math.LO</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A heuristic model procedure for determining satisfiability of CNF-formulae is
set up and described by nonlinear recursion relations for m (number of
clauses), n (number of variables) and clause filling k. The system mimicked by
the recursion undergoes a sharp transition from bounded running times (easy) to
uncontrolled runaway behaviour (hard). Thus the parameter space turns out to be
separated into regions with qualitatively different efficiency of the model
procedure. The transition results from a competition of exponential blow up by
branching versus growing number of orthogonal clauses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2917</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2917</id><created>2014-11-11</created><authors><author><keyname>Held</keyname><forenames>Stephan</forenames></author><author><keyname>Spirkl</keyname><forenames>Sophie</forenames></author></authors><title>Fast Prefix Adders for Non-Uniform Input Arrival Times</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of constructing fast and small parallel prefix adders
for non-uniform input arrival times. This problem arises whenever the adder is
embedded into a more complex circuit, e. g. a multiplier.
  Most previous results are based on representing binary carry-propagate adders
as so-called parallel prefix graphs, in which pairs of generate and propagate
signals are combined using complex gates known as prefix gates. Adders
constructed in this model usually minimize the delay in terms of these prefix
gates. However, the delay in terms of logic gates can be worse by a factor of
two.
  In contrast, we aim to minimize the delay of the underlying logic circuit
directly. We prove a lower bound on the delay of a carry bit computation
achievable by any prefix carry bit circuit and develop an algorithm that
computes a prefix carry bit circuit with optimum delay up to a small additive
constant. Furthermore, we use this algorithm to construct a small parallel
prefix adder.
  Compared to existing algorithms we simultaneously improve the delay and size
guarantee, as well as the running time for constructing prefix carry bit and
adder circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2918</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2918</id><created>2014-11-11</created><updated>2014-11-12</updated><authors><author><keyname>Lattimore</keyname><forenames>Tor</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Asymptotics of Continuous Bayes for Non-i.i.d. Sources</title><categories>cs.IT math.IT</categories><comments>16 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clarke and Barron analysed the relative entropy between an i.i.d. source and
a Bayesian mixture over a continuous class containing that source. In this
paper a comparable result is obtained when the source is permitted to be both
non-stationary and dependent. The main theorem shows that Bayesian methods
perform well for both compression and sequence prediction even in this most
general setting with only mild technical assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2919</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2919</id><created>2014-11-11</created><authors><author><keyname>Lattimore</keyname><forenames>Tor</forenames></author><author><keyname>Munos</keyname><forenames>Remi</forenames></author></authors><title>Bounded Regret for Finite-Armed Structured Bandits</title><categories>cs.LG</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a new type of K-armed bandit problem where the expected return of
one arm may depend on the returns of other arms. We present a new algorithm for
this general class of problems and show that under certain circumstances it is
possible to achieve finite expected cumulative regret. We also give
problem-dependent lower bounds on the cumulative regret showing that at least
in special cases the new algorithm is nearly optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2928</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2928</id><created>2014-11-11</created><authors><author><keyname>Brimkov</keyname><forenames>Valentin E.</forenames></author><author><keyname>Junosza-Szaniawski</keyname><forenames>Konstanty</forenames></author><author><keyname>Kafer</keyname><forenames>Sean</forenames></author><author><keyname>Kratochv&#xed;l</keyname><forenames>Jan</forenames></author><author><keyname>Pergel</keyname><forenames>Martin</forenames></author><author><keyname>Rz&#x105;&#x17c;ewski</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Szczepankiewicz</keyname><forenames>Matthew</forenames></author><author><keyname>Terhaar</keyname><forenames>Joshua</forenames></author></authors><title>Homothetic Polygons and Beyond: Intersection Graphs, Recognition, and
  Maximum Clique</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the {\sc Clique} problem in classes of intersection graphs of convex
sets in the plane. The problem is known to be NP-complete in convex-set
intersection graphs and straight-line-segment intersection graphs, but solvable
in polynomial time in intersection graphs of homothetic triangles. We extend
the latter result by showing that for every convex polygon $P$ with sides
parallel to $k$ directions, every $n$-vertex graph which is an intersection
graph of homothetic copies of $P$ contains at most $n^{k}$ inclusion-wise
maximal cliques. We actually prove this result for a more general class of
graphs, the so called $k_{\text{DIR}}-\text{CONV}$, which are intersection
graphs of convex polygons whose sides are parallel to some fixed $k$
directions. Moreover, we provide some lower bounds on the numbers of maximal
cliques, discuss the complexity of recognizing these classes of graphs and
present a relationship with other classes of convex-set intersection graphs.
Finally, we generalize the upper bound on the number of maximal cliques to
intersection graphs of higher-dimensional convex polytopes in Euclidean space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2931</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2931</id><created>2014-10-10</created><authors><author><keyname>Akand</keyname><forenames>Md. Mamunur Rashid</forenames></author><author><keyname>Nayeem</keyname><forenames>Mir Tafseer</forenames></author><author><keyname>Sumon</keyname><forenames>Md. Rokon Uz Zaman</forenames></author><author><keyname>Alam</keyname><forenames>Muhammad Mahbub</forenames></author></authors><title>A Probabilistic Delay Model for Bidirectional VANETs in City
  Environments</title><categories>cs.NI</categories><comments>5 pages, In Proc. of the 10th IEEE Vehicular Technology Society Asia
  Pacific Wireless Communications Symposium (IEEE APWCS 2013)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Routing in VANETs (Vehicular Ad hoc NETworks) is a challenging task due to
large network sizes, rapidly changing topology and frequent network
disconnections. State-of-the-art routing protocols tried to address these
specific problems especially in city environments (vehicles constrained by road
geometry, signal transmissions blocked by obstacles, degree of congestion in
roads etc). It was noticed that in city scenarios codirectional roads consist
of a collection of disconnected clusters because of traffic control strategies
(e.g., RSU (Road Side Units), stop signs and traffic lights). In this paper, we
propose an intervehicle ad-hoc routing metric called EFD (Expected Forwarding
Delay) based on the vehicular traffic statistics (e.g., densities and
velocities) collected on-the-fly. We derive an analytical expression for the
expected size of a cluster in co-directional traffic. In case of disconnection
between two co-directional clusters the opposite directional clusters are used
as a bridge to propagate a message in the actual forwarding direction to reduce
the delay due to carry and forward. Through theoretical analysis and extensive
simulation, it is shown that our link delay model provides the accurate link
delay estimation in bidirectional city environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2939</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2939</id><created>2014-11-11</created><authors><author><keyname>Saha</keyname><forenames>Amal</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Analysis of Applicability of ISO 9564 PIN based Authentication to
  Closed-Loop Mobile Payment Systems</title><categories>cs.CR</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Payment transactions initiated through a mobile device are growing and
security concerns must be ad-dressed. People coming from payment card industry
often talk passionately about porting ISO 9564 PIN standard based
authentication in open-loop card payment to closed-loop mobile financial
transactions and certification of closed-loop payment product or solution
against this standard. In reality, so far this standard has not been adopted in
closed-loop mobile payment authentication and applicability of this ISO
standard must be studied carefully before adoption. The authors do a critical
analysis of the applicability of this ISO specification and makes categorical
statement about relevance of compliance to closed-loop mobile payment. Security
requirements for authentication in closed-loop mobile payment systems are not
standardized through ISO 9564 standard, Common Criteria, etc. Since closed-loop
mobile payment is a relatively new field, the authors make a case for Common
Criteria Recognition Agreement (CCRA) or other standards organization to push
for publication of a mobile device-agnostic Protection Profile or standard for
it, incorporating the suggested authentication approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2940</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2940</id><created>2014-11-11</created><updated>2016-02-24</updated><authors><author><keyname>McRae</keyname><forenames>Andrew T. T.</forenames></author><author><keyname>Bercea</keyname><forenames>Gheorghe-Teodor</forenames></author><author><keyname>Mitchell</keyname><forenames>Lawrence</forenames></author><author><keyname>Ham</keyname><forenames>David A.</forenames></author><author><keyname>Cotter</keyname><forenames>Colin J.</forenames></author></authors><title>Automated generation and symbolic manipulation of tensor product finite
  elements</title><categories>math.NA cs.MS cs.NA</categories><comments>Submitted to SISC special issue on CSE Software. Updated version,
  following reviewer comments</comments><acm-class>G.1.8; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe and implement a symbolic algebra for scalar and vector-valued
finite elements, enabling the computer generation of elements with tensor
product structure on quadrilateral, hexahedral and triangular prismatic cells.
The algebra is implemented as an extension to the domain-specific language UFL,
the Unified Form Language. This allows users to construct many finite element
spaces beyond those supported by existing software packages. We have made
corresponding extensions to FIAT, the FInite element Automatic Tabulator, to
enable numerical tabulation of such spaces. This tabulation is consequently
used during the automatic generation of low-level code that carries out local
assembly operations, within the wider context of solving finite element
problems posed over such function spaces. We have done this work within the
code-generation pipeline of the software package Firedrake; we make use of the
full Firedrake package to present numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2942</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2942</id><created>2014-11-11</created><updated>2015-06-01</updated><authors><author><keyname>Zhou</keyname><forenames>Xiaowei</forenames></author><author><keyname>Leonardos</keyname><forenames>Spyridon</forenames></author><author><keyname>Hu</keyname><forenames>Xiaoyan</forenames></author><author><keyname>Daniilidis</keyname><forenames>Kostas</forenames></author></authors><title>3D Shape Estimation from 2D Landmarks: A Convex Relaxation Approach</title><categories>cs.CV</categories><comments>In Proceedings of CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of estimating the 3D shape of an object, given a
set of 2D landmarks in a single image. To alleviate the reconstruction
ambiguity, a widely-used approach is to confine the unknown 3D shape within a
shape space built upon existing shapes. While this approach has proven to be
successful in various applications, a challenging issue remains, i.e., the
joint estimation of shape parameters and camera-pose parameters requires to
solve a nonconvex optimization problem. The existing methods often adopt an
alternating minimization scheme to locally update the parameters, and
consequently the solution is sensitive to initialization. In this paper, we
propose a convex formulation to address this problem and develop an efficient
algorithm to solve the proposed convex program. We demonstrate the exact
recovery property of the proposed method, its merits compared to alternative
methods, and the applicability in human pose and car shape estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.2953</identifier>
 <datestamp>2014-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.2953</id><created>2014-11-11</created><authors><author><keyname>Kumbhkar</keyname><forenames>Ratnesh</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan</forenames></author><author><keyname>Seskar</keyname><forenames>Ivan</forenames></author></authors><title>HetNetwork Coding: Scaling Throughput in Heterogeneous Networks using
  Multiple Radio Interfaces</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The explosive demand for data has called for solution approaches that range
from spectrally agile cognitive radios with novel spectrum sharing, to use of
higher frequency spectrum as well as smaller and denser cell deployments with
diverse access technologies, referred to as heterogeneous networks (HetNets).
Simultaneously, advances in electronics and storage, has led to the advent of
wireless devices equipped with multiple radio interfaces (e.g. WiFi, WiMAX,
LTE, etc.) and the ability to store and efficiently process large amounts of
data. Motivated by the convergence of HetNets and multi-platform radios, we
propose HetNetwork Coding as a means to utilize the available radio interfaces
in parallel along with network coding to increase wireless data throughput.
Specifically we explore the use of random linear network coding at the network
layer where packets can travel through multiple interfaces and be received via
multihoming. Using both simulations and experimentation with real hardware on
WiFi and WiMAX platforms, we study the scaling of throughput enabled by such
HetNetwork coding. We find from our simulations and experiments that the use of
this method increases the throughput, with greater gains achieved for cases
when the system is heavily loaded or the channel quality is poor. Our results
also reveal that the throughput gains achieved scale linearly with the number
of radio interfaces at the nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3000</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3000</id><created>2014-11-03</created><updated>2014-12-11</updated><authors><author><keyname>Caviglione</keyname><forenames>Luca</forenames></author><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author></authors><title>Understanding Information Hiding in iOS</title><categories>cs.CR</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Apple operating system (iOS) has so far proved resistant to
information-hiding techniques, which help attackers covertly communicate.
However, Siri - a native iOS service that controls iPhones and iPads via voice
commands - could change this trend.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3010</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3010</id><created>2014-11-11</created><authors><author><keyname>Levin</keyname><forenames>Leonid A.</forenames></author></authors><title>Computational Complexity of Functions</title><categories>cs.CC</categories><comments>Partial translation from [Levin 74]; preliminary version is in [Levin
  73]</comments><journal-ref>Theoretical Computer Science, 157:267-271, 1996</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Below is a translation from my Russian paper. I added references, unavailable
to me in Moscow. Similar results have been also given in [Schnorr Stumpf 75]
(see also [Lynch 75]). Earlier relevant work (classical theorems like
Compression, Speed-up, etc.) was done in [Tseitin 56, Rabin 59, Hartmanis
Stearns 65, Blum 67, Trakhtenbrot 67, Meyer Fischer 72].
  I translated only the part with the statement of the results. Instead of the
proof part I appended a later (1979, unpublished) proof sketch of a slightly
tighter version. The improvement is based on the results of [Meyer Winklmann
78, Sipser 78]. Meyer and Winklmann extended earlier versions to machines with
a separate input and working tape, thus allowing complexities smaller than the
input length (down to its log). Sipser showed the space-bounded Halting Problem
to require only additive constant overhead. The proof in the appendix below
employs both advances to extend the original proofs to machines with a fixed
alphabet and a separate input and working space. The extension has no (even
logarithmic) restrictions on complexity and no overhead (beyond an additive
constant). The sketch is very brief and a more detailed exposition is expected
later: [Seiferas Meyer].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3015</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3015</id><created>2014-11-11</created><authors><author><keyname>Drabent</keyname><forenames>Wlodzimierz</forenames></author></authors><title>On completeness of logic programs</title><categories>cs.LO cs.PL</categories><comments>20 pages</comments><acm-class>D.1.6; F.3.1; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Program correctness (in imperative and functional programming) splits in
logic programming into correctness and completeness. Completeness means that a
program produces all the answers required by its specification. Little work has
been devoted to reasoning about completeness. This paper presents a few
sufficient conditions for completeness of definite programs. We also study
preserving completeness under some cases of pruning of SLD-trees (e.g. due to
using the cut).
  We treat logic programming as a declarative paradigm, abstracting from any
operational semantics as far as possible. We argue that the proposed methods
are simple enough to be applied, possibly at an informal level, in practical
Prolog programming. We point out importance of approximate specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3017</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3017</id><created>2014-11-11</created><authors><author><keyname>Shomorony</keyname><forenames>Ilan</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author></authors><title>Sampling Large Data on Graphs</title><categories>cs.IT math.IT</categories><comments>To be presented at GlobalSIP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of sampling from data defined on the nodes of a
weighted graph, where the edge weights capture the data correlation structure.
As shown recently, using spectral graph theory one can define a cut-off
frequency for the bandlimited graph signals that can be reconstructed from a
given set of samples (i.e., graph nodes). In this work, we show how this
cut-off frequency can be computed exactly. Using this characterization, we
provide efficient algorithms for finding the subset of nodes of a given size
with the largest cut-off frequency and for finding the smallest subset of nodes
with a given cut-off frequency. In addition, we study the performance of random
uniform sampling when compared to the centralized optimal sampling provided by
the proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3035</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3035</id><created>2014-11-11</created><updated>2015-04-29</updated><authors><author><keyname>Chiribella</keyname><forenames>Giulio</forenames></author></authors><title>Distinguishability and copiability of programs in general process
  theories</title><categories>quant-ph cs.IT cs.LO math-ph math.CT math.IT math.MP</categories><comments>14 pages, contribution to the Special Issue on &quot;Quantum Computation
  and Quantum Information&quot;, International Journal of Software and Informatics,
  S.-L. Luo, M. G. A. Paris, and Y. Shang editors, published version</comments><journal-ref>International Journal of Software and Informatics, 8(3-4), 209 223
  (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a notion of state distinguishability that does not refer to
probabilities, but rather to the ability of a set of states to serve as
programs for a desired set of gates. Using this notion, we reconstruct the
structural features of the task of state discrimination, such as the
equivalence with cloning and the impossibility to extract information from two
non-distinguishable pure states without causing a disturbance. All these
features express intrinsic links among operational tasks, which are valid
independently of the particular theory under consideration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3041</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3041</id><created>2014-11-11</created><authors><author><keyname>Vedantam</keyname><forenames>Ramakrishna</forenames></author><author><keyname>Zitnick</keyname><forenames>C. Lawrence</forenames></author><author><keyname>Parikh</keyname><forenames>Devi</forenames></author></authors><title>Collecting Image Description Datasets using Crowdsourcing</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe our two new datasets with images described by humans. Both the
datasets were collected using Amazon Mechanical Turk, a crowdsourcing platform.
The two datasets contain significantly more descriptions per image than other
existing datasets. One is based on a popular image description dataset called
the UIUC Pascal Sentence Dataset, whereas the other is based on the Abstract
Scenes dataset con- taining images made from clipart objects. In this paper we
describe our interfaces, analyze some properties of and show example
descriptions from our two datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3044</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3044</id><created>2014-11-11</created><authors><author><keyname>Barth</keyname><forenames>Kimberly</forenames></author><author><keyname>Furcy</keyname><forenames>David</forenames></author><author><keyname>Summers</keyname><forenames>Scott M.</forenames></author><author><keyname>Totzke</keyname><forenames>Paul</forenames></author></authors><title>Scaled tree fractals do not strictly self-assemble</title><categories>cs.CG</categories><comments>13 pages, 3 figures, Appeared in the Proceedings of UCNC-2014, pp
  27-39; Unconventional Computation and Natural Computation - 13th
  International Conference, UCNC 2014, London, ON, Canada, July 14-18, 2014,
  Springer Lecture Notes in Computer Science ISBN 978-3-319-08122-9</comments><doi>10.1007/978-3-319-08123-6_3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that any scaled-up version of any discrete
self-similar {\it tree} fractal does not strictly self-assemble, at any
temperature, in Winfree's abstract Tile Assembly Model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3047</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3047</id><created>2014-11-11</created><authors><author><keyname>Cai</keyname><forenames>Xing Shi</forenames></author><author><keyname>Perarnau</keyname><forenames>Guillem</forenames></author><author><keyname>Reed</keyname><forenames>Bruce</forenames></author><author><keyname>Watts</keyname><forenames>Adam Bene</forenames></author></authors><title>Acyclic edge colourings of graphs with large girth</title><categories>math.CO cs.DM</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An edge colouring of a graph $G$ is called acyclic if it is proper and every
cycle contains at least three colours. We show that for every $\varepsilon&gt;0$,
there exists a $g=g(\varepsilon)$ such that if $G$ has girth at least $g$ then
$G$ admits an acyclic edge colouring with at most $(1+\varepsilon)\Delta$
colours.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3049</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3049</id><created>2014-11-11</created><authors><author><keyname>Kabir</keyname><forenames>Md. Humaun</forenames></author><author><keyname>Kwak</keyname><forenames>Kyung Sup</forenames></author></authors><title>Enhanced Modulation Technique for Molecular Communication: OOMoSK</title><categories>cs.IT cs.ET math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Molecular communication in nanonetworks is an emerging communication paradigm
where molecules are used as information carriers. Concentration Shift Keying
(CSK) and Molecule Shift Keying (MoSK) are being studied extensively for the
short and medium range molecular nanonetworks. It is observed that MoSK
outperforms CSK. However, MoSK requires different types of molecules for
encoding which render transmitter and receiver complexities. We propose a
modulation scheme called On-Off MoSK (OOMoSK) in which, molecules are released
for information bit 1 and no molecule is released for 0. The proposed scheme
enjoys reduced number of the types of molecules for encoding. Numerical results
show that the proposed scheme enhances channel capacity and Symbol Error Rate
(SER).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3061</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3061</id><created>2014-11-11</created><authors><author><keyname>Zeng</keyname><forenames>Yong</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Full-Duplex Wireless-Powered Relay with Self-Energy Recycling</title><categories>cs.IT math.IT</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter studies a wireless-powered amplify-and-forward relaying system,
where an energy-constrained relay node assists the information transmission
from the source to the destination using the energy harvested from the source.
We propose a novel two-phase protocol for efficient energy transfer and
information relaying, in which the relay operates in full-duplex mode with
simultaneous energy harvesting and information transmission. Compared with the
existing protocols, the proposed design possesses two main advantages: i) it
ensures uninterrupted information transmission since no time switching or power
splitting is needed at the relay for energy harvesting; ii) it enables the
so-called self-energy recycling, i.e., part of the energy (loop energy) that is
used for information transmission by the relay can be harvested and reused in
addition to the dedicated energy sent by the source. Under the multiple-input
single-output (MISO) channel setup, the optimal power allocation and
beamforming design at the relay are derived. Numerical results show a
significant throughput gain achieved by our proposed design over the existing
time switching-based relay protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3071</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3071</id><created>2014-11-12</created><updated>2014-11-14</updated><authors><author><keyname>Kumar</keyname><forenames>Sunil</forenames></author><author><keyname>Ranjan</keyname><forenames>Priya</forenames></author><author><keyname>Radhakrishnan</keyname><forenames>R.</forenames></author></authors><title>EMEEDP: Enhanced Multi-hop Energy Efficient Distributed Protocol for
  Heterogeneous Wireless Sensor Network</title><categories>cs.NI</categories><comments>6 pages, 4 figures. arXiv admin note: substantial text overlap with
  arXiv:1409.1412 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In WSN (Wireless Sensor Network) every sensor node sensed the data and
transmit it to the CH (Cluster head) or BS (Base Station). Sensors are randomly
deployed in unreachable areas, where battery replacement or battery charge is
not possible. For this reason, Energy conservation is the important design goal
while developing a routing and distributed protocol to increase the lifetime of
WSN. In this paper, an enhanced energy efficient distributed protocol for
heterogeneous WSN have been reported. EMEEDP is proposed for heterogeneous WSN
to increase the lifetime of the network. An efficient algorithm is proposed in
the form of flowchart and based on various clustering equation proved that the
proposed work accomplishes longer lifetime with improved QOS parameters
parallel to MEEP. A WSN implemented and tested using Raspberry Pi devices as a
base station, temperature sensors as a node and xively.com as a cloud. Users
use data for decision purpose or business purposes from xively.com using
internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3084</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3084</id><created>2014-11-12</created><authors><author><keyname>Zhao</keyname><forenames>Jichang</forenames></author><author><keyname>Liang</keyname><forenames>Xiao</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author></authors><title>Competition Between Homophily and Information Entropy Maximization in
  Social Networks</title><categories>cs.SI physics.soc-ph</categories><doi>10.1371/journal.pone.0136896</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In social networks, it is conventionally thought that two individuals with
more overlapped friends tend to establish a new friendship, which could be
stated as homophily breeding new connections. While the recent hypothesis of
maximum information entropy is presented as the possible origin of effective
navigation in small-world networks. We find there exists a competition between
information entropy maximization and homophily in local structure through both
theoretical and experimental analysis. This competition means that a newly
built relationship between two individuals with more common friends would lead
to less information entropy gain for them. We conjecture that in the evolution
of the social network, both of the two assumptions coexist. The rule of maximum
information entropy produces weak ties in the network, while the law of
homophily makes the network highly clustered locally and the individuals would
obtain strong and trust ties. Our findings shed light on the social network
modeling from a new perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3089</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3089</id><created>2014-11-12</created><authors><author><keyname>Saha</keyname><forenames>Amal</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Application Layer Intrusion Detection with Combination of Explicit-Rule-
  Based and Machine Learning Algorithms and Deployment in Cyber- Defence
  Program</title><categories>cs.CR cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There have been numerous works on network intrusion detection and prevention
systems, but work on application layer intrusion detection and prevention is
rare and not very mature. Intrusion detection and prevention at both network
and application layers are important for cyber-security and enterprise system
security. Since application layer intrusion is increasing day by day, it is
imperative to give adequate attention to it and use state-of-the-art algorithms
for effective detection and prevention. This paper talks about current state of
application layer intrusion detection and prevention capabilities in commercial
and open-source space and provides a path for evolution to more mature state
that will address not only enterprise system security, but also national
cyber-defence. Scalability and cost-effectiveness were important factors which
shaped the proposed solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3107</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3107</id><created>2014-11-12</created><authors><author><keyname>Ren</keyname><forenames>Xiaoqiang</forenames></author><author><keyname>Chen</keyname><forenames>Jiming</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author><author><keyname>Shi</keyname><forenames>Ling</forenames></author></authors><title>Quickest Change Detection with a Censoring Sensor in the Minimax Setting</title><categories>cs.SY cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of quickest change detection with a wireless sensor node is
studied in this paper. The sensor that is deployed to monitor the environment
has limited energy constraint to the classical quickest change detection
problem. We consider the &quot;censoring&quot; strategy at the sensor side, i.e., the
sensor selectively sends its observations to the decision maker. The quickest
change detection problem is formulated in a minimax way. In particular, our
goal is to find the optimal censoring strategy and stopping time such that the
detection delay is minimized subject to constraints on both average run length
(ARL) and average energy cost before the change. We show that the censoring
strategy that has the maximal post-censoring Kullback-Leibler (K-L) divergence
coupled with Cumulative Sum (CuSum) and Shiryaev-Roberts-Pollak (SRP) detection
procedure is asymptotically optimal for the Lorden's and Pollak's problem as
the ARL goes to infinity, respectively. We also show that the asymptotically
optimal censoring strategy should use up the available energy and has a very
special structure, i.e., the likelihood ratio of the no send region is a single
interval, which can be utilized to significantly reduce the computational
complexity. Numerical examples are shown to illustrate our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3111</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3111</id><created>2014-11-12</created><authors><author><keyname>Mishra</keyname><forenames>Amit Kumar</forenames></author><author><keyname>Kumar</keyname><forenames>Abhishek</forenames></author><author><keyname>Deb</keyname><forenames>Dipankar</forenames></author></authors><title>Prefrontal Cortex Motivated Cognitive Architecture for Multiple Robots</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a cerebral cortex inspired architecture for
robots in which we have mapped hierarchical cortical representation of human
brain to logic flow and decision making process. Our work focuses on the two
major features of human cognitive process, viz. the perception action cycle and
its hierarchical organization, and the decision making process. To prove the
effectiveness of our proposed method, we incorporated this architecture in our
robot which we named as Cognitive Insect Robot inspired by Brain Architecture
(CIRBA). We have extended our research to the implementation of this cognitive
architecture of CIRBA in multiple robots and have analyzed the level of
cognition attained by them
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3124</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3124</id><created>2014-11-12</created><authors><author><keyname>Bhavani</keyname><forenames>A B</forenames></author></authors><title>Cross Site Request Forgery on Android WebView</title><categories>cs.CR</categories><journal-ref>International Journal for Computer Science and Network Volume 3
  Issue 3 June 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Android has always been about connectivity and providing great browsing
experience. Web-based content can be embedded into the Android application
using WebView. It is a User Interface component that displays webpages. It can
either display a remote webpage or can also load static HTML data. This
encompasses the functionality of a browser that can be integrated to
application. WebView provides a number of APIs which enables the applications
to interact with the web content inside WebView. In the current paper Cross
site request forgery or XSRF attack specific to android WebView is
investigated. In XSRF attack the trusts of a web application in its
authenticated users is exploited by letting the attacker make arbitrary HTTP
requests on behalf of a victim user. When the user is logged into the trusted
site through the WebView the site authenticates the WebView and not
application. The application can launch attacks on the behalf of user with the
APIs of Webview exploiting user credentials resulting in Cross site request
forgery. Attacks can also be launched by setting cookies as HTTP headers and
making malicious HTTP Request on behalf of victim.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3128</identifier>
 <datestamp>2014-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3128</id><created>2014-11-12</created><updated>2014-12-10</updated><authors><author><keyname>Kotzias</keyname><forenames>Dimitrios</forenames></author><author><keyname>Denil</keyname><forenames>Misha</forenames></author><author><keyname>Blunsom</keyname><forenames>Phil</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author></authors><title>Deep Multi-Instance Transfer Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach for transferring knowledge from groups to
individuals that comprise them. We evaluate our method in text, by inferring
the ratings of individual sentences using full-review ratings. This approach,
which combines ideas from transfer learning, deep learning and multi-instance
learning, reduces the need for laborious human labelling of fine-grained data
when abundant labels are available at the group level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3130</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3130</id><created>2014-11-12</created><updated>2015-02-23</updated><authors><author><keyname>Blaszczyszyn</keyname><forenames>Bartlomiej</forenames></author><author><keyname>Muhlethaler</keyname><forenames>Paul</forenames></author></authors><title>Interference and SINR coverage in spatial non-slotted Aloha networks</title><categories>cs.NI math.PR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1002.1629;
  Annales des telecommunications -- annals of telecommunications. Publised
  online 19 February 2015</comments><journal-ref>Annales des t\'el\'ecommunications- Annals of telecommunications,
  2015, 70 (7), pp.345-358</journal-ref><doi>10.1007/s12243-014-0455-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose two analytically tractable stochastic-geometric
models of interference in ad-hoc networks using pure (non-slotted) Aloha as the
medium access. In contrast the slotted model, the interference in pure Aloha
may vary during the transmission of a tagged packet. We develop closed form
expressions for the Laplace transform of the empirical average of the
interference experienced during the transmission of a typical packet. Both
models assume a power-law path-loss function with arbitrarily distributed
fading and feature configurations of transmitters randomly located in the
Euclidean plane according to a Poisson point process. Depending on the model,
these configurations vary over time or are static. We apply our analysis of the
interference to study the Signal-to-Interference-and-Noise Ratio (SINR) outage
probability for a typical transmission in pure Aloha. The results are used to
compare the performance of non-slotted Aloha to the slotted one, which has
almost exclusively been previously studied in the same context of mobile ad-hoc
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3140</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3140</id><created>2014-11-12</created><updated>2014-11-19</updated><authors><author><keyname>Llorente</keyname><forenames>Alejandro</forenames></author><author><keyname>Garcia-Herranz</keyname><forenames>Manuel</forenames></author><author><keyname>Cebrian</keyname><forenames>Manuel</forenames></author><author><keyname>Moro</keyname><forenames>Esteban</forenames></author></authors><title>Social media fingerprints of unemployment</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>19 pages (8 main article, 11 Supplementary Information)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent wide-spread adoption of electronic and pervasive technologies has
enabled the study of human behavior at an unprecedented level, uncovering
universal patterns underlying human activity, mobility, and inter-personal
communication. In the present work, we investigate whether deviations from
these universal patterns may reveal information about the socio-economical
status of geographical regions. We quantify the extent to which deviations in
diurnal rhythm, mobility patterns, and communication styles across regions
relate to their unemployment incidence. For this we examine a country-scale
publicly articulated social media dataset, where we quantify individual
behavioral features from over 145 million geo-located messages distributed
among more than 340 different Spanish economic regions, inferred by computing
communities of cohesive mobility fluxes. We find that regions exhibiting more
diverse mobility fluxes, earlier diurnal rhythms, and more correct grammatical
styles display lower unemployment rates. As a result, we provide a simple model
able to produce accurate, easily interpretable reconstruction of regional
unemployment incidence from their social-media digital fingerprints alone. Our
results show that cost-effective economical indicators can be built based on
publicly-available social media datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3146</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3146</id><created>2014-11-12</created><authors><author><keyname>Hermann</keyname><forenames>Karl Moritz</forenames></author></authors><title>Distributed Representations for Compositional Semantics</title><categories>cs.CL</categories><comments>DPhil Thesis, University of Oxford, Submitted and accepted in 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mathematical representation of semantics is a key issue for Natural
Language Processing (NLP). A lot of research has been devoted to finding ways
of representing the semantics of individual words in vector spaces.
Distributional approaches --- meaning distributed representations that exploit
co-occurrence statistics of large corpora --- have proved popular and
successful across a number of tasks. However, natural language usually comes in
structures beyond the word level, with meaning arising not only from the
individual words but also the structure they are contained in at the phrasal or
sentential level. Modelling the compositional process by which the meaning of
an utterance arises from the meaning of its parts is an equally fundamental
task of NLP.
  This dissertation explores methods for learning distributed semantic
representations and models for composing these into representations for larger
linguistic units. Our underlying hypothesis is that neural models are a
suitable vehicle for learning semantically rich representations and that such
representations in turn are suitable vehicles for solving important tasks in
natural language processing. The contribution of this thesis is a thorough
evaluation of our hypothesis, as part of which we introduce several new
approaches to representation learning and compositional semantics, as well as
multiple state-of-the-art models which apply distributed semantic
representations to various tasks in NLP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3159</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3159</id><created>2014-11-12</created><updated>2014-11-14</updated><authors><author><keyname>Simon</keyname><forenames>Marcel</forenames></author><author><keyname>Rodner</keyname><forenames>Erik</forenames></author><author><keyname>Denzler</keyname><forenames>Joachim</forenames></author></authors><title>Part Detector Discovery in Deep Convolutional Neural Networks</title><categories>cs.CV</categories><comments>Accepted for publication on Asian Conference on Computer Vision
  (ACCV) 2014</comments><acm-class>I.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current fine-grained classification approaches often rely on a robust
localization of object parts to extract localized feature representations
suitable for discrimination. However, part localization is a challenging task
due to the large variation of appearance and pose. In this paper, we show how
pre-trained convolutional neural networks can be used for robust and efficient
object part discovery and localization without the necessity to actually train
the network on the current dataset. Our approach called &quot;part detector
discovery&quot; (PDD) is based on analyzing the gradient maps of the network outputs
and finding activation centers spatially related to annotated semantic parts or
bounding boxes.
  This allows us not just to obtain excellent performance on the CUB200-2011
dataset, but in contrast to previous approaches also to perform detection and
bird classification jointly without requiring a given bounding box annotation
during testing and ground-truth parts during training. The code is available at
http://www.inf-cv.uni-jena.de/part_discovery and
https://github.com/cvjena/PartDetectorDisovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3164</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3164</id><created>2014-11-12</created><updated>2015-11-13</updated><authors><author><keyname>Lin</keyname><forenames>Anthony Widjaja</forenames></author><author><keyname>Zhou</keyname><forenames>Sanming</forenames></author></authors><title>A linear time algorithm for the orbit problem over cyclic groups</title><categories>cs.CC cs.DS cs.LO</categories><comments>Accepted in Acta Informatica in Nov 2015</comments><msc-class>68N30, 11A05, 11Y16, 68Q25, 68Q60, 68Q85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The orbit problem is at the heart of symmetry reduction methods for model
checking concurrent systems. It asks whether two given configurations in a
concurrent system (represented as finite strings over some finite alphabet) are
in the same orbit with respect to a given finite permutation group (represented
by their generators) acting on this set of configurations by permuting indices.
It is known that the problem is in general as hard as the graph isomorphism
problem, whose precise complexity (whether it is solvable in polynomial-time)
is a long-standing open problem. In this paper, we consider the restriction of
the orbit problem when the permutation group is cyclic (i.e. generated by a
single permutation), an important restriction of the problem. It is known that
this subproblem is solvable in polynomial-time. Our main result is a
linear-time algorithm for this subproblem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3169</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3169</id><created>2014-11-12</created><authors><author><keyname>Ghaderi</keyname><forenames>Ali</forenames></author></authors><title>On Coarse Graining of Information and Its Application to Pattern
  Recognition</title><categories>cs.CV stat.ML</categories><doi>10.1063/1.4906011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method based on finite mixture models for classifying a set of
observations into number of different categories. In order to demonstrate the
method, we show how the component densities for the mixture model can be
derived by using the maximum entropy method in conjunction with conservation of
Pythagorean means. Several examples of distributions belonging to the
Pythagorean family are derived. A discussion on estimation of model parameters
and the number of categories is also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3197</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3197</id><created>2014-11-11</created><authors><author><keyname>Singh</keyname><forenames>Karamjit</forenames></author><author><keyname>Agarwal</keyname><forenames>Puneet</forenames></author><author><keyname>Shroff</keyname><forenames>Gautam</forenames></author></authors><title>Warranty Cost Estimation Using Bayesian Network</title><categories>cs.AI cs.LG</categories><comments>Selected for publication in Poster Proceedings of &quot;Industrial
  Conference on Data Mining (ICDM 2014)&quot;
  http://www.data-mining-forum.de/files/Program%202014%20DIN%20Lang.pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All multi-component product manufacturing companies face the problem of
warranty cost estimation. Failure rate analysis of components plays a key role
in this problem. Data source used for failure rate analysis has traditionally
been past failure data of components. However, failure rate analysis can be
improved by means of fusion of additional information, such as symptoms
observed during after-sale service of the product, geographical information
(hilly or plains areas), and information from tele-diagnostic analytics. In
this paper, we propose an approach, which learns dependency between
part-failures and symptoms gleaned from such diverse sources of information, to
predict expected number of failures with better accuracy. We also indicate how
the optimum warranty period can be computed. We demonstrate, through empirical
results, that our method can improve the warranty cost estimates significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3201</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3201</id><created>2014-11-12</created><authors><author><keyname>Srinivasan</keyname><forenames>Swetha P. T.</forenames></author><author><keyname>Bellur</keyname><forenames>Umesh</forenames></author></authors><title>Novel Power and Completion Time Models for Virtualized Environments</title><categories>cs.DC</categories><acm-class>C.0; D.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power consumption costs takes upto half of operational expenses of
datacenters making power management a critical concern. Advances in processor
technology provide fine-grained control over operating frequency and voltage of
processors and this control can be used to tradeoff power for performance.
Although many power and performance models exist, they have a significant error
margin while predicting the performance of memory or file-intensive tasks and
HPC applications. Our investigations reveal that the prediction error is due in
part to the fact that they do not take frequency AND CPU variations account,
rather they just depend on the CPU by itself.
  In this paper, we empirically derive power and completion time models using
linear regression with CPU utilization and operating frequency as parameters.
We validate our power model on several Intel and AMD processors by predicting
within 2-7% of measured power. We validate our completion time model using five
kernels of NASA Parallel Benchmark suite and five CPU, memory and
file-intensive benchmarks on four heterogeneous systems and predicting within
1-6% of observed performance. We then show how these models can be employed to
realize as much as 15% savings in power while delivering 44% better performance
for applications deployed in a virtualized environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3212</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3212</id><created>2014-11-12</created><authors><author><keyname>Lettich</keyname><forenames>Francesco</forenames></author><author><keyname>Orlando</keyname><forenames>Salvatore</forenames></author><author><keyname>Silvestri</keyname><forenames>Claudio</forenames></author><author><keyname>Jensen</keyname><forenames>Christian S.</forenames></author></authors><title>Manycore processing of repeated range queries over massive moving
  objects observations</title><categories>cs.DB cs.DC cs.DS</categories><acm-class>D.1.3; C.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to timely process significant amounts of continuously updated
spatial data is mandatory for an increasing number of applications. Parallelism
enables such applications to face this data-intensive challenge and allows the
devised systems to feature low latency and high scalability. In this paper we
focus on a specific data-intensive problem, concerning the repeated processing
of huge amounts of range queries over massive sets of moving objects, where the
spatial extents of queries and objects are continuously modified over time. To
tackle this problem and significantly accelerate query processing we devise a
hybrid CPU/GPU pipeline that compresses data output and save query processing
work. The devised system relies on an ad-hoc spatial index leading to a problem
decomposition that results in a set of independent data-parallel tasks. The
index is based on a point-region quadtree space decomposition and allows to
tackle effectively a broad range of spatial object distributions, even those
very skewed. Also, to deal with the architectural peculiarities and limitations
of the GPUs, we adopt non-trivial GPU data structures that avoid the need of
locked memory accesses and favour coalesced memory accesses, thus enhancing the
overall memory throughput. To the best of our knowledge this is the first work
that exploits GPUs to efficiently solve repeated range queries over massive
sets of continuously moving objects, characterized by highly skewed spatial
distributions. In comparison with state-of-the-art CPU-based implementations,
our method highlights significant speedups in the order of 14x-20x, depending
on the datasets, even when considering very cheap GPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3214</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3214</id><created>2014-11-12</created><authors><author><keyname>Servia-Rodr&#xed;guez</keyname><forenames>Sandra</forenames></author><author><keyname>Huberman</keyname><forenames>Bernardo A.</forenames></author><author><keyname>Asur</keyname><forenames>Sitaram</forenames></author></authors><title>Deciding what to display: maximizing the information value of social
  media</title><categories>cs.CY cs.HC cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In information-rich environments, the competition for users' attention leads
to a flood of content from which people often find hard to sort out the most
relevant and useful pieces. Using Twitter as a case study, we applied an
attention economy solution to generate the most informative tweets for its
users. By considering the novelty and popularity of tweets as objective
measures of their relevance and utility, we used the Huberman-Wu algorithm to
automatically select the ones that will receive the most attention in the next
time interval. Their predicted popularity was confirmed by using Twitter data
collected for a period of 2 months.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3224</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3224</id><created>2014-11-12</created><updated>2015-09-01</updated><authors><author><keyname>Korda</keyname><forenames>Nathaniel</forenames></author><author><keyname>Prashanth</keyname><forenames>L. A.</forenames></author></authors><title>On TD(0) with function approximation: Concentration bounds and a
  centered variant with exponential convergence</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide non-asymptotic bounds for the well-known temporal difference
learning algorithm TD(0) with linear function approximators. These include
high-probability bounds as well as bounds in expectation. Our analysis suggests
that a step-size inversely proportional to the number of iterations cannot
guarantee optimal rate of convergence unless we assume (partial) knowledge of
the stationary distribution for the Markov chain underlying the policy
considered. We also provide bounds for the iterate averaged TD(0) variant,
which gets rid of the step-size dependency while exhibiting the optimal rate of
convergence. Furthermore, we propose a variant of TD(0) with linear
approximators that incorporates a centering sequence, and establish that it
exhibits an exponential rate of convergence in expectation. We demonstrate the
usefulness of our bounds on two synthetic experimental settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3229</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3229</id><created>2014-11-12</created><updated>2015-01-13</updated><authors><author><keyname>Cao</keyname><forenames>Tian</forenames></author><author><keyname>Zach</keyname><forenames>Christopher</forenames></author><author><keyname>Modla</keyname><forenames>Shannon</forenames></author><author><keyname>Powell</keyname><forenames>Debbie</forenames></author><author><keyname>Czymmek</keyname><forenames>Kirk</forenames></author><author><keyname>Niethammer</keyname><forenames>Marc</forenames></author></authors><title>Multi-modal Image Registration for Correlative Microscopy</title><categories>cs.CV</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correlative microscopy is a methodology combining the functionality of light
microscopy with the high resolution of electron microscopy and other microscopy
technologies. Image registration for correlative microscopy is quite
challenging because it is a multi-modal, multi-scale and multi-dimensional
registration problem. In this report, I introduce two methods of image
registration for correlative microscopy. The first method is based on fiducials
(beads). I generate landmarks from the fiducials and compute the similarity
transformation matrix based on three pairs of nearest corresponding landmarks.
A least-squares matching process is applied afterwards to further refine the
registration. The second method is inspired by the image analogies approach. I
introduce the sparse representation model into image analogies. I first train
representative image patches (dictionaries) for pre-registered datasets from
two different modalities, and then I use the sparse coding technique to
transfer a given image to a predicted image from one modality to another based
on the learned dictionaries. The final image registration is between the
predicted image and the original image corresponding to the given image in the
different modality. The method transforms a multi-modal registration problem to
a mono-modal one. I test my approaches on Transmission Electron Microscopy
(TEM) and confocal microscopy images. Experimental results of the methods are
also shown in this report.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3230</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3230</id><created>2014-11-12</created><updated>2014-12-06</updated><authors><author><keyname>Mairal</keyname><forenames>Julien</forenames><affiliation>Inria</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>Inria</affiliation></author><author><keyname>Ponce</keyname><forenames>Jean</forenames><affiliation>Ecole Normale Sup&#xe9;rieure</affiliation></author></authors><title>Sparse Modeling for Image and Vision Processing</title><categories>cs.CV</categories><comments>205 pages, to appear in Foundations and Trends in Computer Graphics
  and Vision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, a large amount of multi-disciplinary research has been
conducted on sparse models and their applications. In statistics and machine
learning, the sparsity principle is used to perform model selection---that is,
automatically selecting a simple model among a large collection of them. In
signal processing, sparse coding consists of representing data with linear
combinations of a few dictionary elements. Subsequently, the corresponding
tools have been widely adopted by several scientific communities such as
neuroscience, bioinformatics, or computer vision. The goal of this monograph is
to offer a self-contained view of sparse modeling for visual recognition and
image processing. More specifically, we focus on applications where the
dictionary is learned and adapted to data, yielding a compact representation
that has been successful in various contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3251</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3251</id><created>2014-11-12</created><authors><author><keyname>Omkar</keyname><forenames>S. N.</forenames></author><author><keyname>Mudigere</keyname><forenames>Dheevatsa</forenames></author><author><keyname>Senthilnath</keyname><forenames>J</forenames></author><author><keyname>Kumar</keyname><forenames>M. Vijaya</forenames></author></authors><title>Identification of Helicopter Dynamics based on Flight Data using Nature
  Inspired Techniques</title><categories>cs.CE cs.NE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The complexity of helicopter flight dynamics makes modeling and helicopter
system identification a very difficult task. Most of the traditional techniques
require a model structure to be defined apriori and in case of helicopter
dynamics, this is difficult due to its complexity and the interplay between
various subsystems.To overcome this difficulty, non-parametric approaches are
commonly adopted for helicopter system identification. Artificial Neural
Network are a widely used class of algorithms for non-parametric system
identification, among them, the Nonlinear Auto Regressive eXogeneous input
network (NARX) model is very popular, but it also necessitates some in depth
knowledge regarding the system being modeled. There have been many approaches
proposed to circumvent this and yet still retain the advantageous
characteristics. In this paper we carry out an extensive study of one such
newly proposed approach using a modified NARX model with a two tiered,
externally driven recurrent neural network architecture. This is coupled with
an outer optimization routine for evolving the order of the system. This
generic architecture is comprehensively explored to ascertain its usability and
critically asses its potential. Different instantiations of this architecture,
based on nature inspired computational techniques (Artificial Bee Colony,
Artificial Immune System and Particle Swarm Optimization) are evaluated and
critically compared in this paper. Simulations have been carried out for
identifying the longitudinally uncoupled dynamics. Results of identification
indicate a quite close correlation between the actual and the predicted
response of the helicopter for all the models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3271</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3271</id><created>2014-11-12</created><authors><author><keyname>Wu</keyname><forenames>Yueping</forenames></author><author><keyname>Cui</keyname><forenames>Ying</forenames></author><author><keyname>Clerckx</keyname><forenames>Bruno</forenames></author></authors><title>Analysis and Optimization of Inter-tier Interference Coordination in
  Downlink Multi-Antenna HetNets with Offloading</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous networks (HetNets) with offloading is considered as an
effective way to meet the high data rate demand of future wireless service.
However, the offloaded users suffer from strong inter-tier interference, which
reduces the benefits of offloading and is one of the main limiting factors of
the system performance. In this paper, we investigate an interference nulling
(IN) scheme in improving the system performance by carefully managing the
inter-tier interference to the offloaded users in downlink two-tier HetNets
with multi-antenna base stations. Utilizing tools from stochastic geometry, we
first derive a tractable expression for the rate coverage probability of the IN
scheme. Then, by studying its order, we obtain the optimal design parameter,
i.e., the degrees of freedom that can be used for IN, to maximize the rate
coverage probability. Finally, we analyze the rate coverage probabilities of
the simple offloading scheme without interference management and the
multi-antenna version of the almost blank subframes (ABS) scheme in 3GPP LTE,
and compare the performance of the IN scheme with these two schemes. Both
analytical and numerical results show that the IN scheme can achieve good
performance gains over both of these two schemes, especially in the large
antenna regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3273</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3273</id><created>2014-11-12</created><authors><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>Efficiency of Matrix Multiplication on the Cross-Wired Mesh Array</title><categories>cs.DC</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note looks at the efficiency of the cross-wired mesh array in the
context of matrix multiplication. It is shown that in case of repeated
operations, the average number of steps to multiply sets of nxn matrices on a
2D cross-wired mesh array approaches n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3277</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3277</id><created>2014-11-12</created><authors><author><keyname>Ismkhan</keyname><forenames>Hassan</forenames></author></authors><title>Using Ants as a Genetic Crossover Operator in GLS to Solve STSP</title><categories>cs.NE</categories><comments>2010 International Conference of Soft Computing and Pattern
  Recognition</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Ant Colony Algorithm (ACA) and Genetic Local Search (GLS) are two
optimization algorithms that have been successfully applied to the Traveling
Salesman Problem (TSP). In this paper we define new crossover operator then
redefine ACAs ants as operate according to defined crossover operator then put
forward our GLS that uses these ants to solve Symmetric TSP (STSP) instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3285</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3285</id><created>2014-11-12</created><updated>2015-06-10</updated><authors><author><keyname>Welk</keyname><forenames>Martin</forenames></author></authors><title>Amoeba Techniques for Shape and Texture Analysis</title><categories>cs.CV</categories><comments>38 pages, 19 figures v2: minor corrections and rephrasing, Section 5
  (pre-smoothing) extended</comments><acm-class>I.4.3; I.4.6; I.2.10; G.1.8; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Morphological amoebas are image-adaptive structuring elements for
morphological and other local image filters introduced by Lerallut et al. Their
construction is based on combining spatial distance with contrast information
into an image-dependent metric. Amoeba filters show interesting parallels to
image filtering methods based on partial differential equations (PDEs), which
can be confirmed by asymptotic equivalence results. In computing amoebas, graph
structures are generated that hold information about local image texture. This
paper reviews and summarises the work of the author and his coauthors on
morphological amoebas, particularly their relations to PDE filters and texture
analysis. It presents some extensions and points out directions for future
investigation on the subject.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3292</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3292</id><created>2014-11-12</created><updated>2015-10-09</updated><authors><author><keyname>Vazquez-Vilar</keyname><forenames>Gonzalo</forenames></author><author><keyname>Campo</keyname><forenames>Adri&#xe0; Tauste</forenames></author><author><keyname>F&#xe0;bregas</keyname><forenames>Albert Guill&#xe9;n i</forenames></author><author><keyname>Martinez</keyname><forenames>Alfonso</forenames></author></authors><title>Bayesian M-ary Hypothesis Testing: The Meta-Converse and Verd\'u-Han
  Bounds are Tight</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><msc-class>62C05, 94A13, 94A15,</msc-class><acm-class>G.3; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two alternative exact characterizations of the minimal error probability of
Bayesian M-ary hypothesis testing are derived. The first expression corresponds
the error probability in an induced binary hypothesis test and implies the
tightness of the meta-converse bound by Polyanskiy, Poor and Verd\'u; the
second expression implies the tightness of a generalized Verd\'u-Han lower
bound. The expressions help to characterize the minimal error probability of
several problems in information theory and to identify the steps where existing
converse bounds are loose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3294</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3294</id><created>2014-10-13</created><authors><author><keyname>Shree</keyname><forenames>S. Udhaya</forenames></author><author><keyname>Basha</keyname><forenames>Dr. M. S. Saleem</forenames></author></authors><title>An exhaustive survey of trust models in p2p network</title><categories>cs.CR cs.NI</categories><comments>12 pages, 4 figures, 1 table, International Journal on Web Service
  Computing (IJWSC), Vol.5, No.3, September 2014</comments><doi>10.5121/ijwsc.2014.5301</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Most of the peers accessing the services are under the assumption that the
service accessed in a P2P network is utmost secured. By means of prevailing
hard security mechanisms, security goals like authentication, authorization,
privacy, non repudiation of services and other hard security issues are
resolved. But these mechanisms fail to provide soft security. An exhaustive
survey of existing trust and reputation models in P2P network regarding service
provisioning is presented and challenges are listed.p2p Trust issues like trust
bootstrapping, trust evidence procurement, trust assessment, trust interaction
outcome evaluation and other trust based classification of peers behaviour into
trusted, inconsistent, un trusted, malicious, betraying, redemptive are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3302</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3302</id><created>2014-11-12</created><authors><author><keyname>Sarkar</keyname><forenames>Chandrima</forenames></author><author><keyname>Roy</keyname><forenames>Atanu</forenames></author></authors><title>Using Gaussian Measures for Efficient Constraint Based Clustering</title><categories>cs.LG cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a novel iterative multiphase clustering technique
for efficiently clustering high dimensional data points. For this purpose we
implement clustering feature (CF) tree on a real data set and a Gaussian
density distribution constraint on the resultant CF tree. The post processing
by the application of Gaussian density distribution function on the
micro-clusters leads to refinement of the previously formed clusters thus
improving their quality. This algorithm also succeeds in overcoming the
inherent drawbacks of conventional hierarchical methods of clustering like
inability to undo the change made to the dendogram of the data points.
Moreover, the constraint measure applied in the algorithm makes this clustering
technique suitable for need driven data analysis. We provide veracity of our
claim by evaluating our algorithm with other similar clustering algorithms.
Introduction
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3304</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3304</id><created>2014-11-12</created><authors><author><keyname>Pudlak</keyname><forenames>Pavel</forenames></author></authors><title>On the complexity of finding falsifying assignments for Herbrand
  disjunctions</title><categories>cs.LO math.LO</categories><msc-class>03D15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that $\Phi$ is a consistent sentence. Then there is no Herbrand proof
of $\neg \Phi$, which means that any Herbrand disjunction made from the prenex
form of $\neg \Phi$ is falsifiable. We show that the problem of finding such a
falsifying assignment is hard in the following sense. For every total
polynomial search problem $R$, there exists a consistent $\Phi$ such that
finding solutions to $R$ can be reduced to finding a falsifying assignment to
an Herbrand disjunction made from $\neg \Phi$. It has been conjectured that
there are no complete total polynomial search problems. If this conjecture is
true, then for every consistent sentence $\Phi$, there exists a consistence
sentence $\Psi$, such that the search problem associated with $\Psi$ cannot be
reduced to the search problem associated with $\Phi$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3305</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3305</id><created>2014-11-12</created><authors><author><keyname>Ghasemalizadeh</keyname><forenames>Omid</forenames></author><author><keyname>Taheri</keyname><forenames>Saied</forenames></author><author><keyname>Singh</keyname><forenames>Amandeep</forenames></author><author><keyname>Goryca</keyname><forenames>Jill</forenames></author></authors><title>Semi-active Suspension Control using Modern Methodology: Comprehensive
  Comparison Study</title><categories>cs.SY</categories><comments>2014 Ground Vehicle System Engineering and Technology Symposium
  (GVSETS), NDIA, August 12-14, 2014, Novi, Michigan, USA</comments><doi>10.13140/2.1.3319.0083</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semi-active suspensions have drawn particular attention due to their superior
performance over the other types of suspensions. One of their advantages is
that their damping coefficient can be controlled without the need for any
external source of power. In this study, three control approaches are
implemented on a quarter-car model using MATLAB/Simulink. The investigated
control methodologies are Acceleration Driven Damper, Power Driven Damper, and
H_infinity Robust Control. The three controllers are known as comfort-oriented
approaches. H_infinity Robust Control is an advanced method that guarantees
transient performance and rejects external disturbances. It is shown that
H_infinity with the proposed modification, has the best performance although
its relatively high cost of computation could be potentially considered as a
drawback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3312</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3312</id><created>2014-11-12</created><updated>2015-03-09</updated><authors><author><keyname>Sariyuce</keyname><forenames>Ahmet Erdem</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author><author><keyname>Catalyurek</keyname><forenames>Umit V.</forenames></author></authors><title>Finding the Hierarchy of Dense Subgraphs using Nucleus Decompositions</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding dense substructures in a graph is a fundamental graph mining
operation, with applications in bioinformatics, social networks, and
visualization to name a few. Yet most standard formulations of this problem
(like clique, quasiclique, k-densest subgraph) are NP-hard. Furthermore, the
goal is rarely to find the &quot;true optimum&quot;, but to identify many (if not all)
dense substructures, understand their distribution in the graph, and ideally
determine relationships among them. Current dense subgraph finding algorithms
usually optimize some objective, and only find a few such subgraphs without
providing any structural relations. We define the nucleus decomposition of a
graph, which represents the graph as a forest of nuclei. Each nucleus is a
subgraph where smaller cliques are present in many larger cliques. The forest
of nuclei is a hierarchy by containment, where the edge density increases as we
proceed towards leaf nuclei. Sibling nuclei can have limited intersections,
which enables discovering overlapping dense subgraphs. With the right
parameters, the nucleus decomposition generalizes the classic notions of
k-cores and k-truss decompositions. We give provably efficient algorithms for
nucleus decompositions, and empirically evaluate their behavior in a variety of
real graphs. The tree of nuclei consistently gives a global, hierarchical
snapshot of dense substructures, and outputs dense subgraphs of higher quality
than other state-of-the-art solutions. Our algorithm can process graphs with
tens of millions of edges in less than an hour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3315</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3315</id><created>2014-11-12</created><authors><author><keyname>Kulkarni</keyname><forenames>Vivek</forenames></author><author><keyname>Al-Rfou</keyname><forenames>Rami</forenames></author><author><keyname>Perozzi</keyname><forenames>Bryan</forenames></author><author><keyname>Skiena</keyname><forenames>Steven</forenames></author></authors><title>Statistically Significant Detection of Linguistic Change</title><categories>cs.CL cs.IR cs.LG</categories><comments>11 pages, 7 figures, 4 tables</comments><acm-class>H.3.3; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new computational approach for tracking and detecting
statistically significant linguistic shifts in the meaning and usage of words.
Such linguistic shifts are especially prevalent on the Internet, where the
rapid exchange of ideas can quickly change a word's meaning. Our meta-analysis
approach constructs property time series of word usage, and then uses
statistically sound change point detection algorithms to identify significant
linguistic shifts.
  We consider and analyze three approaches of increasing complexity to generate
such linguistic property time series, the culmination of which uses
distributional characteristics inferred from word co-occurrences. Using
recently proposed deep neural language models, we first train vector
representations of words for each time period. Second, we warp the vector
spaces into one unified coordinate system. Finally, we construct a
distance-based distributional time series for each word to track it's
linguistic displacement over time.
  We demonstrate that our approach is scalable by tracking linguistic change
across years of micro-blogging using Twitter, a decade of product reviews using
a corpus of movie reviews from Amazon, and a century of written books using the
Google Book-ngrams. Our analysis reveals interesting patterns of language usage
change commensurate with each medium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3317</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3317</id><created>2014-11-12</created><updated>2015-12-01</updated><authors><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Devroye</keyname><forenames>Luc</forenames></author><author><keyname>Lugosi</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>Finding Adam in random growing trees</title><categories>math.PR cs.DM cs.SI math.ST stat.TH</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate algorithms to find the first vertex in large trees generated
by either the uniform attachment or preferential attachment model. We require
the algorithm to output a set of $K$ vertices, such that, with probability at
least $1-\epsilon$, the first vertex is in this set. We show that for any
$\epsilon$, there exist such algorithms with $K$ independent of the size of the
input tree. Moreover, we provide almost tight bounds for the best value of $K$
as a function of $\epsilon$. In the uniform attachment case we show that the
optimal $K$ is subpolynomial in $1/\epsilon$, and that it has to be at least
superpolylogarithmic. On the other hand, the preferential attachment case is
exponentially harder, as we prove that the best $K$ is polynomial in
$1/\epsilon$. We conclude the paper with several open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3320</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3320</id><created>2014-11-12</created><authors><author><keyname>Ortiz</keyname><forenames>Luis E.</forenames></author></authors><title>On Sparse Discretization for Graphical Games</title><categories>cs.AI cs.GT</categories><comments>30 pages. Original research note drafted in Dec. 2002 and posted
  online Spring'03 (http://www.cis.upenn.
  edu/~mkearns/teaching/cgt/revised_approx_bnd.pdf) as part of a course on
  computational game theory taught by Prof. Michael Kearns at the University of
  Pennsylvania; First major revision sent to WINE'10; Current version sent to
  JAIR on April 25, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short paper concerns discretization schemes for representing and
computing approximate Nash equilibria, with emphasis on graphical games, but
briefly touching on normal-form and poly-matrix games. The main technical
contribution is a representation theorem that informally states that to account
for every exact Nash equilibrium using a nearby approximate Nash equilibrium on
a grid over mixed strategies, a uniform discretization size linear on the
inverse of the approximation quality and natural game-representation parameters
suffices. For graphical games, under natural conditions, the discretization is
logarithmic in the game-representation size, a substantial improvement over the
linear dependency previously required. The paper has five other objectives: (1)
given the venue, to highlight the important, but often ignored, role that work
on constraint networks in AI has in simplifying the derivation and analysis of
algorithms for computing approximate Nash equilibria; (2) to summarize the
state-of-the-art on computing approximate Nash equilibria, with emphasis on
relevance to graphical games; (3) to help clarify the distinction between
sparse-discretization and sparse-support techniques; (4) to illustrate and
advocate for the deliberate mathematical simplicity of the formal proof of the
representation theorem; and (5) to list and discuss important open problems,
emphasizing graphical-game generalizations, which the AI community is most
suitable to solve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3334</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3334</id><created>2014-11-12</created><updated>2015-07-10</updated><authors><author><keyname>Bacon</keyname><forenames>Dave</forenames></author><author><keyname>Flammia</keyname><forenames>Steven T.</forenames></author><author><keyname>Harrow</keyname><forenames>Aram W.</forenames></author><author><keyname>Shi</keyname><forenames>Jonathan</forenames></author></authors><title>Sparse Quantum Codes from Quantum Circuits</title><categories>quant-ph cs.IT math.IT</categories><comments>24 pages, 2 figures. v2. Improved presentation and fixed some proofs</comments><journal-ref>Proc. of STOC '15, pp. 327-334, 2015</journal-ref><doi>10.1145/2746539.2746608</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We describe a general method for turning quantum circuits into sparse quantum
subsystem codes. Using this prescription, we can map an arbitrary stabilizer
code into a new subsystem code with the same distance and number of encoded
qubits but where all the generators have constant weight, at the cost of adding
some ancilla qubits. With an additional overhead of ancilla qubits, the new
code can also be made spatially local.
  Applying our construction to certain concatenated stabilizer codes yields
families of subsystem codes with constant-weight generators and with minimum
distance $d = n^{1-\varepsilon}$, where $\varepsilon = O(1/\sqrt{\log n})$. For
spatially local codes in $D$ dimensions we nearly saturate a bound due to
Bravyi and Terhal and achieve $d = n^{1-\varepsilon-1/D}$. Previously the best
code distance achievable with constant-weight generators in any dimension, due
to Freedman, Meyer and Luo, was $O(\sqrt{n\log n})$ for a stabilizer code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3346</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3346</id><created>2014-11-12</created><authors><author><keyname>Verhodubs</keyname><forenames>Olegs</forenames></author></authors><title>Membership Function Assignment for Elements of Single OWL Ontology</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops the idea of membership function assignment for OWL (Web
Ontology Language) ontology elements in order to subsequently generate fuzzy
rules from this ontology. The task of membership function assignment for OWL
ontology elements had already been partially described, but this concerned the
case, when several OWL ontologies of the same domain were available, and they
were merged into a single ontology. The purpose of this paper is to present the
way of membership function assignment for OWL ontology elements in the case,
when there is the only one available ontology. Fuzzy rules, generated from the
OWL ontology, are necessary for supplement of the SWES (Semantic Web Expert
System) knowledge base. SWES is an expert system, which will be able to extract
knowledge from OWL ontologies, found in the Web, and will serve as a universal
expert for the user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3359</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3359</id><created>2014-10-23</created><authors><author><keyname>Zhu</keyname><forenames>Sanguo</forenames></author><author><keyname>Zhou</keyname><forenames>Youming</forenames></author><author><keyname>Sheng</keyname><forenames>Yongjian</forenames></author></authors><title>Asymptotics of the geometric mean error for in-homogeneous self-similar
  measures</title><categories>math.DS cs.IT math.FA math.IT</categories><msc-class>28A80, 28A78, 94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $(f_i)_{i=1}^N$ be a family of contractive similitudes on $\mathbb{R}^q$
satisfying the open set condition. Let $(p_i)_{i=0}^N$ be a probability vector
with $p_i&gt;0$ for all $i=0,1,\ldots,N$. We study the asymptotic geometric mean
errors $e_{n,0}(\mu),n\geq 1$, in the quantization for the in-homogeneous
self-similar measure $\mu$ associated with the condensation system
$((f_i)_{i=1}^N,(p_i)_{i=0}^N,\nu)$. We focus on the following two independent
cases: (I) $\nu$ is a self-similar measure on $\mathbb{R}^q$ associated with
$(f_i)_{i=1}^N$; (II) $\nu$ is a self-similar measure associated with another
family of contractive similitudes $(g_i)_{i=1}^M$ on $\mathbb{R}^q$ satisfying
the open set condition and $((f_i)_{i=1}^N,(p_i)_{i=0}^N,\nu)$ satisfies a
version of in-homogeneous open set condition. We show that, in both cases, the
quantization dimension $D_0(\mu)$ of $\mu$ of order zero exists and agrees with
that of $\nu$, which is independent of the probability vector $(p_i)_{i=0}^N$.
We determine the convergence order of $(e_{n,0}(\mu))_{n=1}^\infty$; namely,
for $D_0(\mu)=:d_0$, there exists a constant $D&gt;0$, such that \[
D^{-1}n^{-\frac{1}{d_0}}\leq e_{n,0}(\mu)\leq D n^{-\frac{1}{d_0}}, n\geq 1. \]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3374</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3374</id><created>2014-11-12</created><authors><author><keyname>Joglekar</keyname><forenames>Manas</forenames></author><author><keyname>Garcia-Molina</keyname><forenames>Hector</forenames></author><author><keyname>Parameswaran</keyname><forenames>Aditya</forenames></author><author><keyname>Re</keyname><forenames>Christopher</forenames></author></authors><title>Exploiting Correlations for Expensive Predicate Evaluation</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User Defined Function(UDFs) are used increasingly to augment query languages
with extra, application dependent functionality. Selection queries involving
UDF predicates tend to be expensive, either in terms of monetary cost or
latency. In this paper, we study ways to efficiently evaluate selection queries
with UDF predicates. We provide a family of techniques for processing queries
at low cost while satisfying user-specified precision and recall constraints.
Our techniques are applicable to a variety of scenarios including when
selection probabilities of tuples are available beforehand, when this
information is available but noisy, or when no such prior information is
available. We also generalize our techniques to more complex queries. Finally,
we test our techniques on real datasets, and show that they achieve significant
savings in cost of up to $80\%$, while incurring only a small reduction in
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3377</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3377</id><created>2014-11-12</created><authors><author><keyname>Joglekar</keyname><forenames>Manas</forenames></author><author><keyname>Garcia-Molina</keyname><forenames>Hector</forenames></author><author><keyname>Parameswaran</keyname><forenames>Aditya</forenames></author></authors><title>Comprehensive and Reliable Crowd Assessment Algorithms</title><categories>cs.DB</categories><comments>ICDE 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluating workers is a critical aspect of any crowdsourcing system. In this
paper, we devise techniques for evaluating workers by finding confidence
intervals on their error rates. Unlike prior work, we focus on
&quot;conciseness&quot;---that is, giving as tight a confidence interval as possible.
Conciseness is of utmost importance because it allows us to be sure that we
have the best guarantee possible on worker error rate. Also unlike prior work,
we provide techniques that work under very general scenarios, such as when not
all workers have attempted every task (a fairly common scenario in practice),
when tasks have non-boolean responses, and when workers have different biases
for positive and negative tasks. We demonstrate conciseness as well as accuracy
of our confidence intervals by testing them on a variety of conditions and
multiple real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3406</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3406</id><created>2014-11-12</created><updated>2016-02-15</updated><authors><author><keyname>Goldstein</keyname><forenames>Tom</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard</forenames></author></authors><title>A Field Guide to Forward-Backward Splitting with a FASTA Implementation</title><categories>cs.NA</categories><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-differentiable and constrained optimization play a key role in machine
learning, signal and image processing, communications, and beyond. For
high-dimensional minimization problems involving large datasets or many
unknowns, the forward-backward splitting method provides a simple, practical
solver. Despite its apparently simplicity, the performance of the
forward-backward splitting is highly sensitive to implementation details.
  This article is an introductory review of forward-backward splitting with a
special emphasis on practical implementation concerns. Issues like stepsize
selection, acceleration, stopping conditions, and initialization are
considered. Numerical experiments are used to compare the effectiveness of
different approaches.
  Many variations of forward-backward splitting are implemented in the solver
FASTA (short for Fast Adaptive Shrinkage/Thresholding Algorithm). FASTA
provides a simple interface for applying forward-backward splitting to a broad
range of problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3409</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3409</id><created>2014-11-12</created><authors><author><keyname>Mineiro</keyname><forenames>Paul</forenames></author><author><keyname>Karampatziakis</keyname><forenames>Nikos</forenames></author></authors><title>A Randomized Algorithm for CCA</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present RandomizedCCA, a randomized algorithm for computing canonical
analysis, suitable for large datasets stored either out of core or on a
distributed file system. Accurate results can be obtained in as few as two data
passes, which is relevant for distributed processing frameworks in which
iteration is expensive (e.g., Hadoop). The strategy also provides an excellent
initializer for standard iterative solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3410</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3410</id><created>2014-11-12</created><authors><author><keyname>Jang</keyname><forenames>Kwangchol</forenames></author><author><keyname>Han</keyname><forenames>Sokmin</forenames></author><author><keyname>Kim</keyname><forenames>Insong</forenames></author></authors><title>Person Re-identification Based on Color Histogram and Spatial
  Configuration of Dominant Color Regions</title><categories>cs.CV</categories><comments>12 pages, 6 figures</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  There is a requirement to determine whether a given person of interest has
already been observed over a network of cameras in video surveillance systems.
A human appearance obtained in one camera is usually different from the ones
obtained in another camera due to difference in illumination, pose and
viewpoint, camera parameters. Being related to appearance-based approaches for
person re-identification, we propose a novel method based on the dominant color
histogram and spatial configuration of dominant color regions on human body
parts. Dominant color histogram and spatial configuration of the dominant color
regions based on dominant color descriptor(DCD) can be considered to be robust
to illumination and pose, viewpoint changes. The proposed method is evaluated
using benchmark video datasets. Experimental results using the cumulative
matching characteristic(CMC) curve demonstrate the effectiveness of our
approach for person re-identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3413</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3413</id><created>2014-11-12</created><authors><author><keyname>Iwata</keyname><forenames>Tomoharu</forenames></author><author><keyname>Yamada</keyname><forenames>Makoto</forenames></author></authors><title>Multi-view Anomaly Detection via Probabilistic Latent Variable Models</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a nonparametric Bayesian probabilistic latent variable model for
multi-view anomaly detection, which is the task of finding instances that have
inconsistent views. With the proposed model, all views of a non-anomalous
instance are assumed to be generated from a single latent vector. On the other
hand, an anomalous instance is assumed to have multiple latent vectors, and its
different views are generated from different latent vectors. By inferring the
number of latent vectors used for each instance with Dirichlet process priors,
we obtain multi-view anomaly scores. The proposed model can be seen as a robust
extension of probabilistic canonical correlation analysis for noisy multi-view
data. We present Bayesian inference procedures for the proposed model based on
a stochastic EM algorithm. The effectiveness of the proposed model is
demonstrated in terms of performance when detecting multi-view anomalies and
imputing missing values in multi-view data with anomalies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3419</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3419</id><created>2014-11-12</created><authors><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author><author><keyname>Bavarian</keyname><forenames>Mohammad</forenames></author><author><keyname>Gao</keyname><forenames>Yihan</forenames></author><author><keyname>Mao</keyname><forenames>Jieming</forenames></author><author><keyname>Sun</keyname><forenames>Xiaoming</forenames></author><author><keyname>Zuo</keyname><forenames>Song</forenames></author></authors><title>Tighter Relations Between Sensitivity and Other Complexity Measures</title><categories>cs.CC</categories><comments>This is the merged form of arXiv submission 1306.4466 with another
  work. Appeared in ICALP 2014, 14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensitivity conjecture is a longstanding and fundamental open problem in the
area of complexity measures of Boolean functions and decision tree complexity.
The conjecture postulates that the maximum sensitivity of a Boolean function is
polynomially related to other major complexity measures. Despite much attention
to the problem and major advances in analysis of Boolean functions in the past
decade, the problem remains wide open with no positive result toward the
conjecture since the work of Kenyon and Kutin from 2004.
  In this work, we present new upper bounds for various complexity measures in
terms of sensitivity improving the bounds provided by Kenyon and Kutin.
Specifically, we show that deg(f)^{1-o(1)}=O(2^{s(f)}) and C(f) &lt; 2^{s(f)-1}
s(f); these in turn imply various corollaries regarding the relation between
sensitivity and other complexity measures, such as block sensitivity, via known
results. The gap between sensitivity and other complexity measures remains
exponential but these results are the first improvement for this difficult
problem that has been achieved in a decade.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3423</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3423</id><created>2014-11-12</created><authors><author><keyname>Hassan</keyname><forenames>Ghulam Mubashar</forenames></author><author><keyname>Dyskin</keyname><forenames>Arcady V.</forenames></author><author><keyname>MacNish</keyname><forenames>Cara K.</forenames></author></authors><title>A Comparative Study of Techniques of Distant Reconstruction of
  Displacement Fields by using DISTRESS Simulator</title><categories>cs.CV</categories><comments>in review process of a Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconstruction and monitoring of displacement and strain fields is an
important problem in engineering. We analyze the remote and non-obtrusive
methods of strain measurement based on photogrammetry and Digital Image
Correlation (DIC). The method is based on covering the photographed surface
with a pattern of speckles and comparing the images taken before and after the
deformation. In this study, a comprehensive literature review and comparative
analysis of photogrammetric solutions is presented. The analysis is based on a
specially developed Digital Image Synthesizer To Reconstruct Strain in Solids
(DISTRESS) Simulator to generate synthetic images of displacement and stress
fields in order to investigate the intrinsic accuracy of the existing variants
of DIC. We investigated the Basic DIC and a commercial software VIC 2D, both
based on displacement field reconstruction with post processing strain
determination based on numerical differentiation. We also investigated what we
call the Extended DIC where the strain field is determined independently of the
displacement field. While the Basic DIC and VIC 2D are faster, the Extended DIC
delivers the best accuracy of strain reconstruction. The speckle pattern is
found to be playing a critical role in achieving high accuracy for DIC.
Increase in subset size for DIC does not significantly improves the accuracy,
while the smallest subset size depends on the speckle pattern and speckle size.
Increase in the overall image size provides more details but does not play
significant role in improving the accuracy, while significantly increasing the
computation cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3425</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3425</id><created>2014-11-12</created><authors><author><keyname>Karami</keyname><forenames>Alireza</forenames></author><author><keyname>Attari</keyname><forenames>Mahmoud Ahmadian</forenames></author></authors><title>Novel LDPC Decoder via MLP Neural Networks</title><categories>cs.IT math.IT</categories><comments>20 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new method for decoding Low Density Parity Check (LDPC)
codes, based on Multi-Layer Perceptron (MLP) neural networks is proposed. Due
to the fact that in neural networks all procedures are processed in parallel,
this method can be considered as a viable alternative to Message Passing
Algorithm (MPA), with high computational complexity. Our proposed algorithm
runs with soft criterion and concurrently does not use probabilistic quantities
to decide what the estimated codeword is. Although the neural decoder
performance is close to the error performance of Sum Product Algorithm (SPA),
it is comparatively less complex. Therefore, the proposed decoder emerges as a
new infrastructure for decoding LDPC codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3433</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3433</id><created>2014-11-12</created><authors><author><keyname>Jiang</keyname><forenames>Yichen</forenames></author></authors><title>Privacy-Preserving Vehicular Announcements Aggregation Scheme Based on
  Threshold Ring Signature</title><categories>cs.CR cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the most promising application in VANETs, the vehicular announcement
allows vehicles to send announcement messages about road conditions to other
vehicles far away. The security requirements of reliability and privacy in the
vehicular announcement are not easily achieved simultaneously due to the
notorious sybil attack. In this paper, we present a novel privacy-preserving
vehicular announcements aggregation scheme. The proposed scheme provides
threshold authentication and flexible anonymity using message aggregation and
interactive threshold ring signature which allows nondeterministic different
signers to generate signature commonly in the environment not fully trusted.
Different from existing works in an attack-then-trace mode, our scheme defends
against the sybil attack beforehand. To our best knowledge, it is the first
privacy-preserving scheme preventing the malicious users from launching the
sybil attack in advance. Through extensive evaluation, we show the availability
and the efficiency of the proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3436</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3436</id><created>2014-11-12</created><authors><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author></authors><title>SelfieBoost: A Boosting Algorithm for Deep Learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe and analyze a new boosting algorithm for deep learning called
SelfieBoost. Unlike other boosting algorithms, like AdaBoost, which construct
ensembles of classifiers, SelfieBoost boosts the accuracy of a single network.
We prove a $\log(1/\epsilon)$ convergence rate for SelfieBoost under some &quot;SGD
success&quot; assumption which seems to hold in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3444</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3444</id><created>2014-11-12</created><updated>2014-12-01</updated><authors><author><keyname>Hassan</keyname><forenames>Kamrul</forenames></author><author><keyname>Islam</keyname><forenames>Liana</forenames></author></authors><title>Growing Scale-free Networks by a Mediation-Driven Attachment Rule</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>4 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a model that generates a new class of networks exhibiting
power-law degree distribution with a spectrum of exponents depending on the
number of links ($m$) with which incoming nodes join the existing network.
Unlike the Barab\'{a}si-Albert (BA) model, each new node first picks an
existing node at random, and connects not with this but with $m$ of its
neighbors also picked at random. Counterintuitively enough, such a
mediation-driven attachment rule results not only in preferential but
super-preferential attachment, albeit in disguise. We show that for small $m$,
the dynamics of our model is governed by winners take all phenomenon, and for
higher $m$ it is governed by winners take some. Besides, we show that the mean
of the inverse harmonic mean of degrees of the neighborhood of all existing
nodes is a measure that can well qualify how straight the degree distribution
is.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3450</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3450</id><created>2014-11-13</created><authors><author><keyname>Zhou</keyname><forenames>Yuzhe</forenames></author></authors><title>Future Communication Model for High-speed Railway Based on Unmanned
  Aerial Vehicles</title><categories>cs.NI</categories><comments>21 pages, 4 figures, 1 table, submitted to Telecommuniation Systems
  in April, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-speed railway is playing an important role in mass transportation, due
to its lower energy consumption, less environmental pollution, larger capacity
and higher safety features. The development of high-speed railway makes
people's life more and more convenient. Meanwhile, providing high quality of
service broadband communications for fast-moving users still remains unsolved,
despite the fact that new solutions of incremental improvements are keeping up
with this unprecedented communication requirement growth. This article proposes
a communication system infrastructure based on airborne relay for high-speed
trains in the further Cyber-Physical Systems. Comparisons and feasibility
analysis are provided as well as discussions of key wireless technologies and
obstacles in this system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3453</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3453</id><created>2014-11-13</created><authors><author><keyname>Bersani</keyname><forenames>Marcello Maria</forenames><affiliation>Politecnico di Milano</affiliation></author><author><keyname>Bresolin</keyname><forenames>Davide</forenames><affiliation>Universit&#xe0; di Bologna</affiliation></author><author><keyname>Ferrucci</keyname><forenames>Luca</forenames><affiliation>Politecnico di Milano</affiliation></author><author><keyname>Mazzara</keyname><forenames>Manuel</forenames><affiliation>Innopolis University and ETH Zuerich</affiliation></author></authors><title>Proceedings First Workshop on Logics and Model-checking for Self-*
  Systems</title><categories>cs.LO cs.FL cs.MA</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 168, 2014</journal-ref><doi>10.4204/EPTCS.168</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the First Workshop on Logics and
Model-checking for self-* systems (MOD* 2014). The worshop took place in
Bertinoro, Italy, on 12th of September 2014, and was a satellite event of iFM
2014 (the 11th International Conference on Integrated Formal Methods). The
workshop focuses on demonstrating the applicability of Formal Methods on modern
complex systems with a high degree of self-adaptivity and reconfigurability, by
bringing together researchers and practitioners with the goal of pushing
forward the state of the art on logics and model checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3464</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3464</id><created>2014-11-13</created><updated>2015-02-06</updated><authors><author><keyname>Katz</keyname><forenames>Daniel S.</forenames></author><author><keyname>Allen</keyname><forenames>Gabrielle</forenames></author><author><keyname>Hong</keyname><forenames>Neil Chue</forenames></author><author><keyname>Cranston</keyname><forenames>Karen</forenames></author><author><keyname>Parashar</keyname><forenames>Manish</forenames></author><author><keyname>Proctor</keyname><forenames>David</forenames></author><author><keyname>Turk</keyname><forenames>Matthew</forenames></author><author><keyname>Venters</keyname><forenames>Colin C.</forenames></author><author><keyname>Wilkins-Diehr</keyname><forenames>Nancy</forenames></author></authors><title>Second Workshop on Sustainable Software for Science: Practice and
  Experiences (WSSSPE2): Submission, Peer-Review and Sorting Process, and
  Results</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This technical report discusses the submission and peer-review process used
by the Second Workshop on Sustainable Software for Science: Practice and
Experiences (WSSSPE2) and the results of that process. It is intended to record
both the alternative submission and program organization model used by WSSSPE2
as well as the papers associated with the workshop that resulted from that
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3489</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3489</id><created>2014-11-13</created><updated>2015-07-13</updated><authors><author><keyname>Seoane</keyname><forenames>Lu&#xed;s F.</forenames></author><author><keyname>Gabler</keyname><forenames>Stephan</forenames></author><author><keyname>Blankertz</keyname><forenames>Benjamin</forenames></author></authors><title>Images from the Mind: BCI image evolution based on Rapid Serial Visual
  Presentation of polygon primitives</title><categories>q-bio.NC cs.HC</categories><comments>22 pages, 8 figures</comments><doi>10.1080/2326263X.2015.1060819</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a proof of concept for an EEG-based reconstruction of a
visual image which is on a user's mind. Our approach is based on the Rapid
Serial Visual Presentation (RSVP) of polygon primitives and Brain-Computer
Interface (BCI) technology. The presentation of polygons that contribute to
build a target image (because they match the shape and/or color of the target)
trigger attention-related EEG patterns. Accordingly, these target primitives
can be determined using BCI classification of Event-Related Potentials (ERPs).
They are then accumulated in the display until a satisfactory reconstruction is
reached. Selection steps have an average classification accuracy of $75\%$.
$25\%$ of the images could be reconstructed completely, while more than $65\%$
of the available visual details could be captured on average. Most of the
misclassifications were not misinterpretations of the BCI concerning users'
intent; rather, users tried to select polygons that were different than what
was intended by the experimenters. Open problems and alternatives to develop a
practical BCI-based image reconstruction application are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3492</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3492</id><created>2014-11-13</created><authors><author><keyname>Berejuck</keyname><forenames>Marcelo Daniel</forenames></author><author><keyname>Fr&#xf6;hlich</keyname><forenames>Ant&#xf4;nio Augusto</forenames></author></authors><title>Evaluation of silicon consumption for a connectionless Network-on-Chip</title><categories>cs.AR</categories><comments>11 pages, 9 figures and 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the design and evaluation of a predictable Network-on-Chip (NoC)
to interconnect processing units running multimedia applications with
variable-bit-rate. The design is based on a connectionless strategy in which
flits from different communication flows are interleaved in the same
communication channel between routers. Each flit carries routing information
used by routers to perform arbitration and scheduling of the corresponding
output communication channel. Analytic comparisons show that our approach keeps
average latency lower than a network based on resource reservation, when both
networks are working over 80% of offered load. We also evaluate the proposed
NoC on FPGA and ASIC technologies to understand the trade-off due to our
approach, in terms of silicon consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3506</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3506</id><created>2014-11-13</created><authors><author><keyname>Mesri</keyname><forenames>Alireza</forenames></author><author><keyname>Pirbazari</keyname><forenames>Mahmoud Mahdipour</forenames></author><author><keyname>Hadidi</keyname><forenames>Khayrollah</forenames></author><author><keyname>Khoei</keyname><forenames>Abdollah</forenames></author></authors><title>High gain two-stage amplifier with positive capacitive feedback
  compensation</title><categories>cs.OH</categories><comments>26 pages, 12 figures, 7 tables Accepted for publication in IET
  Circuits, Devices and Systems</comments><journal-ref>Circuits, Devices &amp; Systems, IET (Volume:9 , Issue: 3 ), 2015</journal-ref><doi>10.1049/iet-cds.2014.0139</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel topology for a high gain two-stage amplifier is proposed. The
proposed circuit is designed in a way that the non-dominant pole is at output
of the first stage. A positive capacitive feedback (PCF) around the second
stage introduces a left half plane (LHP) zero which cancels the phase shift
introduced by the non-dominant pole, considerably. The dominant pole is at the
output node which means that increasing the load capacitance has minimal effect
on stability. Moreover, a simple and effective method is proposed to enhance
slew rate. Simulation shows that slew rate is improved by a factor of 2.44
using the proposed method. The proposed amplifier is designed in a 0.18um CMOS
process. It consumes 0.86mW power from a 1.8V power supply and occupies
3038.5um2 of chip area. The DC gain is 82.7dB and gain bandwidth (GBW) is 88.9
MHz when driving a 5pF capacitive load. Also low frequency CMRR and PSRR+ are
127dB and 83.2dB, respectively. They are 24.8dB and 24.2dB at GBW frequency,
which are relatively high and are other important properties of the proposed
amplifier. Moreover, Simulations show convenient performance of the circuit in
process corners and also presence of mismatch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3508</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3508</id><created>2014-11-13</created><authors><author><keyname>Tran</keyname><forenames>Loc V.</forenames></author><author><keyname>Lee</keyname><forenames>Jaehong</forenames></author><author><keyname>Nguyen-Van</keyname><forenames>H.</forenames></author><author><keyname>Nguyen-Xuan</keyname><forenames>H.</forenames></author><author><keyname>Wahab</keyname><forenames>M. Abdel</forenames></author></authors><title>Geometrically nonlinear isogeometric analysis of laminated composite
  plates based on higher-order shear deformation theory</title><categories>cs.CE</categories><doi>10.1016/j.ijnonlinmec.2015.02.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an effectively numerical approach based on
isogeometric analysis (IGA) and higher-order shear deformation theory (HSDT)
for geometrically nonlinear analysis of laminated composite plates. The HSDT
allows us to approximate displacement field that ensures by itself the
realistic shear strain energy part without shear correction factors. IGA
utilizing basis functions namely B-splines or non-uniform rational B-splines
(NURBS) enables to satisfy easily the stringent continuity requirement of the
HSDT model without any additional variables. The nonlinearity of the plates is
formed in the total Lagrange approach based on the von-Karman strain
assumptions. Numerous numerical validations for the isotropic, orthotropic,
cross-ply and angle-ply laminated plates are provided to demonstrate the
effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3517</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3517</id><created>2014-11-13</created><updated>2015-02-10</updated><authors><author><keyname>Dinur</keyname><forenames>Irit</forenames></author><author><keyname>Harsha</keyname><forenames>Prahladh</forenames></author><author><keyname>Srinivasan</keyname><forenames>Srikanth</forenames></author><author><keyname>Varma</keyname><forenames>Girish</forenames></author></authors><title>Derandomized Graph Product Results using the Low Degree Long Code</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the question of whether the recent derandomization
results obtained by the use of the low-degree long code can be extended to
other product settings. We consider two settings: (1) the graph product results
of Alon, Dinur, Friedgut and Sudakov [GAFA, 2004] and (2) the &quot;majority is
stablest&quot; type of result obtained by Dinur, Mossel and Regev [SICOMP, 2009] and
Dinur and Shinkar [In Proc. APPROX, 2010] while studying the hardness of
approximate graph coloring.
  In our first result, we show that there exists a considerably smaller
subgraph of $K_3^{\otimes R}$ which exhibits the following property (shown for
$K_3^{\otimes R}$ by Alon et al.): independent sets close in size to the
maximum independent set are well approximated by dictators.
  The &quot;majority is stablest&quot; type of result of Dinur et al. and Dinur and
Shinkar shows that if there exist two sets of vertices $A$ and $B$ in
$K_3^{\otimes R}$ with very few edges with one endpoint in $A$ and another in
$B$, then it must be the case that the two sets $A$ and $B$ share a single
influential coordinate. In our second result, we show that a similar &quot;majority
is stablest&quot; statement holds good for a considerably smaller subgraph of
$K_3^{\otimes R}$. Furthermore using this result, we give a more efficient
reduction from Unique Games to the graph coloring problem, leading to improved
hardness of approximation results for coloring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3519</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3519</id><created>2014-11-13</created><updated>2014-11-17</updated><authors><author><keyname>Torki</keyname><forenames>Marwan</forenames></author><author><keyname>Hussein</keyname><forenames>Mohamed E.</forenames></author><author><keyname>Elsallamy</keyname><forenames>Ahmed</forenames></author><author><keyname>Fayyaz</keyname><forenames>Mahmoud</forenames></author><author><keyname>Yaser</keyname><forenames>Shehab</forenames></author></authors><title>Window-Based Descriptors for Arabic Handwritten Alphabet Recognition: A
  Comparative Study on a Novel Dataset</title><categories>cs.CV</categories><acm-class>I.5.2; I.7.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a comparative study for window-based descriptors on the
application of Arabic handwritten alphabet recognition. We show a detailed
experimental evaluation of different descriptors with several classifiers. The
objective of the paper is to evaluate different window-based descriptors on the
problem of Arabic letter recognition. Our experiments clearly show that they
perform very well. Moreover, we introduce a novel spatial pyramid partitioning
scheme that enhances the recognition accuracy for most descriptors. In
addition, we introduce a novel dataset for Arabic handwritten isolated alphabet
letters, which can serve as a benchmark for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3525</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3525</id><created>2014-11-13</created><authors><author><keyname>Roncone</keyname><forenames>Alessandro</forenames></author><author><keyname>Pattacini</keyname><forenames>Ugo</forenames></author><author><keyname>Metta</keyname><forenames>Giorgio</forenames></author><author><keyname>Natale</keyname><forenames>Lorenzo</forenames></author></authors><title>Gaze Stabilization for Humanoid Robots: a Comprehensive Framework</title><categories>cs.RO cs.CV</categories><comments>6 pages, appears in 2014 IEEE-RAS International Conference on
  Humanoid Robots</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaze stabilization is an important requisite for humanoid robots. Previous
work on this topic has focused on the integration of inertial and visual
information. Little attention has been given to a third component, which is the
knowledge that the robot has about its own movement. In this work we propose a
comprehensive framework for gaze stabilization in a humanoid robot. We focus on
the problem of compensating for disturbances induced in the cameras due to
self-generated movements of the robot. In this work we employ two separate
signals for stabilization: (1) an anticipatory term obtained from the velocity
commands sent to the joints while the robot moves autonomously; (2) a feedback
term from the on board gyroscope, which compensates unpredicted external
disturbances. We first provide the mathematical formulation to derive the
forward and the differential kinematics of the fixation point of the stereo
system. We finally test our method on the iCub robot. We show that the
stabilization consistently reduces the residual optical flow during the
movement of the robot and in presence of external disturbances. We also
demonstrate that proper integration of the neck DoF is crucial to achieve
correct stabilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3530</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3530</id><created>2014-11-13</created><updated>2015-03-27</updated><authors><author><keyname>Atay</keyname><forenames>Fatihcan M.</forenames></author><author><keyname>Liu</keyname><forenames>Shiping</forenames></author></authors><title>Cheeger constants, structural balance, and spectral clustering analysis
  for signed graphs</title><categories>math.CO cs.DS math.SP</categories><comments>29 pages. All comments are welcome!</comments><msc-class>05C50, 05C22, 05C85, 39A12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a family of multi-way Cheeger-type constants $\{h_k^{\sigma},
k=1,2,\ldots, N\}$ on a signed graph $\Gamma=(G,\sigma)$ such that
$h_k^{\sigma}=0$ if and only if $\Gamma$ has $k$ balanced connected components.
These constants are switching invariant and bring together in a unified
viewpoint a number of important graph-theoretical concepts, including the
classical Cheeger constant, the non-bipartiteness parameter of Desai and Rao,
the bipartiteness ratio of Trevisan, the dual Cheeger constant of Bauer and
Jost on unsigned graphs, and the frustration index (originally called the line
index of balance by Harary) on signed graphs. We further unify the
(higher-order or improved) Cheeger and dual Cheeger inequalities for unsigned
graphs as well as the underlying algorithmic proof techniques by establishing
their corresponding versions on signed graphs. In particular, we develop a
spectral clustering method for finding $k$ almost-balanced subgraphs, each
defining a sparse cut. The proper metric for such a clustering is the metric on
a real projective space. We also prove estimates of the extremal eigenvalues of
signed Laplace matrix in terms of number of signed triangles ($3$-cycles).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3539</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3539</id><created>2014-11-13</created><authors><author><keyname>Mendes</keyname><forenames>Nuno D.</forenames></author><author><keyname>Monteiro</keyname><forenames>Pedro T.</forenames></author><author><keyname>Carneiro</keyname><forenames>Jorge</forenames></author><author><keyname>Remy</keyname><forenames>Elisabeth</forenames></author><author><keyname>Chaouiya</keyname><forenames>Claudine</forenames></author></authors><title>Quantification of reachable attractors in asynchronous discrete dynamics</title><categories>cs.DM q-bio.MN</categories><comments>19 pages, 2 figures, 2 algorithms and 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: Models of discrete concurrent systems often lead to huge and
complex state transition graphs that represent their dynamics. This makes
difficult to analyse dynamical properties. In particular, for logical models of
biological regulatory networks, it is of real interest to study attractors and
their reachability from specific initial conditions, i.e. to assess the
potential asymptotical behaviours of the system. Beyond the identification of
the reachable attractors, we propose to quantify this reachability.
  Results: Relying on the structure of the state transition graph, we estimate
the probability of each attractor reachable from a given initial condition or
from a portion of the state space. First, we present a quasi-exact solution
with an original algorithm called Firefront, based on the exhaustive
exploration of the reachable state space. Then, we introduce an adapted version
of Monte Carlo simulation algorithm, termed Avatar, better suited to larger
models. Firefront and Avatar methods are validated and compared to other
related approaches, using as test cases logical models of synthetic and
biological networks.
  Availability: Both algorithms are implemented as Perl scripts that can be
freely downloaded from http://compbio.igc.gulbenkian.pt/nmd/node/59 along with
Supplementary Material.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3545</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3545</id><created>2014-11-13</created><updated>2015-02-02</updated><authors><author><keyname>Dib</keyname><forenames>St&#xe9;phanie</forenames><affiliation>I2M</affiliation></author><author><keyname>Rodier</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>I2M</affiliation></author></authors><title>Error-Correction Capability of Reed-Muller codes</title><categories>cs.IT math.IT math.NT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an asymptotic limit between correctable and uncor-rectable errors
on the Reed-Muller codes of any order. This limit is theoretical and does not
depend of any decoding algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3550</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3550</id><created>2014-11-13</created><authors><author><keyname>Finn</keyname><forenames>Samantha</forenames></author><author><keyname>Metaxas</keyname><forenames>Panagiotis Takis</forenames></author><author><keyname>Mustafaraj</keyname><forenames>Eni</forenames></author></authors><title>Investigating Rumor Propagation with TwitterTrails</title><categories>cs.SI</categories><comments>10 pages, 8 figures, under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media have become part of modern news reporting, used by journalists
to spread information and find sources, or as a news source by individuals. The
quest for prominence and recognition on social media sites like Twitter can
sometimes eclipse accuracy and lead to the spread of false information. As a
way to study and react to this trend, we introduce {\sc TwitterTrails}, an
interactive, web-based tool ({\tt twittertrails.com}) that allows users to
investigate the origin and propagation characteristics of a rumor and its
refutation, if any, on Twitter. Visualizations of burst activity, propagation
timeline, retweet and co-retweeted networks help its users trace the spread of
a story. Within minutes {\sc TwitterTrails} will collect relevant tweets and
automatically answer several important questions regarding a rumor: its
originator, burst characteristics, propagators and main actors according to the
audience. In addition, it will compute and report the rumor's level of
visibility and, as an example of the power of crowdsourcing, the audience's
skepticism towards it which correlates with the rumor's credibility. We
envision {\sc TwitterTrails} as valuable tool for individual use, but we
especially for amateur and professional journalists investigating recent and
breaking stories. Further, its expanding collection of investigated rumors can
be used to answer questions regarding the amount and success of misinformation
on Twitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3553</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3553</id><created>2014-11-13</created><authors><author><keyname>Xu</keyname><forenames>Lin</forenames></author><author><keyname>Lin</keyname><forenames>Shaobo</forenames></author><author><keyname>Zeng</keyname><forenames>Jinshan</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author></authors><title>Greedy metrics in orthogonal greedy learning</title><categories>cs.LG</categories><comments>33 pages, 8 figures</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orthogonal greedy learning (OGL) is a stepwise learning scheme that adds a
new atom from a dictionary via the steepest gradient descent and build the
estimator via orthogonal projecting the target function to the space spanned by
the selected atoms in each greedy step. Here, &quot;greed&quot; means choosing a new atom
according to the steepest gradient descent principle. OGL then avoids the
overfitting/underfitting by selecting an appropriate iteration number. In this
paper, we point out that the overfitting/underfitting can also be avoided via
redefining &quot;greed&quot; in OGL. To this end, we introduce a new greedy metric,
called $\delta$-greedy thresholds, to refine &quot;greed&quot; and theoretically verifies
its feasibility. Furthermore, we reveals that such a greedy metric can bring an
adaptive termination rule on the premise of maintaining the prominent learning
performance of OGL. Our results show that the steepest gradient descent is not
the unique greedy metric of OGL and some other more suitable metric may lessen
the hassle of model-selection of OGL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3555</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3555</id><created>2014-11-13</created><authors><author><keyname>Metaxas</keyname><forenames>Panagiotis Takis</forenames></author><author><keyname>Mustafaraj</keyname><forenames>Eni</forenames></author><author><keyname>Wong</keyname><forenames>Kily</forenames></author><author><keyname>Zeng</keyname><forenames>Laura</forenames></author><author><keyname>O'Keefe</keyname><forenames>Megan</forenames></author><author><keyname>Finn</keyname><forenames>Samantha</forenames></author></authors><title>Do Retweets indicate Interest, Trust, Agreement? (Extended Abstract)</title><categories>cs.SI</categories><comments>5 pages, 2 figures, extended abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arguably one of the most important features of Twitter is the support for
&quot;retweets&quot; or messages re-posted verbatim by a user that were originated by
someone else. (This does not include modified tweets that sometimes are
referred to as retweets.) Despite the fact that retweets are routinely studied
and reported, many important questions remain about user motivation for their
use and their significance. In this paper we answer the question of what users
indicate when they retweet. We do so in a comprehensive fashion, by employing a
user survey, a study of user profiles, and a meta-analysis of over 100 research
publications from three related major conferences. Our findings indicate that
retweeting indicates not only interest in a message, but also trust in the
message and the originator, and agreement with the message contents. However,
the findings are significantly weaker for journalists, some of whom beg to
differ declaring so in their own user profiles. On the other hand, the
inclusion of hashtags strengthens the signal of agreement, especially when the
hashtags are related to politics. While in the past there have been additional
claims in the literature about possible reasons for retweeting, many of them
are not supported, especially given the technical changes introduced recently
by Twitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3561</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3561</id><created>2014-11-13</created><authors><author><keyname>Singh</keyname><forenames>Prabhsimran</forenames></author><author><keyname>Singh</keyname><forenames>Amritpal</forenames></author></authors><title>A Text to Speech (TTS) System with English to Punjabi Conversion</title><categories>cs.CL</categories><comments>5 pages, 8 figures, 3 tables</comments><journal-ref>International Journal of Computer and Communication System
  Engineering, Volume 1, Issue 04, December 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper aims to show how an application can be developed that converts the
English language into the Punjabi Language, and the same application can
convert the Text to Speech(TTS) i.e. pronounce the text. This application can
be really beneficial for those with special needs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3575</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3575</id><created>2014-11-13</created><updated>2016-02-26</updated><authors><author><keyname>Raginsky</keyname><forenames>Maxim</forenames></author></authors><title>Strong data processing inequalities and $\Phi$-Sobolev inequalities for
  discrete channels</title><categories>cs.IT math.IT math.PR</categories><comments>74 pages, 3 figures; revised according to comments of the referees</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The noisiness of a channel can be measured by comparing suitable functionals
of the input and output distributions. For instance, the worst-case ratio of
output relative entropy to input relative entropy for all possible pairs of
input distributions is bounded from above by unity, by the data processing
theorem. However, for a fixed reference input distribution, this quantity may
be strictly smaller than one, giving so-called strong data processing
inequalities (SDPIs). The same considerations apply to an arbitrary
$\Phi$-divergence. This paper presents a systematic study of optimal constants
in SDPIs for discrete channels, including their variational characterizations,
upper and lower bounds, structural results for channels on product probability
spaces, and the relationship between SDPIs and so-called $\Phi$-Sobolev
inequalities (another class of inequalities that can be used to quantify the
noisiness of a channel by controlling entropy-like functionals of the input
distribution by suitable measures of input-output correlation). Several
applications to information theory, discrete probability, and statistical
physics are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3597</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3597</id><created>2014-11-13</created><authors><author><keyname>Reani</keyname><forenames>Avraham</forenames></author><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Universal Quantization for Separate Encodings and Joint Decoding of
  Correlated Sources</title><categories>cs.IT math.IT</categories><comments>24 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the multi-user lossy source-coding problem for continuous
alphabet sources. In a previous work, Ziv proposed a single-user universal
coding scheme which uses uniform quantization with dither, followed by a
lossless source encoder (entropy coder). In this paper, we generalize Ziv's
scheme to the multi-user setting. For this generalized universal scheme, upper
bounds are derived on the redundancies, defined as the differences between the
actual rates and the closest corresponding rates on the boundary of the rate
region. It is shown that this scheme can achieve redundancies of no more than
0.754 bits per sample for each user. These bounds are obtained without
knowledge of the multi-user rate region, which is an open problem in general.
As a direct consequence of these results, inner and outer bounds on the
rate-distortion achievable region are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3601</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3601</id><created>2014-11-13</created><authors><author><keyname>Cossidente</keyname><forenames>Antonio</forenames></author><author><keyname>Pavese</keyname><forenames>Francesco</forenames></author></authors><title>Subspace codes in PG(2n-1,q)</title><categories>math.CO cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $(r,M,2\delta;k)_q$ constant--dimension subspace code, $\delta &gt;1$, is a
collection $\cal C$ of $(k-1)$--dimensional projective subspaces of ${\rm
PG(r-1,q)}$ such that every $(k-\delta)$--dimensional projective subspace of
${\rm PG(r-1,q)}$ is contained in at most a member of $\cal C$.
Constant--dimension subspace codes gained recently lot of interest due to the
work by Koetter and Kschischang, where they presented an application of such
codes for error-correction in random network coding. Here a $(2n,M,4;n)_q$
constant--dimension subspace code is constructed, for every $n \ge 4$. The size
of our codes is considerably larger than all known constructions so far,
whenever $n &gt; 4$. When $n=4$ a further improvement is provided by constructing
an $(8,M,4;4)_q$ constant--dimension subspace code, with $M =
q^{12}+q^2(q^2+1)^2(q^2+q+1)+1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3603</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3603</id><created>2014-11-13</created><updated>2014-11-18</updated><authors><author><keyname>Canonne</keyname><forenames>Cl&#xe9;ment L.</forenames></author><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Meka</keyname><forenames>Raghu</forenames></author><author><keyname>Sudan</keyname><forenames>Madhu</forenames></author></authors><title>Communication with Imperfectly Shared Randomness</title><categories>cs.CC cs.IT math.IT math.PR</categories><comments>Updated the Agreement Distillation section, after being informed that
  the result Lemma 4.1 was already known in [2]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The communication complexity of many fundamental problems reduces greatly
when the communicating parties share randomness that is independent of the
inputs to the communication task. Natural communication processes (say between
humans) however often involve large amounts of shared correlations among the
communicating players, but rarely allow for perfect sharing of randomness. Can
the communication complexity benefit from shared correlations as well as it
does from shared randomness? This question was considered mainly in the context
of simultaneous communication by Bavarian et al. (ICALP 2014). In this work we
study this problem in the standard interactive setting and give some general
results. In particular, we show that every problem with communication
complexity of $k$ bits with perfectly shared randomness has a protocol using
imperfectly shared randomness with complexity $\exp(k)$ bits. We also show that
this is best possible by exhibiting a promise problem with complexity $k$ bits
with perfectly shared randomness which requires $\exp(k)$ bits when the
randomness is imperfectly shared. Along the way we also highlight some other
basic problems such as compression, and agreement distillation, where shared
randomness plays a central role and analyze the complexity of these problems in
the imperfectly shared randomness model.
  The technical highlight of this work is the lower bound that goes into the
result showing the tightness of our general connection. This result builds on
the intuition that communication with imperfectly shared randomness needs to be
less sensitive to its random inputs than communication with perfectly shared
randomness. The formal proof invokes results about the small-set expansion of
the noisy hypercube and an invariance principle to convert this intuition to a
proof, thus giving a new application domain for these fundamental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3617</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3617</id><created>2014-11-13</created><updated>2016-01-20</updated><authors><author><keyname>Dettmann</keyname><forenames>Carl P.</forenames></author><author><keyname>Georgiou</keyname><forenames>Orestis</forenames></author></authors><title>Random geometric graphs with general connection functions</title><categories>cond-mat.stat-mech cs.NI</categories><comments>16 pages; improved figures and minor edits</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the original (1961) Gilbert model of random geometric graphs, nodes are
placed according to a Poisson point process, and links formed between those
within a fixed range. Motivated by wireless ad-hoc networks &quot;soft&quot; or
&quot;probabilistic&quot; connection models have recently been introduced, involving a
&quot;connection function&quot; H(r) that gives the probability that two nodes at
distance r are linked (directly connect). In many applications (not only
wireless networks), it is desirable that the graph is connected, that is every
node is linked to every other node in a multihop fashion. Here, the connection
probability of a dense network in a convex domain in two or three dimensions is
expressed in terms of contributions from boundary components, for a very
general class of connection functions. It turns out that only a few quantities
such as moments of the connection function appear. Good agreement is found with
special cases from previous studies and with numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3622</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3622</id><created>2014-11-13</created><authors><author><keyname>Motik</keyname><forenames>Boris</forenames></author><author><keyname>Nenov</keyname><forenames>Yavor</forenames></author><author><keyname>Piro</keyname><forenames>Robert</forenames></author><author><keyname>Horrocks</keyname><forenames>Ian</forenames></author></authors><title>Handling owl:sameAs via Rewriting</title><categories>cs.DB cs.AI cs.DC</categories><comments>This is the technical report supporting the AAAI 2015 Conference
  submission with the same title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rewriting is widely used to optimise owl:sameAs reasoning in materialisation
based OWL 2 RL systems. We investigate issues related to both the correctness
and efficiency of rewriting, and present an algorithm that guarantees
correctness, improves efficiency, and can be effectively parallelised. Our
evaluation shows that our approach can reduce reasoning times on practical data
sets by orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3625</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3625</id><created>2014-11-13</created><updated>2015-03-16</updated><authors><author><keyname>Ahmad</keyname><forenames>Rami Ali</forenames></author><author><keyname>Lacan</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Arnal</keyname><forenames>Fabrice</forenames></author><author><keyname>Gineste</keyname><forenames>Mathieu</forenames></author><author><keyname>Clarac</keyname><forenames>Laurence</forenames></author></authors><title>Enhancing Satellite System Throughput Using Adaptive HARQ for Delay
  Tolerant Services in Mobile Communications</title><categories>cs.NI</categories><comments>7 pages, 8 figures, Wireless Telecommunications Symposium WTS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose the introduction of adaptive hybrid automatic repeat
request (HARQ) in the context of mobile satellite communications. HARQ schemes
which are commonly used in terrestrial links, can be adapted to improve the
throughput for delay tolerant services. The proposed method uses the estimation
of the mutual information between the received and the sent symbols, in order
to estimate the number of bits necessary to decode the message at next
transmission. We evaluate the performance of our method by simulating a land
mobile satellite (LMS) channel. We compare our results with the static HARQ
scheme, showing that our adaptive retransmission technique has better
efficiency while keeping an acceptable delay for services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3632</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3632</id><created>2014-11-13</created><authors><author><keyname>Yang</keyname><forenames>Yong-Liang</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Mitra</keyname><forenames>Niloy J.</forenames></author></authors><title>Mesh2Fab: Reforming Shapes for Material-specific Fabrication</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As humans, we regularly associate shape of an object with its built material.
In the context of geometric modeling, however, this interrelation between form
and material is rarely explored. In this work, we propose a novel data-driven
reforming (i.e., reshaping) algorithm that adapts an input multi-component
model for a target fabrication material. The algorithm adapts both the part
geometry and the inter-part topology of the input shape to better align with
material specific fabrication requirements. As output, we produce the reshaped
model along with respective part dimensions and inter-part junction
specifications. We evaluate our algorithm on a range of man-made models and
demonstrate non-trivial model reshaping examples focusing only on metal and
wooden materials. We also appraise the output of our algorithm using a user
study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3640</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3640</id><created>2014-11-13</created><authors><author><keyname>Gutfraind</keyname><forenames>Alexander</forenames></author><author><keyname>Kun</keyname><forenames>Jeremy</forenames></author><author><keyname>Lelkes</keyname><forenames>&#xc1;d&#xe1;m D.</forenames></author><author><keyname>Reyzin</keyname><forenames>Lev</forenames></author></authors><title>Network installation and recovery: approximation lower bounds and faster
  exact formulations</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Neighbor Aided Network Installation Problem (NANIP) introduced
previously which asks for a minimal cost ordering of the vertices of a graph,
where the cost of visiting a node is a function of the number of neighbors that
have already been visited. This problem has applications in resource management
and disaster recovery. In this paper we analyze the computational hardness of
NANIP. In particular we show that this problem is NP-hard even when restricted
to convex decreasing cost functions, give a linear approximation lower bound
for the greedy algorithm, and prove a general sub-constant approximation lower
bound. Then we give a new integer programming formulation of NANIP and
empirically observe its speedup over the original integer program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3643</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3643</id><created>2014-11-13</created><authors><author><keyname>Ding</keyname><forenames>Cunsheng</forenames></author><author><keyname>Tan</keyname><forenames>Chik How</forenames></author><author><keyname>Tan</keyname><forenames>Yin</forenames></author></authors><title>Six Constructions of Difference Families</title><categories>cs.IT math.CO math.IT</categories><msc-class>05B05, 05B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, six constructions of difference families are presented. These
constructions make use of difference sets, almost difference sets and disjoint
difference families, and give new point of views of relationships among these
combinatorial objects. Most of the constructions work for all finite groups.
Though these constructions look simple, they produce many difference families
with new parameters. In addition to the six new constructions, new results
about intersection numbers are also derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3645</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3645</id><created>2014-11-13</created><authors><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>Authentication Using Piggy Bank Approach to Secure Double-Lock
  Cryptography</title><categories>cs.CR</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The piggy bank idea allows one-way encryption of information that can be
accessed only by authorized parties. Here we show how the piggy bank idea can
be used to authenticate parties to counter man-in-the-middle (MIM) attack that
can jeopardize the double-lock cryptography protocol. We call this method
double-signature double lock cryptography and it can be implemented in ways
that go beyond hash-based message authentication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3650</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3650</id><created>2014-11-13</created><authors><author><keyname>Ashkan</keyname><forenames>Azin</forenames></author><author><keyname>Kveton</keyname><forenames>Branislav</forenames></author><author><keyname>Berkovsky</keyname><forenames>Shlomo</forenames></author><author><keyname>Wen</keyname><forenames>Zheng</forenames></author></authors><title>DUM: Diversity-Weighted Utility Maximization for Recommendations</title><categories>cs.IR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need for diversification of recommendation lists manifests in a number of
recommender systems use cases. However, an increase in diversity may undermine
the utility of the recommendations, as relevant items in the list may be
replaced by more diverse ones. In this work we propose a novel method for
maximizing the utility of the recommended items subject to the diversity of
user's tastes, and show that an optimal solution to this problem can be found
greedily. We evaluate the proposed method in two online user studies as well as
in an offline analysis incorporating a number of evaluation metrics. The
results of evaluations show the superiority of our method over a number of
baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3651</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3651</id><created>2014-10-24</created><updated>2015-03-01</updated><authors><author><keyname>Stankovic</keyname><forenames>Srdjan</forenames></author><author><keyname>Orovic</keyname><forenames>Irena</forenames></author><author><keyname>Stankovic</keyname><forenames>Ljubisa</forenames></author></authors><title>Polynomial Fourier Domain as a Domain of Signal Sparsity</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions of Signal Processing (10 pages, 11
  Figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A compressive sensing (CS) reconstruction method for polynomial phase signals
is proposed in this paper. It relies on the Polynomial Fourier transform, which
is used to establish a relationship between the observation and sparsity
domain. Polynomial phase signals are not sparse in commonly used domains such
as Fourier or wavelet domain. Therefore, for polynomial phase signals standard
CS algorithms applied in these transformation domains cannot provide
satisfactory results. In that sense, the Polynomial Fourier transform is used
to ensure sparsity. The proposed approach is generalized using time-frequency
representations obtained by the Local Polynomial Fourier transform (LPFT). In
particular, the first-order LPFT can produce linear time-frequency
representation for chirps. It provides revealing signal local behavior, which
leads to sparse representation. The theory is illustrated on examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3652</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3652</id><created>2014-11-13</created><authors><author><keyname>Amuru</keyname><forenames>SaiDhiraj</forenames></author><author><keyname>Tekin</keyname><forenames>Cem</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author><author><keyname>Buehrer</keyname><forenames>R. Michael</forenames></author></authors><title>Jamming Bandits</title><categories>cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can an intelligent jammer learn and adapt to unknown environments in an
electronic warfare-type scenario? In this paper, we answer this question in the
positive, by developing a cognitive jammer that adaptively and optimally
disrupts the communication between a victim transmitter-receiver pair. We
formalize the problem using a novel multi-armed bandit framework where the
jammer can choose various physical layer parameters such as the signaling
scheme, power level and the on-off/pulsing duration in an attempt to obtain
power efficient jamming strategies. We first present novel online learning
algorithms to maximize the jamming efficacy against static transmitter-receiver
pairs and prove that our learning algorithm converges to the optimal (in terms
of the error rate inflicted at the victim and the energy used) jamming
strategy. Even more importantly, we prove that the rate of convergence to the
optimal jamming strategy is sub-linear, i.e. the learning is fast in comparison
to existing reinforcement learning algorithms, which is particularly important
in dynamically changing wireless environments. Also, we characterize the
performance of the proposed bandit-based learning algorithm against multiple
static and adaptive transmitter-receiver pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3656</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3656</id><created>2014-11-12</created><authors><author><keyname>Ad&#xe1;mek</keyname><forenames>Karel</forenames></author><author><keyname>Novotn&#xfd;</keyname><forenames>Jan</forenames></author><author><keyname>Armour</keyname><forenames>Wes</forenames></author></authors><title>The Implementation of a Real-Time Polyphase Filter</title><categories>cs.DC cs.PF</categories><comments>Proceedings of WDS 2014, Charles University in Prague, Faculty of
  Mathematics and Physics Troja, Prague</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we study the suitability of dierent computational
accelerators for the task of real-time data processing. The algorithm used for
comparison is the polyphase filter, a standard tool in signal processing and a
well established algorithm. We measure performance in FLOPs and execution time,
which is a critical factor for real-time systems. For our real-time studies we
have chosen a data rate of 6.5GB/s, which is the estimated data rate for a
single channel on the SKAs Low Frequency Aperture Array. Our findings how that
GPUs are the most likely candidate for real-time data processing. GPUs are
better in both performance and power consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3662</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3662</id><created>2014-11-13</created><authors><author><keyname>Gillani</keyname><forenames>Nabeel</forenames></author><author><keyname>Yasseri</keyname><forenames>Taha</forenames></author><author><keyname>Eynon</keyname><forenames>Rebecca</forenames></author><author><keyname>Hjorth</keyname><forenames>Isis</forenames></author></authors><title>Structural limitations of learning in a crowd: communication
  vulnerability and information diffusion in MOOCs</title><categories>physics.soc-ph cs.CY cs.SI physics.data-an</categories><comments>Pre-print version. Published version available at
  http://dx.doi.org/10.1038/srep06447</comments><doi>10.1038/srep06447</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive Open Online Courses (MOOCs) bring together a global crowd of
thousands of learners for several weeks or months. In theory, the openness and
scale of MOOCs can promote iterative dialogue that facilitates group cognition
and knowledge construction. Using data from two successive instances of a
popular business strategy MOOC, we filter observed communication patterns to
arrive at the &quot;significant&quot; interaction networks between learners and use
complex network analysis to explore the vulnerability and information diffusion
potential of the discussion forums. We find that different discussion topics
and pedagogical practices promote varying levels of 1) &quot;significant&quot;
peer-to-peer engagement, 2) participant inclusiveness in dialogue, and
ultimately, 3) modularity, which impacts information diffusion to prevent a
truly &quot;global&quot; exchange of knowledge and learning. These results indicate the
structural limitations of large-scale crowd-based learning and highlight the
different ways that learners in MOOCs leverage, and learn within, social
contexts. We conclude by exploring how these insights may inspire new
developments in online education.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3675</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3675</id><created>2014-11-13</created><updated>2015-07-01</updated><authors><author><keyname>Zhu</keyname><forenames>Linhong</forenames></author><author><keyname>Guo</keyname><forenames>Dong</forenames></author><author><keyname>Yin</keyname><forenames>Junming</forenames></author><author><keyname>Steeg</keyname><forenames>Greg Ver</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author></authors><title>Scalable Link Prediction in Dynamic Networks via Non-Negative Matrix
  Factorization</title><categories>cs.SI cs.AI cs.IR</categories><comments>Technical report for paper &quot;Scalable Temporal Latent Space Inference
  for Link Prediction in Dynamic Social Networks&quot;</comments><acm-class>H.2.8; H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a scalable temporal latent space model for link prediction in
dynamic social networks, where the goal is to predict links over time based on
a sequence of previous graph snapshots. The model assumes that each user lies
in an unobserved latent space and interactions are more likely to form between
similar users in the latent space representation. In addition, the model allows
each user to gradually move its position in the latent space as the network
structure evolves over time. We present a global optimization algorithm to
effectively infer the temporal latent space, with a quadratic convergence rate.
Two alternative optimization algorithms with local and incremental updates are
also proposed, allowing the model to scale to larger networks without
compromising prediction accuracy. Empirically, we demonstrate that our model,
when evaluated on a number of real-world dynamic networks, significantly
outperforms existing approaches for temporal link prediction in terms of both
scalability and predictive power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3698</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3698</id><created>2014-11-13</created><updated>2015-12-14</updated><authors><author><keyname>Huang</keyname><forenames>Qingqing</forenames></author><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Kakade</keyname><forenames>Sham</forenames></author><author><keyname>Dahleh</keyname><forenames>Munther</forenames></author></authors><title>Minimal Realization Problems for Hidden Markov Models</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a stationary discrete random process with alphabet size d, which is
assumed to be the output process of an unknown stationary Hidden Markov Model
(HMM). Given the joint probabilities of finite length strings of the process,
we are interested in finding a finite state generative model to describe the
entire process. In particular, we focus on two classes of models: HMMs and
quasi-HMMs, which is a strictly larger class of models containing HMMs. In the
main theorem, we show that if the random process is generated by an HMM of
order less or equal than k, and whose transition and observation probability
matrix are in general position, namely almost everywhere on the parameter
space, both the minimal quasi-HMM realization and the minimal HMM realization
can be efficiently computed based on the joint probabilities of all the length
N strings, for N &gt; 4 lceil log_d(k) rceil +1. In this paper, we also aim to
compare and connect the two lines of literature: realization theory of HMMs,
and the recent development in learning latent variable models with tensor
decomposition techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3706</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3706</id><created>2014-11-13</created><authors><author><keyname>Blake</keyname><forenames>Ian</forenames></author><author><keyname>Murty</keyname><forenames>V. Kumar</forenames></author><author><keyname>Usefi</keyname><forenames>Hamid</forenames></author></authors><title>A note on diagonal and Hermitian surfaces</title><categories>cs.IT math.AG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aspects of the properties, enumeration and construction of points on diagonal
and Hermitian surfaces have been considered extensively in the literature and
are further considered here. The zeta function of diagonal surfaces is given as
a direct result of the work of Wolfmann. Recursive construction techniques for
the set of rational points of Hermitian surfaces are of interest. The
relationship of these techniques here to the construction of codes on surfaces
is briefly noted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3708</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3708</id><created>2014-11-13</created><authors><author><keyname>Chen</keyname><forenames>Xiaojie</forenames></author><author><keyname>Zhang</keyname><forenames>Yanling</forenames></author><author><keyname>Huang</keyname><forenames>Ting-Zhu</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Solving the collective-risk social dilemma with risky assets in
  well-mixed and structured populations</title><categories>physics.soc-ph cs.GT q-bio.PE</categories><comments>10 two-column pages, 5 figures; accepted for publication in Physical
  Review E</comments><journal-ref>Phys. Rev. E 90 (2014) 052823</journal-ref><doi>10.1103/PhysRevE.90.052823</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the collective-risk social dilemma, players lose their personal endowments
if contributions to the common pool are too small. This fact alone, however,
does not always deter selfish individuals from defecting. The temptations to
free-ride on the prosocial efforts of others are strong because we are
hardwired to maximize our own fitness regardless of the consequences this might
have for the public good. Here we show that the addition of risky assets to the
personal endowments, both of which are lost if the collective target is not
reached, can contribute to solving the collective-risk social dilemma. In
infinite well-mixed populations risky assets introduce new stable and unstable
mixed steady states, whereby the stable mixed steady state converges to full
cooperation as either the risk of collective failure or the amount of risky
assets increases. Similarly, in finite well-mixed populations the introduction
of risky assets enforces configurations where cooperative behavior thrives. In
structured populations cooperation is promoted as well, but the distribution of
assets amongst the groups is crucial. Surprisingly, we find that the completely
rational allocation of assets only to the most successful groups is not
optimal, and this regardless of whether the risk of collective failure is high
or low. Instead, in low-risk situations bounded rational allocation of assets
works best, while in high-risk situations the simplest uniform distribution of
assets among all the groups is optimal. These results indicate that prosocial
behavior depends sensitively on the potential losses individuals are likely to
endure if they fail to cooperate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3715</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3715</id><created>2014-11-13</created><authors><author><keyname>Barchiesi</keyname><forenames>Daniele</forenames></author><author><keyname>Giannoulis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Stowell</keyname><forenames>Dan</forenames></author><author><keyname>Plumbley</keyname><forenames>Mark D.</forenames></author></authors><title>Acoustic Scene Classification</title><categories>cs.SD cs.LG</categories><journal-ref>IEEE Signal Processing Magazine 32(3) (May 2015) 16-34</journal-ref><doi>10.1109/MSP.2014.2326181</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we present an account of the state-of-the-art in acoustic
scene classification (ASC), the task of classifying environments from the
sounds they produce. Starting from a historical review of previous research in
this area, we define a general framework for ASC and present different imple-
mentations of its components. We then describe a range of different algorithms
submitted for a data challenge that was held to provide a general and fair
benchmark for ASC techniques. The dataset recorded for this purpose is
presented, along with the performance metrics that are used to evaluate the
algorithms and statistical significance tests to compare the submitted methods.
We use a baseline method that employs MFCCS, GMMS and a maximum likelihood
criterion as a benchmark, and only find sufficient evidence to conclude that
three algorithms significantly outperform it. We also evaluate the human
classification accuracy in performing a similar classification task. The best
performing algorithm achieves a mean accuracy that matches the median accuracy
obtained by humans, and common pairs of classes are misclassified by both
computers and humans. However, all acoustic scenes are correctly classified by
at least some individuals, while there are scenes that are misclassified by all
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3716</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3716</id><created>2014-11-14</created><authors><author><keyname>Feghhi</keyname><forenames>Mahmood Mohassel</forenames></author><author><keyname>Mirmohseni</keyname><forenames>Mahtab</forenames></author><author><keyname>Abbasfar</keyname><forenames>Aliazam</forenames></author></authors><title>Power Allocation in the Energy Harvesting Full-Duplex Gaussian Relay
  Channels</title><categories>cs.NI cs.ET cs.IT math.IT math.OC</categories><comments>Accepted for publication in International Journal of Communication
  Systems (Special Issue on Energy Efficient Wireless Communication Networks
  with QoS), October 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a general model to study the full-duplex
non-coherent decode-and-forward Gaussian relay channel with energy harvesting
(EH) nodes, called NC-EH-$\mathcal{RC}$, in three cases: $i)$ no energy
transfer (ET), $ii)$ one-way ET from the source (S) to the relay (R), and
$iii)$ two-way ET. We consider the problem of optimal power allocation in
NC-EH-$\mathcal{RC}$ in order to maximize the total transmitted bits from S to
the destination in a given time duration. General stochastic energy arrivals at
S and R with known EH times and values are assumed. In NC-EH-$\mathcal{RC}$
with no ET, the complicated min-max optimization form along with its
constraints make the problem intractable. It is shown that this problem can be
transformed to a solvable convex form; however, convex optimization solution
does not provide the structural properties of the optimal solution. Therefore,
following an alternative perspective, we investigate conditions on harvesting
process of S and R where we find optimal algorithmic solution. Further, we
propose some suboptimal algorithms and provide some examples, in which the
algorithms are optimal. Moreover, we find a class of problems for
NC-EH-$\mathcal{RC}$ with one-way ET from S to R, where the optimal algorithmic
solution is devised. For NC-EH-$\mathcal{RC}$ with two-way ET, we propose
\emph{general} optimal algorithmic solution. Furthermore, the performance of
the proposed algorithms are evaluated numerically and compared with optimal
numerical convex optimization tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3736</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3736</id><created>2014-11-13</created><updated>2015-03-24</updated><authors><author><keyname>Sheikholeslami</keyname><forenames>Azadeh</forenames></author><author><keyname>Ghaderi</keyname><forenames>Majid</forenames></author><author><keyname>Pishro-Nik</keyname><forenames>Hossein</forenames></author><author><keyname>Goeckel</keyname><forenames>Dennis</forenames></author></authors><title>Minimum Energy Routing in Wireless Networks in the Presence of Jamming</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The effectiveness and simple implementation of physical layer jammers make
them an essential threat for wireless networks. In a multihop wireless network,
where jammers can interfere with the transmission of user messages at every
intermediate nodes along the path, one can employ jamming oblivious routing and
then employ physical-layer techniques (e.g. spread spectrum) to suppress
jamming. However, whereas these approaches can provide significant gains, the
residual jamming can still severely limit system performance. This motivates
the consideration of routing approaches that account for the differences in the
jamming environment between different paths. First, we take a straightforward
approach where an equal outage probability is allocated to each of the links
along the path and develop a minimum energy routing solution. Next, we
demonstrate the shortcomings of this approach and then consider the optimal
outage allocation along paths by employing an approximation to the link outage
probability. This yields an efficient and effective routing algorithm that only
requires knowledge of the measured jamming at each node. Numerical results
demonstrate that the amount of energy saved by the proposed methods with
respect to standard shortest path routing, especially for parameters
appropriate for terrestrial wireless networks, is substantial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3737</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3737</id><created>2014-11-13</created><authors><author><keyname>Elmisery</keyname><forenames>Ahmed M.</forenames></author><author><keyname>Rho</keyname><forenames>Seungmin</forenames></author><author><keyname>Botvich</keyname><forenames>Dmitri</forenames></author></authors><title>Holistic Collaborative Privacy Framework for Users' Privacy in Social
  Recommender Service</title><categories>cs.CR cs.IR</categories><report-no>ICT Platform Society Volume 02-01</report-no><acm-class>D.4.6; K.6.m</acm-class><journal-ref>Journal of Platform Technology, March 2014 Volume 02-01 Pages
  11-31</journal-ref><doi>10.13140/2.1.1714.5289</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current business model for existing recommender services is centered
around the availability of users' personal data at their side whereas consumers
have to trust that the recommender service providers will not use their data in
a malicious way. With the increasing number of cases for privacy breaches,
different countries and corporations have issued privacy laws and regulations
to define the best practices for the protection of personal information. The
data protection directive 95/46/EC and the privacy principles established by
the Organization for Economic Cooperation and Development (OECD) are examples
of such regulation frameworks. In this paper, we assert that utilizing
third-party recommender services to generate accurate referrals are feasible,
while preserving the privacy of the users' sensitive information which will be
residing on a clear form only on his/her own device. As a result, each user who
benefits from the third-party recommender service will have absolute control
over what to release from his/her own preferences. We proposed a collaborative
privacy middleware that executes a two stage concealment process within a
distributed data collection protocol in order to attain this claim.
Additionally, the proposed solution complies with one of the common privacy
regulation frameworks for fair information practice in a natural and functional
way -which is OECD privacy principles. The approach presented in this paper is
easily integrated into the current business model as it is implemented using a
middleware that runs at the end-users side and utilizes the social nature of
content distribution services to implement a topological data collection
protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3742</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3742</id><created>2014-11-13</created><authors><author><keyname>Ahmed</keyname><forenames>Mohammed</forenames></author></authors><title>TCP Congestion Control Identification</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Transmission Control Protocol (TCP) carries most of the traffic on the
Internet these days. There are several implementations of TCP, and the most
important difference among them is their mechanism for controlling congestion.
One of the methods for determining type of a TCP is active probing. Active
probing considers a TCP implementation as a black box, sends different streams
of data to the appropriate host. According to the response received from the
host, it figures out the type of TCP version implemented.
  TCP Behavior Inference Tool (TBIT) is an implemented tool that uses active
probing to check the running TCP on web servers. It can check several aspects
of the running TCP including initial value of congestion window, congestion
control algorithm, conformant congestion control, response to selective
acknowledgment, response to Explicit Congestion Notification (ECN) and time
wait duration. In this paper we focus on congestion control algorithm aspect of
it, explain the mechanism used by TBIT and present the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3747</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3747</id><created>2014-11-13</created><authors><author><keyname>Hub&#xe1;&#x10d;ek</keyname><forenames>Pavel</forenames></author><author><keyname>Park</keyname><forenames>Sunoo</forenames></author></authors><title>Cryptographically Blinded Games: Leveraging Players' Limitations for
  Equilibria and Profit</title><categories>cs.GT</categories><comments>This is a working paper, of which an extended abstract appeared in
  the 15th ACM Conference on Economics and Computation (EC 2014)</comments><acm-class>F.0; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we apply methods from cryptography to enable any number of
mutually distrusting players to implement broad classes of mediated equilibria
of strategic games without the need for trusted mediation.
  Our implementation makes use of a (standard) pre-play &quot;cheap talk&quot; phase, in
which players engage in free and non-binding communication prior to playing in
the original game. In our cheap talk phase, the players execute a secure
multi-party computation protocol to sample an action profile from an
equilibrium of a &quot;cryptographically blinded&quot; version of the original game, in
which actions are encrypted. The essence of our approach is to exploit the
power of encryption to selectively restrict the information available to
players about sampled action profiles, such that these desirable equilibria can
be stably achieved. In contrast to previous applications of cryptography to
game theory, this work is the first to employ the paradigm of using encryption
to allow players to benefit from hiding information \emph{from themselves},
rather than from others; and we stress that rational players would
\emph{choose} to hide the information from themselves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3749</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3749</id><created>2014-11-13</created><authors><author><keyname>La Fond</keyname><forenames>Timothy</forenames></author><author><keyname>Neville</keyname><forenames>Jennifer</forenames></author><author><keyname>Gallagher</keyname><forenames>Brian</forenames></author></authors><title>Anomaly Detection in Dynamic Networks of Varying Size</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic networks, also called network streams, are an important data
representation that applies to many real-world domains. Many sets of network
data such as e-mail networks, social networks, or internet traffic networks are
best represented by a dynamic network due to the temporal component of the
data. One important application in the domain of dynamic network analysis is
anomaly detection. Here the task is to identify points in time where the
network exhibits behavior radically different from a typical time, either due
to some event (like the failure of machines in a computer network) or a shift
in the network properties. This problem is made more difficult by the fluid
nature of what is considered &quot;normal&quot; network behavior. The volume of traffic
on a network, for example, can change over the course of a month or even vary
based on the time of the day without being considered unusual. Anomaly
detection tests using traditional network statistics have difficulty in these
scenarios due to their Density Dependence: as the volume of edges changes the
value of the statistics changes as well making it difficult to determine if the
change in signal is due to the traffic volume or due to some fundamental shift
in the behavior of the network. To more accurately detect anomalies in dynamic
networks, we introduce the concept of Density-Consistent network statistics. On
synthetically generated graphs anomaly detectors using these statistics show a
a 20-400% improvement in the recall when distinguishing graphs drawn from
different distributions. When applied to several real datasets
Density-Consistent statistics recover multiple network events which standard
statistics failed to find.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3757</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3757</id><created>2014-11-13</created><authors><author><keyname>Keeler</keyname><forenames>Holger Paul</forenames></author><author><keyname>Ross</keyname><forenames>Nathan</forenames></author><author><keyname>Xia</keyname><forenames>Aihua</forenames></author></authors><title>When do wireless network signals appear Poisson?</title><categories>math.PR cs.NI</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the point process of signal strengths from transmitters in a
wireless network observed from a fixed position under models with general
signal path loss and random propagation effects. We show via coupling arguments
that under general conditions this point process of signal strengths can be
well-approximated by an inhomogeneous Poisson or a Cox point processes on the
positive real line. We also provide some bounds on the total variation distance
between the laws of these point processes and both Poisson and Cox point
processes. Under appropriate conditions, these results support the use of a
spatial Poisson point process for the underlying positioning of transmitters in
models of wireless networks, even if in reality the positioning does not appear
Poisson. We apply the results to a number of models with popular choices for
positioning of transmitters, path loss functions, and distributions of
propagation effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3761</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3761</id><created>2014-11-13</created><authors><author><keyname>Cameron</keyname><forenames>Delroy</forenames></author><author><keyname>Sheth</keyname><forenames>Amit</forenames></author><author><keyname>Jaykumar</keyname><forenames>Nishita</forenames></author><author><keyname>Thirunarayan</keyname><forenames>Krishnaprasad</forenames></author><author><keyname>Anand</keyname><forenames>Gaurish</forenames></author><author><keyname>Smith</keyname><forenames>Gary A.</forenames></author></authors><title>A Hybrid Approach to Finding Relevant Social Media Content for Complex
  Domain Specific Information Needs</title><categories>cs.IR</categories><comments>Accepted for publication: Journal of Web Semantics, Elsevier</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While contemporary semantic search systems offer to improve classical
keyword-based search, they are not always adequate for complex domain specific
information needs. The domain of prescription drug abuse, for example, requires
knowledge of both ontological concepts and 'intelligible constructs' not
typically modeled in ontologies. These intelligible constructs convey essential
information that include notions of intensity, frequency, interval, dosage and
sentiments, which could be important to the holistic needs of the information
seeker. We present a hybrid approach to domain specific information retrieval
(or knowledge-aware search) that integrates ontology-driven query
interpretation with synonym-based query expansion and domain specific rules, to
facilitate search in social media. Our framework is based on a context-free
grammar (CFG) that defines the query language of constructs interpretable by
the search system. The grammar provides two levels of semantic interpretation:
1) a top-level CFG that facilitates retrieval of diverse textual patterns,
which belong to broad templates and 2) a low-level CFG that enables
interpretation of certain specific expressions that belong to such patterns.
These low-level expressions occur as concepts from four different categories of
data: 1) ontological concepts, 2) concepts in lexicons (such as emotions and
sentiments), 3) concepts in lexicons with only partial ontology representation,
called lexico-ontology concepts (such as side effects and routes of
administration (ROA)), and 4) domain specific expressions (such as date, time,
interval, frequency and dosage) derived solely through rules. Our approach is
embodied in a novel Semantic Web platform called PREDOSE developed for
prescription drug abuse epidemiology.
  Keywords: Knowledge-Aware Search, Ontology, Semantic Search, Background
Knowledge, Context-Free Grammar
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3764</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3764</id><created>2014-11-13</created><authors><author><keyname>Standish</keyname><forenames>Russell</forenames></author></authors><title>Mechanical generation of networks with surplus complexity</title><categories>cs.IT math.IT nlin.CD</categories><comments>Accepted for ACALCI 2015 Newcastle, Australia</comments><msc-class>94A17</msc-class><acm-class>E.4; G.2.2; H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work I examined an information based complexity measure of
networks with weighted links. The measure was compared with that obtained from
by randomly shuffling the original network, forming an Erdos-Renyi random
network preserving the original link weight distribution. It was found that
real world networks almost invariably had higher complexity than their shuffled
counterparts, whereas networks mechanically generated via preferential
attachment did not. The same experiment was performed on foodwebs generated by
an artificial life system, Tierra, and a couple of evolutionary ecology
systems, EcoLab and WebWorld. These latter systems often exhibited the same
complexity excess shown by real world networks, suggesting that the complexity
surplus indicates the presence of evolutionary dynamics.
  In this paper, I report on a mechanical network generation system that does
produce this complexity surplus. The heart of the idea is to construct the
network of state transitions of a chaotic dynamical system, such as the Lorenz
equation. This indicates that complexity surplus is a more fundamental trait
than that of being an evolutionary system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3771</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3771</id><created>2014-11-13</created><authors><author><keyname>Shoaib</keyname><forenames>Yasir</forenames></author><author><keyname>Das</keyname><forenames>Olivia</forenames></author></authors><title>Pouring Cloud Virtualization Security Inside Out</title><categories>cs.CR cs.DC</categories><comments>13 pages, 2 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, virtualization security concerns in the cloud computing
domain are reviewed. The focus is toward virtual machine (VM) security where
attacks and vulnerabilities such as VM escape, VM hopping, cross-VM
side-channel, VM-based rootkits (VMBRs), VM mobility, and VM remote are
mentioned and discussed according to their relevance in the clouds. For each
attack we outline how they affect the security of cloud systems.
Countermeasures and security measures to detect or prevent them through
techniques such as VM detection, GuardHype, VM introspection, VM image
scanning, etc. are also discussed. Through the surveyed work we present a
classification of VM threats within the clouds. Finally, we include our
observations and those of other researchers on this matter of cloud
virtualization security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3776</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3776</id><created>2014-11-13</created><authors><author><keyname>Xia</keyname><forenames>Donggeng</forenames></author><author><keyname>Mankad</keyname><forenames>Shawn</forenames></author><author><keyname>Michailidis</keyname><forenames>George</forenames></author></authors><title>Measuring Influence in Twitter Ecosystems using a Counting Process
  Modeling Framework</title><categories>cs.SI stat.ME</categories><comments>30 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data extracted from social media platforms, such as Twitter, are both large
in scale and complex in nature, since they contain both unstructured text, as
well as structured data, such as time stamps and interactions between users. A
key question for such platforms is to determine influential users, in the sense
that they generate interactions between members of the platform. Common
measures used both in the academic literature and by companies that provide
analytics services are variants of the popular web-search PageRank algorithm
applied to networks that capture connections between users. In this work, we
develop a modeling framework using multivariate interacting counting processes
to capture the detailed actions that users undertake on such platforms, namely
posting original content, reposting and/or mentioning other users' postings.
Based on the proposed model, we also derive a novel influence measure. We
discuss estimation of the model parameters through maximum likelihood and
establish their asymptotic properties. The proposed model and the accompanying
influence measure are illustrated on a data set covering a five year period of
the Twitter actions of the members of the US Senate, as well as mainstream news
organizations and media personalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3777</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3777</id><created>2014-11-13</created><authors><author><keyname>Sani</keyname><forenames>Ardalan Amiri</forenames></author><author><keyname>Zhong</keyname><forenames>Lin</forenames></author><author><keyname>Wallach</keyname><forenames>Dan S.</forenames></author></authors><title>Glider: A GPU Library Driver for Improved System Security</title><categories>cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Legacy device drivers implement both device resource management and
isolation. This results in a large code base with a wide high-level interface
making the driver vulnerable to security attacks. This is particularly
problematic for increasingly popular accelerators like GPUs that have large,
complex drivers. We solve this problem with library drivers, a new driver
architecture. A library driver implements resource management as an untrusted
library in the application process address space, and implements isolation as a
kernel module that is smaller and has a narrower lower-level interface (i.e.,
closer to hardware) than a legacy driver. We articulate a set of device and
platform hardware properties that are required to retrofit a legacy driver into
a library driver. To demonstrate the feasibility and superiority of library
drivers, we present Glider, a library driver implementation for two GPUs of
popular brands, Radeon and Intel. Glider reduces the TCB size and attack
surface by about 35% and 84% respectively for a Radeon HD 6450 GPU and by about
38% and 90% respectively for an Intel Ivy Bridge GPU. Moreover, it incurs no
performance cost. Indeed, Glider outperforms a legacy driver for applications
requiring intensive interactions with the device driver, such as applications
using the OpenGL immediate mode API.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3784</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3784</id><created>2014-11-13</created><updated>2015-04-10</updated><authors><author><keyname>Montufar</keyname><forenames>Guido</forenames></author></authors><title>Deep Narrow Boltzmann Machines are Universal Approximators</title><categories>stat.ML cs.LG math.PR</categories><comments>Published as a conference paper at ICLR 2015</comments><journal-ref>http://www.iclr.cc/doku.php?id=iclr2015:main</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that deep narrow Boltzmann machines are universal approximators of
probability distributions on the activities of their visible units, provided
they have sufficiently many hidden layers, each containing the same number of
units as the visible layer. We show that, within certain parameter domains,
deep Boltzmann machines can be studied as feedforward networks. We provide
upper and lower bounds on the sufficient depth and width of universal
approximators. These results settle various intuitions regarding undirected
networks and, in particular, they show that deep narrow Boltzmann machines are
at least as compact universal approximators as narrow sigmoid belief networks
and restricted Boltzmann machines, with respect to the currently available
bounds for those models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3787</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3787</id><created>2014-11-13</created><authors><author><keyname>Shrivastava</keyname><forenames>Anshumali</forenames></author><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>Asymmetric Minwise Hashing</title><categories>stat.ML cs.DB cs.DS cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minwise hashing (Minhash) is a widely popular indexing scheme in practice.
Minhash is designed for estimating set resemblance and is known to be
suboptimal in many applications where the desired measure is set overlap (i.e.,
inner product between binary vectors) or set containment. Minhash has inherent
bias towards smaller sets, which adversely affects its performance in
applications where such a penalization is not desirable. In this paper, we
propose asymmetric minwise hashing (MH-ALSH), to provide a solution to this
problem. The new scheme utilizes asymmetric transformations to cancel the bias
of traditional minhash towards smaller sets, making the final &quot;collision
probability&quot; monotonic in the inner product. Our theoretical comparisons show
that for the task of retrieving with binary inner products asymmetric minhash
is provably better than traditional minhash and other recently proposed hashing
algorithms for general inner products. Thus, we obtain an algorithmic
improvement over existing approaches in the literature. Experimental
evaluations on four publicly available high-dimensional datasets validate our
claims and the proposed scheme outperforms, often significantly, other hashing
algorithms on the task of near neighbor retrieval with set containment. Our
proposal is simple and easy to implement in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3790</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3790</id><created>2014-11-13</created><authors><author><keyname>Alberti</keyname><forenames>Francesco</forenames><affiliation>University of Lugano, Lugano, Switzerland</affiliation></author><author><keyname>Ghilardi</keyname><forenames>Silvio</forenames><affiliation>Universi&#xe0; degli Studi di Milano, Milano, Italy</affiliation></author><author><keyname>Sharygina</keyname><forenames>Natasha</forenames><affiliation>University of Lugano, Lugano, Switzerland</affiliation></author></authors><title>Monotonic Abstraction Techniques: from Parametric to Software Model
  Checking</title><categories>cs.LO cs.SE</categories><comments>In Proceedings MOD* 2014, arXiv:1411.3453</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 168, 2014, pp. 1-11</journal-ref><doi>10.4204/EPTCS.168.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monotonic abstraction is a technique introduced in model checking
parameterized distributed systems in order to cope with transitions containing
global conditions within guards. The technique has been re-interpreted in a
declarative setting in previous papers of ours and applied to the verification
of fault tolerant systems under the so-called &quot;stopping failures&quot; model. The
declarative reinterpretation consists in logical techniques (quantifier
relativizations and, especially, quantifier instantiations) making sense in a
broader context. In fact, we recently showed that such techniques can
over-approximate array accelerations, so that they can be employed as a
meaningful (and practically effective) component of CEGAR loops in software
model checking too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3791</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3791</id><created>2014-11-13</created><authors><author><keyname>Bravetti</keyname><forenames>Mario</forenames><affiliation>University of Bologna, Italy / INRIA, France</affiliation></author><author><keyname>Zavattaro</keyname><forenames>Gianluigi</forenames><affiliation>University of Bologna, Italy / INRIA, France</affiliation></author></authors><title>Choreographies and Behavioural Contracts on the Way to Dynamic Updates</title><categories>cs.LO</categories><comments>In Proceedings MOD* 2014, arXiv:1411.3453</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 168, 2014, pp. 12-31</journal-ref><doi>10.4204/EPTCS.168.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey our work on choreographies and behavioural contracts in multiparty
interactions. In particular theories of behavioural contracts are presented
which enable reasoning about correct service composition (contract compliance)
and service substitutability (contract refinement preorder) under different
assumptions concerning service communication: synchronous address or name based
communication with patient non-preemptable or impatient invocations, or
asynchronous communication. Correspondingly relations between behavioural
contracts and choreographic descriptions are considered, where a contract for
each communicating party is, e.g., derived by projection. The considered
relations are induced as the maximal preoders which preserve contract
compliance and global traces: we show maximality to hold (permitting services
to be discovered/substituted independently for each party) when contract
refinement preorders with all the above asymmetric communication means are
considered and, instead, not to hold if the standard symmetric CCS/pi-calculus
communication is considered (or when directly relating choreographies to
behavioral contracts via a preorder, no matter the communication mean). The
obtained maximal preorders are then characterized in terms of a new form of
testing, called compliance testing, where not only tests must succeed but also
the system under test (thus relating to controllability theory), and compared
with classical preorders such as may/must testing, trace inclusion, etc.
Finally, recent work about adaptable choreographies and behavioural contracts
is presented, where the theory above is extended to update mechanisms allowing
choreographies/contracts to be modified at run-time by internal
(self-adaptation) or external intervention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3792</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3792</id><created>2014-11-13</created><authors><author><keyname>Garanina</keyname><forenames>Natalia</forenames><affiliation>A.P. Ershov Institute of Informatics Systems, Novosibirsk, Russia</affiliation></author><author><keyname>Bodin</keyname><forenames>Eugene</forenames><affiliation>A.P. Ershov Institute of Informatics Systems, Novosibirsk, Russia</affiliation></author><author><keyname>Sidorova</keyname><forenames>Elena</forenames><affiliation>A.P. Ershov Institute of Informatics Systems, Novosibirsk, Russia</affiliation></author></authors><title>An Approach to Model Checking of Multi-agent Data Analysis</title><categories>cs.AI cs.MA cs.SE</categories><comments>In Proceedings MOD* 2014, arXiv:1411.3453</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 168, 2014, pp. 32-44</journal-ref><doi>10.4204/EPTCS.168.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents an approach to verification of a multi-agent data analysis
algorithm. We base correct simulation of the multi-agent system by a finite
integer model. For verification we use model checking tool SPIN. Protocols of
agents are written in Promela language and properties of the multi-agent data
analysis system are expressed in logic LTL. We run several experiments with
SPIN and the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3793</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3793</id><created>2014-11-13</created><authors><author><keyname>Suzuki</keyname><forenames>Masaya</forenames><affiliation>Department of Computer Science, Tokyo Institute of Technology</affiliation></author><author><keyname>Watanabe</keyname><forenames>Takuo</forenames><affiliation>Department of Computer Science, Tokyo Institute of Technology</affiliation></author></authors><title>A Language Support for Exhaustive Fault-Injection in Message-Passing
  System Models</title><categories>cs.SE cs.PL</categories><comments>In Proceedings MOD* 2014, arXiv:1411.3453</comments><proxy>EPTCS</proxy><acm-class>D 2.4</acm-class><journal-ref>EPTCS 168, 2014, pp. 45-58</journal-ref><doi>10.4204/EPTCS.168.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach towards specifying and verifying adaptive
distributed systems. We here take fault-handling as an example of adaptive
behavior and propose a modeling language Sandal for describing fault-prone
message-passing systems. One of the unique mechanisms of the language is a
linguistic support for abstracting typical faults such as unexpected
termination of processes and random loss of messages. The Sandal compiler
translates a model into a set of NuSMV modules. During the compilation process,
faults specified in the model will be woven into the output. One can thus enjoy
full-automatic exhaustive fault-injection without writing faulty behaviors
explicitly. We demonstrate the advantage of the language by verifying a model
of the two-phase commit protocol under faulty environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3794</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3794</id><created>2014-11-13</created><authors><author><keyname>Conroy</keyname><forenames>Parker</forenames></author><author><keyname>Bareiss</keyname><forenames>Daman</forenames></author><author><keyname>Beall</keyname><forenames>Matt</forenames></author><author><keyname>Berg</keyname><forenames>Jur van den</forenames></author></authors><title>3-D Reciprocal Collision Avoidance on Physical Quadrotor Helicopters
  with On-Board Sensing for Relative Positioning</title><categories>cs.RO</categories><comments>8 pages, 9 Figures, 1 Table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an implementation of 3-D reciprocal collision
avoidance on real quadrotor helicopters where each quadrotor senses the
relative position and velocity of other quadrotors using an on-board camera. We
show that using our approach, quadrotors are able to successfully avoid
pairwise collisions in GPS and motion-capture denied environments, without
communication between the quadrotors, and even when human operators
deliberately attempt to induce collisions. To our knowledge, this is the first
time that reciprocal collision avoidance has been successfully implemented on
real robots where each agent independently observes the others using on-board
sensors. We theoretically analyze the response of the collision-avoidance
algorithm to the violated assumptions by the use of real robots. We
quantitatively analyze our experimental results. A particularly striking
observation is that at times the quadrotors exhibit &quot;reciprocal dance&quot;
behavior, which is also observed when humans move past each other in
constrained environments. This seems to be the result of sensing uncertainty,
which causes both robots involved to have a different belief about the relative
positions and velocities and, as a result, choose the same side on which to
pass.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3796</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3796</id><created>2014-11-13</created><authors><author><keyname>Iyer</keyname><forenames>Srikanth K.</forenames></author><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author><author><keyname>Narasimha</keyname><forenames>Dheeraj</forenames></author></authors><title>Autoregressive Cascades on Random Networks</title><categories>physics.soc-ph cs.SI math.PR</categories><comments>A longer version of paper submitted to PRL</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a model for cascades on random networks in which the
cascade propagation at any node depends on the load at the failed neighbor, the
degree of the neighbor as well as the load at that node. Each node in the
network bears an initial load that is below the capacity of the node. The
trigger for the cascade emanates at a single node or a small fraction of the
nodes from some external shock. Upon failure, the load at the failed node gets
divided randomly and added to the existing load at those neighboring nodes that
have not yet failed. Subsequently, a neighboring node fails if its accumulated
load exceeds its capacity. The failed node then plays no further part in the
process. The cascade process stops as soon as the accumulated load at all nodes
that have not yet failed is below their respective capacities. The model is
shown to operate in two regimes, one in which the cascade terminates with only
a finite number of node failures. In the other regime there is a positive
probability that the cascade continues indefinitely. Bounds are obtained on the
critical parameter where the phase transition occurs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3799</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3799</id><created>2014-11-14</created><authors><author><keyname>Goyal</keyname><forenames>Navin</forenames></author><author><keyname>Rademacher</keyname><forenames>Luis</forenames></author><author><keyname>Vempala</keyname><forenames>Santosh</forenames></author></authors><title>Query complexity of sampling and small geometric partitions</title><categories>cs.CC math.CO</categories><doi>10.1017/S0963548314000704</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the following problem:
  Discrete partitioning problem (DPP): Let $\mathbb{F}_q P^n$ denote the
$n$-dimensional finite projective space over $\mathbb{F}_q$. For positive
integer $k \leq n$, let $\{ A^i\}_{i=1}^N$ be a partition of $(\mathbb{F}_q
P^n)^k$ such that
  (1) for all $i \leq N$, $A^i = \prod_{j=1}^k A^i_j$ (partition into product
sets),
  (2) for all $i \leq N$, there is a $(k-1)$-dimensional subspace $L^i
\subseteq \mathbb{F}_q P^n$ such that $A^i \subseteq (L^i)^k$.
  What is the minimum value of $N$ as a function of $q,n,k$? We will be mainly
interested in the case $k=n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3806</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3806</id><created>2014-11-14</created><authors><author><keyname>Bansal</keyname><forenames>Sandhya</forenames></author><author><keyname>Katiyar</keyname><forenames>V.</forenames></author></authors><title>Integrating Fuzzy and Ant Colony System for Fuzzy Vehicle Routing
  Problem with Time Windows</title><categories>cs.AI cs.CE cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper fuzzy VRPTW with an uncertain travel time is considered.
Credibility theory is used to model the problem and specifies a preference
index at which it is desired that the travel times to reach the customers fall
into their time windows. We propose the integration of fuzzy and ant colony
system based evolutionary algorithm to solve the problem while preserving the
constraints. Computational results for certain benchmark problems having short
and long time horizons are presented to show the effectiveness of the
algorithm. Comparison between different preferences indexes have been obtained
to help the user in making suitable decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3808</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3808</id><created>2014-11-14</created><updated>2015-06-04</updated><authors><author><keyname>Zhao</keyname><forenames>Junzhou</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author><author><keyname>Wang</keyname><forenames>Pinghui</forenames></author><author><keyname>Guan</keyname><forenames>Xiaohong</forenames></author></authors><title>Tracking Triadic Cardinality Distributions for Burst Detection in Social
  Activity Streams</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In everyday life, we often observe unusually frequent interactions among
people before or during important events, e.g., we receive/send more greetings
from/to our friends on Christmas Day, than usual. We also observe that some
videos suddenly go viral through people's sharing in online social networks
(OSNs). Do these seemingly different phenomena share a common structure?
  All these phenomena are associated with sudden surges of user activities in
networks, which we call &quot;bursts&quot; in this work. We find that the emergence of a
burst is accompanied with the formation of triangles in networks. This finding
motivates us to propose a new method to detect bursts in OSNs. We first
introduce a new measure, &quot;triadic cardinality distribution&quot;, corresponding to
the fractions of nodes with different numbers of triangles, i.e., triadic
cardinalities, within a network. We demonstrate that this distribution changes
when a burst occurs, and is naturally immunized against spamming social-bot
attacks. Hence, by tracking triadic cardinality distributions, we can reliably
detect bursts in OSNs. To avoid handling massive activity data generated by OSN
users, we design an efficient sample-estimate solution to estimate the triadic
cardinality distribution from sampled data. Extensive experiments conducted on
real data demonstrate the usefulness of this triadic cardinality distribution
and the effectiveness of our sample-estimate solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3810</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3810</id><created>2014-11-14</created><authors><author><keyname>Choudhary</keyname><forenames>Sunav</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>Fundamental Limits of Blind Deconvolution Part I: Ambiguity Kernel</title><categories>cs.IT math.IT</categories><comments>20 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind deconvolution is an ubiquitous non-linear inverse problem in
applications like wireless communications and image processing. This problem is
generally ill-posed, and there have been efforts to use sparse models for
regularizing blind deconvolution to promote signal identifiability. Part I of
this two-part paper characterizes the ambiguity space of blind deconvolution
and shows unidentifiability of this inverse problem for almost every pair of
unconstrained input signals. The approach involves lifting the deconvolution
problem to a rank one matrix recovery problem and analyzing the rank two null
space of the resultant linear operator. A measure theoretically tight
(parametric and recursive) representation of the key rank two null space is
stated and proved. This representation is a novel foundational result for
signal and code design strategies promoting identifiability under convolutive
observation models. Part II of this paper analyzes the identifiability of
sparsity constrained blind deconvolution and establishes surprisingly strong
negative results on scaling laws for the sparsity-ambiguity trade-off.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3811</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3811</id><created>2014-11-14</created><authors><author><keyname>Ivanov</keyname><forenames>Todor</forenames></author><author><keyname>Zicari</keyname><forenames>Roberto V.</forenames></author><author><keyname>Izberovic</keyname><forenames>Sead</forenames></author><author><keyname>Tolle</keyname><forenames>Karsten</forenames></author></authors><title>Performance Evaluation of Virtualized Hadoop Clusters</title><categories>cs.DC cs.DB</categories><report-no>Technical Report No. 2014-1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report we investigate the performance of Hadoop clusters, deployed
with separated storage and compute layers, on top of a hypervisor managing a
single physical host. We have analyzed and evaluated the different Hadoop
cluster configurations by running CPU bound and I/O bound workloads. The report
is structured as follows: Section 2 provides a brief description of the
technologies involved in our study. An overview of the experimental platform,
setup test and configurations are presented in Section 3. Our benchmark
methodology is defined in Section 4. The performed experiments together with
the evaluation of the results are presented in Section 5. Finally, Section 6
concludes with lessons learned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3815</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3815</id><created>2014-11-14</created><updated>2015-04-16</updated><authors><author><keyname>Zhao</keyname><forenames>Mingmin</forenames></author><author><keyname>Zhuang</keyname><forenames>Chengxu</forenames></author><author><keyname>Wang</keyname><forenames>Yizhou</forenames></author><author><keyname>Lee</keyname><forenames>Tai Sing</forenames></author></authors><title>Predictive Encoding of Contextual Relationships for Perceptual
  Inference, Interpolation and Prediction</title><categories>cs.LG cs.CV cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new neurally-inspired model that can learn to encode the global
relationship context of visual events across time and space and to use the
contextual information to modulate the analysis by synthesis process in a
predictive coding framework. The model learns latent contextual representations
by maximizing the predictability of visual events based on local and global
contextual information through both top-down and bottom-up processes. In
contrast to standard predictive coding models, the prediction error in this
model is used to update the contextual representation but does not alter the
feedforward input for the next layer, and is thus more consistent with
neurophysiological observations. We establish the computational feasibility of
this model by demonstrating its ability in several aspects. We show that our
model can outperform state-of-art performances of gated Boltzmann machines
(GBM) in estimation of contextual information. Our model can also interpolate
missing events or predict future events in image sequences while simultaneously
estimating contextual information. We show it achieves state-of-art
performances in terms of prediction accuracy in a variety of tasks and
possesses the ability to interpolate missing frames, a function that is lacking
in GBM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3818</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3818</id><created>2014-11-14</created><authors><author><keyname>Descher</keyname><forenames>Marco</forenames></author><author><keyname>Feilhauer</keyname><forenames>Thomas</forenames></author><author><keyname>Amann</keyname><forenames>Lucia</forenames></author></authors><title>Automated user documentation generation based on the Eclipse application
  model</title><categories>cs.SE</categories><comments>9 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An application's user documentation, also referred to as the user manual, is
one of the core elements required in application distribution. While there
exist many tools to aid an application's developer in creating and maintaining
documentation on and for the code itself, there are no tools that complement
code development with user documentation for modern graphical applications.
Approaches like literate programming are not applicable to this scenario, as
not a library, but a full application is to be documented to an end-user.
Documentation generation on applications up to now was only partially feasible
due to the gap between the code and its semantics. The new generation of
Eclipse rich client platform developed applications is based on an application
model, closing a broad semantic gap between code and visible interface. We use
this application model to provide a semantic description for the contained
elements. Combined with the internal relationships of the application model,
these semantic descriptions are aggregated to well-structured user
documentations that comply to the ISO/IEC 26514. This paper delivers a report
on the Ecrit research project, where the potentials and limitations of user
documentation generation based on the Eclipse application model were
investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3827</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3827</id><created>2014-11-14</created><updated>2014-11-20</updated><authors><author><keyname>Delpeuch</keyname><forenames>Antonin</forenames></author></authors><title>Autonomization of Monoidal Categories</title><categories>math.CT cs.CL</categories><comments>Under review. Comments welcome!</comments><msc-class>18D10</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We define the free autonomous category generated by a monoidal category and
study some of its properties. From a linguistic perspective, this expands the
range of possible models of meaning within the distributional compositional
framework, by allowing nonlinearities in maps. From a categorical point of
view, this provides a factorization of the construction in [Preller and Lambek,
2007] of the free autonomous category generated by a category.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3834</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3834</id><created>2014-11-14</created><authors><author><keyname>Nejad</keyname><forenames>Bijan Chokoufe</forenames></author><author><keyname>Ohl</keyname><forenames>Thorsten</forenames></author><author><keyname>Reuter</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Simple, Parallel, High-Performance Virtual Machines for Extreme
  Computations</title><categories>physics.comp-ph cs.MS hep-ph</categories><comments>19 pages, 8 figures</comments><report-no>DESY 14-206</report-no><journal-ref>Computer Physics Communications (2015), pp. 58-69</journal-ref><doi>10.1016/j.cpc.2015.05.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a high-performance virtual machine (VM) written in a numerically
fast language like Fortran or C to evaluate very large expressions. We discuss
the general concept of how to perform computations in terms of a VM and present
specifically a VM that is able to compute tree-level cross sections for any
number of external legs, given the corresponding byte code from the optimal
matrix element generator, O'Mega. Furthermore, this approach allows to
formulate the parallel computation of a single phase space point in a simple
and obvious way. We analyze hereby the scaling behaviour with multiple threads
as well as the benefits and drawbacks that are introduced with this method. Our
implementation of a VM can run faster than the corresponding native, compiled
code for certain processes and compilers, especially for very high
multiplicities, and has in general runtimes in the same order of magnitude. By
avoiding the tedious compile and link steps, which may fail for source code
files of gigabyte sizes, new processes or complex higher order corrections that
are currently out of reach could be evaluated with a VM given enough computing
power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3838</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3838</id><created>2014-11-14</created><authors><author><keyname>Alves</keyname><forenames>Hirley</forenames></author><author><keyname>de Lima</keyname><forenames>Carlos H. M.</forenames></author><author><keyname>Nardelli</keyname><forenames>Pedro. H. J.</forenames></author><author><keyname>Souza</keyname><forenames>Richard Demo</forenames></author><author><keyname>Latva-aho</keyname><forenames>Matti</forenames></author></authors><title>On the Secrecy of Interference-Limited Networks under Composite Fading
  Channels</title><categories>cs.IT math.IT</categories><doi>10.1109/LSP.2015.2398514</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the secrecy capacity of the radio channel in
interference-limited regime. We assume that interferers are uniformly scattered
over the network area according to a Point Poisson Process and the channel
model consists of path-loss, log-normal shadowing and Nakagami-m fading. Both
the probability of non-zero secrecy capacity and the secrecy outage probability
are then derived in closed-form expressions using tools of stochastic geometry
and higher-order statistics. Our numerical results show how the secrecy metrics
are affected by the disposition of the desired receiver, the eavesdropper and
the legitimate transmitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3841</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3841</id><created>2014-11-14</created><updated>2015-05-07</updated><authors><author><keyname>Jiang</keyname><forenames>Bomin</forenames></author><author><keyname>Deghat</keyname><forenames>Mohammad</forenames></author><author><keyname>Anderson</keyname><forenames>Brian D. O.</forenames></author></authors><title>Simultaneous Velocity and Position Estimation via Distance-only
  Measurements with Application to Multi-Agent System Control</title><categories>cs.SY</categories><comments>Submitted to IEEE Transactions on Automatic Control as a Technical
  Note</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a strategy to estimate the velocity and position of
neighbor agents using distance measurements only. Since with agents executing
arbitrary motions, instantaneous distance-only measurements cannot provide
enough information for our objectives, we postulate that agents engage in a
combination of circular motion and linear motion. The proposed estimator can be
used to develop control algorithms where only distance measurements are
available to each agent. As an example, we show how this estimation method can
be used to control the formation shape and velocity of the agents in a multi
agent system. Simulation results are provided to illustrate the performance of
the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3843</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3843</id><created>2014-11-14</created><authors><author><keyname>Zapatrin</keyname><forenames>Rom&#xe0;n</forenames></author></authors><title>Quantum emulation of query extension in information retrieval</title><categories>cs.IR</categories><comments>Latex, 8 pages</comments><msc-class>68P20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An operationalistic scheme, called Melucci metaphor, is suggested
representing Information Retrieval as physical measurements with beam of
particles playing the role of the flow of retrieved documents. The
possibilities of query expansion by extra term are studied from this
perspective, when the particles-`docuscles' are assumed to be of classical or
quantum nature. It is shown that in both cases the choice of an extra term
based on Bayesian belief revision is still valid on the qualitative level for
boosting the relevance of the retrieved documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3856</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3856</id><created>2014-11-14</created><updated>2014-11-21</updated><authors><author><keyname>Alves</keyname><forenames>Hirley</forenames></author><author><keyname>Brante</keyname><forenames>Glauber</forenames></author><author><keyname>Souza</keyname><forenames>Richard D.</forenames></author><author><keyname>da Costa</keyname><forenames>Daniel B.</forenames></author><author><keyname>Latva-aho</keyname><forenames>Matti</forenames></author></authors><title>On the Performance of Secure Full-Duplex Relaying under Composite Fading
  Channels</title><categories>cs.IT math.IT</categories><doi>10.1109/LSP.2014.2374994</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We assume a full-duplex (FD) cooperative network subject to hostile attacks
and undergoing composite fading channels. We focus on two scenarios:
\textit{a)} the transmitter has full CSI, for which we derive closed-form
expressions for the \textit{average secrecy rate}; and \textit{b)} the
transmitter only knows the CSI of the legitimate nodes, for which we obtain
closed-form expressions for the \textit{secrecy outage probability}. We show
that secure FD relaying is feasible, even under strong self-interference and in
the presence of sophisticated multiple antenna eavesdropper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3857</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3857</id><created>2014-11-14</created><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Statistical physics of random binning</title><categories>cs.IT cond-mat.stat-mech math.IT</categories><comments>26 pages. Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the model of random binning and finite-temperature decoding for
Slepian-Wolf codes, from a statistical-mechanical perspective. While ordinary
random channel coding is intimately related to the random energy model (REM) -
a statistical-mechanical model of disordered magnetic materials, it turns out
that random binning (for Slepian-Wolf coding) is analogous to another, related
statistical mechanical model of strong disorder, which we call the random
dilution model (RDM). We use the latter analogy to characterize phase
transitions pertaining to finite- temperature Slepian-Wolf decoding, which are
somewhat similar, but not identical, to those of finite-temperature channel
decoding. We then provide the exact random coding exponent of the bit error
rate (BER) as a function of the coding rate and the decoding temperature, and
discuss its properties. Finally, a few modifications and extensions of our
results are outlined and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3870</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3870</id><created>2014-11-14</created><updated>2015-12-08</updated><authors><author><keyname>Zheng</keyname><forenames>Shenggen</forenames></author><author><keyname>Li</keyname><forenames>Lvzhou</forenames></author><author><keyname>Qiu</keyname><forenames>Daowen</forenames></author><author><keyname>Gruska</keyname><forenames>Jozef</forenames></author></authors><title>Promise problems solved by quantum and classical finite automata</title><categories>cs.FL quant-ph</categories><comments>Substantial changes are made. Some new resutls are added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of promise problems was introduced and started to be
systematically explored by Even, Selman, Yacobi, Goldreich, and other scholars.
It has been argued that promise problems should be seen as partial decision
problems and as such that they are more fundamental than decision problems and
formal languages that used to be considered as the basic ones for complexity
theory. The main purpose of this paper is to explore the promise problems
accepted by classical, quantum and also semi-quantum finite automata. More
specifically, we first introduce two acceptance modes of promise problems,
recognizability and solvability, and explore their basic properties.
Afterwards, we show several results concerning descriptional complexity on
promise problems. In particular, we prove: (1) there is a promise problem that
can be recognized exactly by measure-once one-way quantum finite automata
(MO-1QFA), but no deterministic finite automata (DFA) can recognize it; (2)
there is a promise problem that can be solved with error probability
$\epsilon\leq 1/3$ by one-way finite automaton with quantum and classical
states (1QCFA), but no one-way probability finite automaton (PFA) can solve it
with error probability $\epsilon\leq 1/3$; and especially, (3) there are
promise problems $A(p)$ with prime $p$ that can be solved {\em with any error
probability} by MO-1QFA with only two quantum basis states, but they can not be
solved exactly by any MO-1QFA with two quantum basis states; in contrast, the
minimal PFA solving $A(p)$ with any error probability (usually smaller than
$1/2$) has $p$ states. Finally, we mention a number of problems related to
promise for further study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3880</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3880</id><created>2014-11-14</created><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Chmel&#xed;k</keyname><forenames>Martin</forenames></author><author><keyname>Gupta</keyname><forenames>Raghav</forenames></author><author><keyname>Kanodia</keyname><forenames>Ayush</forenames></author></authors><title>Optimal Cost Almost-sure Reachability in POMDPs</title><categories>cs.AI</categories><comments>Full Version of Optimal Cost Almost-sure Reachability in POMDPs, AAAI
  2015. arXiv admin note: text overlap with arXiv:1207.4166 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider partially observable Markov decision processes (POMDPs) with a
set of target states and every transition is associated with an integer cost.
The optimization objective we study asks to minimize the expected total cost
till the target set is reached, while ensuring that the target set is reached
almost-surely (with probability 1). We show that for integer costs
approximating the optimal cost is undecidable. For positive costs, our results
are as follows: (i) we establish matching lower and upper bounds for the
optimal cost and the bound is double exponential; (ii) we show that the problem
of approximating the optimal cost is decidable and present approximation
algorithms developing on the existing algorithms for POMDPs with finite-horizon
objectives. While the worst-case running time of our algorithm is double
exponential, we also present efficient stopping criteria for the algorithm and
show experimentally that it performs well in many examples of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3887</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3887</id><created>2014-11-14</created><updated>2015-08-18</updated><authors><author><keyname>Im</keyname><forenames>Sungjin</forenames></author><author><keyname>Kell</keyname><forenames>Nathaniel</forenames></author><author><keyname>Kulkarni</keyname><forenames>Janardhan</forenames></author><author><keyname>Panigrahi</keyname><forenames>Debmalya</forenames></author></authors><title>Tight Bounds for Online Vector Scheduling</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern data centers face a key challenge of effectively serving user requests
that arrive online. Such requests are inherently multi-dimensional and
characterized by demand vectors over multiple resources such as processor
cycles, storage space, and network bandwidth. Typically, different resources
require different objectives to be optimized, and $L_r$ norms of loads are
among the most popular objectives considered. To address these problems, we
consider the online vector scheduling problem in this paper. Introduced by
Chekuri and Khanna (SIAM J of Comp. 2006), vector scheduling is a
generalization of classical load balancing, where every job has a vector load
instead of a scalar load. In this paper, we resolve the online complexity of
the vector scheduling problem and its important generalizations. Our main
results are:
  -For identical machines, we show that the optimal competitive ratio is
$\Theta(\log d / \log \log d)$ by giving an online lower bound and an algorithm
with an asymptotically matching competitive ratio. The lower bound is
technically challenging, and is obtained via an online lower bound for the
minimum mono-chromatic clique problem using a novel online coloring game and
randomized coding scheme.
  -For unrelated machines, we show that the optimal competitive ratio is
$\Theta(\log m + \log d)$ by giving an online lower bound that matches a
previously known upper bound. Unlike identical machines, however, extending
these results, particularly the upper bound, to general $L_r$ norms requires
new ideas. In particular, we use a carefully constructed potential function
that balances the individual $L_r$ objectives with the overall (convexified)
min-max objective to guide the online algorithm and track the changes in
potential to bound the competitive ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3895</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3895</id><created>2014-11-14</created><authors><author><keyname>Rodr&#xed;guez-Fdez</keyname><forenames>I.</forenames></author><author><keyname>Mucientes</keyname><forenames>M.</forenames></author><author><keyname>Bugar&#xed;n</keyname><forenames>A.</forenames></author></authors><title>Learning Fuzzy Controllers in Mobile Robotics with Embedded
  Preprocessing</title><categories>cs.RO cs.AI cs.LG</categories><journal-ref>Applied Soft Computing, vol 26, pp. 123-142, 2015</journal-ref><doi>10.1016/j.asoc.2014.09.021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The automatic design of controllers for mobile robots usually requires two
stages. In the first stage,sensorial data are preprocessed or transformed into
high level and meaningful values of variables whichare usually defined from
expert knowledge. In the second stage, a machine learning technique is applied
toobtain a controller that maps these high level variables to the control
commands that are actually sent tothe robot. This paper describes an algorithm
that is able to embed the preprocessing stage into the learningstage in order
to get controllers directly starting from sensorial raw data with no expert
knowledgeinvolved. Due to the high dimensionality of the sensorial data, this
approach uses Quantified Fuzzy Rules(QFRs), that are able to transform
low-level input variables into high-level input variables, reducingthe
dimensionality through summarization. The proposed learning algorithm, called
Iterative QuantifiedFuzzy Rule Learning (IQFRL), is based on genetic
programming. IQFRL is able to learn rules with differentstructures, and can
manage linguistic variables with multiple granularities. The algorithm has been
testedwith the implementation of the wall-following behavior both in several
realistic simulated environmentswith different complexity and on a Pioneer 3-AT
robot in two real environments. Results have beencompared with several
well-known learning algorithms combined with different data
preprocessingtechniques, showing that IQFRL exhibits a better and statistically
significant performance. Moreover,three real world applications for which IQFRL
plays a central role are also presented: path and objecttracking with static
and moving obstacles avoidance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3902</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3902</id><created>2014-11-14</created><updated>2015-05-04</updated><authors><author><keyname>Korner</keyname><forenames>Janos</forenames></author><author><keyname>Monti</keyname><forenames>Angelo</forenames></author></authors><title>Families of locally separated Hamilton paths</title><categories>math.CO cs.IT math.IT</categories><comments>In this version an error in the previous manuscript is corrected</comments><msc-class>05D99, 05C35, 05C62, 94A24</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We improve by an exponential factor the lower bound of Korner and Muzi for
the cardinality of the largest family of Hamilton paths in a complete graph of
n vertices in which the union of any two paths has degree 4. The improvement is
through an explicit construction while the previous bound was obtained by a
greedy algorithm. We solve a similar problem for permutations up to an
exponential factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3908</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3908</id><created>2014-11-14</created><authors><author><keyname>Maseng</keyname><forenames>Torleiv</forenames></author><author><keyname>Kure</keyname><forenames>&#xd8;ivind</forenames></author><author><keyname>Skjegstad</keyname><forenames>Magnus</forenames></author></authors><title>Distributed Discovery Clients for Spectrum Allocation</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By using a distributed P2P system where the agents reside in in
microprocessors already present in most radio nodes like Wi-Fi access points,
base stations, TVs connected to Internet etc, these agents can discover other
agents over the back-haul network. As a result, client node lists (similar to
neighbor lists used in 3GPP) are created. An alternative is to use a
centralized database. Why this best is done by distributed agents is discussed
in this paper along with security considerations. Examples of other
applications which will benefit from this system is also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3919</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3919</id><created>2014-11-14</created><authors><author><keyname>Arandjelovic</keyname><forenames>Ognjen</forenames></author></authors><title>Sample-targeted clinical trial adaptation</title><categories>cs.LG</categories><comments>AAAI Conference on Artificial Intelligence, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clinical trial adaptation refers to any adjustment of the trial protocol
after the onset of the trial. The main goal is to make the process of
introducing new medical interventions to patients more efficient by reducing
the cost and the time associated with evaluating their safety and efficacy. The
principal question is how should adaptation be performed so as to minimize the
chance of distorting the outcome of the trial. We propose a novel method for
achieving this. Unlike previous work our approach focuses on trial adaptation
by sample size adjustment. We adopt a recently proposed stratification
framework based on collected auxiliary data and show that this information
together with the primary measured variables can be used to make a
probabilistically informed choice of the particular sub-group a sample should
be removed from. Experiments on simulated data are used to illustrate the
effectiveness of our method and its application in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3923</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3923</id><created>2014-11-13</created><authors><author><keyname>Alexandersen</keyname><forenames>Joe</forenames></author><author><keyname>Lazarov</keyname><forenames>Boyan S.</forenames></author></authors><title>Robust topology optimisation of microstructural details without length
  scale separation - using a spectral coarse basis preconditioner</title><categories>cs.CE</categories><journal-ref>Comput.Method.Appl.M. 290 (2015) 156-182</journal-ref><doi>10.1016/j.cma.2015.02.028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper applies topology optimisation to the design of structures with
periodic microstructural details without length scale separation, i.e.
considering the complete macroscopic structure and its response, while
resolving all microstructural details, as compared to the often used
homogenisation approach. The approach takes boundary conditions into account
and ensures connected and macroscopically optimised microstructures regardless
of the difference in micro- and macroscopic length scales. This results in
microstructures tailored for specific applications rather than specific
properties.
  Dealing with the complete macroscopic structure and its response is
computationally challenging as very fine discretisations are needed in order to
resolve all microstructural details. Therefore, this article shows the benefits
of applying a contrast-independent spectral preconditioner based on the
multiscale finite element method (MsFEM) to large structures with
fully-resolved microstructural details.
  The density-based topology optimisation approach combined with a Heaviside
projection filter and a stochastic robust formulation is used on various
problems, with both periodic and layered microstructures. The presented
approach is shown to allow for the topology optimisation of very large problems
in \textsc{Matlab}, specifically a problem with 26 million displacement degrees
of freedom in 26 hours using a single computational thread.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3929</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3929</id><created>2014-11-14</created><authors><author><keyname>Athreyas</keyname><forenames>Nihar</forenames></author><author><keyname>Lai</keyname><forenames>Zhiguo</forenames></author><author><keyname>Gupta</keyname><forenames>Jai</forenames></author><author><keyname>Gupta</keyname><forenames>Dev</forenames></author></authors><title>Analog Signal Processing Solution for Image Alignment</title><categories>cs.AR</categories><comments>Third International Conference on Advanced Information Technologies &amp;
  Applications (ICAITA-2014) November 7~8, Dubai, UAE ISBN : 978-1-921987-17-5</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Imaging and Image sensors is a field that is continuously evolving. There are
new products coming into the market every day. Some of these have very severe
Size, Weight and Power constraints whereas other devices have to handle very
high computational loads. Some require both these conditions to be met
simultaneously. Current imaging architectures and digital image processing
solutions will not be able to meet these ever increasing demands. There is a
need to develop novel imaging architectures and image processing solutions to
address these requirements. In this work we propose analog signal processing as
a solution to this problem. The analog processor is not suggested as a
replacement to a digital processor but it will be used as an augmentation
device which works in parallel with the digital processor, making the system
faster and more efficient. In order to show the merits of analog processing the
highly computational Normalized Cross Correlation algorithm is implemented. We
propose two novel modifications to the algorithm and a new imaging architecture
which, significantly reduces the computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3935</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3935</id><created>2014-11-14</created><authors><author><keyname>Amelio</keyname><forenames>Alessia</forenames></author><author><keyname>Pizzuti</keyname><forenames>Clara</forenames></author></authors><title>Overlapping Community Discovery Methods: A Survey</title><categories>cs.SI physics.soc-ph</categories><comments>20 pages, Book Chapter, appears as Social networks: Analysis and Case
  Studies, A. Gunduz-Oguducu and A. S. Etaner-Uyar eds, Lecture Notes in Social
  Networks, pp. 105-125, Springer,2014</comments><doi>10.1007/978-3-7091-1797-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The detection of overlapping communities is a challenging problem which is
gaining increasing interest in recent years because of the natural attitude of
individuals, observed in real-world networks, to participate in multiple groups
at the same time. This review gives a description of the main proposals in the
field. Besides the methods designed for static networks, some new approaches
that deal with the detection of overlapping communities in networks that change
over time, are described. Methods are classified with respect to the underlying
principles guiding them to obtain a network division in groups sharing part of
their nodes. For each of them we also report, when available, computational
complexity and web site address from which it is possible to download the
software implementing the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3948</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3948</id><created>2014-11-14</created><authors><author><keyname>Sommerville</keyname><forenames>Ian</forenames></author></authors><title>Designing for the Don't Cares: A story about a sociotechnical system</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article discusses the difficulties that arose when attempting to specify
and design a large scale digital learning environment for Scottish schools.
This had a potential user base of about 1 million users and was intended to
replace an existing, under-used system. We found that the potential system
users were not interested in engaging with the project and that there were
immense problems with system governance. The only technique that we found to be
useful were user stories, presenting scenarios of how the system might be used
by students and their teachers. The designed architecture was based around a
layered set of replaceable services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3949</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3949</id><created>2014-11-14</created><updated>2015-01-22</updated><authors><author><keyname>Akinwande</keyname><forenames>Olumide J.</forenames></author><author><keyname>Bi</keyname><forenames>Huibo</forenames></author></authors><title>Routing Diverse Crowds in Emergency with Dynamic Grouping</title><categories>cs.OH</categories><comments>Contains 6 pages, 5 pages. Accepted by PerNEM' 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evacuee routing algorithms in emergency typically adopt one single criterion
to compute desired paths and ignore the specific requirements of users caused
by different physical strength, mobility and level of resistance to hazard. In
this paper, we present a quality of service (QoS) driven multi-path routing
algorithm to provide diverse paths for different categories of evacuees. This
algorithm borrows the concept of Cognitive Packet Network (CPN), which is a
flexible protocol that can rapidly solve optimal solution for any user-defined
goal function. Spatial information regarding the location and spread of hazards
is taken into consideration to avoid that evacuees be directed towards
hazardous zones. Furthermore, since previous emergency navigation algorithms
are normally insensitive to sudden changes in the hazard environment such as
abrupt congestion or injury of civilians, evacuees are dynamically assigned to
several groups to adapt their course of action with regard to their on-going
physical condition and environments. Simulation results indicate that the
proposed algorithm which is sensitive to the needs of evacuees produces better
results than the use of a single metric. Simulations also show that the use of
dynamic grouping to adjust the evacuees' category and routing algorithms with
regard for their on-going health conditions and mobility, can achieve higher
survival rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3961</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3961</id><created>2014-11-14</created><updated>2014-12-01</updated><authors><author><keyname>Blanco-Justicia</keyname><forenames>Alberto</forenames></author><author><keyname>Domingo-Ferrer</keyname><forenames>Josep</forenames></author></authors><title>Privacy-preserving Loyalty Programs</title><categories>cs.CR</categories><comments>Presented at the 9th DPM International Workshop on Data Privacy
  Management (DPM 2014, held on Sep. 10, 2014). To appear in workshop
  proceedings, LNCS, Springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Loyalty programs are promoted by vendors to incentivize loyalty in buyers.
Although such programs have become widespread, they have been criticized by
business experts and consumer associations: loyalty results in profiling and
hence in loss of privacy of consumers. We propose a protocol for
privacy-preserving loyalty programs that allows vendors and consumers to enjoy
the benefits of loyalty (returning customers and discounts, respectively),
while allowing consumers to stay anonymous and empowering them to decide how
much of their profile they reveal to the vendor. The vendor must offer
additional reward if he wants to learn more details on the consumer's profile.
Our protocol is based on partially blind signatures and generalization
techniques, and provides anonymity to consumers and their purchases, while
still allowing negotiated consumer profiling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3962</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3962</id><created>2014-11-14</created><updated>2015-10-05</updated><authors><author><keyname>Darais</keyname><forenames>David</forenames></author><author><keyname>Might</keyname><forenames>Matthew</forenames></author><author><keyname>Van Horn</keyname><forenames>David</forenames></author></authors><title>Galois Transformers and Modular Abstract Interpreters</title><categories>cs.PL</categories><comments>OOPSLA '15, October 25-30, 2015, Pittsburgh, PA, USA</comments><acm-class>F.3.2</acm-class><doi>10.1145/2814270.2814308</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design and implementation of static analyzers has become increasingly
systematic. Yet for a given language or analysis feature, it often requires
tedious and error prone work to implement an analyzer and prove it sound. In
short, static analysis features and their proofs of soundness do not compose
well, causing a dearth of reuse in both implementation and metatheory.
  We solve the problem of systematically constructing static analyzers by
introducing Galois transformers: monad transformers that transport Galois
connection properties. In concert with a monadic interpreter, we define a
library of monad transformers that implement building blocks for classic
analysis parameters like context, path, and heap (in)sensitivity. Moreover,
these can be composed together independent of the language being analyzed.
  Significantly, a Galois transformer can be proved sound once and for all,
making it a reusable analysis component. As new analysis features and
abstractions are developed and mixed in, soundness proofs need not be
reconstructed, as the composition of a monad transformer stack is sound by
virtue of its constituents. Galois transformers provide a viable foundation for
reusable and composable metatheory for program analysis.
  Finally, these Galois transformers shift the level of abstraction in analysis
design and implementation to a level where non-specialists have the ability to
synthesize sound analyzers over a number of parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3967</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3967</id><created>2014-11-14</created><updated>2015-04-21</updated><authors><author><keyname>Nguyen</keyname><forenames>Phuc C.</forenames></author><author><keyname>Van Horn</keyname><forenames>David</forenames></author></authors><title>Relatively Complete Counterexamples for Higher-Order Programs</title><categories>cs.PL</categories><comments>In Proceedings of the 36th annual ACM SIGPLAN conference on
  Programming Language Design and Implementation, Portland, Oregon, June 2015</comments><acm-class>D.2.4; D.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of generating inputs to a higher-order
program causing it to error. We first study the problem in the setting of PCF,
a typed, core functional language and contribute the first relatively complete
method for constructing counterexamples for PCF programs. The method is
relatively complete in the sense of Hoare logic; completeness is reduced to the
completeness of a first-order solver over the base types of PCF. In practice,
this means an SMT solver can be used for the effective, automated generation of
higher-order counterexamples for a large class of programs.
  We achieve this result by employing a novel form of symbolic execution for
higher-order programs. The remarkable aspect of this symbolic execution is that
even though symbolic higher-order inputs and values are considered, the path
condition remains a first-order formula. Our handling of symbolic function
application enables the reconstruction of higher-order counterexamples from
this first-order formula.
  After establishing our main theoretical results, we sketch how to apply the
approach to untyped, higher-order, stateful languages with first-class
contracts and show how counterexample generation can be used to detect contract
violations in this setting. To validate our approach, we implement a tool
generating counterexamples for erroneous modules written in Racket.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3969</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3969</id><created>2014-11-13</created><authors><author><keyname>Liao</keyname><forenames>Y.</forenames><affiliation>PPGEPS</affiliation></author><author><keyname>Lezoche</keyname><forenames>M.</forenames><affiliation>CRAN</affiliation></author><author><keyname>Panetto</keyname><forenames>H.</forenames><affiliation>CRAN</affiliation></author><author><keyname>Boudjlida</keyname><forenames>N.</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Loures</keyname><forenames>Eduardo Rocha</forenames><affiliation>ISE</affiliation></author></authors><title>Formal Semantic Annotations for Models Interoperability in a PLM
  environment</title><categories>cs.SE cs.DB</categories><proxy>ccsd</proxy><journal-ref>Proceedings of the 19th IFAC World Congress, 2014, Aug 2014, Cape
  Town International Convention Centre, Cape Town, South Africa, South Africa.
  International Federation of Automatic Control, World Congress, Volume # 19 |
  Part# 1, 19 (1), pp.2382-2393, World Congress. http://www.ifac2014.org/</journal-ref><doi>10.3182/20140824-6-ZA-1003.02551</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, the need for system interoperability in or across enterprises has
become more and more ubiquitous. Lots of research works have been carried out
in the information exchange, transformation, discovery and reuse. One of the
main challenges in these researches is to overcome the semantic heterogeneity
between enterprise applications along the lifecycle of a product. As a possible
solution to assist the semantic interoperability, semantic annotation has
gained more and more attentions and is widely used in different domains. In
this paper, based on the investigation of the context and the related works, we
identify some existing drawbacks and propose a formal semantic annotation
approach to support the semantics enrichment of models in a PLM environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3970</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3970</id><created>2014-11-14</created><updated>2015-10-09</updated><authors><author><keyname>Reynolds</keyname><forenames>Andrew</forenames></author></authors><title>Approaches for Synthesis Conjectures in an SMT Solver</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report describes several approaches for handling synthesis conjectures
within an Satisfiability Modulo Theories (SMT) solver. We describe approaches
that primarily focus on determining the unsatisfiability of the negated form of
synthesis conjectures using new techniques for quantifier instantiation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3990</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3990</id><created>2014-11-14</created><updated>2015-05-06</updated><authors><author><keyname>Song</keyname><forenames>Jiajia</forenames></author><author><keyname>Cotilla-Sanchez</keyname><forenames>Eduardo</forenames></author><author><keyname>Ghanavati</keyname><forenames>Goodarz</forenames></author><author><keyname>Hines</keyname><forenames>Paul D. H.</forenames></author></authors><title>Dynamic Modeling of Cascading Failure in Power Systems</title><categories>physics.soc-ph cs.SY</categories><comments>8 pages, 9 figures, 5 tables, submitted to IEEE transactions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modeling of cascading failure in power systems is difficult because of
the many different mechanisms involved; no single model captures all of these
mechanisms. Understanding the relative importance of these different mechanisms
is an important step in choosing which mechanisms need to be modeled for
particular types of cascading failure analysis. This work presents a dynamic
simulation model of both power networks and protection systems, which can
simulate a wider variety of cascading outage mechanisms, relative to existing
quasi-steady state (QSS) models. The model allows one to test the impact of
different load models and protections on cascading outage sizes. This paper
describes each module of the developed dynamic model and demonstrates how
different mechanisms interact. In order to test the model we simulated a batch
of randomly selected $N-2$ contingencies for several different static load
configurations, and found that the distribution of blackout sizes and event
lengths from the proposed dynamic simulator correlates well with historical
trends. The results also show that load models have significant impacts on the
cascading risks. This dynamic model was also compared against a QSS model based
on the dc power flow approximations; we find that the two models largely agree,
but produce substantially different results for later stages of cascading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.3995</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.3995</id><created>2014-11-14</created><updated>2014-12-22</updated><authors><author><keyname>Buss</keyname><forenames>Sam</forenames><affiliation>University of California, San Diego</affiliation></author><author><keyname>Cenzer</keyname><forenames>Douglas</forenames><affiliation>University of Florida</affiliation></author><author><keyname>Remmel</keyname><forenames>Jeffrey B.</forenames><affiliation>University of California, San Deigo</affiliation></author></authors><title>Sub-computable Boundedness Randomness</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (December
  24, 2014) lmcs:979</journal-ref><doi>10.2168/LMCS-10(4:15)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper defines a new notion of bounded computable randomness for certain
classes of sub-computable functions which lack a universal machine. In
particular, we define such versions of randomness for primitive recursive
functions and for PSPACE functions. These new notions are robust in that there
are equivalent formulations in terms of (1) Martin-L\&quot;of tests, (2) Kolmogorov
complexity, and (3) martingales. We show these notions can be equivalently
defined with prefix-free Kolmogorov complexity. We prove that one direction of
van Lambalgen's theorem holds for relative computability, but the other
direction fails. We discuss statistical properties of these notions of
randomness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4000</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4000</id><created>2014-11-14</created><updated>2015-06-17</updated><authors><author><keyname>Lu</keyname><forenames>Zhiyun</forenames></author><author><keyname>May</keyname><forenames>Avner</forenames></author><author><keyname>Liu</keyname><forenames>Kuan</forenames></author><author><keyname>Garakani</keyname><forenames>Alireza Bagheri</forenames></author><author><keyname>Guo</keyname><forenames>Dong</forenames></author><author><keyname>Bellet</keyname><forenames>Aur&#xe9;lien</forenames></author><author><keyname>Fan</keyname><forenames>Linxi</forenames></author><author><keyname>Collins</keyname><forenames>Michael</forenames></author><author><keyname>Kingsbury</keyname><forenames>Brian</forenames></author><author><keyname>Picheny</keyname><forenames>Michael</forenames></author><author><keyname>Sha</keyname><forenames>Fei</forenames></author></authors><title>How to Scale Up Kernel Methods to Be As Good As Deep Neural Nets</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computational complexity of kernel methods has often been a major barrier
for applying them to large-scale learning problems. We argue that this barrier
can be effectively overcome. In particular, we develop methods to scale up
kernel models to successfully tackle large-scale learning problems that are so
far only approachable by deep learning architectures. Based on the seminal work
by Rahimi and Recht on approximating kernel functions with features derived
from random projections, we advance the state-of-the-art by proposing methods
that can efficiently train models with hundreds of millions of parameters, and
learn optimal representations from multiple kernels. We conduct extensive
empirical studies on problems from image recognition and automatic speech
recognition, and show that the performance of our kernel models matches that of
well-engineered deep neural nets (DNNs). To the best of our knowledge, this is
the first time that a direct comparison between these two methods on
large-scale problems is reported. Our kernel methods have several appealing
properties: training with convex optimization, cost for training a single model
comparable to DNNs, and significantly reduced total cost due to fewer
hyperparameters to tune for model selection. Our contrastive study between
these two very different but equally competitive models sheds light on
fundamental questions such as how to learn good representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4005</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4005</id><created>2014-11-14</created><authors><author><keyname>Sim&#xf5;es</keyname><forenames>Miguel</forenames></author><author><keyname>Bioucas-Dias</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Almeida</keyname><forenames>Luis B.</forenames></author><author><keyname>Chanussot</keyname><forenames>Jocelyn</forenames></author></authors><title>A convex formulation for hyperspectral image superresolution via
  subspace-based regularization</title><categories>cs.CV physics.data-an stat.ML</categories><comments>IEEE Trans. Geosci. Remote Sens., to be published</comments><doi>10.1109/TGRS.2014.2375320</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral remote sensing images (HSIs) usually have high spectral
resolution and low spatial resolution. Conversely, multispectral images (MSIs)
usually have low spectral and high spatial resolutions. The problem of
inferring images which combine the high spectral and high spatial resolutions
of HSIs and MSIs, respectively, is a data fusion problem that has been the
focus of recent active research due to the increasing availability of HSIs and
MSIs retrieved from the same geographical area.
  We formulate this problem as the minimization of a convex objective function
containing two quadratic data-fitting terms and an edge-preserving regularizer.
The data-fitting terms account for blur, different resolutions, and additive
noise. The regularizer, a form of vector Total Variation, promotes
piecewise-smooth solutions with discontinuities aligned across the
hyperspectral bands.
  The downsampling operator accounting for the different spatial resolutions,
the non-quadratic and non-smooth nature of the regularizer, and the very large
size of the HSI to be estimated lead to a hard optimization problem. We deal
with these difficulties by exploiting the fact that HSIs generally &quot;live&quot; in a
low-dimensional subspace and by tailoring the Split Augmented Lagrangian
Shrinkage Algorithm (SALSA), which is an instance of the Alternating Direction
Method of Multipliers (ADMM), to this optimization problem, by means of a
convenient variable splitting. The spatial blur and the spectral linear
operators linked, respectively, with the HSI and MSI acquisition processes are
also estimated, and we obtain an effective algorithm that outperforms the
state-of-the-art, as illustrated in a series of experiments with simulated and
real-life data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4006</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4006</id><created>2014-11-14</created><authors><author><keyname>Xu</keyname><forenames>Zhongwen</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author><author><keyname>Hauptmann</keyname><forenames>Alexander G.</forenames></author></authors><title>A Discriminative CNN Video Representation for Event Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a discriminative video representation for event
detection over a large scale video dataset when only limited hardware resources
are available. The focus of this paper is to effectively leverage deep
Convolutional Neural Networks (CNNs) to advance event detection, where only
frame level static descriptors can be extracted by the existing CNN toolkit.
This paper makes two contributions to the inference of CNN video
representation. First, while average pooling and max pooling have long been the
standard approaches to aggregating frame level static features, we show that
performance can be significantly improved by taking advantage of an appropriate
encoding method. Second, we propose using a set of latent concept descriptors
as the frame descriptor, which enriches visual information while keeping it
computationally affordable. The integration of the two contributions results in
a new state-of-the-art performance in event detection over the largest video
datasets. Compared to improved Dense Trajectories, which has been recognized as
the best video representation for event detection, our new representation
improves the Mean Average Precision (mAP) from 27.6% to 36.8% for the TRECVID
MEDTest 14 dataset and from 34.0% to 44.6% for the TRECVID MEDTest 13 dataset.
This work is the core part of the winning solution of our CMU-Informedia team
in TRECVID MED 2014 competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4011</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4011</id><created>2014-11-12</created><authors><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Ghorbanzadeh</keyname><forenames>Mo</forenames></author><author><keyname>Clancy</keyname><forenames>Charles</forenames></author></authors><title>Optimal Radio Resource Allocation for Hybrid Traffic in Cellular
  Networks: Centralized and Distributed Architecture</title><categories>cs.NI cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal resource allocation is of paramount importance in utilizing the
scarce radio spectrum efficiently and provisioning quality of service for
miscellaneous user applications, generating hybrid data traffic streams in
present-day wireless communications systems. A dynamism of the hybrid traffic
stemmed from concurrently running mobile applications with temporally varying
usage percentages in addition to subscriber priorities impelled from network
providers' perspective necessitate resource allocation schemes assigning the
spectrum to the applications accordingly and optimally. This manuscript
concocts novel centralized and distributed radio resource allocation
optimization problems for hybrid traffic-conveying cellular networks
communicating users with simultaneously running multiple delay-tolerant and
real-time applications modelled as logarithmic and sigmoidal utility functions,
volatile application percent usages, and diverse subscriptions. Casting under a
utility proportional fairness entail no lost calls for the proposed modi
operandi, for which we substantiate the convexity, devise computationally
efficient algorithms catering optimal rates to the applications, and prove a
mutual mathematical equivalence. Ultimately, the algorithms performance is
evaluated via simulations and discussing germane numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4012</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4012</id><created>2014-11-12</created><authors><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Ghorbanzadeh</keyname><forenames>Mo</forenames></author><author><keyname>Clancy</keyname><forenames>Charles</forenames></author></authors><title>Optimal Radio Resource Allocation for Hybrid Traffic in Cellular
  Networks: Traffic Analysis and Implementation</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In part I of this paper, a distributed and a centralized architecture for an
optimal radio resource allocation aware of the traffic delay-tolerance nature,
user subscription type, and application usage variations were developed. In the
current article, a transmission overhead analysis of the aforementioned
distributed and a centralized architectures is investigated and it is proved
that the centralized scheme endures a significantly lower transmission overhead
than does the distributed approach. Furthermore, the lower bounds of the
transmission overhead for both the centralized and the distributed
architectures are derived. Moreover, a sensitivity analysis of the resource
allocation procedures of the aforesaid centralized and distributed
architectures to the changes in the number of users in the system is presented.
Besides, a sensitivity analysis of the centralized and distributed approaches
to the temporal changes in application usages are investigated. Ultimately, the
transmission overhead and sensitivity relevant statements are verified through
appropriate simulations. And last but not the least, a real-world
implementation of the resource allocation methods developed in Part I is
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4017</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4017</id><created>2014-11-14</created><updated>2015-03-10</updated><authors><author><keyname>Dai</keyname><forenames>Liang</forenames></author><author><keyname>Sch&#xf6;n</keyname><forenames>Thomas</forenames></author></authors><title>On the exponential convergence of the Kaczmarz algorithm</title><categories>cs.SY</categories><doi>10.1109/LSP.2015.2412253</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Kaczmarz algorithm (KA) is a popular method for solving a system of
linear equations. In this note we derive a new exponential convergence result
for the KA. The key allowing us to establish the new result is to rewrite the
KA in such a way that its solution path can be interpreted as the output from a
particular dynamical system. The asymptotic stability results of the
corresponding dynamical system can then be leveraged to prove exponential
convergence of the KA. The new bound is also compared to existing bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4018</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4018</id><created>2014-11-14</created><authors><author><keyname>Dai</keyname><forenames>Liang</forenames></author><author><keyname>Sch&#xf6;n</keyname><forenames>Thomas B.</forenames></author></authors><title>A new structure exploiting derivation of recursive direct weight
  optimization</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recursive direct weight optimization method is used to solve challenging
nonlinear system identification problems. This note provides a new derivation
and a new interpretation of the method. The key underlying the note is to
acknowledge and exploit a certain structure inherent in the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4020</identifier>
 <datestamp>2014-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4020</id><created>2014-11-14</created><authors><author><keyname>Dutta</keyname><forenames>Sanghamitra</forenames></author><author><keyname>De</keyname><forenames>Arijit</forenames></author></authors><title>LAMP: A Locally Adapting Matching Pursuit Framework for Group Sparse
  Signatures in Ultra-Wide Band Radar Imaging</title><categories>cs.IT math.IT</categories><comments>14 pages,22 figures, Draft to be submitted to journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been found that radar returns of extended targets are not only sparse
but also exhibit a tendency to cluster into randomly located, variable sized
groups. However, the standard techniques of Compressive Sensing as applied in
radar imaging hardly considers the clustering tendency into account while
reconstructing the image from the compressed measurements. If the group
sparsity is taken into account, it is intuitive that one might obtain better
results both in terms of accuracy and time complexity as compared to the
conventional recovery techniques like Orthogonal Matching Pursuit (OMP). In
order to remedy this, techniques like Block OMP have been used in the existing
literature. An alternate approach is via reconstructing the signal by
transforming into the Hough Transform Domain where they become point-wise
sparse. However, these techniques essentially assume specific size and
structure of the groups and are not always effective if the exact
characteristics of the groups are not known, prior to reconstruction. In this
manuscript, a novel framework that we call locally adapting matching pursuit
(LAMP) have been proposed for efficient reconstruction of group sparse signals
from compressed measurements without assuming any specific size, location, or
structure of the groups. The recovery guarantee of the LAMP and its superiority
compared to the existing algorithms has been established with respect to
accuracy, time complexity and flexibility in group size. LAMP has been
successfully used on a real-world, experimental data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4022</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4022</id><created>2014-11-14</created><updated>2015-06-01</updated><authors><author><keyname>Skryzalin</keyname><forenames>Jacek</forenames></author><author><keyname>Carlsson</keyname><forenames>Gunnar</forenames></author></authors><title>Numeric Invariants from Multidimensional Persistence</title><categories>cs.CG</categories><comments>v1. initial upload. v2. fixed typos and rephrased sentence in
  introduction. v3. updated parameterization of rectangular persistence modules</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We extend the results of Adcock, Carlsson, and Carlsson by constructing
numeric invariants from the computation of a multidimensional persistence
module as given by Carlsson, Singh, and Zomorodian.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4023</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4023</id><created>2014-11-14</created><authors><author><keyname>Ahmed</keyname><forenames>Umair Z.</forenames></author><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Gulwani</keyname><forenames>Sumit</forenames></author></authors><title>Automatic Generation of Alternative Starting Positions for Simple
  Traditional Board Games</title><categories>cs.AI</categories><comments>A conference version of the paper will appear in AAAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simple board games, like Tic-Tac-Toe and CONNECT-4, play an important role
not only in the development of mathematical and logical skills, but also in the
emotional and social development. In this paper, we address the problem of
generating targeted starting positions for such games. This can facilitate new
approaches for bringing novice players to mastery, and also leads to discovery
of interesting game variants. We present an approach that generates starting
states of varying hardness levels for player~$1$ in a two-player board game,
given rules of the board game, the desired number of steps required for
player~$1$ to win, and the expertise levels of the two players. Our approach
leverages symbolic methods and iterative simulation to efficiently search the
extremely large state space. We present experimental results that include
discovery of states of varying hardness levels for several simple grid-based
board games. The presence of such states for standard game variants like $4
\times 4$ Tic-Tac-Toe opens up new games to be played that have never been
played as the default start state is heavily biased.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4024</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4024</id><created>2014-11-14</created><updated>2015-04-30</updated><authors><author><keyname>Peterlongo</keyname><forenames>P.</forenames></author><author><keyname>Sala</keyname><forenames>M.</forenames></author><author><keyname>Tinnirello</keyname><forenames>C.</forenames></author></authors><title>A Discrete Logarithm-based Approach to Compute Low-Weight Multiples of
  Binary Polynomials</title><categories>cs.DM cs.CR cs.IT math.IT</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Being able to compute efficiently a low-weight multiple of a given binary
polynomial is often a key ingredient of correlation attacks to LFSR-based
stream ciphers. The best known general purpose algorithm is based on the
generalized birthday problem. We describe an alternative approach which is
based on discrete logarithms and has much lower memory complexity requirements
with a comparable time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4033</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4033</id><created>2014-11-14</created><updated>2015-02-08</updated><authors><author><keyname>Baghaie</keyname><forenames>Ahmadreza</forenames></author><author><keyname>D'souza</keyname><forenames>Roshan M.</forenames></author><author><keyname>Yu</keyname><forenames>Zeyun</forenames></author></authors><title>Sparse And Low Rank Decomposition Based Batch Image Alignment for
  Speckle Reduction of retinal OCT Images</title><categories>cs.CV</categories><comments>Accepted for presentation at ISBI'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical Coherence Tomography (OCT) is an emerging technique in the field of
biomedical imaging, with applications in ophthalmology, dermatology, coronary
imaging etc. Due to the underlying physics, OCT images usually suffer from a
granular pattern, called speckle noise, which restricts the process of
interpretation. Here, a sparse and low rank decomposition based method is used
for speckle reduction in retinal OCT images. This technique works on input data
that consists of several B-scans of the same location. The next step is the
batch alignment of the images using a sparse and low-rank decomposition based
technique. Finally the denoised image is created by median filtering of the
low-rank component of the processed data. Simultaneous decomposition and
alignment of the images result in better performance in comparison to simple
registration-based methods that are used in the literature for noise reduction
of OCT images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4037</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4037</id><created>2014-11-14</created><updated>2015-04-09</updated><authors><author><keyname>Salnikov</keyname><forenames>Vladimir</forenames></author><author><keyname>Lemaitre</keyname><forenames>Sophie</forenames></author><author><keyname>Cho&#xef;</keyname><forenames>Daniel</forenames></author><author><keyname>Karamian-Surville</keyname><forenames>Philippe</forenames></author></authors><title>Measure of combined effects of morphological parameters of inclusions
  within composite materials via stochastic homogenization to determine
  effective mechanical properties</title><categories>cs.CE cond-mat.mtrl-sci</categories><comments>23 pages, updated figures, version accepted to Composite Structures
  2015</comments><doi>10.1016/j.compstruct.2015.03.076</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our previous papers we have described efficient and reliable methods of
generation of representative volume elements (RVE) perfectly suitable for
analysis of composite materials via stochastic homogenization.
  In this paper we profit from these methods to analyze the influence of the
morphology on the effective mechanical properties of the samples. More
precisely, we study the dependence of main mechanical characteristics of a
composite medium on various parameters of the mixture of inclusions composed of
spheres and cylinders. On top of that we introduce various imperfections to
inclusions and observe the evolution of effective properties related to that.
  The main computational approach used throughout the work is the FFT-based
homogenization technique, validated however by comparison with the direct
finite elements method. We give details on the features of the method and the
validation campaign as well.
  Keywords: Composite materials, Cylindrical and spherical reinforcements,
Mechanical properties, Stochastic homogenization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4038</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4038</id><created>2014-11-14</created><updated>2015-03-08</updated><authors><author><keyname>Long</keyname><forenames>Jonathan</forenames></author><author><keyname>Shelhamer</keyname><forenames>Evan</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>Fully Convolutional Networks for Semantic Segmentation</title><categories>cs.CV</categories><comments>to appear in CVPR (2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional networks are powerful visual models that yield hierarchies of
features. We show that convolutional networks by themselves, trained
end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic
segmentation. Our key insight is to build &quot;fully convolutional&quot; networks that
take input of arbitrary size and produce correspondingly-sized output with
efficient inference and learning. We define and detail the space of fully
convolutional networks, explain their application to spatially dense prediction
tasks, and draw connections to prior models. We adapt contemporary
classification networks (AlexNet, the VGG net, and GoogLeNet) into fully
convolutional networks and transfer their learned representations by
fine-tuning to the segmentation task. We then define a novel architecture that
combines semantic information from a deep, coarse layer with appearance
information from a shallow, fine layer to produce accurate and detailed
segmentations. Our fully convolutional network achieves state-of-the-art
segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012),
NYUDv2, and SIFT Flow, while inference takes one third of a second for a
typical image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4044</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4044</id><created>2014-11-14</created><updated>2014-12-16</updated><authors><author><keyname>Ivanov</keyname><forenames>Todor</forenames></author><author><keyname>Niemann</keyname><forenames>Raik</forenames></author><author><keyname>Izberovic</keyname><forenames>Sead</forenames></author><author><keyname>Rosselli</keyname><forenames>Marten</forenames></author><author><keyname>Tolle</keyname><forenames>Karsten</forenames></author><author><keyname>Zicari</keyname><forenames>Roberto V.</forenames></author></authors><title>Benchmarking DataStax Enterprise/Cassandra with HiBench</title><categories>cs.DC cs.DB</categories><comments>arXiv admin note: substantial text overlap with arXiv:1411.3811</comments><report-no>Technical Report No. 2014-2</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report evaluates the new analytical capabilities of DataStax Enterprise
(DSE) [1] through the use of standard Hadoop workloads. In particular, we run
experiments with CPU and I/O bound micro-benchmarks as well as OLAP-style
analytical query workloads. The performed tests should show that DSE is capable
of successfully executing Hadoop applications without the need to adapt them
for the underlying Cassandra distributed storage system [2]. Due to the
Cassandra File System (CFS) [3], which supports the Hadoop Distributed File
System API, Hadoop stack applications should seamlessly run in DSE. The report
is structured as follows: Section 2 provides a brief description of the
technologies involved in our study. An overview of our used hardware and
software components of the experimental environment is given in Section 3. Our
benchmark methodology is defined in Section 4. The performed experiments
together with the evaluation of the results are presented in Section 5.
Finally, Section 6 concludes with lessons learned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4045</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4045</id><created>2014-11-14</created><updated>2015-05-10</updated><authors><author><keyname>Pham</keyname><forenames>Quang-Cuong</forenames></author><author><keyname>Caron</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Lertkultanon</keyname><forenames>Puttichai</forenames></author><author><keyname>Nakamura</keyname><forenames>Yoshihiko</forenames></author></authors><title>Admissible Velocity Propagation : Beyond Quasi-Static Path Planning for
  High-Dimensional Robots</title><categories>cs.RO</categories><comments>35 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Path-velocity decomposition is an intuitive yet powerful approach to address
the complexity of kinodynamic motion planning. The difficult trajectory
planning problem is solved in two separate, simpler, steps: first, find a path
in the configuration space that satisfies the geometric constraints (path
planning), and second, find a time-parameterization of that path satisfying the
kinodynamic constraints. A fundamental requirement is that the path found in
the first step should be time-parameterizable. Most existing works fulfill this
requirement by enforcing quasi-static constraints in the path planning step,
resulting in an important loss in completeness. We propose a method that
enables path-velocity decomposition to discover truly dynamic motions, i.e.
motions that are not quasi-statically executable. At the heart of the proposed
method is a new algorithm -- Admissible Velocity Propagation -- which, given a
path and an interval of reachable velocities at the beginning of that path,
computes exactly and efficiently the interval of all the velocities the system
can reach after traversing the path while respecting the system kinodynamic
constraints. Combining this algorithm with usual sampling-based planners then
gives rise to a family of new trajectory planners that can appropriately handle
kinodynamic constraints while retaining the advantages associated with
path-velocity decomposition. We demonstrate the efficiency of the proposed
method on some difficult kinodynamic planning problems, where, in particular,
quasi-static methods are guaranteed to fail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4046</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4046</id><created>2014-11-14</created><authors><author><keyname>Keyvanrad</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Homayounpour</keyname><forenames>Mohammad Mehdi</forenames></author></authors><title>Deep Belief Network Training Improvement Using Elite Samples Minimizing
  Free Energy</title><categories>cs.LG cs.CV</categories><comments>18 pages. arXiv admin note: substantial text overlap with
  arXiv:1408.3264</comments><journal-ref>Int. J. Patt. Recogn. Artif. Intell. 29, 1551006 (2015)</journal-ref><doi>10.1142/S0218001415510064</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays this is very popular to use deep architectures in machine learning.
Deep Belief Networks (DBNs) are deep architectures that use stack of Restricted
Boltzmann Machines (RBM) to create a powerful generative model using training
data. In this paper we present an improvement in a common method that is
usually used in training of RBMs. The new method uses free energy as a
criterion to obtain elite samples from generative model. We argue that these
samples can more accurately compute gradient of log probability of training
data. According to the results, an error rate of 0.99% was achieved on MNIST
test set. This result shows that the proposed method outperforms the method
presented in the first paper introducing DBN (1.25% error rate) and general
classification methods such as SVM (1.4% error rate) and KNN (with 1.6% error
rate). In another test using ISOLET dataset, letter classification error
dropped to 3.59% compared to 5.59% error rate achieved in those papers using
this dataset. The implemented method is available online at
&quot;http://ceit.aut.ac.ir/~keyvanrad/DeeBNet Toolbox.html&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4059</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4059</id><created>2014-11-14</created><updated>2014-11-17</updated><authors><author><keyname>Berec</keyname><forenames>Vesna</forenames></author></authors><title>Phase space dynamics and control of the quantum particles associated to
  hypergraph states</title><categories>quant-ph cs.IT math.AT math.IT math.QA</categories><comments>8 pages, 5 figures, New Frontiers in Physics - ICNFP 2014</comments><journal-ref>EPJ Web of Conferences, 95, 04007 (2015)</journal-ref><doi>10.1051/epjconf/20159504007</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  As today's nanotechnology focus becomes primarily oriented toward production
and manipulation of materials at the subatomic level, allowing the performance
and complexity of interconnects where the device density accepts more than
hundreds devices on a single chip, the manipulation of semiconductor
nanostructures at the subatomic level sets its prime tasks on preserving and
adequate transmission of information encoded in specified (quantum) states. The
presented study employs the quantum communication protocol based on the
hypergraph network model where the numerical solutions of equations of motion
of quantum particles are associated to vertices (assembled with device chip),
which follow specific controllable paths in the phase space. We address these
findings towards ultimate quest for prediction and selective control of quantum
particle trajectories. In addition, presented protocols could represent
valuable tool for reducing background noise and uncertainty in low-dimensional
and operationally meaningful, scalable complex systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4061</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4061</id><created>2014-11-14</created><authors><author><keyname>Heindlmaier</keyname><forenames>Michael</forenames></author><author><keyname>Soljanin</keyname><forenames>Emina</forenames></author></authors><title>Isn't Hybrid ARQ Sufficient?</title><categories>cs.IT math.IT</categories><comments>Paper presented at Allerton Conference 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In practical systems, reliable communication is often accomplished by coding
at different network layers. We question the necessity of this approach and
examine when it can be beneficial. Through conceptually simple probabilistic
models (based on coin tossing), we argue that multicast scenarios and protocol
restrictions may make concatenated multi-layer coding preferable to physical
layer coding alone, which is mostly not the case in point-to-point
communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4064</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4064</id><created>2014-11-14</created><authors><author><keyname>Yu</keyname><forenames>Haonan</forenames></author><author><keyname>Barrett</keyname><forenames>Daniel P.</forenames></author><author><keyname>Siskind</keyname><forenames>Jeffrey Mark</forenames></author></authors><title>A Faster Method for Tracking and Scoring Videos Corresponding to
  Sentences</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prior work presented the sentence tracker, a method for scoring how well a
sentence describes a video clip or alternatively how well a video clip depicts
a sentence. We present an improved method for optimizing the same cost function
employed by this prior work, reducing the space complexity from exponential in
the sentence length to polynomial, as well as producing a qualitatively
identical result in time polynomial in the sentence length instead of
exponential. Since this new method is plug-compatible with the prior method, it
can be used for the same applications: video retrieval with sentential queries,
generating sentential descriptions of video clips, and focusing the attention
of a tracker with a sentence, while allowing these applications to scale with
significantly larger numbers of object detections, word meanings modeled with
HMMs with significantly larger numbers of states, and significantly longer
sentences, with no appreciable degradation in quality of results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4068</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4068</id><created>2014-11-14</created><authors><author><keyname>Pham</keyname><forenames>Anh T.</forenames></author><author><keyname>Raich</keyname><forenames>Raviv</forenames></author><author><keyname>Fern</keyname><forenames>Xiaoli Z.</forenames></author></authors><title>Dynamic Programming for Instance Annotation in Multi-instance
  Multi-label Learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Labeling data for classification requires significant human effort. To reduce
labeling cost, instead of labeling every instance, a group of instances (bag)
is labeled by a single bag label. Computer algorithms are then used to infer
the label for each instance in a bag, a process referred to as instance
annotation. This task is challenging due to the ambiguity regarding the
instance labels. We propose a discriminative probabilistic model for the
instance annotation problem and introduce an expectation maximization framework
for inference, based on the maximum likelihood approach. For many probabilistic
approaches, brute-force computation of the instance label posterior probability
given its bag label is exponential in the number of instances in the bag. Our
key contribution is a dynamic programming method for computing the posterior
that is linear in the number of instances. We evaluate our methods using both
benchmark and real world data sets, in the domain of bird song, image
annotation, and activity recognition. In many cases, the proposed framework
outperforms, sometimes significantly, the current state-of-the-art MIML
learning methods, both in instance label prediction and bag label prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4070</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4070</id><created>2014-11-14</created><authors><author><keyname>Jacobs</keyname><forenames>Abigail Z.</forenames></author><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author></authors><title>A unified view of generative models for networks: models, methods,
  opportunities, and challenges</title><categories>stat.ML cs.LG cs.SI physics.soc-ph</categories><comments>10 pages. To appear at the NIPS 2014 Workshop on Networks: From
  Graphs to Rich Data</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on probabilistic models of networks now spans a wide variety of
fields, including physics, sociology, biology, statistics, and machine
learning. These efforts have produced a diverse ecology of models and methods.
Despite this diversity, many of these models share a common underlying
structure: pairwise interactions (edges) are generated with probability
conditional on latent vertex attributes. Differences between models generally
stem from different philosophical choices about how to learn from data or
different empirically-motivated goals. The highly interdisciplinary nature of
work on these generative models, however, has inhibited the development of a
unified view of their similarities and differences. For instance, novel
theoretical models and optimization techniques developed in machine learning
are largely unknown within the social and biological sciences, which have
instead emphasized model interpretability. Here, we describe a unified view of
generative models for networks that draws together many of these disparate
threads and highlights the fundamental similarities and differences that span
these fields. We then describe a number of opportunities and challenges for
future work that are revealed by this view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4072</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4072</id><created>2014-11-14</created><authors><author><keyname>Yang</keyname><forenames>Bishan</forenames></author><author><keyname>Yih</keyname><forenames>Wen-tau</forenames></author><author><keyname>He</keyname><forenames>Xiaodong</forenames></author><author><keyname>Gao</keyname><forenames>Jianfeng</forenames></author><author><keyname>Deng</keyname><forenames>Li</forenames></author></authors><title>Learning Multi-Relational Semantics Using Neural-Embedding Models</title><categories>cs.CL cs.LG stat.ML</categories><comments>7 pages, 2 figures, NIPS 2014 workshop on Learning Semantics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a unified framework for modeling multi-relational
representations, scoring, and learning, and conduct an empirical study of
several recent multi-relational embedding models under the framework. We
investigate the different choices of relation operators based on linear and
bilinear transformations, and also the effects of entity representations by
incorporating unsupervised vectors pre-trained on extra textual resources. Our
results show several interesting findings, enabling the design of a simple
embedding model that achieves the new state-of-the-art performance on a popular
knowledge base completion task evaluated on Freebase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4073</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4073</id><created>2014-11-14</created><authors><author><keyname>Nasre</keyname><forenames>Meghana</forenames></author><author><keyname>Pontecorvi</keyname><forenames>Matteo</forenames></author><author><keyname>Ramachandran</keyname><forenames>Vijaya</forenames></author></authors><title>Decremental All-Pairs ALL Shortest Paths and Betweenness Centrality</title><categories>cs.DS</categories><comments>An extended abstract of this paper will appear in Proc. ISAAC 2014</comments><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the all pairs all shortest paths (APASP) problem, which maintains
the shortest path dag rooted at every vertex in a directed graph G=(V,E) with
positive edge weights. For this problem we present a decremental algorithm
(that supports the deletion of a vertex, or weight increases on edges incident
to a vertex). Our algorithm runs in amortized O(\vstar^2 \cdot \log n) time per
update, where n=|V|, and \vstar bounds the number of edges that lie on shortest
paths through any given vertex. Our APASP algorithm can be used for the
decremental computation of betweenness centrality (BC), a graph parameter that
is widely used in the analysis of large complex networks. No nontrivial
decremental algorithm for either problem was known prior to our work. Our
method is a generalization of the decremental algorithm of Demetrescu and
Italiano [DI04] for unique shortest paths, and for graphs with \vstar =O(n), we
match the bound in [DI04]. Thus for graphs with a constant number of shortest
paths between any pair of vertices, our algorithm maintains APASP and BC scores
in amortized time O(n^2 \log n) under decremental updates, regardless of the
number of edges in the graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4074</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4074</id><created>2014-11-14</created><authors><author><keyname>Huber</keyname><forenames>Mark</forenames></author></authors><title>Improving Monte Carlo randomized approximation schemes</title><categories>math.ST cs.CC math.PR stat.TH</categories><comments>12 pages</comments><msc-class>68U20, 62F25, 68W20,</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a central problem in randomized approximation schemes that use a
Monte Carlo approach. Given a sequence of independent, identically distributed
random variables $X_1,X_2,\ldots$ with mean $\mu$ and standard deviation at
most $c \mu$, where $c$ is a known constant, and $\epsilon,\delta &gt; 0$, create
an estimate $\hat \mu$ for $\mu$ such that $\text{P}(|\hat \mu - \mu| &gt;
\epsilon \mu) \leq \delta$. This technique has been used for building
randomized approximation schemes for the volume of a convex body, the permanent
of a nonnegative matrix, the number of linear extensions of a poset, the
partition function of the Ising model and many other problems. Existing methods
use (to the leading order) $19.35 (c/\epsilon)^2 \ln(\delta^{-1})$ samples.
This is the best possible number up to the constant factor, and it is an open
question as to what is the best constant possible. This work gives an easy to
apply estimate that only uses $6.96 (c/\epsilon)^2 \ln(\delta^{-1})$ samples in
the leading order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4076</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4076</id><created>2014-11-14</created><authors><author><keyname>Dhawan</keyname><forenames>Amiraj</forenames></author><author><keyname>Bhave</keyname><forenames>Shruti</forenames></author><author><keyname>Aurora</keyname><forenames>Amrita</forenames></author><author><keyname>Iyer</keyname><forenames>Vishwanathan</forenames></author></authors><title>Association Rule Based Flexible Machine Learning Module for Embedded
  System Platforms like Android</title><categories>cs.CY cs.HC cs.LG</categories><comments>International Journal of Advanced Research in Artificial
  Intelligence(IJARAI), Volume 3 Issue 1, 2014</comments><doi>10.14569/IJARAI.2014.030101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The past few years have seen a tremendous growth in the popularity of
smartphones. As newer features continue to be added to smartphones to increase
their utility, their significance will only increase in future. Combining
machine learning with mobile computing can enable smartphones to become
'intelligent' devices, a feature which is hitherto unseen in them. Also, the
combination of machine learning and context aware computing can enable
smartphones to gauge user's requirements proactively, depending upon their
environment and context. Accordingly, necessary services can be provided to
users.
  In this paper, we have explored the methods and applications of integrating
machine learning and context aware computing on the Android platform, to
provide higher utility to the users. To achieve this, we define a Machine
Learning (ML) module which is incorporated in the basic Android architecture.
Firstly, we have outlined two major functionalities that the ML module should
provide. Then, we have presented three architectures, each of which
incorporates the ML module at a different level in the Android architecture.
The advantages and shortcomings of each of these architectures have been
evaluated. Lastly, we have explained a few applications in which our proposed
system can be incorporated such that their functionality is improved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4080</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4080</id><created>2014-11-14</created><authors><author><keyname>Redi</keyname><forenames>Miriam</forenames></author><author><keyname>Hare</keyname><forenames>Neil O</forenames></author><author><keyname>Schifanella</keyname><forenames>Rossano</forenames></author><author><keyname>Trevisiol</keyname><forenames>Michele</forenames></author><author><keyname>Jaimes</keyname><forenames>Alejandro</forenames></author></authors><title>6 Seconds of Sound and Vision: Creativity in Micro-Videos</title><categories>cs.MM cs.CV cs.HC</categories><comments>8 pages, 1 figures, conference IEEE CVPR 2014</comments><doi>10.1109/CVPR.2014.544</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of creativity, as opposed to related concepts such as beauty or
interestingness, has not been studied from the perspective of automatic
analysis of multimedia content. Meanwhile, short online videos shared on social
media platforms, or micro-videos, have arisen as a new medium for creative
expression. In this paper we study creative micro-videos in an effort to
understand the features that make a video creative, and to address the problem
of automatic detection of creative content. Defining creative videos as those
that are novel and have aesthetic value, we conduct a crowdsourcing experiment
to create a dataset of over 3,800 micro-videos labelled as creative and
non-creative. We propose a set of computational features that we map to the
components of our definition of creativity, and conduct an analysis to
determine which of these features correlate most with creative video. Finally,
we evaluate a supervised approach to automatically detect creative video, with
promising results, showing that it is necessary to model both aesthetic value
and novelty to achieve optimal classification accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4086</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4086</id><created>2014-11-14</created><authors><author><keyname>Li</keyname><forenames>Hongwei</forenames></author><author><keyname>Yu</keyname><forenames>Bin</forenames></author></authors><title>Error Rate Bounds and Iterative Weighted Majority Voting for
  Crowdsourcing</title><categories>stat.ML cs.HC cs.LG math.PR math.ST stat.TH</categories><comments>Journal Submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing has become an effective and popular tool for human-powered
computation to label large datasets. Since the workers can be unreliable, it is
common in crowdsourcing to assign multiple workers to one task, and to
aggregate the labels in order to obtain results of high quality. In this paper,
we provide finite-sample exponential bounds on the error rate (in probability
and in expectation) of general aggregation rules under the Dawid-Skene
crowdsourcing model. The bounds are derived for multi-class labeling, and can
be used to analyze many aggregation methods, including majority voting,
weighted majority voting and the oracle Maximum A Posteriori (MAP) rule. We
show that the oracle MAP rule approximately optimizes our upper bound on the
mean error rate of weighted majority voting in certain setting. We propose an
iterative weighted majority voting (IWMV) method that optimizes the error rate
bound and approximates the oracle MAP rule. Its one step version has a provable
theoretical guarantee on the error rate. The IWMV method is intuitive and
computationally simple. Experimental results on simulated and real data show
that IWMV performs at least on par with the state-of-the-art methods, and it
has a much lower computational cost (around one hundred times faster) than the
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4097</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4097</id><created>2014-11-14</created><updated>2015-12-04</updated><authors><author><keyname>Yang</keyname><forenames>Zhi</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author></authors><title>A Game Theoretic Model for the Formation of Navigable Small-World
  Networks --- the Balance between Distance and Reciprocity</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kleinberg proposed a family of small-world networks to explain the
navigability of large-scale real-world social networks. However, the underlying
mechanism that drives real networks to be navigable is not yet well understood.
In this paper, we present a game theoretic model for the formation of navigable
small world networks. We model the network formation as a Distance-Reciprocity
Balanced (DRB) game in which people seek for both high reciprocity and
long-distance relationships. We show that the game has only two Nash
equilibria: One is the navigable small-world network, and the other is the
random network in which each node connects with each other node with equal
probability. We further show that the navigable small world is very stable ---
(a) no collusion of any size would benefit from deviating from it; and (b)
after an arbitrary deviations of a large random set of nodes, the network would
return to the navigable small world as soon as every node takes one
best-response step. In contrast, for the random network, a small group
collusion or random perturbations is guaranteed to move the network to the
navigable network as soon as every node takes one best-response step. Moreover,
we show that navigable small world has much better social welfare than the
random network, and provide the price-of-anarchy and price-of-stability results
of the game. Our empirical evaluation demonstrates that the system always
converges to the navigable network even when limited or no information about
other players' strategies is available, and the DRB game simulated on the
real-world network leads to navigability characteristic that is very close to
that of the real network. Our theoretical and empirical analyses provide
important new insight on the connection between distance, reciprocity and
navigability in social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4098</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4098</id><created>2014-11-14</created><authors><author><keyname>Sawhney</keyname><forenames>Rahul</forenames></author><author><keyname>Li</keyname><forenames>Fuxin</forenames></author><author><keyname>Christensen</keyname><forenames>Henrik I.</forenames></author></authors><title>GASP : Geometric Association with Surface Patches</title><categories>cs.CV cs.GR cs.RO</categories><comments>International Conference on 3D Vision, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental challenge to sensory processing tasks in perception and
robotics is the problem of obtaining data associations across views. We present
a robust solution for ascertaining potentially dense surface patch (superpixel)
associations, requiring just range information. Our approach involves
decomposition of a view into regularized surface patches. We represent them as
sequences expressing geometry invariantly over their superpixel neighborhoods,
as uniquely consistent partial orderings. We match these representations
through an optimal sequence comparison metric based on the Damerau-Levenshtein
distance - enabling robust association with quadratic complexity (in contrast
to hitherto employed joint matching formulations which are NP-complete). The
approach is able to perform under wide baselines, heavy rotations, partial
overlaps, significant occlusions and sensor noise.
  The technique does not require any priors -- motion or otherwise, and does
not make restrictive assumptions on scene structure and sensor movement. It
does not require appearance -- is hence more widely applicable than appearance
reliant methods, and invulnerable to related ambiguities such as textureless or
aliased content. We present promising qualitative and quantitative results
under diverse settings, along with comparatives with popular approaches based
on range as well as RGB-D data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4101</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4101</id><created>2014-11-14</created><authors><author><keyname>Mohan</keyname><forenames>Rahul</forenames></author></authors><title>Deep Deconvolutional Networks for Scene Parsing</title><categories>stat.ML cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Scene parsing is an important and challenging prob- lem in computer vision.
It requires labeling each pixel in an image with the category it belongs to.
Tradition- ally, it has been approached with hand-engineered features from
color information in images. Recently convolutional neural networks (CNNs),
which automatically learn hierar- chies of features, have achieved record
performance on the task. These approaches typically include a post-processing
technique, such as superpixels, to produce the final label- ing. In this paper,
we propose a novel network architecture that combines deep deconvolutional
neural networks with CNNs. Our experiments show that deconvolutional neu- ral
networks are capable of learning higher order image structure beyond edge
primitives in comparison to CNNs. The new network architecture is employed for
multi-patch training, introduced as part of this work. Multi-patch train- ing
makes it possible to effectively learn spatial priors from scenes. The proposed
approach yields state-of-the-art per- formance on four scene parsing datasets,
namely Stanford Background, SIFT Flow, CamVid, and KITTI. In addition, our
system has the added advantage of having a training system that can be
completely automated end-to-end with- out requiring any post-processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4102</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4102</id><created>2014-11-14</created><authors><author><keyname>Sawhney</keyname><forenames>Rahul</forenames></author><author><keyname>Christensen</keyname><forenames>Henrik I.</forenames></author><author><keyname>Bradski</keyname><forenames>Gary R.</forenames></author></authors><title>Anisotropic Agglomerative Adaptive Mean-Shift</title><categories>cs.CV cs.LG</categories><comments>British Machine Vision Conference, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mean Shift today, is widely used for mode detection and clustering. The
technique though, is challenged in practice due to assumptions of isotropicity
and homoscedasticity. We present an adaptive Mean Shift methodology that allows
for full anisotropic clustering, through unsupervised local bandwidth
selection. The bandwidth matrices evolve naturally, adapting locally through
agglomeration, and in turn guiding further agglomeration. The online
methodology is practical and effecive for low-dimensional feature spaces,
preserving better detail and clustering salience. Additionally, conventional
Mean Shift either critically depends on a per instance choice of bandwidth, or
relies on offline methods which are inflexible and/or again data instance
specific. The presented approach, due to its adaptive design, also alleviates
this issue - with a default form performing generally well. The methodology
though, allows for effective tuning of results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4105</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4105</id><created>2014-11-14</created><authors><author><keyname>Han</keyname><forenames>Shuo</forenames></author><author><keyname>Topcu</keyname><forenames>Ufuk</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author></authors><title>Differentially Private Distributed Constrained Optimization</title><categories>math.OC cs.DS cs.SY</categories><comments>Submitted to the IEEE Transactions on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many resource allocation problems can be formulated as an optimization
problem whose constraints contain sensitive information about participating
users. This paper concerns solving this kind of optimization problem in a
distributed manner while protecting the privacy of user information. Without
privacy considerations, existing distributed algorithms normally consist in a
central entity computing and broadcasting certain public coordination signals
to participating users. However, the coordination signals often depend on user
information, so that an adversary who has access to the coordination signals
can potentially decode information on individual users and put user privacy at
risk. We present a distributed optimization algorithm that preserves
differential privacy, which is a strong notion that guarantees user privacy
regardless of any auxiliary information an adversary may have. The algorithm
achieves privacy by perturbing the public signals with additive noise, whose
magnitude is determined by the sensitivity of the projection operation onto
user-specified constraints. By viewing the differentially private algorithm as
an implementation of stochastic gradient descent, we are able to derive a bound
for the suboptimality of the algorithm. We illustrate the implementation of our
algorithm via a case study of electric vehicle charging. Specifically, we
derive the sensitivity and present numerical simulations for the algorithm.
Through numerical simulations, we are able to investigate various aspects of
the algorithm when being used in practice, including the choice of step size,
number of iterations, and the trade-off between privacy level and
suboptimality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4108</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4108</id><created>2014-11-14</created><authors><author><keyname>Zhang</keyname><forenames>Li</forenames></author><author><keyname>Liu</keyname><forenames>Xuejun</forenames></author><author><keyname>Chen</keyname><forenames>Songcan</forenames></author></authors><title>Detecting Differential Expression from RNA-seq Data with Expression
  Measurement Uncertainty</title><categories>q-bio.GN cs.CE q-bio.QM</categories><comments>20 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-throughput RNA sequencing (RNA-seq) has emerged as a revolutionary and
powerful technology for expression profiling. Most proposed methods for
detecting differentially expressed (DE) genes from RNA-seq are based on
statistics that compare normalized read counts between conditions. However,
there are few methods considering the expression measurement uncertainty into
DE detection. Moreover, most methods are only capable of detecting DE genes,
and few methods are available for detecting DE isoforms. In this paper, a
Bayesian framework (BDSeq) is proposed to detect DE genes and isoforms with
consideration of expression measurement uncertainty. This expression
measurement uncertainty provides useful information which can help to improve
the performance of DE detection. Three real RAN-seq data sets are used to
evaluate the performance of BDSeq and results show that the inclusion of
expression measurement uncertainty improves accuracy in detection of DE genes
and isoforms. Finally, we develop a GamSeq-BDSeq RNA-seq analysis pipeline to
facilitate users, which is freely available at the website
http://parnec.nuaa.edu.cn/liux/GSBD/GamSeq-BDSeq.html.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4109</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4109</id><created>2014-11-14</created><authors><author><keyname>Hofford</keyname><forenames>Glenn R.</forenames></author></authors><title>Resolution of Difficult Pronouns Using the ROSS Method</title><categories>cs.CL cs.AI</categories><comments>106 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new natural language understanding method for disambiguation of difficult
pronouns is described. Difficult pronouns are those pronouns for which a level
of world or domain knowledge is needed in order to perform anaphoral or other
types of resolution. Resolution of difficult pronouns may in some cases require
a prior step involving the application of inference to a situation that is
represented by the natural language text. A general method is described: it
performs entity resolution and pronoun resolution. An extension to the general
pronoun resolution method performs inference as an embedded commonsense
reasoning method. The general method and the embedded method utilize features
of the ROSS representational scheme; in particular the methods use ROSS
ontology classes and the ROSS situation model. The overall method is a working
solution that solves the following Winograd schemas: a) trophy and suitcase, b)
person lifts person, c) person pays detective, and d) councilmen and
demonstrators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4110</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4110</id><created>2014-11-14</created><updated>2014-12-17</updated><authors><author><keyname>Qiang</keyname><forenames>Dong</forenames></author><author><keyname>Xi-Jin</keyname><forenames>Zhang</forenames></author><author><keyname>Ning</keyname><forenames>Zhao</forenames></author></authors><title>Dynamic aerodynamic-structural coupling numerical simulation on the
  flexible wing of a cicada based on ansys</title><categories>cs.CE</categories><comments>9 pages, 6 figures, International Journal of Recent advances in
  Mechanical Engineering (IJMECH) Vol.3, No.4, November 2014</comments><msc-class>74F10</msc-class><acm-class>J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most biological flyers undergo orderly deformation in flight, and the
deformations of wings lead to complex fluid-structure interactions. In this
paper, an aerodynamic-structural coupling method of flapping wing is developed
based on ANSYS to simulate the flapping of flexible wing. Fluent module and
Transient Structural module are connected through the System Coupling module to
make a two-way fluid-structure Coupling computational framework. Comparing with
the rigid wing of a cicada, the coupling results of the flexible wing shows
that the flexible deformation can increase the aerodynamic performances of
flapping flight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4114</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4114</id><created>2014-11-15</created><authors><author><keyname>Won</keyname><forenames>Ha Jong</forenames><affiliation>College of Computer Science, Kim Il Sung University</affiliation></author><author><keyname>Chol</keyname><forenames>Li Gwang</forenames><affiliation>College of Computer Science, Kim Il Sung University</affiliation></author><author><keyname>Chol</keyname><forenames>Kim Hyok</forenames><affiliation>College of Computer Science, Kim Il Sung University</affiliation></author><author><keyname>Song</keyname><forenames>Li Kum</forenames><affiliation>College of Computer Science, Kim Il Sung University</affiliation></author></authors><title>Definition of Visual Speech Element and Research on a Method of
  Extracting Feature Vector for Korean Lip-Reading</title><categories>cs.CL cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In this paper, we defined the viseme (visual speech element) and described
about the method of extracting visual feature vector. We defined the 10 visemes
based on vowel by analyzing of Korean utterance and proposed the method of
extracting the 20-dimensional visual feature vector, combination of static
features and dynamic features. Lastly, we took an experiment in recognizing
words based on 3-viseme HMM and evaluated the efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4116</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4116</id><created>2014-11-15</created><authors><author><keyname>Cheng</keyname><forenames>Jianpeng</forenames></author><author><keyname>Kartsaklis</keyname><forenames>Dimitri</forenames></author><author><keyname>Grefenstette</keyname><forenames>Edward</forenames></author></authors><title>Investigating the Role of Prior Disambiguation in Deep-learning
  Compositional Models of Meaning</title><categories>cs.CL cs.LG cs.NE</categories><comments>NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to explore the effect of prior disambiguation on neural
network- based compositional models, with the hope that better semantic
representations for text compounds can be produced. We disambiguate the input
word vectors before they are fed into a compositional deep net. A series of
evaluations shows the positive effect of prior disambiguation for such deep
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4139</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4139</id><created>2014-11-15</created><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Duan</keyname><forenames>Lingjie</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Cost-Aware Green Cellular Networks with Energy and Communication
  Cooperation</title><categories>cs.IT math.IT</categories><comments>Submitted for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy cost of cellular networks is ever-increasing to match the surge of
wireless data traffic, and the saving of this cost is important to reduce the
operational expenditure (OPEX) of wireless operators in future. The recent
advancements of renewable energy integration and two-way energy flow in smart
grid provide potential new solutions to save the cost. However, they also
impose challenges, especially on how to use the stochastically and spatially
distributed renewable energy harvested at cellular base stations (BSs) to
reliably supply time- and space-varying wireless traffic over cellular
networks. To overcome these challenges, in this article we present three
approaches, namely, {\emph{energy cooperation, communication cooperation, and
joint energy and communication cooperation}}, in which different BSs
bidirectionally trade or share energy via the aggregator in smart grid, and/or
share wireless resources and shift loads with each other to reduce the total
energy cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4141</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4141</id><created>2014-11-15</created><authors><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Gao</keyname><forenames>Xinyu</forenames></author><author><keyname>Han</keyname><forenames>Shuangfeng</forenames></author><author><keyname>I</keyname><forenames>Chih-Lin</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>Near-Optimal Linear Precoding with Low Complexity for Massive MIMO</title><categories>cs.IT math.IT</categories><comments>This paper has been submitted for publication. The MATLAB code can be
  provided via request to reduplicate the results in this paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear precoding techniques can achieve near- optimal capacity due to the
special channel property in down- link massive MIMO systems, but involve high
complexity since complicated matrix inversion of large size is required. In
this paper, we propose a low-complexity linear precoding scheme based on the
Gauss-Seidel (GS) method. The proposed scheme can achieve the
capacity-approaching performance of the classical linear precoding schemes in
an iterative way without complicated matrix inversion, which can reduce the
overall complexity by one order of magnitude. The performance guarantee of the
proposed GS-based precoding is analyzed from the following three aspects. At
first, we prove that GS-based precoding satisfies the transmit power
constraint. Then, we prove that GS-based precoding enjoys a faster convergence
rate than the recently proposed Neumann-based precoding. At last, the
convergence rate achieved by GS-based precoding is quantified, which reveals
that GS-based precoding converges faster with the increasing number of BS
antennas. To further accelerate the convergence rate and reduce the complexity,
we propose a zone-based initial solution to GS-based precoding, which is much
closer to the final solution than the traditional initial solution. Simulation
results demonstrate that the proposed scheme outperforms Neumann- based
precoding, and achieves the exact capacity-approaching performance of the
classical linear precoding schemes with only a small number of iterations both
in Rayleigh fading channels and spatially correlated channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4144</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4144</id><created>2014-11-15</created><authors><author><keyname>Douik</keyname><forenames>Ahmed</forenames></author><author><keyname>Dahrouj</keyname><forenames>Hayssam</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Coordinated Scheduling for the Downlink of Cloud Radio-Access Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the coordinated scheduling problem in cloud-enabled
networks. Consider the downlink of a cloud-radio access network (C-RAN), where
the cloud is only responsible for the scheduling policy and the synchronization
of the transmit frames across the connected base-stations (BS). The transmitted
frame of every BS consists of several time/frequency blocks, called power-zones
(PZ), maintained at fixed transmit power. The paper considers the problem of
scheduling users to PZs and BSs in a coordinated fashion across the network, by
maximizing a network-wide utility under the practical constraint that each user
cannot be served by more than one base-station, but can be served by one or
more power-zone within each base-station frame. The paper solves the problem
using a graph theoretical approach by introducing the scheduling graph in which
each vertex represents an association of users, PZs and BSs. The problem is
formulated as a maximum weight clique, in which the weight of each vertex is
the benefit of the association represented by that vertex. The paper further
presents heuristic algorithms with low computational complexity. Simulation
results show the performance of the proposed algorithms and suggest that the
heuristics perform near optimal in low shadowing environments
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4148</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4148</id><created>2014-11-15</created><authors><author><keyname>Bhattacharya</keyname><forenames>Maumita</forenames></author></authors><title>Diversity Handling In Evolutionary Landscape</title><categories>cs.NE</categories><comments>In the &quot;Proceedings of the International Workshop on Combinations of
  Intelligent Methods and Applications (CIMA 2014)&quot;, pp. 1-8, November'2014</comments><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The search ability of an Evolutionary Algorithm (EA) depends on the variation
among the individuals in the population. Maintaining an optimal level of
diversity in the EA population is imperative to ensure that progress of the EA
search is unhindered by premature convergence to suboptimal solutions. Clearer
understanding of the concept of population diversity, in the context of
evolutionary search and premature convergence in particular, is the key to
designing efficient EAs. To this end, this paper first presents a comprehensive
analysis of the EA population diversity issues. Next we present an
investigation on a counter-niching EA technique that introduces and maintains
constructive diversity in the population. The proposed approach uses informed
genetic operations to reach promising, but un-explored or under-explored areas
of the search space, while discouraging premature local convergence. Simulation
runs on a number of standard benchmark test functions with Genetic Algorithm
(GA) implementation shows promising results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4156</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4156</id><created>2014-11-15</created><updated>2015-01-21</updated><authors><author><keyname>Patel-Schneider</keyname><forenames>Peter F.</forenames></author></authors><title>Using Description Logics for RDF Constraint Checking and Closed-World
  Recognition</title><categories>cs.AI</categories><comments>Extended version of a paper of the same name that will appear in
  AAAI-2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RDF and Description Logics work in an open-world setting where absence of
information is not information about absence. Nevertheless, Description Logic
axioms can be interpreted in a closed-world setting and in this setting they
can be used for both constraint checking and closed-world recognition against
information sources. When the information sources are expressed in well-behaved
RDF or RDFS (i.e., RDF graphs interpreted in the RDF or RDFS semantics) this
constraint checking and closed-world recognition is simple to describe. Further
this constraint checking can be implemented as SPARQL querying and thus
effectively performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4157</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4157</id><created>2014-11-15</created><authors><author><keyname>He</keyname><forenames>Chaodong</forenames></author></authors><title>A Polynomial Time Algorithm for Deciding Branching Bisimilarity on
  Totally Normed BPA</title><categories>cs.LO</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Strong bisimilarity on normed BPA is polynomial-time decidable, while weak
bisimilarity on totally normed BPA is NP-hard. It is natural to ask where the
computational complexity of branching bisimilarity on totally normed BPA lies.
This paper confirms that this problem is polynomial-time decidable. To our
knowledge, in the presence of silent transitions, this is the first
bisimilarity checking algorithm on infinite state systems which runs in
polynomial time. This result spots an instance in which branching bisimilarity
and weak bisimilarity are both decidable but lie in different complexity
classes (unless NP=P), which is not known before.
  The algorithm takes the partition refinement approach and the final
implementation can be thought of as a generalization of the previous algorithm
of Czerwi\'{n}ski and Lasota. However, unexpectedly, the correctness of the
algorithm cannot be directly generalized from previous works, and the
correctness proof turns out to be subtle. The proof depends on the existence of
a carefully defined refinement operation fitted for our algorithm and the
proposal of elaborately developed techniques, which are quite different from
previous works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4166</identifier>
 <datestamp>2015-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4166</id><created>2014-11-15</created><updated>2015-03-22</updated><authors><author><keyname>Faruqui</keyname><forenames>Manaal</forenames></author><author><keyname>Dodge</keyname><forenames>Jesse</forenames></author><author><keyname>Jauhar</keyname><forenames>Sujay K.</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author><author><keyname>Hovy</keyname><forenames>Eduard</forenames></author><author><keyname>Smith</keyname><forenames>Noah A.</forenames></author></authors><title>Retrofitting Word Vectors to Semantic Lexicons</title><categories>cs.CL</categories><comments>Proceedings of NAACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vector space word representations are learned from distributional information
of words in large corpora. Although such statistics are semantically
informative, they disregard the valuable information that is contained in
semantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. This
paper proposes a method for refining vector space representations using
relational information from semantic lexicons by encouraging linked words to
have similar vector representations, and it makes no assumptions about how the
input vectors were constructed. Evaluated on a battery of standard lexical
semantic evaluation tasks in several languages, we obtain substantial
improvements starting with a variety of word vector models. Our refinement
method outperforms prior techniques for incorporating semantic lexicons into
the word vector training algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4179</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4179</id><created>2014-11-15</created><authors><author><keyname>Ha</keyname><forenames>Sieu D.</forenames></author><author><keyname>Shi</keyname><forenames>Jian</forenames></author><author><keyname>Meroz</keyname><forenames>Yasmine</forenames></author><author><keyname>Mahadevan</keyname><forenames>L.</forenames></author><author><keyname>Ramanathan</keyname><forenames>Shriram</forenames></author></authors><title>Neuromimetic Circuits with Synaptic Devices based on Strongly Correlated
  Electron Systems</title><categories>cond-mat.str-el cs.ET q-bio.NC</categories><journal-ref>Phys. Rev. Applied 2, 064003 (2014)</journal-ref><doi>10.1103/PhysRevApplied.2.064003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Strongly correlated electron systems such as the rare-earth nickelates
(RNiO3, R = rare-earth element) can exhibit synapse-like continuous long term
potentiation and depression when gated with ionic liquids; exploiting the
extreme sensitivity of coupled charge, spin, orbital, and lattice degrees of
freedom to stoichiometry. We present experimental real-time, device-level
classical conditioning and unlearning using nickelate-based synaptic devices in
an electronic circuit compatible with both excitatory and inhibitory neurons.
We establish a physical model for the device behavior based on electric-field
driven coupled ionic-electronic diffusion that can be utilized for design of
more complex systems. We use the model to simulate a variety of associate and
non-associative learning mechanisms, as well as a feedforward recurrent network
for storing memory. Our circuit intuitively parallels biological neural
architectures, and it can be readily generalized to other forms of cellular
learning and extinction. The simulation of neural function with electronic
device analogues may provide insight into biological processes such as decision
making, learning and adaptation, while facilitating advanced parallel
information processing in hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4182</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4182</id><created>2014-11-15</created><authors><author><keyname>Ashikhmin</keyname><forenames>Alexei</forenames></author><author><keyname>Marzetta</keyname><forenames>Thomas L.</forenames></author><author><keyname>Li</keyname><forenames>Liangbin</forenames></author></authors><title>Interference Reduction in Multi-Cell Massive MIMO Systems I: Large-Scale
  Fading Precoding and Decoding</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A wireless massive MIMO system entails a large number (tens or hundreds) of
base station antennas serving a much smaller number of users, with large gains
in spectral-efficiency and energy-efficiency compared with conventional MIMO
technology. Until recently it was believed that in multi-cellular massive MIMO
system, even in the asymptotic regime, as the number of service antennas tends
to infinity, the performance is limited by directed inter-cellular
interference. This interference results from unavoidable re-use of reverse-link
training sequences (pilot contamination) by users in different cells.
  We devise a new concept that leads to the effective elimination of inter-cell
interference in massive MIMO systems. This is achieved by outer multi-cellular
precoding, which we call Large-Scale Fading Precoding (LSFP). The main idea of
LSFP is that each base station linearly combines messages aimed to users from
different cells that re-use the same training sequence. Crucially, the
combining coefficients depend only on the slow-fading coefficients between the
users and the base stations. Each base station independently transmits its
LSFP-combined symbols using conventional linear precoding that is based on
estimated fast-fading coefficients. Further, we derive estimates for downlink
and uplink SINRs and capacity lower bounds for the case of massive MIMO systems
with LSFP and a finite number of base station antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4183</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4183</id><created>2014-11-15</created><authors><author><keyname>Li</keyname><forenames>Liangbin</forenames></author><author><keyname>Ashikhmin</keyname><forenames>Alexei</forenames></author><author><keyname>Marzetta</keyname><forenames>Thomas</forenames></author></authors><title>Interference Reduction in Multi-Cell Massive MIMO Systems II: Downlink
  Analysis for a Finite Number of Antennas</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Sharing global channel information at base stations (BSs) is commonly assumed
for downlink multi-cell precoding. In the context of massive multi-input
multi-output (MIMO) systems where each BS is equipped with a large number of
antennas, sharing instant fading channel coefficients consumes a large amount
of resource. To consider practically implementable methods, we study in this
paper interference reduction based on precoding using the large-scale fading
coefficients that depend on the path-loss model and are independent of a
specific antenna. We focus on the downlink multi-cell precoding designs when
each BS is equipped with a practically finite number of antennas. In this
operation regime, pilot contamination is not the dominant source of
interference, and mitigation of all types of interference is required. This
paper uses an optimization approach to design precoding methods for equal
qualities of service (QoS) to all users in the network, i.e.,maximizing the
minimum signal-to-interference-plus-noise ratios (SINRs) among all users. The
formulated optimization is proved to be quasi-convex, and can be solved
optimally. We also propose low-complexity suboptimal algorithms through uplink
and downlink duality. Simulation results show that the proposed precoding
methods improve 5% outage rate for more than 1000 times, compared to other
known interference mitigation techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4184</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4184</id><created>2014-11-15</created><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Marx</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author></authors><title>Hitting forbidden subgraphs in graphs of bounded treewidth</title><categories>cs.DS cs.CC</categories><comments>A full version of a paper presented at MFCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of a generic hitting problem H-Subgraph Hitting,
where given a fixed pattern graph $H$ and an input graph $G$, the task is to
find a set $X \subseteq V(G)$ of minimum size that hits all subgraphs of $G$
isomorphic to $H$. In the colorful variant of the problem, each vertex of $G$
is precolored with some color from $V(H)$ and we require to hit only
$H$-subgraphs with matching colors. Standard techniques shows that for every
fixed $H$, the problem is fixed-parameter tractable parameterized by the
treewidth of $G$; however, it is not clear how exactly the running time should
depend on treewidth. For the colorful variant, we demonstrate matching upper
and lower bounds showing that the dependence of the running time on treewidth
of $G$ is tightly governed by $\mu(H)$, the maximum size of a minimal vertex
separator in $H$. That is, we show for every fixed $H$ that, on a graph of
treewidth $t$, the colorful problem can be solved in time
$2^{\mathcal{O}(t^{\mu(H)})}\cdot|V(G)|$, but cannot be solved in time
$2^{o(t^{\mu(H)})}\cdot |V(G)|^{O(1)}$, assuming the Exponential Time
Hypothesis (ETH). Furthermore, we give some preliminary results showing that,
in the absence of colors, the parameterized complexity landscape of H-Subgraph
Hitting is much richer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4186</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4186</id><created>2014-11-15</created><updated>2015-01-03</updated><authors><author><keyname>Olshevsky</keyname><forenames>Alex</forenames></author></authors><title>Linear Time Average Consensus on Fixed Graphs and Implications for
  Decentralized Optimization and Multi-Agent Control</title><categories>math.OC cs.DC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a protocol for the average consensus problem on any fixed
undirected graph whose convergence time scales linearly in the total number
nodes $n$. More precisely, we provide a protocol which results in each node
having a value within an $\epsilon$ of the initial average after $O \left( n
\ln \frac{||{\bf x}(1) - \overline{x} {\bf 1}||_2}{\epsilon} \right)$
iterations. The protocol is completely distributed, with the exception of
requiring each node to know an upper bound $U$ on the total number of nodes
which is correct within a constant multiplicative factor.
  We next discuss applications of this protocol to problems in multi-agent
control connected to the consensus problem. In particular, we describe
protocols for formation maintenance and leader-following with convergence times
which also scale linearly with the number of nodes.
  Most importantly, we develop a distributed protocol for minimizing an average
of (possibly nondifferentiable) convex functions $ (1/n) \sum_{i=1}^n
f_i(\theta)$, in the setting where only node $i$ in an undirected, connected
graph knows the function $f_i(\theta)$. Under the same assumption about all
nodes knowing $U$, and additionally assuming that the subgradients of each
$f_i(\theta)$ have norms upper bounded by some constant $L$ known to the nodes,
we show that after $T$ iterations our protocol has error which is $O(L
\sqrt{n/T})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4192</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4192</id><created>2014-11-15</created><authors><author><keyname>Hofford</keyname><forenames>Glenn R.</forenames></author></authors><title>Introduction to ROSS: A New Representational Scheme</title><categories>cs.AI</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ROSS (&quot;Representation, Ontology, Structure, Star&quot;) is introduced as a new
method for knowledge representation that emphasizes representational constructs
for physical structure. The ROSS representational scheme includes a language
called &quot;Star&quot; for the specification of ontology classes. The ROSS method also
includes a formal scheme called the &quot;instance model&quot;. Instance models are used
in the area of natural language meaning representation to represent situations.
This paper provides both the rationale and the philosophical background for the
ROSS method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4194</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4194</id><created>2014-11-15</created><authors><author><keyname>Hofford</keyname><forenames>Glenn R.</forenames></author></authors><title>ROSS User's Guide and Reference Manual (Version 1.0)</title><categories>cs.AI cs.CL</categories><comments>128 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ROSS method is a new approach in the area of knowledge representation
that is useful for many artificial intelligence and natural language
understanding representation and reasoning tasks. (ROSS stands for
&quot;Representation&quot;, &quot;Ontology&quot;, &quot;Structure&quot;, &quot;Star&quot; language). ROSS is a physical
symbol-based representational scheme. ROSS provides a complex model for the
declarative representation of physical structure and for the representation of
processes and causality. From the metaphysical perspective, the ROSS view of
external reality involves a 4D model, wherein discrete single-time-point
unit-sized locations with states are the basis for all objects, processes and
aspects that can be modeled. ROSS includes a language called &quot;Star&quot; for the
specification of ontology classes. The ROSS method also includes a formal
scheme called the &quot;instance model&quot;. Instance models are used in the area of
natural language meaning representation to represent situations. This document
is an in-depth specification of the ROSS method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4199</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4199</id><created>2014-11-15</created><authors><author><keyname>Jiang</keyname><forenames>Ke</forenames></author><author><keyname>Que</keyname><forenames>Qichao</forenames></author><author><keyname>Kulis</keyname><forenames>Brian</forenames></author></authors><title>Revisiting Kernelized Locality-Sensitive Hashing for Improved
  Large-Scale Image Retrieval</title><categories>cs.CV cs.LG stat.ML</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple but powerful reinterpretation of kernelized
locality-sensitive hashing (KLSH), a general and popular method developed in
the vision community for performing approximate nearest-neighbor searches in an
arbitrary reproducing kernel Hilbert space (RKHS). Our new perspective is based
on viewing the steps of the KLSH algorithm in an appropriately projected space,
and has several key theoretical and practical benefits. First, it eliminates
the problematic conceptual difficulties that are present in the existing
motivation of KLSH. Second, it yields the first formal retrieval performance
bounds for KLSH. Third, our analysis reveals two techniques for boosting the
empirical performance of KLSH. We evaluate these extensions on several
large-scale benchmark image retrieval data sets, and show that our analysis
leads to improved recall performance of at least 12%, and sometimes much
higher, over the standard KLSH method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4214</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4214</id><created>2014-11-15</created><authors><author><keyname>Hasan</keyname><forenames>Monowar</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author><author><keyname>Balasubramaniam</keyname><forenames>Sasitharan</forenames></author><author><keyname>Koucheryavy</keyname><forenames>Yevgeni</forenames></author></authors><title>Social Behavior in Bacterial Nanonetworks: Challenges and Opportunities</title><categories>cs.ET</categories><comments>Accepted for publication in IEEE Network Magazine as an open call
  article</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Molecular communication promises to enable communication between nanomachines
with a view to increasing their functionalities and open up new possible
applications. Due to some of the biological properties, bacteria have been
proposed as a possible information carrier for molecular communication, and the
corresponding communication networks are known as \textit{bacterial
nanonetworks}. The biological properties include the ability for bacteria to
mobilize between locations and carry the information encoded in
Deoxyribonucleic Acid (DNA) molecules. However, similar to most organisms,
bacteria have complex social properties that govern their colony. These social
characteristics enable the bacteria to evolve through various fluctuating
environmental conditions by utilizing cooperative and non-cooperative
behaviors. This article provides an overview of the different types of
cooperative and non-cooperative social behavior of bacteria. The challenges
(due to non-cooperation) and the opportunities (due to cooperation) these
behaviors can bring to the reliability of communication in bacterial
nanonetworks are also discussed. Finally, simulation results on the impact of
bacterial cooperative social behavior on the end-to-end reliability of a
single-link bacterial nanonetwork are presented. The article concludes with
highlighting the potential future research opportunities in this emerging
field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4221</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4221</id><created>2014-11-16</created><authors><author><keyname>Cheng</keyname><forenames>Zhi</forenames></author></authors><title>A dynamic mechanism of Alzheimer based on artificial neural network</title><categories>cs.OH</categories><comments>9 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide another angle to analyze the reasons why Alzheimer
Disease exists. We analyze the dynamic mechanism of Alzheimer Disease based on
the cognitive model that established from artificial neural network. We can
provide some theoretic explanations to Alzheimer Disease through the analyzing
of this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4226</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4226</id><created>2014-11-16</created><authors><author><keyname>Dharmawansa</keyname><forenames>Prathapasinghe</forenames></author><author><keyname>Nadler</keyname><forenames>Boaz</forenames></author><author><keyname>Shwartz</keyname><forenames>Ofer</forenames></author></authors><title>Roy's largest root under rank-one alternatives:The complex valued case
  and applications</title><categories>math.ST cs.IT math.IT stat.TH</categories><msc-class>15B52, 62H10, 62H15, 94A13, 94A14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The largest eigenvalue of a Wishart matrix, known as Roy's largest root
(RLR), plays an important role in a variety of applications. Most works to date
derived approximations to its distribution under various asymptotic regimes,
such as degrees of freedom, dimension, or both tending to infinity. However,
several applications involve finite and relative small parameters, for which
the above approximations may be inaccurate. Recently, via a small noise
perturbation approach with fixed dimension and degrees of freedom, Johnstone
and Nadler derived simple yet accurate stochastic approximations to the
distribution of Roy's largest root in the real valued case, under a rank-one
alternative. In this paper, we extend their results to the complex valued case.
Furthermore, we analyze the behavior of the leading eigenvector by developing
new stochastic approximations. Specifically, we derive simple stochastic
approximations to the distribution of the largest eigenvalue under five common
complex single-matrix and double-matrix scenarios. We then apply these results
to investigate several problems in signal detection and communications. In
particular, we analyze the performance of RLR detector in cognitive radio
spectrum sensing and constant modulus signal detection in the high
signal-to-noise ratio (SNR) regime. Moreover, we address the problem of
determining the optimal transmit-receive antenna configuration (here optimality
is in the sense of outage minimization) for rank-one multiple-input
multiple-output Rician Fading channels at high SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4228</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4228</id><created>2014-11-16</created><authors><author><keyname>He</keyname><forenames>Peng</forenames></author><author><keyname>Li</keyname><forenames>Bing</forenames></author><author><keyname>Ma</keyname><forenames>Yutao</forenames></author></authors><title>Towards Cross-Project Defect Prediction with Imbalanced Feature Sets</title><categories>cs.SE</categories><comments>10 pages, 8 figures, 7 tables</comments><msc-class>68N30</msc-class><acm-class>D.2.8; D.2.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cross-project defect prediction (CPDP) has been deemed as an emerging
technology of software quality assurance, especially in new or inactive
projects, and a few improved methods have been proposed to support better
defect prediction. However, the regular CPDP always assumes that the features
of training and test data are all identical. Hence, very little is known about
whether the method for CPDP with imbalanced feature sets (CPDP-IFS) works well.
Considering the diversity of defect data sets available on the Internet as well
as the high cost of labeling data, to address the issue, in this paper we
proposed a simple approach according to a distribution characteristic-based
instance (object class) mapping, and demonstrated the validity of our method
based on three public defect data sets (i.e., PROMISE, ReLink and AEEEM).
Besides, the empirical results indicate that the hybrid model composed of CPDP
and CPDP-IFS does improve the prediction performance of the regular CPDP to
some extent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4229</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4229</id><created>2014-11-16</created><authors><author><keyname>Zhang</keyname><forenames>Xiangyu</forenames></author><author><keyname>Zou</keyname><forenames>Jianhua</forenames></author><author><keyname>Ming</keyname><forenames>Xiang</forenames></author><author><keyname>He</keyname><forenames>Kaiming</forenames></author><author><keyname>Sun</keyname><forenames>Jian</forenames></author></authors><title>Efficient and Accurate Approximations of Nonlinear Convolutional
  Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to accelerate the test-time computation of deep convolutional
neural networks (CNNs). Unlike existing methods that are designed for
approximating linear filters or linear responses, our method takes the
nonlinear units into account. We minimize the reconstruction error of the
nonlinear responses, subject to a low-rank constraint which helps to reduce the
complexity of filters. We develop an effective solution to this constrained
nonlinear optimization problem. An algorithm is also presented for reducing the
accumulated error when multiple layers are approximated. A whole-model speedup
ratio of 4x is demonstrated on a large network trained for ImageNet, while the
top-5 error rate is only increased by 0.9%. Our accelerated model has a
comparably fast speed as the &quot;AlexNet&quot;, but is 4.7% more accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4246</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4246</id><created>2014-11-16</created><authors><author><keyname>Islam</keyname><forenames>Md. Lisul</forenames></author><author><keyname>Shatabda</keyname><forenames>Swakkhar</forenames></author><author><keyname>Rahman</keyname><forenames>M. Sohel</forenames></author></authors><title>GreMuTRRR: A Novel Genetic Algorithm to Solve Distance Geometry Problem
  for Protein Structures</title><categories>cs.NE cs.CE</categories><comments>Accepted for publication in the 8th International Conference on
  Electrical and Computer Engineering (ICECE 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nuclear Magnetic Resonance (NMR) Spectroscopy is a widely used technique to
predict the native structure of proteins. However, NMR machines are only able
to report approximate and partial distances between pair of atoms. To build the
protein structure one has to solve the Euclidean distance geometry problem
given the incomplete interval distance data produced by NMR machines. In this
paper, we propose a new genetic algorithm for solving the Euclidean distance
geometry problem for protein structure prediction given sparse NMR data. Our
genetic algorithm uses a greedy mutation operator to intensify the search, a
twin removal technique for diversification in the population and a random
restart method to recover stagnation. On a standard set of benchmark dataset,
our algorithm significantly outperforms standard genetic algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4249</identifier>
 <datestamp>2014-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4249</id><created>2014-11-16</created><updated>2014-12-11</updated><authors><author><keyname>Xing</keyname><forenames>Chengwen</forenames></author><author><keyname>Gao</keyname><forenames>Feifei</forenames></author><author><keyname>Zhou</keyname><forenames>Yiqing</forenames></author></authors><title>A Framework for Transceiver Designs for Multi-Hop Communications with
  Covariance Shaping Constraints</title><categories>cs.IT math.IT</categories><comments>31 Pages, 9 Figures, Submitted to Signal Processing IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For multiple-input multiple-output (MIMO) transceiver designs, sum power
constraint is an elegant and ideal model. When various practical limitations
are taken into account e.g., peak power constraints, per-antenna power
constraints, etc., covariance shaping constraints will act as an effective and
reasonable model. In this paper, we develop a framework for transceiver designs
for multi-hop communications under covariance shaping constraints.
Particularly, we focus on multi-hop amplify-and-forward (AF) MIMO relaying
communications which are recognized as a key enabling technology for
device-to-device (D2D) communications for next generation wireless systems such
as 5G. The proposed framework includes a broad range of various linear and
nonlinear transceiver designs as its special cases. It reveals an interesting
fact that the relaying operation in each hop can be understood as a matrix
version weighting operation. Furthermore, the nonlinear operations of
Tomolision-Harashima Precoding (THP) and Decision Feedback Equalizer (DFE) also
belong to the category of this kind of matrix version weighting operation.
Furthermore, for both the cases with only pure shaping constraints or joint
power constraints, the closed-form optimal solutions have been derived. At the
end of this paper, the performance of the various designs is assessed by
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4253</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4253</id><created>2014-11-16</created><updated>2015-02-16</updated><authors><author><keyname>Li</keyname><forenames>Tongxin</forenames></author><author><keyname>Bakshi</keyname><forenames>Mayank</forenames></author><author><keyname>Grover</keyname><forenames>Pulkit</forenames></author></authors><title>Energy-efficient Decoders for Compressive Sensing: Fundamental Limits
  and Implementations</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT2015; 23 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fundamental problem considered in this paper is &quot;What is the
\textit{energy} consumed for the implementation of a \emph{compressive sensing}
decoding algorithm on a circuit?&quot;. Using the &quot;information-friction&quot; framework,
we examine the smallest amount of \textit{bit-meters} as a measure for the
energy consumed by a circuit. We derive a fundamental lower bound for the
implementation of compressive sensing decoding algorithms on a circuit. In the
setting where the number of measurements scales linearly with the sparsity and
the sparsity is sub-linear with the length of the signal, we show that the
\textit{bit-meters} consumption for these algorithms is order-tight, i.e., it
matches the lower bound asymptotically up to a constant factor. Our
implementations yield interesting insights into design of energy-efficient
circuits that are not captured by the notion of computational efficiency alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4256</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4256</id><created>2014-11-16</created><updated>2015-03-05</updated><authors><author><keyname>Berger</keyname><forenames>Martin</forenames><affiliation>Department of Informatics, University of Sussex</affiliation></author><author><keyname>Tratt</keyname><forenames>Laurence</forenames><affiliation>Department of Informatics, King's College London</affiliation></author></authors><title>Program Logics for Homogeneous Generative Run-Time Meta-Programming</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 11, Issue 1 (March 6,
  2015) lmcs:929</journal-ref><doi>10.2168/LMCS-11(1:5)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides the first program logic for homogeneous generative
run-time meta-programming---using a variant of MiniML by Davies and Pfenning as
its underlying meta-programming language. We show the applicability of our
approach by reasoning about example meta-programs from the literature. We also
demonstrate that our logics are relatively complete in the sense of Cook,
enable the inductive derivation of characteristic formulae, and exactly capture
the observational properties induced by the operational semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4261</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4261</id><created>2014-11-16</created><authors><author><keyname>Smirnova</keyname><forenames>Vera</forenames></author><author><keyname>Proskurnikov</keyname><forenames>Anton</forenames></author><author><keyname>Utina</keyname><forenames>Natalia V.</forenames></author></authors><title>Cycle slipping in nonlinear circuits under periodic nonlinearities and
  time delays</title><categories>cs.SY</categories><comments>The preprint it submitted to ISCAS 2015</comments><doi>10.1109/ISCAS.2015.7169339</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phase-locked loops (PLL), Costas loops and other synchronizing circuits are
featured by the presence of a nonlinear phase detector, described by a periodic
nonlinearity. In general, nonlinearities can cause complex behavior of the
system such multi-stability and chaos. However, even phase locking may be
guaranteed under any initial conditions, the transient behavior of the circuit
can be unsatisfactory due to the cycle slipping. Growth of the phase error
caused by cycle slipping is undesirable, leading e.g. to demodulation and
decoding errors. This makes the problem of estimating the phase error
oscillations and number of slipped cycles in nonlinear PLL-based circuits
extremely important for modern telecommunications. Most mathematical results in
this direction, available in the literature, examine the probability density of
the phase error and expected number of slipped cycles under stochastic noise in
the signal. At the same time, cycle slipping occurs also in deterministic
systems with periodic nonlinearities, depending on the initial conditions,
properties of the linear part and the periodic nonlinearity and other factors
such as delays in the loop. In the present paper we give analytic estimates for
the number of slipped cycles in PLL-based systems, governed by
integro-differential equations, allowing to capture effects of high-order
dynamics, discrete and distributed delays. We also consider the effects of
singular small parameter perturbations on the cycle slipping behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4264</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4264</id><created>2014-11-16</created><authors><author><keyname>Smirnova</keyname><forenames>Vera</forenames></author><author><keyname>Proskurnikov</keyname><forenames>Anton V.</forenames></author><author><keyname>Utina</keyname><forenames>Natalia V.</forenames></author></authors><title>Estimation of transient process for singularly perturbed synchronization
  system with distributed parameters</title><categories>cs.SY</categories><comments>This preprint is submitted to European Control Conference ECC-2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many systems, arising in electrical and electronic engineering are based on
controlled phase synchronization of several periodic processes (&quot;phase
synchronization&quot; systems, or PSS). Typically such systems are featured by the
gradient-like behavior, i.e. the system has infinite sequence of equilibria
points, and any solution converges to one of them. This property however says
nothing about the transient behavior of the system, whose important qualitative
index is the maximal phase error. The synchronous regime of gradient-like
system may be preceded by cycle slipping, i.e. the increase of the absolute
phase error. Since the cycle slipping is considered to be undesired behavior of
PSSs, it is important to find efficient estimates for the number of slipped
cycles. In the present paper, we address the problem of cycle-slipping for
phase synchronization systems described by integro-differential Volterra
equations with a small parameter at the higher derivative. New effective
estimates for a number of slipped cycles are obtained by means of Popov's
method of &quot;a priori integral indices&quot;. The estimates are uniform with respect
to the small parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4266</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4266</id><created>2014-11-16</created><authors><author><keyname>Ma</keyname><forenames>Shuai</forenames></author><author><keyname>Li</keyname><forenames>Jia</forenames></author><author><keyname>Hu</keyname><forenames>Chunming</forenames></author><author><keyname>Lin</keyname><forenames>Xuelian</forenames></author><author><keyname>Huai</keyname><forenames>Jinpeng</forenames></author></authors><title>Big Graph Search: Challenges and Techniques</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On one hand, compared with traditional relational and XML models, graphs have
more expressive power and are widely used today. On the other hand, various
applications of social computing trigger the pressing need of a new search
paradigm. In this article, we argue that big graph search is the one filling
this gap. To show this, we first introduce the application of graph search in
various scenarios. We then formalize the graph search problem, and give an
analysis of graph search from an evolutionary point of view, followed by the
evidences from both the industry and academia. After that, we analyze the
difficulties and challenges of big graph search. Finally, we present three
classes of techniques towards big graph search: query techniques, data
techniques and distributed computing techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4271</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4271</id><created>2014-11-16</created><authors><author><keyname>Qiao</keyname><forenames>Deli</forenames></author></authors><title>Achievable Rate of Two-Hop Channels under Statistical Delay Constraints</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1107.4346</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the impact of statistical delay constraints on the
achievable rate of a two-hop wireless communication link, in which the
communication between a source and a destination is accomplished via an
intermediate full-duplex relay node. It is assumed that there is no direct link
between the source and the destination, and the relay forwards the information
to the destination by employing the decode-and-forward scheme. Both the queues
at the source and relay node are subject to statistical queueing constraints
imposed on the limitations of buffer violation probability. Given statistical
delay constraints specified via maximum delay and delay violation probability,
the tradeoff between the statistical delay constraints imposed to any two
concatenated queues is identified. With this characterization, the maximum
constant arrival rates that can be supported by this two-hop link are obtained
by determining the effective capacity of such links as a function of the
statistical delay constraints and signal-to-noise ratios (SNR) at the source
and relay, and the fading distributions of the links. It is shown that imposing
unbalanced statistical delay constraints to the queues at the source and relay
can improve the achievable rate. Overall, the impact of statistical delay
constraints on the achievable throughput is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4274</identifier>
 <datestamp>2015-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4274</id><created>2014-11-16</created><updated>2015-05-19</updated><authors><author><keyname>Chrobak</keyname><forenames>Marek</forenames></author><author><keyname>Durr</keyname><forenames>Christoph</forenames></author><author><keyname>Nilsson</keyname><forenames>Bengt</forenames></author></authors><title>Competitive Strategies for Online Clique Clustering</title><categories>cs.DS</categories><comments>title changed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A clique clustering of a graph is a partitioning of its vertices into
disjoint cliques. The quality of a clique clustering is measured by the total
number of edges in its cliques. We consider the online variant of the clique
clustering problem, where the vertices of the input graph arrive one at a time.
At each step, the newly arrived vertex forms a singleton clique, and the
algorithm can merge any existing cliques in its partitioning into larger
cliques, but splitting cliques is not allowed. We give an online algorithm with
competitive ratio 15.645 and we prove a lower bound of 6 on the competitive
ratio, improving the previous respective bounds of 31 and 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4280</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4280</id><created>2014-11-16</created><updated>2015-06-09</updated><authors><author><keyname>Tompson</keyname><forenames>Jonathan</forenames></author><author><keyname>Goroshin</keyname><forenames>Ross</forenames></author><author><keyname>Jain</keyname><forenames>Arjun</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author><author><keyname>Bregler</keyname><forenames>Christopher</forenames></author></authors><title>Efficient Object Localization Using Convolutional Networks</title><categories>cs.CV</categories><comments>8 pages with 1 page of citations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent state-of-the-art performance on human-body pose estimation has been
achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet
architectures include pooling and sub-sampling layers which reduce
computational requirements, introduce invariance and prevent over-training.
These benefits of pooling come at the cost of reduced localization accuracy. We
introduce a novel architecture which includes an efficient `position
refinement' model that is trained to estimate the joint offset location within
a small region of the image. This refinement model is jointly trained in
cascade with a state-of-the-art ConvNet model to achieve improved accuracy in
human joint location estimation. We show that the variance of our detector
approaches the variance of human annotations on the FLIC dataset and
outperforms all existing approaches on the MPII-human-pose dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4281</identifier>
 <datestamp>2015-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4281</id><created>2014-11-16</created><updated>2015-04-09</updated><authors><author><keyname>Rached</keyname><forenames>Nadhir Ben</forenames></author><author><keyname>Kammoun</keyname><forenames>Abla</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author><author><keyname>Tempone</keyname><forenames>Raul</forenames></author></authors><title>An Improved Hazard Rate Twisting Approach for the Statistic of the Sum
  of Subexponential Variates (Extended Version)</title><categories>cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we present an improved hazard rate twisting technique for the
estimation of the probability that a sum of independent but not necessarily
identically distributed subexponential Random Variables (RVs) exceeds a given
threshold. Instead of twisting all the components in the summation, we propose
to twist only the RVs which have the biggest impact on the right-tail of the
sum distribution and keep the other RVs unchanged. A minmax approach is
performed to determine the optimal twisting parameter which leads to an
asymptotic optimality criterion. Moreover, we show through some selected
simulation results that our proposed approach results in a variance reduction
compared to the technique where all the components are twisted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4286</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4286</id><created>2014-11-16</created><authors><author><keyname>Qin</keyname><forenames>Zhiwei</forenames></author><author><keyname>Tang</keyname><forenames>Xiaocheng</forenames></author><author><keyname>Akrotirianakis</keyname><forenames>Ioannis</forenames></author><author><keyname>Chakraborty</keyname><forenames>Amit</forenames></author></authors><title>HIPAD - A Hybrid Interior-Point Alternating Direction algorithm for
  knowledge-based SVM and feature selection</title><categories>stat.ML cs.LG</categories><comments>Proceedings of 8th Learning and Intelligent OptimizatioN (LION8)
  Conference, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider classification tasks in the regime of scarce labeled training
data in high dimensional feature space, where specific expert knowledge is also
available. We propose a new hybrid optimization algorithm that solves the
elastic-net support vector machine (SVM) through an alternating direction
method of multipliers in the first phase, followed by an interior-point method
for the classical SVM in the second phase. Both SVM formulations are adapted to
knowledge incorporation. Our proposed algorithm addresses the challenges of
automatic feature selection, high optimization accuracy, and algorithmic
flexibility for taking advantage of prior knowledge. We demonstrate the
effectiveness and efficiency of our algorithm and compare it with existing
methods on a collection of synthetic and real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4287</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4287</id><created>2014-11-16</created><authors><author><keyname>Avendi</keyname><forenames>M. R.</forenames></author><author><keyname>Jafarkhani</keyname><forenames>Hamid</forenames></author></authors><title>Differential Distributed Space-Time Coding with Imperfect
  Synchronization in Frequency-Selective Channels</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transaction on Wireless Communications, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential distributed space-time coding (D-DSTC) is a cooperative
transmission technique that can improve diversity in wireless relay networks in
the absence of channel information. Conventionally, it is assumed that channels
are flat-fading and relays are perfectly synchronized at the symbol level.
However, due to the delay spread in broadband systems and the distributed
nature of relay networks, these assumptions may be violated. Hence,
inter-symbol interference (ISI) may appear. This paper proposes a new
differential encoding and decoding process for D-DSTC systems with multiple
relays over slow frequency-selective fading channels with imperfect
synchronization. The proposed method overcomes the ISI caused by
frequency-selectivity and is robust against synchronization errors while not
requiring any channel information at the relays and destination. Moreover, the
maximum possible diversity with a decoding complexity similar to that of the
conventional D-DSTC is attained. Simulation results are provided to show the
performance of the proposed method in various scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4290</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4290</id><created>2014-11-16</created><authors><author><keyname>Guerreiro</keyname><forenames>Rui F. C.</forenames></author><author><keyname>Aguiar</keyname><forenames>Pedro M. Q.</forenames></author></authors><title>Maximizing compression efficiency through block rotation</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Discrete Cosine Transform (DCT) is widely used in lossy image and video
compression schemes, e.g., JPEG and MPEG. In this paper, we show that the
compression efficiency of the DCT is dependent on the edge directions within a
block. In particular, higher compression ratios are achieved when edges are
aligned with the image axes. To maximize compression for general images, we
propose a rotated block DCT method. It consists of rotating each block, before
applying the DCT, by an angle that aligns the edges, and rotating back the
block in the decompression stage. We show how to compute the rotation angle and
analyze two alternative block rotation approaches. Our experiments show that
our method enables both a perceptual improvement and a PSNR increase of up to
2dB, compared with the standard DCT, for low and medium bit rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4294</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4294</id><created>2014-10-14</created><updated>2015-07-05</updated><authors><author><keyname>Madadipouya</keyname><forenames>Kasra</forenames></author></authors><title>An Examination And Report On Potential Methods Of Strategic
  Location-Based Service Applications On Mobile Networks And Devices</title><categories>cs.CY</categories><comments>7 pages</comments><journal-ref>International Journal of Managing Public Sector Information and
  Communication Technologies, Volume 5, Issue 3, 25-31 (2014)</journal-ref><doi>10.5121/ijmpict.2014.5303</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Mobile technologies are growing significantly in past few years. Many new
features and enhancement have implemented in mobile technologies in both
software and hardware aspects. Nowadays, cell phones are not just only use for
making calls or sending text messages, however, technologies behind mobile
phones expanded vastly which facilitate them to offer various types of
services. Location-based service is one of the most popular mobile technologies
which equipped in new generation of hand phones. The main focus of this paper
is to review various strategic location-based applications on mobile networks
and devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4296</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4296</id><created>2014-11-16</created><authors><author><keyname>Guerreiro</keyname><forenames>Rui F. C.</forenames></author></authors><title>Combining contextual and local edges for line segment extraction in
  cluttered images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic extraction methods typically assume that line segments are
pronounced, thin, few and far between, do not cross each other, and are noise
and clutter-free. Since these assumptions often fail in realistic scenarios,
many line segments are not detected or are fragmented. In more severe cases,
i.e., many who use the Hough Transform, extraction can fail entirely. In this
paper, we propose a method that tackles these issues. Its key aspect is the
combination of thresholded image derivatives obtained with filters of large and
small footprints, which we denote as contextual and local edges, respectively.
Contextual edges are robust to noise and we use them to select valid local
edges, i.e., local edges that are of the same type as contextual ones:
dark-to-bright transition of vice-versa. If the distance between valid local
edges does not exceed a maximum distance threshold, we enforce connectivity by
marking them and the pixels in between as edge points. This originates
connected edge maps that are robust and well localized. We use a powerful
two-sample statistical test to compute contextual edges, which we introduce
briefly, as they are unfamiliar to the image processing community. Finally, we
present experiments that illustrate, with synthetic and real images, how our
method is efficient in extracting complete segments of all lengths and widths
in several situations where current methods fail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4297</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4297</id><created>2014-11-05</created><authors><author><keyname>Kalyani</keyname><forenames>Rishita</forenames></author></authors><title>Application of Multi-core Parallel Programming to a Combination of Ant
  Colony Optimization and Genetic Algorithm</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This Paper will deal with a combination of Ant Colony and Genetic Programming
Algorithm to optimize Travelling Salesmen problem (NP-Hard). However, the
complexity of the algorithm requires considerable computational time and
resources. Parallel implementation can reduce the computational time. In this
paper, emphasis in the parallelizing section is given to Multi-core
architecture and Multi-Processor Systems which is developed and used almost
everywhere today and hence, multi-core parallelization to the combination of
algorithm is achieved by OpenMP library by Intel Corporation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4299</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4299</id><created>2014-11-16</created><authors><author><keyname>Aggarwal</keyname><forenames>Anupama</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>What They Do in Shadows: Twitter Underground Follower Market</title><categories>cs.SI</categories><comments>arXiv admin note: text overlap with arXiv:1408.1534</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet users and businesses are increasingly using online social networks
(OSN) to drive audience traffic and increase their popularity. In order to
boost social presence, OSN users need to increase the visibility and reach of
their online profile, like - Facebook likes, Twitter followers, Instagram
comments and Yelp reviews. For example, an increase in Twitter followers not
only improves the audience reach of the user but also boosts the perceived
social reputation and popularity. This has led to a scope for an underground
market that provides followers, likes, comments, etc. via a network of
fraudulent and compromised accounts and various collusion techniques.
  In this paper, we landscape the underground markets that provide Twitter
followers by studying their basic building blocks - merchants, customers and
phony followers. We charecterize the services provided by merchants to
understand their operational structure and market hierarchy. Twitter
underground markets can operationalize using a premium monetary scheme or other
incentivized freemium schemes. We find out that freemium market has an
oligopoly structure with few merchants being the market leaders. We also show
that merchant popularity does not have any correlation with the quality of
service provided by the merchant to its customers. Our findings also shed light
on the characteristics and quality of market customers and the phony followers
provided. We draw comparison between legitimate users and phony followers, and
find out key identifiers to separate such users. With the help of these
differentiating features, we build a supervised learning model to predict
suspicious following behaviour with an accuracy of 89.2%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4304</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4304</id><created>2014-11-16</created><authors><author><keyname>Benenson</keyname><forenames>Rodrigo</forenames></author><author><keyname>Omran</keyname><forenames>Mohamed</forenames></author><author><keyname>Hosang</keyname><forenames>Jan</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>Ten Years of Pedestrian Detection, What Have We Learned?</title><categories>cs.CV</categories><comments>To appear in ECCV 2014 CVRSUAD workshop proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Paper-by-paper results make it easy to miss the forest for the trees.We
analyse the remarkable progress of the last decade by discussing the main ideas
explored in the 40+ detectors currently present in the Caltech pedestrian
detection benchmark. We observe that there exist three families of approaches,
all currently reaching similar detection quality. Based on our analysis, we
study the complementarity of the most promising ideas by combining multiple
published strategies. This new decision forest detector achieves the current
best known performance on the challenging Caltech-USA dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4314</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4314</id><created>2014-11-16</created><authors><author><keyname>Sims</keyname><forenames>Benjamin H.</forenames></author><author><keyname>Sinitsyn</keyname><forenames>Nikolai</forenames></author><author><keyname>Eidenbenz</keyname><forenames>Stephan J.</forenames></author></authors><title>Hierarchical and Matrix Structures in a Large Organizational Email
  Network: Visualization and Modeling Approaches</title><categories>cs.SI physics.soc-ph stat.AP</categories><comments>15 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents findings from a study of the email network of a large
scientific research organization, focusing on methods for visualizing and
modeling organizational hierarchies within large, complex network datasets. In
the first part of the paper, we find that visualization and interpretation of
complex organizational network data is facilitated by integration of network
data with information on formal organizational divisions and levels. By
aggregating and visualizing email traffic between organizational units at
various levels, we derive several insights into how large subdivisions of the
organization interact with each other and with outside organizations. Our
analysis shows that line and program management interactions in this
organization systematically deviate from the idealized pattern of interaction
prescribed by &quot;matrix management.&quot; In the second part of the paper, we propose
a power law model for predicting degree distribution of organizational email
traffic based on hierarchical relationships between managers and employees.
This model considers the influence of global email announcements sent from
managers to all employees under their supervision, and the role support staff
play in generating email traffic, acting as agents for managers. We also
analyze patterns in email traffic volume over the course of a work week.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4315</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4315</id><created>2014-11-16</created><authors><author><keyname>Schl&#xe4;pfer</keyname><forenames>Markus</forenames></author><author><keyname>Mancarella</keyname><forenames>Pierluigi</forenames></author></authors><title>Probabilistic Modeling and Simulation of Transmission Line Temperatures
  under Fluctuating Power Flows</title><categories>cs.SY</categories><journal-ref>IEEE Transactions on Power Delivery, vol. 26, no. 4, pp. 2235 -
  2243, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing shares of fluctuating renewable energy sources induce higher and
higher power flow variability at the transmission level. The question arises as
to what extent existing networks can absorb additional fluctuating power
injection without exceeding thermal limits. At the same time, the resulting
power flow characteristics call for revisiting classical approaches to line
temperature prediction. This paper presents a probabilistic modeling and
simulation methodology for estimating the occurrence of critical line
temperatures in the presence of fluctuating power flows. Cumbersome integration
of the dynamic thermal equations at each Monte Carlo simulation trial is sped
up by a specific algorithm that makes use of a variance reduction technique
adapted from the telecommunications field. The substantial reduction in
computational time allows estimations closer to real time, relevant to
short-term operational assessments. A case study performed on a single line
model provides fundamental insights into the probability of hitting critical
line temperatures under given power flow fluctuations. A transmission system
application shows how the proposed method can be used for a fast yet accurate
operational assessment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4324</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4324</id><created>2014-11-16</created><authors><author><keyname>Xu</keyname><forenames>Yangyang</forenames></author></authors><title>On Higher-order Singular Value Decomposition from Incomplete Data</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Higher-order singular value decomposition (HOSVD) is an efficient way for
data reduction and also eliciting intrinsic structure of multi-dimensional
array data. It has been used in many applications, and some of them involve
incomplete data. To obtain HOSVD of the data with missing values, one can first
impute the missing entries through a certain tensor completion method and then
perform HOSVD to the reconstructed data. However, the two-step procedure can be
inefficient and does not make reliable decomposition.
  In this paper, we formulate an incomplete HOSVD problem and combine the two
steps into solving a single optimization problem, which simultaneously achieves
imputation of missing values and also tensor decomposition. We also present two
algorithms for solving the problem based on block coordinate update. Global
convergence of both algorithms is shown under mild assumptions. The convergence
of the second algorithm implies that of the popular higher-order orthogonality
iteration (HOOI) method, and thus we, for the first time, give global
convergence of HOOI.
  In addition, we compare the proposed methods to state-of-the-art ones for
solving incomplete HOSVD and also low-rank tensor completion problems and
demonstrate the superior performance of our methods over other compared ones.
Furthermore, we apply them to face recognition and MRI image reconstruction to
show their practical performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4327</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4327</id><created>2014-11-16</created><authors><author><keyname>Brunetto</keyname><forenames>Matteo</forenames></author></authors><title>Automatic Synthesis of Test Cases to Identify Software Redundancy</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software system can include redundant implementation elements, such as,
different methods that can produce indistinguishable results. This type of
redundancy is called intrinsic if it is already available in the software,
although not intentionally planned. Redundancy can be a key element to increase
the reliability of a system. Some fault tolerance and self-healing techniques
exploit the redundancy to avoid failures at runtime. Unfortunately, inferring
which operations are equivalent manually can be expensive and error prone.
  A technique proposed in previous work allows to automatically synthesizes
method sequences that are equivalent to a target method. However this technique
needs an execution scenario to work. Currently, this execution scenario is
generated manually that is expensive and makes the technique hard to use.
  This paper proposes a technique to generate execution scenarios for a target
method for which we are searching equivalent sequences. The experimental
results obtained on the Java class Stack show that the proposed approach
correctly generates execution scenarios within reasonable execution time.
Besides, the execution scenarios generated allow to maximize the effectiveness
of the technique described above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4328</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4328</id><created>2014-11-16</created><authors><author><keyname>Allahverdyan</keyname><forenames>A. E.</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author></authors><title>Opinion Dynamics with Confirmation Bias</title><categories>physics.soc-ph cs.SI</categories><comments>18 pages</comments><journal-ref>PLoS ONE 9(7), e99557 (2014)</journal-ref><doi>10.1371/journal.pone.0099557</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Confirmation bias is the tendency to acquire or evaluate new
information in a way that is consistent with one's preexisting beliefs. It is
omnipresent in psychology, economics, and even scientific practices. Prior
theoretical research of this phenomenon has mainly focused on its economic
implications possibly missing its potential connections with broader notions of
cognitive science. Methodology/Principal Findings: We formulate a
(non-Bayesian) model for revising subjective probabilistic opinion of a
confirmationally-biased agent in the light of a persuasive opinion. The
revision rule ensures that the agent does not react to persuasion that is
either far from his current opinion or coincides with it. We demonstrate that
the model accounts for the basic phenomenology of the social judgment theory,
and allows to study various phenomena such as cognitive dissonance and
boomerang effect. The model also displays the order of presentation effect|when
consecutively exposed to two opinions, the preference is given to the last
opinion (recency) or the first opinion (primacy)|and relates recency to
confirmation bias. Finally, we study the model in the case of repeated
persuasion and analyze its convergence properties. Conclusions: The standard
Bayesian approach to probabilistic opinion revision is inadequate for
describing the observed phenomenology of persuasion process. The simple
non-Bayesian model proposed here does agree with this phenomenology and is
capable of reproducing a spectrum of effects observed in psychology:
primacy-recency phenomenon, boomerang effect and cognitive dissonance. We point
out several limitations of the model that should motivate its future
development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4331</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4331</id><created>2014-11-16</created><authors><author><keyname>Zhang</keyname><forenames>Weipeng</forenames></author><author><keyname>Shen</keyname><forenames>Jie</forenames></author><author><keyname>Liu</keyname><forenames>Guangcan</forenames></author><author><keyname>Yu</keyname><forenames>Yong</forenames></author></authors><title>A Latent Clothing Attribute Approach for Human Pose Estimation</title><categories>cs.CV</categories><comments>accepted to ACCV 2014, preceding work http://arxiv.org/abs/1404.4923</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a fundamental technique that concerns several vision tasks such as image
parsing, action recognition and clothing retrieval, human pose estimation (HPE)
has been extensively investigated in recent years. To achieve accurate and
reliable estimation of the human pose, it is well-recognized that the clothing
attributes are useful and should be utilized properly. Most previous
approaches, however, require to manually annotate the clothing attributes and
are therefore very costly. In this paper, we shall propose and explore a
\emph{latent} clothing attribute approach for HPE. Unlike previous approaches,
our approach models the clothing attributes as latent variables and thus
requires no explicit labeling for the clothing attributes. The inference of the
latent variables are accomplished by utilizing the framework of latent
structured support vector machines (LSSVM). We employ the strategy of
\emph{alternating direction} to train the LSSVM model: In each iteration, one
kind of variables (e.g., human pose or clothing attribute) are fixed and the
others are optimized. Our extensive experiments on two real-world benchmarks
show the state-of-the-art performance of our proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4332</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4332</id><created>2014-11-16</created><updated>2014-12-24</updated><authors><author><keyname>Laroussinie</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>LIAFA -- Universite Paris Diderot and CNRS</affiliation></author><author><keyname>Markey</keyname><forenames>Nicolas</forenames><affiliation>LSV -- ENS Cachan and CNRS</affiliation></author></authors><title>Quantified CTL: Expressiveness and Complexity</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (December
  25, 2014) lmcs:1029</journal-ref><doi>10.2168/LMCS-10(4:17)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While it was defined long ago, the extension of CTL with quantification over
atomic propositions has never been studied extensively. Considering two
different semantics (depending whether propositional quantification refers to
the Kripke structure or to its unwinding tree), we study its expressiveness
(showing in particular that QCTL coincides with Monadic Second-Order Logic for
both semantics) and characterise the complexity of its model-checking and
satisfiability problems, depending on the number of nested propositional
quantifiers (showing that the structure semantics populates the polynomial
hierarchy while the tree semantics populates the exponential hierarchy).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4340</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4340</id><created>2014-11-16</created><authors><author><keyname>Yan</keyname><forenames>Tongjiang</forenames></author><author><keyname>Gong</keyname><forenames>Guang</forenames></author></authors><title>Some Notes on Constructions of Binary Sequences with Optimal
  Autocorrelation</title><categories>cs.IT cs.CR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constructions of binary sequences with low autocorrelation are considered in
the paper. Based on recent progresses about this topic, several more general
constructions of binary sequences with optimal autocorrelations and other low
autocorrelations are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4342</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4342</id><created>2014-11-16</created><updated>2015-06-19</updated><authors><author><keyname>Kandasamy</keyname><forenames>Kirthevasan</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Akshay</forenames></author><author><keyname>Poczos</keyname><forenames>Barnabas</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author><author><keyname>Robins</keyname><forenames>James M.</forenames></author></authors><title>Influence Functions for Machine Learning: Nonparametric Estimators for
  Entropies, Divergences and Mutual Informations</title><categories>stat.ML cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and analyze estimators for statistical functionals of one or more
distributions under nonparametric assumptions. Our estimators are based on the
theory of influence functions, which appear in the semiparametric statistics
literature. We show that estimators based either on data-splitting or a
leave-one-out technique enjoy fast rates of convergence and other favorable
theoretical properties. We apply this framework to derive estimators for
several popular information theoretic quantities, and via empirical evaluation,
show the advantage of this approach over existing estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4345</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4345</id><created>2014-11-16</created><authors><author><keyname>Alas</keyname><forenames>Yabit</forenames></author><author><keyname>Anshari</keyname><forenames>Muhammad</forenames></author></authors><title>Constructing Strategy of Online Learning in Higher Education:
  Transaction Cost Economy</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The online learning tools and management also known as Learning Management
System (LMS) have been adopted by higher education as it allows convenient and
flexibility in learning process between students and instructors or tutors with
minimal cost. The adoption of online learning tools in university has allowed
users (students and instructors) to interact, share and discuss
anytime-anywhere conveniently. Many students nowadays rely on online resources
based using their mobile devices, substituting traditional learning
interactions. Universities need strategy to sustain in providing intensive
interactions and spreading word out mouth of good services through online
learning tools by focusing on niche markets and creating close relationship
with their stakeholders. The study presented in this paper analyses how
universities design best practices in adopting LMS and evaluate its current
state for future improvement. In fact, with proper strategies of LMS,
universities have opportunities to sustain their business by offering
interesting packages and to improve their services through intensive
interactions with their users. In this study, we deploy Transaction Cost
Economics (TCE) to understand the change business environment and to construct
a model for higher institution to regulate their scenario on online learning
strategies in fast changing and threatening business environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4346</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4346</id><created>2014-11-16</created><updated>2015-08-27</updated><authors><author><keyname>Wang</keyname><forenames>Yunpeng</forenames></author><author><keyname>Cheng</keyname><forenames>Long</forenames></author><author><keyname>Ren</keyname><forenames>Wei</forenames></author><author><keyname>Hou</keyname><forenames>Zeng-Guang</forenames></author><author><keyname>Tan</keyname><forenames>Min</forenames></author></authors><title>Containment Control of Multi-Agent Systems with Dynamic Leaders Based on
  a $PI^n$-Type Approach</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the containment control problem of multi-agent systems
with multiple dynamic leaders in both the discrete-time domain and the
continuous-time domain. The leaders' motions are described by $(n-1)$-order
polynomial trajectories. This setting makes practical sense because given some
critical points, the leaders' trajectories are usually planned by the
polynomial interpolations. In order to drive all followers into the convex hull
spanned by the leaders, a $PI^n$-type ($P$ and $I$ are short for {\it
Proportion} and {\it Integration}, respectively; $I^n$ implies that the
algorithm includes high-order integral terms) containment algorithm is
proposed. It is theoretically proved that the $PI^n$-type containment algorithm
is able to solve the containment problem of multi-agent systems where the
followers are described by any order integral dynamics. Compared with the
previous results on the multi-agent systems with dynamic leaders, the
distinguished features of this paper are that: (1) the containment problem is
studied not only in the continuous-time domain but also in the discrete-time
domain while most existing results only work in the continuous-time domain; (2)
to deal with the leaders with the $(n-1)$-order polynomial trajectories,
existing results require the follower's dynamics to be $n$-order integral while
the followers considered in this paper can be described by any-order integral;
and (3) the &quot;sign&quot; function is not employed in the proposed algorithm, which
avoids the chattering phenomenon. Furthermore, in order to illustrate the
practical value of the proposed approach, an application, the containment
control of multiple mobile robots is studied. Finally, two simulation examples
are given to demonstrate the effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4351</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4351</id><created>2014-11-16</created><updated>2015-03-17</updated><authors><author><keyname>Krishnan</keyname><forenames>Vinodh</forenames></author><author><keyname>Eisenstein</keyname><forenames>Jacob</forenames></author></authors><title>&quot;You're Mr. Lebowski, I'm the Dude&quot;: Inducing Address Term Formality in
  Signed Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>In Proceedings of NAACL-HLT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an unsupervised model for inducing signed social networks from the
content exchanged across network edges. Inference in this model solves three
problems simultaneously: (1) identifying the sign of each edge; (2)
characterizing the distribution over content for each edge type; (3) estimating
weights for triadic features that map to theoretical models such as structural
balance. We apply this model to the problem of inducing the social function of
address terms, such as 'Madame', 'comrade', and 'dude'. On a dataset of movie
scripts, our system obtains a coherent clustering of address terms, while at
the same time making intuitively plausible judgments of the formality of social
relations in each film. As an additional contribution, we provide a
bootstrapping technique for identifying and tagging address terms in dialogue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4357</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4357</id><created>2014-11-16</created><updated>2015-02-10</updated><authors><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>Sketching as a Tool for Numerical Linear Algebra</title><categories>cs.DS</categories><comments>fixed minor errors/typos in section 4.3, e.g., Fact 6 and its
  propagation, clarified when Lemma 4.2 can be applied, typos in section 4.2.3
  (G should be applied on the left), other typos throughout</comments><journal-ref>Foundations and Trends in Theoretical Computer Science, Vol 10,
  Issue 1--2, 2014, pp 1--157</journal-ref><doi>10.1561/0400000060</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This survey highlights the recent advances in algorithms for numerical linear
algebra that have come from the technique of linear sketching, whereby given a
matrix, one first compresses it to a much smaller matrix by multiplying it by a
(usually) random matrix with certain properties. Much of the expensive
computation can then be performed on the smaller matrix, thereby accelerating
the solution for the original problem. In this survey we consider least squares
as well as robust regression problems, low rank approximation, and graph
sparsification. We also discuss a number of variants of these problems.
Finally, we discuss the limitations of sketching methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4366</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4366</id><created>2014-11-17</created><authors><author><keyname>Dahiwale</keyname><forenames>Prashant</forenames></author><author><keyname>Raghuwanshi</keyname><forenames>M M</forenames></author><author><keyname>malik</keyname><forenames>Latesh</forenames></author></authors><title>PDD Crawler: A focused web crawler using link and content analysis for
  relevance prediction</title><categories>cs.IR</categories><comments>9 pages, SEAS-2014, Dubai, UAE, International Conference 7-8 Nov 2014</comments><msc-class>70-XX</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Majority of the computer or mobile phone enthusiasts make use of the web for
searching activity. Web search engines are used for the searching; The results
that the search engines get are provided to it by a software module known as
the Web Crawler. The size of this web is increasing round-the-clock. The
principal problem is to search this huge database for specific information. To
state whether a web page is relevant to a search topic is a dilemma. This paper
proposes a crawler called as PDD crawler which will follow both a link based as
well as a content based approach. This crawler follows a completely new
crawling strategy to compute the relevance of the page. It analyses the content
of the page based on the information contained in various tags within the HTML
source code and then computes the total weight of the page. The page with the
highest weight, thus has the maximum content and highest relevance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4369</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4369</id><created>2014-11-17</created><authors><author><keyname>Lehmann</keyname><forenames>Karsten</forenames></author><author><keyname>Grastien</keyname><forenames>Alban</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author></authors><title>The Complexity of DC-Switching Problems</title><categories>cs.CC math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report provides a comprehensive complexity study of line switching in
the Linear DC model for the feasibility problem and the optimization problems
of maximizing the load that can be served (maximum switching flow, MSF) and
minimizing generation cost (optimal transmission switching, OTS). Our results
show that these problems are NP-complete and that there is no fully
polynomial-time approximation scheme for planar networks with a maximum-node
degree of 3. Additionally, we demonstrate that the problems are still NP-hard
if we restrict the network structure to cacti with a maximum degree of 3. We
also show that the optimization problems can not be approximated within any
constant factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4373</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4373</id><created>2014-11-17</created><authors><author><keyname>Wei</keyname><forenames>Lai</forenames></author><author><keyname>Mitchell</keyname><forenames>David G. M.</forenames></author><author><keyname>Fuja</keyname><forenames>Thomas E.</forenames></author><author><keyname>Costello</keyname><forenames>Daniel J.</forenames></author></authors><title>Design of Spatially Coupled LDPC Codes over GF(q) for Windowed Decoding</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the generalization of binary spatially coupled
low-density parity-check (SC-LDPC) codes to finite fields GF$(q)$, $q\geq 2$,
and develop design rules for $q$-ary SC-LDPC code ensembles based on their
iterative belief propagation (BP) decoding thresholds, with particular emphasis
on low-latency windowed decoding (WD). We consider transmission over both the
binary erasure channel (BEC) and the binary-input additive white Gaussian noise
channel (BIAWGNC) and present results for a variety of $(J,K)$-regular SC-LDPC
code ensembles constructed over GF$(q)$ using protographs. Thresholds are
calculated using protograph versions of $q$-ary density evolution (for the BEC)
and $q$-ary extrinsic information transfer analysis (for the BIAWGNC). We show
that WD of $q$-ary SC-LDPC codes provides significant threshold gains compared
to corresponding (uncoupled) $q$-ary LDPC block code (LDPC-BC) ensembles when
the window size $W$ is large enough and that these gains increase as the finite
field size $q=2^m$ increases. Moreover, we demonstrate that the new design
rules provide WD thresholds that are close to capacity, even when both $m$ and
$W$ are relatively small (thereby reducing decoding complexity and latency).
The analysis further shows that, compared to standard flooding-schedule
decoding, WD of $q$-ary SC-LDPC code ensembles results in significant
reductions in both decoding complexity and decoding latency, and that these
reductions increase as $m$ increases. For applications with a near-threshold
performance requirement and a constraint on decoding latency, we show that
using $q$-ary SC-LDPC code ensembles, with moderate $q&gt;2$, instead of their
binary counterparts results in reduced decoding complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4379</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4379</id><created>2014-11-17</created><authors><author><keyname>Islam</keyname><forenames>Md. Lisul</forenames></author><author><keyname>Nurain</keyname><forenames>Novia</forenames></author><author><keyname>Shatabda</keyname><forenames>Swakkhar</forenames></author><author><keyname>Rahman</keyname><forenames>M Sohel</forenames></author></authors><title>FGPGA: An Efficient Genetic Approach for Producing Feasible Graph
  Partitions</title><categories>cs.NE cs.AI cs.DC</categories><comments>Accepted in the 1st International Conference on Networking Systems
  and Security 2015 (NSysS 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph partitioning, a well studied problem of parallel computing has many
applications in diversified fields such as distributed computing, social
network analysis, data mining and many other domains. In this paper, we
introduce FGPGA, an efficient genetic approach for producing feasible graph
partitions. Our method takes into account the heterogeneity and capacity
constraints of the partitions to ensure balanced partitioning. Such approach
has various applications in mobile cloud computing that include feasible
deployment of software applications on the more resourceful infrastructure in
the cloud instead of mobile hand set. Our proposed approach is light weight and
hence suitable for use in cloud architecture. We ensure feasibility of the
partitions generated by not allowing over-sized partitions to be generated
during the initialization and search. Our proposed method tested on standard
benchmark datasets significantly outperforms the state-of-the-art methods in
terms of quality of partitions and feasibility of the solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4380</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4380</id><created>2014-11-17</created><updated>2015-01-28</updated><authors><author><keyname>Grellois</keyname><forenames>Charles</forenames></author><author><keyname>Melli&#xe8;s</keyname><forenames>Paul-Andr&#xe9;</forenames></author></authors><title>An infinitary model of linear logic</title><categories>cs.LO</categories><comments>Accepted at Fossacs 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we construct an infinitary variant of the relational model of
linear logic, where the exponential modality is interpreted as the set of
finite or countable multisets. We explain how to interpret in this model the
fixpoint operator Y as a Conway operator alternatively defined in an inductive
or a coinductive way. We then extend the relational semantics with a notion of
color or priority in the sense of parity games. This extension enables us to
define a new fixpoint operator Y combining both inductive and coinductive
policies. We conclude the paper by sketching the connection between the
resulting model of lambda-calculus with recursion and higher-order
model-checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4384</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4384</id><created>2014-11-17</created><authors><author><keyname>Huang</keyname><forenames>Zhiyi</forenames></author><author><keyname>Kim</keyname><forenames>Anthony</forenames></author></authors><title>Welfare Maximization with Production Costs: A Primal Dual Approach</title><categories>cs.DS cs.GT</categories><comments>To appear in the ACM-SIAM Symposium on Discrete Algorithms (SODA),
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study online combinatorial auctions with production costs proposed by Blum
et al. using the online primal dual framework. In this model, buyers arrive
online, and the seller can produce multiple copies of each item subject to a
non-decreasing marginal cost per copy. The goal is to allocate items to
maximize social welfare less total production cost. For arbitrary (strictly
convex and differentiable) production cost functions, we characterize the
optimal competitive ratio achievable by online mechanisms/algorithms. We show
that online posted pricing mechanisms, which are incentive compatible, can
achieve competitive ratios arbitrarily close to the optimal, and construct
lower bound instances on which no online algorithms, not necessarily incentive
compatible, can do better. Our positive results improve or match the results in
several previous work, e.g., Bartal et al., Blum et al., and Buchbinder and
Gonen. Our lower bounds apply to randomized algorithms and resolve an open
problem by Buchbinder and Gonen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4389</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4389</id><created>2014-11-17</created><updated>2015-02-17</updated><authors><author><keyname>Donahue</keyname><forenames>Jeff</forenames></author><author><keyname>Hendricks</keyname><forenames>Lisa Anne</forenames></author><author><keyname>Guadarrama</keyname><forenames>Sergio</forenames></author><author><keyname>Rohrbach</keyname><forenames>Marcus</forenames></author><author><keyname>Venugopalan</keyname><forenames>Subhashini</forenames></author><author><keyname>Saenko</keyname><forenames>Kate</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>Long-term Recurrent Convolutional Networks for Visual Recognition and
  Description</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Models based on deep convolutional networks have dominated recent image
interpretation tasks; we investigate whether models which are also recurrent,
or &quot;temporally deep&quot;, are effective for tasks involving sequences, visual and
otherwise. We develop a novel recurrent convolutional architecture suitable for
large-scale visual learning which is end-to-end trainable, and demonstrate the
value of these models on benchmark video recognition tasks, image description
and retrieval problems, and video narration challenges. In contrast to current
models which assume a fixed spatio-temporal receptive field or simple temporal
averaging for sequential processing, recurrent convolutional models are &quot;doubly
deep&quot;' in that they can be compositional in spatial and temporal &quot;layers&quot;. Such
models may have advantages when target concepts are complex and/or training
data are limited. Learning long-term dependencies is possible when
nonlinearities are incorporated into the network state updates. Long-term RNN
models are appealing in that they directly can map variable-length inputs
(e.g., video frames) to variable length outputs (e.g., natural language text)
and can model complex temporal dynamics; yet they can be optimized with
backpropagation. Our recurrent long-term models are directly connected to
modern visual convnet models and can be jointly trained to simultaneously learn
temporal dynamics and convolutional perceptual representations. Our results
show such models have distinct advantages over state-of-the-art models for
recognition or generation which are separately defined and/or optimized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4398</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4398</id><created>2014-11-17</created><authors><author><keyname>Asbullah</keyname><forenames>Muhammad Asyraf</forenames></author><author><keyname>Ariffin</keyname><forenames>Muhammad Rezal Kamel</forenames></author></authors><title>Rabin-$p$ Cryptosystem: Practical and Efficient Method for Rabin based
  Encryption Scheme</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we introduce a new, efficient and practical scheme based on the
Rabin cryptosystem without using the Jacobi symbol, message redundancy
technique or the needs of extra bits in order to specify the correct plaintext.
Our system involves only a single prime number as the decryption key and does
only one modular exponentiation. Consequently, this will practically reduce the
computational efforts during decryption process. We demonstrate that the
decryption is unique and proven to be equivalent to factoring.The scheme is
performs better when compared to a number of Rabin cryptosystem variants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4399</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4399</id><created>2014-11-17</created><updated>2015-10-14</updated><authors><author><keyname>Samadi</keyname><forenames>Zainalabedin</forenames></author><author><keyname>Vakily</keyname><forenames>Vahid Tabataba</forenames></author><author><keyname>Haddadi</keyname><forenames>Farzan</forenames></author></authors><title>Channel Aided Interference Alignment</title><categories>cs.IT math.IT</categories><comments>27 pages, 4 figure, submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference alignment is a transmission strategy that attempts to exploit
all degrees of freedom available at the transmitter or receiver in a multiuser
channel. This paper proposes a new scheme called channel aided IA. It makes use
of the channel structure beside the linear interference alignment schemes to
achieve the optimum degrees of freedom in a $K$ user interference channel (IC).
In case the channel matrix does meet the specified structure, the proposed
scheme allows each user to achieve at least $1/2$ of its interference-free
ergodic capacity at any signal-to-noise ratio. This technique turns to the
usual linear vector interference alignment scheme if channel coefficients do
not meet necessary structure within a limited time frame. For the case of 3
user IC, it is shown that if only one of interfering channel coefficients can
be designed to a specific value, interference would be aligned perfectly at all
receivers. Derived channel aiding conditions can equivalently be considered as
the perfect interference alignment feasibility conditions on channel structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4407</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4407</id><created>2014-11-17</created><authors><author><keyname>Santhanam</keyname><forenames>N.</forenames></author><author><keyname>Anantharam</keyname><forenames>V.</forenames></author><author><keyname>Kavcic</keyname><forenames>A.</forenames></author><author><keyname>Szpankowski</keyname><forenames>W.</forenames></author></authors><title>Data driven consistency (working title)</title><categories>cs.IT math.IT</categories><comments>Working paper. Please email authors for the current version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are motivated by applications that need rich model classes to represent
them. Examples of rich model classes include distributions over large,
countably infinite supports, slow mixing Markov processes, etc. But such rich
classes may be too complex to admit estimators that converge to the truth with
convergence rates that can be uniformly bounded over the entire model class as
the sample size increases (uniform consistency). However, these rich classes
may still allow for estimators with pointwise guarantees whose performance can
be bounded in a model dependent way. The pointwise angle of course has the
drawback that the estimator performance is a function of the very unknown model
that is being estimated, and is therefore unknown. Therefore, even if the
estimator is consistent, how well it is doing may not be clear no matter what
the sample size is. Departing from the dichotomy of uniform and pointwise
consistency, a new analysis framework is explored by characterizing rich model
classes that may only admit pointwise guarantees, yet all the information about
the model needed to guage estimator accuracy can be inferred from the sample at
hand. To retain focus, we analyze the universal compression problem in this
data driven pointwise consistency framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4419</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4419</id><created>2014-11-17</created><updated>2015-05-15</updated><authors><author><keyname>Peng</keyname><forenames>Xi</forenames></author><author><keyname>Lu</keyname><forenames>Jiwen</forenames></author><author><keyname>Yan</keyname><forenames>Rui</forenames></author><author><keyname>Yi</keyname><forenames>Zhang</forenames></author></authors><title>Automatic Subspace Learning via Principal Coefficients Embedding</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address two problems in unsupervised subspace learning: 1)
how to automatically identify the feature dimension of the learned subspace,
and 2) how to learn the underlying subspace in the presence of gross
corruptions such as Gaussian noise. We show that these two problems are two
sides of one coin, i.e. they can be solved by removing possible errors from
training data $\mathbf{D}\in \mathds{R}^{m\times n}$. To achieve this, we
propose a new method (called Principal Coefficients Embedding, PCE) that can
simultaneously learn a clean data set $\mathbf{D}_{0}\in \mathds{R}^{m\times
n}$ and a linear representation (denoted by $\mathbf{C}$) from $\mathbf{D}$. By
embedding $\mathbf{C}$ into a $k$-dimensional space, PCE obtains a projection
matrix that preserves some desirable properties of inputs, where $k\ll m$ is
exactly the rank of $\mathbf{C}$. PCE has three advantages: 1) it can
automatically determine the feature dimension even though data are sampled from
a union of multiple linear subspaces; 2) it is robust to various noises and
real disguises; 3) it has a closed-form solution and can be calculated very
fast. Extensive experimental results show the superiority of PCE on a range of
databases with respect to classification accuracy, robustness and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4423</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4423</id><created>2014-11-17</created><updated>2015-09-23</updated><authors><author><keyname>Chatzis</keyname><forenames>Sotirios P.</forenames></author></authors><title>A Nonparametric Bayesian Approach Toward Stacked Convolutional
  Independent Component Analysis</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised feature learning algorithms based on convolutional formulations
of independent components analysis (ICA) have been demonstrated to yield
state-of-the-art results in several action recognition benchmarks. However,
existing approaches do not allow for the number of latent components (features)
to be automatically inferred from the data in an unsupervised manner. This is a
significant disadvantage of the state-of-the-art, as it results in considerable
burden imposed on researchers and practitioners, who must resort to tedious
cross-validation procedures to obtain the optimal number of latent features. To
resolve these issues, in this paper we introduce a convolutional nonparametric
Bayesian sparse ICA architecture for overcomplete feature learning from
high-dimensional data. Our method utilizes an Indian buffet process prior to
facilitate inference of the appropriate number of latent features under a
hybrid variational inference algorithm, scalable to massive datasets. As we
show, our model can be naturally used to obtain deep unsupervised hierarchical
feature extractors, by greedily stacking successive model layers, similar to
existing approaches. In addition, inference for this model is completely
heuristics-free; thus, it obviates the need of tedious parameter tuning, which
is a major challenge most deep learning approaches are faced with. We evaluate
our method on several action recognition benchmarks, and exhibit its advantages
over the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4433</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4433</id><created>2014-11-17</created><authors><author><keyname>Bortolussi</keyname><forenames>Luca</forenames></author><author><keyname>Galpin</keyname><forenames>Vashti</forenames></author><author><keyname>Hillston</keyname><forenames>Jane</forenames></author></authors><title>Stochastic HYPE: Flow-based modelling of stochastic hybrid systems</title><categories>cs.SY cs.LO cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic HYPE is a novel process algebra that models stochastic,
instantaneous and continuous behaviour. It develops the flow-based approach of
the hybrid process algebra HYPE by replacing non-urgent events with events with
exponentially-distributed durations and also introduces random resets. The
random resets allow for general stochasticity, and in particular allow for the
use of event durations drawn from distributions other than the exponential
distribution. To account for stochasticity, the semantics of stochastic HYPE
target piecewise deterministic Markov processes (PDMPs), via intermediate
transition-driven stochastic hybrid automata (TDSHA) in contrast to the hybrid
automata used as semantic target for HYPE. Stochastic HYPE models have a
specific structure where the controller of a system is separate from the
continuous aspect of this system providing separation of concerns and
supporting reasoning. A novel equivalence is defined which captures when two
models have the same stochastic behaviour (as in stochastic bisimulation),
instantaneous behaviour (as in classical bisimulation) and continuous
behaviour. These techniques are illustrated via an assembly line example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4435</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4435</id><created>2014-11-17</created><updated>2015-04-28</updated><authors><author><keyname>Casta&#xf1;eda</keyname><forenames>Eduardo</forenames></author><author><keyname>Silva</keyname><forenames>Ad&#xe3;o</forenames></author><author><keyname>Samano-Robles</keyname><forenames>Ramiro</forenames></author><author><keyname>Gameiro</keyname><forenames>Atilio</forenames></author></authors><title>Distributed Linear Precoding and User Selection in Coordinated Multicell
  Systems</title><categories>cs.IT math.IT</categories><comments>12 pages, 6 figures</comments><msc-class>90B40 (Primary), 90B50</msc-class><acm-class>C.2.1; C.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this manuscript we tackle the problem of semi-distributed user selection
with distributed linear precoding for sum rate maximization in multiuser
multicell systems. A set of adjacent base stations (BS) form a cluster in order
to perform coordinated transmission to cell-edge users, and coordination is
carried out through a central processing unit (CU). However, the message
exchange between BSs and the CU is limited to scheduling control signaling and
no user data or channel state information (CSI) exchange is allowed. In the
considered multicell coordinated approach, each BS has its own set of cell-edge
users and transmits only to one intended user while interference to
non-intended users at other BSs is suppressed by signal steering (precoding).
We use two distributed linear precoding schemes, Distributed Zero Forcing (DZF)
and Distributed Virtual Signal-to-Interference-plus-Noise Ratio (DVSINR).
Considering multiple users per cell and the backhaul limitations, the BSs rely
on local CSI to solve the user selection problem. First we investigate how the
signal-to-noise-ratio (SNR) regime and the number of antennas at the BSs affect
the effective channel gain (the magnitude of the channels after precoding) and
its relationship with multiuser diversity. Considering that user selection must
be based on the type of implemented precoding, we develop metrics of
compatibility (estimations of the effective channel gains) that can be computed
from local CSI at each BS and reported to the CU for scheduling decisions.
Based on such metrics, we design user selection algorithms that can find a set
of users that potentially maximizes the sum rate. Numerical results show the
effectiveness of the proposed metrics and algorithms for different
configurations of users and antennas at the base stations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4437</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4437</id><created>2014-11-17</created><authors><author><keyname>Kuriakose</keyname><forenames>Jeril</forenames></author><author><keyname>Amruth</keyname><forenames>V.</forenames></author><author><keyname>Nandhini</keyname><forenames>Swathy</forenames></author><author><keyname>Abhilash</keyname><forenames>V.</forenames></author></authors><title>Sequestration of Malevolent Anchor Nodes in Wireless Sensor Networks
  using Mahalanobis Distance</title><categories>cs.NI cs.CR</categories><comments>9 pages, 9 figures, ICC conference proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discovering the malicious or vulnerable anchor node is an essential problem
in wireless sensor networks (WSNs). In wireless sensor networks, anchor nodes
are the nodes that know its current location. Neighbouring nodes or non-anchor
nodes calculate its location coordinate (or location reference) with the help
of anchor nodes. Ingenuous localization is not possible in the presence of a
cheating anchor node or a cheating node. Nowadays, its a challenging task to
identify the cheating anchor node or cheating node in a network. Even after
finding out the location of the cheating anchor node, there is no assurance,
that the identified node is legitimate or not. This paper aims to localize the
cheating anchor nodes using trilateration algorithm and later associate it with
Mahalanobis distance to obtain maximum accuracy in detecting malicious or
cheating anchor nodes during localization. We were able to attain a
considerable reduction in the error achieved during localization. For
implementation purpose, we simulated our scheme using ns3 network simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4439</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4439</id><created>2014-11-17</created><authors><author><keyname>Kaczmarek</keyname><forenames>O.</forenames></author><author><keyname>Schmidt</keyname><forenames>C.</forenames></author><author><keyname>Steinbrecher</keyname><forenames>P.</forenames></author><author><keyname>Wagner</keyname><forenames>M.</forenames></author></authors><title>Conjugate gradient solvers on Intel Xeon Phi and NVIDIA GPUs</title><categories>physics.comp-ph cs.MS hep-lat</categories><comments>7 pages, proceedings, presented at 'GPU Computing in High Energy
  Physics', September 10-12, 2014, Pisa, Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lattice Quantum Chromodynamics simulations typically spend most of the
runtime in inversions of the Fermion Matrix. This part is therefore frequently
optimized for various HPC architectures. Here we compare the performance of the
Intel Xeon Phi to current Kepler-based NVIDIA Tesla GPUs running a conjugate
gradient solver. By exposing more parallelism to the accelerator through
inverting multiple vectors at the same time, we obtain a performance greater
than 300 GFlop/s on both architectures. This more than doubles the performance
of the inversions. We also give a short overview of the Knights Corner
architecture, discuss some details of the implementation and the effort
required to obtain the achieved performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4449</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4449</id><created>2014-11-17</created><updated>2015-10-16</updated><authors><author><keyname>Bastounis</keyname><forenames>Alexander</forenames></author><author><keyname>Hansen</keyname><forenames>Anders C.</forenames></author></authors><title>On the absence of the RIP in real-world applications of compressed
  sensing and the RIP in levels</title><categories>cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is twofold. The first is to point out that the
Restricted Isometry Property (RIP) does not hold in many applications where
compressed sensing is successfully used. This includes fields like Magnetic
Resonance Imaging (MRI), Computerized Tomography, Electron Microscopy, Radio
Interferometry and Fluorescence Microscopy. We demonstrate that for natural
compressed sensing matrices involving a level based reconstruction basis (e.g.
wavelets), the number of measurements required to recover all $s$-sparse
signals for reasonable $s$ is excessive. In particular, uniform recovery of all
$s$-sparse signals is quite unrealistic. This realisation shows that the RIP is
insufficient for explaining the success of compressed sensing in various
practical applications. The second purpose of the paper is to introduce a new
framework based on a generalised RIP-like definition that fits the applications
where compressed sensing is used. We show that the shortcomings that show that
uniform recovery is unreasonable no longer apply if we instead ask for
structured recovery that is uniform only within each of the levels. To examine
this phenomenon, a new tool, termed the 'Restricted Isometry Property in
Levels' is described and analysed. Furthermore, we show that with certain
conditions on the Restricted Isometry Property in Levels, a form of uniform
recovery within each level is possible. Finally, we conclude the paper by
providing examples that demonstrate the optimality of the results obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4455</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4455</id><created>2014-11-17</created><authors><author><keyname>Fan</keyname><forenames>Miao</forenames></author><author><keyname>Zhao</keyname><forenames>Deli</forenames></author><author><keyname>Zhou</keyname><forenames>Qiang</forenames></author><author><keyname>Liu</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Zheng</keyname><forenames>Thomas Fang</forenames></author><author><keyname>Chang</keyname><forenames>Edward Y.</forenames></author></authors><title>Errata: Distant Supervision for Relation Extraction with Matrix
  Completion</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The essence of distantly supervised relation extraction is that it is an
incomplete multi-label classification problem with sparse and noisy features.
To tackle the sparsity and noise challenges, we propose solving the
classification problem using matrix completion on factorized matrix of
minimized rank. We formulate relation classification as completing the unknown
labels of testing items (entity pairs) in a sparse matrix that concatenates
training and testing textual features with training labels. Our algorithmic
framework is based on the assumption that the rank of item-by-feature and
item-by-label joint matrix is low. We apply two optimization models to recover
the underlying low-rank matrix leveraging the sparsity of feature-label matrix.
The matrix completion problem is then solved by the fixed point continuation
(FPC) algorithm, which can find the global optimum. Experiments on two widely
used datasets with different dimensions of textual features demonstrate that
our low-rank matrix completion approach significantly outperforms the baseline
and the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4464</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4464</id><created>2014-11-17</created><authors><author><keyname>Kang</keyname><forenames>Kai</forenames></author><author><keyname>Wang</keyname><forenames>Xiaogang</forenames></author></authors><title>Fully Convolutional Neural Networks for Crowd Segmentation</title><categories>cs.CV</categories><comments>9 pages,7 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we propose a fast fully convolutional neural network (FCNN)
for crowd segmentation. By replacing the fully connected layers in CNN with 1
by 1 convolution kernels, FCNN takes whole images as inputs and directly
outputs segmentation maps by one pass of forward propagation. It has the
property of translation invariance like patch-by-patch scanning but with much
lower computation cost. Once FCNN is learned, it can process input images of
any sizes without warping them to a standard size. These attractive properties
make it extendable to other general image segmentation problems. Based on FCNN,
a multi-stage deep learning is proposed to integrate appearance and motion cues
for crowd segmentation. Both appearance filters and motion filers are
pretrained stage-by-stage and then jointly optimized. Different combination
methods are investigated. The effectiveness of our approach and component-wise
analysis are evaluated on two crowd segmentation datasets created by us, which
include image frames from 235 and 11 scenes, respectively. They are currently
the largest crowd segmentation datasets and will be released to the public.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4465</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4465</id><created>2014-11-17</created><authors><author><keyname>Papanikos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Papapetrou</keyname><forenames>Evangelos</forenames></author></authors><title>Deterministic Broadcasting and Random Linear Network Coding in Mobile Ad
  Hoc Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>10 pages, 7 figues</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network coding has been successfully used in the past for efficient
broadcasting in wireless multi-hop networks. Two coding approaches are suitable
for mobile networks; Random Linear Network Coding (RLNC) and XOR-based coding.
In this work, we make the observation that RLNC provides increased resilience
to packet losses compared to XOR-based coding. We develop an analytical model
that justifies our intuition. However, the model also reveals that combining
RLNC with probabilistic forwarding, which is the approach taken in the
literature, may significantly impact RLNC's performance. Therefore, we take the
novel approach to combine RLNC with a deterministic broadcasting algorithm in
order to prune transmissions. More specifically, we propose a Connected
Dominating Set (CDS) based algorithm that works in synergy with RLNC on the
&quot;packet generation level&quot;. Since managing packet generations is a key issue in
RLNC, we propose a distributed scheme, which is also suitable for mobile
environments and does not compromise the coding efficiency. We show that the
proposed algorithm outperforms XOR-based as well as RLNC-based schemes even
when global knowledge is used for managing packet generations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4472</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4472</id><created>2014-11-17</created><authors><author><keyname>Gajduk</keyname><forenames>Andrej</forenames></author><author><keyname>Kocarev</keyname><forenames>Ljupco</forenames></author></authors><title>Opinion mining of text documents written in Macedonian language</title><categories>cs.CL</categories><comments>In press, MASA proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to extract public opinion from web portals such as review sites,
social networks and blogs will enable companies and individuals to form a view,
an attitude and make decisions without having to do lengthy and costly
researches and surveys. In this paper machine learning techniques are used for
determining the polarity of forum posts on kajgana which are written in
Macedonian language. The posts are classified as being positive, negative or
neutral. We test different feature metrics and classifiers and provide detailed
evaluation of their participation in improving the overall performance on a
manually generated dataset. By achieving 92% accuracy, we show that the
performance of systems for automated opinion mining is comparable to a human
evaluator, thus making it a viable option for text data analysis. Finally, we
present a few statistics derived from the forum posts using the developed
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4476</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4476</id><created>2014-11-17</created><authors><author><keyname>An</keyname><forenames>Hyung-Chan</forenames></author><author><keyname>Norouzi-Fard</keyname><forenames>Ashkan</forenames></author><author><keyname>Svensson</keyname><forenames>Ola</forenames></author></authors><title>Dynamic Facility Location via Exponential Clocks</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The \emph{dynamic facility location problem} is a generalization of the
classic facility location problem proposed by Eisenstat, Mathieu, and Schabanel
to model the dynamics of evolving social/infrastructure networks. The
generalization lies in that the distance metric between clients and facilities
changes over time. This leads to a trade-off between optimizing the classic
objective function and the &quot;stability&quot; of the solution: there is a switching
cost charged every time a client changes the facility to which it is connected.
While the standard linear program (LP) relaxation for the classic problem
naturally extends to this problem, traditional LP-rounding techniques do not,
as they are often sensitive to small changes in the metric resulting in
frequent switches.
  We present a new LP-rounding algorithm for facility location problems, which
yields the first constant approximation algorithm for the dynamic facility
location problem. Our algorithm installs competing exponential clocks on the
clients and facilities, and connect every client by the path that repeatedly
follows the smallest clock in the neighborhood. The use of exponential clocks
gives rise to several properties that distinguish our approach from previous
LP-roundings for facility location problems. In particular, we use \emph{no
clustering} and we allow clients to connect through paths of \emph{arbitrary
lengths}. In fact, the clustering-free nature of our algorithm is crucial for
applying our LP-rounding approach to the dynamic problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4484</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4484</id><created>2014-11-17</created><updated>2015-07-12</updated><authors><author><keyname>Laufer</keyname><forenames>Paul</forenames></author><author><keyname>Wagner</keyname><forenames>Claudia</forenames></author><author><keyname>Fl&#xf6;ck</keyname><forenames>Fabian</forenames></author><author><keyname>Strohmaier</keyname><forenames>Markus</forenames></author></authors><title>Mining cross-cultural relations from Wikipedia - A study of 31 European
  food cultures</title><categories>cs.SI cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many people, Wikipedia represents one of the primary sources of knowledge
about foreign cultures. Yet, different Wikipedia language editions offer
different descriptions of cultural practices. Unveiling diverging
representations of cultures provides an important insight, since they may
foster the formation of cross-cultural stereotypes, misunderstandings and
potentially even conflict. In this work, we explore to what extent the
descriptions of cultural practices in various European language editions of
Wikipedia differ on the example of culinary practices and propose an approach
to mine cultural relations between different language communities trough their
description of and interest in their own and other communities' food culture.
We assess the validity of the extracted relations using 1) various external
reference data sources (i.e., the European Social Survey, migration
statistics), 2) crowdsourcing methods and 3) simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4491</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4491</id><created>2014-11-17</created><updated>2015-04-28</updated><authors><author><keyname>Fernando</keyname><forenames>Basura</forenames></author><author><keyname>Tommasi</keyname><forenames>Tatiana</forenames></author><author><keyname>Tuytelaars</keyname><forenames>Tinne</forenames></author></authors><title>Joint cross-domain classification and subspace learning for unsupervised
  adaptation</title><categories>cs.CV cs.LG</categories><comments>Paper is under consideration at Pattern Recognition Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain adaptation aims at adapting the knowledge acquired on a source domain
to a new different but related target domain. Several approaches have
beenproposed for classification tasks in the unsupervised scenario, where no
labeled target data are available. Most of the attention has been dedicated to
searching a new domain-invariant representation, leaving the definition of the
prediction function to a second stage. Here we propose to learn both jointly.
Specifically we learn the source subspace that best matches the target subspace
while at the same time minimizing a regularized misclassification loss. We
provide an alternating optimization technique based on stochastic sub-gradient
descent to solve the learning problem and we demonstrate its performance on
several domain adaptation tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4495</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4495</id><created>2014-11-17</created><authors><author><keyname>Knapp</keyname><forenames>Alexander</forenames></author><author><keyname>Mossakowski</keyname><forenames>Till</forenames></author><author><keyname>Roggenbach</keyname><forenames>Markus</forenames></author><author><keyname>Glauer</keyname><forenames>Martin</forenames></author></authors><title>An Institution for Simple UML State Machines</title><categories>cs.SE cs.LO</categories><comments>24 pages. arXiv admin note: substantial text overlap with
  arXiv:1403.7747</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an institution for UML state machines without hierarchical states.
The interaction with UML class diagrams is handled via institutions for guards
and actions, which provide dynamic components of states (such as valuations of
attributes) but abstract away from details of class diagrams. We also study a
notion of interleaving product, which captures the interaction of several state
machines. The interleaving product construction is the basis for a semantics of
composite structure diagrams, which can be used to specify the interaction of
state machines. This work is part of a larger effort to build a framework for
formal software development with UML, based on a heterogeneous approach using
institutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4498</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4498</id><created>2014-11-17</created><updated>2015-08-04</updated><authors><author><keyname>Chlebus</keyname><forenames>Bogdan S.</forenames></author><author><keyname>De Marco</keyname><forenames>Gianluca</forenames></author><author><keyname>Kowalski</keyname><forenames>Dariusz R.</forenames></author></authors><title>Scalable Wake-up of Multi-Channel Single-Hop Radio Networks</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider single-hop radio networks with multiple channels as a model of
wireless networks. There are $n$ stations connected to $b$ radio channels that
do not provide collision detection. A station uses all the channels
concurrently and independently. Some $k$ stations may become active
spontaneously at arbitrary times. The goal is to wake up the network, which
occurs when all the stations hear a successful transmission on some channel.
Duration of a waking-up execution is measured starting from the first
spontaneous activation. We present a deterministic algorithm for the general
problem that wakes up the network in $O(k\log^{1/b} k\log n)$ time, where $k$
is unknown. We give a deterministic scalable algorithm for the special case
when $b&gt;d \log \log n$, for some constant $d&gt;1$, which wakes up the network in
$O(\frac{k}{b}\log n\log(b\log n))$ time, with $k$ unknown. This algorithm
misses time optimality by at most a factor of $O(\log n(\log b +\log\log n))$,
because any deterministic algorithm requires $\Omega(\frac{k}{b}\log
\frac{n}{k})$ time. We give a randomized algorithm that wakes up the network
within $O(k^{1/b}\ln \frac{1}{\epsilon})$ rounds with a probability that is at
least $1-\epsilon$, for any $0&lt;\epsilon&lt;1$, where $k$ is known. We also
consider a model of jamming, in which each channel in any round may be jammed
to prevent a successful transmission, which happens with some known parameter
probability $p$, independently across all channels and rounds. For this model,
we give two deterministic algorithms for unknown~$k$: one wakes up the network
in time $O(\log^{-1}(\frac{1}{p})\, k\log n\log^{1/b} k)$, and the other in
time $O(\log^{-1}(\frac{1}{p}) \, \frac{k}{b} \log n\log(b\log n))$ but
assuming the inequality $b&gt;\log(128b\log n)$, both with a probability that is
at least $1-1/\mbox{poly}(n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4503</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4503</id><created>2014-11-17</created><updated>2014-11-18</updated><authors><author><keyname>Katz</keyname><forenames>Itamar</forenames></author><author><keyname>Crammer</keyname><forenames>Koby</forenames></author></authors><title>Outlier-Robust Convex Segmentation</title><categories>cs.LG stat.ML</categories><comments>* Accepted to AAAI-15, this version includes the
  appendix/supplementary material referenced in the AAAI-15 submission, as well
  as color figures * This version include some minor typos correction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a convex optimization problem for the task of segmenting sequential
data, which explicitly treats presence of outliers. We describe two algorithms
for solving this problem, one exact and one a top-down novel approach, and we
derive a consistency results for the case of two segments and no outliers.
Robustness to outliers is evaluated on two real-world tasks related to speech
segmentation. Our algorithms outperform baseline segmentation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4510</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4510</id><created>2014-11-17</created><authors><author><keyname>Low</keyname><forenames>Kian Hsiang</forenames></author><author><keyname>Yu</keyname><forenames>Jiangbo</forenames></author><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Jaillet</keyname><forenames>Patrick</forenames></author></authors><title>Parallel Gaussian Process Regression for Big Data: Low-Rank
  Representation Meets Markov Approximation</title><categories>stat.ML cs.DC cs.LG</categories><comments>29th AAAI Conference on Artificial Intelligence (AAAI 2015), Extended
  version with proofs, 10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The expressive power of a Gaussian process (GP) model comes at a cost of poor
scalability in the data size. To improve its scalability, this paper presents a
low-rank-cum-Markov approximation (LMA) of the GP model that is novel in
leveraging the dual computational advantages stemming from complementing a
low-rank approximate representation of the full-rank GP based on a support set
of inputs with a Markov approximation of the resulting residual process; the
latter approximation is guaranteed to be closest in the Kullback-Leibler
distance criterion subject to some constraint and is considerably more refined
than that of existing sparse GP models utilizing low-rank representations due
to its more relaxed conditional independence assumption (especially with larger
data). As a result, our LMA method can trade off between the size of the
support set and the order of the Markov property to (a) incur lower
computational cost than such sparse GP models while achieving predictive
performance comparable to them and (b) accurately represent features/patterns
of any scale. Interestingly, varying the Markov order produces a spectrum of
LMAs with PIC approximation and full-rank GP at the two extremes. An advantage
of our LMA method is that it is amenable to parallelization on multiple
machines/cores, thereby gaining greater scalability. Empirical evaluation on
three real-world datasets in clusters of up to 32 computing nodes shows that
our centralized and parallel LMA methods are significantly more time-efficient
and scalable than state-of-the-art sparse and full-rank GP regression methods
while achieving comparable predictive performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4516</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4516</id><created>2014-11-17</created><authors><author><keyname>Calvanese</keyname><forenames>Diego</forenames></author><author><keyname>Delzanno</keyname><forenames>Giorgio</forenames></author><author><keyname>Montali</keyname><forenames>Marco</forenames></author></authors><title>Verification of Relational Multiagent Systems with Data Types (Extended
  Version)</title><categories>cs.AI cs.DB cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the extension of relational multiagent systems (RMASs), where agents
manipulate full-fledged relational databases, with data types and facets
equipped with domain-specific, rigid relations (such as total orders).
Specifically, we focus on design-time verification of RMASs against rich
first-order temporal properties expressed in a variant of first-order
mu-calculus with quantification across states. We build on previous
decidability results under the &quot;state-bounded&quot; assumption, i.e., in each single
state only a bounded number of data objects is stored in the agent databases,
while unboundedly many can be encountered over time. We recast this condition,
showing decidability in presence of dense, linear orders, and facets defined on
top of them. Our approach is based on the construction of a finite-state, sound
and complete abstraction of the original system, in which dense linear orders
are reformulated as non-rigid relations working on the active domain of the
system only. We also show undecidability when including a data type equipped
with the successor relation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4521</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4521</id><created>2014-11-17</created><authors><author><keyname>Krijthe</keyname><forenames>Jesse H.</forenames></author><author><keyname>Loog</keyname><forenames>Marco</forenames></author></authors><title>Implicitly Constrained Semi-Supervised Linear Discriminant Analysis</title><categories>stat.ML cs.LG</categories><comments>6 pages, 3 figures and 3 tables. International Conference on Pattern
  Recognition (ICPR) 2014, Stockholm, Sweden</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semi-supervised learning is an important and active topic of research in
pattern recognition. For classification using linear discriminant analysis
specifically, several semi-supervised variants have been proposed. Using any
one of these methods is not guaranteed to outperform the supervised classifier
which does not take the additional unlabeled data into account. In this work we
compare traditional Expectation Maximization type approaches for
semi-supervised linear discriminant analysis with approaches based on intrinsic
constraints and propose a new principled approach for semi-supervised linear
discriminant analysis, using so-called implicit constraints. We explore the
relationships between these methods and consider the question if and in what
sense we can expect improvement in performance over the supervised procedure.
The constraint based approaches are more robust to misspecification of the
model, and may outperform alternatives that make more assumptions on the data,
in terms of the log-likelihood of unseen objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4555</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4555</id><created>2014-11-17</created><updated>2015-04-20</updated><authors><author><keyname>Vinyals</keyname><forenames>Oriol</forenames></author><author><keyname>Toshev</keyname><forenames>Alexander</forenames></author><author><keyname>Bengio</keyname><forenames>Samy</forenames></author><author><keyname>Erhan</keyname><forenames>Dumitru</forenames></author></authors><title>Show and Tell: A Neural Image Caption Generator</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatically describing the content of an image is a fundamental problem in
artificial intelligence that connects computer vision and natural language
processing. In this paper, we present a generative model based on a deep
recurrent architecture that combines recent advances in computer vision and
machine translation and that can be used to generate natural sentences
describing an image. The model is trained to maximize the likelihood of the
target description sentence given the training image. Experiments on several
datasets show the accuracy of the model and the fluency of the language it
learns solely from image descriptions. Our model is often quite accurate, which
we verify both qualitatively and quantitatively. For instance, while the
current state-of-the-art BLEU-1 score (the higher the better) on the Pascal
dataset is 25, our approach yields 59, to be compared to human performance
around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66,
and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we
achieve a BLEU-4 of 27.7, which is the current state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4565</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4565</id><created>2014-11-17</created><authors><author><keyname>Chandu</keyname><forenames>Drona Pratap</forenames></author></authors><title>A Parallel Genetic Algorithm for Three Dimensional Bin Packing with
  Heterogeneous Bins</title><categories>cs.DC cs.NE</categories><comments>6 pages, 4 figures</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V17(1):33-38, Nov 2014</journal-ref><doi>10.14445/22312803/IJCTT-V17P108</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a parallel genetic algorithm for three dimensional bin
packing with heterogeneous bins using Hadoop Map-Reduce framework. The most
common three dimensional bin packing problem which packs given set of boxes
into minimum number of equal sized bins is proven to be NP Hard. The variation
of three dimensional bin packing problem that allows heterogeneous bin sizes
and rotation of boxes is computationally more harder than common three
dimensional bin packing problem. The proposed Map-Reduce implementation helps
to run the genetic algorithm for three dimensional bin packing with
heterogeneous bins on multiple machines parallely and computes the solution in
relatively short time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4568</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4568</id><created>2014-11-17</created><updated>2015-03-12</updated><authors><author><keyname>Verdie</keyname><forenames>Yannick</forenames></author><author><keyname>Yi</keyname><forenames>Kwang Moo</forenames></author><author><keyname>Fua</keyname><forenames>Pascal</forenames></author><author><keyname>Lepetit</keyname><forenames>Vincent</forenames></author></authors><title>TILDE: A Temporally Invariant Learned DEtector</title><categories>cs.CV</categories><doi>10.1109/CVPR.2015.7299165</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a learning-based approach to detect repeatable keypoints under
drastic imaging changes of weather and lighting conditions to which
state-of-the-art keypoint detectors are surprisingly sensitive. We first
identify good keypoint candidates in multiple training images taken from the
same viewpoint. We then train a regressor to predict a score map whose maxima
are those points so that they can be found by simple non-maximum suppression.
As there are no standard datasets to test the influence of these kinds of
changes, we created our own, which we will make publicly available. We will
show that our method significantly outperforms the state-of-the-art methods in
such challenging conditions, while still achieving state-of-the-art performance
on the untrained standard Oxford dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4573</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4573</id><created>2014-11-17</created><authors><author><keyname>Post</keyname><forenames>Ian</forenames></author><author><keyname>Swamy</keyname><forenames>Chaitanya</forenames></author></authors><title>Linear-Programming based Approximation Algorithms for Multi-Vehicle
  Minimum Latency Problems</title><categories>cs.DS</categories><acm-class>F.2.2; G.1.6; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider various {\em multi-vehicle versions of the minimum latency
problem}. There is a fleet of $k$ vehicles located at one or more depot nodes,
and we seek a collection of routes for these vehicles that visit all nodes so
as to minimize the total latency incurred, which is the sum of the client
waiting times. We obtain an $8.497$-approximation for the version where
vehicles may be located at multiple depots and a $7.183$-approximation for the
version where all vehicles are located at the same depot, both of which are the
first improvements on this problem in a decade. Perhaps more significantly, our
algorithms exploit various LP-relaxations for minimum-latency problems. We show
how to effectively leverage two classes of LPs---{\em configuration LPs} and
{\em bidirected LP-relaxations}---that are often believed to be quite powerful
but have only sporadically been effectively leveraged for network-design and
vehicle-routing problems. This gives the first concrete evidence of the
effectiveness of LP-relaxations for this class of problems. The
$8.497$-approximation the multiple-depot version is obtained by rounding a
near-optimal solution to an underlying configuration LP for the problem. The
$7.183$-approximation can be obtained both via rounding a bidirected LP for the
single-depot problem or via more combinatorial means. The latter approach uses
a bidirected LP to obtain the following key result that is of independent
interest: for any $k$, we can efficiently compute a rooted tree that is at
least as good, with respect to the prize-collecting objective (i.e., edge cost
+ number of uncovered nodes) as the best collection of $k$ rooted paths. Our
algorithms are versatile and extend easily to handle various extensions
involving: (i) weighted sum of latencies, (ii) constraints specifying which
depots may serve which nodes, (iii) node service times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4575</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4575</id><created>2014-11-17</created><updated>2015-09-18</updated><authors><author><keyname>Drange</keyname><forenames>P&#xe5;l Gr&#xf8;n&#xe5;s</forenames></author><author><keyname>Dregi</keyname><forenames>Markus S.</forenames></author><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Kreutzer</keyname><forenames>Stephan</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Reidl</keyname><forenames>Felix</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author><author><keyname>Villaamil</keyname><forenames>Fernando S&#xe1;nchez</forenames></author><author><keyname>Siebertz</keyname><forenames>Sebastian</forenames></author><author><keyname>Sikdar</keyname><forenames>Somnath</forenames></author></authors><title>Kernelization and Sparseness: the case of Dominating Set</title><categories>cs.DS</categories><comments>v2: new author, added results for r-Dominating Sets in bounded
  expansion graphs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that for every positive integer $r$ and for every graph class
$\mathcal G$ of bounded expansion, the $r$-Dominating Set problem admits a
linear kernel on graphs from $\mathcal G$. Moreover, when $\mathcal G$ is only
assumed to be nowhere dense, then we give an almost linear kernel on $\mathcal
G$ for the classic Dominating Set problem, i.e., for the case $r=1$. These
results generalize a line of previous research on finding linear kernels for
Dominating Set and $r$-Dominating Set. However, the approach taken in this
work, which is based on the theory of sparse graphs, is radically different and
conceptually much simpler than the previous approaches.
  We complement our findings by showing that for the closely related Connected
Dominating Set problem, the existence of such kernelization algorithms is
unlikely, even though the problem is known to admit a linear kernel on
$H$-topological-minor-free graphs. Also, we prove that for any somewhere dense
class $\mathcal G$, there is some $r$ for which $r$-Dominating Set is
W[$2$]-hard on $\mathcal G$. Thus, our results fall short of proving a sharp
dichotomy for the parameterized complexity of $r$-Dominating Set on
subgraph-monotone graph classes: we conjecture that the border of tractability
lies exactly between nowhere dense and somewhere dense graph classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4584</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4584</id><created>2014-11-17</created><authors><author><keyname>Gopalan</keyname><forenames>Parikshit</forenames></author><author><keyname>Kane</keyname><forenames>Daniel</forenames></author><author><keyname>Meka</keyname><forenames>Raghu</forenames></author></authors><title>Pseudorandomness for concentration bounds and signed majorities</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of constructing pseudorandom generators that fool halfspaces has
been studied intensively in recent times. For fooling halfspaces over the
hypercube with polynomially small error, the best construction known requires
seed-length O(log^2 n) (MekaZ13). Getting the seed-length down to O(log(n)) is
a natural challenge in its own right, which needs to be overcome in order to
derandomize RL. In this work we make progress towards this goal by obtaining
near-optimal generators for two important special cases:
  1) We give a near optimal derandomization of the Chernoff bound for
independent, uniformly random bits. Specifically, we show how to generate a x
in {1,-1}^n using $\tilde{O}(\log (n/\epsilon))$ random bits such that for any
unit vector u, &lt;u,x&gt; matches the sub-Gaussian tail behaviour predicted by the
Chernoff bound up to error eps.
  2) We construct a generator which fools halfspaces with {0,1,-1} coefficients
with error eps with a seed-length of $\tilde{O}(\log(n/\epsilon))$. This
includes the important special case of majorities.
  In both cases, the best previous results required seed-length of $O(\log n +
\log^2(1/\epsilon))$.
  Technically, our work combines new Fourier-analytic tools with the iterative
dimension reduction techniques and the gradually increasing independence
paradigm of previous works (KaneMN11, CelisRSW13, GopalanMRTV12).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4586</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4586</id><created>2014-11-17</created><authors><author><keyname>Hanebeck</keyname><forenames>Uwe D.</forenames></author></authors><title>Optimal Reduction of Multivariate Dirac Mixture Densities</title><categories>cs.SY</categories><comments>18 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the optimal approximation of a given
multivariate Dirac mixture, i.e., a density comprising weighted Dirac
distributions on a continuous domain, by an equally weighted Dirac mixture with
a reduced number of components. The parameters of the approximating density are
calculated by minimizing a smooth global distance measure, a generalization of
the well-known Cram\'{e}r-von Mises Distance to the multivariate case. This
generalization is achieved by defining an alternative to the classical
cumulative distribution, the Localized Cumulative Distribution (LCD), as a
characterization of discrete random quantities (on continuous domains), which
is unique and symmetric also in the multivariate case. The resulting
approximation method provides the basis for various efficient nonlinear state
and parameter estimation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4590</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4590</id><created>2014-11-17</created><authors><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author><author><keyname>Shpilka</keyname><forenames>Amir</forenames></author><author><keyname>Wigderson</keyname><forenames>Avi</forenames></author></authors><title>Reed-Muller codes for random erasures and errors</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the parameters for which Reed-Muller (RM) codes over
$GF(2)$ can correct random erasures and random errors with high probability,
and in particular when can they achieve capacity for these two classical
channels. Necessarily, the paper also studies properties of evaluations of
multi-variate $GF(2)$ polynomials on random sets of inputs.
  For erasures, we prove that RM codes achieve capacity both for very high rate
and very low rate regimes. For errors, we prove that RM codes achieve capacity
for very low rate regimes, and for very high rates, we show that they can
uniquely decode at about square root of the number of errors at capacity.
  The proofs of these four results are based on different techniques, which we
find interesting in their own right. In particular, we study the following
questions about $E(m,r)$, the matrix whose rows are truth tables of all
monomials of degree $\leq r$ in $m$ variables. What is the most (resp. least)
number of random columns in $E(m,r)$ that define a submatrix having full column
rank (resp. full row rank) with high probability? We obtain tight bounds for
very small (resp. very large) degrees $r$, which we use to show that RM codes
achieve capacity for erasures in these regimes.
  Our decoding from random errors follows from the following novel reduction.
For every linear code $C$ of sufficiently high rate we construct a new code
$C'$, also of very high rate, such that for every subset $S$ of coordinates, if
$C$ can recover from erasures in $S$, then $C'$ can recover from errors in $S$.
Specializing this to RM codes and using our results for erasures imply our
result on unique decoding of RM codes at high rate.
  Finally, two of our capacity achieving results require tight bounds on the
weight distribution of RM codes. We obtain such bounds extending the recent
\cite{KLP} bounds from constant degree to linear degree polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4591</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4591</id><created>2014-11-17</created><updated>2015-01-17</updated><authors><author><keyname>Vehkalahti</keyname><forenames>Roope</forenames></author><author><keyname>Luzzi</keyname><forenames>Laura</forenames></author></authors><title>Number field lattices achieve Gaussian and Rayleigh channel capacity
  within a constant gap</title><categories>cs.IT math.IT math.NT</categories><comments>Will be submitted to ISIT. Comments, suggestions for references etc.
  are warmly welcome. Edit:Appendix added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proves that a family of number field lattice codes simultaneously
achieves a constant gap to capacity in Rayleigh fast fading and Gaussian
channels.
  The key property in the proof is the existence of infinite towers of Hilbert
class fields with bounded root discriminant. The gap to capacity of the
proposed families is determined by the root discriminant.
  The comparison between the Gaussian and fading case reveals that in Rayleigh
fading channels the normalized minimum product distance plays an analogous role
to the Hermite invariant in Gaussian channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4597</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4597</id><created>2014-11-14</created><updated>2015-06-08</updated><authors><author><keyname>Corradini</keyname><forenames>Anadrea</forenames></author><author><keyname>Duval</keyname><forenames>Dominique</forenames></author><author><keyname>Echahed</keyname><forenames>Rachid</forenames></author><author><keyname>Prost</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Ribeiro</keyname><forenames>Leila</forenames></author></authors><title>AGREE -- Algebraic Graph Rewriting with Controlled Embedding (Long
  Version)</title><categories>cs.LO</categories><msc-class>68Q42</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The several algebraic approaches to graph transformation proposed in the
literature all ensure that if an item is preserved by a rule, so are its
connections with the context graph where it is embedded. But there are
applications in which it is desirable, for example when cloning an item, to
specify different embeddings for the original and for the copy. Therefore we
propose a conservative extension of these approaches where a rule can specify
how the embedding of a preserved item should be changed, typically by removing
certain connections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4604</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4604</id><created>2014-11-17</created><authors><author><keyname>Bloem</keyname><forenames>Roderick</forenames></author><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Jacobs</keyname><forenames>Swen</forenames></author><author><keyname>Koenighofer</keyname><forenames>Robert</forenames></author></authors><title>Assume-Guarantee Synthesis for Concurrent Reactive Programs with Partial
  Information</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synthesis of program parts is very useful for concurrent systems. However,
most synthesis approaches do not support common design tasks, like modifying a
single process without having to re-synthesize or verify the whole system.
Assume-guarantee synthesis (AGS) provides robustness against modifications of
system parts, but thus far has been limited to the perfect information setting.
This means that local variables cannot be hidden from other processes, which
renders synthesis results cumbersome or even impossible to realize. We resolve
this shortcoming by defining AGS in a partial information setting. We analyze
the complexity and decidability in different settings, showing that the problem
has a high worst-case complexity and is undecidable in many interesting cases.
Based on these observations, we present a pragmatic algorithm based on bounded
synthesis, and demonstrate its practical applicability on several examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4613</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4613</id><created>2014-11-17</created><updated>2015-09-01</updated><authors><author><keyname>Anari</keyname><forenames>Nima</forenames></author><author><keyname>Gharan</keyname><forenames>Shayan Oveis</forenames></author></authors><title>Effective-Resistance-Reducing Flows, Spectrally Thin Trees, and
  Asymmetric TSP</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the integrality gap of the natural LP relaxation of the
Asymmetric Traveling Salesman Problem is $\text{polyloglog}(n)$. In other
words, there is a polynomial time algorithm that approximates the value of the
optimum tour within a factor of $\text{polyloglog}(n)$, where
$\text{polyloglog}(n)$ is a bounded degree polynomial of $\log\log(n)$. We
prove this by showing that any $k$-edge-connected unweighted graph has a
$\text{polyloglog}(n)/k$-thin spanning tree.
  Our main new ingredient is a procedure, albeit an exponentially sized convex
program, that &quot;transforms&quot; graphs that do not admit any spectrally thin trees
into those that provably have spectrally thin trees. More precisely, given a
$k$-edge-connected graph $G=(V,E)$ where $k\geq 7\log(n)$, we show that there
is a matrix $D$ that &quot;preserves&quot; the structure of all cuts of $G$ such that for
a set $F\subseteq E$ that induces an $\Omega(k)$-edge-connected graph, the
effective resistance of every edge in $F$ w.r.t. $D$ is at most
$\text{polylog}(k)/k$. Then, we use a recent extension of the seminal work of
Marcus, Spielman, and Srivastava [MSS13] by the authors [AO14] to prove the
existence of a $\text{polylog}(k)/k$-spectrally thin tree with respect to $D$.
Such a tree is $\text{polylog}(k)/k$-combinatorially thin with respect to $G$
as $D$ preserves the structure of cuts of $G$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4614</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4614</id><created>2014-09-26</created><authors><author><keyname>Vaillant</keyname><forenames>Pascal</forenames></author><author><keyname>Lamy</keyname><forenames>Jean-Baptiste</forenames></author></authors><title>Using graph transformation algorithms to generate natural language
  equivalents of icons expressing medical concepts</title><categories>cs.CL</categories><comments>Presented at the TSD 2014 conference: Text, Speech and Dialogue, 17th
  international conference. Brno, Czech Republic, September 8-12, 2014. 10
  pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graphical language addresses the need to communicate medical information in
a synthetic way. Medical concepts are expressed by icons conveying fast visual
information about patients' current state or about the known effects of drugs.
In order to increase the visual language's acceptance and usability, a natural
language generation interface is currently developed. In this context, this
paper describes the use of an informatics method ---graph transformation--- to
prepare data consisting of concepts in an OWL-DL ontology for use in a natural
language generation component. The OWL concept may be considered as a
star-shaped graph with a central node. The method transforms it into a graph
representing the deep semantic structure of a natural language phrase. This
work may be of future use in other contexts where ontology concepts have to be
mapped to half-formalized natural language expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4616</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4616</id><created>2014-11-17</created><authors><author><keyname>Lig&#x119;za</keyname><forenames>Antoni</forenames></author></authors><title>A Note on Systematic Conflict Generation in CA-EN-type Causal Structures</title><categories>cs.AI</categories><comments>This report is available form LAAS - Toulouse, France, from 1996.
  Report No.: 96317 http://www.laas.fr/pulman/pulman-isens/web/app.php/</comments><report-no>LAAS Report No. 96317, 22 pp. (1996)</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is aimed at providing a very first, more &quot;global&quot;, systematic
point of view with respect to possible conflict generation in CA-EN-like causal
structures. For simplicity, only the outermost level of graphs is taken into
account. Localization of the &quot;conflict area&quot;, diagnostic preferences, and bases
for systematic conflict generation are considered. A notion of {\em Potential
Conflict Structure} ({\em PCS}) constituting a basic tool for identification of
possible conflicts is proposed and its use is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4617</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4617</id><created>2014-11-17</created><authors><author><keyname>Ma</keyname><forenames>Xudong</forenames></author></authors><title>Joint Write-Once-Memory and Error-Control Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Write-Once-Memory (WOM) is a model for many modern non-volatile memories,
such as flash memories. Recently, several capacity-achieving WOM coding schemes
have been proposed based on polar coding. Due to the fact that practical
computer memory systems always contain noises, a nature question to ask next is
how may we generalize these coding schemes, such that they may also have the
error-control capabilities. In this paper, we discuss a joint WOM and
error-control coding scheme, which is a generalization of the
capacity-achieving WOM codes based on source polarization. In this paper, we
prove a sufficient and necessary condition for the noisy reading channel being
less noisy than the test channel in data encoding in the polar WOM coding. Such
a sufficient and necessary condition is usually satisfied in reality. As a
consequence of the sufficient and necessary condition, the high entropy set
related to the noisy channel is usually strictly contained in the high entropy
set related to the test channel in data encoding. Therefore the low-complexity
polar joint WOM and error-control codes are sufficient for most practical
coding scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4618</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4618</id><created>2014-11-17</created><authors><author><keyname>Burges</keyname><forenames>Christopher J. C.</forenames></author><author><keyname>Renshaw</keyname><forenames>Erin</forenames></author><author><keyname>Pastusiak</keyname><forenames>Andrzej</forenames></author></authors><title>Relations World: A Possibilistic Graphical Model</title><categories>cs.CL cs.AI</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the idea of using a &quot;possibilistic graphical model&quot; as the basis
for a world model that drives a dialog system. As a first step we have
developed a system that uses text-based dialog to derive a model of the user's
family relations. The system leverages its world model to infer relational
triples, to learn to recover from upstream coreference resolution errors and
ambiguities, and to learn context-dependent paraphrase models. We also explore
some theoretical aspects of the underlying graphical model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4619</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4619</id><created>2014-11-17</created><authors><author><keyname>Caragiannis</keyname><forenames>Ioannis</forenames></author><author><keyname>Krimpas</keyname><forenames>George A.</forenames></author><author><keyname>Voudouris</keyname><forenames>Alexandros A.</forenames></author></authors><title>Aggregating partial rankings with applications to peer grading in
  massive online open courses</title><categories>cs.AI cs.DS cs.MA</categories><comments>16 pages, 3 figures, 2 tables</comments><acm-class>F.2; I.2.11; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the potential of using ordinal peer grading for the evaluation
of students in massive online open courses (MOOCs). According to such grading
schemes, each student receives a few assignments (by other students) which she
has to rank. Then, a global ranking (possibly translated into numerical scores)
is produced by combining the individual ones. This is a novel application area
for social choice concepts and methods where the important problem to be solved
is as follows: how should the assignments be distributed so that the collected
individual rankings can be easily merged into a global one that is as close as
possible to the ranking that represents the relative performance of the
students in the assignment? Our main theoretical result suggests that using
very simple ways to distribute the assignments so that each student has to rank
only $k$ of them, a Borda-like aggregation method can recover a $1-O(1/k)$
fraction of the true ranking when each student correctly ranks the assignments
she receives. Experimental results strengthen our analysis further and also
demonstrate that the same method is extremely robust even when students have
imperfect capabilities as graders. We believe that our results provide strong
evidence that ordinal peer grading can be a highly effective and scalable
solution for evaluation in MOOCs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4630</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4630</id><created>2014-10-18</created><authors><author><keyname>Rizopoulos</keyname><forenames>Antonis S.</forenames></author><author><keyname>Kallergis</keyname><forenames>Dimitrios N.</forenames></author><author><keyname>Prezerakos</keyname><forenames>George N.</forenames></author></authors><title>Security Evaluation for Mail Distribution Systems</title><categories>cs.CR cs.CY</categories><comments>6 pages, eRA 5th International Scientific Conference, September 15-18
  2010, Piraeus, Greece</comments><msc-class>68M12</msc-class><acm-class>C.2.0; C.2.2; D.4.6; H.3.4</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The security evaluation for Mail Distribution Systems focuses on
certification and reliability of sensitive data between mail servers. The need
to certify the information conveyed is a result of known weaknesses in the
simple mail transfer protocol (SMTP). The most important consequence of these
weaknesses is the possibility to mislead the recipient, which is achieved via
spam (especially email spoofing). Email spoofing refers to alterations in the
headers and/or the content of the message. Therefore, the authenticity of the
message is compromised. Unfortunately, the broken link between certification
and reliability of the information is unsolicited email (spam).
  Unlike the current practice of estimating the cost of spam, which prompts
organizations to purchase and maintain appropriate anti-spam software, our
approach offers an alternative perspective of the economic and moral
consequences of unsolicited mail. The financial data provided in this paper
show that spam is a major contributor to the financial and production cost of
an organization, necessitating further attention. Additionally, this paper
highlights the importance and severity of the weaknesses of the SMTP protocol,
which can be exploited even with the use of simple applications incorporated
within most commonly used Operating Systems (e.g. Telnet).
  As a consequence of these drawbacks Mail Distribution Systems need to be
appropriate configured so as to provide the necessary security services to the
users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4670</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4670</id><created>2014-11-17</created><authors><author><keyname>Hussein</keyname><forenames>Mohamed E.</forenames></author><author><keyname>Torki</keyname><forenames>Marwan</forenames></author><author><keyname>Elsallamy</keyname><forenames>Ahmed</forenames></author><author><keyname>Fayyaz</keyname><forenames>Mahmoud</forenames></author></authors><title>AlexU-Word: A New Dataset for Isolated-Word Closed-Vocabulary Offline
  Arabic Handwriting Recognition</title><categories>cs.CV</categories><comments>6 pages, 8 figure, and 6 tables</comments><acm-class>I.5.2; I.7.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce the first phase of a new dataset for offline
Arabic handwriting recognition. The aim is to collect a very large dataset of
isolated Arabic words that covers all letters of the alphabet in all possible
shapes using a small number of simple words. The end goal is to collect a very
large dataset of segmented letter images, which can be used to build and
evaluate Arabic handwriting recognition systems that are based on segmented
letter recognition. The current version of the dataset contains $25114$ samples
of $109$ unique Arabic words that cover all possible shapes of all alphabet
letters. The samples were collected from $907$ writers. In its current form,
the dataset can be used for the problem of closed-vocabulary word recognition.
We evaluated a number of window-based descriptors and classifiers on this task
and obtained an accuracy of $92.16\%$ using a SIFT-based descriptor and ANN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4679</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4679</id><created>2014-11-17</created><authors><author><keyname>Paudel</keyname><forenames>S.</forenames></author><author><keyname>Elmtiri</keyname><forenames>M.</forenames></author><author><keyname>Kling</keyname><forenames>W. L.</forenames></author><author><keyname>Corre</keyname><forenames>O. Le</forenames></author><author><keyname>Lacarriere</keyname><forenames>B.</forenames></author></authors><title>Pseudo Dynamic Transitional Modeling of Building Heating Energy Demand
  Using Artificial Neural Network</title><categories>cs.CE cs.NE</categories><doi>10.1016/j.enbuild.2013.11.051</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the building heating demand prediction model with
occupancy profile and operational heating power level characteristics in short
time horizon (a couple of days) using artificial neural network. In addition,
novel pseudo dynamic transitional model is introduced, which consider time
dependent attributes of operational power level characteristics and its effect
in the overall model performance is outlined. Pseudo dynamic model is applied
to a case study of French Institution building and compared its results with
static and other pseudo dynamic neural network models. The results show the
coefficients of correlation in static and pseudo dynamic neural network model
of 0.82 and 0.89 (with energy consumption error of 0.02%) during the learning
phase, and 0.61 and 0.85 during the prediction phase respectively. Further,
orthogonal array design is applied to the pseudo dynamic model to check the
schedule of occupancy profile and operational heating power level
characteristics. The results show the new schedule and provide the robust
design for pseudo dynamic model. Due to prediction in short time horizon, it
finds application for Energy Services Company (ESCOs) to manage the heating
load for dynamic control of heat production system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4686</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4686</id><created>2014-11-17</created><updated>2015-08-21</updated><authors><author><keyname>Gu&#xe9;don</keyname><forenames>Olivier</forenames></author><author><keyname>Vershynin</keyname><forenames>Roman</forenames></author></authors><title>Community detection in sparse networks via Grothendieck's inequality</title><categories>math.ST cs.SI stat.TH</categories><comments>This is the final version, incorporating the referee's comments</comments><msc-class>62H30, 05C85, 60B20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple and flexible method to prove consistency of semidefinite
optimization problems on random graphs. The method is based on Grothendieck's
inequality. Unlike the previous uses of this inequality that lead to constant
relative accuracy, we achieve any given relative accuracy by leveraging
randomness. We illustrate the method with the problem of community detection in
sparse networks, those with bounded average degrees. We demonstrate that even
in this regime, various simple and natural semidefinite programs can be used to
recover the community structure up to an arbitrarily small fraction of
misclassified vertices. The method is general; it can be applied to a variety
of stochastic models of networks and semidefinite programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4692</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4692</id><created>2014-11-17</created><authors><author><keyname>Haviv</keyname><forenames>Ishay</forenames></author><author><keyname>Xie</keyname><forenames>Ning</forenames></author></authors><title>Sunflowers and Testing Triangle-Freeness of Functions</title><categories>cs.DS math.CO</categories><comments>21 pages, ITCS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A function $f: \mathbb{F}_2^n \rightarrow \{0,1\}$ is triangle-free if there
are no $x_1,x_2,x_3 \in \mathbb{F}_2^n$ satisfying $x_1+x_2+x_3=0$ and
$f(x_1)=f(x_2)=f(x_3)=1$. In testing triangle-freeness, the goal is to
distinguish with high probability triangle-free functions from those that are
$\varepsilon$-far from being triangle-free. It was shown by Green that the
query complexity of the canonical tester for the problem is upper bounded by a
function that depends only on $\varepsilon$ (GAFA, 2005), however the best
known upper bound is a tower type function of $1/\varepsilon$. The best known
lower bound on the query complexity of the canonical tester is
$1/\varepsilon^{13.239}$ (Fu and Kleinberg, RANDOM, 2014).
  In this work we introduce a new approach to proving lower bounds on the query
complexity of triangle-freeness. We relate the problem to combinatorial
questions on collections of vectors in $\mathbb{Z}_D^n$ and to sunflower
conjectures studied by Alon, Shpilka, and Umans (Comput. Complex., 2013). The
relations yield that a refutation of the Weak Sunflower Conjecture over
$\mathbb{Z}_4$ implies a super-polynomial lower bound on the query complexity
of the canonical tester for triangle-freeness. Our results are extended to
testing $k$-cycle-freeness of functions with domain $\mathbb{F}_p^n$ for every
$k \geq 3$ and a prime $p$. In addition, we generalize the lower bound of Fu
and Kleinberg to $k$-cycle-freeness for $k \geq 4$ by generalizing the
construction of uniquely solvable puzzles due to Coppersmith and Winograd (J.
Symbolic Comput., 1990).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4695</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4695</id><created>2014-11-17</created><authors><author><keyname>Heydari</keyname><forenames>Ali</forenames></author></authors><title>Feedback Solution to Optimal Switching Problems with Switching Cost</title><categories>cs.SY math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of optimal switching between nonlinear autonomous subsystems is
investigated in this study where the objective is not only bringing the states
to close to the desired point, but also adjusting the switching pattern, in the
sense of penalizing switching occurrences and assigning different preferences
to utilization of different modes. The mode sequence is unspecified and a
switching cost term is used in the cost function for penalizing each switching.
It is shown that once a switching cost is incorporated, the optimal cost-to-go
function depends on the already active subsystem, i.e., the subsystem which was
engaged in the previous time step. Afterwards, an approximate dynamic
programming based method is developed which provides an approximation of the
optimal solution to the problem in a feedback form and for different initial
conditions. Finally, the performance of the method is analyzed through
numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4696</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4696</id><created>2014-11-17</created><authors><author><keyname>Lee</keyname><forenames>Kwangsu</forenames></author><author><keyname>Lee</keyname><forenames>Dong Hoon</forenames></author></authors><title>Security Analysis of the Unrestricted Identity-Based Aggregate Signature
  Scheme</title><categories>cs.CR</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aggregate signatures allow anyone to combine different signatures signed by
different signers on different messages into a single short signature. An ideal
aggregate signature scheme is an identity-based aggregate signature (IBAS)
scheme that supports full aggregation since it can reduce the total transmitted
data by using an identity string as a public key and anyone can freely
aggregate different signatures. Constructing a secure IBAS scheme that supports
full aggregation in bilinear maps is an important open problem. Recently, Yuan
{\it et al.} proposed an IBAS scheme with full aggregation in bilinear maps and
claimed its security in the random oracle model under the computational
Diffie-Hellman assumption. In this paper, we show that there exists an
efficient forgery attacker on their IBAS scheme and their security proof has a
serious flaw.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4701</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4701</id><created>2014-11-17</created><authors><author><keyname>Yu</keyname><forenames>Zhiding</forenames></author><author><keyname>Zhang</keyname><forenames>Wende</forenames></author><author><keyname>Kumar</keyname><forenames>B. V. K. Vijaya</forenames></author><author><keyname>Levi</keyname><forenames>Dan</forenames></author></authors><title>Structured Hough Voting for Vision-based Highway Border Detection</title><categories>cs.CV cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a vision-based highway border detection algorithm using structured
Hough voting. Our approach takes advantage of the geometric relationship
between highway road borders and highway lane markings. It uses a strategy
where a number of trained road border and lane marking detectors are triggered,
followed by Hough voting to generate corresponding detection of the border and
lane marking. Since the initially triggered detectors usually result in large
number of positives, conventional frame-wise Hough voting is not able to always
generate robust border and lane marking results. Therefore, we formulate this
problem as a joint detection-and-tracking problem under the structured Hough
voting model, where tracking refers to exploiting inter-frame structural
information to stabilize the detection results. Both qualitative and
quantitative evaluations show the superiority of the proposed structured Hough
voting model over a number of baseline methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4702</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4702</id><created>2014-11-17</created><authors><author><keyname>Ferrier</keyname><forenames>Michael R.</forenames></author></authors><title>Toward a Universal Cortical Algorithm: Examining Hierarchical Temporal
  Memory in Light of Frontal Cortical Function</title><categories>q-bio.NC cs.AI cs.NE</categories><comments>105 pages, 6 figures</comments><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide range of evidence points toward the existence of a common algorithm
underlying the processing of information throughout the cerebral cortex.
Several hypothesized features of this cortical algorithm are reviewed,
including sparse distributed representation, Bayesian inference, hierarchical
organization composed of alternating template matching and pooling layers,
temporal slowness and predictive coding. Hierarchical Temporal Memory (HTM) is
a family of learning algorithms and corresponding theories of cortical function
that embodies these principles. HTM has previously been applied mainly to
perceptual tasks typical of posterior cortex. In order to evaluate HTM as a
candidate model of cortical function, it is necessary also to investigate its
compatibility with the requirements of frontal cortical function. To this end,
a variety of models of frontal cortical function are reviewed and integrated,
to arrive at the hypothesis that frontal functions including attention, working
memory and action selection depend largely upon the same basic algorithms as do
posterior functions, with the notable additions of a mechanism for the active
maintenance of representations and of multiple cortico-striato-thalamo-cortical
loops that allow communication between regions of frontal cortex to be gated in
an adaptive manner. Computational models of this system are reviewed. Finally,
there is a discussion of how HTM can contribute to the understanding of frontal
cortical function, and of what the requirements of frontal cortical function
mean for the future development of HTM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4726</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4726</id><created>2014-11-17</created><updated>2015-03-16</updated><authors><author><keyname>Rawassizadeh</keyname><forenames>Reza</forenames></author><author><keyname>Momeni</keyname><forenames>Elaheh</forenames></author><author><keyname>Shetty</keyname><forenames>Prajna</forenames></author></authors><title>Scalable Mining of Daily Behavioral Patterns in Context Sensing Life-Log
  Data</title><categories>cs.HC cs.CY</categories><comments>10 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the advent of wearable devices and the proliferation of smartphones,
there still is no ideal platform that can continuously sense and precisely
collect all available contextual information. Ideally, mobile sensing data
collection approaches should deal with uncertainty and data loss originating
from software and hardware restrictions. We have conducted life logging data
collection experiments from 35 users and created a rich dataset (9.26 million
records) to represent the real-world deployment issues of mobile sensing
systems. We create a novel set of algorithms to identify human behavioral
motifs while considering the uncertainty of collected data objects. Our work
benefits from combinations of sensors available on a device and identifies
behavioral patterns with a temporal granularity similar to human time
perception. Employing a combination of sensors rather than focusing on only one
sensor can handle uncertainty by neglecting sensor data that is not available
and focusing instead on available data. Moreover, by experimenting on two real,
large datasets, we demonstrate that using a sliding window significantly
improves the scalability of our algorithms, which can be used by applications
for small devices, such as smartphones and wearables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4732</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4732</id><created>2014-11-17</created><updated>2015-03-04</updated><authors><author><keyname>Griffith</keyname><forenames>Virgil</forenames></author><author><keyname>Ho</keyname><forenames>Tracey</forenames></author></authors><title>Quantifying Redundant Information in Predicting a Target Random Variable</title><categories>cs.IT math.IT</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of defining a measure of redundant
information that quantifies how much common information two or more random
variables specify about a target random variable. We discussed desired
properties of such a measure, and propose new measures with some desirable
properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4733</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4733</id><created>2014-11-17</created><authors><author><keyname>Mahmood</keyname><forenames>Kashif</forenames></author><author><keyname>Chilwan</keyname><forenames>Ameen</forenames></author><author><keyname>&#xd8;sterb&#xf8;</keyname><forenames>Olav N.</forenames></author><author><keyname>Jarschel</keyname><forenames>Michael</forenames></author></authors><title>On The Modeling of OpenFlow-based SDNs: The Single Node Case</title><categories>cs.PF cs.NI</categories><comments>Published in Proceedings of CS &amp; IT for NeCOM 2014</comments><journal-ref>Proceedings of Computer Science and Information Technology (CS &amp;
  IT), Vol.4, 2014, pp 207-214</journal-ref><doi>10.5121/csit.2014.41120</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  OpenFlow is one of the most commonly used protocols for communication between
the controller and the forwarding element in a software defined network (SDN).
A model based on M/M/1 queues is proposed in [1] to capture the communication
between the forwarding element and the controller. Albeit the model provides
useful insight, it is accurate only for the case when the probability of
expecting a new flow is small. Secondly, it is not straight forward to extend
the model in [1] to more than one forwarding element in the data plane. In this
work we propose a model which addresses both these challenges. The model is
based on Jackson assumption but with corrections tailored to the OpenFlow based
SDN network. Performance analysis using the proposed model indicates that the
model is accurate even for the case when the probability of new flow is quite
large. Further we show by a toy example that the model can be extended to more
than one node in the data plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4734</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4734</id><created>2014-11-17</created><updated>2015-12-16</updated><authors><author><keyname>Eigen</keyname><forenames>David</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author></authors><title>Predicting Depth, Surface Normals and Semantic Labels with a Common
  Multi-Scale Convolutional Architecture</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address three different computer vision tasks using a single
basic architecture: depth prediction, surface normal estimation, and semantic
labeling. We use a multiscale convolutional network that is able to adapt
easily to each task using only small modifications, regressing from the input
image to the output map directly. Our method progressively refines predictions
using a sequence of scales, and captures many image details without any
superpixels or low-level segmentation. We achieve state-of-the-art performance
on benchmarks for all three tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4738</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4738</id><created>2014-11-18</created><updated>2015-12-16</updated><authors><author><keyname>Kang</keyname><forenames>Cuicui</forenames></author><author><keyname>Liao</keyname><forenames>Shengcai</forenames></author><author><keyname>He</keyname><forenames>Yonghao</forenames></author><author><keyname>Wang</keyname><forenames>Jian</forenames></author><author><keyname>Niu</keyname><forenames>Wenjia</forenames></author><author><keyname>Xiang</keyname><forenames>Shiming</forenames></author><author><keyname>Pan</keyname><forenames>Chunhong</forenames></author></authors><title>Cross-Modal Similarity Learning : A Low Rank Bilinear Formulation</title><categories>cs.MM cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cross-media retrieval problem has received much attention in recent years
due to the rapid increasing of multimedia data on the Internet. A new approach
to the problem has been raised which intends to match features of different
modalities directly. In this research, there are two critical issues: how to
get rid of the heterogeneity between different modalities and how to match the
cross-modal features of different dimensions. Recently metric learning methods
show a good capability in learning a distance metric to explore the
relationship between data points. However, the traditional metric learning
algorithms only focus on single-modal features, which suffer difficulties in
addressing the cross-modal features of different dimensions. In this paper, we
propose a cross-modal similarity learning algorithm for the cross-modal feature
matching. The proposed method takes a bilinear formulation, and with the
nuclear-norm penalization, it achieves low-rank representation. Accordingly,
the accelerated proximal gradient algorithm is successfully imported to find
the optimal solution with a fast convergence rate O(1/t^2). Experiments on
three well known image-text cross-media retrieval databases show that the
proposed method achieves the best performance compared to the state-of-the-art
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4759</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4759</id><created>2014-11-18</created><updated>2015-10-18</updated><authors><author><keyname>Leonardi</keyname><forenames>Emilio</forenames></author><author><keyname>Torrisi</keyname><forenames>Giovanni Luca</forenames></author></authors><title>Least Recently Used caches under the Shot Noise Model</title><categories>cs.PF</categories><comments>Submitted to SIAP- A preliminary, incomplete version of this work has
  appeared in Proceedings of IEEE Infocom 2015, HK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze Least Recently Used (LRU) caches operating under the
Shot Noise requests Model (SNM). The SNM was recently proposed to better
capture the main characteristics of today Video on Demand (VoD) traffic. We
investigate the validity of Che's approximation through an asymptotic analysis
of the cache eviction time. In particular, we provide a large deviation
principle, a law of large numbers and a central limit theorem for the cache
eviction time, as the cache size grows large. Finally, we derive upper and
lower bounds for the &quot;hit&quot; probability in tandem networks of caches under Che's
approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4762</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4762</id><created>2014-11-18</created><authors><author><keyname>Harshan</keyname><forenames>J.</forenames></author><author><keyname>Oggier</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames></author><author><keyname>Datta</keyname><forenames>Anwitaman</forenames></author></authors><title>Sparsity Exploiting Erasure Coding for Resilient Storage and Efficient
  I/O Access in Delta based Versioning Systems</title><categories>cs.IT cs.DC math.IT</categories><comments>10 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the problem of storing reliably an archive of
versioned data. Specifically, we focus on systems where the differences
(deltas) between subsequent versions rather than the whole objects are stored -
a typical model for storing versioned data. For reliability, we propose erasure
encoding techniques that exploit the sparsity of information in the deltas
while storing them reliably in a distributed back-end storage system, resulting
in improved I/O read performance to retrieve the whole versioned archive. Along
with the basic techniques, we propose a few optimization heuristics, and
evaluate the techniques' efficacy analytically and with numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4763</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4763</id><created>2014-11-18</created><authors><author><keyname>Bellili</keyname><forenames>Faouzi</forenames></author><author><keyname>Meftehi</keyname><forenames>Rabii</forenames></author><author><keyname>Affes</keyname><forenames>Sofiene</forenames></author><author><keyname>Stephenne</keyname><forenames>Alex</forenames></author></authors><title>Maximum Likelihood SNR Estimation of Linearly-Modulated Signals over
  Time-Varying Flat-Fading SIMO Channels</title><categories>stat.AP cs.IT math.IT</categories><comments>38 pages, 8 figures, Extended Version of a paper recently accepted
  for publication in IEEE Trans. on Signal Processing, 2014</comments><doi>10.1109/TSP.2014.2364017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we tackle for the first time the problem of maximum likelihood
(ML) estimation of the signal-to-noise ratio (SNR) parameter over time-varying
single-input multiple-output (SIMO) channels. Both the data-aided (DA) and the
non-data-aided (NDA) schemes are investigated. Unlike classical techniques
where the channel is assumed to be slowly time-varying and, therefore,
considered as constant over the entire observation period, we address the more
challenging problem of instantaneous (i.e., short-term or local) SNR estimation
over fast time-varying channels. The channel variations are tracked locally
using a polynomial-in-time expansion. First, we derive in closed-form
expressions the DA ML estimator and its bias. The latter is subsequently
subtracted in order to obtain a new unbiased DA estimator whose variance and
the corresponding Cram\'er-Rao lower bound (CRLB) are also derived in closed
form. Due to the extreme nonlinearity of the log-likelihood function (LLF) in
the NDA case, we resort to the expectation-maximization (EM) technique to
iteratively obtain the exact NDA ML SNR estimates within very few iterations.
Most remarkably, the new EM-based NDA estimator is applicable to any
linearly-modulated signal and provides sufficiently accurate soft estimates
(i.e., soft detection) for each of the unknown transmitted symbols. Therefore,
hard detection can be easily embedded in the iteration loop in order to improve
its performance at low to moderate SNR levels. We show by extensive computer
simulations that the new estimators are able to accurately estimate the
instantaneous per-antenna SNRs as they coincide with the DA CRLB over a wide
range of practical SNRs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4781</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4781</id><created>2014-11-18</created><updated>2014-11-25</updated><authors><author><keyname>Sheng</keyname><forenames>Min</forenames></author><author><keyname>Wen</keyname><forenames>Juan</forenames></author><author><keyname>Li</keyname><forenames>Jiandong</forenames></author><author><keyname>Liang</keyname><forenames>Ben</forenames></author></authors><title>Correlations of Interference and Link Successes in Heterogeneous
  Cellular Networks</title><categories>cs.IT math.IT</categories><comments>26 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In heterogeneous cellular networks (HCNs), the interference received at a
user is correlated over time slots since it comes from the same set of randomly
located BSs. This results in the correlations of link successes, thus affecting
network performance. Under the assumptions of a K-tier Poisson network,
strongest-candidate based BS association, and independent Rayleigh fading, we
first quantify the correlation coefficients of interference. We observe that
the interference correlation is independent of the number of tiers, BS density,
SIR threshold, and transmit power. Then, we study the correlations of link
successes in terms of the joint success probability over multiple time slots.
We show that the joint success probability is decided by the success
probability in a single time slot and a diversity polynomial, which represents
the temporal interference correlation. Moreover, the parameters of HCNs have an
important influence on the joint success probability by affecting the success
probability in a single time slot. Particularly, we obtain the condition under
which the joint success probability increases with the BS density and transmit
power. We further show that the conditional success probability given prior
successes only depends on the path loss exponent and the number of time slots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4790</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4790</id><created>2014-11-18</created><authors><author><keyname>Ishengoma</keyname><forenames>Fredrick R.</forenames></author></authors><title>The Art of Data Hiding with Reed-Solomon Error Correcting Codes</title><categories>cs.MM cs.IT math.IT</categories><journal-ref>International Journal of Computer Applications 106(14):28-31,
  November 2014</journal-ref><doi>10.5120/18590-9902</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the tremendous advancements in technology and the Internet, data
security has become a major issue around the globe. To guarantee that data is
protected and does not go to an unintended endpoint, the art of data hiding
(steganography) emerged. Steganography is the art of hiding information such
that it is not detectable to the naked eye. Various techniques have been
proposed for hiding a secret message in a carrier document. In this paper, we
present a novel design that applies Reed-Solomon (RS) error correcting codes in
steganographic applications. The model works by substituting the redundant RS
codes with the steganographic message. The experimental results show that the
proposed design is satisfactory with the percentage of decoded information 100%
and percentage of decoded secret message 97. 36%. The proposed model proved
that it could be applied in various steganographic applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4796</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4796</id><created>2014-11-18</created><updated>2015-04-27</updated><authors><author><keyname>Halava</keyname><forenames>Vesa</forenames></author><author><keyname>Harju</keyname><forenames>Tero</forenames></author><author><keyname>Niskanen</keyname><forenames>Reino</forenames></author><author><keyname>Potapov</keyname><forenames>Igor</forenames></author></authors><title>Weighted automata on infinite words in the context of Attacker-Defender
  games</title><categories>cs.FL cs.GT cs.LO</categories><comments>23 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider infinite-state Attacker-Defender games with reachability
objectives. The results of the paper are twofold. Firstly we prove a new
language-theoretic result for weighted automata on infinite words and show its
encoding into the framework of Attacker-Defender games. Secondly we use this
novel concept to prove undecidability for checking existence of a winning
strategy in several low-dimensional mathematical games including vector
reachability games, word games and braid games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4798</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4798</id><created>2014-11-18</created><updated>2015-07-07</updated><authors><author><keyname>Traversa</keyname><forenames>Fabio L.</forenames></author><author><keyname>Ramella</keyname><forenames>Chiara</forenames></author><author><keyname>Bonani</keyname><forenames>Fabrizio</forenames></author><author><keyname>Di Ventra</keyname><forenames>Massimiliano</forenames></author></authors><title>Memcomputing NP-complete problems in polynomial time using polynomial
  resources and collective states</title><categories>cs.ET cs.NE</categories><comments>We have corrected minor typos and improved the presentation</comments><journal-ref>Science Advances, Vol. 1, no. 6, pag e1500031, year 2015</journal-ref><doi>10.1126/sciadv.1500031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memcomputing is a novel non-Turing paradigm of computation that uses
interacting memory cells (memprocessors for short) to store and process
information on the same physical platform. It was recently proved
mathematically that memcomputing machines have the same computational power of
non-deterministic Turing machines. Therefore, they can solve NP-complete
problems in polynomial time and, using the appropriate architecture, with
resources that only grow polynomially with the input size. The reason for this
computational power stems from properties inspired by the brain and shared by
any universal memcomputing machine, in particular intrinsic parallelism and
information overhead, namely the capability of compressing information in the
collective state of the memprocessor network. Here, we show an experimental
demonstration of an actual memcomputing architecture that solves the
NP-complete version of the subset-sum problem in only one step and is composed
of a number of memprocessors that scales linearly with the size of the problem.
We have fabricated this architecture using standard microelectronic technology
so that it can be easily realized in any laboratory setting. Even though the
particular machine presented here is eventually limited by noise--and will thus
require error-correcting codes to scale to an arbitrary number of
memprocessors--it represents the first proof-of-concept of a machine capable of
working with the collective state of interacting memory cells, unlike the
present-day single-state machines built using the von Neumann architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4813</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4813</id><created>2014-11-18</created><authors><author><keyname>Breuer</keyname><forenames>Peter T.</forenames></author><author><keyname>Bowen</keyname><forenames>Jonathan P.</forenames></author></authors><title>On the Security of Fully Homomorphic Encryption and Encrypted Computing:
  Is Division safe?</title><categories>cs.CR cs.DM</categories><comments>10 pages, as first submitted to short paper section of ESSoS 2015</comments><acm-class>B.2.0; E.3; G.0; G.2.m; G.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since fully homomorphic encryption and homomorphically encrypted computing
preserve algebraic identities such as 2*2=2+2, a natural question is whether
this extremely utilitarian feature also sets up cryptographic attacks that use
the encrypted arithmetic operators to generate or identify the encryptions of
known constants. In particular, software or hardware might use encrypted
addition and multiplication to do encrypted division and deliver the encryption
of x/x=1. That can then be used to generate 1+1=2, etc, until a complete
codebook is obtained.
  This paper shows that there is no formula or computation using 32-bit
multiplication x*y and three-input addition x+y+z that yields a known constant
from unknown inputs. We characterise what operations are similarly `safe' alone
or in company, and show that 32-bit division is not safe in this sense, but
there are trivial modifications that make it so.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4814</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4814</id><created>2014-11-18</created><updated>2015-02-26</updated><authors><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author></authors><title>Optimal control of the convergence time in the Hegselmann--Krause
  dynamics</title><categories>math.DS cs.SY</categories><comments>14 pages</comments><msc-class>39A60, 37N35, 91D10, 93C55</msc-class><acm-class>F.2.1; G.1.6</acm-class><doi>10.1080/10236198.2015.1045890</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the optimal control problem of minimizing the convergence time in
the discrete Hegselmann--Krause model of opinion dynamics. The underlying model
is extended with a set of strategic agents that can freely place their opinion
at every time step. Indeed, if suitably coordinated, the strategic agents can
significantly lower the convergence time of an instance of the
Hegselmann--Krause model. We give several lower and upper worst-case bounds for
the convergence time of a Hegselmann--Krause system with a given number of
strategic agents, while still leaving some gaps for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4819</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4819</id><created>2014-11-18</created><updated>2015-05-14</updated><authors><author><keyname>Miltzow</keyname><forenames>Tillmann</forenames></author><author><keyname>Schmidt</keyname><forenames>Jens M.</forenames></author><author><keyname>Xia</keyname><forenames>Mingji</forenames></author></authors><title>Counting K_4-Subdivisions</title><categories>cs.DM math.CO</categories><comments>5 figures</comments><doi>10.1016/j.disc.2015.06.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental theorem in graph theory states that any 3-connected graph
contains a subdivision of $K_4$. As a generalization, we ask for the minimum
number of $K_4$-subdivisions that are contained in every $3$-connected graph on
$n$ vertices. We prove that there are $\Omega(n^3)$ such $K_4$-subdivisions and
show that the order of this bound is tight for infinitely many graphs. We
further investigate a better bound in dependence on $m$ and prove that the
computational complexity of the problem of counting the exact number of
$K_4$-subdivisions is $\#P$-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4823</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4823</id><created>2014-11-18</created><authors><author><keyname>Furbach</keyname><forenames>Ulrich</forenames></author><author><keyname>Schon</keyname><forenames>Claudia</forenames></author><author><keyname>Stolzenburg</keyname><forenames>Frieder</forenames></author></authors><title>Automated Reasoning in Deontic Logic</title><categories>cs.AI</categories><doi>10.1007/978-3-319-13365-2_6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deontic logic is a very well researched branch of mathematical logic and
philosophy. Various kinds of deontic logics are discussed for different
application domains like argumentation theory, legal reasoning, and acts in
multi-agent systems. In this paper, we show how standard deontic logic can be
stepwise transformed into description logic and DL- clauses, such that it can
be processed by Hyper, a high performance theorem prover which uses a
hypertableau calculus. Two use cases, one from multi-agent research and one
from the development of normative system are investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4825</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4825</id><created>2014-11-18</created><authors><author><keyname>Furbach</keyname><forenames>Ulrich</forenames></author><author><keyname>Schon</keyname><forenames>Claudia</forenames></author><author><keyname>Stolzenburg</keyname><forenames>Frieder</forenames></author></authors><title>Cognitive Systems and Question Answering</title><categories>cs.AI cs.CL</categories><journal-ref>Industrie 4.0 Management, 31(1):29-32, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper briefly characterizes the field of cognitive computing. As an
exemplification, the field of natural language question answering is introduced
together with its specific challenges. A possibility to master these challenges
is illustrated by a detailed presentation of the LogAnswer system, which is a
successful representative of the field of natural language question answering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4836</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4836</id><created>2014-11-18</created><authors><author><keyname>Farhadzadeh</keyname><forenames>Farzad</forenames></author><author><keyname>Willems</keyname><forenames>Frans M. J.</forenames></author><author><keyname>Voloshinovskiy</keyname><forenames>Sviatoslav</forenames></author></authors><title>Information Theoretical Analysis of Identification based on Active
  Content Fingerprinting</title><categories>cs.IT math.IT</categories><comments>35th WIC Symposium on Information Theory in the Benelux</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content fingerprinting and digital watermarking are techniques that are used
for content protection and distribution monitoring. Over the past few years,
both techniques have been well studied and their shortcomings understood.
Recently, a new content fingerprinting scheme called {\em active content
fingerprinting} was introduced to overcome these shortcomings. Active content
fingerprinting aims to modify a content to extract robuster fingerprints than
the conventional content fingerprinting. Moreover, contrary to digital
watermarking, active content fingerprinting does not embed any message
independent of contents thus does not face host interference. The main goal of
this paper is to analyze fundamental limits of active content fingerprinting in
an information theoretical framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4840</identifier>
 <datestamp>2015-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4840</id><created>2014-11-18</created><updated>2015-03-19</updated><authors><author><keyname>Qin</keyname><forenames>Yuhao</forenames></author><author><keyname>Zhao</keyname><forenames>Zhidan</forenames></author><author><keyname>Cai</keyname><forenames>Shimin</forenames></author><author><keyname>Gao</keyname><forenames>Liang</forenames></author></authors><title>Dual-induced multifractality of human online activity</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>14 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent discoveries of human activity reveal the existence of long-term
correlation and its relation with the fat-tailed distribution of inter-event
times, which imply that there exists the fractality of human activity. However,
works further analyzing the category of fractality and its origin still lack.
Herein, both the DFA and MFDFA are applied in the analysis of time series of
online reviewing activity from Movielens and Netflix. Results show the
long-term correlation at individual and whole community level, while the extent
of correlation at individual level is restricted to activity level. Such
long-term correlation also reveals the fractality of online reviewing activity.
In our further investigation of this fractality, we \emph{first} demonstrate it
is multifractality, which results from the dual effect of broad probability
density function and long-term correlation of time series of online reviewing
activity. This result is also verified by three synthesized series. Therefore,
we conclude that the combining impact of both broad probability density
function and long-term correlation is the origin of multifractality behaving in
human online activity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4848</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4848</id><created>2014-11-18</created><updated>2015-02-01</updated><authors><author><keyname>Lee</keyname><forenames>Jemin</forenames></author><author><keyname>Quek</keyname><forenames>Tony Q. S.</forenames></author></authors><title>Hybrid Full-/Half-Duplex System Analysis in Heterogeneous Wireless
  Networks</title><categories>cs.IT math.IT</categories><comments>13 pages, 10 figures, to appear in IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Full-duplex (FD) radio has been introduced for bidirectional communications
on the same temporal and spectral resources so as to maximize spectral
efficiency. In this paper, motivated by the recent advances in FD radios, we
provide a foundation for hybrid-duplex heterogeneous networks (HDHNs), composed
of multi-tier networks with a mixture of access points (APs), operating either
in bidirectional FD mode or downlink half-duplex (HD) mode. Specifically, we
characterize the net- work interference from FD-mode cells, and derive the HDHN
throughput by accounting for AP spatial density, self-interference cancellation
(IC) capability, and transmission power of APs and users. By quantifying the
HDHN throughput, we present the effect of network parameters and the self-IC
capability on the HDHN throughput, and show the superiority of FD mode for
larger AP densities (i.e., larger network interference and shorter
communication distance) or higher self-IC capability. Furthermore, our results
show operating all APs in FD or HD achieves higher throughput compared to the
mixture of two mode APs in each tier network, and introducing hybrid-duplex for
different tier networks improves the heterogenous network throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4862</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4862</id><created>2014-11-18</created><updated>2015-07-17</updated><authors><author><keyname>Ferreira</keyname><forenames>Bernardo</forenames></author><author><keyname>Rodrigues</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Leit&#xe3;o</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Domingos</keyname><forenames>Henrique</forenames></author></authors><title>Privacy-Preserving Content-Based Image Retrieval in the Cloud</title><categories>cs.CR</categories><comments>This paper has been withdrawn by the author, as it is outdated and a
  more recent version with major differences has been published. due to a
  crucial sign error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Storage requirements for visual data have been increasing in recent years,
following the emergence of many new highly interactive, multimedia services and
applications for both personal and corporate use. This has been a key driving
factor for the adoption of cloud-based data outsourcing solutions. However,
outsourcing data storage to the Cloud also leads to new challenges that must be
carefully addressed, especially regarding privacy. In this paper we propose a
secure framework for outsourced privacy-preserving storage and retrieval in
large image repositories. Our proposal is based on a novel cryptographic
scheme, named IES-CBIR, specifically designed for media image data. Our
solution enables both encrypted storage and querying using Content Based Image
Retrieval (CBIR) while preserving privacy. We have built a prototype of the
proposed framework, formally analyzed and proven its security properties, and
experimentally evaluated its performance and precision. Our results show that
IES-CBIR is provably secure, allows more efficient operations than existing
proposals, both in terms of time and space complexity, and enables more
realistic, interesting and practical application scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4867</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4867</id><created>2014-11-11</created><authors><author><keyname>Malmros</keyname><forenames>Jens</forenames></author><author><keyname>Liljeros</keyname><forenames>Fredrik</forenames></author><author><keyname>Britton</keyname><forenames>Tom</forenames></author></authors><title>Respondent-driven sampling and an unusual epidemic</title><categories>physics.soc-ph cs.SI math.PR stat.AP</categories><comments>20 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Respondent-driven sampling (RDS) is frequently used when sampling
hard-to-reach and/or stigmatized communities. RDS utilizes a peer-driven
recruitment mechanism where sampled individuals pass on participation coupons
to at most $c$ of their acquaintances in the community ($c=3$ being a common
choice), who then in turn pass on to their acquaintances if they choose to
participate, and so on. This process of distributing coupons is shown to behave
like a new Reed-Frost type network epidemic model, in which becoming infected
corresponds to receiving a coupon. The difference from existing network
epidemic models is that an infected individual can not infect (i.e.\ sample)
all of its contacts, but only at most $c$ of them. We calculate $R_0$, the
probability of a major &quot;outbreak&quot;, and the relative size of a major outbreak in
the limit of infinite population size and evaluate their adequacy in finite
populations. We study the effect of varying $c$ and compare RDS to the
corresponding usual epidemic models, i.e.\ the case of $c=\infty$. Our results
suggest that the number of coupons has a large effect on RDS recruitment.
Additionally, we use our findings to explain previous empirical observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4884</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4884</id><created>2014-11-18</created><authors><author><keyname>Summers</keyname><forenames>Tyler</forenames></author><author><keyname>Shames</keyname><forenames>Iman</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author><author><keyname>D&#xf6;rfler</keyname><forenames>Florian</forenames></author></authors><title>Topology Design for Optimal Network Coherence</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a network topology design problem in which an initial undirected
graph underlying the network is given and the objective is to select a set of
edges to add to the graph to optimize the coherence of the resulting network.
We show that network coherence is a submodular function of the network
topology. As a consequence, a simple greedy algorithm is guaranteed to produce
near optimal edge set selections. We also show that fast rank one updates of
the Laplacian pseudoinverse using generalizations of the Sherman-Morrison
formula and an accelerated variant of the greedy algorithm can speed up the
algorithm by several orders of magnitude in practice. These allow our
algorithms to scale to network sizes far beyond those that can be handled by
convex relaxation heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4890</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4890</id><created>2014-11-17</created><authors><author><keyname>Zhang</keyname><forenames>Xing</forenames></author></authors><title>Which Are You In A Photo?</title><categories>cs.SD cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic image tagging has been a long standing problem, it mainly relies on
image recognition techniques of which the accuracy is still not satisfying.
This paper attempts to explore out-of-band sensing base on the mobile phone to
sense the people in a picture while the picture is being taken and create name
tags on-the-fly. The major challenges pertain to two aspects - &quot;Who&quot; and
&quot;Which&quot;. (1) &quot;Who&quot;: discriminating people who are in the picture from those
that are not; (2) &quot;Which&quot;: correlating each name tag with its corresponding
people in the picture. We propose an accurate acoustic scheme applying on the
mobile phones, which leverages the Doppler effect of sound wave to address
these two challenges. As a proof of concept, we implement the scheme on 7
android phones and take pictures in various real-life scenarios with people
positioning in different ways. Extensive experiments show that the accuracy of
tag correlation is above 85% within 3m for picturing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4894</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4894</id><created>2014-11-18</created><updated>2015-04-14</updated><authors><author><keyname>Chakrabarti</keyname><forenames>Ayan</forenames></author><author><keyname>Xiong</keyname><forenames>Ying</forenames></author><author><keyname>Gortler</keyname><forenames>Steven J.</forenames></author><author><keyname>Zickler</keyname><forenames>Todd</forenames></author></authors><title>Low-level Vision by Consensus in a Spatial Hierarchy of Regions</title><categories>cs.CV</categories><comments>Accepted to CVPR 2015. Project page:
  http://www.ttic.edu/chakrabarti/consensus/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a multi-scale framework for low-level vision, where the goal is
estimating physical scene values from image data---such as depth from stereo
image pairs. The framework uses a dense, overlapping set of image regions at
multiple scales and a &quot;local model,&quot; such as a slanted-plane model for stereo
disparity, that is expected to be valid piecewise across the visual field.
Estimation is cast as optimization over a dichotomous mixture of variables,
simultaneously determining which regions are inliers with respect to the local
model (binary variables) and the correct co-ordinates in the local model space
for each inlying region (continuous variables). When the regions are organized
into a multi-scale hierarchy, optimization can occur in an efficient and
parallel architecture, where distributed computational units iteratively
perform calculations and share information through sparse connections between
parents and children. The framework performs well on a standard benchmark for
binocular stereo, and it produces a distributional scene representation that is
appropriate for combining with higher-level reasoning and other low-level cues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4906</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4906</id><created>2014-11-18</created><updated>2015-08-25</updated><authors><author><keyname>Gundert</keyname><forenames>Anna</forenames></author><author><keyname>Wagner</keyname><forenames>Uli</forenames></author></authors><title>On Eigenvalues of Random Complexes</title><categories>math.CO cs.DM</categories><comments>Extended full version of an extended abstract that appeared at SoCG
  2012, to appear in Israel Journal of Mathematics</comments><msc-class>55U10, 05C80, 05C65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider higher-dimensional generalizations of the normalized Laplacian
and the adjacency matrix of graphs and study their eigenvalues for the
Linial-Meshulam model $X^k(n,p)$ of random $k$-dimensional simplicial complexes
on $n$ vertices. We show that for $p=\Omega(\log n/n)$, the eigenvalues of
these matrices are a.a.s. concentrated around two values. The main tool, which
goes back to the work of Garland, are arguments that relate the eigenvalues of
these matrices to those of graphs that arise as links of $(k-2)$-dimensional
faces. Garland's result concerns the Laplacian; we develop an analogous result
for the adjacency matrix. The same arguments apply to other models of random
complexes which allow for dependencies between the choices of $k$-dimensional
simplices. In the second part of the paper, we apply this to the question of
possible higher-dimensional analogues of the discrete Cheeger inequality, which
in the classical case of graphs relates the eigenvalues of a graph and its edge
expansion. It is very natural to ask whether this generalizes to higher
dimensions and, in particular, whether the higher-dimensional Laplacian spectra
capture the notion of coboundary expansion - a generalization of edge expansion
that arose in recent work of Linial and Meshulam and of Gromov. We show that
this most straightforward version of a higher-dimensional discrete Cheeger
inequality fails, in quite a strong way: For every $k\geq 2$ and $n\in
\mathbb{N}$, there is a $k$-dimensional complex $Y^k_n$ on $n$ vertices that
has strong spectral expansion properties (all nontrivial eigenvalues of the
normalised $k$-dimensional Laplacian lie in the interval
$[1-O(1/\sqrt{n}),1+O(1/\sqrt{n})]$) but whose coboundary expansion is bounded
from above by $O(\log n/n)$ and so tends to zero as $n\rightarrow \infty$;
moreover, $Y^k_n$ can be taken to have vanishing integer homology in dimension
less than $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4916</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4916</id><created>2014-11-18</created><authors><author><keyname>Feldman</keyname><forenames>Michal</forenames></author><author><keyname>Gravin</keyname><forenames>Nick</forenames></author><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author></authors><title>Combinatorial Auctions via Posted Prices</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study anonymous posted price mechanisms for combinatorial auctions in a
Bayesian framework. In a posted price mechanism, item prices are posted, then
the consumers approach the seller sequentially in an arbitrary order, each
purchasing her favorite bundle from among the unsold items at the posted
prices. These mechanisms are simple, transparent and trivially dominant
strategy incentive compatible (DSIC).
  We show that when agent preferences are fractionally subadditive (which
includes all submodular functions), there always exist prices that, in
expectation, obtain at least half of the optimal welfare. Our result is
constructive: given black-box access to a combinatorial auction algorithm A,
sample access to the prior distribution, and appropriate query access to the
sampled valuations, one can compute, in polytime, prices that guarantee at
least half of the expected welfare of A. As a corollary, we obtain the first
polytime (in n and m) constant-factor DSIC mechanism for Bayesian submodular
combinatorial auctions, given access to demand query oracles. Our results also
extend to valuations with complements, where the approximation factor degrades
linearly with the level of complementarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4925</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4925</id><created>2014-11-18</created><authors><author><keyname>Ramos-Soto</keyname><forenames>A.</forenames></author><author><keyname>Bugar&#xed;n</keyname><forenames>A.</forenames></author><author><keyname>Barro</keyname><forenames>S.</forenames></author><author><keyname>Taboada</keyname><forenames>J.</forenames></author></authors><title>Linguistic Descriptions for Automatic Generation of Textual Short-Term
  Weather Forecasts on Real Prediction Data</title><categories>cs.AI cs.CL</categories><comments>13 pages, 20 figures, IEEE Transactions on Fuzzy Systems, 2014</comments><doi>10.1109/TFUZZ.2014.2328011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present in this paper an application which automatically generates textual
short-term weather forecasts for every municipality in Galicia (NW Spain),
using the real data provided by the Galician Meteorology Agency (MeteoGalicia).
This solution combines in an innovative way computing with perceptions
techniques and strategies for linguistic description of data together with a
natural language generation (NLG) system. The application, named GALiWeather,
extracts relevant information from weather forecast input data and encodes it
into intermediate descriptions using linguistic variables and temporal
references. These descriptions are later translated into natural language texts
by the natural language generation system. The obtained forecast results have
been thoroughly validated by an expert meteorologist from MeteoGalicia using a
quality assessment methodology which covers two key dimensions of a text: the
accuracy of its content and the correctness of its form. Following this
validation GALiWeather will be released as a real service offering custom
forecasts for a wide public.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4940</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4940</id><created>2014-11-18</created><updated>2015-04-22</updated><authors><author><keyname>Xu</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Xiong</keyname><forenames>Li</forenames></author><author><keyname>Sunderam</keyname><forenames>Vaidy</forenames></author><author><keyname>Liu</keyname><forenames>Jinfei</forenames></author><author><keyname>Luo</keyname><forenames>Jun</forenames></author></authors><title>Speed Partitioning for Indexing Moving Objects</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Indexing moving objects has been extensively studied in the past decades.
Moving objects, such as vehicles and mobile device users, usually exhibit some
patterns on their velocities, which can be utilized for velocity-based
partitioning to improve performance of the indexes. Existing velocity-based
partitioning techniques rely on some kinds of heuristics rather than
analytically calculate the optimal solution. In this paper, we propose a novel
speed partitioning technique based on a formal analysis over speed values of
the moving objects. We first show that speed partitioning will significantly
reduce the search space expansion which has direct impacts on query performance
of the indexes. Next we formulate the optimal speed partitioning problem based
on search space expansion analysis and then compute the optimal solution using
dynamic programming. We then build the partitioned indexing system where
queries are duplicated and processed in each index partition. Extensive
experiments demonstrate that our method dramatically improves the performance
of indexes for moving objects and outperforms other state-of-the-art
velocity-based partitioning approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4942</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4942</id><created>2014-11-18</created><authors><author><keyname>Jha</keyname><forenames>Madhav</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author></authors><title>Path Sampling: A Fast and Provable Method for Estimating 4-Vertex
  Subgraph Counts</title><categories>cs.DS cs.DM cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counting the frequency of small subgraphs is a fundamental technique in
network analysis across various domains, most notably in bioinformatics and
social networks. The special case of triangle counting has received much
attention. Getting results for 4-vertex patterns is highly challenging, and
there are few practical results known that can scale to massive sizes. Indeed,
even a highly tuned enumeration code takes more than a day on a graph with
millions of edges. Most previous work that runs for truly massive graphs employ
clusters and massive parallelization.
  We provide a sampling algorithm that provably and accurately approximates the
frequencies of all 4-vertex pattern subgraphs. Our algorithm is based on a
novel technique of 3-path sampling and a special pruning scheme to decrease the
variance in estimates. We provide theoretical proofs for the accuracy of our
algorithm, and give formal bounds for the error and confidence of our
estimates. We perform a detailed empirical study and show that our algorithm
provides estimates within 1% relative error for all subpatterns (over a large
class of test graphs), while being orders of magnitude faster than enumeration
and other sampling based algorithms. Our algorithm takes less than a minute (on
a single commodity machine) to process an Orkut social network with 300 million
edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4943</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4943</id><created>2014-11-18</created><authors><author><keyname>Meir</keyname><forenames>Reshef</forenames></author><author><keyname>Parkes</keyname><forenames>David</forenames></author></authors><title>Congestion Games with Distance-Based Strict Uncertainty</title><categories>cs.GT</categories><comments>The full version of a paper from AAAI'15 (to appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We put forward a new model of congestion games where agents have uncertainty
over the routes used by other agents. We take a non-probabilistic approach,
assuming that each agent knows that the number of agents using an edge is
within a certain range. Given this uncertainty, we model agents who either
minimize their worst-case cost (WCC) or their worst-case regret (WCR), and
study implications on equilibrium existence, convergence through adaptive play,
and efficiency. Under the WCC behavior the game reduces to a modified
congestion game, and welfare improves when agents have moderate uncertainty.
Under WCR behavior the game is not, in general, a congestion game, but we show
convergence and efficiency bounds for a simple class of games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4949</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4949</id><created>2014-11-18</created><authors><author><keyname>Meir</keyname><forenames>Reshef</forenames></author></authors><title>Plurality Voting under Uncertainty</title><categories>cs.MA</categories><comments>The full version of a paper from AAAI'15 (to appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the nature of strategic voting is the holy grail of social
choice theory, where game-theory, social science and recently computational
approaches are all applied in order to model the incentives and behavior of
voters.
  In a recent paper, Meir et al.[EC'14] made another step in this direction, by
suggesting a behavioral game-theoretic model for voters under uncertainty. For
a specific variation of best-response heuristics, they proved initial existence
and convergence results in the Plurality voting system.
  In this paper, we extend the model in multiple directions, considering voters
with different uncertainty levels, simultaneous strategic decisions, and a more
permissive notion of best-response. We prove that a voting equilibrium exists
even in the most general case. Further, any society voting in an iterative
setting is guaranteed to converge.
  We also analyze an alternative behavior where voters try to minimize their
worst-case regret. We show that the two behaviors coincide in the simple
setting of Meir et al., but not in the general case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4952</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4952</id><created>2014-11-18</created><updated>2015-04-14</updated><authors><author><keyname>Fang</keyname><forenames>Hao</forenames></author><author><keyname>Gupta</keyname><forenames>Saurabh</forenames></author><author><keyname>Iandola</keyname><forenames>Forrest</forenames></author><author><keyname>Srivastava</keyname><forenames>Rupesh</forenames></author><author><keyname>Deng</keyname><forenames>Li</forenames></author><author><keyname>Doll&#xe1;r</keyname><forenames>Piotr</forenames></author><author><keyname>Gao</keyname><forenames>Jianfeng</forenames></author><author><keyname>He</keyname><forenames>Xiaodong</forenames></author><author><keyname>Mitchell</keyname><forenames>Margaret</forenames></author><author><keyname>Platt</keyname><forenames>John C.</forenames></author><author><keyname>Zitnick</keyname><forenames>C. Lawrence</forenames></author><author><keyname>Zweig</keyname><forenames>Geoffrey</forenames></author></authors><title>From Captions to Visual Concepts and Back</title><categories>cs.CV cs.CL</categories><comments>version corresponding to CVPR15 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach for automatically generating image
descriptions: visual detectors, language models, and multimodal similarity
models learnt directly from a dataset of image captions. We use multiple
instance learning to train visual detectors for words that commonly occur in
captions, including many different parts of speech such as nouns, verbs, and
adjectives. The word detector outputs serve as conditional inputs to a
maximum-entropy language model. The language model learns from a set of over
400,000 image descriptions to capture the statistics of word usage. We capture
global semantics by re-ranking caption candidates using sentence-level features
and a deep multimodal similarity model. Our system is state-of-the-art on the
official Microsoft COCO benchmark, producing a BLEU-4 score of 29.1%. When
human judges compare the system captions to ones written by other people on our
held-out test set, the system captions have equal or better quality 34% of the
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4956</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4956</id><created>2014-11-17</created><authors><author><keyname>Burdet</keyname><forenames>Etienne</forenames></author><author><keyname>Colombert</keyname><forenames>Morgane</forenames></author><author><keyname>Morand</keyname><forenames>Denis</forenames></author><author><keyname>Diab</keyname><forenames>Youssef</forenames></author></authors><title>Integrated canopy, building energy and radiosity model for 3D urban
  design</title><categories>cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an integrated, three dimensional, model of urban canopy, building
energy and radiosity, for early stage urban designs and test it on four urban
morphologies. All sub-models share a common descriptions of the urban
morphology, similar to 3D urban design master plans and have simple parameters.
The canopy model is a multilayer model, with a new discrete layer approach that
does not rely on simplified geometry such as canyon or regular arrays. The
building energy model is a simplified RC equivalent model, with no hypotheses
on internal zoning or wall composition. We use the CitySim software for the
radiosity model. We study the effects of convexity, the number of buildings and
building height, at constant density and thermal characteristics. Our results
suggest that careful three dimensional morphology design can reduce heat demand
by a factor of 2, especially by improving insolation of lower levels. The most
energy efficient morphology in our simulations has both the highest
surface/volume ratio and the biggest impact on the urban climate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4958</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4958</id><created>2014-11-18</created><authors><author><keyname>Wang</keyname><forenames>Xiaolong</forenames></author><author><keyname>Fouhey</keyname><forenames>David F.</forenames></author><author><keyname>Gupta</keyname><forenames>Abhinav</forenames></author></authors><title>Designing Deep Networks for Surface Normal Estimation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past few years, convolutional neural nets (CNN) have shown incredible
promise for learning visual representations. In this paper, we use CNNs for the
task of predicting surface normals from a single image. But what is the right
architecture we should use? We propose to build upon the decades of hard work
in 3D scene understanding, to design new CNN architecture for the task of
surface normal estimation. We show by incorporating several constraints
(man-made, manhattan world) and meaningful intermediate representations (room
layout, edge labels) in the architecture leads to state of the art performance
on surface normal estimation. We also show that our network is quite robust and
show state of the art results on other datasets as well without any
fine-tuning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4960</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4960</id><created>2014-11-18</created><authors><author><keyname>Rizvi&#x107;</keyname><forenames>Hana</forenames></author><author><keyname>Martin&#x10d;i&#x107;-Ip&#x161;i&#x107;</keyname><forenames>Sanda</forenames></author><author><keyname>Me&#x161;trovi&#x107;</keyname><forenames>Ana</forenames></author></authors><title>Network Motifs Analysis of Croatian Literature</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyse network motifs in the co-occurrence directed
networks constructed from five different texts (four books and one portal) in
the Croatian language. After preparing the data and network construction, we
perform the network motif analysis. We analyse the motif frequencies and
Z-scores in the five networks. We present the triad significance profile for
five datasets. Furthermore, we compare our results with the existing results
for the linguistic networks. Firstly, we show that the triad significance
profile for the Croatian language is very similar with the other languages and
all the networks belong to the same family of networks. However, there are
certain differences between the Croatian language and other analysed languages.
We conclude that this is due to the free word-order of the Croatian language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4972</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4972</id><created>2014-11-15</created><authors><author><keyname>Liao</keyname><forenames>Hao</forenames></author><author><keyname>Zeng</keyname><forenames>An</forenames></author><author><keyname>Zhang</keyname><forenames>Yi-Cheng</forenames></author></authors><title>Towards an objective ranking in online reputation systems: the effect of
  the rating projection</title><categories>cs.IR</categories><comments>6 pages, 4 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online reputation systems are commonly used by e-commerce providers nowadays.
In order to generate an objective ranking of online items' quality according to
users' ratings, many sophisticated algorithms have been proposed in the
literature. In this paper, instead of proposing new algorithms we focus on a
more fundamental problem: the rating projection. The basic idea is that even
though the rating values given by users are linearly separated, the real
preference of users to items between different values gave is nonlinear. We
thus design an approach to project the original ratings of users to more
representative values. This approach can be regarded as a data pretreatment
method. Simulation in both artificial and real networks shows that the
performance of the ranking algorithms can be improved when the projected
ratings are used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.4982</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.4982</id><created>2014-11-18</created><updated>2015-08-01</updated><authors><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>Sample(x)=(a*x&lt;=t) is a distinguisher with probability 1/8</title><categories>cs.DS</categories><comments>To appear in different format in the proceedings of FOCS'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A random sampling function Sample:U-&gt;{0,1} for a key universe U is a
distinguisher with probability p if for any given assignment of values v(x) to
the keys x in U, including at least one non-zero v(x)!=0, the sampled sum sum{
v(x) | x in U and Sample(x) } is non-zero with probability at least p. Here the
key values may come from any commutative monoid (addition is commutative and
associative and zero is neutral). Such distinguishers were introduced by
Vazirani [PhD thesis 1986], and Naor and Naor used them for their small bias
probability spaces [STOC'90]. Constant probability distinguishers are used for
testing in contexts where the key values are not computed directly, yet where
the sum is easily computed. A simple example is when we get a stream of key
value pairs (x_1,v_1),(x_2,v_2),...,(x_n,v_n) where the same key may appear
many times. The accumulated value of key x is v(x)=sum{v_i | x_i=x}. For space
reasons, we may not be able to maintain v(x) for every key x, but the sampled
sum is easily maintained as the single value sum{v_i | Sample(x_i)}. Here we
show that when dealing with w-bit integers, if a is a uniform odd w-bit integer
and t is a uniform w-bit integer, then Sample(x)=[ax mod 2^w &lt;= t] is a
distinguisher with probability 1/8. Working with standard units, that is, w=8,
16, 32, 64, we exploit that w-bit multiplication works modulo 2^w, discarding
overflow automatically, and then the sampling decision is implemented by the
C-code a*x&lt;=t. Previous such samplers were much less computer friendly, e.g.,
the distinguisher of Naor and Naor [STOC'90] was more complicated and involved
a 7-independent hash function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5003</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5003</id><created>2014-11-18</created><updated>2015-05-18</updated><authors><author><keyname>Goel</keyname><forenames>Utkarsh</forenames></author><author><keyname>Wittie</keyname><forenames>Mike P.</forenames></author><author><keyname>Claffy</keyname><forenames>Kimberly C.</forenames></author><author><keyname>Le</keyname><forenames>Andrew</forenames></author></authors><title>Survey of End-to-End Mobile Network Measurement Testbeds, Tools, and
  Services</title><categories>cs.NI</categories><comments>Submitted to IEEE Communications Surveys and Tutorials. arXiv does
  not format the URL references correctly. For a correctly formatted version of
  this paper go to
  http://www.cs.montana.edu/mwittie/publications/Goel14Survey.pdf</comments><acm-class>C.2.1; C.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile (cellular) networks enable innovation, but can also stifle it and lead
to user frustration when network performance falls below expectations. As
mobile networks become the predominant method of Internet access, developer,
research, network operator, and regulatory communities have taken an increased
interest in measuring end-to-end mobile network performance to, among other
goals, minimize negative impact on application responsiveness. In this survey
we examine current approaches to end-to-end mobile network performance
measurement, diagnosis, and application prototyping. We compare available tools
and their shortcomings with respect to the needs of researchers, developers,
regulators, and the public. We intend for this survey to provide a
comprehensive view of currently active efforts and some auspicious directions
for future work in mobile network measurement and mobile application
performance evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5005</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5005</id><created>2014-11-18</created><updated>2014-11-24</updated><authors><author><keyname>Oprea</keyname><forenames>Alina</forenames></author><author><keyname>Li</keyname><forenames>Zhou</forenames></author><author><keyname>Yen</keyname><forenames>Ting-Fang</forenames></author><author><keyname>Chin</keyname><forenames>Sang</forenames></author><author><keyname>Alrwais</keyname><forenames>Sumayah</forenames></author></authors><title>Detection of Early-Stage Enterprise Infection by Mining Large-Scale Log
  Data</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have seen the rise of more sophisticated attacks including
advanced persistent threats (APTs) which pose severe risks to organizations and
governments by targeting confidential proprietary information. Additionally,
new malware strains are appearing at a higher rate than ever before. Since many
of these malware are designed to evade existing security products, traditional
defenses deployed by most enterprises today, e.g., anti-virus, firewalls,
intrusion detection systems, often fail at detecting infections at an early
stage.
  We address the problem of detecting early-stage infection in an enterprise
setting by proposing a new framework based on belief propagation inspired from
graph theory. Belief propagation can be used either with &quot;seeds&quot; of compromised
hosts or malicious domains (provided by the enterprise security operation
center -- SOC) or without any seeds. In the latter case we develop a detector
of C&amp;C communication particularly tailored to enterprises which can detect a
stealthy compromise of only a single host communicating with the C&amp;C server.
  We demonstrate that our techniques perform well on detecting enterprise
infections. We achieve high accuracy with low false detection and false
negative rates on two months of anonymized DNS logs released by Los Alamos
National Lab (LANL), which include APT infection attacks simulated by LANL
domain experts. We also apply our algorithms to 38TB of real-world web proxy
logs collected at the border of a large enterprise. Through careful manual
investigation in collaboration with the enterprise SOC, we show that our
techniques identified hundreds of malicious domains overlooked by
state-of-the-art security products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5007</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5007</id><created>2014-11-18</created><authors><author><keyname>Waugh</keyname><forenames>Kevin</forenames></author><author><keyname>Bagnell</keyname><forenames>J. Andrew</forenames></author></authors><title>A Unified View of Large-scale Zero-sum Equilibrium Computation</title><categories>cs.AI cs.GT</categories><comments>AAAI Workshop on Computer Poker and Imperfect Information</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of computing approximate Nash equilibria in large zero-sum
extensive-form games has received a tremendous amount of attention due mainly
to the Annual Computer Poker Competition. Immediately after its inception, two
competing and seemingly different approaches emerged---one an application of
no-regret online learning, the other a sophisticated gradient method applied to
a convex-concave saddle-point formulation. Since then, both approaches have
grown in relative isolation with advancements on one side not effecting the
other. In this paper, we rectify this by dissecting and, in a sense, unify the
two views.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5010</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5010</id><created>2014-11-18</created><authors><author><keyname>Stein</keyname><forenames>Noah D.</forenames></author></authors><title>Nonnegative Tensor Factorization for Directional Blind Audio Source
  Separation</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We augment the nonnegative matrix factorization method for audio source
separation with cues about directionality of sound propagation. This improves
separation quality greatly and removes the need for training data, but doubles
the computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5014</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5014</id><created>2014-11-18</created><authors><author><keyname>Gupta</keyname><forenames>Shubhanshu</forenames></author></authors><title>Music Data Analysis: A State-of-the-art Survey</title><categories>cs.DB cs.LG cs.SD</categories><comments>10 pages, 6 figures</comments><msc-class>97M80</msc-class><acm-class>H.5.5; J.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Music accounts for a significant chunk of interest among various online
activities. This is reflected by wide array of alternatives offered in music
related web/mobile apps, information portals, featuring millions of artists,
songs and events attracting user activity at similar scale. Availability of
large scale structured and unstructured data has attracted similar level of
attention by data science community. This paper attempts to offer current
state-of-the-art in music related analysis. Various approaches involving
machine learning, information theory, social network analysis, semantic web and
linked open data are represented in the form of taxonomy along with data
sources and use cases addressed by the research community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5050</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5050</id><created>2014-11-18</created><authors><author><keyname>Elbassioni</keyname><forenames>Khaled</forenames></author><author><keyname>Nguyen</keyname><forenames>Trung Thanh</forenames></author></authors><title>Approximation Schemes for Binary Quadratic Programming Problems with Low
  cp-Rank Decompositions</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary quadratic programming problems have attracted much attention in the
last few decades due to their potential applications. This type of problems are
NP-hard in general, and still considered a challenge in the design of efficient
approximation algorithms for their solutions. The purpose of this paper is to
investigate the approximability for a class of such problems where the
constraint matrices are {\it completely positive} and have low {\it cp-rank}.
In the first part of the paper, we show that a completely positive rational
factorization of such matrices can be computed in polynomial time, within any
desired accuracy. We next consider binary quadratic programming problems of the
following form: Given matrices $Q_1,...,Q_n\in\mathbb{R}_+^{n\times n}$, and a
system of $m$ constrains $x^TQ_ix\le C_i^2$ ($x^TQ_ix\ge C_i^2$), $i=1,...,m$,
we seek to find a vector $x^*\in \{0,1\}^n$ that maximizes (minimizes) a given
function $f$. This class of problems generalizes many fundamental problems in
discrete optimization such as packing and covering integer programs/knapsack
problems, quadratic knapsack problems, submodular maximization, etc. We
consider the case when $m$ and the cp-ranks of the matrices $Q_i$ are bounded
by a constant.
  Our approximation results for the maximization problem are as follows. For
the case when the objective function is nonnegative submodular, we give an
$(1/4-\epsilon)$-approximation algorithm, for any $\epsilon&gt;0$; when the
function $f$ is linear, we present a PTAS. We next extend our PTAS result to a
wider class of non-linear objective functions including quadratic functions,
multiplicative functions, and sum-of-ratio functions. The minimization problem
seems to be much harder due to the fact that the relaxation is {\it not}
convex. For this case, we give a QPTAS for $m=1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5053</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5053</id><created>2014-11-18</created><authors><author><keyname>Red'ko</keyname><forenames>Vladimir G.</forenames></author></authors><title>Model of Interaction between Learning and Evolution</title><categories>cs.NE</categories><comments>18 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The model of interaction between learning and evolutionary optimization is
designed and investigated. The evolving population of modeled organisms is
considered. The mechanism of the genetic assimilation of the acquired features
during a number of generations of Darwinian evolution is studied. It is shown
that the genetic assimilation takes place as follows: phenotypes of modeled
organisms move towards the optimum at learning; then the selection takes place;
genotypes of selected organisms also move towards the optimum. The hiding
effect is also studied; this effect means that strong learning can inhibit the
evolutionary search for the optimal genotype. The mechanism of influence of the
learning load on the interaction between learning and evolution is analyzed. It
is shown that the learning load can lead to a significant acceleration of
evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5057</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5057</id><created>2014-11-18</created><updated>2015-04-28</updated><authors><author><keyname>Chen</keyname><forenames>Chen</forenames></author><author><keyname>Huang</keyname><forenames>Junzhou</forenames></author><author><keyname>He</keyname><forenames>Lei</forenames></author><author><keyname>Li</keyname><forenames>Hongsheng</forenames></author></authors><title>Fast Iteratively Reweighted Least Squares Algorithms for Analysis-Based
  Sparsity Reconstruction</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel algorithm for analysis-based sparsity
reconstruction. It can solve the generalized problem by structured sparsity
regularization with an orthogonal basis and total variation regularization. The
proposed algorithm is based on the iterative reweighted least squares (IRLS)
model, which is further accelerated by the preconditioned conjugate gradient
method. The convergence rate of the proposed algorithm is almost the same as
that of the traditional IRLS algorithms, that is, exponentially fast. Moreover,
with the specifically devised preconditioner, the computational cost for each
iteration is significantly less than that of traditional IRLS algorithms, which
enables our approach to handle large scale problems. In addition to the fast
convergence, it is straightforward to apply our method to standard sparsity,
group sparsity, overlapping group sparsity and TV based problems. Experiments
are conducted on a practical application: compressive sensing magnetic
resonance imaging. Extensive results demonstrate that the proposed algorithm
achieves superior performance over 14 state-of-the-art algorithms in terms of
both accuracy and computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5060</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5060</id><created>2014-11-18</created><authors><author><keyname>Garg</keyname><forenames>Jugal</forenames></author><author><keyname>Mehta</keyname><forenames>Ruta</forenames></author><author><keyname>Vazirani</keyname><forenames>Vijay V.</forenames></author><author><keyname>Yazdanbod</keyname><forenames>Sadra</forenames></author></authors><title>Leontief Exchange Markets Can Solve Multivariate Polynomial Equations,
  Yielding FIXP and ETR Hardness</title><categories>cs.CC cs.GT</categories><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show FIXP-hardness of computing equilibria in Arrow-Debreu exchange
markets under Leontief utility functions, and Arrow-Debreu markets under linear
utility functions and Leontief production sets, thereby settling these open
questions (Vazirani and Yannakakis, 2009). As corollaries, we obtain
FIXP-hardness for piecewise-linear concave (PLC) utilities and for Arrow-Debreu
markets under linear utility functions and polyhedral production sets. In all
cases, as required under FIXP, the set of instances mapped onto will admit
equilibria, i.e., will be &quot;yes&quot; instances. If all instances are under
consideration, then in all cases we prove that the problem of deciding if a
given instance admits an equilibrium is ETR-complete, where ETR is the class
Existential Theory of Reals.
  As a consequence of the results stated above, and the fact that membership in
FIXP has been established for PLC utilities, the entire computational
difficulty of Arrow-Debreu markets under PLC utility functions lies in the
Leontief utility subcase. This is perhaps the most unexpected aspect of our
result, since Leontief utilities are meant for the case that goods are perfect
complements, whereas PLC utilities are very general, capturing not only the
cases when goods are complements and substitutes, but also arbitrary
combinations of these and much more.
  The main technical part of our result is the following reduction: Given a set
'S' of simultaneous multivariate polynomial equations in which the variables
are constrained to be in a closed bounded region in the positive orthant, we
construct a Leontief exchange market 'M' which has one good corresponding to
each variable in 'S'. We prove that the equilibria of 'M', when projected onto
prices of these latter goods, are in one-to-one correspondence with the set of
solutions of the polynomials. This reduction is related to a classic result of
Sonnenschein (1972-73).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5065</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5065</id><created>2014-11-18</created><updated>2015-01-01</updated><authors><author><keyname>Chen</keyname><forenames>Chen</forenames></author><author><keyname>Li</keyname><forenames>Yeqing</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Huang</keyname><forenames>Junzhou</forenames></author></authors><title>SIRF: Simultaneous Image Registration and Fusion in A Unified Framework</title><categories>cs.CV</categories><doi>10.1109/TIP.2015.2456415</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel method for image fusion with a
high-resolution panchromatic image and a low-resolution multispectral image at
the same geographical location. The fusion is formulated as a convex
optimization problem which minimizes a linear combination of a least-squares
fitting term and a dynamic gradient sparsity regularizer. The former is to
preserve accurate spectral information of the multispectral image, while the
latter is to keep sharp edges of the high-resolution panchromatic image. We
further propose to simultaneously register the two images during the fusing
process, which is naturally achieved by virtue of the dynamic gradient sparsity
property. An efficient algorithm is then devised to solve the optimization
problem, accomplishing a linear computational complexity in the size of the
output image in each iteration. We compare our method against seven
state-of-the-art image fusion methods on multispectral image datasets from four
satellites. Extensive experimental results demonstrate that the proposed method
substantially outperforms the others in terms of both spatial and spectral
qualities. We also show that our method can provide high-quality products from
coarsely registered real-world datasets. Finally, a MATLAB implementation is
provided to facilitate future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5077</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5077</id><created>2014-11-18</created><authors><author><keyname>Shoaib</keyname><forenames>Yasir</forenames></author><author><keyname>Das</keyname><forenames>Olivia</forenames></author></authors><title>Performance-oriented Cloud Provisioning: Taxonomy and Survey</title><categories>cs.DC</categories><comments>14 pages, 3 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is being viewed as the technology of today and the future.
Through this paradigm, the customers gain access to shared computing resources
located in remote data centers that are hosted by cloud providers (CP). This
technology allows for provisioning of various resources such as virtual
machines (VM), physical machines, processors, memory, network, storage and
software as per the needs of customers. Application providers (AP), who are
customers of the CP, deploy applications on the cloud infrastructure and then
these applications are used by the end-users. To meet the fluctuating
application workload demands, dynamic provisioning is essential and this
article provides a detailed literature survey of dynamic provisioning within
cloud systems with focus on application performance. The well-known types of
provisioning and the associated problems are clearly and pictorially explained
and the provisioning terminology is clarified. A very detailed and general
cloud provisioning classification is presented, which views provisioning from
different perspectives, aiding in understanding the process inside-out. Cloud
dynamic provisioning is explained by considering resources, stakeholders,
techniques, technologies, algorithms, problems, goals and more.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5078</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5078</id><created>2014-11-18</created><authors><author><keyname>Vo-Huu</keyname><forenames>Triet D.</forenames></author><author><keyname>Noubir</keyname><forenames>Guevara</forenames></author></authors><title>CBM: A Crypto-Coded Modulation Scheme for Rate Information Concealing
  and Robustness Boosting</title><categories>cs.CR</categories><acm-class>C.2.0; C.2.1; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exposing the rate information of wireless transmission enables highly
efficient attacks that can severely degrade the performance of a network at
very low cost. In this paper, we introduce an integrated solution to conceal
the rate information of wireless transmissions while simultaneously boosting
the resiliency against interference. The proposed solution is based on a
generalization of Trellis Coded Modulation combined with Cryptographic
Interleaving. We develop algorithms for discovering explicit codes for
concealing any modulation in {BPSK, QPSK, 8-PSK, 16-QAM, 64-QAM}. We
demonstrate that in most cases this modulation hiding scheme has the side
effect of boosting resiliency by up to 8.5dB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5082</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5082</id><created>2014-11-18</created><updated>2015-05-13</updated><authors><author><keyname>Le</keyname><forenames>Dan</forenames></author><author><keyname>Niu</keyname><forenames>Xiamu</forenames></author></authors><title>An Online Decoding Schedule Generating Algorithm for Successive
  Cancellation Decoder of Polar Codes</title><categories>cs.IT math.IT</categories><comments>16 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Successive cancellation (SC) is the first and widely known decoder of polar
codes, which has received a lot of attentions recently. However, its decoding
schedule generating algorithms are still primitive, which are not only complex
but also offline. This paper proposes a simple and online algorithm to generate
the decoding schedule of SC decoder. Firstly, the dependencies among likelihood
ratios (LR) are explored, which lead to the discovery of a sharing factor.
Secondly, based on the online calculation of the sharing factor, the proposed
algorithm is presented, which is neither based on the depth-first traversal of
the scheduling tree nor based on the recursive construction. As shown by the
comparisons among the proposed algorithm and existed algorithms, the proposed
algorithm has advantages of the online feature and the far less memory taken by
the decoding schedule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5098</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5098</id><created>2014-11-18</created><authors><author><keyname>Meric</keyname><forenames>Hugo</forenames></author></authors><title>Optimal DVB-S2 spectral efficiency with hierarchical modulation</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted for publication. arXiv admin note: text overlap with
  arXiv:1310.0677</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the design of a DVB-S2 system in order to maximise spectral
efficiency. This task is usually challenging due to channel variability. The
solution adopted in modern satellite communications systems such as DVB-SH and
DVB-S2 relies mainly on a time sharing strategy. Recently, we proposed to
combine time sharing with hierarchical modulation to increase the transmission
rate of broadcast systems. However, the optimal spectral efficiency remained an
open question. In this paper, we show that the optimal transmission rate is the
solution of a linear programming problem. We also study the performance of the
optimal scheme for a DVB-S2 use case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5102</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5102</id><created>2014-11-18</created><updated>2015-03-07</updated><authors><author><keyname>Ahuja</keyname><forenames>Kartik</forenames></author><author><keyname>Xiao</keyname><forenames>Yuanzhang</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Distributed Interference Management Policies for Heterogeneous Small
  Cell Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of interference management in large-scale small cell
networks, where each user equipment (UE) needs to determine in a distributed
manner when and at what power level it should transmit to its serving small
cell base station (SBS) such that a given network performance criterion is
maximized subject to minimum quality of service (QoS) requirements by the UEs.
We first propose a distributed algorithm for the UE-SBS pairs to find a subset
of weakly interfering UE-SBS pairs, namely the maximal independent sets (MISs)
of the interference graph in logarithmic time (with respect to the number of
UEs). Then we propose a novel problem formulation which enables UE-SBS pairs to
determine the optimal fractions of time occupied by each MIS in a distributed
manner. We analytically bound the performance of our distributed policy in
terms of the competitive ratio with respect to the optimal network performance,
which is obtained in a centralized manner with NP (non-deterministic polynomial
time) complexity. Remarkably, the competitive ratio is independent of the
network size, which guarantees scalability in terms of performance for
arbitrarily large networks. Through simulations, we show that our proposed
policies achieve significant performance improvements (from 150% to 700%) over
the existing policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5107</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5107</id><created>2014-11-18</created><authors><author><keyname>Ahuja</keyname><forenames>Kartik</forenames></author><author><keyname>Zhang</keyname><forenames>Simpson</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Towards a Theory of Societal Co-Evolution: Individualism versus
  Collectivism</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Substantial empirical research has shown that the level of individualism vs.
collectivism is one of the most critical and important determinants of societal
traits, such as economic growth, economic institutions and health conditions.
But the exact nature of this impact has thus far not been well understood in an
analytical setting. In this work, we develop one of the first theoretical
models that analytically studies the impact of individualism-collectivism on
the society. We model the growth of an individual's welfare (wealth, resources
and health) as depending not only on himself, but also on the level of
collectivism, i.e. the level of dependence on the rest of the individuals in
the society, which leads to a co-evolutionary setting. Based on our model, we
are able to predict the impact of individualism-collectivism on various
societal metrics, such as average welfare, average life-time, total population,
cumulative welfare and average inequality. We analytically show that
individualism has a positive impact on average welfare and cumulative welfare,
but comes with the drawbacks of lower average life-time, lower total population
and higher average inequality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5110</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5110</id><created>2014-11-18</created><authors><author><keyname>Kwon</keyname><forenames>Keehang</forenames></author></authors><title>Mutually Exclusive Procedures in Imperative Languages</title><categories>cs.PL</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To represent mutually exclusive procedures, we propose a choice-conjunctive
declaration statement of the form $uchoo(S,R)$ where $S, R$ are the procedure
declaration statements within a module. This statement has the following
semantics: request the machine to choose a successful one between $S$ and $R$.
This statement is useful for representing objects with mutually exclusive
procedures. We illustrate our idea via C^uchoo, an extension of the core C with
a new statement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5118</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5118</id><created>2014-11-19</created><updated>2014-12-08</updated><authors><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Xu</keyname><forenames>Baowen</forenames></author><author><keyname>Wu</keyname><forenames>Yurong</forenames></author><author><keyname>Zhou</keyname><forenames>Xiaoyu</forenames></author></authors><title>Link Prediction in Social Networks: the State-of-the-Art</title><categories>cs.SI physics.soc-ph</categories><comments>38 pages, 13 figures, Science China: Information Science, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In social networks, link prediction predicts missing links in current
networks and new or dissolution links in future networks, is important for
mining and analyzing the evolution of social networks. In the past decade, many
works have been done about the link prediction in social networks. The goal of
this paper is to comprehensively review, analyze and discuss the
state-of-the-art of the link prediction in social networks. A systematical
category for link prediction techniques and problems is presented. Then link
prediction techniques and problems are analyzed and discussed. Typical
applications of link prediction are also addressed. Achievements and roadmaps
of some active research groups are introduced. Finally, some future challenges
of the link prediction in social networks are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5121</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5121</id><created>2014-11-19</created><authors><author><keyname>K&#xf6;ppe</keyname><forenames>Matthias</forenames></author><author><keyname>Zhou</keyname><forenames>Yuan</forenames></author></authors><title>An Electronic Compendium of Extreme Functions for the Gomory--Johnson
  Infinite Group Problem</title><categories>math.OC cs.DM</categories><comments>14 pages, 4 figures</comments><msc-class>90C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we announce the availability of an electronic compendium of
extreme functions for Gomory--Johnson's infinite group problem. These functions
serve as the strongest cut-generating functions for integer linear optimization
problems. We also close several gaps in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5123</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5123</id><created>2014-11-19</created><updated>2015-12-02</updated><authors><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>Deterministic Edge Connectivity in Near-Linear Time</title><categories>cs.DS cs.DM</categories><comments>Biggest difference is that we changed the title from previous
  &quot;Deterministic Global Minimum Cut of a Simple Graph in Near-Linear Time&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deterministic near-linear time algorithm that computes the
edge-connectivity and finds a minimum cut for a simple undirected unweighted
graph G with n vertices and m edges. This is the first o(mn) time deterministic
algorithm for the problem. In near-linear time we can also construct the
classic cactus representation of all minimum cuts.
  The previous fastest deterministic algorithm by Gabow from STOC'91 took
~O(m+k^2 n), where k is the edge connectivity, but k could be Omega(n).
  At STOC'96 Karger presented a randomized near linear time Monte Carlo
algorithm for the minimum cut problem. As he points out, there is no better way
of certifying the minimality of the returned cut than to use Gabow's slower
deterministic algorithm and compare sizes.
  Our main technical contribution is a near-linear time algorithm that contract
vertex sets of a simple input graph G with minimum degree d, producing a
multigraph with ~O(m/d) edges which preserves all minimum cuts of G with at
least 2 vertices on each side.
  In our deterministic near-linear time algorithm, we will decompose the
problem via low-conductance cuts found using PageRank a la Brin and Page
(1998), as analyzed by Andersson, Chung, and Lang at FOCS'06. Normally such
algorithms for low-conductance cuts are randomized Monte Carlo algorithms,
because they rely on guessing a good start vertex. However, in our case, we
have so much structure that no guessing is needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5125</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5125</id><created>2014-11-19</created><updated>2015-03-26</updated><authors><author><keyname>Zgonnikov</keyname><forenames>Arkady</forenames></author><author><keyname>Lubashevsky</keyname><forenames>Ihor</forenames></author></authors><title>Bistable dynamics of control activation in human intermittent control</title><categories>physics.bio-ph cs.SY nlin.AO</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When facing a task of balancing a dynamic system near an unstable
equilibrium, humans often adopt intermittent control strategy: instead of
continuously controlling the system, they repeatedly switch the control on and
off. Paradigmatic example of such a task is stick balancing. Despite the
simplicity of the task itself, the complexity of human intermittent control
dynamics in stick balancing still puzzles researchers in motor control. Here we
attempt to model one of the key mechanisms of human intermittent control,
control activation, using as an example the task of overdamped stick balancing.
In so doing, we focus on the concept of noise-driven activation, a more general
alternative to the conventional threshold-driven activation. We describe
control activation as a random walk in an energy potential, which changes in
response to the state of the controlled system. By way of numerical
simulations, we show that the developed model captures the core properties of
human control activation observed previously in the experiments on overdamped
stick balancing. Our results demonstrate that the double-well potential model
provides tractable mathematical description of human control activation at
least in the considered task, and suggest that the adopted approach can
potentially aid in understanding human intermittent control in more complex
processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5127</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5127</id><created>2014-11-19</created><updated>2014-11-20</updated><authors><author><keyname>Stiffelman</keyname><forenames>Oscar</forenames></author></authors><title>PivotCompress: Compression by Sorting</title><categories>cs.DS</categories><comments>preprint, compression by sorting, quicksort as universal code; this
  version describes the permutation vector and its inverse</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sorted data is usually easier to compress than unsorted permutations of the
same data. This motivates a simple compression scheme: specify the sorted
permutation of the data along with a representation of the sorted data
compressed recursively. The sorted permutation can be specified by recording
the decisions made by quicksort. If the size of the data is known, then the
quicksort decisions describe the data at a rate that is nearly as efficient as
the minimal prefix-free code for the distribution, which is bounded by the
entropy of the distribution. This is possible even though the distribution is
unknown ahead of time. Used in this way, quicksort acts as a universal code in
that it is asymptotically optimal for any stationary source. The Shannon
entropy is a lower bound when describing stochastic, independent symbols.
However, it is possible to encode non-uniform, finite strings below the entropy
of the sample distribution by also encoding symbol counts because the values in
the sequence are no longer independent once the counts are known. The key
insight is that sparse quicksort comparison vectors can also be compressed to
achieve an even lower rate when data is highly non-uniform while incurring only
a modest penalty when data is random.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5131</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5131</id><created>2014-11-19</created><authors><author><keyname>Gange</keyname><forenames>Graeme</forenames></author><author><keyname>Navas</keyname><forenames>Jorge A.</forenames></author><author><keyname>Schachte</keyname><forenames>Peter</forenames></author><author><keyname>Sondergaard</keyname><forenames>Harald</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter J.</forenames></author></authors><title>A Complete Refinement Procedure for Regular Separability of Context-Free
  Languages</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Often, when analyzing the behaviour of systems modelled as context-free
languages, we wish to know if two languages overlap. To this end, we present an
effective semi-decision procedure for regular separability of context-free
languages, based on counter-example guided abstraction refinement. We propose
two refinement methods, one inexpensive but incomplete, and the other complete
but more expensive. We provide an experimental evaluation of this procedure,
and demonstrate its practicality on a range of verification and
language-theoretic instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5132</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5132</id><created>2014-11-19</created><authors><author><keyname>Randrianantenaina</keyname><forenames>Itsikiantsoa</forenames></author><author><keyname>Benjillali</keyname><forenames>Mustapha</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Consumption Factor Optimization for Multihop Relaying over Nakagami-m
  Fading channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the energy efficiency of multihop relaying over Nakagami-$m$
fading channels is investigated. The &quot;consumption factor&quot; is used as a metric
to evaluate the energy efficiency, and it is derived in closed-form for both
amplify-and-forward and decode-and-forward relaying. Then, based on the
obtained expressions, we propose a power allocation strategy maximizing the
consumption factor. In addition, two sub-optimal, low complexity, power
allocation algorithms are proposed and analyzed, and the obtained power
allocation schemes are compared, in terms of energy efficiency as well as other
common performance metrics, to other power allocation schemes from the
literature. Analytical and simulation results confirm the accuracy of our
derivations, and assess the performance gains of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5137</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5137</id><created>2014-11-19</created><authors><author><keyname>Vasave</keyname><forenames>Sandeep</forenames></author><author><keyname>Plave</keyname><forenames>Amol</forenames></author></authors><title>Study of Gesture Recognition methods and augmented reality</title><categories>cs.HC</categories><comments>7 PAGES, 5 FUGURES</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  With the growing technology, we humans always need something that stands out
from the other thing. Gestures are most desirable source to Communicate with
the Machines. Human Computer Interaction finds its importance when it comes to
working with the Human gestures to control the computer applications. Usually
we control the applications using mouse, keyboard, laser pointers etc. but,
with the recent advent in the technology it has even left behind their usage by
introducing more efficient techniques to control applications. There are many
Gesture Recognition techniques that have been implemented using image
processing in the past.
  However recognizing the gestures in the noisy background has always been a
difficult task to achieve. In the proposed system, we are going to use one such
technique called Augmentation in Image processing to control Media Player. We
will recognize Gestures using which we are going to control the operations on
Media player. Augmentation usually is one step ahead when it comes to virtual
reality. It has no restrictions on the background. Moreover it also does not
rely on certain things like gloves, color pointers etc. for recognizing the
gesture. This system mainly appeals to those users who always looks out for a
better option that makes their interaction with computer more simpler or
easier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5140</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5140</id><created>2014-11-19</created><authors><author><keyname>Wang</keyname><forenames>Qian</forenames></author><author><keyname>Zhang</keyname><forenames>Jiaxing</forenames></author><author><keyname>Song</keyname><forenames>Sen</forenames></author><author><keyname>Zhang</keyname><forenames>Zheng</forenames></author></authors><title>Attentional Neural Network: Feature Selection Using Cognitive Feedback</title><categories>cs.CV cs.NE</categories><comments>Poster in Neural Information Processing Systems (NIPS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attentional Neural Network is a new framework that integrates top-down
cognitive bias and bottom-up feature extraction in one coherent architecture.
The top-down influence is especially effective when dealing with high noise or
difficult segmentation problems. Our system is modular and extensible. It is
also easy to train and cheap to run, and yet can accommodate complex behaviors.
We obtain classification accuracy better than or competitive with state of art
results on the MNIST variation dataset, and successfully disentangle overlaid
digits with high success rates. We view such a general purpose framework as an
essential foundation for a larger system emulating the cognitive abilities of
the whole brain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5153</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5153</id><created>2014-11-19</created><authors><author><keyname>Berrouk</keyname><forenames>Walid</forenames></author><author><keyname>Aissaoui</keyname><forenames>Ouanes</forenames></author></authors><title>A Rewriting Logic Approach for Automatic Composition of Web Services</title><categories>cs.SE</categories><comments>9 pages, 10 figures</comments><acm-class>D.2.0</acm-class><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 11,
  Issue 5, No 1, pp.41-49, September 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, Web Services (WS) remain a main actor in the implementation of
distributed applications. They represent a new promising paradigm for the
development, deployment and integration of Internet applications. These
services are in most cases unable to provide the required functionality; they
must be composed to provide appropriate services, richer and more interesting
for other applications as well as for human users. The composition of Web
services is considered as a strong point, which allows answering complex
queries by combining the functionality of multiple services within a same
composition. In this work we showed how the formalism of graphs can be used to
improve the composition of web services and make it automatic. We have proposed
the rewriting logic and its language Maude as a support for a graph-based
approach to automatic composition of web services. The proposed model has made
possible the exploration of different composition schemas as well as the formal
analysis of service compositions. The paper introduces a case study showing how
to apply our formalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5161</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5161</id><created>2014-11-19</created><authors><author><keyname>Mutiara</keyname><forenames>A. B.</forenames></author><author><keyname>Refianti</keyname><forenames>R.</forenames></author><author><keyname>Witono</keyname><forenames>B. A.</forenames></author></authors><title>Developing a SAAS-Cloud Integrated Development Environment (IDE) for C,
  C++, and Java</title><categories>cs.SE</categories><comments>9 pages. arXiv admin note: text overlap with arXiv:1410.1159,
  arXiv:1304.4047 by other authors</comments><acm-class>D.2.2</acm-class><journal-ref>Journal of Theoretical and Applied Information Technology, Vol. 68
  No.1, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud era brought revolution of computerization world. People could access
their data from anywhere and anytime with different devices. One of the cloud's
model is Software as a Service, which capable to provide applications that run
on a cloud infrastructure.An IDE (Integrated Development Environment) is the
most popular tool to develop application in the network or single computer
development. By installing IDE in each computer of the network could causes the
lot of time and budget spending. The objective of the research is developing an
efficient cloud based IDE. The IDE could compile the code which sent from
client browser through SaaS IDE to the server and send it back to the client.
The method that used in the research is the System Development Life-Cycle:
Waterfall and Unified Model Language as system designing tool. The research
successfully produced the cloud-based SaaS IDE with excellent result from
several testing in local network and internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5166</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5166</id><created>2014-11-19</created><updated>2014-12-08</updated><authors><author><keyname>AbdelGawad</keyname><forenames>Moez A.</forenames></author></authors><title>Subtyping in Java with Generics and Wildcards is a Fractal</title><categories>cs.PL math.GT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For helping themselves in writing, debugging and maintaining their software,
professional OO software developers keep in their minds an image of the
subtyping relation between types in their software while they are developing
their software. In pre-generics Java, the structure of the subtyping mental
image was simple: the graph of the subtyping relation between classes and
interfaces was a directed-acyclic graph, and the graph of the subtyping
relation between classes alone was simply a tree. This fact about the graph of
the subtyping relation applied not only to Java but, more generally, also to
the non-generic sublanguage of nominally-typed OO languages similar to Java,
such as C#, C++, and Scala.
  The goal of this casual essay is to present and defend, even if incompletely
and not quite rigorously, a hunch and intuition the author had years ago about
the graph of the subtyping relation in Java after generics were added to it.
The author observed that: after the addition of generics---and of wildcards in
particular---to Java, the graph of the subtyping relation is still a DAG, but
no longer a simple DAG but rather one whose structure can be better understood
as a /fractal/. Today, generics and wildcards (or some other form of `variance
annotations') are a standard feature of mainstream nominally-typed OO
languages. Accordingly, the shape of the subtyping relation in nominally-typed
OO languages is more complex than a tree or a simple DAG. Given the popularity
of fractals, the fractals observation may help OO software developers keep a
useful and intuitive mental image of their software's subtyping relation, even
if it is one a little more frightening, and amazing, than before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5172</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5172</id><created>2014-11-19</created><authors><author><keyname>Heinonen</keyname><forenames>Markus</forenames></author><author><keyname>d'Alch&#xe9;-Buc</keyname><forenames>Florence</forenames></author></authors><title>Learning nonparametric differential equations with operator-valued
  kernels and gradient matching</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling dynamical systems with ordinary differential equations implies a
mechanistic view of the process underlying the dynamics. However in many cases,
this knowledge is not available. To overcome this issue, we introduce a general
framework for nonparametric ODE models using penalized regression in
Reproducing Kernel Hilbert Spaces (RKHS) based on operator-valued kernels.
Moreover, we extend the scope of gradient matching approaches to nonparametric
ODE. A smooth estimate of the solution ODE is built to provide an approximation
of the derivative of the ODE solution which is in turn used to learn the
nonparametric ODE model. This approach benefits from the flexibility of
penalized regression in RKHS allowing for ridge or (structured) sparse
regression as well. Very good results are shown on 3 different ODE systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5173</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5173</id><created>2014-11-19</created><authors><author><keyname>Kelif</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Senecal</keyname><forenames>Stephane</forenames></author><author><keyname>Coupechoux</keyname><forenames>Marceau</forenames></author><author><keyname>Bridon</keyname><forenames>Constant</forenames></author></authors><title>Analytical Performance Model for Poisson Wireless Networks with Pathloss
  and Shadowing Propagation</title><categories>cs.NI</categories><comments>5 pages, 5 figures, Globecom 2014 Workshop WONC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The SINR (signal to interference plus noise ratio) is a key factor for
wireless networks analysis. Indeed, the SINR distribution allows the derivation
of performance and quality of service (QoS) evaluation. Moreover, it also
enables the analysis of radio resources allocation and scheduling policies,
since they depend on the SINR reached by a UE (User Equipment). Therefore, it
is particularly interesting to develop an analytical method which allows to
evaluate the SINR, in a simple and quick way, for a realistic environment.
Considering a stochastic Poisson network model, we establish the CDF
(cumulative distributed function) of the SINR. We show that the shadowing can
be neglected, in many cases, as long as mobiles are connected to their best
serving base station (BS), i.e. the BS which offers them the most powerful
useful signal. As a consequence, the analysis of performance and quality of
service, directly derived from the CDF of SINR, can be established by using a
propagation model which takes into account only the pathloss. Moreover, we
establish that the Fluid network model we have proposed can be used to analyze
stochastic Poisson distributed network. Therefore, the analysis of stochastic
Poisson network can be done in an easy and quick way, by using the analytical
expression of the SINR established thanks to the Fluid network model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5178</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5178</id><created>2014-11-19</created><authors><author><keyname>Fang</keyname><forenames>Hao</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author><author><keyname>Jiang</keyname><forenames>Hai</forenames></author></authors><title>Performance Limits of Segmented Compressive Sampling: Correlated Samples
  versus Bits</title><categories>cs.IT math.IT</categories><comments>27 pages, 8 figures, Submitted to IEEE Trans. Signal Processing on
  November 2014</comments><journal-ref>IEEE Trans. Signal Processing, vol. 63, no. 22, pp. 6061-6073,
  Nov. 2015</journal-ref><doi>10.1109/TSP.2015.2463252</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives performance limits of the segmented compressive sampling
(CS) which collects correlated samples. It is shown that the effect of
correlation among samples for the segmented CS can be characterized by a
penalty term in the corresponding bounds on the sampling rate. Moreover, this
penalty term is vanishing as the signal dimension increases. It means that the
performance degradation due to the fixed correlation among samples obtained by
the segmented CS (as compared to the standard CS with equivalent size sampling
matrix) is negligible for a high-dimensional signal. In combination with the
fact that the signal reconstruction quality improves with additional samples
obtained by the segmented CS (as compared to the standard CS with sampling
matrix of the size given by the number of original uncorrelated samples), the
fact that the additional correlated samples also provide new information about
a signal is a strong argument for the segmented CS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5187</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5187</id><created>2014-11-19</created><updated>2015-12-05</updated><authors><author><keyname>Choi</keyname><forenames>Jun Won</forenames></author><author><keyname>Shim</keyname><forenames>Byonghyo</forenames></author></authors><title>Statistical Recovery of Simultaneously Sparse Time-Varying Signals from
  Multiple Measurement Vectors</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Trans. Signal Processing, vol. 63, no. 22, pp. 6136-6148,
  Nov. 2015</journal-ref><doi>10.1109/TSP.2015.2463259</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new sparse signal recovery algorithm, referred to
as sparse Kalman tree search (sKTS), that provides a robust reconstruction of
the sparse vector when the sequence of correlated observation vectors are
available. The proposed sKTS algorithm builds on expectation-maximization (EM)
algorithm and consists of two main operations: 1) Kalman smoothing to obtain
the a posteriori statistics of the source signal vectors and 2) greedy tree
search to estimate the support of the signal vectors. Through numerical
experiments, we demonstrate that the proposed sKTS algorithm is effective in
recovering the sparse signals and performs close to the Oracle (genie-based)
Kalman estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5190</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5190</id><created>2014-11-19</created><updated>2015-05-05</updated><authors><author><keyname>Malinowski</keyname><forenames>Mateusz</forenames></author><author><keyname>Fritz</keyname><forenames>Mario</forenames></author></authors><title>A Pooling Approach to Modelling Spatial Relations for Image Retrieval
  and Annotation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last two decades we have witnessed strong progress on modeling
visual object classes, scenes and attributes that have significantly
contributed to automated image understanding. On the other hand, surprisingly
little progress has been made on incorporating a spatial representation and
reasoning in the inference process. In this work, we propose a pooling
interpretation of spatial relations and show how it improves image retrieval
and annotations tasks involving spatial language. Due to the complexity of the
spatial language, we argue for a learning-based approach that acquires a
representation of spatial relations by learning parameters of the pooling
operator. We show improvements on previous work on two datasets and two
different tasks as well as provide additional insights on a new dataset with an
explicit focus on spatial relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5196</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5196</id><created>2014-11-19</created><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Bernardo</forenames></author><author><keyname>Porto</keyname><forenames>Fabio</forenames></author></authors><title>Design-theoretic encoding of deterministic hypotheses as constraints and
  correlations into U-relational databases</title><categories>cs.DB</categories><comments>17 pages, 7 figures, submitted to ACM PODS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In view of the paradigm shift that makes science ever more data-driven, in
this paper we consider deterministic scientific hypotheses as uncertain data.
In the form of mathematical equations, hypotheses symmetrically relate aspects
of the studied phenomena. For computing predictions, however, deterministic
hypotheses are used asymmetrically as functions. We refer to Simon's notion of
structural equations in order to extract the (so-called) causal ordering
embedded in a hypothesis. Then we encode it into a set of functional
dependencies (fd's) that is basic input to a design-theoretic method for the
synthesis of U-relational databases (DB's). The causal ordering captured from a
formally-specified system of mathematical equations into fd's determines not
only the constraints (structure), but also the correlations (uncertainty
chaining) hidden in the hypothesis predictive data. We show how to process it
effectively through original algorithms for encoding and reasoning on the given
hypotheses as constraints and correlations into U-relational DB's. The method
is applicable to both quantitative and qualitative hypotheses and has underwent
initial tests in a realistic use case from computational science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5197</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5197</id><created>2014-11-19</created><authors><author><keyname>Halava</keyname><forenames>Vesa</forenames></author></authors><title>Another proof of undecidability for the correspondence decision problem
  - Had I been Emil Post</title><categories>cs.LO cs.DM</categories><comments>5 pages, manuscript</comments><msc-class>03D35, 03D03, 68Q01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1946 Emil Leon Post (Bulletin of Amer. Math. Soc. 52 (1946), 264 - 268)
defined a famous correspondence decision problem which is nowadays called the
Post Correspondence Problem, and he proved that the problem is undecidable. In
this article we follow the steps of Post, and give another, simpler and more
straightforward proof of the undecidability of the problem using the same
source of reduction as Post original did, namely, the Post Normal Systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5204</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5204</id><created>2014-11-19</created><authors><author><keyname>Venerandi</keyname><forenames>Alessandro</forenames></author><author><keyname>Quattrone</keyname><forenames>Giovanni</forenames></author><author><keyname>Capra</keyname><forenames>Licia</forenames></author><author><keyname>Quercia</keyname><forenames>Daniele</forenames></author><author><keyname>Saez-Trumper</keyname><forenames>Diego</forenames></author></authors><title>Measuring Urban Deprivation from User Generated Content</title><categories>cs.SI</categories><comments>CSCW'15, March 14 - 18 2015, Vancouver, BC, Canada</comments><doi>10.1145/2675133.2675233</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring socioeconomic deprivation of cities in an accurate and timely
fashion has become a priority for governments around the world, as the massive
urbanization process we are witnessing is causing high levels of inequalities
which require intervention. Traditionally, deprivation indexes have been
derived from census data, which is however very expensive to obtain, and thus
acquired only every few years. Alternative computational methods have been
proposed in recent years to automatically extract proxies of deprivation at a
fine spatio-temporal level of granularity; however, they usually require access
to datasets (e.g., call details records) that are not publicly available to
governments and agencies.
  To remedy this, we propose a new method to automatically mine deprivation at
a fine level of spatio-temporal granularity that only requires access to freely
available user-generated content. More precisely, the method needs access to
datasets describing what urban elements are present in the physical
environment; examples of such datasets are Foursquare and OpenStreetMap. Using
these datasets, we quantitatively describe neighborhoods by means of a metric,
called {\em Offering Advantage}, that reflects which urban elements are
distinctive features of each neighborhood. We then use that metric to {\em (i)}
build accurate classifiers of urban deprivation and {\em (ii)} interpret the
outcomes through thematic analysis. We apply the method to three UK urban areas
of different scale and elaborate on the results in terms of precision and
recall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5210</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5210</id><created>2014-11-19</created><authors><author><keyname>Fu</keyname><forenames>Songwei</forenames></author><author><keyname>Zhang</keyname><forenames>Yan</forenames></author><author><keyname>Jiang</keyname><forenames>Yuming</forenames></author><author><keyname>Shih</keyname><forenames>Chia-Yen</forenames></author><author><keyname>Marron</keyname><forenames>Pedro Jose</forenames></author></authors><title>An Experimental Study Towards Understanding Data Delivery Performance
  Over a WSN Link</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of a wireless sensor network (WSN) depends fundamentally on
how its various parameters are configured under different link quality
conditions. Surprisingly, even though WSNs have been extensively researched,
there still lacks an in-depth understanding on how parameter configurations
affect, in particular jointly, the performance under different link quality
conditions. To fill the gap, this paper presents an extensive experimental
study on the performance of a wireless sensor network link, where measurement
data of more than 200 million packets were collected. Different from existing
work, we consider major parameters from different layers together and measure
their joint effects on key performance metrics under an extensive set of
parameter configurations. Based on the large amount of measurement data, we
investigate the impacts of these parameters and their joint configurations on
the performance, introduce empirical models to theoretically reason the
impacts, discuss implications of the obtained results, and suggest practical
guidelines for parameter configurations. Through these, a comprehensive
overview of parameter configuration impacts on the erformance is provided,
which provides new insights on wireless link performance in WSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5213</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5213</id><created>2014-11-19</created><authors><author><keyname>Vaibhav</keyname><forenames>Atul</forenames></author></authors><title>Security in Monitoring Schemes: A Survey</title><categories>cs.DC cs.CR cs.NI</categories><comments>Attacks, Monitoring Schemes, Aggregation, Analysis, Dissemination,
  Gossip, Tree, Hybrid, Distributed Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With our growing reliability on distributed networks, the security aspect of
such networks becomes of prime importance. In large scale distributed networks
it becomes cardinal to have an efficient and effective monitoring scheme. The
monitoring schemes supervise the node behaviour in the network and look out for
any discrepancy. Monitoring schemes comprise of monitoring components that work
together to help schemes in meeting various security requirement parameters for
the networks. These security parameters are breached via various attacks by
manipulation of monitoring components of particular monitoring schemes to
produce faulty results and thereby reducing efficiency of networks, reliability
and security. In this paper we have discussed these components of monitoring,
multiple monitoring schemes, their security parameters and various types of
attacks possible on these monitoring components by manipulating assumptions of
monitoring schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5220</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5220</id><created>2014-11-19</created><updated>2015-01-08</updated><authors><author><keyname>Zhang</keyname><forenames>Heng</forenames></author><author><keyname>Zhang</keyname><forenames>Yan</forenames></author><author><keyname>You</keyname><forenames>Jia-Huai</forenames></author></authors><title>Existential Rule Languages with Finite Chase: Complexity and
  Expressiveness</title><categories>cs.AI cs.DB cs.LO</categories><comments>Extended version of a paper to appear on AAAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finite chase, or alternatively chase termination, is an important condition
to ensure the decidability of existential rule languages. In the past few
years, a number of rule languages with finite chase have been studied. In this
work, we propose a novel approach for classifying the rule languages with
finite chase. Using this approach, a family of decidable rule languages, which
extend the existing languages with the finite chase property, are naturally
defined. We then study the complexity of these languages. Although all of them
are tractable for data complexity, we show that their combined complexity can
be arbitrarily high. Furthermore, we prove that all the rule languages with
finite chase that extend the weakly acyclic language are of the same
expressiveness as the weakly acyclic one, while rule languages with higher
combined complexity are in general more succinct than those with lower combined
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5224</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5224</id><created>2014-11-17</created><authors><author><keyname>Aur</keyname><forenames>Dorian</forenames></author></authors><title>Can we build a conscious machine?</title><categories>cs.ET cs.AI</categories><comments>15 pages, 2 figures</comments><doi>10.13140/2.1.2286.5608</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The underlying physiological mechanisms of generating conscious states are
still unknown. To make progress on the problem of consciousness, we will need
to experimentally design a system that evolves in a similar way our brains do.
Recent experimental data show that the multiscale nature of the evolving human
brain can be implemented by reprogramming human cells. A hybrid system can be
designed to include an evolving brain equipped with digital computers that
maintain homeostasis and provide the right amount of nutrients and oxygen for
the brain growth. Shaping the structure of the evolving brain will be
progressively achieved by controlling spatial organization of various types of
cells. Following a specific program, the evolving brain can be trained using
substitutional reality to learn and experience live scenes. We already know
from neuroelectrodynamics that meaningful information in the brain is
electrically (wirelessly) read out and written fast in neurons and synapses at
the molecular (protein) level during the generation of action potentials and
synaptic activities. Since with training, meaningful information accumulates
and is electrically integrated in the brain, one can predict, that this gradual
process of training will trigger a tipping point for conscious experience to
emerge in the hybrid system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5225</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5225</id><created>2014-11-19</created><authors><author><keyname>Merrouch</keyname><forenames>Farid</forenames></author><author><keyname>Hnida</keyname><forenames>Meriem</forenames></author><author><keyname>Idrissi</keyname><forenames>Mohammed Khalidi</forenames></author><author><keyname>Bennani</keyname><forenames>Samir</forenames></author></authors><title>Online placement test based on Item Response Theory and IMS Global
  standards</title><categories>cs.CY</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 11,
  Issue 5, No 2, September 2014 ISSN (Print): 1694-0814 | ISSN (Online):
  1694-0784</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to present an online placement test. It is based on the Item
Response Theory to provide relevant estimates of learner competences. The
proposed test is the entry point of our e-Learning system. It gathers the
learner response to a set of questions and uses a specific developed algorithm
to estimate its level. This algorithm identifies learning gaps, which allows
tutors to conceive sequence of courses and remediation adapted to each case of
learner, in order to achieve a competence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5228</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5228</id><created>2014-11-19</created><authors><author><keyname>Biswas</keyname><forenames>Souham</forenames></author><author><keyname>Nene</keyname><forenames>Manisha J.</forenames></author></authors><title>Hostile Intent Enumeration using Soft Computing Techniques</title><categories>cs.OH</categories><comments>Published in the International Journal for Computer Science Issues
  (IJCSI Volume 11, Issue 5, September 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In any tactical scenario, the successful quantification and triangulation of
potential hostile elements is instrumental to minimize any casualties which
might be incurred. The most commonly deployed infrastructures to cater to this
have mostly been surveillance systems which only extract some data pertaining
to the targets of interest in the area of observation and convey the
information to the human operators. Accordingly, with the ever increasing rate
at which warfare tactics are evolving, there has been a growing need for
smarter solutions to this problem of hostile intent enumeration. Recently, a
number of developments have been made to ameliorate the efficacy and the
certitude with which this task is performed. This paper discusses two of the
most prominent approaches which address this problem and posits the outline of
a novel solution which seeks to address the shortcomings faced by the existing
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5235</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5235</id><created>2014-11-19</created><updated>2015-06-04</updated><authors><author><keyname>Rhebergen</keyname><forenames>Sander</forenames></author><author><keyname>Wells</keyname><forenames>Garth N.</forenames></author><author><keyname>Wathen</keyname><forenames>Andrew J.</forenames></author><author><keyname>Katz</keyname><forenames>Richard F.</forenames></author></authors><title>Three-field block-preconditioners for models of coupled magma/mantle
  dynamics</title><categories>math.NA cs.CE cs.NA physics.geo-ph</categories><comments>To appear in SIAM Journal on Scientific Computing</comments><msc-class>65F08, 76M10, 86A17, 86-08</msc-class><acm-class>G.1.3; G.1.8; J.2</acm-class><journal-ref>SIAM J. Sci. Comput., 2015, 37(5), A2270-A2294</journal-ref><doi>10.1137/14099718X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a prescribed porosity, the coupled magma/mantle flow equations can be
formulated as a two-field system of equations with velocity and pressure as
unknowns. Previous work has shown that while optimal preconditioners for the
two-field formulation can be obtained, the construction of preconditioners that
are uniform with respect to model parameters is difficult. This limits the
applicability of two-field preconditioners in certain regimes of practical
interest. We address this issue by reformulating the governing equations as a
three-field problem, which removes a term that was problematic in the two-field
formulation in favour of an additional equation for a pressure-like field. For
the three-field problem, we develop and analyse new preconditioners and we show
numerically that they are optimal in terms of problem size and less sensitive
to model parameters, compared to the two-field preconditioner. This extends the
applicability of optimal preconditioners for coupled mantle/magma dynamics into
parameter regimes of physical interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5240</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5240</id><created>2014-11-19</created><updated>2015-10-30</updated><authors><author><keyname>&#xc1;gueda</keyname><forenames>Raquel</forenames></author><author><keyname>Borozan</keyname><forenames>Valentin</forenames></author><author><keyname>D&#xed;az</keyname><forenames>Raquel</forenames></author><author><keyname>Manoussakis</keyname><forenames>Yannis</forenames></author><author><keyname>Montero</keyname><forenames>Leandro</forenames></author></authors><title>Proper Hamiltonian Cycles in Edge-Colored Multigraphs</title><categories>cs.DM math.CO</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $c$-edge-colored multigraph has each edge colored with one of the $c$
available colors and no two parallel edges have the same color. A proper
Hamiltonian cycle is a cycle containing all the vertices of the multigraph such
that no two adjacent edges have the same color. In this work we establish
sufficient conditions for a multigraph to have a proper Hamiltonian cycle,
depending on several parameters such as the number of edges and the rainbow
degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5245</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5245</id><created>2014-11-19</created><authors><author><keyname>Razmjou</keyname><forenames>Amir</forenames></author></authors><title>Correlation of Scholarly Networks and Social Networks</title><categories>cs.SI cs.CY cs.DL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous studies, much attention from multidisciplinary fields has been
devoted to understand the mechanism of underlying scholarly networks including
bibliographic networks, citation networks and co-citation networks.
Particularly focusing on networks constructed by means of either authors
affinities or the mutual content. Missing a valuable dimension of network,
which is an audience scholarly paper. We aim at this paper to assess the impact
that social networks and media can have on scholarly papers. We also examine
the process of information flow in such networks. We also mention some observa-
tions of attractive incidents that our proposed network model revealed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5254</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5254</id><created>2014-11-19</created><updated>2015-05-12</updated><authors><author><keyname>Tan</keyname><forenames>Si-Hui</forenames></author><author><keyname>Kettlewell</keyname><forenames>Joshua A.</forenames></author><author><keyname>Ouyang</keyname><forenames>Yingkai</forenames></author><author><keyname>Chen</keyname><forenames>Lin</forenames></author><author><keyname>Fitzsimons</keyname><forenames>Joseph F.</forenames></author></authors><title>A quantum approach to homomorphic encryption</title><categories>quant-ph cs.CR</categories><comments>5 pages, 1 figure. Improved security bound over previous version. An
  erroneous proof of universality has been removed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Encryption schemes often derive their power from the properties of the
underlying algebra on the symbols used. Inspired by group theoretic tools, we
use the centralizer of a subgroup of operations to present a private-key
quantum homomorphic encryption scheme that enables a broad class of quantum
computation on encrypted data. A particular instance of our encoding hides up
to a constant fraction of the information encrypted. This fraction can be made
arbitrarily close to unity with overhead scaling only polynomially in the
message length. This highlights the potential of our protocol to hide a
non-trivial amount of information, and is suggestive of a large class of
encodings that might yield better security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5255</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5255</id><created>2014-11-19</created><authors><author><keyname>James</keyname><forenames>Alex Pappachen</forenames></author><author><keyname>Kumar</keyname><forenames>Dinesh S.</forenames></author><author><keyname>Ajayan</keyname><forenames>Arun</forenames></author></authors><title>Threshold Logic Computing: Memristive-CMOS Circuits for Fast Fourier
  Transform and Vedic Multiplication</title><categories>cs.ET cs.AR</categories><comments>5 Pages, IEEE Transactions on Very Large Scale Integration (VLSI)
  Systems, 2014</comments><doi>10.1109/TVLSI.2014.2371857</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Brain inspired circuits can provide an alternative solution to implement
computing architectures taking advantage of fault tolerance and generalisation
ability of logic gates. In this brief, we advance over the memristive threshold
circuit configuration consisting of memristive averaging circuit in combination
with operational amplifier and/or CMOS inverters in application to realizing
complex computing circuits. The developed memristive threshold logic gates are
used for designing FFT and multiplication circuits useful for modern
microprocessors. Overall, the proposed threshold logic outperforms previous
memristive-CMOS logic cells on every aspect, however, indicate a lower chip
area, lower THD, and controllable leakage power, but a higher power dissipation
with respect to CMOS logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5260</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5260</id><created>2014-11-19</created><authors><author><keyname>Kimes</keyname><forenames>Patrick K.</forenames></author><author><keyname>Hayes</keyname><forenames>D. Neil</forenames></author><author><keyname>Marron</keyname><forenames>J. S.</forenames></author><author><keyname>Liu</keyname><forenames>Yufeng</forenames></author></authors><title>Large-Margin Classification with Multiple Decision Rules</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary classification is a common statistical learning problem in which a
model is estimated on a set of covariates for some outcome indicating the
membership of one of two classes. In the literature, there exists a distinction
between hard and soft classification. In soft classification, the conditional
class probability is modeled as a function of the covariates. In contrast, hard
classification methods only target the optimal prediction boundary. While hard
and soft classification methods have been studied extensively, not much work
has been done to compare the actual tasks of hard and soft classification. In
this paper we propose a spectrum of statistical learning problems which span
the hard and soft classification tasks based on fitting multiple decision rules
to the data. By doing so, we reveal a novel collection of learning tasks of
increasing complexity. We study the problems using the framework of
large-margin classifiers and a class of piecewise linear convex surrogates, for
which we derive statistical properties and a corresponding sub-gradient descent
algorithm. We conclude by applying our approach to simulation settings and a
magnetic resonance imaging (MRI) dataset from the Alzheimer's Disease
Neuroimaging Initiative (ADNI) study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5268</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5268</id><created>2014-11-19</created><authors><author><keyname>Sudhakarana</keyname><forenames>Swathikiran</forenames></author><author><keyname>James</keyname><forenames>Alex Pappachen</forenames></author></authors><title>Sparse distributed localized gradient fused features of objects</title><categories>cs.CV cs.AI</categories><comments>Pages 13</comments><journal-ref>Pattern Recognition, Available online 31 October 2014</journal-ref><doi>10.1016/j.patcog.2014.10.002</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The sparse, hierarchical, and modular processing of natural signals is
related to the ability of humans to recognize objects with high accuracy. In
this study, we report a sparse feature processing and encoding method, which
improved the recognition performance of an automated object recognition system.
Randomly distributed localized gradient enhanced features were selected before
employing aggregate functions for representation, where we used a modular and
hierarchical approach to detect the object features. These object features were
combined with a minimum distance classifier, thereby obtaining object
recognition system accuracies of 93% using the Amsterdam library of object
images (ALOI) database, 92% using the Columbia object image library (COIL)-100
database, and 69% using the PASCAL visual object challenge 2007 database. The
object recognition performance was shown to be robust to variations in noise,
object scaling, and object shifts. Finally, a comparison with eight existing
object recognition methods indicated that our new method improved the
recognition accuracy by 10% with ALOI, 8% with the COIL-100 database, and 10%
with the PASCAL visual object challenge 2007 database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5275</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5275</id><created>2014-11-19</created><authors><author><keyname>Gravier</keyname><forenames>Sylvain</forenames></author><author><keyname>Parreau</keyname><forenames>Aline</forenames></author><author><keyname>Rottey</keyname><forenames>Sara</forenames></author><author><keyname>Storme</keyname><forenames>Leo</forenames></author><author><keyname>Vandomme</keyname><forenames>Elise</forenames></author></authors><title>Identifying codes in vertex-transitive graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing identifying codes of graphs and its
fractional relaxation. The ratio between the size of optimal integer and
fractional solutions is between 1 and 2 ln(|V|)+1 where V is the set of
vertices of the graph. We focus on vertex-transitive graphs for which we can
compute the exact fractional solution. There are known examples of
vertex-transitive graphs that reach both bounds. We exhibit infinite families
of vertex-transitive graphs with integer and fractional identifying codes of
order |V|^a with a in {1/4,1/3,2/5}. These families are generalized quadrangles
(strongly regular graphs based on finite geometries). They also provide
examples for metric dimension of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5281</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5281</id><created>2014-11-19</created><updated>2015-09-09</updated><authors><author><keyname>Carrascosa</keyname><forenames>J. M.</forenames></author><author><keyname>Mikians</keyname><forenames>J.</forenames></author><author><keyname>Cuevas</keyname><forenames>R.</forenames></author><author><keyname>Erramilli</keyname><forenames>V.</forenames></author><author><keyname>Laoutaris</keyname><forenames>N.</forenames></author></authors><title>I Always Feel Like Somebody's Watching Me. Measuring Online Behavioural
  Advertising</title><categories>cs.CY</categories><comments>To appear in ACM CoNEXT 2015, Heidelberg, Germany. Please cite the
  conference version of this paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online Behavioural targeted Advertising (OBA) has risen in prominence as a
method to increase the effectiveness of online advertising. OBA operates by
associating tags or labels to users based on their online activity and then
using these labels to target them. This rise has been accompanied by privacy
concerns from researchers, regulators and the press. In this paper, we present
a novel methodology for measuring and understanding OBA in the online
advertising market. We rely on training artificial online personas representing
behavioural traits like 'cooking', 'movies', 'motor sports', etc. and build a
measurement system that is automated, scalable and supports testing of multiple
configurations. We observe that OBA is a frequent practice and notice that
categories valued more by advertisers are more intensely targeted. In addition,
we provide evidences showing that the advertising market targets sensitive
topics (e.g, religion or health) despite the existence of regulation that bans
such practices. We also compare the volume of OBA advertising for our personas
in two different geographical locations (US and Spain) and see little
geographic bias in terms of intensity of OBA targeting. Finally, we check for
targeting with do-not-track (DNT) enabled and discovered that DNT is not yet
enforced in the web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5282</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5282</id><created>2014-11-19</created><updated>2015-04-15</updated><authors><author><keyname>Su</keyname><forenames>Lili</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author></authors><title>Reaching Approximate Byzantine Consensus with Multi-hop Communication</title><categories>cs.DC</categories><comments>24 pages, 1 figure. arXiv admin note: text overlap with
  arXiv:1203.1888</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We address the problem of reaching consensus in the presence of Byzantine
faults. In particular, we are interested in investigating the impact of
messages relay on the network connectivity for a correct iterative approximate
Byzantine consensus algorithm to exist. The network is modeled by a simple
directed graph. We assume a node can send messages to another node that is up
to $l$ hops away via forwarding by the intermediate nodes on the routes, where
$l\in \mathbb{N}$ is a natural number. We characterize the necessary and
sufficient topological conditions on the network structure. The tight
conditions we found are consistent with the tight conditions identified for
$l=1$, where only local communication is allowed, and are strictly weaker for
$l&gt;1$. Let $l^*$ denote the length of a longest path in the given network. For
$l\ge l^*$ and undirected graphs, our conditions hold if and only if $n\ge
3f+1$ and the node-connectivity of the given graph is at least $2f+1$ , where
$n$ is the total number of nodes and $f$ is the maximal number of Byzantine
nodes; and for $l\ge l^*$ and directed graphs, our conditions is equivalent to
the tight condition found for exact Byzantine consensus.
  Our sufficiency is shown by constructing a correct algorithm, wherein the
trim function is constructed based on investigating a newly introduced minimal
messages cover property. The trim function proposed also works over
multi-graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5283</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5283</id><created>2014-11-19</created><authors><author><keyname>Alyasseri</keyname><forenames>Zaid Abdi Alkareem</forenames></author><author><keyname>Al-Attar</keyname><forenames>Kadhim</forenames></author><author><keyname>Nasser</keyname><forenames>Mazin</forenames></author></authors><title>Parallelize Bubble and Merge Sort Algorithms Using Message Passing
  Interface (MPI)</title><categories>cs.DC</categories><comments>5 pages, 5 figures. arXiv admin note: substantial text overlap with
  arXiv:1407.6603</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sorting has been a profound area for the algorithmic researchers and many
resources are invested to suggest more works for sorting algorithms. For this
purpose, many existing sorting algorithms were observed in terms of the
efficiency of the algorithmic complexity. In this paper we implemented the
bubble and merge sort algorithms using Message Passing Interface (MPI)
approach. The proposed work tested on two standard datasets (text file) with
different size. The main idea of the proposed algorithm is distributing the
elements of the input datasets into many additional temporary sub-arrays
according to a number of characters in each word. The sizes of each of these
sub-arrays are decided depending on a number of elements with the same number
of characters in the input array. We implemented MPI using Intel core i7-3610QM
,(8 CPUs),using two approaches (vectors of string and array 3D) . Finally, we
get the data structure effects on the performance of the algorithm for that we
choice the second approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5289</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5289</id><created>2014-11-19</created><updated>2014-11-21</updated><authors><author><keyname>Khedker</keyname><forenames>Uday P.</forenames></author><author><keyname>Kanvar</keyname><forenames>Vini</forenames></author></authors><title>Generalizing the Liveness Based Points-to Analysis</title><categories>cs.PL</categories><acm-class>F.3.1; F.3.2; D.2.4; D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The original liveness based flow and context sensitive points-to analysis
(LFCPA) is restricted to scalar pointer variables and scalar pointees on stack
and static memory. In this paper, we extend it to support heap memory and
pointer expressions involving structures, unions, arrays, and pointer
arithmetic. The key idea behind these extensions involves constructing bounded
names for locations in terms of compile time constants (names and fixed
offsets), and introducing sound approximations when it is not possible to do
so. We achieve this by defining a grammar for pointer expressions, suitable
memory models and location naming conventions, and some key evaluations of
pointer expressions that compute the named locations. These extensions preserve
the spirit of the original LFCPA which is evidenced by the fact that although
the lattices and flow functions change, the overall data flow equations remain
unchanged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5299</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5299</id><created>2014-11-19</created><updated>2015-11-16</updated><authors><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Jamali</keyname><forenames>Vahid</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Robert</keyname></author></authors><title>On the Capacity of the Two-Hop Half-Duplex Relay Channel</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although extensively investigated, the capacity of the two-hop half-duplex
(HD) relay channel is not fully understood. In particular, a capacity
expression which can be easily evaluated is not available and an explicit
coding scheme which achieves the capacity is not known either. In this paper,
we derive a new expression for the capacity of the two-hop HD relay channel by
simplifying previously derived converse expressions. Compared to previous
results, the new capacity expression can be easily evaluated. Moreover, we
propose an explicit coding scheme which achieves the capacity. To achieve the
capacity, the relay does not only send information to the destination by
transmitting information-carrying symbols but also with the zero symbols
resulting from the relay's silence during reception. As examples, we compute
the capacities of the two-hop HD relay channel for the cases when the
source-relay and relay-destination links are both binary-symmetric channels
(BSCs) and additive white Gaussian noise (AWGN) channels, respectively, and
numerically compare the capacities with the rates achieved by conventional
relaying where the relay receives and transmits in a codeword-by-codeword
fashion and switches between reception and transmission in a strictly
alternating manner. Our numerical results show that the capacities of the
two-hop HD relay channel for BSC and AWGN links are significantly larger than
the rates achieved with conventional relaying.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5302</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5302</id><created>2014-11-19</created><updated>2014-11-24</updated><authors><author><keyname>Merwaday</keyname><forenames>Arvind</forenames></author><author><keyname>Yuksel</keyname><forenames>Murat</forenames></author><author><keyname>Quint</keyname><forenames>Thomas</forenames></author><author><keyname>Guvenc</keyname><forenames>Ismail</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Kapucu</keyname><forenames>Naim</forenames></author></authors><title>Incentivizing Spectrum Sharing via Subsidy Regulations</title><categories>cs.NI cs.GT</categories><comments>Technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional regulatory methods for spectrum licensing have been recently
identified as one of the causes for the under-utilization of the valuable radio
spectrum. Governmental agencies such as the Federal Communications Commission
(FCC) are seeking ways to remove stringent regulatory barriers and facilitate
broader access to the spectrum resources. The goal is to allow for an improved
and ubiquitous sharing of the precious radio spectrum between commercial
service providers.
  In this paper, we propose a novel noncooperative game theoretic approach, to
show how to foster more sharing of the radio spectrum via the use of regulatory
power. We define a two stage game in which the government regulators move
first, followed by the providers. The providers are incentivized by lower
spectrum allocation fees from the regulators in return for proof-of-sharing.
The providers are offered discounted spectrum bands, potentially at different
locations, but will be asked to provide coverage to users that are not
subscribed to them so as to maintain their subsidy incentives from the
government. In a simplification of the model, analytical expressions for the
providers' perfect equilibrium strategies are derived, and we argue for the
existence of the government's part of a perfect equilibrium. Our analysis shows
that through subsidization, the government can provide small service providers
a fair chance to compete with the large providers, thereby avoiding
monopolization in the market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5307</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5307</id><created>2014-11-19</created><authors><author><keyname>Shih</keyname><forenames>Kevin</forenames></author><author><keyname>Di</keyname><forenames>Wei</forenames></author><author><keyname>Jagadeesh</keyname><forenames>Vignesh</forenames></author><author><keyname>Piramuthu</keyname><forenames>Robinson</forenames></author></authors><title>Efficient Media Retrieval from Non-Cooperative Queries</title><categories>cs.IR cs.CV</categories><comments>8 pages, 9 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text is ubiquitous in the artificial world and easily attainable when it
comes to book title and author names. Using the images from the book cover set
from the Stanford Mobile Visual Search dataset and additional book covers and
metadata from openlibrary.org, we construct a large scale book cover retrieval
dataset, complete with 100K distractor covers and title and author strings for
each. Because our query images are poorly conditioned for clean text
extraction, we propose a method for extracting a matching noisy and erroneous
OCR readings and matching it against clean author and book title strings in a
standard document look-up problem setup. Finally, we demonstrate how to use
this text-matching as a feature in conjunction with popular retrieval features
such as VLAD using a simple learning setup to achieve significant improvements
in retrieval accuracy over that of either VLAD or the text alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5309</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5309</id><created>2014-11-19</created><authors><author><keyname>Wan</keyname><forenames>Li</forenames></author><author><keyname>Eigen</keyname><forenames>David</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author></authors><title>End-to-End Integration of a Convolutional Network, Deformable Parts
  Model and Non-Maximum Suppression</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deformable Parts Models and Convolutional Networks each have achieved notable
performance in object detection. Yet these two approaches find their strengths
in complementary areas: DPMs are well-versed in object composition, modeling
fine-grained spatial relationships between parts; likewise, ConvNets are adept
at producing powerful image features, having been discriminatively trained
directly on the pixels. In this paper, we propose a new model that combines
these two approaches, obtaining the advantages of each. We train this model
using a new structured loss function that considers all bounding boxes within
an image, rather than isolated object instances. This enables the non-maximal
suppression (NMS) operation, previously treated as a separate post-processing
stage, to be integrated into the model. This allows for discriminative training
of our combined Convnet + DPM + NMS model in end-to-end fashion. We evaluate
our system on PASCAL VOC 2007 and 2011 datasets, achieving competitive results
on both benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5313</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5313</id><created>2014-11-19</created><updated>2014-11-20</updated><authors><author><keyname>Romero</keyname><forenames>Ana Armas</forenames></author><author><keyname>Kaminski</keyname><forenames>Mark</forenames></author><author><keyname>Grau</keyname><forenames>Bernardo Cuenca</forenames></author><author><keyname>Horrocks</keyname><forenames>Ian</forenames></author></authors><title>Ontology Module Extraction via Datalog Reasoning</title><categories>cs.AI cs.LO</categories><comments>13 pages. To appear in AAAI-15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Module extraction - the task of computing a (preferably small) fragment M of
an ontology T that preserves entailments over a signature S - has found many
applications in recent years. Extracting modules of minimal size is, however,
computationally hard, and often algorithmically infeasible. Thus, practical
techniques are based on approximations, where M provably captures the relevant
entailments, but is not guaranteed to be minimal. Existing approximations,
however, ensure that M preserves all second-order entailments of T w.r.t. S,
which is stronger than is required in many applications, and may lead to large
modules in practice. In this paper we propose a novel approach in which module
extraction is reduced to a reasoning problem in datalog. Our approach not only
generalises existing approximations in an elegant way, but it can also be
tailored to preserve only specific kinds of entailments, which allows us to
extract significantly smaller modules. An evaluation on widely-used ontologies
has shown very encouraging results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5319</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5319</id><created>2014-11-19</created><updated>2016-01-24</updated><authors><author><keyname>Hara</keyname><forenames>Kota</forenames></author><author><keyname>Jagadeesh</keyname><forenames>Vignesh</forenames></author><author><keyname>Piramuthu</keyname><forenames>Robinson</forenames></author></authors><title>Fashion Apparel Detection: The Role of Deep Convolutional Neural Network
  and Pose-dependent Priors</title><categories>cs.CV</categories><comments>Accepted for publication at IEEE Winter Conference on Applications of
  Computer Vision (WACV) 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose and address a new computer vision task, which we
call fashion item detection, where the aim is to detect various fashion items a
person in the image is wearing or carrying. The types of fashion items we
consider in this work include hat, glasses, bag, pants, shoes and so on. The
detection of fashion items can be an important first step of various e-commerce
applications for fashion industry. Our method is based on state-of-the-art
object detection method pipeline which combines object proposal methods with a
Deep Convolutional Neural Network. Since the locations of fashion items are in
strong correlation with the locations of body joints positions, we incorporate
contextual information from body poses in order to improve the detection
performance. Through the experiments, we demonstrate the effectiveness of the
proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5323</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5323</id><created>2014-11-19</created><authors><author><keyname>Mehboob</keyname><forenames>Usama</forenames></author><author><keyname>Qadir</keyname><forenames>Junaid</forenames></author><author><keyname>Ali</keyname><forenames>Salman</forenames></author><author><keyname>Vasilakos</keyname><forenames>Athanasios</forenames></author></authors><title>Genetic Algorithms in Wireless Networking: Techniques, Applications, and
  Issues</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent times, wireless access technology is becoming increasingly
commonplace due to the ease of operation and installation of untethered
wireless media. The design of wireless networking is challenging due to the
highly dynamic environmental condition that makes parameter optimization a
complex task. Due to the dynamic, and often unknown, operating conditions,
modern wireless networking standards increasingly rely on machine learning and
artificial intelligence algorithms. Genetic algorithms (GAs) provide a
well-established framework for implementing artificial intelligence tasks such
as classification, learning, and optimization. GAs are well-known for their
remarkable generality and versatility, and have been applied in a wide variety
of settings in wireless networks. In this paper, we provide a comprehensive
survey of the applications of GAs in wireless networks. We provide both an
exposition of common GA models and configuration and provide a broad ranging
survey of GA techniques in wireless networks. We also point out open research
issues and define potential future work. While various surveys on GAs exist in
literature, our paper is the first paper, to the best of our knowledge, which
focuses on their application in wireless networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5326</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5326</id><created>2014-11-19</created><authors><author><keyname>Veness</keyname><forenames>Joel</forenames></author><author><keyname>Bellemare</keyname><forenames>Marc G.</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author><author><keyname>Chua</keyname><forenames>Alvin</forenames></author><author><keyname>Desjardins</keyname><forenames>Guillaume</forenames></author></authors><title>Compress and Control</title><categories>cs.AI cs.IT math.IT</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new information-theoretic policy evaluation technique
for reinforcement learning. This technique converts any compression or density
model into a corresponding estimate of value. Under appropriate stationarity
and ergodicity conditions, we show that the use of a sufficiently powerful
model gives rise to a consistent value function estimator. We also study the
behavior of this technique when applied to various Atari 2600 video games,
where the use of suboptimal modeling techniques is unavoidable. We consider
three fundamentally different models, all too limited to perfectly model the
dynamics of the system. Remarkably, we find that our technique provides
sufficiently accurate value estimates for effective on-policy control. We
conclude with a suggestive study highlighting the potential of our technique to
scale to large problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5328</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5328</id><created>2014-11-19</created><authors><author><keyname>Zhou</keyname><forenames>Bolei</forenames></author><author><keyname>Jagadeesh</keyname><forenames>Vignesh</forenames></author><author><keyname>Piramuthu</keyname><forenames>Robinson</forenames></author></authors><title>ConceptLearner: Discovering Visual Concepts from Weakly Labeled Image
  Collections</title><categories>cs.CV cs.AI cs.LG</categories><comments>9 pages, 8 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discovering visual knowledge from weakly labeled data is crucial to scale up
computer vision recognition system, since it is expensive to obtain fully
labeled data for a large number of concept categories. In this paper, we
propose ConceptLearner, which is a scalable approach to discover visual
concepts from weakly labeled image collections. Thousands of visual concept
detectors are learned automatically, without human in the loop for additional
annotation. We show that these learned detectors could be applied to recognize
concepts at image-level and to detect concepts at image region-level
accurately. Under domain-specific supervision, we further evaluate the learned
concepts for scene recognition on SUN database and for object detection on
Pascal VOC 2007. ConceptLearner shows promising performance compared to fully
supervised and weakly supervised methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5331</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5331</id><created>2014-11-19</created><authors><author><keyname>Greene</keyname><forenames>Michelle R.</forenames></author><author><keyname>Botros</keyname><forenames>Abraham P.</forenames></author><author><keyname>Beck</keyname><forenames>Diane M.</forenames></author><author><keyname>Fei-Fei</keyname><forenames>Li</forenames></author></authors><title>Visual Noise from Natural Scene Statistics Reveals Human Scene Category
  Representations</title><categories>cs.CV cs.HC</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Our perceptions are guided both by the bottom-up information entering our
eyes, as well as our top-down expectations of what we will see. Although
bottom-up visual processing has been extensively studied, comparatively little
is known about top-down signals. Here, we describe REVEAL (Representations
Envisioned Via Evolutionary ALgorithm), a method for visualizing an observer's
internal representation of a complex, real-world scene, allowing us to, for the
first time, visualize the top-down information in an observer's mind. REVEAL
rests on two innovations for solving this high dimensional problem: visual
noise that samples from natural image statistics, and a computer algorithm that
collaborates with human observers to efficiently obtain a solution. In this
work, we visualize observers' internal representations of a visual scene
category (street) using an experiment in which the observer views the
naturalistic visual noise and collaborates with the algorithm to externalize
his internal representation. As no scene information was presented, observers
had to use their internal knowledge of the target, matching it with the visual
features in the noise. We matched reconstructed images with images of
real-world street scenes to enhance visualization. Critically, we show that the
visualized mental images can be used to predict rapid scene detection
performance, as each observer had faster and more accurate responses to
detecting real-world images that were the most similar to his reconstructed
street templates. These results show that it is possible to visualize
previously unobservable mental representations of real world stimuli. More
broadly, REVEAL provides a general method for objectively examining the content
of previously private, subjective mental experiences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5336</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5336</id><created>2014-11-18</created><authors><author><keyname>Cai</keyname><forenames>Ning</forenames></author><author><keyname>Ma</keyname><forenames>Hai-Ying</forenames></author><author><keyname>Khan</keyname><forenames>M. Junaid</forenames></author></authors><title>Agent-Based Model for Rural-Urban Migration: A Dynamic Consideration</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a dynamic agent-based model for rural-urban migration,
based on the previous relevant works. The model conforms to the typical dynamic
linear multi-agent systems model concerned extensively in systems science, in
which the communication network is formulated as a digraph. Simulations reveal
that consensus of certain variable could be harmful to the overall stability
and should be avoided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5340</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5340</id><created>2014-11-19</created><authors><author><keyname>Greene</keyname><forenames>Michelle R.</forenames></author><author><keyname>Baldassano</keyname><forenames>Christopher</forenames></author><author><keyname>Esteva</keyname><forenames>Andre</forenames></author><author><keyname>Beck</keyname><forenames>Diane M.</forenames></author><author><keyname>Fei-Fei</keyname><forenames>Li</forenames></author></authors><title>Affordances Provide a Fundamental Categorization Principle for Visual
  Scenes</title><categories>q-bio.NC cs.CV cs.HC</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  How do we know that a kitchen is a kitchen by looking? Relatively little is
known about how we conceptualize and categorize different visual environments.
Traditional models of visual perception posit that scene categorization is
achieved through the recognition of a scene's objects, yet these models cannot
account for the mounting evidence that human observers are relatively
insensitive to the local details in an image. Psychologists have long theorized
that the affordances, or actionable possibilities of a stimulus are pivotal to
its perception. To what extent are scene categories created from similar
affordances? Using a large-scale experiment using hundreds of scene categories,
we show that the activities afforded by a visual scene provide a fundamental
categorization principle. Affordance-based similarity explained the majority of
the structure in the human scene categorization patterns, outperforming
alternative similarities based on objects or visual features. We all models
were combined, affordances provided the majority of the predictive power in the
combined model, and nearly half of the total explained variance is captured
only by affordances. These results challenge many existing models of high-level
visual perception, and provide immediately testable hypotheses for the
functional organization of the human perceptual system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5343</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5343</id><created>2014-11-19</created><authors><author><keyname>Paudel</keyname><forenames>Subodh</forenames></author><author><keyname>Shrestha</keyname><forenames>Jagan Nath</forenames></author><author><keyname>Neto</keyname><forenames>Fernando J</forenames></author><author><keyname>Ferreira</keyname><forenames>Jorge AF</forenames></author><author><keyname>Adhikari</keyname><forenames>Muna</forenames></author></authors><title>Optimization of Hybrid PV/Wind Power System for Remote Telecom Station</title><categories>cs.SY</categories><doi>10.1109/ICPES.2011.6156618</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid depletion of fossil fuel resources and environmental concerns has
given awareness on generation of renewable energy resources. Among the various
renewable resources, hybrid solar and wind energy seems to be promising
solutions to provide reliable power supply with improved system efficiency and
reduced storage requirements for stand-alone applications. This paper presents
a feasibility assessment and optimum size of photovoltaic (PV) array, wind
turbine and battery bank for a standalone hybrid Solar/Wind Power system
(HSWPS) at remote telecom station of Nepal at Latitude (27{\deg}23'50&quot;) and
Longitude (86{\deg}44'23&quot;) consisting a telecommunication load of Very Small
Aperture Terminal (VSAT), Repeater station and Code Division Multiple Access
Base Transceiver Station (CDMA 2C10 BTS). In any RES based system, the
feasibility assessment is considered as the first step analysis. In this work,
feasibility analysis is carried through hybrid optimization model for electric
renewables (HOMER) and mathematical models were implemented in the MATLAB
environment to perform the optimal configuration for a given load and a desired
loss of power supply probability (LPSP) from a set of systems components with
the lowest value of cost function defined in terms of reliability and levelized
unit electricity cost (LUCE). The simulation results for the existing and the
proposed models are compared. The simulation results shows that existing
architecture consisting of 6.12 kW KC85T photovoltaic modules, 1kW H3.1 wind
turbine and 1600 Ah GFM-800 battery bank have a 36.6% of unmet load during a
year. On the other hand, the proposed system includes 1kW *2 H3.1 Wind turbine,
8.05 kW TSM-175DA01 photovoltaic modules and 1125 Ah T-105 battery bank with
system reliability of 99.99% with a significant cost reduction as well as
reliable energy production.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5350</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5350</id><created>2014-11-19</created><authors><author><keyname>Sun</keyname><forenames>Jie</forenames></author><author><keyname>Cafaro</keyname><forenames>Carlo</forenames></author><author><keyname>Bollt</keyname><forenames>Erik M.</forenames></author></authors><title>Identifying Coupling Structure in Complex Systems through the Optimal
  Causation Entropy Principle</title><categories>physics.data-an cs.IT math.IT</categories><journal-ref>Entropy 16, 3416-3433 (2014)</journal-ref><doi>10.3390/e16063416</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inferring the coupling structure of complex systems from time series data in
general by means of statistical and information-theoretic techniques is a
challenging problem in applied science. The reliability of statistical
inferences requires the construction of suitable information-theoretic measures
that take into account both direct and indirect influences, manifest in the
form of information flows, between the components within the system. In this
work, we present an application of the optimal causation entropy (oCSE)
principle to identify the coupling structure of a synthetic biological system,
the repressilator. Specifically, when the system reaches an equilibrium state,
we use a stochastic perturbation approach to extract time series data that
approximate a linear stochastic process. Then, we present and jointly apply the
aggregative discovery and progressive removal algorithms based on the oCSE
principle to infer the coupling structure of the system from the measured data.
Finally, we show that the success rate of our coupling inferences not only
improves with the amount of available data, but it also increases with a higher
frequency of sampling and is especially immune to false positives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5371</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5371</id><created>2014-11-19</created><updated>2015-07-28</updated><authors><author><keyname>Kinney</keyname><forenames>Justin B.</forenames></author></authors><title>Unification of field theory and maximum entropy methods for learning
  probability densities</title><categories>physics.data-an cs.LG q-bio.QM stat.ML</categories><comments>16 pages, 4 figures. Minor clarifying changes have been made
  throughout. Software is available at https://github.com/jbkinney/14_maxent</comments><journal-ref>Phys. Rev. E 92, 032107 (2015)</journal-ref><doi>10.1103/PhysRevE.92.032107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need to estimate smooth probability distributions (a.k.a. probability
densities) from finite sampled data is ubiquitous in science. Many approaches
to this problem have been described, but none is yet regarded as providing a
definitive solution. Maximum entropy estimation and Bayesian field theory are
two such approaches. Both have origins in statistical physics, but the
relationship between them has remained unclear. Here I unify these two methods
by showing that every maximum entropy density estimate can be recovered in the
infinite smoothness limit of an appropriate Bayesian field theory. I also show
that Bayesian field theory estimation can be performed without imposing any
boundary conditions on candidate densities, and that the infinite smoothness
limit of these theories recovers the most common types of maximum entropy
estimates. Bayesian field theory is thus seen to provide a natural test of the
validity of the maximum entropy null hypothesis. Bayesian field theory also
returns a lower entropy density estimate when the maximum entropy hypothesis is
falsified. The computations necessary for this approach can be performed
rapidly for one-dimensional data, and software for doing this is provided.
Based on these results, I argue that Bayesian field theory is poised to provide
a definitive solution to the density estimation problem in one dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5379</identifier>
 <datestamp>2014-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5379</id><created>2014-11-19</created><updated>2014-12-16</updated><authors><author><keyname>Zhao</keyname><forenames>Kai</forenames></author><author><keyname>Huang</keyname><forenames>Liang</forenames></author></authors><title>Type-Driven Incremental Semantic Parsing with Polymorphism</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic parsing has made significant progress, but most current semantic
parsers are extremely slow (CKY-based) and rather primitive in representation.
We introduce three new techniques to tackle these problems. First, we design
the first linear-time incremental shift-reduce-style semantic parsing algorithm
which is more efficient than conventional cubic-time bottom-up semantic
parsers. Second, our parser, being type-driven instead of syntax-driven, uses
type-checking to decide the direction of reduction, which eliminates the need
for a syntactic grammar such as CCG. Third, to fully exploit the power of
type-driven semantic parsing beyond simple types (such as entities and truth
values), we borrow from programming language theory the concepts of subtype
polymorphism and parametric polymorphism to enrich the type system in order to
better guide the parsing. Our system learns very accurate parses in GeoQuery,
Jobs and Atis domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5383</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5383</id><created>2014-11-19</created><authors><author><keyname>Allen-Zhu</keyname><forenames>Zeyuan</forenames></author><author><keyname>Gelashvili</keyname><forenames>Rati</forenames></author><author><keyname>Micali</keyname><forenames>Silvio</forenames></author><author><keyname>Shavit</keyname><forenames>Nir</forenames></author></authors><title>Johnson-Lindenstrauss Compression with Neuroscience-Based Constraints</title><categories>q-bio.NC cs.DS math.PR math.ST stat.TH</categories><comments>A shorter version of this paper has appeared in the Proceedings of
  the National Academy of Sciences</comments><doi>10.1073/pnas.1419100111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Johnson-Lindenstrauss (JL) matrices implemented by sparse random synaptic
connections are thought to be a prime candidate for how convergent pathways in
the brain compress information. However, to date, there is no complete
mathematical support for such implementations given the constraints of real
neural tissue. The fact that neurons are either excitatory or inhibitory
implies that every so implementable JL matrix must be sign-consistent (i.e.,
all entries in a single column must be either all non-negative or all
non-positive), and the fact that any given neuron connects to a relatively
small subset of other neurons implies that the JL matrix had better be sparse.
  We construct sparse JL matrices that are sign-consistent, and prove that our
construction is essentially optimal. Our work answers a mathematical question
that was triggered by earlier work and is necessary to justify the existence of
JL compression in the brain, and emphasizes that inhibition is crucial if
neurons are to perform efficient, correlation-preserving compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5392</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5392</id><created>2014-11-18</created><authors><author><keyname>&#x160;uvakov</keyname><forenames>M.</forenames></author><author><keyname>Tadi&#x107;</keyname><forenames>B.</forenames></author></authors><title>Collective emotion dynamics in chats with agents, moderators and Bots</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 7 figures</comments><proxy>Bohdan Markiv</proxy><journal-ref>Condens. Matter Phys., 2014, vol. 17, No. 3, 33801</journal-ref><doi>10.5488/CMP.17.33801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using agent-directed simulations, we investigate fluctuations in the
collective emotional states on a chat network where agents interchange messages
with a fixed number of moderators and emotional Bot. To design a realistic chat
system, the interaction rules and some statistical parameters, as well as the
agent's attributes, are inferred from the empirical chat channel
\texttt{Ubuntu}. In the simulations, the Bot's emotion is fixed; the moderators
tune the level of its activity by passing a fraction $\epsilon$ of messages to
the Bot. At $\epsilon \gtrsim 0$, the collective emotional state matching the
Bot's emotion polarity gradually arises; the average growth rate of the
dominant emotional charge serves as an order parameter. Due to self-organizing
effects, the collective dynamics is more explosive when positive emotions arise
by positive Bot than the onset of negative emotions in the presence of negative
Bot at the same $\epsilon$. Furthermore, when the emotions matching the Bot's
emotion polarity are spread over the system, the underlying fractal processes
exhibit higher persistence and stronger clustering of events than the processes
spreading of emotion polarity opposite to the Bot's emotion. On the other hand,
the relaxation dynamics is controlled by the external noise; the related
nonextensive parameter, estimated from the statistics of returns, is virtually
independent of the Bot's activity level and emotion contents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5394</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5394</id><created>2014-11-19</created><authors><author><keyname>Nandakumar</keyname><forenames>Rajalakshmi</forenames></author><author><keyname>Kellogg</keyname><forenames>Bryce</forenames></author><author><keyname>Gollakota</keyname><forenames>Shyamnath</forenames></author></authors><title>Wi-Fi Gesture Recognition on Existing Devices</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the first wireless gesture recognition system that
operates using existingWi-Fi signals and devices. To achieve this, we first
identify limitations of existing wireless gesture recognition approaches that
limit their applicability to Wi-Fi. We then introduce algorithms that can
classify gestures using information that is readily available on Wi-Fi devices.
We demonstrate the feasibility of our design using a prototype implementation
on off-the-shelf Wi-Fi devices. Our results show that we can achieve a
classification accuracy of 91% while classifying four gestures across six
participants, without the need for per-participant training. Finally, we show
the feasibility of gesture recognition in non-line-ofsight situations with the
participants interacting with a Wi-Fi device placed in a backpack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5404</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5404</id><created>2014-11-19</created><updated>2015-01-28</updated><authors><author><keyname>Xu</keyname><forenames>Kevin S.</forenames></author></authors><title>Stochastic Block Transition Models for Dynamic Networks</title><categories>cs.SI cs.LG physics.soc-ph stat.ME</categories><comments>To appear in proceedings of AISTATS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been great interest in recent years on statistical models for
dynamic networks. In this paper, I propose a stochastic block transition model
(SBTM) for dynamic networks that is inspired by the well-known stochastic block
model (SBM) for static networks and previous dynamic extensions of the SBM.
Unlike most existing dynamic network models, it does not make a hidden Markov
assumption on the edge-level dynamics, allowing the presence or absence of
edges to directly influence future edge probabilities while retaining the
interpretability of the SBM. I derive an approximate inference procedure for
the SBTM and demonstrate that it is significantly better at reproducing
durations of edges in real social network data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5410</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5410</id><created>2014-11-19</created><authors><author><keyname>Aziz</keyname><forenames>Rehan Abdul</forenames></author><author><keyname>Chu</keyname><forenames>Geoffrey</forenames></author><author><keyname>Muise</keyname><forenames>Christian</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter</forenames></author></authors><title>Stable Model Counting and Its Application in Probabilistic Logic
  Programming</title><categories>cs.AI</categories><comments>Accepted in AAAI, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model counting is the problem of computing the number of models that satisfy
a given propositional theory. It has recently been applied to solving inference
tasks in probabilistic logic programming, where the goal is to compute the
probability of given queries being true provided a set of mutually independent
random variables, a model (a logic program) and some evidence. The core of
solving this inference task involves translating the logic program to a
propositional theory and using a model counter. In this paper, we show that for
some problems that involve inductive definitions like reachability in a graph,
the translation of logic programs to SAT can be expensive for the purpose of
solving inference tasks. For such problems, direct implementation of stable
model semantics allows for more efficient solving. We present two
implementation techniques, based on unfounded set detection, that extend a
propositional model counter to a stable model counter. Our experiments show
that for particular problems, our approach can outperform a state-of-the-art
probabilistic logic programming solver by several orders of magnitude in terms
of running time and space requirements, and can solve instances of
significantly larger sizes on which the current solver runs out of time or
memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5412</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5412</id><created>2014-11-19</created><authors><author><keyname>Angulo</keyname><forenames>Marco Tulio</forenames></author><author><keyname>Liu</keyname><forenames>Yang-Yu</forenames></author><author><keyname>Slotine</keyname><forenames>Jean-Jacques</forenames></author></authors><title>Network motifs emerge from interconnections that favor stability</title><categories>cs.SY physics.bio-ph physics.soc-ph</categories><comments>6 pages plus 7 page supplement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network motifs are overrepresented interconnection patterns found in
real-world networks. What functional advantages may they offer for building
complex systems? We show that most network motifs emerge from interconnections
patterns that best exploit the intrinsic stability characteristics of
individual nodes. This feature is observed at different scales in a network,
from nodes to modules, suggesting an efficient mechanism to stably build
complex systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5414</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5414</id><created>2014-11-19</created><authors><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author><author><keyname>Filmus</keyname><forenames>Yuval</forenames></author><author><keyname>Gall</keyname><forenames>Fran&#xe7;ois Le</forenames></author></authors><title>Fast Matrix Multiplication: Limitations of the Laser Method</title><categories>cs.CC cs.DS</categories><comments>38 pages + cover page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Until a few years ago, the fastest known matrix multiplication algorithm, due
to Coppersmith and Winograd (1990), ran in time $O(n^{2.3755})$. Recently, a
surge of activity by Stothers, Vassilevska-Williams, and Le Gall has led to an
improved algorithm running in time $O(n^{2.3729})$. These algorithms are
obtained by analyzing higher and higher tensor powers of a certain identity of
Coppersmith and Winograd. We show that this exact approach cannot result in an
algorithm with running time $O(n^{2.3725})$, and identify a wide class of
variants of this approach which cannot result in an algorithm with running time
$O(n^{2.3078})$; in particular, this approach cannot prove the conjecture that
for every $\epsilon &gt; 0$, two $n\times n$ matrices can be multiplied in time
$O(n^{2+\epsilon})$.
  We describe a new framework extending the original laser method, which is the
method underlying the previously mentioned algorithms. Our framework
accommodates the algorithms by Coppersmith and Winograd, Stothers,
Vassilevska-Williams and Le Gall. We obtain our main result by analyzing this
framework. The framework is also the first to explain why taking tensor powers
of the Coppersmith-Winograd identity results in faster algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5415</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5415</id><created>2014-11-19</created><authors><author><keyname>Chen</keyname><forenames>Lin</forenames></author><author><keyname>Fan</keyname><forenames>Ruolin</forenames></author><author><keyname>Bian</keyname><forenames>Kaigui</forenames></author><author><keyname>Chen</keyname><forenames>Lin</forenames></author><author><keyname>Gerla</keyname><forenames>Mario</forenames></author><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Li</keyname><forenames>Xiaoming</forenames></author></authors><title>On Heterogeneous Neighbor Discovery in Wireless Sensor Networks</title><categories>cs.NI</categories><comments>Accepted by IEEE INFOCOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neighbor discovery plays a crucial role in the formation of wireless sensor
networks and mobile networks where the power of sensors (or mobile devices) is
constrained. Due to the difficulty of clock synchronization, many asynchronous
protocols based on wake-up scheduling have been developed over the years in
order to enable timely neighbor discovery between neighboring sensors while
saving energy. However, existing protocols are not fine-grained enough to
support all heterogeneous battery duty cycles, which can lead to a more rapid
deterioration of long-term battery health for those without support. Existing
research can be broadly divided into two categories according to their
neighbor-discovery techniques---the quorum based protocols and the co-primality
based protocols.In this paper, we propose two neighbor discovery protocols,
called Hedis and Todis, that optimize the duty cycle granularity of quorum and
co-primality based protocols respectively, by enabling the finest-grained
control of heterogeneous duty cycles. We compare the two optimal protocols via
analytical and simulation results, which show that although the optimal
co-primality based protocol (Todis) is simpler in its design, the optimal
quorum based protocol (Hedis) has a better performance since it has a lower
relative error rate and smaller discovery delay, while still allowing the
sensor nodes to wake up at a more infrequent rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5416</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5416</id><created>2014-11-19</created><authors><author><keyname>Silaghi</keyname><forenames>Marius C.</forenames></author><author><keyname>Roussev</keyname><forenames>Roussi</forenames></author></authors><title>Recommending the Most Encompassing Opposing and Endorsing Arguments in
  Debates</title><categories>cs.AI</categories><comments>10 pages. This report was reviewed by a committee within Florida Tech
  during April 2014, and had been written in Summer 2013 by summarizing a set
  of emails exchanged during Spring 2013, concerning the DirectDemocracyP2P.net
  system</comments><msc-class>68T27, 97E30, 03A10, 68T30</msc-class><acm-class>I.2.4; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arguments are essential objects in DirectDemocracyP2P, where they can occur
both in association with signatures for petitions, or in association with other
debated decisions, such as bug sorting by importance. The arguments of a signer
on a given issue are grouped into one single justification, are classified by
the type of signature (e.g., supporting or opposing), and can be subject to
various types of threading.
  Given the available inputs, the two addressed problems are: (i) how to
recommend the best justification, of a given type, to a new voter, (ii) how to
recommend a compact list of justifications subsuming the majority of known
arguments for (or against) an issue.
  We investigate solutions based on weighted bipartite graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5417</identifier>
 <datestamp>2015-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5417</id><created>2014-11-19</created><updated>2015-03-04</updated><authors><author><keyname>Talwar</keyname><forenames>Kunal</forenames></author><author><keyname>Thakurta</keyname><forenames>Abhradeep</forenames></author><author><keyname>Zhang</keyname><forenames>Li</forenames></author></authors><title>Private Empirical Risk Minimization Beyond the Worst Case: The Effect of
  the Constraint Set Geometry</title><categories>cs.LG cs.CR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Empirical Risk Minimization (ERM) is a standard technique in machine
learning, where a model is selected by minimizing a loss function over
constraint set. When the training dataset consists of private information, it
is natural to use a differentially private ERM algorithm, and this problem has
been the subject of a long line of work started with Chaudhuri and Monteleoni
2008. A private ERM algorithm outputs an approximate minimizer of the loss
function and its error can be measured as the difference from the optimal value
of the loss function. When the constraint set is arbitrary, the required error
bounds are fairly well understood~\cite{BassilyST14}. In this work, we show
that the geometric properties of the constraint set can be used to derive
significantly better results. Specifically, we show that a differentially
private version of Mirror Descent leads to error bounds of the form
$\tilde{O}(G_{\mathcal{C}}/n)$ for a lipschitz loss function, improving on the
$\tilde{O}(\sqrt{p}/n)$ bounds in Bassily, Smith and Thakurta 2014. Here $p$ is
the dimensionality of the problem, $n$ is the number of data points in the
training set, and $G_{\mathcal{C}}$ denotes the Gaussian width of the
constraint set that we optimize over. We show similar improvements for strongly
convex functions, and for smooth functions. In addition, we show that when the
loss function is Lipschitz with respect to the $\ell_1$ norm and $\mathcal{C}$
is $\ell_1$-bounded, a differentially private version of the Frank-Wolfe
algorithm gives error bounds of the form $\tilde{O}(n^{-2/3})$. This captures
the important and common case of sparse linear regression (LASSO), when the
data $x_i$ satisfies $|x_i|_{\infty} \leq 1$ and we optimize over the $\ell_1$
ball. We show new lower bounds for this setting, that together with known
bounds, imply that all our upper bounds are tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5428</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5428</id><created>2014-11-19</created><updated>2014-11-21</updated><authors><author><keyname>Stoddard</keyname><forenames>Ben</forenames></author><author><keyname>Chen</keyname><forenames>Yan</forenames></author><author><keyname>Machanavajjhala</keyname><forenames>Ashwin</forenames></author></authors><title>Differentially Private Algorithms for Empirical Machine Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important use of private data is to build machine learning classifiers.
While there is a burgeoning literature on differentially private classification
algorithms, we find that they are not practical in real applications due to two
reasons. First, existing differentially private classifiers provide poor
accuracy on real world datasets. Second, there is no known differentially
private algorithm for empirically evaluating the private classifier on a
private test dataset.
  In this paper, we develop differentially private algorithms that mirror real
world empirical machine learning workflows. We consider the private classifier
training algorithm as a blackbox. We present private algorithms for selecting
features that are input to the classifier. Though adding a preprocessing step
takes away some of the privacy budget from the actual classification process
(thus potentially making it noisier and less accurate), we show that our novel
preprocessing techniques significantly increase classifier accuracy on three
real-world datasets. We also present the first private algorithms for
empirically constructing receiver operating characteristic (ROC) curves on a
private test set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5433</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5433</id><created>2014-11-19</created><authors><author><keyname>Semenov</keyname><forenames>Alexander</forenames></author><author><keyname>Zaikin</keyname><forenames>Oleg</forenames></author><author><keyname>Otpuschennikov</keyname><forenames>Ilya</forenames></author></authors><title>Using Volunteer Computing for Mounting SAT-based Cryptographic Attacks</title><categories>cs.DC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe the volunteer computing project SAT@home, developed
and maintained by us. This project is aimed at solving hard instances of the
Boolean satisfiability problem (SAT). We believe that this project can be a
useful tool for computational study of inversion problems of some cryptographic
functions. In particular we describe a series of experiments performed in
SAT@home on the cryptanalysis of the widely known keystream generator A5/1. In
all experiments we analyzed one known burst (114 bits) of keystream produced by
A5/1. Before the cryptanalysis itself there is a stage on which the
partitioning of the original problem to a family of subproblems is carried out.
Each of subproblems should be easy enough so that it could be solved in
relatively small amount of time by volunteer's PC. We construct such
partitioning using the special technique based on the Monte Carlo method and
discrete optimization algorithms for special predictive functions. Besides this
in the paper we describe the technique for reducing inversion problems of
cryptographic functions to SAT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5437</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5437</id><created>2014-11-19</created><updated>2014-11-28</updated><authors><author><keyname>Fenner</keyname><forenames>Stephen A.</forenames><affiliation>University of South Carolina, USA</affiliation></author></authors><title>The complexity of some regex crossword problems</title><categories>cs.CC cs.FL</categories><comments>25 pages, 3 figures; three references added with explanation,
  citation added to Corollary 5, more detail in proof of Theorem 8, other minor
  corrections (results unchanged)</comments><msc-class>68Q17, 68Q45, 03D05, 03D10, 03D15</msc-class><acm-class>F.1.1; F.1.3; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a typical regular expression (regex) crossword puzzle, you are given two
nonempty lists $R_1,\ldots,R_m$ and $C_1,\ldots,C_n$ of regular expressions
over some alphabet, and your goal is to fill in an $m\times n$ grid with
letters from that alphabet so that the string formed by the $i$th row is in
$L(R_i)$, and the string formed by the $j$th column is in $L(C_j)$, for all
$1\le i\le m$ and $1\le j\le n$. Such a grid is a solution to the puzzle. It is
known that determining whether a solution exists is NP-complete. We consider a
number of restrictions and variants to this problem where all the $R_i$ are
equal to some regular expression $R$, and all the $C_j$ are equal to some
regular expression $C$. We call the solution to such a puzzle an
$(R,C)$-crossword. Our main results are the following:
  1. There exists a fixed regular expression $C$ over the alphabet $\{0,1\}$
such that the following problem is NP-complete: &quot;Given a regular expression $R$
over $\{0,1\}$ and positive integers $m$ and $n$ given in unary, does an
$m\times n$ $(R,C)$-crossword exist?&quot; This improves the result mentioned above.
  2. The following problem is NP-hard: &quot;Given a regular expression $E$ over
$\{0,1\}$ and positive integers $m$ and $n$ given in unary, does an $m\times n$
$(E,E)$-crossword exist?&quot;
  3. There exists a fixed regular expression $C$ over $\{0,1\}$ such that the
following problem is undecidable (equivalent to the Halting Problem): &quot;Given a
regular expression $R$ over $\{0,1\}$, does an $(R,C)$-crossword exist (of any
size)?&quot;
  4. The following problem is undecidable (equivalent to the Halting Problem):
&quot;Given a regular expression $E$ over $\{0,1\}$, does an $(E,E)$-crossword exist
(of any size)?&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5442</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5442</id><created>2014-11-20</created><authors><author><keyname>Gamble</keyname><forenames>Jennifer</forenames></author><author><keyname>Chintakunta</keyname><forenames>Harish</forenames></author><author><keyname>Krim</keyname><forenames>Hamid</forenames></author></authors><title>Adaptive tracking of representative cycles in regular and zigzag
  persistent homology</title><categories>cs.CG</categories><comments>21 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Persistent homology and zigzag persistent homology are techniques which track
the homology over a sequence of spaces, outputting a set of intervals
corresponding to birth and death times of homological features in the sequence.
This paper presents a method for choosing a homology class to correspond to
each of the intervals at each time point. For each homology class a specific
representative cycle is stored, with the choice of homology class and
representative cycle being both geometrically relevant and compatible with the
birth-death interval decomposition. After describing the method in detail and
proving its correctness, we illustrate the utility of the method by applying it
to the study of coverage holes in time-varying sensor networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5451</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5451</id><created>2014-11-20</created><authors><author><keyname>Kooti</keyname><forenames>Farshad</forenames></author><author><keyname>Magno</keyname><forenames>Gabriel</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author></authors><title>The Social Name-Letter Effect on Online Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>12 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Name-Letter Effect states that people have a preference for brands,
places, and even jobs that start with the same letter as their own first name.
So Sam might like Snickers and live in Seattle. We use social network data from
Twitter and Google+ to replicate this effect in a new environment. We find
limited to no support for the Name-Letter Effect on social networks. We do,
however, find a very robust Same-Name Effect where, say, Michaels would be more
likely to link to other Michaels than Johns. This effect persists when
accounting for gender, nationality, race, and age. The fundamentals behind
these effects have implications beyond psychology as understanding how a
positive self-image is transferred to other entities is important in domains
ranging from studying homophily to personalized advertising and to link
formation in social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5455</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5455</id><created>2014-11-20</created><authors><author><keyname>Kowaluk</keyname><forenames>Miros&#x142;aw</forenames></author><author><keyname>Majewska</keyname><forenames>Gabriela</forenames></author></authors><title>Generalized $\beta$-skeletons</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $\beta$-skeletons, a prominent member of the neighborhood graph family, have
interesting geometric properties and various applications ranging from
geographic networks to archeology. This paper focuses on developing a new, more
general than the present one, definition of $\beta$-skeletons based only on the
distance criterion. It allows us to consider them in many different cases, e.g.
for weighted graphs or objects other than points. Two types of
$\beta$-skeletons are especially well-known: the Gabriel Graph (for $\beta =
1$) and the Relative Neighborhood Graph (for $\beta = 2$). The new definition
retains relations between those graphs and the other well-known ones (minimum
spanning tree and Delaunay triangulation). We also show several new algorithms
finding $\beta$-skeletons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5457</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5457</id><created>2014-11-20</created><updated>2015-08-12</updated><authors><author><keyname>Kowaluk</keyname><forenames>Miros&#x142;aw</forenames></author><author><keyname>Majewska</keyname><forenames>Gabriela</forenames></author></authors><title>$\beta$-skeletons for a set of line segments in $R^2$</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $\beta$-skeletons are well-known neighborhood graphs for a set of points. We
extend this notion to sets of line segments in the Euclidean plane and present
algorithms computing such skeletons for the entire range of $\beta$ values. The
main reason of such extension is the possibility to study $\beta$-skeletons for
points moving along given line segments. We show that relations between
$\beta$-skeletons for $\beta &gt; 1$, $1$-skeleton (Gabriel Graph), and the
Delaunay triangulation for sets of points hold also for sets of segments. We
present algorithms for computing circle-based and lune-based $\beta$-skeletons.
We describe an algorithm that for $\beta \geq 1$ computes the $\beta$-skeleton
for a set $S$ of $n$ segments in the Euclidean plane in $O(n^2 \alpha (n) \log
n)$ time in the circle-based case and in $O(n^2 \lambda_4(n))$ in the
lune-based one, where the construction relies on the Delaunay triangulation for
$S$, $\alpha$ is a functional inverse of Ackermann function and $\lambda_4(n)$
denotes the maximum possible length of a $(n,4)$ Davenport-Schinzel sequence.
When $0 &lt; \beta &lt; 1$, the $\beta$-skeleton can be constructed in a $O(n^3
\lambda_4(n))$ time. In the special case of $\beta = 1$, which is a
generalization of Gabriel Graph, the construction can be carried out in a $O(n
\log n)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5458</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5458</id><created>2014-11-20</created><authors><author><keyname>Roy</keyname><forenames>Subhrajit</forenames></author><author><keyname>Banerjee</keyname><forenames>Amitava</forenames></author><author><keyname>Basu</keyname><forenames>Arindam</forenames></author></authors><title>Liquid State Machine with Dendritically Enhanced Readout for Low-power,
  Neuromorphic VLSI Implementations</title><categories>cs.ET cs.NE</categories><comments>14 pages, 19 figures, Journal</comments><journal-ref>IEEE Transactions on Biomedical Circuits and Systems, vol.8, no.5,
  pp.681,695, Oct. 2014</journal-ref><doi>10.1109/TBCAS.2014.2362969</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe a new neuro-inspired, hardware-friendly readout
stage for the liquid state machine (LSM), a popular model for reservoir
computing. Compared to the parallel perceptron architecture trained by the
p-delta algorithm, which is the state of the art in terms of performance of
readout stages, our readout architecture and learning algorithm can attain
better performance with significantly less synaptic resources making it
attractive for VLSI implementation. Inspired by the nonlinear properties of
dendrites in biological neurons, our readout stage incorporates neurons having
multiple dendrites with a lumped nonlinearity. The number of synaptic
connections on each branch is significantly lower than the total number of
connections from the liquid neurons and the learning algorithm tries to find
the best 'combination' of input connections on each branch to reduce the error.
Hence, the learning involves network rewiring (NRW) of the readout network
similar to structural plasticity observed in its biological counterparts. We
show that compared to a single perceptron using analog weights, this
architecture for the readout can attain, even by using the same number of
binary valued synapses, up to 3.3 times less error for a two-class spike train
classification problem and 2.4 times less error for an input rate approximation
task. Even with 60 times larger synapses, a group of 60 parallel perceptrons
cannot attain the performance of the proposed dendritically enhanced readout.
An additional advantage of this method for hardware implementations is that the
'choice' of connectivity can be easily implemented exploiting address event
representation (AER) protocols commonly used in current neuromorphic systems
where the connection matrix is stored in memory. Also, due to the use of binary
synapses, our proposed method is more robust against statistical variations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5459</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5459</id><created>2014-11-20</created><authors><author><keyname>Kowaluk</keyname><forenames>Miros&#x142;aw</forenames></author></authors><title>Planar $\beta$-skeletons via point location in monotone subdivisions of
  subset of lunes</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for lune-based $\beta$-skeletons for sets of $n$
points in the plane, for $\beta \in (2,\infty]$, the only case when optimal
algorithms are not known. The running time of the algorithm is $O(n^{3/2}
\log^{1/2} n)$, which is the best known and is an improvement of Rao and
Mukhopadhyay \cite{rm97} result. The method is based on point location in
monotonic subdivisions of arrangements of curve segments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5461</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5461</id><created>2014-11-20</created><updated>2015-07-28</updated><authors><author><keyname>Asadi</keyname><forenames>Behzad</forenames></author><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author></authors><title>Optimal Coding Schemes for the Three-Receiver AWGN Broadcast Channel
  with Receiver Message Side Information</title><categories>cs.IT math.IT</categories><comments>Authors' final version (to appear in IEEE Transactions on Information
  Theory)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the capacity region of the three-receiver AWGN
broadcast channel where the receivers (i) have private-message requests and
(ii) may know some of the messages requested by other receivers as side
information. We first classify all 64 possible side information configurations
into eight groups, each consisting of eight members. We next construct
transmission schemes, and derive new inner and outer bounds for the groups.
This establishes the capacity region for 52 out of 64 possible side information
configurations. For six groups (i.e., groups 1, 2, 3, 5, 6, and 8 in our
terminology), we establish the capacity region for all their members, and show
that it tightens both the best known inner and outer bounds. For group 4, our
inner and outer bounds tighten the best known inner bound and/or outer bound
for all the group members. Moreover, our bounds coincide at certain regions,
which can be characterized by two thresholds. For group 7, our inner and outer
bounds coincide for four members, thereby establishing the capacity region. For
the remaining four members, our bounds tighten both the best known inner and
outer bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5465</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5465</id><created>2014-11-20</created><authors><author><keyname>Kuriakose</keyname><forenames>Jeril</forenames></author><author><keyname>Amruth</keyname><forenames>V.</forenames></author><author><keyname>Nandhini</keyname><forenames>Swathy</forenames></author><author><keyname>Abhilash</keyname><forenames>V.</forenames></author></authors><title>Identifying Cheating Anchor Nodes using Maximum Likelihood and
  Mahalanobis Distance</title><categories>cs.CR</categories><comments>12 pages, 18 figures, IJSP. arXiv admin note: substantial text
  overlap with arXiv:1411.4437</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Malicious anchor nodes will constantly hinder genuine and appropriate
localization. Discovering the malicious or vulnerable anchor node is an
essential problem in Wireless Sensor Networks (WSNs). In wireless sensor
networks, anchor nodes are the nodes that know its current location.
Neighbouring nodes or non-anchor nodes calculate its location (or its location
reference) with the help of anchor nodes. Ingenuous localization is not
possible in the presence of a cheating anchor node or a cheating node.
Nowadays, it's a challenging task to identify the cheating anchor node or
cheating node in a network. Even after finding out the location of the cheating
anchor node, there is no assurance, that the identified node is legitimate or
not. This paper aims to localize the cheating anchor nodes using trilateration
algorithm and later associate it with maximum likelihood expectation technique
(MLE), and Mahalanobis distance to obtain maximum accuracy in identifying
malicious or cheating anchor nodes during localization. We were able to attain
a considerable reduction in the error achieved during localization. For
implementation purpose we simulated our scheme using ns-3 network simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5472</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5472</id><created>2014-11-20</created><authors><author><keyname>Kowaluk</keyname><forenames>Miros&#x142;aw</forenames></author><author><keyname>Majewska</keyname><forenames>Gabriela</forenames></author></authors><title>Multidimensional $\beta$-skeletons in $L_1$ and $L_{\infty}$ metric</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $\beta$-skeleton $\{G_{\beta}(V)\}$ for a point set V is a family of
geometric graphs, defined by the notion of neighborhoods parameterized by real
number $0 &lt; \beta &lt; \infty$. By using the distance-based version definition of
$\beta$-skeletons we study those graphs for a set of points in $\mathbb{R}^d$
space with $l_1$ and $l_{\infty}$ metrics. We present algorithms for the entire
spectrum of $\beta$ values and we discuss properties of lens-based and
circle-based $\beta$-skeletons in those metrics.
  Let $V \in \mathbb{R}^d$ in $L_{\infty}$ metric be a set of $n$ points in
general position. Then, for $\beta&lt;2$ lens-based $\beta$-skeleton
$G_{\beta}(V)$ can be computed in $O(n^2 \log^d n)$ time. For $\beta \geq 2$
there exists an $O(n \log^{d-1} n)$ time algorithm that constructs
$\beta$-skeleton for the set $V$. We show that in $\mathbb{R}^d$ with
$L_{\infty}$ metric, for $\beta&lt;2$ $\beta$-skeleton $G_{\beta}(V)$ for $n$
points can be computed in $O(n^2 \log^d n)$ time. For $\beta \geq 2$ there
exists an $O(n \log^{d-1} n)$ time algorithm. In $\mathbb{R}^d$ with $L_1$
metric for a set of $n$ points in arbitrary position $\beta$-skeleton
$G_{\beta}(V)$ can be computed in $O(n^2 \log^{d+2} n)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5474</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5474</id><created>2014-11-20</created><updated>2015-05-26</updated><authors><author><keyname>Peltom&#xe4;ki</keyname><forenames>Jarkko</forenames></author></authors><title>Characterization of repetitions in Sturmian words: A new proof</title><categories>cs.DM</categories><comments>9 pages, 1 figure</comments><msc-class>68R15</msc-class><journal-ref>Information Processing Letters 115.11 (2015), 886-891</journal-ref><doi>10.1016/j.ipl.2015.05.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new, dynamical way to study powers (that is, repetitions) in
Sturmian words based on results from Diophantine approximation theory. As a
result, we provide an alternative and shorter proof of a result by Damanik and
Lenz characterizing powers in Sturmian words [Powers in Sturmian sequences,
Eur. J. Combin. 24 (2003), 377--390]. Further, as a consequence, we obtain a
previously known formula for the fractional index of a Sturmian word based on
the continued fraction expansion of its slope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5494</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5494</id><created>2014-11-20</created><authors><author><keyname>Bova</keyname><forenames>Simone</forenames></author><author><keyname>Slivovsky</keyname><forenames>Friedrich</forenames></author></authors><title>On Compiling Structured CNFs to OBDDs</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new results on the size of OBDD representations of structurally
characterized classes of CNF formulas. First, we identify a natural sufficient
condition, which we call the few subterms property, for a class of CNFs to have
polynomial OBDD size; we then prove that CNFs whose incidence graphs are
variable convex have few subterms (and hence have polynomial OBDD size), and
observe that the few subterms property also explains the known fact that
classes of CNFs of bounded treewidth have polynomial OBDD size. Second, we
prove an exponential lower bound on the OBDD size of a family of CNF classes
with incidence graphs of bounded degree, exploiting the combinatorial
properties of expander graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5547</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5547</id><created>2014-11-20</created><authors><author><keyname>Tassi</keyname><forenames>Andrea</forenames></author><author><keyname>Chatzigeorgiou</keyname><forenames>Ioannis</forenames></author><author><keyname>Vukobratovi&#x107;</keyname><forenames>Dejan</forenames></author></authors><title>Resource Allocation Frameworks for Network-coded Layered Multimedia
  Multicast Services</title><categories>cs.IT cs.MM cs.NI cs.PF math.IT</categories><comments>IEEE Journal on Selected Areas in Communications - Special Issue on
  Fundamental Approaches to Network Coding in Wireless Communication Systems.
  To appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The explosive growth of content-on-the-move, such as video streaming to
mobile devices, has propelled research on multimedia broadcast and multicast
schemes. Multi-rate transmission strategies have been proposed as a means of
delivering layered services to users experiencing different downlink channel
conditions. In this paper, we consider Point-to-Multipoint layered service
delivery across a generic cellular system and improve it by applying different
random linear network coding approaches. We derive packet error probability
expressions and use them as performance metrics in the formulation of resource
allocation frameworks. The aim of these frameworks is both the optimization of
the transmission scheme and the minimization of the number of broadcast packets
on each downlink channel, while offering service guarantees to a predetermined
fraction of users. As a case of study, our proposed frameworks are then adapted
to the LTE-A standard and the eMBMS technology. We focus on the delivery of a
video service based on the H.264/SVC standard and demonstrate the advantages of
layered network coding over multi-rate transmission. Furthermore, we establish
that the choice of both the network coding technique and resource allocation
method play a critical role on the network footprint, and the quality of each
received video layer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5548</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5548</id><created>2014-11-20</created><authors><author><keyname>Simsek</keyname><forenames>Meryem</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Guvenc</keyname><forenames>Ismail</forenames></author></authors><title>Learning Based Frequency- and Time-Domain Inter-Cell Interference
  Coordination in HetNets</title><categories>cs.NI</categories><doi>10.1109/TVT.2014.2374237</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we focus on inter-cell interference coordination (ICIC)
techniques in heterogeneous network (Het-Net) deployments, whereby macro- and
picocells autonomously optimize their downlink transmissions, with loose
coordination. We model this strategic coexistence as a multi-agent system,
aiming at joint interference management and cell association. Using tools from
Reinforcement Learning (RL), agents (i.e., macro- and picocells) sense their
environment, and self-adapt based on local information so as to maximize their
network performance. Specifically, we explore both time- and frequency domain
ICIC scenarios, and propose a two-level RL formulation. Here, picocells learn
their optimal cell range expansion (CRE) bias and transmit power allocation, as
well as appropriate frequency bands for multi-flow transmissions, in which a
user equipment (UE) can be simultaneously served by two or more base stations
(BSs) from macro- and pico-layers. To substantiate our theoretical findings,
Long Term Evolution Advanced (LTEA) based system level simulations are carried
out in which our proposed approaches are compared with a number of baseline
approaches, such as resource partitioning (RP), static CRE, and single-flow
Carrier Aggregation (CA). Our proposed solutions yield substantial gains up to
125% compared to static ICIC approaches in terms of average UE throughput in
the timedomain. In the frequency-domain our proposed solutions yield gains up
to 240% in terms of cell-edge UE throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5553</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5553</id><created>2014-11-20</created><authors><author><keyname>Perotti</keyname><forenames>Juan Ignacio</forenames></author><author><keyname>Jo</keyname><forenames>Hang-Hyun</forenames></author><author><keyname>Holme</keyname><forenames>Petter</forenames></author><author><keyname>Saram&#xe4;ki</keyname><forenames>Jari</forenames></author></authors><title>Temporal network sparsity and the slowing down of spreading</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactions in time-varying complex systems are often very heterogeneous at
the topological level (who interacts with whom) and at the temporal level (when
interactions occur and how often). While it is known that temporal
heterogeneities often have strong effects on dynamical processes, e.g. the
burstiness of contact sequences is associated with slower spreading dynamics,
the picture is far from complete. In this paper, we show that temporal
heterogeneities result in temporal sparsity} at the time scale of average
inter-event times, and that temporal sparsity determines the amount of slowdown
of Susceptible-Infectious (SI) spreading dynamics on temporal networks. This
result is based on the analysis of several empirical temporal network data
sets. An approximate solution for a simple network model confirms the
association between temporal sparsity and slowdown of SI spreading dynamics.
Since deterministic SI spreading always follows the fastest temporal paths, our
results generalize -- paths are slower to traverse because of temporal
sparsity, and therefore all dynamical processes are slower as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5555</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5555</id><created>2014-11-20</created><authors><author><keyname>Savchenko</keyname><forenames>Andrey</forenames></author></authors><title>Maximum Likelihood Directed Enumeration Method in Piecewise-Regular
  Object Recognition</title><categories>cs.CV</categories><comments>13 pages, 6 figures, 20 references</comments><msc-class>68T10</msc-class><acm-class>I.5.1; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the problems of classification of composite object (images, speech
signals) with low number of models per class. We study the question of
improving recognition performance for medium-sized database (thousands of
classes). The key issue of fast approximate nearest-neighbor methods widely
applied in this task is their heuristic nature. It is possible to strongly
prove their efficiency by using the theory of algorithms only for simple
similarity measures and artificially generated tasks. On the contrary, in this
paper we propose an alternative, statistically optimal greedy algorithm. At
each step of this algorithm joint density (likelihood) of distances to
previously checked models is estimated for each class. The next model to check
is selected from the class with the maximal likelihood. The latter is estimated
based on the asymptotic properties of the Kullback-Leibler information
discrimination and mathematical model of piecewise-regular object with
distribution of each regular segment of exponential type. Experimental results
in face recognition for FERET dataset prove that the proposed method is much
more effective than not only brute force and the baseline (directed enumeration
method) but also approximate nearest neighbor methods from FLANN and
NonMetricSpaceLib libraries (randomized kd-tree, composite index, perm-sort).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5563</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5563</id><created>2014-11-20</created><authors><author><keyname>Burgess</keyname><forenames>Mark</forenames></author></authors><title>Spacetimes with Semantics</title><categories>cs.MA</categories><comments>Lengthy notes with background materials, laying foundations for
  subsequent work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relationships between objects constitute our notion of space. When these
relationships change we interpret this as the passage of time. Observer
interpretations are essential to the way we understand these relationships.
Hence observer semantics are an integral part of what we mean by spacetime.
  Semantics make up the essential difference in how one describes and uses the
concept of space in physics, chemistry, biology and technology. In these notes,
I have tried to assemble what seems to be a set of natural, and pragmatic,
considerations about discrete, finite spacetimes, to unify descriptions of
these areas.
  It reviews familiar notions of spacetime, and brings them together into a
less familiar framework of promise theory (autonomous agents), in order to
illuminate the goal of encoding the semantics of observers into a description
of spacetime itself. Autonomous agents provide an exacting atomic and local
model for finite spacetime, which quickly reveals the issues of incomplete
information and non-locality. From this we should be able to reconstruct all
other notions of spacetime.
  The aim of this exercise is to apply related tools and ideas to an initial
unification of real and artificial spaces, e.g. databases and information webs
with natural spacetime. By reconstructing these spaces from autonomous agents,
we may better understand naming and coordinatization of semantic spaces, from
crowds and swarms to datacentres and libraries, as well as the fundamental
arena of natural science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5573</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5573</id><created>2014-11-20</created><authors><author><keyname>Morales</keyname><forenames>Jose F.</forenames></author><author><keyname>Carro</keyname><forenames>Manuel</forenames></author><author><keyname>Hermenegildo</keyname><forenames>Manuel</forenames></author></authors><title>Description and Optimization of Abstract Machines in a Dialect of Prolog</title><categories>cs.PL</categories><comments>56 pages, 46 figures, 5 tables, To appear in Theory and Practice of
  Logic Programming (TPLP)</comments><acm-class>D.1.6; D.3.3; D.3.4</acm-class><journal-ref>Theory and Practice of Logic Programming 16 (2015) 1-58</journal-ref><doi>10.1017/S1471068414000672</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to achieve competitive performance, abstract machines for Prolog and
related languages end up being large and intricate, and incorporate
sophisticated optimizations, both at the design and at the implementation
levels. At the same time, efficiency considerations make it necessary to use
low-level languages in their implementation. This makes them laborious to code,
optimize, and, especially, maintain and extend. Writing the abstract machine
(and ancillary code) in a higher-level language can help tame this inherent
complexity. We show how the semantics of most basic components of an efficient
virtual machine for Prolog can be described using (a variant of) Prolog. These
descriptions are then compiled to C and assembled to build a complete bytecode
emulator. Thanks to the high level of the language used and its closeness to
Prolog, the abstract machine description can be manipulated using standard
Prolog compilation and optimization techniques with relative ease. We also show
how, by applying program transformations selectively, we obtain abstract
machine implementations whose performance can match and even exceed that of
state-of-the-art, highly-tuned, hand-crafted emulators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5595</identifier>
 <datestamp>2014-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5595</id><created>2014-11-20</created><updated>2014-11-26</updated><authors><author><keyname>Shi</keyname><forenames>Tianze</forenames></author><author><keyname>Liu</keyname><forenames>Zhiyuan</forenames></author></authors><title>Linking GloVe with word2vec</title><categories>cs.CL cs.LG stat.ML</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Global Vectors for word representation (GloVe), introduced by Jeffrey
Pennington et al. is reported to be an efficient and effective method for
learning vector representations of words. State-of-the-art performance is also
provided by skip-gram with negative-sampling (SGNS) implemented in the word2vec
tool. In this note, we explain the similarities between the training objectives
of the two models, and show that the objective of SGNS is similar to the
objective of a specialized form of GloVe, though their cost functions are
defined differently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5599</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5599</id><created>2014-11-20</created><updated>2015-05-07</updated><authors><author><keyname>Estrada</keyname><forenames>Ernesto</forenames></author><author><keyname>Arrigo</keyname><forenames>Francesca</forenames></author></authors><title>Predicting triadic closure in networks using communicability distance
  functions</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a communication-driven mechanism for predicting triadic closure in
complex networks. It is mathematically formulated on the basis of
communicability distance functions that account for the quality of
communication between nodes in the network. We study $25$ real-world networks
and show that the proposed method predicts correctly $20\%$ of triadic closures
in these networks, in contrast to the $7.6\%$ predicted by a random mechanism.
We also show that the communication-driven method outperforms the random
mechanism in explaining the clustering coefficient, average path length, and
average communicability. The new method also displays some interesting features
with regards to optimizing communication in networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5611</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5611</id><created>2014-11-20</created><updated>2014-12-12</updated><authors><author><keyname>S&#xe9;nizergues</keyname><forenames>G&#xe9;raud</forenames><affiliation>Bordeaux, France</affiliation></author></authors><title>FREC 14: FRontiers of RECognizability</title><categories>cs.FL cs.LO</categories><comments>Proceedings of the conference FREC 14 held in Marseille, 38-30 April
  2014. 12 papers contributed by 14 authors</comments><acm-class>F.1.1; F.4.1; F.4.3; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These proceedings are gathering twelve different research papers developping
the theory of recognizability for various kinds of discrete objects: words.
terms, graphs, etc...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5620</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5620</id><created>2014-11-20</created><updated>2016-01-15</updated><authors><author><keyname>Carli</keyname><forenames>Francesca Paola</forenames></author><author><keyname>Chen</keyname><forenames>Tianshi</forenames></author><author><keyname>Ljung</keyname><forenames>Lennart</forenames></author></authors><title>Maximum Entropy Kernels for System Identification</title><categories>math.OC cs.IT math.IT stat.ML</categories><comments>Extends results of 2014 IEEE MSC Conference Proceedings
  (arXiv:1406.5706)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new nonparametric approach for system identification has been recently
proposed where the impulse response is modeled as the realization of a
zero-mean Gaussian process whose covariance (kernel) has to be estimated from
data. In this scheme, quality of the estimates crucially depends on the
parametrization of the covariance of the Gaussian process. A family of kernels
that have been shown to be particularly effective in the system identification
framework is the family of Diagonal/Correlated (DC) kernels. Maximum entropy
properties of a related family of kernels, the Tuned/Correlated (TC) kernels,
have been recently pointed out in the literature. In this paper we show that
maximum entropy properties indeed extend to the whole family of DC kernels. The
maximum entropy interpretation can be exploited in conjunction with results on
matrix completion problems in the graphical models literature to shed light on
the structure of the DC kernel. In particular, we prove that the DC kernel
admits a closed-form factorization, inverse and determinant. These results can
be exploited both to improve the numerical stability and to reduce the
computational complexity associated with the computation of the DC estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5630</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5630</id><created>2014-11-20</created><updated>2015-07-11</updated><authors><author><keyname>Li</keyname><forenames>Shi</forenames></author></authors><title>Approximating capacitated $k$-median with $(1+\epsilon)k$ open
  facilities</title><categories>cs.DS</categories><comments>17 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the capacitated $k$-median (\CKM) problem, we are given a set $F$ of
facilities, each facility $i \in F$ with a capacity $u_i$, a set $C$ of
clients, a metric $d$ over $F \cup C$ and an integer $k$. The goal is to open
$k$ facilities in $F$ and connect the clients $C$ to the open facilities such
that each facility $i$ is connected by at most $u_i$ clients, so as to minimize
the total connection cost.
  In this paper, we give the first constant approximation for \CKM, that only
violates the cardinality constraint by a factor of $1+\epsilon$. This
generalizes the result of [Li15], which only works for the uniform capacitated
case. Moreover, the approximation ratio we obtain is
$O\big(\frac{1}{\epsilon^2}\log\frac1\epsilon\big)$, which is an exponential
improvement over the ratio of $\exp(O(1/\epsilon^2))$ in [Li15]. The natural LP
relaxation for the problem, which almost all previous algorithms for \CKM are
based on, has unbounded integrality gap even if $(2-\epsilon)k$ facilities can
be opened. We introduce a novel configuration LP for the problem, that
overcomes this integrality gap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5635</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5635</id><created>2014-11-20</created><updated>2014-12-02</updated><authors><author><keyname>Schulz</keyname><forenames>Claudia</forenames></author><author><keyname>Toni</keyname><forenames>Francesca</forenames></author></authors><title>Justifying Answer Sets using Argumentation</title><categories>cs.AI</categories><comments>This article has been accepted for publication in Theory and Practice
  of Logic Programming</comments><acm-class>I.2.3; I.2.4; F.4.1</acm-class><journal-ref>Theory and Practice of Logic Programming 16 (2015) 59-110</journal-ref><doi>10.1017/S1471068414000702</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An answer set is a plain set of literals which has no further structure that
would explain why certain literals are part of it and why others are not. We
show how argumentation theory can help to explain why a literal is or is not
contained in a given answer set by defining two justification methods, both of
which make use of the correspondence between answer sets of a logic program and
stable extensions of the Assumption-Based Argumentation (ABA) framework
constructed from the same logic program. Attack Trees justify a literal in
argumentation-theoretic terms, i.e. using arguments and attacks between them,
whereas ABA-Based Answer Set Justifications express the same justification
structure in logic programming terms, that is using literals and their
relationships. Interestingly, an ABA-Based Answer Set Justification corresponds
to an admissible fragment of the answer set in question, and an Attack Tree
corresponds to an admissible fragment of the stable extension corresponding to
this answer set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5649</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5649</id><created>2014-11-20</created><updated>2015-01-08</updated><authors><author><keyname>Flajolet</keyname><forenames>Arthur</forenames></author><author><keyname>Jaillet</keyname><forenames>Patrick</forenames></author></authors><title>No-Regret Learnability for Piecewise Linear Losses</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the convex optimization approach to online regret minimization, many
methods have been developed to guarantee a $O(\sqrt{T})$ regret bound for
subdifferentiable convex loss functions with bounded subgradients by means of a
reduction to bounded linear loss functions. This suggests that the latter tend
to be the hardest loss functions to learn against. We investigate this question
in a systematic fashion as a function of the decision set and the environment's
set of moves. On the one hand, we exhibit a localization property for linear
losses leading to $o(\sqrt{T})$ learning rates and provide examples where this
property holds. On the other hand, we establish $\Omega(\sqrt{T})$ lower bounds
on the minimum achievable regret for a class of piecewise linear loss functions
that subsumes the class of bounded linear loss functions and for polyhedral
decision sets. These results hold in a completely adversarial setting. In
contrast, we show that the minimum achievable regret can be significantly
smaller when the opponent is greedy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5654</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5654</id><created>2014-11-20</created><authors><author><keyname>Chen</keyname><forenames>Xinlei</forenames></author><author><keyname>Zitnick</keyname><forenames>C. Lawrence</forenames></author></authors><title>Learning a Recurrent Visual Representation for Image Caption Generation</title><categories>cs.CV cs.AI cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore the bi-directional mapping between images and their
sentence-based descriptions. We propose learning this mapping using a recurrent
neural network. Unlike previous approaches that map both sentences and images
to a common embedding, we enable the generation of novel sentences given an
image. Using the same model, we can also reconstruct the visual features
associated with an image given its visual description. We use a novel recurrent
visual memory that automatically learns to remember long-term visual concepts
to aid in both sentence generation and visual feature reconstruction. We
evaluate our approach on several tasks. These include sentence generation,
sentence retrieval and image retrieval. State-of-the-art results are shown for
the task of generating novel image descriptions. When compared to human
generated captions, our automatically generated captions are preferred by
humans over $19.8\%$ of the time. Results are better than or comparable to
state-of-the-art results on the image and sentence retrieval tasks for methods
using similar visual features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5661</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5661</id><created>2014-11-20</created><updated>2015-11-09</updated><authors><author><keyname>Khachatrian</keyname><forenames>Hrant H.</forenames></author><author><keyname>Petrosyan</keyname><forenames>Petros A.</forenames></author></authors><title>Interval edge-colorings of complete graphs</title><categories>cs.DM math.CO</categories><msc-class>05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An edge-coloring of a graph $G$ with colors $1,2,\ldots,t$ is an interval
$t$-coloring if all colors are used, and the colors of edges incident to each
vertex of $G$ are distinct and form an interval of integers. A graph $G$ is
interval colorable if it has an interval $t$-coloring for some positive integer
$t$. For an interval colorable graph $G$, $W(G)$ denotes the greatest value of
$t$ for which $G$ has an interval $t$-coloring. It is known that the complete
graph is interval colorable if and only if the number of its vertices is even.
However, the exact value of $W(K_{2n})$ is known only for $n \leq 4$. The
second author showed that if $n = p2^q$, where $p$ is odd and $q$ is
nonnegative, then $W(K_{2n}) \geq 4n-2-p-q$. Later, he conjectured that if $n
\in \mathbb{N}$, then $W(K_{2n}) = 4n - 2 - \left\lfloor\log_2{n}\right\rfloor
- \left \| n_2 \right \|$, where $\left \| n_2 \right \|$ is the number of
$1$'s in the binary representation of $n$.
  In this paper we introduce a new technique to construct interval colorings of
complete graphs based on their 1-factorizations, which is used to disprove the
conjecture, improve lower and upper bounds on $W(K_{2n})$ and determine its
exact values for $n \leq 12$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5668</identifier>
 <datestamp>2015-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5668</id><created>2014-11-20</created><updated>2015-02-06</updated><authors><author><keyname>Herbert-Voss</keyname><forenames>Ariel</forenames></author><author><keyname>Hirn</keyname><forenames>Matthew J.</forenames></author><author><keyname>McCollum</keyname><forenames>Frederick</forenames></author></authors><title>Computing minimal interpolants in $C^{1,1}(\mathbb{R}^d)$</title><categories>math.NA cs.DS cs.NA math.CA</categories><comments>39 pages, 5 figures, 2 tables. Replaces arXiv:1307.3292. Submitted.
  v2: Minor edits, formatting changed</comments><msc-class>26B35, 41A05, 41A58, 41A63, 52A41, 65D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following interpolation problem. Suppose one is given a
finite set $E \subset \mathbb{R}^d$, a function $f: E \rightarrow \mathbb{R}$,
and possibly the gradients of $f$ at the points of $E$ as well. We want to
interpolate the given information with a function $F \in C^{1,1}(\mathbb{R}^d)$
with the minimum possible value of $\mathrm{Lip} (\nabla F)$. We present
practical, efficient algorithms for constructing an $F$ such that
$\mathrm{Lip}(\nabla F)$ is minimal, or for less computational effort, within a
small dimensionless constant of being minimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5679</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5679</id><created>2014-11-20</created><authors><author><keyname>Kim</keyname><forenames>Bryce M.</forenames></author></authors><title>Zeno machines and Running Turing machine for infinite time</title><categories>cs.FL</categories><comments>14 pages</comments><acm-class>F.1.1; F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores and clarifies several issues surrounding Zeno machines
and the issue of running a Turing machine for infinite time. Without a minimum
hypothetical bound on physical conditions, any magical machine can be created,
and therefore, a thesis on the bound is formulated. This paper then proves that
the halting problem algorithm for every Turing-recognizable program and every
input cannot be devised whatever method is used to exploit infinite
running-time of Turing machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5681</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5681</id><created>2014-11-20</created><updated>2015-06-15</updated><authors><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author><author><keyname>Kreutzer</keyname><forenames>Stephan</forenames></author></authors><title>The Directed Grid Theorem</title><categories>cs.DM math.CO</categories><comments>43 pages, 21 figures</comments><msc-class>05C20, 05C83</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The grid theorem, originally proved by Robertson and Seymour in Graph Minors
V in 1986, is one of the most central results in the study of graph minors. It
has found numerous applications in algorithmic graph structure theory, for
instance in bidimensionality theory, and it is the basis for several other
structure theorems developed in the graph minors project.
  In the mid-90s, Reed and Johnson, Robertson, Seymour and Thomas (see [Reed
97, Johnson, Robertson, Seymour, Thomas 01]), independently, conjectured an
analogous theorem for directed graphs, i.e. the existence of a function f : N
-&gt; N such that every digraph of directed tree-width at least f(k) contains a
directed grid of order k. In an unpublished manuscript from 2001, Johnson,
Robertson, Seymour and Thomas give a proof of this conjecture for planar
digraphs. But for over a decade, this was the most general case proved for the
Reed, Johnson, Robertson, Seymour and Thomas conjecture.
  Only very recently, this result has been extended to all classes of digraphs
excluding a fixed undirected graph as a minor (see [Kawarabayashi, Kreutzer
14]). In this paper, nearly two decades after the conjecture was made, we are
finally able to confirm the Reed, Johnson, Robertson, Seymour and Thomas
conjecture in full generality and to prove the directed grid theorem.
  As consequence of our results we are able to improve results in Reed et al.
in 1996 [Reed, Robertson, Seymour, Thomas 96] (see also [Open Problem Garden])
on disjoint cycles of length at least l and in [Kawarabayashi, Kobayashi,
Kreutzer 14] on quarter-integral disjoint paths. We expect many more
algorithmic results to follow from the grid theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5712</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5712</id><created>2014-11-20</created><updated>2015-08-16</updated><authors><author><keyname>Feldman</keyname><forenames>Michal</forenames></author><author><keyname>Geri</keyname><forenames>Ofir</forenames></author></authors><title>Do Capacity Constraints Constrain Coalitions?</title><categories>cs.GT</categories><comments>Full version of a paper from AAAI-15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study strong equilibria in symmetric capacitated cost-sharing games. In
these games, a graph with designated source $s$ and sink $t$ is given, and each
edge is associated with some cost. Each agent chooses strategically an $s$-$t$
path, knowing that the cost of each edge is shared equally between all agents
using it. Two variants of cost-sharing games have been previously studied: (i)
games where coalitions can form, and (ii) games where edges are associated with
capacities; both variants are inspired by real-life scenarios. In this work we
combine these variants and analyze strong equilibria (profiles where no
coalition can deviate) in capacitated games. This combination gives rise to new
phenomena that do not occur in the previous variants. Our contribution is
two-fold. First, we provide a topological characterization of networks that
always admit a strong equilibrium. Second, we establish tight bounds on the
efficiency loss that may be incurred due to strategic behavior, as quantified
by the strong price of anarchy (and stability) measures. Interestingly, our
results are qualitatively different than those obtained in the analysis of each
variant alone, and the combination of coalitions and capacities entails the
introduction of more refined topology classes than previously studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5713</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5713</id><created>2014-11-20</created><updated>2015-11-21</updated><authors><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Ding</keyname><forenames>Jian</forenames></author><author><keyname>Eldan</keyname><forenames>Ronen</forenames></author><author><keyname>R&#xe1;cz</keyname><forenames>Mikl&#xf3;s</forenames></author></authors><title>Testing for high-dimensional geometry in random graphs</title><categories>math.ST cs.SI math.PR stat.TH</categories><comments>28 pages; v2 contains minor changes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of detecting the presence of an underlying
high-dimensional geometric structure in a random graph. Under the null
hypothesis, the observed graph is a realization of an Erd\H{o}s-R\'enyi random
graph $G(n,p)$. Under the alternative, the graph is generated from the
$G(n,p,d)$ model, where each vertex corresponds to a latent independent random
vector uniformly distributed on the sphere $\mathbb{S}^{d-1}$, and two vertices
are connected if the corresponding latent vectors are close enough. In the
dense regime (i.e., $p$ is a constant), we propose a near-optimal and
computationally efficient testing procedure based on a new quantity which we
call signed triangles. The proof of the detection lower bound is based on a new
bound on the total variation distance between a Wishart matrix and an
appropriately normalized GOE matrix. In the sparse regime, we make a conjecture
for the optimal detection boundary. We conclude the paper with some preliminary
steps on the problem of estimating the dimension in $G(n,p,d)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5720</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5720</id><created>2014-11-20</created><authors><author><keyname>Hashimoto</keyname><forenames>Tatsunori B.</forenames></author><author><keyname>Sun</keyname><forenames>Yi</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi S.</forenames></author></authors><title>Metric recovery from directed unweighted graphs</title><categories>stat.ML cs.SI math.ST stat.ME stat.TH</categories><comments>Poster at NIPS workshop on networks. Submitted to AISTATS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze directed, unweighted graphs obtained from $x_i\in \mathbb{R}^d$ by
connecting vertex $i$ to $j$ iff $|x_i - x_j| &lt; \epsilon(x_i)$. Examples of
such graphs include $k$-nearest neighbor graphs, where $\epsilon(x_i)$ varies
from point to point, and, arguably, many real world graphs such as
co-purchasing graphs. We ask whether we can recover the underlying Euclidean
metric $\epsilon(x_i)$ and the associated density $p(x_i)$ given only the
directed graph and $d$.
  We show that consistent recovery is possible up to isometric scaling when the
vertex degree is at least $\omega(n^{2/(2+d)}\log(n)^{d/(d+2)})$. Our estimator
is based on a careful characterization of a random walk over the directed graph
and the associated continuum limit. As an algorithm, it resembles the PageRank
centrality metric. We demonstrate empirically that the estimator performs well
on simulated examples as well as on real-world co-purchasing graphs even with a
small number of points and degree scaling as low as $\log(n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5726</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5726</id><created>2014-11-20</created><updated>2015-06-02</updated><authors><author><keyname>Vedantam</keyname><forenames>Ramakrishna</forenames></author><author><keyname>Zitnick</keyname><forenames>C. Lawrence</forenames></author><author><keyname>Parikh</keyname><forenames>Devi</forenames></author></authors><title>CIDEr: Consensus-based Image Description Evaluation</title><categories>cs.CV cs.CL cs.IR</categories><comments>To appear in CVPR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatically describing an image with a sentence is a long-standing
challenge in computer vision and natural language processing. Due to recent
progress in object detection, attribute classification, action recognition,
etc., there is renewed interest in this area. However, evaluating the quality
of descriptions has proven to be challenging. We propose a novel paradigm for
evaluating image descriptions that uses human consensus. This paradigm consists
of three main parts: a new triplet-based method of collecting human annotations
to measure consensus, a new automated metric (CIDEr) that captures consensus,
and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences
describing each image. Our simple metric captures human judgment of consensus
better than existing metrics across sentences generated by various sources. We
also evaluate five state-of-the-art image description approaches using this new
protocol and provide a benchmark for future comparisons. A version of CIDEr
named CIDEr-D is available as a part of MS COCO evaluation server to enable
systematic evaluation and benchmarking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5729</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5729</id><created>2014-11-20</created><authors><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author></authors><title>Forrelation: A Problem that Optimally Separates Quantum from Classical
  Computing</title><categories>quant-ph cs.CC</categories><comments>60 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We achieve essentially the largest possible separation between quantum and
classical query complexities. We do so using a property-testing problem called
Forrelation, where one needs to decide whether one Boolean function is highly
correlated with the Fourier transform of a second function. This problem can be
solved using 1 quantum query, yet we show that any randomized algorithm needs
~sqrt(N)/log(N) queries (improving an ~N^{1/4} lower bound of Aaronson).
Conversely, we show that this 1 versus ~sqrt(N) separation is optimal: indeed,
any t-query quantum algorithm whatsoever can be simulated by an
O(N^{1-1/2t})-query randomized algorithm. Thus, resolving an open question of
Buhrman et al. from 2002, there is no partial Boolean function whose quantum
query complexity is constant and whose randomized query complexity is linear.
We conjecture that a natural generalization of Forrelation achieves the optimal
t versus ~N^{1-1/2t} separation for all t. As a bonus, we show that this
generalization is BQP-complete. This yields what's arguably the simplest
BQP-complete problem yet known, and gives a second sense in which Forrelation
&quot;captures the maximum power of quantum computation.&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5731</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5731</id><created>2014-11-20</created><authors><author><keyname>Xu</keyname><forenames>Can</forenames></author><author><keyname>Cetintas</keyname><forenames>Suleyman</forenames></author><author><keyname>Lee</keyname><forenames>Kuang-Chih</forenames></author><author><keyname>Li</keyname><forenames>Li-Jia</forenames></author></authors><title>Visual Sentiment Prediction with Deep Convolutional Neural Networks</title><categories>cs.CV cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Images have become one of the most popular types of media through which users
convey their emotions within online social networks. Although vast amount of
research is devoted to sentiment analysis of textual data, there has been very
limited work that focuses on analyzing sentiment of image data. In this work,
we propose a novel visual sentiment prediction framework that performs image
understanding with Deep Convolutional Neural Networks (CNN). Specifically, the
proposed sentiment prediction framework performs transfer learning from a CNN
with millions of parameters, which is pre-trained on large-scale data for
object recognition. Experiments conducted on two real-world datasets from
Twitter and Tumblr demonstrate the effectiveness of the proposed visual
sentiment analysis framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5732</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5732</id><created>2014-11-20</created><authors><author><keyname>Cetintas</keyname><forenames>Suleyman</forenames></author><author><keyname>Si</keyname><forenames>Luo</forenames></author><author><keyname>Xin</keyname><forenames>Yan Ping</forenames></author><author><keyname>Zhang</keyname><forenames>Dake</forenames></author><author><keyname>Park</keyname><forenames>Joo Young</forenames></author><author><keyname>Tzur</keyname><forenames>Ron</forenames></author></authors><title>A Joint Probabilistic Classification Model of Relevant and Irrelevant
  Sentences in Mathematical Word Problems</title><categories>cs.CL cs.IR cs.LG stat.ML</categories><comments>appears in Journal of Educational Data Mining (JEDM, 2010)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the difficulty level of math word problems is an important task
for many educational applications. Identification of relevant and irrelevant
sentences in math word problems is an important step for calculating the
difficulty levels of such problems. This paper addresses a novel application of
text categorization to identify two types of sentences in mathematical word
problems, namely relevant and irrelevant sentences. A novel joint probabilistic
classification model is proposed to estimate the joint probability of
classification decisions for all sentences of a math word problem by utilizing
the correlation among all sentences along with the correlation between the
question sentence and other sentences, and sentence text. The proposed model is
compared with i) a SVM classifier which makes independent classification
decisions for individual sentences by only using the sentence text and ii) a
novel SVM classifier that considers the correlation between the question
sentence and other sentences along with the sentence text. An extensive set of
experiments demonstrates the effectiveness of the joint probabilistic
classification model for identifying relevant and irrelevant sentences as well
as the novel SVM classifier that utilizes the correlation between the question
sentence and other sentences. Furthermore, empirical results and analysis show
that i) it is highly beneficial not to remove stopwords and ii) utilizing part
of speech tagging does not make a significant improvement although it has been
shown to be effective for the related task of math word problem type
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5735</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5735</id><created>2014-11-20</created><updated>2014-11-24</updated><authors><author><keyname>Zhang</keyname><forenames>Shuai</forenames></author><author><keyname>Xin</keyname><forenames>Jack</forenames></author></authors><title>Minimization of Transformed L_1 Penalty: Theory, Difference of Convex
  Function Algorithm, and Robust Application in Compressed Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the minimization problem of a non-convex sparsity promoting penalty
function, the transformed $l_1$ (TL1), and its application in compressed
sensing (CS). The TL1 penalty interpolates $l_0$ and $l_1$ norms through a
nonnegative parameter $a \in (0,+\infty)$, similar to $l_p$ with $p \in (0,1]$.
TL1 is known in the statistics literature to enjoy three desired properties:
unbiasedness, sparsity and Lipschitz continuity. We first consider the
constrained minimization problem and prove the uniqueness of global minimizer
and its equivalence to $l_0$ norm minimization if the sensing matrix $A$
satisfies a restricted isometry property (RIP) and if $a &gt; a^*$, where $a^*$
depends only on $A$. The solution is stable under noisy measurement. For
general sensing matrix $A$, we show that the support set of a local minimizer
corresponds to linearly independent columns of $A$, and recall sufficient
conditions for a critical point to be a local minimum. Next, we present
difference of convex algorithms for TL1 (DCATL1) in computing TL1-regularized
constrained and unconstrained problems in CS. For the unconstrained problem, we
prove convergence of DCALT1 to a stationary point satisfying the first order
optimality condition. Finally in numerical experiments, we identify the optimal
value $a=1$, and compare DCATL1 with other CS algorithms on three classes of
sensing matrices: Gaussian random matrices, over-sampled discrete cosine
transform matrices (ODCT), and uniformly distributed M-sphere matrices. We find
that for all three classes of sensing matrices, the performance of DCATL1
algorithm (initiated with $L_1$ minimization) always ranks near the top (if not
the top), and is the most robust choice insensitive to RIP (incoherence) of the
underlying CS problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5737</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5737</id><created>2014-11-20</created><updated>2015-10-05</updated><authors><author><keyname>Damelin</keyname><forenames>S. B.</forenames></author><author><keyname>Gu</keyname><forenames>Y.</forenames></author><author><keyname>Wunsch</keyname><forenames>D. C.</forenames><suffix>II</suffix></author><author><keyname>Xu</keyname><forenames>R.</forenames></author></authors><title>Fuzzy Adaptive Resonance Theory, Diffusion Maps and their applications
  to Clustering and Biclustering</title><categories>cs.NE cs.LG</categories><comments>Accepted in Math.Model.Nat.Phenom</comments><msc-class>94A15, 62H30, 60J20, 68T05, 68T45, 68T10</msc-class><journal-ref>Math.Model.Nat.Phenom. Vol. 10, No 3, 2015, pp. 206-211</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe an algorithm FARDiff (Fuzzy Adaptive Resonance
Dif- fusion) which combines Diffusion Maps and Fuzzy Adaptive Resonance Theory
to do clustering on high dimensional data. We describe some applications of
this method and some problems for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5739</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5739</id><created>2014-11-20</created><authors><author><keyname>Pananjady</keyname><forenames>Ashwin</forenames></author><author><keyname>Bagaria</keyname><forenames>Vivek Kumar</forenames></author><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author></authors><title>The Online Disjoint Set Cover Problem and its Applications</title><categories>cs.DS cs.NI</categories><comments>To appear in IEEE INFOCOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a universe $U$ of $n$ elements and a collection of subsets
$\mathcal{S}$ of $U$, the maximum disjoint set cover problem (DSCP) is to
partition $\mathcal{S}$ into as many set covers as possible, where a set cover
is defined as a collection of subsets whose union is $U$. We consider the
online DSCP, in which the subsets arrive one by one (possibly in an order
chosen by an adversary), and must be irrevocably assigned to some partition on
arrival with the objective of minimizing the competitive ratio. The competitive
ratio of an online DSCP algorithm $A$ is defined as the maximum ratio of the
number of disjoint set covers obtained by the optimal offline algorithm to the
number of disjoint set covers obtained by $A$ across all inputs. We propose an
online algorithm for solving the DSCP with competitive ratio $\ln n$. We then
show a lower bound of $\Omega(\sqrt{\ln n})$ on the competitive ratio for any
online DSCP algorithm. The online disjoint set cover problem has wide ranging
applications in practice, including the online crowd-sourcing problem, the
online coverage lifetime maximization problem in wireless sensor networks, and
in online resource allocation problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5752</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5752</id><created>2014-11-20</created><updated>2015-04-25</updated><authors><author><keyname>Hariharan</keyname><forenames>Bharath</forenames></author><author><keyname>Arbel&#xe1;ez</keyname><forenames>Pablo</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Hypercolumns for Object Segmentation and Fine-grained Localization</title><categories>cs.CV</categories><comments>CVPR Camera ready</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognition algorithms based on convolutional networks (CNNs) typically use
the output of the last layer as feature representation. However, the
information in this layer may be too coarse to allow precise localization. On
the contrary, earlier layers may be precise in localization but will not
capture semantics. To get the best of both worlds, we define the hypercolumn at
a pixel as the vector of activations of all CNN units above that pixel. Using
hypercolumns as pixel descriptors, we show results on three fine-grained
localization tasks: simultaneous detection and segmentation[22], where we
improve state-of-the-art from 49.7[22] mean AP^r to 60.0, keypoint
localization, where we get a 3.3 point boost over[20] and part labeling, where
we show a 6.6 point gain over a strong baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5765</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5765</id><created>2014-11-20</created><authors><author><keyname>Dernoncourt</keyname><forenames>Franck</forenames></author></authors><title>TrackMania is NP-complete</title><categories>cs.CC</categories><acm-class>F.2.0; K.8.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that completing an untimed, unbounded track in TrackMania Nations
Forever is NP-complete by using a reduction from 3-SAT and showing that a
solution can be checked in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5767</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5767</id><created>2014-11-20</created><updated>2015-07-08</updated><authors><author><keyname>Saldi</keyname><forenames>Naci</forenames></author><author><keyname>Linder</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Y&#xfc;ksel</keyname><forenames>Serdar</forenames></author></authors><title>Output Constrained Lossy Source Coding with Limited Common Randomness</title><categories>cs.IT math.IT</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a Shannon-theoretic version of the generalized
distribution preserving quantization problem where a stationary and memoryless
source is encoded subject to a distortion constraint and the additional
requirement that the reproduction also be stationary and memoryless with a
given distribution. The encoder and decoder are stochastic and assumed to have
access to independent common randomness. Recent work has characterized the
minimum achievable coding rate at a given distortion level when unlimited
common randomness is available. Here we consider the general case where the
available common randomness may be rate limited. Our main result completely
characterizes the set of achievable coding and common randomness rate pairs at
any distortion level, thereby providing the optimal tradeoff between these two
rate quantities. We also consider two variations of this problem where we
investigate the effect of relaxing the strict output distribution constraint
and the role of `private randomness' used by the decoder on the rate region.
Our results have strong connections with Cuff's recent work on distributed
channel synthesis. In particular, our achievability proof combines a coupling
argument with the approach developed by Cuff, where instead of explicitly
constructing the encoder-decoder pair, a joint distribution is constructed from
which a desired encoder-decoder pair is established. We show however that for
our problem, the separated solution of first finding an optimal channel and
then synthesizing this channel results in a suboptimal rate region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5768</identifier>
 <datestamp>2015-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5768</id><created>2014-11-20</created><updated>2015-04-15</updated><authors><author><keyname>Polyakovskiy</keyname><forenames>Sergey</forenames></author><author><keyname>Neumann</keyname><forenames>Frank</forenames></author></authors><title>Packing While Traveling: Mixed Integer Programming for a Class of
  Nonlinear Knapsack Problems</title><categories>cs.DS</categories><doi>10.1007/978-3-319-18008-3_23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Packing and vehicle routing problems play an important role in the area of
supply chain management. In this paper, we introduce a non-linear knapsack
problem that occurs when packing items along a fixed route and taking into
account travel time. We investigate constrained and unconstrained versions of
the problem and show that both are NP-hard. In order to solve the problems, we
provide a pre-processing scheme as well as exact and approximate mixed integer
programming (MIP) solutions. Our experimental results show the effectiveness of
the MIP solutions and in particular point out that the approximate MIP approach
often leads to near optimal results within far less computation time than the
exact approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5782</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5782</id><created>2014-11-21</created><authors><author><keyname>Shangguan</keyname><forenames>Chong</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Ge</keyname><forenames>Gennian</forenames></author><author><keyname>Miao</keyname><forenames>Ying</forenames></author></authors><title>New Bounds For Frameproof Codes</title><categories>cs.IT math.CO math.IT</categories><comments>7 pages, submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frameproof codes are used to fingerprint digital data. It can prevent
copyrighted materials from unauthorized use. In this paper, we study upper and
lower bounds for $w$-frameproof codes of length $N$ over an alphabet of size
$q$. The upper bound is based on a combinatorial approach and the lower bound
is based on a probabilistic construction. Both bounds can improve previous
results when $q$ is small compared to $w$, say $cq\leq w$ for some constant
$c\leq q$. Furthermore, we pay special attention to binary frameproof codes. We
show a binary $w$-frameproof code of length $N$ can not have more than $N$
codewords if $N&lt;\binom{w+1}{2}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5784</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5784</id><created>2014-11-21</created><authors><author><keyname>Li</keyname><forenames>Tao</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Xiong</keyname><forenames>Ke</forenames></author><author><keyname>Letaief</keyname><forenames>K. B.</forenames></author></authors><title>QoS-distinguished Achievable Rate Region for High Speed Railway Wireless
  Communications</title><categories>cs.IT math.IT</categories><comments>11 pages, 6 figures, we will submit it to an international journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In high speed railways (HSRs) communication system, when a train travels
along the railway with high velocity, the wireless channel between the train
and base station varies strenuously, which makes it essential to implement
appropriate power allocations to guarantee system performance. What's more, how
to evaluate the performance limits in this new scenario is also needed to
consider. To this end, this paper investigates the performance limits of
wireless communication in HSRs scenario. Since the hybrid information
transmitted between train and base station usually has diverse quality of
service (QoS) requirements, QoS-based achievable rate region is utilized to
characterize the transmission performance in this paper. It is proved that
traditional ergodic capacity and outage capacity with unique QoS requirement
can be regarded as two extreme cases of the achievable rate region proposed in
this paper. The corresponding optimal power allocation strategy is also given
to achieve the maximal boundary of achievable rate region. Compared with
conventional strategies, the advantages of the proposed strategy are validated
in terms of green communication, namely minimizing average transmit power.
Besides, the hybrid information transmission in a non-uniform generalized
motion scenario is analyzed to confirm the robust performance of proposed
strategy. The performance loss caused by non-uniform motion compared with that
in uniform motion is also indicated, where a deterministic worst case for
instantaneous speed realization is proposed to serve as the lower bound for
system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5795</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5795</id><created>2014-11-21</created><authors><author><keyname>Luo</keyname><forenames>Tie</forenames></author><author><keyname>Tham</keyname><forenames>Chen-Khong</forenames></author></authors><title>Fairness and Social Welfare in Incentivizing Participatory Sensing</title><categories>cs.GT cs.CY cs.HC cs.MA</categories><comments>Game theory; demand-supply model; network economics; stochastic
  programming; chance-constrained programming</comments><journal-ref>Proc. IEEE SECON, June 2012, pp. 425-433</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Participatory sensing has emerged recently as a promising approach to
large-scale data collection. However, without incentives for users to regularly
contribute good quality data, this method is unlikely to be viable in the long
run. In this paper, we link incentive to users' demand for consuming compelling
services, as an approach complementary to conventional credit or reputation
based approaches. With this demand-based principle, we design two incentive
schemes, Incentive with Demand Fairness (IDF) and Iterative Tank Filling (ITF),
for maximizing fairness and social welfare, respectively. Our study shows that
the IDF scheme is max-min fair and can score close to 1 on the Jain's fairness
index, while the ITF scheme maximizes social welfare and achieves a unique Nash
equilibrium which is also Pareto and globally optimal. We adopted a game
theoretic approach to derive the optimal service demands. Furthermore, to
address practical considerations, we use a stochastic programming technique to
handle uncertainty that is often encountered in real life situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5796</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5796</id><created>2014-11-21</created><authors><author><keyname>Kaur</keyname><forenames>Rajveer</forenames></author><author><keyname>Sharma</keyname><forenames>Saurabh</forenames></author></authors><title>Pre-processing of Domain Ontology Graph Generation System in Punjabi</title><categories>cs.CL</categories><comments>6 pages, 17 figures, 1 table, &quot;Published with International Journal
  of Engineering Trends and Technology (IJETT)&quot;</comments><journal-ref>International Journal of Engineering Trends and Technology
  (IJETT), V17(3),141-146, Nov 2014. Published by Seventh Sense Research Group</journal-ref><doi>10.14445/22315381/IJETT-V17P229</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes pre-processing phase of ontology graph generation system
from Punjabi text documents of different domains. This research paper focuses
on pre-processing of Punjabi text documents. Pre-processing is structured
representation of the input text. Pre-processing of ontology graph generation
includes allowing input restrictions to the text, removal of special symbols
and punctuation marks, removal of duplicate terms, removal of stop words,
extract terms by matching input terms with dictionary and gazetteer lists
terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5797</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5797</id><created>2014-11-21</created><updated>2015-10-30</updated><authors><author><keyname>Clark</keyname><forenames>Andrew</forenames></author><author><keyname>Alomair</keyname><forenames>Basel</forenames></author><author><keyname>Bushnell</keyname><forenames>Linda</forenames></author><author><keyname>Poovendran</keyname><forenames>Radha</forenames></author></authors><title>Global Practical Node and Edge Synchronization in Kuramoto Networks: A
  Submodular Optimization Framework</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synchronization underlies phenomena including memory and perception in the
brain, coordinated motion of animal flocks, and stability of the power grid.
These synchronization phenomena are often modeled through networks of
phase-coupled oscillating nodes. Heterogeneity in the node dynamics, however,
may prevent such networks from achieving the required level of synchronization.
In order to guarantee synchronization, external inputs can be used to pin a
subset of nodes to a reference frequency, while the remaining nodes are steered
toward synchronization via local coupling. In this paper, we present a
submodular optimization framework for selecting a set of nodes to act as
external inputs in order to achieve synchronization from almost any initial
network state. We derive threshold-based sufficient conditions for
synchronization, and then prove that these conditions are equivalent to
connectivity of a class of augmented network graphs. Based on this connection,
we map the sufficient conditions for synchronization to constraints on
submodular functions, leading to efficient algorithms with provable optimality
bounds for selecting input nodes. We illustrate our approach via numerical
studies of synchronization in networks from power systems, wireless networks,
and neuronal networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5820</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5820</id><created>2014-11-21</created><updated>2015-07-28</updated><authors><author><keyname>Berwanger</keyname><forenames>Dietmar</forenames></author><author><keyname>Mathew</keyname><forenames>Anup Basil</forenames></author></authors><title>Infinite games with finite knowledge gaps</title><categories>cs.GT</categories><comments>39 pages; 2nd revision; submitted to Information and Computation</comments><msc-class>05C57, 68M14, 91A06, 91A28, 93B50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Infinite games where several players seek to coordinate under imperfect
information are deemed to be undecidable, unless the information is
hierarchically ordered among the players.
  We identify a class of games for which joint winning strategies can be
constructed effectively without restricting the direction of information flow.
Instead, our condition requires that the players attain common knowledge about
the actual state of the game over and over again along every play.
  We show that it is decidable whether a given game satisfies the condition,
and prove tight complexity bounds for the strategy synthesis problem under
$\omega$-regular winning conditions given by parity automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5822</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5822</id><created>2014-11-21</created><authors><author><keyname>Kokkala</keyname><forenames>Janne I.</forenames></author><author><keyname>Krotov</keyname><forenames>Denis S.</forenames></author><author><keyname>&#xd6;sterg&#xe5;rd</keyname><forenames>Patric R. J.</forenames></author></authors><title>On the Classification of MDS Codes</title><categories>cs.IT math.CO math.IT</categories><comments>Submitted to IEEE transactions on Information Theory; presented in
  part at the 4th International Castle Meeting in Coding Theory and
  Applications, Palmela, Portugal, September 2014</comments><journal-ref>IEEE Trans. Inf. Theory 61(12) 2015, 6485-6492</journal-ref><doi>10.1109/TIT.2015.2488659</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $q$-ary code of length $n$, size $M$, and minimum distance $d$ is called an
$(n,M,d)_q$ code. An $(n,q^{k},n-k+1)_q$ code is called a maximum distance
separable (MDS) code. In this work, some MDS codes over small alphabets are
classified. It is shown that every $(k+d-1,q^k,d)_q$ code with $k\geq 3$, $d
\geq 3$, $q \in \{5,7\}$ is equivalent to a linear code with the same
parameters. This implies that the $(6,5^4,3)_5$ code and the $(n,7^{n-2},3)_7$
MDS codes for $n\in\{6,7,8\}$ are unique. The classification of
one-error-correcting $8$-ary MDS codes is also finished; there are $14$, $8$,
$4$, and $4$ equivalence classes of $(n,8^{n-2},3)_8$ codes for $n=6,7,8,9$,
respectively. One of the equivalence classes of perfect $(9,8^7,3)_8$ codes
corresponds to the Hamming code and the other three are nonlinear codes for
which there exists no previously known construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5825</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5825</id><created>2014-11-21</created><authors><author><keyname>Veta</keyname><forenames>Mitko</forenames></author><author><keyname>van Diest</keyname><forenames>Paul J.</forenames></author><author><keyname>Willems</keyname><forenames>Stefan M.</forenames></author><author><keyname>Wang</keyname><forenames>Haibo</forenames></author><author><keyname>Madabhushi</keyname><forenames>Anant</forenames></author><author><keyname>Cruz-Roa</keyname><forenames>Angel</forenames></author><author><keyname>Gonzalez</keyname><forenames>Fabio</forenames></author><author><keyname>Larsen</keyname><forenames>Anders B. L.</forenames></author><author><keyname>Vestergaard</keyname><forenames>Jacob S.</forenames></author><author><keyname>Dahl</keyname><forenames>Anders B.</forenames></author><author><keyname>Cire&#x15f;an</keyname><forenames>Dan C.</forenames></author><author><keyname>Schmidhuber</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Giusti</keyname><forenames>Alessandro</forenames></author><author><keyname>Gambardella</keyname><forenames>Luca M.</forenames></author><author><keyname>Tek</keyname><forenames>F. Boray</forenames></author><author><keyname>Walter</keyname><forenames>Thomas</forenames></author><author><keyname>Wang</keyname><forenames>Ching-Wei</forenames></author><author><keyname>Kondo</keyname><forenames>Satoshi</forenames></author><author><keyname>Matuszewski</keyname><forenames>Bogdan J.</forenames></author><author><keyname>Precioso</keyname><forenames>Frederic</forenames></author><author><keyname>Snell</keyname><forenames>Violet</forenames></author><author><keyname>Kittler</keyname><forenames>Josef</forenames></author><author><keyname>de Campos</keyname><forenames>Teofilo E.</forenames></author><author><keyname>Khan</keyname><forenames>Adnan M.</forenames></author><author><keyname>Rajpoot</keyname><forenames>Nasir M.</forenames></author><author><keyname>Arkoumani</keyname><forenames>Evdokia</forenames></author><author><keyname>Lacle</keyname><forenames>Miangela M.</forenames></author><author><keyname>Viergever</keyname><forenames>Max A.</forenames></author><author><keyname>Pluim</keyname><forenames>Josien P. W.</forenames></author></authors><title>Assessment of algorithms for mitosis detection in breast cancer
  histopathology images</title><categories>cs.CV</categories><comments>23 pages, 5 figures, accepted for publication in the journal Medical
  Image Analysis</comments><doi>10.1016/j.media.2014.11.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferative activity of breast tumors, which is routinely estimated by
counting of mitotic figures in hematoxylin and eosin stained histology
sections, is considered to be one of the most important prognostic markers.
However, mitosis counting is laborious, subjective and may suffer from low
inter-observer agreement. With the wider acceptance of whole slide images in
pathology labs, automatic image analysis has been proposed as a potential
solution for these issues. In this paper, the results from the Assessment of
Mitosis Detection Algorithms 2013 (AMIDA13) challenge are described. The
challenge was based on a data set consisting of 12 training and 11 testing
subjects, with more than one thousand annotated mitotic figures by multiple
observers. Short descriptions and results from the evaluation of eleven methods
are presented. The top performing method has an error rate that is comparable
to the inter-observer agreement among pathologists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5847</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5847</id><created>2014-11-21</created><updated>2015-03-28</updated><authors><author><keyname>Adj&#xe9;</keyname><forenames>Assal&#xe9;</forenames></author><author><keyname>Garoche</keyname><forenames>Pierre-Lo&#xef;c</forenames></author><author><keyname>Werey</keyname><forenames>Alexis</forenames></author></authors><title>Quadratic Zonotopes:An extension of Zonotopes to Quadratic Arithmetics</title><categories>cs.LO math.OC</categories><comments>17 pages, 5 figures, 1 table</comments><acm-class>D.2.4; F.3.1; F.3.2; G.1.0; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Affine forms are a common way to represent convex sets of $\mathbb{R}$ using
a base of error terms $\epsilon \in [-1, 1]^m$. Quadratic forms are an
extension of affine forms enabling the use of quadratic error terms $\epsilon_i
\epsilon_j$.
  In static analysis, the zonotope domain, a relational abstract domain based
on affine forms has been used in a wide set of settings, e.g. set-based
simulation for hybrid systems, or floating point analysis, providing relational
abstraction of functions with a cost linear in the number of errors terms.
  In this paper, we propose a quadratic version of zonotopes. We also present a
new algorithm based on semi-definite programming to project a quadratic
zonotope, and therefore quadratic forms, to intervals. All presented material
has been implemented and applied on representative examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5849</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5849</id><created>2014-11-21</created><authors><author><keyname>S&#xe6;ther</keyname><forenames>Sigve Hortemo</forenames></author></authors><title>Solving Hamiltonian Cycle by an EPT Algorithm for a Non-sparse Parameter</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many hard graph problems, such as Hamiltonian Cycle, become FPT when
parameterized by treewidth, a parameter that is bounded only on sparse graphs.
When parameterized by the more general parameter clique-width, Hamiltonian
Cycle becomes W[1]-hard, as shown by Fomin et al. [5]. S{\ae}ther and Telle
address this problem in their paper [13] by introducing a new parameter,
split-matching-width, which lies between treewidth and clique-width in terms of
generality. They show that even though graphs of restricted
split-matching-width might be dense, solving problems such as Hamiltonian Cycle
can be done in FPT time.
  Recently, it was shown that Hamiltonian Cycle parameterized by treewidth is
in EPT [1, 6], meaning it can be solved in $n^{O(1)} 2^{O(k)}$-time. In this
paper, using tools from [6], we show that also parameterized by
split-matching-width Hamiltonian Cycle is EPT. To the best of our knowledge,
this is the first EPT algorithm for any &quot;globally constrained&quot; graph problem
parameterized by a non-trivial and non-sparse structural parameter. To
accomplish this, we also give an algorithm constructing a branch decomposition
approximating the minimum split-matching-width to within a constant factor.
Combined, these results show that the algorithms in [13] for Edge Dominating
Set, Chromatic Number and Max Cut all can be improved. We also show that for
Hamiltonian Cycle and Max Cut the resulting algorithms are asymptotically
optimal under the Exponential Time Hypothesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5853</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5853</id><created>2014-11-21</created><updated>2014-12-02</updated><authors><author><keyname>D&#xfa;nlaing</keyname><forenames>Colm &#xd3;</forenames></author></authors><title>An ACCL which is not a CRCL</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is fairly easy to show that every regular set is an almost-confluent
congruential language (ACCL), and it is known that every regular set is a
Church-Rosser congruential language (CRCL). Whether there exists an ACCL, which
is not a CRCL, seems to remain an open question. In this note we present one
such ACCL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5861</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5861</id><created>2014-11-21</created><authors><author><keyname>Karrila</keyname><forenames>Alex</forenames></author><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author></authors><title>A Comparison of Skewed and Orthogonal Lattices in Gaussian Wiretap
  Channels</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider lattice coset-coded transmissions over a wiretap channel with
additive white Gaussian noise (AWGN). Examining a function that can be
interpreted as either the legitimate receiver's error probability or the
eavesdropper's correct decision probability, we rigorously show that, albeit
offering simple bit labeling, orthogonal nested lattices are suboptimal for
coset coding in terms of both the legitimate receiver's and the eavesdropper's
probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5867</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5867</id><created>2014-11-21</created><updated>2015-04-04</updated><authors><author><keyname>Holm</keyname><forenames>Jacob</forenames></author><author><keyname>Rotenberg</keyname><forenames>Eva</forenames></author><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>Planar Reachability in Linear Space and Constant Time</title><categories>cs.DS</categories><comments>20 pages, 5 figures, submitted to FoCS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to represent a planar digraph in linear space so that distance
queries can be answered in constant time. The data structure can be constructed
in linear time. This representation of reachability is thus optimal in both
time and space, and has optimal construction time. The previous best solution
used $O(n\log n)$ space for constant query time [Thorup FOCS'01].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5869</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5869</id><created>2014-11-21</created><updated>2014-11-29</updated><authors><author><keyname>Chang</keyname><forenames>Lubin</forenames></author><author><keyname>Li</keyname><forenames>Jingshu</forenames></author><author><keyname>Li</keyname><forenames>Kailong</forenames></author></authors><title>Optimization-based Alignment for Strapdown Inertial Navigation System
  Comparison and Extension</title><categories>cs.RO</categories><comments>Accepted by IEEE Transactions on Aerospace and Electronic Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the optimization-based alignment (OBA) methods are
investigated with main focus on the vector observations construction procedures
for the strapdown inertial navigation system (SINS). The contributions of this
study are twofold. First the OBA method is extended to be able to estimate the
gyroscopes biases coupled with the attitude based on the construction process
of the existing OBA methods. This extension transforms the initial alignment
into an attitude estimation problem which can be solved using the nonlinear
filtering algorithms. The second contribution is the comprehensive evaluation
of the OBA methods and their extensions with different vector observations
construction procedures in terms of convergent speed and steady-state estimate
using field test data collected from different grades of SINS. This study is
expected to facilitate the selection of appropriate OBA methods for different
grade SINS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1411.5873</identifier>
 <datestamp>2014-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1411.5873</id><created>2014-11-21</created><authors><author><keyname>Qu</keyname><forenames>Zheng</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Randomized Dual Coordinate Ascent with Arbitrary Sampling</title><categories>math.OC cs.LG cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of minimizing the average of a large number of smooth
convex functions penalized with a strongly convex regularizer. We propose and
analyze a novel primal-dual method (Quartz) which at every iteration samples
and updates a random subset of the dual variables, chosen according to an
arbitrary distribution. In contrast to typical analysis, we directly bound the
decrease of the primal-dual error (in expectation), without the need to first
analyze the dual error. Depending on the choice of the sampling, we obtain
efficient serial, parallel and distributed variants of the method. In the
serial case, our bounds match the best known bounds for SDCA (both with uniform
and importance sampling). With standard mini-batching, our bounds predict
initial data-independent speedup as well as additional data-driven speedup
which depends on spectral and sparsity properties of the data. We calculate
theoretical speedup factors and find that they are excellent predictors of
actual speedup in practice. Moreover, we illustrate that it is possible to
design an efficient mini-batch importance sampling. The distributed variant of
Quartz is the first distributed SDCA-like method with an analysis for
non-separable data.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="68000" completeListSize="102538">1122234|69001</resumptionToken>
</ListRecords>
</OAI-PMH>
