<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T01:10:36Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|43001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3440</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3440</id><created>2013-03-14</created><authors><author><keyname>Lizier</keyname><forenames>Joseph T.</forenames></author><author><keyname>Flecker</keyname><forenames>Benjamin</forenames></author><author><keyname>Williams</keyname><forenames>Paul L.</forenames></author></authors><title>Towards a Synergy-based Approach to Measuring Information Modification</title><categories>cs.IT math.IT nlin.CG physics.data-an</categories><comments>9 pages, 4 figures</comments><msc-class>94A15</msc-class><journal-ref>Proceedings of the 2013 IEEE Symposium on Artificial Life (ALIFE),
  Singapore, p. 43-51, 2013</journal-ref><doi>10.1109/ALIFE.2013.6602430</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed computation in artificial life and complex systems is often
described in terms of component operations on information: information storage,
transfer and modification. Information modification remains poorly described
however, with the popularly-understood examples of glider and particle
collisions in cellular automata being only quantitatively identified to date
using a heuristic (separable information) rather than a proper
information-theoretic measure. We outline how a recently-introduced axiomatic
framework for measuring information redundancy and synergy, called partial
information decomposition, can be applied to a perspective of distributed
computation in order to quantify component operations on information. Using
this framework, we propose a new measure of information modification that
captures the intuitive understanding of information modification events as
those involving interactions between two or more information sources. We also
consider how the local dynamics of information modification in space and time
could be measured, and suggest a new axiom that redundancy measures would need
to meet in order to make such local measurements. Finally, we evaluate the
potential for existing redundancy measures to meet this localizability axiom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3445</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3445</id><created>2013-03-14</created><authors><author><keyname>Gouicem</keyname><forenames>Mourad</forenames></author></authors><title>New modular multiplication and division algorithms based on continued
  fraction expansion</title><categories>cs.DS cs.SC</categories><acm-class>G.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we apply results on number systems based on continued fraction
expansions to modular arithmetic. We provide two new algorithms in order to
compute modular multiplication and modular division. The presented algorithms
are based on the Euclidean algorithm and are of quadratic complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3469</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3469</id><created>2013-03-14</created><authors><author><keyname>Bashir</keyname><forenames>Hassan A.</forenames></author><author><keyname>Neville</keyname><forenames>Richard S.</forenames></author></authors><title>Hybrid Evolutionary Computation for Continuous Optimization</title><categories>cs.NE</categories><comments>Companion Publications for this Technical Memorandum, available at
  IEEE Xplore, are: [1] H. A. Bashir and R. S. Neville, &quot;Convergence
  measurement in evolutionary computation using Price's theorem,&quot; IEEE (CEC),
  2012. [2] H. A. Bashir and R. S. Neville, &quot;A hybrid evolutionary computation
  algorithm for global optimization,&quot; IEEE (CEC), 2012</comments><report-no>Technical Memorandum 2011-v.01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid optimization algorithms have gained popularity as it has become
apparent there cannot be a universal optimization strategy which is globally
more beneficial than any other. Despite their popularity, hybridization
frameworks require more detailed categorization regarding: the nature of the
problem domain, the constituent algorithms, the coupling schema and the
intended area of application. This report proposes a hybrid algorithm for
solving small to large-scale continuous global optimization problems. It
comprises evolutionary computation (EC) algorithms and a sequential quadratic
programming (SQP) algorithm; combined in a collaborative portfolio. The SQP is
a gradient based local search method. To optimize the individual contributions
of the EC and SQP algorithms for the overall success of the proposed hybrid
system, improvements were made in key features of these algorithms. The report
proposes enhancements in: i) the evolutionary algorithm, ii) a new convergence
detection mechanism was proposed; and iii) in the methods for evaluating the
search directions and step sizes for the SQP local search algorithm. The
proposed hybrid design aim was to ensure that the two algorithms complement
each other by exploring and exploiting the problem search space. Preliminary
results justify that an adept hybridization of evolutionary algorithms with a
suitable local search method, could yield a robust and efficient means of
solving wide range of global optimization problems. Finally, a discussion of
the outcomes of the initial investigation and a review of the associated
challenges and inherent limitations of the proposed method is presented to
complete the investigation. The report highlights extensive research,
particularly, some potential case studies and application areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3475</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3475</id><created>2013-03-14</created><updated>2013-04-18</updated><authors><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author><author><keyname>Karpuk</keyname><forenames>David</forenames></author></authors><title>Nonasymptotic Probability Bounds for Fading Channels Exploiting Dedekind
  Zeta Functions</title><categories>cs.IT math.IT math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, new probability bounds are derived for algebraic lattice
codes. This is done by using the Dedekind zeta functions of the algebraic
number fields involved in the lattice constructions. In particular, it is shown
how to upper bound the error performance of a finite constellation on a
Rayleigh fading channel and the probability of an eavesdropper's correct
decision in a wiretap channel. As a byproduct, an estimate of the number of
elements with a certain algebraic norm within a finite hyper-cube is derived.
While this type of estimates have been, to some extent, considered in algebraic
number theory before, they are now brought into novel practice in the context
of fading channel communications. Hence, the interest here is in
small-dimensional lattices and finite constellations rather than in the
asymptotic behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3485</identifier>
 <datestamp>2013-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3485</id><created>2013-03-14</created><authors><author><keyname>Kulkarni</keyname><forenames>Ajay</forenames></author><author><keyname>Kulkarni</keyname><forenames>Saurabh</forenames></author><author><keyname>Haridas</keyname><forenames>Ketki</forenames></author><author><keyname>More</keyname><forenames>Aniket</forenames></author></authors><title>Proposed Video Encryption Algorithm v/s Other Existing Algorithms: A
  Comparative Study</title><categories>cs.CR cs.MM</categories><comments>Published with International Journal of Computer Applications (IJCA)
  5 pages 4 images. arXiv admin note: text overlap with arXiv:1104.0800 by
  other authors without attribution</comments><journal-ref>International Journal of Computer Applications 65(1):1-5, March
  2013</journal-ref><doi>10.5120/10885-5777</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Securing multimedia data has become of utmost importance especially in the
applications related to military purposes. With the rise in development in
computer and internet technology, multimedia data has become the most
convenient method for military training. An innovative encryption algorithm for
videos compressed using H.264 was proposed to safely exchange highly
confidential videos. To maintain a balance between security and computational
time, the proposed algorithm shuffles the video frames along with the audio,
and then AES is used to selectively encrypt the sensitive video codewords.
Using this approach unauthorized viewing of the video file can be prevented and
hence this algorithm provides a high level of security. A comparative study of
the proposed algorithm with other existing algorithms has been put forward in
this paper to prove the effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3489</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3489</id><created>2013-03-14</created><updated>2013-05-19</updated><authors><author><keyname>Alsharoa</keyname><forenames>Ahmad</forenames></author><author><keyname>Bader</keyname><forenames>Faouzi</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Relay Selection and Resource Allocation for Two Way DF-AF Cognitive
  Radio Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, the problem of optimal resource power allocation and relay
selection for two way relaying cognitive radio networks using half duplex
Decode and Forward (DF) and Amplify and Forward (AF) systems are investigated.
The primary and secondary networks are assumed to access the spectrum at the
same time, so that the interference introduced to the primary network caused by
the secondary network should be below a certain interference threshold. In
addition, a selection strategy between the AF and DF schemes is applied
depending on the achieved secondary sum rate without affecting the quality of
service of the primary network. A suboptimal approach based on a genetic
algorithm is also presented to solve our problem. Selected simulation results
show that the proposed suboptimal algorithm offers a performance close to the
performance of the optimal solution with a considerable complexity saving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3502</identifier>
 <datestamp>2013-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3502</id><created>2013-03-14</created><updated>2013-09-10</updated><authors><author><keyname>Cardillo</keyname><forenames>Alessio</forenames></author><author><keyname>Reyes-Su&#xe1;rez</keyname><forenames>Catalina</forenames></author><author><keyname>Naranjo</keyname><forenames>Fernando</forenames></author><author><keyname>G&#xf3;mez-Garde&#xf1;es</keyname><forenames>Jes&#xfa;s</forenames></author></authors><title>The Evolutionary Vaccination Dilemma in Complex Networks</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>7 pages, 5 figures. Manuscript final version</comments><journal-ref>Phys. Rev. E 88, 032803 (2013)</journal-ref><doi>10.1103/PhysRevE.88.032803</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we analyze the evolution of voluntary vaccination in networked
populations by entangling the spreading dynamics of an influenza-like disease
with an evolutionary framework taking place at the end of each influenza season
so that individuals take or not the vaccine upon their previous experience. Our
framework thus put in competition two well-known dynamical properties of
scale-free networks: the fast propagation of diseases and the promotion of
cooperative behaviors. Our results show that when vaccine is perfect scale-free
networks enhance the vaccination behavior with respect to random graphs with
homogeneous connectivity patterns. However, when imperfection appears we find a
cross-over effect so that the number of infected (vaccinated) individuals
increases (decreases) with respect to homogeneous networks, thus showing up the
competition between the aforementioned properties of scale-free graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3517</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3517</id><created>2013-03-13</created><authors><author><keyname>Rosen</keyname><forenames>Joshua</forenames></author><author><keyname>Polyzotis</keyname><forenames>Neoklis</forenames></author><author><keyname>Borkar</keyname><forenames>Vinayak</forenames></author><author><keyname>Bu</keyname><forenames>Yingyi</forenames></author><author><keyname>Carey</keyname><forenames>Michael J.</forenames></author><author><keyname>Weimer</keyname><forenames>Markus</forenames></author><author><keyname>Condie</keyname><forenames>Tyson</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Raghu</forenames></author></authors><title>Iterative MapReduce for Large Scale Machine Learning</title><categories>cs.DC cs.DB cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large datasets (&quot;Big Data&quot;) are becoming ubiquitous because the potential
value in deriving insights from data, across a wide range of business and
scientific applications, is increasingly recognized. In particular, machine
learning - one of the foundational disciplines for data analysis, summarization
and inference - on Big Data has become routine at most organizations that
operate large clouds, usually based on systems such as Hadoop that support the
MapReduce programming paradigm. It is now widely recognized that while
MapReduce is highly scalable, it suffers from a critical weakness for machine
learning: it does not support iteration. Consequently, one has to program
around this limitation, leading to fragile, inefficient code. Further, reliance
on the programmer is inherently flawed in a multi-tenanted cloud environment,
since the programmer does not have visibility into the state of the system when
his or her program executes. Prior work has sought to address this problem by
either developing specialized systems aimed at stylized applications, or by
augmenting MapReduce with ad hoc support for saving state across iterations
(driven by an external loop). In this paper, we advocate support for looping as
a first-class construct, and propose an extension of the MapReduce programming
paradigm called {\em Iterative MapReduce}. We then develop an optimizer for a
class of Iterative MapReduce programs that cover most machine learning
techniques, provide theoretical justifications for the key optimization steps,
and empirically demonstrate that system-optimized programs for significant
machine learning tasks are competitive with state-of-the-art specialized
solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3525</identifier>
 <datestamp>2013-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3525</id><created>2013-03-14</created><updated>2013-07-03</updated><authors><author><keyname>Van Vaerenbergh</keyname><forenames>Steven</forenames></author><author><keyname>Via</keyname><forenames>Javier</forenames></author><author><keyname>Santamaria</keyname><forenames>Ignacio</forenames></author></authors><title>Blind Identification of SIMO Wiener Systems based on Kernel Canonical
  Correlation Analysis</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Trans. Signal Process., 61 (2013), 2219-2230</journal-ref><doi>10.1109/TSP.2013.2248004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of blind identification and equalization of
single-input multiple-output (SIMO) nonlinear channels. Specifically, the
nonlinear model consists of multiple single-channel Wiener systems that are
excited by a common input signal. The proposed approach is based on a
well-known blind identification technique for linear SIMO systems. By
transforming the output signals into a reproducing kernel Hilbert space (RKHS),
a linear identification problem is obtained, which we propose to solve through
an iterative procedure that alternates between canonical correlation analysis
(CCA) to estimate the linear parts, and kernel canonical correlation (KCCA) to
estimate the memoryless nonlinearities. The proposed algorithm is able to
operate on systems with as few as two output channels, on relatively small data
sets and on colored signals. Simulations are included to demonstrate the
effectiveness of the proposed technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3527</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3527</id><created>2013-03-12</created><authors><author><keyname>Rout</keyname><forenames>Ranjeet Kumar</forenames></author><author><keyname>Choudhury</keyname><forenames>Pabitra Pal</forenames></author><author><keyname>Sahoo</keyname><forenames>Sudhakar</forenames></author></authors><title>Classification of Boolean Functions where Affine Functions are Uniformly
  Distributed</title><categories>cs.LO cs.DM</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classification of Non-linear Boolean functions is a long-standing problem in
the area of theoretical computer science. In this paper, effort has been made
to achieve a systematic classification of all n-variable Boolean functions,
where only one affine Boolean function belongs to each class. Two different
methods are proposed to achieve this classification. The first method is a
recursive procedure that uses the Cartesian product of sets starting from the
set of 1-variable Boolean function and in the second method classification is
achieved through a set of invariant bit positions with respect to an affine
function belonging to that class. The invariant bit positions also provide
information concerning the size and symmetry properties of the
classes/sub-classes, such that the members of classes/sub-classes satisfy
certain similar properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3533</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3533</id><created>2013-03-14</created><authors><author><keyname>Svore&#x148;ov&#xe1;</keyname><forenames>M&#xe1;ria</forenames></author><author><keyname>&#x10c;ern&#xe1;</keyname><forenames>Ivana</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>Optimal Receding Horizon Control for Finite Deterministic Systems with
  Temporal Logic Constraints</title><categories>cs.RO</categories><comments>Technical report accompanying the ACC 2013 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a provably correct optimal control strategy for a
finite deterministic transition system. By assuming that penalties with known
probabilities of occurrence and dynamics can be sensed locally at the states of
the system, we derive a receding horizon strategy that minimizes the expected
average cumulative penalty incurred between two consecutive satisfactions of a
desired property. At the same time, we guarantee the satisfaction of
correctness specifications expressed as Linear Temporal Logic formulas. We
illustrate the approach with a persistent surveillance robotics application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3547</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3547</id><created>2013-03-14</created><updated>2013-03-28</updated><authors><author><keyname>Bagheri</keyname><forenames>Saeed</forenames></author><author><keyname>Scaglione</keyname><forenames>Anna</forenames></author></authors><title>Spatial-Spectral Sensing using the Shrink &amp; Match Algorithm in
  Asynchronous MIMO OFDM Signals</title><categories>math.OC cs.IT math.IT</categories><comments>Submitted to Globecom 2013; 11 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectrum sensing (SS) in cognitive radio (CR) systems is of paramount
importance to approach the capacity limits for the Secondary Users (SU), while
ensuring the undisturbed transmission of Primary Users (PU). In this paper, we
formulate a cognitive radio (CR)systems spectrum sensing (SS) problem in which
Secondary Users (SU), with multiple receive antennae, sense a channel shared
among multiple asynchronous Primary Users (PU) transmitting Multiple Input
Multiple Output (MIMO) Orthogonal Frequency Division Multiplexing (OFDM)
signals. The method we propose to estimate the opportunities available to the
SUs combines advances in array processing and compressed channel sensing, and
leverages on both the so called &quot;shrinkage method&quot; as well as on an
over-complete basis expansion of the PUs interference covariance matrix to
detect the occupied and idle angles of arrivals and subcarriers. The covariance
&quot;shrinkage&quot; step and the sparse modeling step that follows, allow to resolve
ambiguities that arise when the observations are scarce, reducing the sensing
cost for the SU, thereby increasing its spectrum exploitation capabilities
compared to competing sensing methods. Simulations corroborate that exploiting
the sparse representation of the covariance matrix in CR sensing resolves the
spatial and frequency spectrum of the sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3557</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3557</id><created>2013-03-14</created><authors><author><keyname>Saeedi</keyname><forenames>Mehdi</forenames></author><author><keyname>Pedram</keyname><forenames>Massoud</forenames></author></authors><title>Linear-Depth Quantum Circuits for n-qubit Toffoli gates with no Ancilla</title><categories>quant-ph cs.ET</categories><comments>5 pages, 7 figures</comments><doi>10.1103/PhysRevA.87.062318</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design a circuit structure with linear depth to implement an $n$-qubit
Toffoli gate. The proposed construction uses a quadratic-size circuit consists
of elementary 2-qubit controlled-rotation gates around the x axis and uses no
ancilla qubit. Circuit depth remains linear in quantum technologies with
finite-distance interactions between qubits. The suggested construction is
related to the long-standing construction by Barenco et al. (Phys. Rev. A, 52:
3457-3467, 1995, arXiv:quant-ph/9503016), which uses a quadratic-size,
quadratic-depth quantum circuit for an $n$-qubit Toffoli gate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3564</identifier>
 <datestamp>2013-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3564</id><created>2013-03-14</created><authors><author><keyname>Fata</keyname><forenames>Elaheh</forenames></author><author><keyname>Smith</keyname><forenames>Stephen L.</forenames></author><author><keyname>Sundaram</keyname><forenames>Shreyas</forenames></author></authors><title>Distributed Dominating Sets on Grids</title><categories>cs.DS</categories><comments>10 pages, 9 figures, accepted in ACC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a distributed algorithm for finding near optimal
dominating sets on grids. The basis for this algorithm is an existing
centralized algorithm that constructs dominating sets on grids. The size of the
dominating set provided by this centralized algorithm is upper-bounded by
$\lceil\frac{(m+2)(n+2)}{5}\rceil$ for $m\times n$ grids and its difference
from the optimal domination number of the grid is upper-bounded by five. Both
the centralized and distributed algorithms are generalized for the $k$-distance
dominating set problem, where all grid vertices are within distance $k$ of the
vertices in the dominating set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3578</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3578</id><created>2013-03-13</created><authors><author><keyname>Jiang</keyname><forenames>Kun</forenames></author><author><keyname>Zhou</keyname><forenames>Xionghui</forenames></author><author><keyname>Li</keyname><forenames>Min</forenames></author><author><keyname>Niu</keyname><forenames>Qiang</forenames></author></authors><title>A 3D Curve Offset Approach for Ruled Surface Generation in Engineering
  Design</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ruled surface is widely used in engineering design such as parting surface
design of injection mold and checking surface design of checking fixture, which
are usually generated by offsetting 3D curves. However, in 3D curve offset,
there often exist break,interaction and overlapping problems which can't be
solved by current CAD software automatically. This paper is targeted at
developing a 3D curve offsetting algorithm for ruled surface generation, and
three key technologies are introduced in details: An improved curve division
method is proposed to reduce the offset accuracy error resulted from different
offset distances and curvatures; An offsetting curve overlapping detection and
elimination method is proposed; And then, a curve transition method is
presented to improve curve offsetting quality for the break and
intersection/overlapping regions, where a new algorithm for generating positive
weights spherical rational quartic Bezier curve is proposed to bridge the
breaks of offset curves to create a smooth ruled surface. Finally, two
practical design cases, parting surface and checking surface generation, show
that the proposed approach can enhance the efficiency and quality for ruled
surface generation in engineering design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3592</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3592</id><created>2013-03-14</created><authors><author><keyname>Makatchev</keyname><forenames>Maxim</forenames></author><author><keyname>Simmons</keyname><forenames>Reid</forenames></author><author><keyname>Sakr</keyname><forenames>Majd</forenames></author><author><keyname>Ziadee</keyname><forenames>Micheline</forenames></author></authors><title>Expressing Ethnicity through Behaviors of a Robot Character</title><categories>cs.CL cs.CY cs.RO</categories><comments>10 pages, 4 figures</comments><journal-ref>Proceedings of the 8th ACM/IEEE international conference on
  Human-robot interaction (HRI), Tokyo, Japan, 2013, pages 357-364</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Achieving homophily, or association based on similarity, between a human user
and a robot holds a promise of improved perception and task performance.
However, no previous studies that address homophily via ethnic similarity with
robots exist. In this paper, we discuss the difficulties of evoking ethnic cues
in a robot, as opposed to a virtual agent, and an approach to overcome those
difficulties based on using ethnically salient behaviors. We outline our
methodology for selecting and evaluating such behaviors, and culminate with a
study that evaluates our hypotheses of the possibility of ethnic attribution of
a robot character through verbal and nonverbal behaviors and of achieving the
homophily effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3605</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3605</id><created>2013-03-14</created><authors><author><keyname>Ajay</keyname><forenames>Adheen</forenames></author><author><keyname>Venkataraman</keyname><forenames>D.</forenames></author></authors><title>A survey on sensing methods and feature extraction algorithms for SLAM
  problem</title><categories>cs.RO cs.CV cs.LG</categories><comments>5 pages, 1 figure,2 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper is a survey work for a bigger project for designing a Visual SLAM
robot to generate 3D dense map of an unknown unstructured environment. A lot of
factors have to be considered while designing a SLAM robot. Sensing method of
the SLAM robot should be determined by considering the kind of environment to
be modeled. Similarly the type of environment determines the suitable feature
extraction method. This paper goes through the sensing methods used in some
recently published papers. The main objective of this survey is to conduct a
comparative study among the current sensing methods and feature extraction
algorithms and to extract out the best for our work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3614</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3614</id><created>2013-03-14</created><authors><author><keyname>Ahn</keyname><forenames>Tae-Hyuk</forenames></author><author><keyname>Sandu</keyname><forenames>Adrian</forenames></author><author><keyname>Han</keyname><forenames>Xiaoying</forenames></author></authors><title>Implicit Simulation Methods for Stochastic Chemical Kinetics</title><categories>cs.CE cs.NA math.NA</categories><comments>Prepare submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In biochemical systems some of the chemical species are present with only
small numbers of molecules. In this situation discrete and stochastic
simulation approaches are more relevant than continuous and deterministic ones.
The fundamental Gillespie's stochastic simulation algorithm (SSA) accounts for
every reaction event, which occurs with a probability determined by the
configuration of the system. This approach requires a considerable
computational effort for models with many reaction channels and chemical
species. In order to improve efficiency, tau-leaping methods represent multiple
firings of each reaction during a simulation step by Poisson random variables.
For stiff systems the mean of this variable is treated implicitly in order to
ensure numerical stability.
  This paper develops fully implicit tau-leaping-like algorithms that treat
implicitly both the mean and the variance of the Poisson variables. The
construction is based on adapting weakly convergent discretizations of
stochastic differential equations to stochastic chemical kinetic systems.
Theoretical analyses of accuracy and stability of the new methods are performed
on a standard test problem. Numerical results demonstrate the performance of
the proposed tau-leaping methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3624</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3624</id><created>2013-03-14</created><authors><author><keyname>Xu</keyname><forenames>Weiqiang</forenames></author><author><keyname>Shi</keyname><forenames>Qingjiang</forenames></author><author><keyname>Wei</keyname><forenames>Xiaoyun</forenames></author><author><keyname>Wang</keyname><forenames>Yaming</forenames></author></authors><title>Distributed Optimal Rate-Reliability-Lifetime Tradeoff in Wireless
  Sensor Networks</title><categories>cs.NI</categories><comments>27 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The transmission rate, delivery reliability and network lifetime are three
fundamental but conflicting design objectives in energy-constrained wireless
sensor networks. In this paper, we address the optimal
rate-reliability-lifetime tradeoff with link capacity constraint, reliability
constraint and energy constraint. By introducing the weight parameters, we
combine the objectives at rate, reliability, and lifetime into a single
objective to characterize the tradeoff among them. However, the optimization
formulation of the rate-reliability-reliability tradeoff is neither separable
nor convex. Through a series of transformations, a separable and convex problem
is derived, and an efficient distributed Subgradient Dual Decomposition
algorithm (SDD) is proposed. Numerical examples confirm its convergence. Also,
numerical examples investigate the impact of weight parameters on the rate
utility, reliability utility and network lifetime, which provide a guidance to
properly set the value of weight parameters for a desired performance of WSNs
according to the realistic application's requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3625</identifier>
 <datestamp>2014-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3625</id><created>2013-03-14</created><updated>2014-01-17</updated><authors><author><keyname>Teslyk</keyname><forenames>Maksym</forenames></author><author><keyname>Teslyk</keyname><forenames>Olena</forenames></author></authors><title>Quantum Logic Dequantization: Information Loss</title><categories>quant-ph cs.IT math.IT</categories><comments>14 pages</comments><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An attempt to ascertain reasons of the quantum logic efficiency was made. To
do this, we reduce the complete set of elementary quantum logic operations to
the classical one via taking the semi-classical limit (dequantization). We
estimate the amount of information loss for any element of the set and obtain
general expression for the loss estimation of any quantum algorithm under
dequantization. We demonstrate the technique on quantum discrete fast Fourier
transform and Grover search algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3626</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3626</id><created>2013-03-14</created><authors><author><keyname>Shafiei</keyname><forenames>Niloufar</forenames></author></authors><title>Non-blocking Patricia Tries with Replace Operations</title><categories>cs.DC</categories><comments>To appear in the 33rd IEEE International Conference on Distributed
  Computing Systems (ICDCS 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a non-blocking Patricia trie implementation for an
asynchronous shared-memory system using Compare&amp;Swap. The trie implements a
linearizable set and supports three update operations: insert adds an element,
delete removes an element and replace replaces one element by another. The
replace operation is interesting because it changes two different locations of
tree atomically. If all update operations modify different parts of the trie,
they run completely concurrently. The implementation also supports a wait-free
find operation, which only reads shared memory and never changes the data
structure. Empirically, we compare our algorithms to some existing set
implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3632</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3632</id><created>2013-03-14</created><authors><author><keyname>Rizvandi</keyname><forenames>Nikzad Babaii</forenames></author><author><keyname>Taheri</keyname><forenames>Javid</forenames></author><author><keyname>Moraveji</keyname><forenames>Reza</forenames></author><author><keyname>Zomaya</keyname><forenames>Albert Y.</forenames></author></authors><title>Statistical Regression to Predict Total Cumulative CPU Usage of
  MapReduce Jobs</title><categories>cs.DC cs.LG cs.PF</categories><comments>16 pages- previously published as &quot;On Modelling and Prediction of
  Total CPU Usage for Applications in MapReduce Enviornments&quot; in IEEE 12th
  International Conference on Algorithms and Architectures for Parallel
  Processing (ICA3PP-12), Fukuoka, Japan, 4-7 September, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, businesses have started using MapReduce as a popular computation
framework for processing large amount of data, such as spam detection, and
different data mining tasks, in both public and private clouds. Two of the
challenging questions in such environments are (1) choosing suitable values for
MapReduce configuration parameters e.g., number of mappers, number of reducers,
and DFS block size, and (2) predicting the amount of resources that a user
should lease from the service provider. Currently, the tasks of both choosing
configuration parameters and estimating required resources are solely the users
responsibilities. In this paper, we present an approach to provision the total
CPU usage in clock cycles of jobs in MapReduce environment. For a MapReduce
job, a profile of total CPU usage in clock cycles is built from the job past
executions with different values of two configuration parameters e.g., number
of mappers, and number of reducers. Then, a polynomial regression is used to
model the relation between these configuration parameters and total CPU usage
in clock cycles of the job. We also briefly study the influence of input data
scaling on measured total CPU usage in clock cycles. This derived model along
with the scaling result can then be used to provision the total CPU usage in
clock cycles of the same jobs with different input data size. We validate the
accuracy of our models using three realistic applications (WordCount, Exim
MainLog parsing, and TeraSort). Results show that the predicted total CPU usage
in clock cycles of generated resource provisioning options are less than 8% of
the measured total CPU usage in clock cycles in our 20-node virtual Hadoop
cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3636</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3636</id><created>2013-03-14</created><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Low-Complexity Adaptive Set-Membership Reduced-rank LCMV Beamforming</title><categories>cs.IT math.IT</categories><comments>2 figures, 5 pages</comments><journal-ref>ISWCS 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new adaptive algorithm for the implementation of the
linearly constrained minimum variance (LCMV) beamformer. The proposed algorithm
utilizes the set-membership filtering (SMF) framework and the reduced-rank
joint iterative optimization (JIO) scheme. We develop a stochastic gradient
(SG) based algorithm for the beamformer design. An effective time-varying bound
is employed in the proposed method to adjust the step sizes, avoid the
misadjustment and the risk of overbounding or underbounding. Simulations are
performed to show the improved performance of the proposed algorithm in
comparison with existing full-rank and reduced-rank methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3638</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3638</id><created>2013-03-14</created><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Adaptive Low-rank Constrained Constant Modulus Beamforming Algorithms
  using Joint Iterative Optimization of Parameters</title><categories>cs.IT math.IT</categories><comments>4 figures, 4 pages. arXiv admin note: substantial text overlap with
  arXiv:1303.1571</comments><journal-ref>SSP 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a robust reduced-rank scheme for adaptive beamforming
based on joint iterative optimization (JIO) of adaptive filters. The scheme
provides an efficient way to deal with filters with large number of elements.
It consists of a bank of full-rank adaptive filters that forms a transformation
matrix and an adaptive reduced-rank filter that operates at the output of the
bank of filters. The transformation matrix projects the received vector onto a
low-dimension vector, which is processed by the reduced-rank filter to estimate
the desired signal. The expressions of the transformation matrix and the
reduced-rank weight vector are derived according to the constrained constant
modulus (CCM) criterion. Two novel low-complexity adaptive algorithms are
devised for the implementation of the proposed scheme with respect to different
constrained conditions. Simulations are performed to show superior performance
of the proposed algorithms in comparison with the existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3643</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3643</id><created>2013-03-14</created><authors><author><keyname>Zarrad</keyname><forenames>Anis</forenames></author></authors><title>A Survey of Routing Techniques for Mobile Collaborative Virtual
  Environment Applications</title><categories>cs.NI</categories><journal-ref>International Journal of Computer Science Issues, Vol. 9, Issue 6,
  No 2, 2012, pp. 379-388</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the rapidly increasing usage of personal mobile devices and the need
of executing CVE applications in environments that have no previous network
infrastructure, Mobile Collaborative Virtual Environments (MCVEs) systems will
become ubiquitous in the future. In such systems, we enable users to act and
interact in three dimensional world shared through their mobile devices in an
ad-hoc network (MANET). A handful of interesting MCVE applications have been
developed in a variety of domains, ranging from multiplayer games to virtual
cities, virtual shopping malls, and various training simulations (i.e.
military, emergency preparedness, Education, Medicine, etc.). The designers of
such applications rely solely on network overhead, latency, limited bandwidth,
and mobile device limitation. In this survey we present a number of ways to
classify and analysis routing techniques that have been applied in Mobile
Collaborative Environment Applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3644</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3644</id><created>2013-03-14</created><updated>2015-07-08</updated><authors><author><keyname>Lessard</keyname><forenames>Laurent</forenames></author><author><keyname>Lall</keyname><forenames>Sanjay</forenames></author></authors><title>Optimal Control of Two-Player Systems with Output Feedback</title><categories>cs.SY math.OC</categories><doi>10.1109/TAC.2015.2400658</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we consider a fundamental decentralized optimal control
problem, which we call the two-player problem. Two subsystems are
interconnected in a nested information pattern, and output feedback controllers
must be designed for each subsystem. Several special cases of this architecture
have previously been solved, such as the state-feedback case or the case where
the dynamics of both systems are decoupled. In this paper, we present a
detailed solution to the general case. The structure of the optimal
decentralized controller is reminiscent of that of the optimal centralized
controller; each player must estimate the state of the system given their
available information and apply static control policies to these estimates to
compute the optimal controller. The previously solved cases benefit from a
separation between estimation and control which allows one to compute the
control and estimation gains separately. This feature is not present in
general, and some of the gains must be solved for simultaneously. We show that
computing the required coupled estimation and control gains amounts to solving
a small system of linear equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3645</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3645</id><created>2013-03-14</created><authors><author><keyname>Bauer</keyname><forenames>Andreas</forenames></author><author><keyname>K&#xfc;ster</keyname><forenames>Jan-Christoph</forenames></author><author><keyname>Vegliach</keyname><forenames>Gil</forenames></author></authors><title>From propositional to first-order monitoring</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main purpose of this paper is to introduce a first-order temporal logic,
LTLFO, and a corresponding monitor construction based on a new type of
automaton, called spawning automaton.
  Specifically, we show that monitoring a specification in LTLFO boils down to
an undecidable decision problem. The proof of this result revolves around
specific ideas on what we consider a &quot;proper&quot; monitor. As these ideas are
general, we outline them first in the setting of standard LTL, before lifting
them to the setting of first-order logic and LTLFO. Although due to the above
result one cannot hope to obtain a complete monitor for LTLFO, we prove the
soundness of our automata-based construction and give experimental results from
an implementation. These seem to substantiate our hypothesis that the
automata-based construction leads to efficient runtime monitors whose size does
not grow with increasing trace lengths (as is often observed in similar
approaches). However, we also discuss formulae for which growth is unavoidable,
irrespective of the chosen monitoring approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3651</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3651</id><created>2013-03-14</created><authors><author><keyname>Gong</keyname><forenames>Jie</forenames></author><author><keyname>Zhou</keyname><forenames>Sheng</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author></authors><title>Optimal Power Allocation for Energy Harvesting and Power Grid Coexisting
  Wireless Communication Systems</title><categories>cs.IT math.IT</categories><comments>29 pages, 11 figures, submitted to IEEE Transactions on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the power allocation of a single-link wireless
communication with joint energy harvesting and grid power supply. We formulate
the problem as minimizing the grid power consumption with random energy and
data arrival, and analyze the structure of the optimal power allocation policy
in some special cases. For the case that all the packets are arrived before
transmission, it is a dual problem of throughput maximization, and the optimal
solution is found by the two-stage water filling (WF) policy, which allocates
the harvested energy in the first stage, and then allocates the power grid
energy in the second stage. For the random data arrival case, we first assume
grid energy or harvested energy supply only, and then combine the results to
obtain the optimal structure of the coexisting system. Specifically, the
reverse multi-stage WF policy is proposed to achieve the optimal power
allocation when the battery capacity is infinite. Finally, some heuristic
online schemes are proposed, of which the performance is evaluated by numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3656</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3656</id><created>2013-03-14</created><authors><author><keyname>Han</keyname><forenames>Guangyue</forenames></author></authors><title>A Randomized Approach to the Capacity of Finite-State Channels</title><categories>cs.IT math.IT</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the ideas from the field of stochastic approximation, we propose
a randomized algorithm to compute the capacity of a finite-state channel with a
Markovian input. When the mutual information rate of the channel is concave
with respect to the chosen parameterization, we show that the proposed
algorithm will almost surely converge to the capacity of the channel and derive
the rate of convergence. We also discuss the convergence behavior of the
algorithm without the concavity assumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3660</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3660</id><created>2013-03-14</created><authors><author><keyname>Nain</keyname><forenames>Philippe</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author><author><keyname>Johnson</keyname><forenames>Matthew P.</forenames></author><author><keyname>Basu</keyname><forenames>Prithwish</forenames></author><author><keyname>Bar-Noy</keyname><forenames>Amotz</forenames></author><author><keyname>Yu</keyname><forenames>Feng</forenames></author></authors><title>Computing Traversal Times on Dynamic Markovian Paths</title><categories>cs.NI cs.DS</categories><comments>11 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In source routing, a complete path is chosen for a packet to travel from
source to destination. While computing the time to traverse such a path may be
straightforward in a fixed, static graph, doing so becomes much more
challenging in dynamic graphs, in which the state of an edge in one time slot
(i.e., its presence or absence) is random, and may depend on its state in the
previous time step. The traversal time is due to both time spent waiting for
edges to appear and time spent crossing them once they become available. We
compute the expected traversal time (ETT) for a dynamic path in a number of
special cases of stochastic edge dynamics models, and for three edge failure
models, culminating in a surprisingly challenging yet realistic setting in
which the initial configuration of edge states for the entire path is known. We
show that the ETT for this &quot;initial configuration&quot; setting can be computed in
quadratic time, by an algorithm based on probability generating functions. We
also give several linear-time upper and lower bounds on the ETT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3661</identifier>
 <datestamp>2013-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3661</id><created>2013-03-14</created><updated>2013-08-21</updated><authors><author><keyname>Hu</keyname><forenames>Collin Feng</forenames></author></authors><title>A biologically-motivated system is poised at a critical state</title><categories>physics.soc-ph cs.SI q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the critical behaviors in the dynamics of information transfer of
a biologically-inspired system by an individual-based model. &quot;Quorum response&quot;,
a type of social interaction which has been recognized taxonomically in animal
groups, is applied as the sole interaction rule among particles. We assume a
truncated Gaussian distribution to quantitatively depict the distribution of
the particles' vigilance level and find that by fine-tuning the parameters of
the mean and the standard deviation of the Gaussian distribution, the system is
poised at a critical state in the dynamics of information transfer. We present
the phase diagrams to exhibit that the phase line divides the parameter space
into a super-critical and a sub-critical zone, in which the dynamics of
information transfer varies largely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3664</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3664</id><created>2013-03-14</created><updated>2013-03-18</updated><authors><author><keyname>Ding</keyname><forenames>Weicong</forenames></author><author><keyname>Rohban</keyname><forenames>Mohammad H.</forenames></author><author><keyname>Ishwar</keyname><forenames>Prakash</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Topic Discovery through Data Dependent and Random Projections</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present algorithms for topic modeling based on the geometry of
cross-document word-frequency patterns. This perspective gains significance
under the so called separability condition. This is a condition on existence of
novel-words that are unique to each topic. We present a suite of highly
efficient algorithms based on data-dependent and random projections of
word-frequency patterns to identify novel words and associated topics. We will
also discuss the statistical guarantees of the data-dependent projections
method based on two mild assumptions on the prior density of topic document
matrix. Our key insight here is that the maximum and minimum values of
cross-document frequency patterns projected along any direction are associated
with novel words. While our sample complexity bounds for topic recovery are
similar to the state-of-art, the computational complexity of our random
projection scheme scales linearly with the number of documents and the number
of words per document. We present several experiments on synthetic and
real-world datasets to demonstrate qualitative and quantitative merits of our
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3665</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3665</id><created>2013-03-14</created><authors><author><keyname>Harshan</keyname><forenames>J.</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>Integer Space-Time Block Codes for Practical MIMO Systems</title><categories>cs.IT math.IT</categories><comments>10 pages and 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Full-rate space-time block codes (STBCs) achieve high spectral-efficiency by
transmitting linear combinations of information symbols through every transmit
antenna. However, the coefficients used for the linear combinations, if not
chosen carefully, results in ({\em i}) large number of processor bits for the
encoder and ({\em ii}) high peak-to-average power ratio (PAPR) values. In this
work, we propose a new class of full-rate STBCs called Integer STBCs (ICs) for
multiple-input multiple-output (MIMO) fading channels. A unique property of ICs
is the presence of integer coefficients in the code structure which enables
reduced numbers of processor bits for the encoder and lower PAPR values. We
show that the reduction in the number of processor bits is significant for
small MIMO channels, while the reduction in the PAPR is significant for large
MIMO channels. We also highlight the advantages of the proposed codes in
comparison with the well known full-rate algebraic STBCs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3668</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3668</id><created>2013-03-14</created><authors><author><keyname>Tamo</keyname><forenames>Itzhak</forenames></author><author><keyname>Wang</keyname><forenames>Zhiying</forenames></author><author><keyname>Bruck</keyname><forenames>Jehoshua</forenames></author></authors><title>Access vs. Bandwidth in Codes for Storage</title><categories>cs.IT math.IT</categories><comments>This paper was presented in part at the IEEE International Symposium
  on Information Theory (ISIT 2012). submitted to IEEE transactions on
  information theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximum distance separable (MDS) codes are widely used in storage systems to
protect against disk (node) failures. A node is said to have capacity $l$ over
some field $\mathbb{F}$, if it can store that amount of symbols of the field.
An $(n,k,l)$ MDS code uses $n$ nodes of capacity $l$ to store $k$ information
nodes. The MDS property guarantees the resiliency to any $n-k$ node failures.
An \emph{optimal bandwidth} (resp. \emph{optimal access}) MDS code communicates
(resp. accesses) the minimum amount of data during the repair process of a
single failed node. It was shown that this amount equals a fraction of
$1/(n-k)$ of data stored in each node. In previous optimal bandwidth
constructions, $l$ scaled polynomially with $k$ in codes with asymptotic rate
$&lt;1$. Moreover, in constructions with a constant number of parities, i.e. rate
approaches 1, $l$ is scaled exponentially w.r.t. $k$. In this paper, we focus
on the later case of constant number of parities $n-k=r$, and ask the following
question: Given the capacity of a node $l$ what is the largest number of
information disks $k$ in an optimal bandwidth (resp. access) $(k+r,k,l)$ MDS
code. We give an upper bound for the general case, and two tight bounds in the
special cases of two important families of codes. Moreover, the bounds show
that in some cases optimal-bandwidth code has larger $k$ than optimal-access
code, and therefore these two measures are not equivalent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3679</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3679</id><created>2013-03-15</created><authors><author><keyname>Tumova</keyname><forenames>Jana</forenames></author><author><keyname>Castro</keyname><forenames>Luis I. Reyes</forenames></author><author><keyname>Karaman</keyname><forenames>Sertac</forenames></author><author><keyname>Frazzoli</keyname><forenames>Emilio</forenames></author><author><keyname>Rus</keyname><forenames>Daniela</forenames></author></authors><title>Minimum-violation LTL Planning with Conflicting Specifications</title><categories>cs.RO</categories><comments>extended version of the ACC 2013 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of automatic generation of control strategies for
robotic vehicles given a set of high-level mission specifications, such as
&quot;Vehicle x must eventually visit a target region and then return to a base,&quot;
&quot;Regions A and B must be periodically surveyed,&quot; or &quot;None of the vehicles can
enter an unsafe region.&quot; We focus on instances when all of the given
specifications cannot be reached simultaneously due to their incompatibility
and/or environmental constraints. We aim to find the least-violating control
strategy while considering different priorities of satisfying different parts
of the mission. Formally, we consider the missions given in the form of linear
temporal logic formulas, each of which is assigned a reward that is earned when
the formula is satisfied. Leveraging ideas from the automata-based model
checking, we propose an algorithm for finding an optimal control strategy that
maximizes the sum of rewards earned if this control strategy is applied. We
demonstrate the proposed algorithm on an illustrative case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3692</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3692</id><created>2013-03-15</created><updated>2015-05-03</updated><authors><author><keyname>Liao</keyname><forenames>Gang</forenames></author><author><keyname>Sun</keyname><forenames>Qi</forenames></author><author><keyname>Ma</keyname><forenames>Longfei</forenames></author><author><keyname>Ding</keyname><forenames>Sha</forenames></author><author><keyname>Xie</keyname><forenames>Wen</forenames></author></authors><title>Ultra-fast Multiple Genome Sequence Matching Using GPU</title><categories>cs.DS cs.DC</categories><comments>The 2013 International Conference on High Performance Computing &amp;
  Simulation (ACM/IEEE HPCS 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a contrastive evaluation of massively parallel implementations
of suffix tree and suffix array to accelerate genome sequence matching are
proposed based on Intel Core i7 3770K quad-core and NVIDIA GeForce GTX680 GPU.
Besides suffix array only held approximately 20%~30% of the space relative to
suffix tree, the coalesced binary search and tile optimization make suffix
array clearly outperform suffix tree using GPU. Consequently, the experimental
results show that multiple genome sequence matching based on suffix array is
more than 99 times speedup than that of CPU serial implementation. There is no
doubt that massively parallel matching algorithm based on suffix array is an
efficient approach to high-performance bioinformatics applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3708</identifier>
 <datestamp>2015-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3708</id><created>2013-03-15</created><updated>2013-08-29</updated><authors><author><keyname>Perrot</keyname><forenames>K&#xe9;vin</forenames></author><author><keyname>Van Pham</keyname><forenames>Trung</forenames></author></authors><title>Feedback arc set problem and NP-hardness of minimum recurrent
  configuration problem of Chip-firing game on directed graphs</title><categories>cs.CC cs.DM math.CO</categories><comments>18 pages, 6 figures</comments><msc-class>68R05, 68Rxx, 90C27, 05C20</msc-class><journal-ref>Ann. Comb. 19 (2015), no. 2, 373-396</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present further studies of recurrent configurations of
Chip-firing games on Eulerian directed graphs (simple digraphs), a class on the
way from undirected graphs to general directed graphs. A computational problem
that arises naturally from this model is to find the minimum number of chips of
a recurrent configuration, which we call the minimum recurrent configuration
(MINREC) problem. We point out a close relationship between MINREC and the
minimum feedback arc set (MINFAS) problem on Eulerian directed graphs, and
prove that both problems are NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3716</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3716</id><created>2013-03-15</created><authors><author><keyname>Heckel</keyname><forenames>Reinhard</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Subspace Clustering via Thresholding and Spectral Clustering</title><categories>cs.IT cs.LG math.IT math.ST stat.ML stat.TH</categories><comments>ICASSP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of clustering a set of high-dimensional data points
into sets of low-dimensional linear subspaces. The number of subspaces, their
dimensions, and their orientations are unknown. We propose a simple and
low-complexity clustering algorithm based on thresholding the correlations
between the data points followed by spectral clustering. A probabilistic
performance analysis shows that this algorithm succeeds even when the subspaces
intersect, and when the dimensions of the subspaces scale (up to a log-factor)
linearly in the ambient dimension. Moreover, we prove that the algorithm also
succeeds for data points that are subject to erasures with the number of
erasures scaling (up to a log-factor) linearly in the ambient dimension.
Finally, we propose a simple scheme that provably detects outliers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3732</identifier>
 <datestamp>2013-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3732</id><created>2013-03-15</created><updated>2013-06-12</updated><authors><author><keyname>Jamali</keyname><forenames>Vahid</forenames></author><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Ikhlef</keyname><forenames>Aissa</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Adaptive Mode Selection in Bidirectional Buffer-aided Relay Networks
  with Fixed Transmit Powers</title><categories>cs.IT math.IT</categories><comments>Invited paper in 21th European Signal Processing Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a bidirectional network in which two users exchange information
with the help of a buffer-aided relay. In such a network without direct link
between user 1 and user 2, there exist six possible transmission modes, i.e.,
four point-to-point modes (user 1-to-relay, user 2-to-relay, relay-to-user 1,
relay-to-user 2), a multiple access mode (both users to the relay), and a
broadcast mode (the relay to both users). Because of the buffering capability
at the relay, the transmissions in the network are not restricted to adhere to
a predefined schedule, and therefore, all the transmission modes in the
bidirectional relay network can be used adaptively based on the instantaneous
channel state information (CSI) of the involved links. For the considered
network, assuming fixed transmit powers for both the users and the relay, we
derive the optimal transmission mode selection policy which maximizes the sum
rate. The proposed policy selects one out of the six possible transmission
modes in each time slot based on the instantaneous CSI. Simulation results
confirm the effectiveness of the proposed protocol compared to existing
protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3733</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3733</id><created>2013-03-15</created><authors><author><keyname>Cai</keyname><forenames>Y.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Adaptive Reduced-Rank MBER Linear Receive Processing for Large Multiuser
  MIMO Systems</title><categories>cs.IT math.IT</categories><comments>2 figures</comments><journal-ref>ICASSP 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a novel adaptive reduced-rank strategy based on
joint interpolation, decimation and filtering (JIDF) for large multiuser
multiple-input multiple-output (MIMO) systems. In this scheme, a reduced-rank
framework is proposed for linear receive processing and multiuser interference
suppression according to the minimization of the bit error rate (BER) cost
function. We present a structure with multiple processing branches that
performs dimensionality reduction, where each branch contains a group of
jointly optimized interpolation and decimation units, followed by a linear
receive filter. We then develop stochastic gradient (SG) algorithms to compute
the parameters of the interpolation and receive filters along with a
low-complexity decimation technique. Simulation results are presented for
time-varying environments and show that the proposed MBER-JIDF receive
processing strategy and algorithms achieve a superior performance to existing
methods at a reduced complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3734</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3734</id><created>2013-03-15</created><authors><author><keyname>Sanabria-Russo</keyname><forenames>Luis</forenames></author><author><keyname>Faridi</keyname><forenames>Azadeh</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author><author><keyname>Barcelo</keyname><forenames>Jaume</forenames></author><author><keyname>Oliver</keyname><forenames>Miquel</forenames></author></authors><title>Future Evolution of CSMA Protocols for the IEEE 802.11 Standard</title><categories>cs.NI</categories><comments>This paper has been accepted in the Second IEEE ICC Workshop 2013 on
  Telecommunication Standards: From Research to Standards</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a candidate protocol to replace the prevalent CSMA/CA medium
access control in Wireless Local Area Networks is presented. The proposed
protocol can achieve higher throughput than CSMA/CA, while maintaining
fairness, and without additional implementation complexity. Under certain
circumstances, it is able to reach and maintain collision-free operation, even
when the number of contenders is variable and potentially large. It is backward
compatible, allowing for new and legacy stations to coexist without degrading
one another's performance, a property that can make the adoption process by
future versions of the standard smooth and inexpensive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3737</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3737</id><created>2013-03-15</created><authors><author><keyname>Bernal</keyname><forenames>Jos&#xe9; Joaqu&#xed;n</forenames></author><author><keyname>Borges</keyname><forenames>Joaquim</forenames></author><author><keyname>Fern&#xe1;ndez-C&#xf3;rdoba</keyname><forenames>Cristina</forenames></author><author><keyname>Villanueva</keyname><forenames>Merc&#xe8;</forenames></author></authors><title>Permutation decoding of Z2Z4-linear codes</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An alternative permutation decoding method is described which can be used for
any binary systematic encoding scheme, regardless whether the code is linear or
not. Thus, the method can be applied to some important codes such as
Z2Z4-linear codes, which are binary and, in general, nonlinear codes in the
usual sense. For this, it is proved that these codes allow a systematic
encoding scheme. As a particular example, this permutation decoding method is
applied to some Hadamard Z2Z4-linear codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3741</identifier>
 <datestamp>2013-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3741</id><created>2013-03-15</created><updated>2013-09-02</updated><authors><author><keyname>Fire</keyname><forenames>Michael</forenames></author><author><keyname>Puzis</keyname><forenames>Rami</forenames></author><author><keyname>Elovici</keyname><forenames>Yuval</forenames></author></authors><title>Organization Mining Using Online Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Draft Version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mature social networking services are one of the greatest assets of today's
organizations. This valuable asset, however, can also be a threat to an
organization's confidentiality. Members of social networking websites expose
not only their personal information, but also details about the organizations
for which they work. In this paper we analyze several commercial organizations
by mining data which their employees have exposed on Facebook, LinkedIn, and
other publicly available sources. Using a web crawler designed for this
purpose, we extract a network of informal social relationships among employees
of a given target organization. Our results, obtained using centrality analysis
and Machine Learning techniques applied to the structure of the informal
relationships network, show that it is possible to identify leadership roles
within the organization solely by this means. It is also possible to gain
valuable non-trivial insights on an organization's structure by clustering its
social network and gathering publicly available information on the employees
within each cluster. Organizations wanting to conceal their internal structure,
identity of leaders, location and specialization of branches offices, etc.,
must enforce strict policies to control the use of social media by their
employees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3751</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3751</id><created>2013-03-15</created><authors><author><keyname>Fire</keyname><forenames>Michael</forenames></author><author><keyname>Kagan</keyname><forenames>Dima</forenames></author><author><keyname>Elyashar</keyname><forenames>Aviad</forenames></author><author><keyname>Elovici</keyname><forenames>Yuval</forenames></author></authors><title>Friend or Foe? Fake Profile Identification in Online Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Draft Version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The amount of personal information unwillingly exposed by users on online
social networks is staggering, as shown in recent research. Moreover, recent
reports indicate that these networks are infested with tens of millions of fake
users profiles, which may jeopardize the users' security and privacy. To
identify fake users in such networks and to improve users' security and
privacy, we developed the Social Privacy Protector software for Facebook. This
software contains three protection layers, which improve user privacy by
implementing different methods. The software first identifies a user's friends
who might pose a threat and then restricts this &quot;friend's&quot; exposure to the
user's personal information. The second layer is an expansion of Facebook's
basic privacy settings based on different types of social network usage
profiles. The third layer alerts users about the number of installed
applications on their Facebook profile, which have access to their private
information. An initial version of the Social Privacy Protection software
received high media coverage, and more than 3,000 users from more than twenty
countries have installed the software, out of which 527 used the software to
restrict more than nine thousand friends. In addition, we estimate that more
than a hundred users accepted the software's recommendations and removed at
least 1,792 Facebook applications from their profiles. By analyzing the unique
dataset obtained by the software in combination with machine learning
techniques, we developed classifiers, which are able to predict which Facebook
profiles have high probabilities of being fake and therefore, threaten the
user's well-being. Moreover, in this study, we present statistics on users'
privacy settings and statistics of the number of applications installed on
Facebook profiles...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3754</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3754</id><created>2013-03-15</created><authors><author><keyname>Moroshko</keyname><forenames>Edward</forenames></author><author><keyname>Crammer</keyname><forenames>Koby</forenames></author></authors><title>A Last-Step Regression Algorithm for Non-Stationary Online Learning</title><categories>cs.LG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1303.0140</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of a learner in standard online learning is to maintain an average
loss close to the loss of the best-performing single function in some class. In
many real-world problems, such as rating or ranking items, there is no single
best target function during the runtime of the algorithm, instead the best
(local) target function is drifting over time. We develop a novel last-step
minmax optimal algorithm in context of a drift. We analyze the algorithm in the
worst-case regret framework and show that it maintains an average loss close to
that of the best slowly changing sequence of linear functions, as long as the
total of drift is sublinear. In some situations, our bound improves over
existing bounds, and additionally the algorithm suffers logarithmic regret when
there is no drift. We also build on the H_infinity filter and its bound, and
develop and analyze a second algorithm for drifting setting. Synthetic
simulations demonstrate the advantages of our algorithms in a worst-case
constant drift setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3761</identifier>
 <datestamp>2013-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3761</id><created>2013-03-15</created><updated>2013-05-14</updated><authors><author><keyname>Benzm&#xfc;ller</keyname><forenames>Christoph</forenames></author><author><keyname>Sultana</keyname><forenames>Nik</forenames></author></authors><title>Update report: LEO-II version 1.5</title><categories>cs.LO cs.AI cs.MS</categories><comments>7 pages</comments><msc-class>03B35, 68T15</msc-class><acm-class>I.2.3; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent improvements of the LEO-II theorem prover are presented. These
improvements include a revised ATP interface, new translations into first-order
logic, rule support for the axiom of choice, detection of defined equality, and
more flexible strategy scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3764</identifier>
 <datestamp>2014-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3764</id><created>2013-03-15</created><updated>2014-07-23</updated><authors><author><keyname>Fire</keyname><forenames>Michael</forenames></author><author><keyname>Goldschmidt</keyname><forenames>Roy</forenames></author><author><keyname>Elovici</keyname><forenames>Yuval</forenames></author></authors><title>Online Social Networks: Threats and Solutions</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Draft Version</comments><doi>10.1109/COMST.2014.2321628</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many online social network (OSN) users are unaware of the numerous security
risks that exist in these networks, including privacy violations, identity
theft, and sexual harassment, just to name a few. According to recent studies,
OSN users readily expose personal and private details about themselves, such as
relationship status, date of birth, school name, email address, phone number,
and even home address. This information, if put into the wrong hands, can be
used to harm users both in the virtual world and in the real world. These risks
become even more severe when the users are children. In this paper we present a
thorough review of the different security and privacy risks which threaten the
well-being of OSN users in general, and children in particular. In addition, we
present an overview of existing solutions that can provide better protection,
security, and privacy for OSN users. We also offer simple-to-implement
recommendations for OSN users which can improve their security and privacy when
using these platforms. Furthermore, we suggest future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3777</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3777</id><created>2013-03-13</created><updated>2013-07-23</updated><authors><author><keyname>Grinshpun</keyname><forenames>A.</forenames></author><author><keyname>Phalitnonkiat</keyname><forenames>P.</forenames></author><author><keyname>Rubin</keyname><forenames>S.</forenames></author><author><keyname>Tarfulea</keyname><forenames>A.</forenames></author></authors><title>Alternating Traps in Muller and Parity Games</title><categories>cs.LO cs.DS cs.GT</categories><comments>27 pages, 1 figure</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Muller games are played by two players moving a token along a graph; the
winner is determined by the set of vertices that occur infinitely often. The
central algorithmic problem is to compute the winning regions for the players.
Different classes and representations of Muller games lead to problems of
varying computational complexity. One such class are parity games; these are of
particular significance in computational complexity, as they remain one of the
few combinatorial problems known to be in NP and co-NP but not known to be in
P. We show that winning regions for a Muller game can be determined from the
alternating structure of its traps. To every Muller game we then associate a
natural number that we call its trap-depth; this parameter measures how
complicated the trap structure is. We present algorithms for parity games that
run in polynomial time for graphs of bounded trap depth, and in general run in
time exponential in the trap depth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3793</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3793</id><created>2013-03-15</created><updated>2014-11-10</updated><authors><author><keyname>Clark</keyname><forenames>Lane</forenames></author><author><keyname>Gaitan</keyname><forenames>Frank</forenames></author></authors><title>The Distribution of Ramsey Numbers</title><categories>math.CO cs.DM math-ph math.MP physics.comp-ph quant-ph</categories><comments>Published version of manuscript; 5 pages, no figures</comments><msc-class>05D10, 05A16</msc-class><journal-ref>Advances and Applications in Discrete Mathematics vol. 14, pp.
  67-74 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the number of integers in the interval [0,x] that are
non-trivial Ramsey numbers r(k,n) (3 &lt;= k &lt;= n) has order of magnitude (x ln
x)**(1/2).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3796</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3796</id><created>2013-03-15</created><authors><author><keyname>Briat</keyname><forenames>C.</forenames></author><author><keyname>Yavuz</keyname><forenames>E. A.</forenames></author><author><keyname>Hjalmarsson</keyname><forenames>H.</forenames></author><author><keyname>Johansson</keyname><forenames>K. H.</forenames></author><author><keyname>J&#xf6;nsson</keyname><forenames>U. T.</forenames></author><author><keyname>Karlsson</keyname><forenames>G.</forenames></author><author><keyname>Sandberg</keyname><forenames>H.</forenames></author></authors><title>The conservation of information, towards an axiomatized modular modeling
  approach to congestion control</title><categories>cs.NI cs.SY math.CA math.OC physics.flu-dyn</categories><comments>Submitted to IEEE Transactions on Networking. arXiv admin note: text
  overlap with arXiv:1208.1230</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a modular fluid-flow network congestion control model based on a
law of fundamental nature in networks: the conservation of information. Network
elements such as queues, users, and transmission channels and network
performance indicators like sending/acknowledgement rates and delays are
mathematically modelled by applying this law locally. Our contributions are
twofold. First, we introduce a modular metamodel that is sufficiently generic
to represent any network topology. The proposed model is composed of building
blocks that implement mechanisms ignored by the existing ones, which can be
recovered from exact reduction or approximation of this new model. Second, we
provide a novel classification of previously proposed models in the literature
and show that they are often not capable of capturing the transient behavior of
the network precisely. Numerical results obtained from packet-level simulations
demonstrate the accuracy of the proposed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3805</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3805</id><created>2013-03-15</created><updated>2013-10-17</updated><authors><author><keyname>Alstott</keyname><forenames>Jeff</forenames></author><author><keyname>Madnick</keyname><forenames>Stuart</forenames></author><author><keyname>Velu</keyname><forenames>Chander</forenames></author></authors><title>Measuring and Predicting Speed of Social Mobilization</title><categories>physics.soc-ph cs.CY cs.SI</categories><comments>15 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale mobilization of individuals across social networks is becoming
increasingly influential in society. However, little is known about what traits
of recruiters and recruits and affect the speed at which one mobilizes the
other. Here we identify and measure traits of individuals and their
relationships that predict mobilization speed. We ran a global social
mobilization contest and recorded personal traits of the participants and those
they recruited. We identified how those traits corresponded with the speed of
mobilization. Recruits mobilized faster when they first heard about the contest
directly from the contest organization, and decreased in speed when hearing
from less personal source types (e.g. family vs. media). Mobilization was
faster when the recruiter and the recruit heard about the contest through the
same source type, and slower when both individuals were in different countries.
Females mobilized other females faster than males mobilized other males.
Younger recruiters mobilized others faster, and older recruits mobilized
slower. These findings suggest relevant factors for engineering social
mobilization tasks for increased speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3807</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3807</id><created>2013-03-15</created><authors><author><keyname>Almeida</keyname><forenames>P.</forenames></author><author><keyname>Napp</keyname><forenames>D.</forenames></author><author><keyname>Pinto</keyname><forenames>R.</forenames></author></authors><title>A new class of superregular matrices and MDP convolutional codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the problem of constructing superregular matrices that
lead to MDP convolutional codes. These matrices are a type of lower block
triangular Toeplitz matrices with the property that all the square submatrices
that can possibly be nonsingular due to the lower block triangular structure
are nonsingular. We present a new class of matrices that are superregular over
a suficiently large finite field F. Such construction works for any given
choice of characteristic of the field F and code parameters (n; k; d) such that
(n-k)|d. Finally, we discuss the size of F needed so that the proposed matrices
are superregular.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3817</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3817</id><created>2013-03-14</created><authors><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Jain</keyname><forenames>Satbir</forenames></author></authors><title>An Efficient Hybrid Localization Technique in Wireless Sensor Networks</title><categories>cs.NI</categories><comments>6 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensor nodes are low cost, low power devices that are used to collect
physical data and monitor environmental conditions from remote locations.
Wireless Sensor Networks(WSN) are collection of sensor nodes, coordinating
among themselves to perform a particular task. Localization is defined as the
deployment of the sensor nodes at known locations in the network. Localization
techniques are classified as Centralized and Distributed. MDS-Map and SDP are
some of the centralized algorithms while Diffusion, Gradient,APIT, Bounding
Box, Relaxation-Based and Coordinate System Stitching come under Distributed
algorithms. In this paper, we propose a new hybrid localization technique,
which combines the advantages of the centralized and distributed algorithms and
overcomes some of the drawbacks of the existing techniques. Simulations done
with J-Sim prove advantage of the proposed scheme in terms of localization
error calculated by varying the sink nodes, increasing node density and
increasing communication range.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3827</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3827</id><created>2013-03-15</created><authors><author><keyname>Ribeiro</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Almeida</keyname><forenames>Jo&#xe3;o Em&#xed;lio</forenames></author><author><keyname>Rossetti</keyname><forenames>Rosaldo J. F.</forenames></author><author><keyname>Coelho</keyname><forenames>Ant&#xf3;nio</forenames></author><author><keyname>Coelho</keyname><forenames>Ant&#xf3;nio Le&#xe7;a</forenames></author></authors><title>Towards a serious games evacuation simulator</title><categories>cs.MA cs.CY</categories><comments>26th European Conference on Modelling and Simulation ECMS 2012,
  Koblenz, Germany; ISBN: 978-0-9564944-4-3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evacuation of complex buildings is a challenge under any circumstances.
Fire drills are a way of training and validating evacuation plans. However,
sometimes these plans are not taken seriously by their participants. It is also
difficult to have the financial and time resources required. In this scenario,
serious games can be used as a tool for training, planning and evaluating
emergency plans. In this paper a prototype of a serious games evacuation
simulator is presented. To make the environment as realistic as possible, 3D
models were made using Blender and loaded onto Unity3D, a popular game engine.
This framework provided us with the appropriate simulation environment. Some
experiences were made and results show that this tool has potential for
practitioners and planners to use it for training building occupants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3828</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3828</id><created>2013-03-15</created><authors><author><keyname>Ribeiro</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Almeida</keyname><forenames>Jo&#xe3;o Em&#xed;lio</forenames></author><author><keyname>Rossetti</keyname><forenames>Rosaldo J. F.</forenames></author><author><keyname>Coelho</keyname><forenames>Ant&#xf3;nio</forenames></author><author><keyname>Coelho</keyname><forenames>Ant&#xf3;nio Le&#xe7;a</forenames></author></authors><title>Using Serious Games to Train Evacuation Behaviour</title><categories>cs.MA</categories><comments>CISTI 2012 - 7 Conferencia Ib\'erica de Sistemas y Tecnolog\'ias de
  Informaci\'on, pp 771-776, Madrid, Spain. ISBN: 978-989-96247-6-4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emergency evacuation plans and evacuation drills are mandatory in public
buildings in many countries. Their importance is considerable when it comes to
guarantee safety and protection during a crisis. However, sometimes
discrepancies arise between the goals of the plan and its outcomes, because
people find it hard to take them very seriously, or due to the financial and
time resources required. Serious games are a possible solution to tackle this
problem. They have been successfully applied in different areas such as health
care and education, since they can simulate an environment/task quite
accurately, making them a practical alternative to real-life simulations. This
paper presents a serious game developed using Unity3D to recreate a virtual
fire evacuation training tool. The prototype application was deployed which
allowed the validation by user testing. A sample of 30 individuals tested the
evacuating scenario, having to leave the building during a fire in the shortest
time possible. Results have shown that users effectively end up learning some
evacuation procedures from the activity, even if only to look for emergency
signs indicating the best evacuation paths. It was also evidenced that users
with higher video game experience had a significantly better performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3844</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3844</id><created>2013-03-15</created><authors><author><keyname>Wang</keyname><forenames>T.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Mitchell</keyname><forenames>P. D.</forenames></author></authors><title>Low-Complexity Channel Estimation with Set-Membership Algorithms for
  Cooperative Wireless Sensor Networks</title><categories>cs.IT math.IT</categories><comments>15 Figures</comments><journal-ref>IEEE Transactions on Vehicular Technology, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a general cooperative wireless sensor network
(WSN) with multiple hops and the problem of channel estimation. Two
matrix-based set-membership algorithms are developed for the estimation of the
complex matrix channel parameters. The main goal is to reduce the computational
complexity significantly as compared with existing channel estimators and
extend the lifetime of the WSN by reducing its power consumption. The first
proposed algorithm is the set-membership normalized least mean squares
(SM-NLMS) algorithm. The second is the set-membership recursive least squares
(RLS) algorithm called BEACON. Then, we present and incorporate an error bound
function into the two channel estimation methods which can adjust the error
bound automatically with the update of the channel estimates. Steady-state
analysis in the output mean-squared error (MSE) are presented and closed-form
formulae for the excess MSE and the probability of update in each recursion are
provided. Computer simulations show good performance of our proposed algorithms
in terms of convergence speed, steady state mean square error and bit error
rate (BER) and demonstrate reduced complexity and robustness against the
time-varying environments and different signal-to-noise ratio (SNR) values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3849</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3849</id><created>2013-03-15</created><authors><author><keyname>Wang</keyname><forenames>T.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Schmeink</keyname><forenames>A.</forenames></author></authors><title>Joint Maximum Sum-Rate Receiver Design and Power Adjustment for Multihop
  Wireless Sensor Networks</title><categories>cs.IT math.IT</categories><comments>3 figures</comments><journal-ref>ICASSP 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a multihop wireless sensor network (WSN) with
multiple relay nodes for each hop where the amplify-and-forward (AF) scheme is
employed. We present a strategy to jointly design the linear receiver and the
power allocation parameters via an alternating optimization approach that
maximizes the sum rate of the WSN. We derive constrained maximum sum-rate (MSR)
expressions along with an algorithm to compute the linear receiver and the
power allocation parameters with the optimal complex amplification coefficients
for each relay node. Computer simulations show good performance of our proposed
methods in terms of sum rate compared to the method with equal power
allocation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3855</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3855</id><created>2013-03-15</created><authors><author><keyname>Gorban</keyname><forenames>A. N.</forenames></author><author><keyname>Yablonsky</keyname><forenames>G. S.</forenames></author></authors><title>Grasping Complexity</title><categories>cs.GL</categories><comments>8 pages, 3 figures, bibliography 52 items</comments><journal-ref>Computers and Mathematics with Applications 65 (2013) 1421-1426</journal-ref><doi>10.1016/j.camwa.2013.04.023</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The century of complexity has come. The face of science has changed.
Surprisingly, when we start asking about the essence of these changes and then
critically analyse the answers, the result are mostly discouraging. Most of the
answers are related to the properties that have been in the focus of scientific
research already for more than a century (like non-linearity). This paper is
Preface to the special issue &quot;Grasping Complexity&quot; of the journal &quot;Computers
and Mathematics with Applications&quot;. We analyse the change of era in science,
its reasons and main changes in scientific activity and give a brief review of
the papers in the issue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3875</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3875</id><created>2013-03-15</created><authors><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author><author><keyname>Costas</keyname><forenames>Rodrigo</forenames></author></authors><title>F1000 recommendations as a new data source for research evaluation: A
  comparison with citations</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  F1000 is a post-publication peer review service for biological and medical
research. F1000 aims to recommend important publications in the biomedical
literature, and from this perspective F1000 could be an interesting tool for
research evaluation. By linking the complete database of F1000 recommendations
to the Web of Science bibliographic database, we are able to make a
comprehensive comparison between F1000 recommendations and citations. We find
that about 2% of the publications in the biomedical literature receive at least
one F1000 recommendation. Recommended publications on average receive 1.30
recommendations, and over 90% of the recommendations are given within half a
year after a publication has appeared. There turns out to be a clear
correlation between F1000 recommendations and citations. However, the
correlation is relatively weak, at least weaker than the correlation between
journal impact and citations. More research is needed to identify the main
reasons for differences between recommendations and citations in assessing the
impact of publications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3891</identifier>
 <datestamp>2013-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3891</id><created>2013-03-15</created><updated>2013-05-01</updated><authors><author><keyname>Paparo</keyname><forenames>G. D.</forenames></author><author><keyname>Mueller</keyname><forenames>M.</forenames></author><author><keyname>Comellas</keyname><forenames>F.</forenames></author><author><keyname>Martin-Delgado</keyname><forenames>M. A.</forenames></author></authors><title>Quantum Google in a Complex Network</title><categories>quant-ph cond-mat.stat-mech cs.NI physics.soc-ph</categories><journal-ref>Sci. Rep. 3, 2773; (2013)</journal-ref><doi>10.1038/srep02773</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the behavior of the recently proposed quantum Google
algorithm, or quantum PageRank, in large complex networks. Applying the quantum
algorithm to a part of the real World Wide Web, we find that the algorithm is
able to univocally reveal the underlying scale-free topology of the network and
to clearly identify and order the most relevant nodes (hubs) of the graph
according to their importance in the network structure. Moreover, our results
show that the quantum PageRank algorithm generically leads to changes in the
hierarchy of nodes. In addition, as compared to its classical counterpart, the
quantum algorithm is capable to clearly highlight the structure of secondary
hubs of the network, and to partially resolve the degeneracy in importance of
the low lying part of the list of rankings, which represents a typical
shortcoming of the classical PageRank algorithm. Complementary to this study,
our analysis shows that the algorithm is able to clearly distinguish scale-free
networks from other widespread and important classes of complex networks, such
as Erd\H{o}s-R\'enyi networks and hierarchical graphs. We show that the ranking
capabilities of the quantum PageRank algorithm are related to an increased
stability with respect to a variation of the damping parameter $\alpha$ that
appears in the Google algorithm, and to a more clearly pronounced power-law
behavior in the distribution of importance among the nodes, as compared to the
classical algorithm. Finally, we study to which extent the increased
sensitivity of the quantum algorithm persists under coordinated attacks of the
most important nodes in scale-free and Erd\H{o}s-R\'enyi random graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3901</identifier>
 <datestamp>2013-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3901</id><created>2013-03-15</created><updated>2013-10-06</updated><authors><author><keyname>Sinha</keyname><forenames>Ankur</forenames></author><author><keyname>Malo</keyname><forenames>Pekka</forenames></author><author><keyname>Deb</keyname><forenames>Kalyanmoy</forenames></author></authors><title>Efficient Evolutionary Algorithm for Single-Objective Bilevel
  Optimization</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bilevel optimization problems are a class of challenging optimization
problems, which contain two levels of optimization tasks. In these problems,
the optimal solutions to the lower level problem become possible feasible
candidates to the upper level problem. Such a requirement makes the
optimization problem difficult to solve, and has kept the researchers busy
towards devising methodologies, which can efficiently handle the problem.
Despite the efforts, there hardly exists any effective methodology, which is
capable of handling a complex bilevel problem. In this paper, we introduce
bilevel evolutionary algorithm based on quadratic approximations (BLEAQ) of
optimal lower level variables with respect to the upper level variables. The
approach is capable of handling bilevel problems with different kinds of
complexities in relatively smaller number of function evaluations. Ideas from
classical optimization have been hybridized with evolutionary methods to
generate an efficient optimization algorithm for generic bilevel problems. The
efficacy of the algorithm has been shown on two sets of test problems. The
first set is a recently proposed SMD test set, which contains problems with
controllable complexities, and the second set contains standard test problems
collected from the literature. The proposed method has been evaluated against
two benchmarks, and the performance gain is observed to be significant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3904</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3904</id><created>2013-03-15</created><authors><author><keyname>Chi</keyname><forenames>Yuejie</forenames></author><author><keyname>Xie</keyname><forenames>Yao</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author></authors><title>Compressive Demodulation of Mutually Interfering Signals</title><categories>cs.IT math.IT</categories><comments>submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-User Detection is fundamental not only to cellular wireless
communication but also to Radio-Frequency Identification (RFID) technology that
supports supply chain management. The challenge of Multi-user Detection (MUD)
is that of demodulating mutually interfering signals, and the two biggest
impediments are the asynchronous character of random access and the lack of
channel state information. Given that at any time instant the number of active
users is typically small, the promise of Compressive Sensing (CS) is the
demodulation of sparse superpositions of signature waveforms from very few
measurements. This paper begins by unifying two front-end architectures
proposed for MUD by showing that both lead to the same discrete signal model.
Algorithms are presented for coherent and noncoherent detection that are based
on iterative matching pursuit. Noncoherent detection is all that is needed in
the application to RFID technology where it is only the identity of the active
users that is required. The coherent detector is also able to recover the
transmitted symbols. It is shown that compressive demodulation requires
$\mathcal{O}(K\log N(\tau+1))$ samples to recover $K$ active users whereas
standard MUD requires $N(\tau+1)$ samples to process $N$ total users with a
maximal delay $\tau$. Performance guarantees are derived for both coherent and
noncoherent detection that are identical in the way they scale with number of
active users. The power profile of the active users is shown to be less
important than the SNR of the weakest user. Gabor frames and Kerdock codes are
proposed as signature waveforms and numerical examples demonstrate the superior
performance of Kerdock codes - the same probability of error with less than
half the samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3921</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3921</id><created>2013-03-15</created><authors><author><keyname>Forbes</keyname><forenames>Michael</forenames></author><author><keyname>Yekhanin</keyname><forenames>Sergey</forenames></author></authors><title>On the Locality of Codeword Symbols in Non-Linear Codes</title><categories>cs.IT cs.DM math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a possibly non-linear (n,K,d)_q code. Coordinate i has locality r if
its value is determined by some r other coordinates. A recent line of work
obtained an optimal trade-off between information locality of codes and their
redundancy. Further, for linear codes meeting this trade-off, structure
theorems were derived. In this work we give a new proof of the locality /
redundancy trade-off and generalize structure theorems to non-linear codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3931</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3931</id><created>2013-03-15</created><authors><author><keyname>Gysel</keyname><forenames>Rob</forenames></author></authors><title>Potential Maximal Clique Algorithms for Perfect Phylogeny Problems</title><categories>cs.DM cs.CE cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kloks, Kratsch, and Spinrad showed how treewidth and minimum-fill, NP-hard
combinatorial optimization problems related to minimal triangulations, are
broken into subproblems by block subgraphs defined by minimal separators. These
ideas were expanded on by Bouchitt\'e and Todinca, who used potential maximal
cliques to solve these problems using a dynamic programming approach in time
polynomial in the number of minimal separators of a graph. It is known that
solutions to the perfect phylogeny problem, maximum compatibility problem, and
unique perfect phylogeny problem are characterized by minimal triangulations of
the partition intersection graph. In this paper, we show that techniques
similar to those proposed by Bouchitt\'e and Todinca can be used to solve the
perfect phylogeny problem with missing data, the two- state maximum
compatibility problem with missing data, and the unique perfect phylogeny
problem with missing data in time polynomial in the number of minimal
separators of the partition intersection graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3934</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3934</id><created>2013-03-15</created><updated>2015-10-06</updated><authors><author><keyname>Tan</keyname><forenames>Feng</forenames></author><author><keyname>Slotine</keyname><forenames>Jean-Jacques</forenames></author></authors><title>A Quorum Sensing Inspired Algorithm for Dynamic Clustering</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quorum sensing is a decentralized biological process, through which a
community of cells with no global awareness coordinate their functional
behaviors based solely on cell-medium interactions and local decisions. This
paper draws inspirations from quorum sensing and colony competition to derive a
new algorithm for data clustering. The algorithm treats each data as a single
cell, and uses knowledge of local connectivity to cluster cells into multiple
colonies simultaneously. It simulates auto-inducers secretion in quorum sensing
to tune the influence radius for each cell. At the same time, sparsely
distributed core cells spread their influences to form colonies, and
interactions between colonies eventually determine each cell's identity. The
algorithm has the flexibility to analyze not only static but also time-varying
data, which surpasses the capacity of many existing algorithms. Its stability
and convergence properties are established. The algorithm is tested on several
applications, including both synthetic and real benchmarks data sets, alleles
clustering, community detection, image segmentation. In particular, the
algorithm's distinctive capability to deal with time-varying data allows us to
experiment it on novel applications such as robotic swarms grouping and
switching model identification. We believe that the algorithm's promising
performance would stimulate many more exciting applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3943</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3943</id><created>2013-03-16</created><authors><author><keyname>Das</keyname><forenames>Abhik Kumar</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>On Finite Alphabet Compressive Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of compressive sensing over a finite
alphabet, where the finite alphabet may be inherent to the nature of the data
or a result of quantization. There are multiple examples of finite alphabet
based static as well as time-series data with inherent sparse structure; and
quantizing real values is an essential step while handling real data in
practice. We show that there are significant benefits to analyzing the problem
while incorporating its finite alphabet nature, versus ignoring it and
employing a conventional real alphabet based toolbox. Specifically, when the
alphabet is finite, our techniques (a) have a lower sample complexity compared
to real-valued compressive sensing for sparsity levels below a threshold; (b)
facilitate constructive designs of sensing matrices based on coding-theoretic
techniques; (c) enable one to solve the exact $\ell_0$-minimization problem in
polynomial time rather than a approach of convex relaxation followed by
sufficient conditions for when the relaxation matches the original problem; and
finally, (d) allow for smaller amount of data storage (in bits).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3945</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3945</id><created>2013-03-16</created><authors><author><keyname>Tanaka</keyname><forenames>Toshiya</forenames></author><author><keyname>I</keyname><forenames>Tomohiro</forenames></author><author><keyname>Inenaga</keyname><forenames>Shunsuke</forenames></author><author><keyname>Bannai</keyname><forenames>Hideo</forenames></author><author><keyname>Takeda</keyname><forenames>Masayuki</forenames></author></authors><title>Computing convolution on grammar-compressed text</title><categories>cs.DS</categories><comments>DCC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The convolution between a text string $S$ of length $N$ and a pattern string
$P$ of length $m$ can be computed in $O(N \log m)$ time by FFT. It is known
that various types of approximate string matching problems are reducible to
convolution. In this paper, we assume that the input text string is given in a
compressed form, as a \emph{straight-line program (SLP)}, which is a context
free grammar in the Chomsky normal form that derives a single string. Given an
SLP $\mathcal{S}$ of size $n$ describing a text $S$ of length $N$, and an
uncompressed pattern $P$ of length $m$, we present a simple $O(nm \log m)$-time
algorithm to compute the convolution between $S$ and $P$. We then show that
this can be improved to $O(\min\{nm, N-\alpha\} \log m)$ time, where $\alpha
\geq 0$ is a value that represents the amount of redundancy that the SLP
captures with respect to the length-$m$ substrings. The key of the improvement
is our new algorithm that computes the convolution between a trie of size $r$
and a pattern string $P$ of length $m$ in $O(r \log m)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3948</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3948</id><created>2013-03-16</created><authors><author><keyname>Shrawankar</keyname><forenames>Urmila</forenames></author><author><keyname>Thakare</keyname><forenames>Vilas</forenames></author></authors><title>An Adaptive Methodology for Ubiquitous ASR System</title><categories>cs.CL cs.HC cs.SD</categories><comments>10 Pages, 05 Tables, 03 Figures</comments><journal-ref>Computer and Information Science;Vol.6,No.1;2013 ISSN 1913-8989
  E-ISSN 1913-8997 Computer and Information Science; Vol. 6, No. 1; 2013, ISSN
  1913-8989 E-ISSN 1913-8997, Published by Canadian Center of Science and
  Education</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Achieving and maintaining the performance of ubiquitous (Automatic Speech
Recognition) ASR system is a real challenge. The main objective of this work is
to develop a method that will improve and show the consistency in performance
of ubiquitous ASR system for real world noisy environment. An adaptive
methodology has been developed to achieve an objective with the help of
implementing followings, -Cleaning speech signal as much as possible while
preserving originality / intangibility using various modified filters and
enhancement techniques. -Extracting features from speech signals using various
sizes of parameter. -Train the system for ubiquitous environment using
multi-environmental adaptation training methods. -Optimize the word recognition
rate with appropriate variable size of parameters using fuzzy technique. The
consistency in performance is tested using standard noise databases as well as
in real world environment. A good improvement is noticed. This work will be
helpful to give discriminative training of ubiquitous ASR system for better
Human Computer Interaction (HCI) using Speech User Interface (SUI).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3954</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3954</id><created>2013-03-16</created><authors><author><keyname>Mishra</keyname><forenames>Sumita</forenames></author><author><keyname>Chaudhary</keyname><forenames>Naresh K</forenames></author><author><keyname>Singh</keyname><forenames>Kalyan</forenames></author></authors><title>Overview of Optical Interconnect Technology</title><categories>cs.ET</categories><journal-ref>International Journal of Scientific &amp; Engineering Research, Volume
  3, Issue 4, April -2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical interconnect is seen as a potential solution to meet the performance
requirements of current and future generation of data processors. Optical
interconnects have negligible frequency dependent loss, low cross talk and high
band width. Optical interconnects are not much used commercially since optical
interconnects technology is incompatible with manufacturing processes and
assembly methods that are currently used in the semiconductor industry. There
are many promising optical interconnect technologies and this paper presents a
brief analysis of current state of optical interconnect technology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3962</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3962</id><created>2013-03-16</created><authors><author><keyname>Saeed</keyname><forenames>Ahmed</forenames></author><author><keyname>Ibrahim</keyname><forenames>Mohamed</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author><author><keyname>Harras</keyname><forenames>Khaled A.</forenames></author></authors><title>Towards Dynamic Real-Time Geo-location Databases for TV White Spaces</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent FCC regulations on TV white spaces allow geo-location databases to be
the sole source of spectrum information for White Space Devices (WSDs).
Geo-location databases protect TV band incumbents by keeping track of TV
transmitters and their protected service areas based on their location,
transmission parameters and sophisticated propagation models. In this article,
we argue that keeping track of both TV transmitters and TV receivers (i.e. TV
sets) can achieve significant improvement in the availability of white spaces.
We first identify wasted spectrum opportunities, both temporal and spatial, due
to the current approach of white spaces detection. We then propose DynaWhite, a
cloud-based architecture that orchestrates the detection and dissemination of
highly-dynamic, real-time, and fine-grained TV white space information.
DynaWhite introduces the next generation of geo-location databases by combining
traditional sensing techniques with a novel unconventional sensing approach
based on the detection of the passive TV receivers using standard cell phones.
We present a quantitative evaluation of the potential gains in white space
availability for large scale deployments of DynaWhite. We finally identify
challenges that need to be addressed in the research community in order to
exploit this potential for leveraging dynamic real-time fine-grained TV white
spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3964</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3964</id><created>2013-03-16</created><authors><author><keyname>Nasution</keyname><forenames>Mahyuddin K. M.</forenames></author></authors><title>Simple Search Engine Model: Selective Properties</title><categories>cs.IR</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the relationship between query and search engine by
exploring the selective properties based on a simple search engine. We used the
set theory and utilized the words and terms for defining singleton and
doubleton in the event spaces and then provided their implementation for
proving the existence of the shadow of micro-cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3965</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3965</id><created>2013-03-16</created><authors><author><keyname>Van</keyname><forenames>Vo Tam</forenames><affiliation>Tiffany</affiliation></author><author><keyname>Mita</keyname><forenames>Seiichi</forenames><affiliation>Tiffany</affiliation></author><author><keyname>Jing</keyname><affiliation>Tiffany</affiliation></author><author><keyname>Li</keyname></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author></authors><title>Bit Level Soft Decision Decoding of Triple Parity Reed Solomon Codes
  through Automorphism Groups</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses bit-level soft decoding of triple-parity Reed-Solomon
(RS) codes through automorphism permutation. A new method for identifying the
automorphism groups of RS binary images is first developed. The new algorithm
runs effectively, and can handle more RS codes and capture more automorphism
groups than the existing ones. Utilizing the automorphism results, a new
bit-level soft-decision decoding algorithm is subsequently developed for
general $(n,n-3,4)$ RS codes. Simulation on $(31,28,4)$ RS codes demonstrates
an impressive gain of more than 1 dB at the bit error rate of $10^{-5}$ over
the existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3984</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3984</id><created>2013-03-16</created><authors><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author><author><keyname>Zargham</keyname><forenames>Michael</forenames></author><author><keyname>Enyioha</keyname><forenames>Chinwendu</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author><author><keyname>Pappas</keyname><forenames>George</forenames></author></authors><title>Optimal Vaccine Allocation to Control Epidemic Outbreaks in Arbitrary
  Networks</title><categories>cs.SI math.OC physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of controlling the propagation of an epidemic
outbreak in an arbitrary contact network by distributing vaccination resources
throughout the network. We analyze a networked version of the
Susceptible-Infected-Susceptible (SIS) epidemic model when individuals in the
network present different levels of susceptibility to the epidemic. In this
context, controlling the spread of an epidemic outbreak can be written as a
spectral condition involving the eigenvalues of a matrix that depends on the
network structure and the parameters of the model. We study the problem of
finding the optimal distribution of vaccines throughout the network to control
the spread of an epidemic outbreak. We propose a convex framework to find
cost-optimal distribution of vaccination resources when different levels of
vaccination are allowed. We also propose a greedy approach with quality
guarantees for the case of all-or-nothing vaccination. We illustrate our
approaches with numerical simulations in a real social network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3987</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3987</id><created>2013-03-16</created><authors><author><keyname>Wang</keyname><forenames>Liping</forenames></author><author><keyname>Chen</keyname><forenames>Songcan</forenames></author></authors><title>$l_{2,p}$ Matrix Norm and Its Application in Feature Selection</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, $l_{2,1}$ matrix norm has been widely applied to many areas such as
computer vision, pattern recognition, biological study and etc. As an extension
of $l_1$ vector norm, the mixed $l_{2,1}$ matrix norm is often used to find
jointly sparse solutions. Moreover, an efficient iterative algorithm has been
designed to solve $l_{2,1}$-norm involved minimizations. Actually,
computational studies have showed that $l_p$-regularization ($0&lt;p&lt;1$) is
sparser than $l_1$-regularization, but the extension to matrix norm has been
seldom considered. This paper presents a definition of mixed $l_{2,p}$ $(p\in
(0, 1])$ matrix pseudo norm which is thought as both generalizations of $l_p$
vector norm to matrix and $l_{2,1}$-norm to nonconvex cases $(0&lt;p&lt;1)$.
Fortunately, an efficient unified algorithm is proposed to solve the induced
$l_{2,p}$-norm $(p\in (0, 1])$ optimization problems. The convergence can also
be uniformly demonstrated for all $p\in (0, 1]$. Typical $p\in (0,1]$ are
applied to select features in computational biology and the experimental
results show that some choices of $0&lt;p&lt;1$ do improve the sparse pattern of
using $p=1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.3990</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.3990</id><created>2013-03-16</created><updated>2013-12-14</updated><authors><author><keyname>Gligorijevic</keyname><forenames>Vladimir</forenames></author></authors><title>Master thesis: Growth and Self-Organization Processes in Directed Social
  Network</title><categories>physics.soc-ph cs.SI</categories><comments>This paper has been withdrawn due to its incompleteness</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large dataset collected from Ubuntu chat channel is studied as a complex
dynamical system with emergent collective behaviour of users. With the
appropriate network mappings we examined wealthy topological structure of
Ubuntu network. The structure of this network is determined by computing
different topological measures. The directed, weighted network, which is a
suitable representation of the dataset from Ubuntu chat channel is
characterized with power law dependencies of various quantities, hierarchical
organization and disassortative mixing patterns. Beyond the topological
features, the emergent collective state is further quantified by analysis of
time series of users activities driven by emotions. Analysis of time series
reveals self-organized dynamics with long-range temporal correlations in user
actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4006</identifier>
 <datestamp>2013-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4006</id><created>2013-03-16</created><updated>2013-10-02</updated><authors><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Lo</keyname><forenames>Ernest S.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Wireless Information and Power Transfer: Energy Efficiency Optimization
  in OFDMA Systems</title><categories>cs.IT math.IT</categories><comments>7 figures and 2 table, accepted for publication in the IEEE
  Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers orthogonal frequency division multiple access systems
with simultaneous wireless information and power transfer.
  We study the resource allocation algorithm design for maximization of the
energy efficiency of data transmission. In particular, we focus on power
splitting hybrid receivers which are able to split the received signals into
two power streams for concurrent information decoding and energy harvesting.
Two scenarios are investigated considering different power splitting abilities
of the receivers. In the first scenario, we assume receivers which can split
the received power into a continuous set of power streams with arbitrary power
splitting ratios. In the second scenario, we examine receivers which can split
the received power only into a discrete set of power streams with fixed power
splitting ratios. In both scenarios, we formulate the corresponding algorithm
design as a non-convex optimization problem which takes into account the
circuit power consumption, the minimum data rate requirements of delay
constrained services, the minimum required system data rate, and the minimum
amount of power that has to be delivered to the receivers. Subsequently, by
exploiting fractional programming and dual decomposition, suboptimal iterative
resource allocation algorithms are proposed to solve the non-convex problems.
Simulation results illustrate that the proposed iterative resource allocation
algorithms approach the optimal solution within a small number of iterations
and unveil the trade-off between energy efficiency, system capacity, and
wireless power transfer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4015</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4015</id><created>2013-03-16</created><updated>2013-11-03</updated><authors><author><keyname>Ko&#xe7;o</keyname><forenames>Sokol</forenames><affiliation>LIF</affiliation></author><author><keyname>Capponi</keyname><forenames>C&#xe9;cile</forenames><affiliation>LIF</affiliation></author></authors><title>On multi-class learning through the minimization of the confusion matrix
  norm</title><categories>cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In imbalanced multi-class classification problems, the misclassification rate
as an error measure may not be a relevant choice. Several methods have been
developed where the performance measure retained richer information than the
mere misclassification rate: misclassification costs, ROC-based information,
etc. Following this idea of dealing with alternate measures of performance, we
propose to address imbalanced classification problems by using a new measure to
be optimized: the norm of the confusion matrix. Indeed, recent results show
that using the norm of the confusion matrix as an error measure can be quite
interesting due to the fine-grain informations contained in the matrix,
especially in the case of imbalanced classes. Our first contribution then
consists in showing that optimizing criterion based on the confusion matrix
gives rise to a common background for cost-sensitive methods aimed at dealing
with imbalanced classes learning problems. As our second contribution, we
propose an extension of a recent multi-class boosting method --- namely
AdaBoost.MM --- to the imbalanced class problem, by greedily minimizing the
empirical norm of the confusion matrix. A theoretical analysis of the
properties of the proposed method is presented, while experimental results
illustrate the behavior of the algorithm and show the relevancy of the approach
compared to other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4017</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4017</id><created>2013-03-16</created><authors><author><keyname>Medjdoub</keyname><forenames>Benachir</forenames><affiliation>LGI, The Martin Centre</affiliation></author><author><keyname>Yannou</keyname><forenames>Bernard</forenames><affiliation>LGI</affiliation></author></authors><title>Separating Topology and Geometry in Space Planning</title><categories>cs.AI physics.med-ph</categories><proxy>ccsd</proxy><journal-ref>Computer Aided-Design 32, 1 (2000) 39-61</journal-ref><doi>10.1016/S0010-4485(99)00084-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are dealing with the problem of space layout planning here. We present an
architectural conceptual CAD approach. Starting with design specifications in
terms of constraints over spaces, a specific enumeration heuristics leads to a
complete set of consistent conceptual design solutions named topological
solutions. These topological solutions which do not presume any precise
definitive dimension correspond to the sketching step that an architect carries
out from the Design specifications on a preliminary design phase in
architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4019</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4019</id><created>2013-03-16</created><authors><author><keyname>Frackiewicz</keyname><forenames>Piotr</forenames></author></authors><title>A comment on the generalization of the Marinatto-Weber quantum game
  scheme</title><categories>cs.GT quant-ph</categories><journal-ref>Acta Physica Polonica B 44 (2013) 29</journal-ref><doi>10.5506/APhysPolB.44.29</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iqbal and Toor [Phys. Rev. A {\bf 65}, 022306 (2002)] and [Commun. Theor.
Phys. {\bf 42}, 335 (2004)] generalized the Marinatto-Weber quantum scheme for
$2 \times 2$ games in order to study bimatrix games of $3 \times 3$ dimension,
in particular the Rock-Paper-Scissors game. In our paper we show that Iqbal and
Toor's generalization exhibits certain undesirable property that can
considerably influence the game result. To support our argumentation, in the
further part of the paper we construct the protocol corresponding to the MW
concept for any finite bimatrix game that is free from the fault.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4025</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4025</id><created>2013-03-16</created><authors><author><keyname>Bonamy</keyname><forenames>Marthe</forenames></author></authors><title>Planar graphs with maximum degree D at least 8 are (D+1)-edge-choosable</title><categories>cs.DM math.CO</categories><comments>31 pages, 20 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of list edge coloring for planar graphs. Edge
coloring is the problem of coloring the edges while ensuring that two edges
that are incident receive different colors. A graph is k-edge-choosable if for
any assignment of k colors to every edge, there is an edge coloring such that
the color of every edge belongs to its color assignment. Vizing conjectured in
1965 that every graph is (D+1)-edge-choosable, where D is the maximum degree.
In 1990, Borodin solved the conjecture for planar graphs with maximum degree at
least 9, and asked whether the bound could be lowered to 8. We prove here that
planar graphs with maximum degree D at least 8 are (D+1)-edge-choosable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4031</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4031</id><created>2013-03-16</created><authors><author><keyname>Rajabi-Alni</keyname><forenames>Fatemeh</forenames></author></authors><title>Two exact algorithms for the generalized assignment problem</title><categories>cs.DS</categories><comments>13 pages, 1 figure. arXiv admin note: substantial text overlap with
  arXiv:1302.4426</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let A={a_1,a_2,...,a_s} and B={b_1,b_2,...,b_t} be two sets of objects with
s+r=n, the generalized assignment problem assigns each element a_i in A to at
least alpha_i and at most alpha '_i elements in B, and each element b_j in B to
at least beta_j and at most beta '_j elements in A for all 1 &lt;= i &lt;= s and 1 &lt;=
j &lt;= t. In this paper, we present an O(n^4) time and O(n) space algorithm for
this problem using the well known Hungarian algorithm. We also present an
O(n^3) algorithm for a special case of the generalized assignment, called the
limited-capacity assignment problem, where alpha_i,beta_j=1 for all i,j.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4036</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4036</id><created>2013-03-17</created><authors><author><keyname>Pramanik</keyname><forenames>I.</forenames></author><author><keyname>Hasan</keyname><forenames>M. A. F. M. Rashidul</forenames></author><author><keyname>Yasmin</keyname><forenames>Rubaiyat</forenames></author><author><keyname>Hossain</keyname><forenames>M. Sakir</forenames></author><author><keyname>K</keyname><forenames>Ahmed Kamal S.</forenames></author></authors><title>Performance Analysis of OFDM-based System for Various Channels</title><categories>cs.IT math.IT</categories><comments>Journal of Donetsk National University</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The demand for high-speed mobile wireless communications is rapidly growing.
Orthogonal Frequency Division Multiplexing (OFDM) technology promises to be a
key technique for achieving the high data capacity and spectral efficiency
requirements for wireless communication systems in the near future. This paper
investigates the performance of OFDM-based system over static and non-static or
fading channels. In order to investigate this, a simulation model has been
created and implemented using MATLAB. A comparison has also been made between
the performances of coherent and differential modulation scheme over static and
fading channels. In the fading channels, it has been found that OFDM-based
system's performance depends severely on Doppler shift which in turn depends on
the velocity of user. It has been found that performance degrades as Doppler
shift increases, as expected. This paper also performs a comparative study of
OFDM-based system's performance on different fading channels and it has been
found that it performs better over Rician channel, as expected and system
performance improves as the value of Rician factor increases, as expected. As a
last task, a coding technique, Gray Coding, has been used to improve system
performace and it is found that it improves system performance by reducing BER
about 25-32 percent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4037</identifier>
 <datestamp>2013-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4037</id><created>2013-03-17</created><updated>2013-04-26</updated><authors><author><keyname>Hossain</keyname><forenames>Md. Sakir</forenames></author><author><keyname>Ahmed</keyname><forenames>Sabbir</forenames></author><author><keyname>Ullah</keyname><forenames>Enayet</forenames></author><author><keyname>Islam</keyname><forenames>Md. Atiqul</forenames></author></authors><title>PAPR Reduction of OFDM System Through Iterative Selection of Input
  Sequences</title><categories>cs.IT math.IT</categories><comments>IJECCT 2013, Vol. 3 (2)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orthogonal Frequency Division Multiplexing (OFDM) based multi-carrier systems
can support high data rate wireless transmission without the requirement of any
extensive equalization and yet offer excellent immunity against fading and
inter-symbol interference. But one of the major drawbacks of these systems is
the large Peak-to-Average Power Ratio (PAPR) of the transmit signal which
renders a straightforward implementation costly and inefficient. In this paper,
a new PAPR reduction scheme is introduced where a number of sequences from the
original data sequence is generated by changing the position of each symbol and
the sequence with lowest PAPR is selected for transmission. A comparison of
performance of this proposed technique with an existing PAPR reduction scheme,
i.e., the Selective Mapping (SLM) is performed. It is shown that considerable
reduction in PAPR along with higher throughput can be achieved at the expense
of some additional computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4051</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4051</id><created>2013-03-17</created><authors><author><keyname>Pchelintsev</keyname><forenames>A. N.</forenames></author></authors><title>The failure risk analysis of digital circuits</title><categories>cs.DM</categories><comments>In this work there was considered the method of failure risk analysis
  of digital circuits using an analytic representation of signals within the
  circuit. At that logical operations had to by replaced by arithmetic
  operations. The transitions from one signal state to another is described by
  the Heaviside function</comments><msc-class>94C05, 06E25, 06E30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To analyze the failure risk of asynchronous digital circuits the
time-parameter is introduced into the Boolean algebra replacing the arithmetic
operations by logical operations. There considered an example of construction
of signals passing through the logical elements, using the described below
mathematical apparatus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4056</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4056</id><created>2013-03-17</created><authors><author><keyname>Amroune</keyname><forenames>Mohamed</forenames></author><author><keyname>Inglebert</keyname><forenames>Jean Michel</forenames></author><author><keyname>Zarour</keyname><forenames>Nacereddine</forenames></author><author><keyname>Charrel</keyname><forenames>Pierre Jean</forenames></author></authors><title>A weaving process to define requirements for Cooperative Information
  System</title><categories>cs.SE</categories><journal-ref>International Journal of Computer Science Issues,IJCSI Volume 10,
  Issue 1, January 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of a Cooperative Information System (CIS) becomes more and
more complex, new challenges arise for managing this complexity. So, the aspect
paradigm is regarded as a promising software development technique which can
reduce the complexity and cost of developing large software systems. This
opportunity can be used to develop a CIS able to support the interconnection of
organizations information systems in order to ensure a common global service
and to support the tempo of change in the business world that is increasing at
an exponential level. We previously proposed an approach named AspeCiS (An
Aspect-oriented Approach to Develop a Cooperative Information System) to
develop a Cooperative Information System from existing Information Systems by
using their artifacts such as existing requirements, and design. In this
approach we have studied how to elicit CIS Requirements called Cooperative
Requirements in AspeCiS. In this paper we propose a weaving process to define
these requirements by reusing existing requirements and new aspectual
requirements that we define to modify these requirements in order to be reused.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4061</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4061</id><created>2013-03-17</created><authors><author><keyname>Kamat</keyname><forenames>Vikram</forenames></author><author><keyname>Misra</keyname><forenames>Neeldhara</forenames></author></authors><title>An Erd\H{o}s--Ko--Rado theorem for matchings in the complete graph</title><categories>math.CO cs.DM</categories><comments>9 pages, 4 figures</comments><msc-class>05D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following higher-order analog of the Erd\H{o}s--Ko--Rado
theorem. For positive integers r and n with r&lt;= n, let M^r_n be the family of
all matchings of size r in the complete graph K_{2n}. For any edge e in
E(K_{2n}), the family M^r_n(e), which consists of all sets in M^r_n containing
e, is called the star centered at e. We prove that if r&lt;n and A is an
intersecting family of matchings in M^r_n, then |A|&lt;=|M^r_n(e)|$, where e is an
edge in E(K_{2n}). We also prove that equality holds if and only if A is a
star. The main technique we use to prove the theorem is an analog of Katona's
elegant cycle method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4078</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4078</id><created>2013-03-17</created><authors><author><keyname>Grispos</keyname><forenames>George</forenames></author><author><keyname>Glisson</keyname><forenames>William Bradley</forenames></author><author><keyname>Storer</keyname><forenames>Tim</forenames></author></authors><title>Using Smartphones as a Proxy for Forensic Evidence contained in Cloud
  Storage Services</title><categories>cs.CR</categories><comments>Paper appeared at the 46th Hawaii International Conference on System
  Sciences (HICSS-46), 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud storage services such as Dropbox, Box and SugarSync have been embraced
by both individuals and organizations. This creates an environment that is
potentially conducive to security breaches and malicious activities. The
investigation of these cloud environments presents new challenges for the
digital forensics community. It is anticipated that smartphone devices will
retain data from these storage services. Hence, this research presents a
preliminary investigation into the residual artifacts created on an iOS and
Android device that has accessed a cloud storage service. The contribution of
this paper is twofold. First, it provides an initial assessment on the extent
to which cloud storage data is stored on these client-side devices. This view
acts as a proxy for data stored in the cloud. Secondly, it provides
documentation on the artifacts that could be useful in a digital forensics
investigation of cloud services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4081</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4081</id><created>2013-03-17</created><updated>2013-03-21</updated><authors><author><keyname>Razgon</keyname><forenames>Igor</forenames></author><author><keyname>Petke</keyname><forenames>Justyna</forenames></author></authors><title>Cliquewidth and Knowledge Compilation</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the role of cliquewidth in succinct representation of
Boolean functions. Our main statement is the following: Let $Z$ be a Boolean
circuit having cliquewidth $k$. Then there is another circuit $Z^*$ computing
the same function as $Z$ having treewidth at most $18k+2$ and which has at most
$4|Z|$ gates where $|Z|$ is the number of gates of $Z$. In this sense,
cliquewidth is not more `powerful' than treewidth for the purpose of
representation of Boolean functions. We believe this is quite a surprising fact
because it contrasts the situation with graphs where an upper bound on the
treewidth implies an upper bound on the cliquewidth but not vice versa.
  We demonstrate the usefulness of the new theorem for knowledge compilation.
In particular, we show that a circuit $Z$ of cliquewidth $k$ can be compiled
into a Decomposable Negation Normal Form ({\sc dnnf}) of size
$O(9^{18k}k^2|Z|)$ and the same runtime. To the best of our knowledge, this is
the first result on efficient knowledge compilation parameterized by
cliquewidth of a Boolean circuit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4085</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4085</id><created>2013-03-17</created><authors><author><keyname>Chepuri</keyname><forenames>Sundeep Prabhakar</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author><author><keyname>van der Veen</keyname><forenames>Alle-Jan</forenames></author></authors><title>Sparsity-Exploiting Anchor Placement for Localization in Sensor Networks</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures, submitted to EUSIPCO 2013 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the anchor placement problem in localization based on one-way
ranging, in which either the sensor or the anchors send the ranging signals.
The number of anchors deployed over a geographical area is generally sparse,
and we show that the anchor placement can be formulated as the design of a
sparse selection vector. Interestingly, the case in which the anchors send the
ranging signals, results in a joint ranging energy optimization and anchor
placement problem. We make abstraction of the localization algorithm and
instead use the Cram\'er-Rao lower bound (CRB) as the performance constraint.
The anchor placement problem is formulated as an elegant convex optimization
problem which can be solved efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4087</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4087</id><created>2013-03-17</created><authors><author><keyname>Rafi</keyname><forenames>Muhammad</forenames></author><author><keyname>Shaikh</keyname><forenames>Mohammad Shahid</forenames></author></authors><title>An improved semantic similarity measure for document clustering based on
  topic maps</title><categories>cs.IR</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major computational burden, while performing document clustering, is the
calculation of similarity measure between a pair of documents. Similarity
measure is a function that assigns a real number between 0 and 1 to a pair of
documents, depending upon the degree of similarity between them. A value of
zero means that the documents are completely dissimilar whereas a value of one
indicates that the documents are practically identical. Traditionally,
vector-based models have been used for computing the document similarity. The
vector-based models represent several features present in documents. These
approaches to similarity measures, in general, cannot account for the semantics
of the document. Documents written in human languages contain contexts and the
words used to describe these contexts are generally semantically related.
Motivated by this fact, many researchers have proposed seman-tic-based
similarity measures by utilizing text annotation through external thesauruses
like WordNet (a lexical database). In this paper, we define a semantic
similarity measure based on documents represented in topic maps. Topic maps are
rapidly becoming an industrial standard for knowledge representation with a
focus for later search and extraction. The documents are transformed into a
topic map based coded knowledge and the similarity between a pair of documents
is represented as a correlation between the common patterns (sub-trees). The
experimental studies on the text mining datasets reveal that this new
similarity measure is more effective as compared to commonly used similarity
measures in text clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4092</identifier>
 <datestamp>2013-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4092</id><created>2013-03-17</created><updated>2013-05-24</updated><authors><author><keyname>Kilic</keyname><forenames>Yakup</forenames></author><author><keyname>Wymeersch</keyname><forenames>Henk</forenames></author><author><keyname>Meijerink</keyname><forenames>Arjan</forenames></author><author><keyname>Bentum</keyname><forenames>Mark J.</forenames></author><author><keyname>Scanlon</keyname><forenames>William G.</forenames></author></authors><title>Device-Free Person Detection and Ranging in UWB Networks</title><categories>cs.OH</categories><comments>submitted to IEEE Journal of Selected Topics in Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel device-free stationary person detection and ranging
method, that is applicable to ultra-wide bandwidth (UWB) networks. The method
utilizes a fixed UWB infrastructure and does not require a training database of
template waveforms. Instead, the method capitalizes on the fact that a human
presence induces small low-frequency variations that stand out against the
background signal, which is mainly affected by wideband noise. We analyze the
detection probability, and validate our findings with numerical simulations and
experiments with off-the-shelf UWB transceivers in an indoor environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4110</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4110</id><created>2013-03-17</created><authors><author><keyname>Poranne</keyname><forenames>Roi</forenames></author><author><keyname>Chen</keyname><forenames>Renjie</forenames></author><author><keyname>Gotsman</keyname><forenames>Craig</forenames></author></authors><title>On Linear Spaces of Polyhedral Meshes</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polyhedral meshes (PM) - meshes having planar faces - have enjoyed a rise in
popularity in recent years due to their importance in architectural and
industrial design. However, they are also notoriously difficult to generate and
manipulate. Previous methods start with a smooth surface and then apply
elaborate meshing schemes to create polyhedral meshes approximating the
surface. In this paper, we describe a reverse approach: given the topology of a
mesh, we explore the space of possible planar meshes with that topology.
  Our approach is based on a complete characterization of the maximal linear
spaces of polyhedral meshes contained in the curved manifold of polyhedral
meshes with a given topology. We show that these linear spaces can be described
as nullspaces of differential operators, much like harmonic functions are
nullspaces of the Laplacian operator. An analysis of this operator provides
tools for global and local design of a polyhedral mesh, which fully expose the
geometric possibilities and limitations of the given topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4114</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4114</id><created>2013-03-17</created><updated>2013-07-22</updated><authors><author><keyname>Ciucu</keyname><forenames>Florin</forenames></author><author><keyname>Poloczek</keyname><forenames>Felix</forenames></author><author><keyname>Schmitt</keyname><forenames>Jens</forenames></author></authors><title>Sharp Bounds in Stochastic Network Calculus</title><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The practicality of the stochastic network calculus (SNC) is often questioned
on grounds of potential looseness of its performance bounds. In this paper it
is uncovered that for bursty arrival processes (specifically Markov-Modulated
On-Off (MMOO)), whose amenability to \textit{per-flow} analysis is typically
proclaimed as a highlight of SNC, the bounds can unfortunately indeed be very
loose (e.g., by several orders of magnitude off). In response to this uncovered
weakness of SNC, the (Standard) per-flow bounds are herein improved by deriving
a general sample-path bound, using martingale based techniques, which
accommodates FIFO, SP, EDF, and GPS scheduling. The obtained (Martingale)
bounds gain an exponential decay factor of ${\mathcal{O}}(e^{-\alpha n})$ in
the number of flows $n$. Moreover, numerical comparisons against simulations
show that the Martingale bounds are remarkably accurate for FIFO, SP, and EDF
scheduling; for GPS scheduling, although the Martingale bounds substantially
improve the Standard bounds, they are numerically loose, demanding for
improvements in the core SNC analysis of GPS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4120</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4120</id><created>2013-03-17</created><authors><author><keyname>Peng</keyname><forenames>T.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Schmeink</keyname><forenames>A.</forenames></author></authors><title>Adaptive Randomized Distributed Space-Time Coding in Cooperative MIMO
  Relay Systems</title><categories>cs.IT math.IT</categories><comments>4 figures</comments><journal-ref>ISWCS 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An adaptive randomized distributed space-time coding (DSTC) scheme and
algorithms are proposed for two-hop cooperative MIMO networks. Linear minimum
mean square error (MMSE) receivers and an amplify-and-forward (AF) cooperation
strategy are considered. In the proposed DSTC scheme, a randomized matrix
obtained by a feedback channel is employed to transform the space-time coded
matrix at the relay node. Linear MMSE expressions are devised to compute the
parameters of the adaptive randomized matrix and the linear receive filter. A
stochastic gradient algorithm is also developed to compute the parameters of
the adaptive randomized matrix with reduced computational complexity. We also
derive the upper bound of the error probability of a cooperative MIMO system
employing the randomized space-time coding scheme first. The simulation results
show that the proposed algorithms obtain significant performance gains as
compared to existing DSTC schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4128</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4128</id><created>2013-03-17</created><updated>2013-11-11</updated><authors><author><keyname>Jaganathan</keyname><forenames>Kishore</forenames></author><author><keyname>Oymak</keyname><forenames>Samet</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Sparse Phase Retrieval: Convex Algorithms and Limitations</title><categories>cs.IT math.IT math.OC</categories><comments>6 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of recovering signals from their power spectral
density. This is a classical problem referred to in literature as the phase
retrieval problem, and is of paramount importance in many fields of applied
sciences. In general, additional prior information about the signal is required
to guarantee unique recovery as the mapping from signals to power spectral
density is not one-to-one. In this paper, we assume that the underlying signals
are sparse. Recently, semidefinite programming (SDP) based approaches were
explored by various researchers. Simulations of these algorithms strongly
suggest that signals upto $o(\sqrt{n})$ sparsity can be recovered by this
technique. In this work, we develop a tractable algorithm based on reweighted
$l_1$-minimization that recovers a sparse signal from its power spectral
density for significantly higher sparsities, which is unprecedented.
  We discuss the square-root bottleneck of the existing convex algorithms and
show that a $k$-sparse signal can be efficiently recovered using $O(k^2logn)$
phaseless Fourier measurements. We also show that a $k$-sparse signal can be
recovered using only $O(k log n)$ phaseless measurements if we are allowed to
design the measurement matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4153</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4153</id><created>2013-03-18</created><updated>2013-06-25</updated><authors><author><keyname>Li</keyname><forenames>Xiao</forenames></author><author><keyname>Scaglione</keyname><forenames>Anna</forenames></author></authors><title>Robust Decentralized State Estimation and Tracking for Power Systems via
  Network Gossiping</title><categories>cs.DC math.OC</categories><comments>to appear in IEEE JSAC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a fully decentralized adaptive re-weighted state
estimation (DARSE) scheme for power systems via network gossiping. The enabling
technique is the proposed Gossip-based Gauss-Newton (GGN) algorithm, which
allows to harness the computation capability of each area (i.e. a database
server that accrues data from local sensors) to collaboratively solve for an
accurate global state. The DARSE scheme mitigates the influence of bad data by
updating their error variances online and re-weighting their contributions
adaptively for state estimation. Thus, the global state can be estimated and
tracked robustly using near-neighbor communications in each area. Compared to
other distributed state estimation techniques, our communication model is
flexible with respect to reconfigurations and resilient to random failures as
long as the communication network is connected. Furthermore, we prove that the
Jacobian of the power flow equations satisfies the Lipschitz condition that is
essential for the GGN algorithm to converge to the desired solution.
Simulations of the IEEE-118 system show that the DARSE scheme can estimate and
track online the global power system state accurately, and degrades gracefully
when there are random failures and bad data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4155</identifier>
 <datestamp>2013-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4155</id><created>2013-03-18</created><updated>2013-06-20</updated><authors><author><keyname>Norcie</keyname><forenames>Gregory</forenames></author><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author><author><keyname>Bellotti</keyname><forenames>Victoria</forenames></author></authors><title>Bootstrapping Trust in Online Dating: Social Verification of Online
  Dating Profiles</title><categories>cs.CR cs.CY cs.SI</categories><comments>In Proceedings of Financial Cryptography and Data Security (FC)
  Workshop on Usable Security (USEC), 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online dating is an increasingly thriving business which boasts
billion-dollar revenues and attracts users in the tens of millions.
Notwithstanding its popularity, online dating is not impervious to worrisome
trust and privacy concerns raised by the disclosure of potentially sensitive
data as well as the exposure to self-reported (and thus potentially
misrepresented) information. Nonetheless, little research has, thus far,
focused on how to enhance privacy and trustworthiness. In this paper, we report
on a series of semi-structured interviews involving 20 participants, and show
that users are significantly concerned with the veracity of online dating
profiles. To address some of these concerns, we present the user-centered
design of an interface, called Certifeye, which aims to bootstrap trust in
online dating profiles using existing social network data. Certifeye verifies
that the information users report on their online dating profile (e.g., age,
relationship status, and/or photos) matches that displayed on their own
Facebook profile. Finally, we present the results of a 161-user Mechanical Turk
study assessing whether our veracity-enhancing interface successfully reduced
concerns in online dating users and find a statistically significant trust
increase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4160</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4160</id><created>2013-03-18</created><authors><author><keyname>Reddy</keyname><forenames>Vikas</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>Improved Foreground Detection via Block-based Classifier Cascade with
  Probabilistic Decision Integration</title><categories>cs.CV</categories><acm-class>I.4.6; I.4.8; G.3; I.5.1; I.5.4</acm-class><journal-ref>IEEE Transactions on Circuits and Systems for Video Technology,
  Vol. 23, No. 1, pp. 83-93, 2013</journal-ref><doi>10.1109/TCSVT.2012.2203199</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background subtraction is a fundamental low-level processing task in numerous
computer vision applications. The vast majority of algorithms process images on
a pixel-by-pixel basis, where an independent decision is made for each pixel. A
general limitation of such processing is that rich contextual information is
not taken into account. We propose a block-based method capable of dealing with
noise, illumination variations and dynamic backgrounds, while still obtaining
smooth contours of foreground objects. Specifically, image sequences are
analysed on an overlapping block-by-block basis. A low-dimensional texture
descriptor obtained from each block is passed through an adaptive classifier
cascade, where each stage handles a distinct problem. A probabilistic
foreground mask generation approach then exploits block overlaps to integrate
interim block-level decisions into final pixel-level foreground segmentation.
Unlike many pixel-based methods, ad-hoc post-processing of foreground masks is
not required. Experiments on the difficult Wallflower and I2R datasets show
that the proposed approach obtains on average better results (both
qualitatively and quantitatively) than several prominent methods. We
furthermore propose the use of tracking performance as an unbiased approach for
assessing the practical usefulness of foreground segmentation methods, and show
that the proposed approach leads to considerable improvements in tracking
accuracy on the CAVIAR dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4164</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4164</id><created>2013-03-18</created><authors><author><keyname>Evans</keyname><forenames>Garrett N.</forenames></author><author><keyname>Collins</keyname><forenames>John C.</forenames></author></authors><title>Neurally Implementable Semantic Networks</title><categories>q-bio.NC cs.NE</categories><comments>32 pages, 12 figures</comments><acm-class>I.2.4; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose general principles for semantic networks allowing them to be
implemented as dynamical neural networks. Major features of our scheme include:
(a) the interpretation that each node in a network stands for a bound
integration of the meanings of all nodes and external events the node links
with; (b) the systematic use of nodes that stand for categories or types, with
separate nodes for instances of these types; (c) an implementation of
relationships that does not use intrinsically typed links between nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4169</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4169</id><created>2013-03-18</created><authors><author><keyname>Noma</keyname><forenames>Yui</forenames></author><author><keyname>Konoshima</keyname><forenames>Makiko</forenames></author></authors><title>Markov Chain Monte Carlo for Arrangement of Hyperplanes in
  Locality-Sensitive Hashing</title><categories>cs.LG</categories><comments>13 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since Hamming distances can be calculated by bitwise computations, they can
be calculated with less computational load than L2 distances. Similarity
searches can therefore be performed faster in Hamming distance space. The
elements of Hamming distance space are bit strings. On the other hand, the
arrangement of hyperplanes induce the transformation from the feature vectors
into feature bit strings. This transformation method is a type of
locality-sensitive hashing that has been attracting attention as a way of
performing approximate similarity searches at high speed. Supervised learning
of hyperplane arrangements allows us to obtain a method that transforms them
into feature bit strings reflecting the information of labels applied to
higher-dimensional feature vectors. In this p aper, we propose a supervised
learning method for hyperplane arrangements in feature space that uses a Markov
chain Monte Carlo (MCMC) method. We consider the probability density functions
used during learning, and evaluate their performance. We also consider the
sampling method for learning data pairs needed in learning, and we evaluate its
performance. We confirm that the accuracy of this learning method when using a
suitable probability density function and sampling method is greater than the
accuracy of existing learning methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4172</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4172</id><created>2013-03-18</created><authors><author><keyname>Telgarsky</keyname><forenames>Matus</forenames></author></authors><title>Margins, Shrinkage, and Boosting</title><categories>cs.LG stat.ML</categories><comments>To appear, ICML 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manuscript shows that AdaBoost and its immediate variants can produce
approximate maximum margin classifiers simply by scaling step size choices with
a fixed small constant. In this way, when the unscaled step size is an optimal
choice, these results provide guarantees for Friedman's empirically successful
&quot;shrinkage&quot; procedure for gradient boosting (Friedman, 2000). Guarantees are
also provided for a variety of other step sizes, affirming the intuition that
increasingly regularized line searches provide improved margin guarantees. The
results hold for the exponential loss and similar losses, most notably the
logistic loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4175</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4175</id><created>2013-03-18</created><authors><author><keyname>Tobenkin</keyname><forenames>Mark M.</forenames></author><author><keyname>Manchester</keyname><forenames>Ian R.</forenames></author><author><keyname>Megretski</keyname><forenames>Alexandre</forenames></author></authors><title>Stable Nonlinear Identification From Noisy Repeated Experiments via
  Convex Optimization</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces new techniques for using convex optimization to fit
input-output data to a class of stable nonlinear dynamical models. We present
an algorithm that guarantees consistent estimates of models in this class when
a small set of repeated experiments with suitably independent measurement noise
is available. Stability of the estimated models is guaranteed without any
assumptions on the input-output data. We first present a convex optimization
scheme for identifying stable state-space models from empirical moments. Next,
we provide a method for using repeated experiments to remove the effect of
noise on these moment and model estimates. The technique is demonstrated on a
simple simulated example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4177</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4177</id><created>2013-03-18</created><authors><author><keyname>Sergeev</keyname><forenames>Igor S.</forenames></author></authors><title>A relation between additive and multiplicative complexity of Boolean
  functions</title><categories>cs.DS</categories><comments>4 pages, in English; 4 pages, in Russian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present note we prove an asymptotically tight relation between
additive and multiplicative complexity of Boolean functions with respect to
implementation by circuits over the basis {+,*,1}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4183</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4183</id><created>2013-03-18</created><authors><author><keyname>Swierczewski</keyname><forenames>Lukasz</forenames></author></authors><title>Generating extrema approximation of analytically incomputable functions
  through usage of parallel computer aided genetic algorithms</title><categories>cs.AI</categories><comments>16 pages, 13 figures</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  This paper presents capabilities of using genetic algorithms to find
approximations of function extrema, which cannot be found using analytic ways.
To enhance effectiveness of calculations, algorithm has been parallelized using
OpenMP library. We gained much increase in speed on platforms using
multithreaded processors with shared memory free access. During analysis we
used different modifications of genetic operator, using them we obtained varied
evolution process of potential solutions. Results allow to choose best methods
among many applied in genetic algorithms and observation of acceleration on
Yorkfield, Bloomfield, Westmere-EX and most recent Sandy Bridge cores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4191</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4191</id><created>2013-03-18</created><updated>2013-07-06</updated><authors><author><keyname>Jain</keyname><forenames>Surabhi</forenames></author><author><keyname>Sadagopan</keyname><forenames>N.</forenames></author></authors><title>Parallel Search with Extended Fibonacci Primitive</title><categories>cs.DC</categories><comments>7 pages, 5 figures This paper has been withdrawn by the author due to
  an error in the simulation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search pattern experienced by the processor to search an element in secondary
storage devices follows a random sequence. Formally, it is a random walk and
its modeling is crucial in studying performance metrics like memory access
time. In this paper, we first model the random walk using extended Fibonacci
series. Our simulation is done on a parallel computing model (PRAM) with EREW
strategy. Three search primitives are proposed under parallel computing model
and each primitive is thoroughly tested on an array of size $10^7$ with the
size of random walk being $10^4$. Our findings reveal that search primitive
with pointer jumping is better than the other two primitives. Our key
contribution lies in modeling random walk as an extended Fibonacci series
generator and simulating the same with various search primitives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4193</identifier>
 <datestamp>2013-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4193</id><created>2013-03-18</created><updated>2013-05-23</updated><authors><author><keyname>Lange</keyname><forenames>Christoph</forenames></author><author><keyname>Caminati</keyname><forenames>Marco B.</forenames></author><author><keyname>Kerber</keyname><forenames>Manfred</forenames></author><author><keyname>Mossakowski</keyname><forenames>Till</forenames></author><author><keyname>Rowat</keyname><forenames>Colin</forenames></author><author><keyname>Wenzel</keyname><forenames>Makarius</forenames></author><author><keyname>Windsteiger</keyname><forenames>Wolfgang</forenames></author></authors><title>A Qualitative Comparison of the Suitability of Four Theorem Provers for
  Basic Auction Theory</title><categories>cs.LO cs.GT cs.MS</categories><comments>Conference on Intelligent Computer Mathematics, 8-12 July, Bath, UK.
  Published as number 7961 in Lecture Notes in Artificial Intelligence,
  Springer</comments><msc-class>68T15, 03B35, 68T35, 91B26, 03B70, 03B10, 03B15</msc-class><acm-class>I.2.3; I.2.4; F.4.1; H.1.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Novel auction schemes are constantly being designed. Their design has
significant consequences for the allocation of goods and the revenues
generated. But how to tell whether a new design has the desired properties,
such as efficiency, i.e. allocating goods to those bidders who value them most?
We say: by formal, machine-checked proofs. We investigated the suitability of
the Isabelle, Theorema, Mizar, and Hets/CASL/TPTP theorem provers for
reproducing a key result of auction theory: Vickrey's 1961 theorem on the
properties of second-price auctions. Based on our formalisation experience,
taking an auction designer's perspective, we give recommendations on what
system to use for formalising auctions, and outline further steps towards a
complete auction theory toolbox.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4194</identifier>
 <datestamp>2013-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4194</id><created>2013-03-18</created><updated>2013-05-18</updated><authors><author><keyname>Lange</keyname><forenames>Christoph</forenames></author><author><keyname>Rowat</keyname><forenames>Colin</forenames></author><author><keyname>Kerber</keyname><forenames>Manfred</forenames></author></authors><title>The ForMaRE Project - Formal Mathematical Reasoning in Economics</title><categories>cs.CE cs.LO</categories><comments>Conference on Intelligent Computer Mathematics, 8--12 July, Bath, UK.
  Published as number 7961 in Lecture Notes in Artificial Intelligence,
  Springer</comments><msc-class>91B26, 68T15</msc-class><acm-class>J.4; I.2.3; K.6.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ForMaRE project applies formal mathematical reasoning to economics. We
seek to increase confidence in economics' theoretical results, to aid in
discovering new results, and to foster interest in formal methods, i.e.
computer-aided reasoning, within economics. To formal methods, we seek to
contribute user experience feedback from new audiences, as well as new
challenge problems. In the first project year, we continued earlier game theory
studies but then focused on auctions, where we are building a toolbox of
formalisations, and have started to study matching and financial risk.
  In parallel to conducting research that connects economics and formal
methods, we organise events and provide infrastructure to connect both
communities, from fostering mutual awareness to targeted matchmaking. These
efforts extend beyond economics, towards generally enabling domain experts to
use mechanised reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4199</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4199</id><created>2013-03-18</created><authors><author><keyname>Hanawal</keyname><forenames>Manjesh Kumar</forenames></author><author><keyname>Altman</keyname><forenames>Eitan</forenames></author></authors><title>Network Non-Neutrality through Preferential Signaling</title><categories>cs.GT</categories><comments>10 pages, 4 figures, Accepted at WiOpt 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the central issues in the debate on network neutrality has been
whether one should allow or prevent preferential treatment by an internet
service provider (ISP) of traffic according to its origin. This raised the
question of whether to allow an ISP to have exclusive agreement with a content
provider (CP). In this paper we consider discrimination in the opposite
direction. We study the impact that a CP can have on the benefits of several
competing ISPs by sharing private information concerning the demand for its
content. More precisely, we consider ISPs that compete over access to one
common CP. Each ISP selects the price that it charges its subscribers for
accessing the content. The CP is assumed to have private information about
demand for its content, and in particular, about the inverse demand function
corresponding to the content. The competing ISPs are assumed to have knowledge
on only the statistical distribution of these functions. We derive in this
paper models for studying the impact that the CP can have on the utilities of
the ISPs by favoring one of them by exclusively revealing its private
information. We also consider the case where CP can charge ISPs for providing
such information. We propose two mechanisms based on {\em weighted proportional
fairness} for payment between ISPs and CP. Finally, we compare the social
utility resulting from these mechanisms with the optimal social utility by
introducing a performance metric termed as {\em price of partial bargaining}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4207</identifier>
 <datestamp>2013-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4207</id><created>2013-03-18</created><updated>2013-10-01</updated><authors><author><keyname>Wang</keyname><forenames>Shusen</forenames></author><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author></authors><title>Improving CUR Matrix Decomposition and the Nystr\&quot;{o}m Approximation via
  Adaptive Sampling</title><categories>cs.LG cs.NA</categories><journal-ref>Journal of Machine Learning Research, 14: 2549-2589, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The CUR matrix decomposition and the Nystr\&quot;{o}m approximation are two
important low-rank matrix approximation techniques. The Nystr\&quot;{o}m method
approximates a symmetric positive semidefinite matrix in terms of a small
number of its columns, while CUR approximates an arbitrary data matrix by a
small number of its columns and rows. Thus, CUR decomposition can be regarded
as an extension of the Nystr\&quot;{o}m approximation.
  In this paper we establish a more general error bound for the adaptive
column/row sampling algorithm, based on which we propose more accurate CUR and
Nystr\&quot;{o}m algorithms with expected relative-error bounds. The proposed CUR
and Nystr\&quot;{o}m algorithms also have low time complexity and can avoid
maintaining the whole data matrix in RAM. In addition, we give theoretical
analysis for the lower error bounds of the standard Nystr\&quot;{o}m method and the
ensemble Nystr\&quot;{o}m method. The main theoretical results established in this
paper are novel, and our analysis makes no special assumption on the data
matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4211</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4211</id><created>2013-03-18</created><updated>2013-03-30</updated><authors><author><keyname>Venkatesan</keyname><forenames>R. C.</forenames></author><author><keyname>Plastino</keyname><forenames>A.</forenames></author></authors><title>Invertible mappings and the large deviation theory for the $q$-maximum
  entropy principle</title><categories>cond-mat.stat-mech cs.IT math-ph math.IT math.MP</categories><comments>9 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:1303.0444. Typographical errors corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The possibility of reconciliation between canonical probability distributions
obtained from the $q$-maximum entropy principle with predictions from the law
of large numbers when empirical samples are held to the same constraints, is
investigated into. Canonical probability distributions are constrained by both:
$(i)$ the additive duality of generalized statistics and $(ii)$ normal averages
expectations. Necessary conditions to establish such a reconciliation are
derived by appealing to a result concerning large deviation properties of
conditional measures. The (dual) $q^*$-maximum entropy principle is shown {\bf
not} to adhere to the large deviation theory. However, the necessary conditions
are proven to constitute an invertible mapping between: $(i)$ a canonical
ensemble satisfying the $q^*$-maximum entropy principle for energy-eigenvalues
$\varepsilon_i^*$, and, $(ii)$ a canonical ensemble satisfying the
Shannon-Jaynes maximum entropy theory for energy-eigenvalues $\varepsilon_i$.
Such an invertible mapping is demonstrated to facilitate an \emph{implicit}
reconciliation between the $q^*$-maximum entropy principle and the large
deviation theory. Numerical examples for exemplary cases are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4224</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4224</id><created>2013-03-18</created><authors><author><keyname>Farchane</keyname><forenames>Abderrazzak</forenames></author><author><keyname>Belkasmi</keyname><forenames>Mostafa</forenames></author><author><keyname>Nouh</keyname><forenames>Said</forenames></author></authors><title>Generalized parallel concatenated block codes based on BCH and RS codes,
  construction and Iterative decoding</title><categories>cs.IT math.IT</categories><journal-ref>Journal Of Telecommunications, Volume 12, Issue 1, January 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a generalization of parallel concatenated block GPCB codes
based on BCH and RS codes is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4227</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4227</id><created>2013-03-18</created><authors><author><keyname>Nouh</keyname><forenames>Said</forenames></author><author><keyname>Belkasmi</keyname><forenames>Mostafa</forenames></author></authors><title>Genetic algorithms for finding the weight enumerator of binary linear
  block codes</title><categories>cs.IT cs.NE math.IT</categories><journal-ref>International Journal of Applied Research on Information
  Technology and Computing (IJARITAC), 80-93, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new method for finding the weight enumerator of
binary linear block codes by using genetic algorithms. This method consists in
finding the binary weight enumerator of the code and its dual and to create
from the famous MacWilliams identity a linear system (S) of integer variables
for which we add all known information obtained from the structure of the code.
The knowledge of some subgroups of the automorphism group, under which the code
remains invariant, permits to give powerful restrictions on the solutions of
(S) and to approximate the weight enumerator. By applying this method and by
using the stability of the Extended Quadratic Residue codes (ERQ) by the
Projective Special Linear group PSL2, we find a list of all possible values of
the weight enumerators for the two ERQ codes of lengths 192 and 200. We also
made a good approximation of the true value for these two enumerators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4231</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4231</id><created>2013-03-18</created><authors><author><keyname>Portillo</keyname><forenames>Ignacio Gomez</forenames></author></authors><title>Towards a model of the human society: A theoretical solution of the
  cooperation problem</title><categories>cs.GT nlin.AO physics.soc-ph</categories><comments>28 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the cooperation problem in structured populations by
considering the prisoner's dilemma game as metaphor of the social interactions
between individuals with imitation capacity. We present a new strategy update
rule called democratic weighted update where the individuals behavior is
socially influenced by each one of their neighbors. In particular, the capacity
of an individual to socially influence other ones is proportional to its wisdom
which is defined by its successful in the game. When in a neighborhood there
are cooperators and defectors, the focal player is contradictorily influenced
by them and, therefore, the effective social influence is given by the
difference of the total wisdom of each strategy in its neighborhood. First, by
considering the growing process of the network and neglecting mutations we show
the evolution of highly cooperative systems. Then, we broadly shown that the
social influence allows to overcome the emergence of mutants into highly
cooperative systems. In this way, we are able to conclude that considering the
growing process of the system, individuals with imitation capacity and the
social influence the cooperation evolves. Therefore, here we present a
theoretical solution of the cooperation problem among unrelated individuals
with imitiation capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4244</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4244</id><created>2013-03-18</created><updated>2013-04-15</updated><authors><author><keyname>Fuchs</keyname><forenames>Michael</forenames></author><author><keyname>Hwang</keyname><forenames>Hsien-Kuei</forenames></author><author><keyname>Zacharovas</keyname><forenames>Vytas</forenames></author></authors><title>An analytic approach to the asymptotic variance of trie statistics and
  related structures</title><categories>math.CO cs.DS</categories><comments>51 pages, the expressions of all Fourier coefficients are largely
  simplified in this version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop analytic tools for the asymptotics of general trie statistics,
which are particularly advantageous for clarifying the asymptotic variance.
Many concrete examples are discussed for which new Fourier expansions are
given. The tools are also useful for other splitting processes with an
underlying binomial distribution. We specially highlight Philippe Flajolet's
contribution in the analysis of these random structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4247</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4247</id><created>2013-03-18</created><authors><author><keyname>Pluchino</keyname><forenames>A.</forenames></author><author><keyname>Rapisarda</keyname><forenames>A.</forenames></author><author><keyname>Garofalo</keyname><forenames>C.</forenames></author><author><keyname>Spagano</keyname><forenames>S.</forenames></author><author><keyname>Caserta</keyname><forenames>M.</forenames></author></authors><title>On the efficiency of the new Italian Senate and the role of 5 Stars
  Movement: Comparison among different possible scenarios by means of a virtual
  Parliament model</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent 2013 Italian elections are over and the situation that President
Napolitano will have to settle soon for the formation of the new government is
not the simplest one. After twenty years of bipolarism (more or less
effective), where we were accustomed to a tight battle between two great
political coalitions, the center-right and center-left, now, in the new
Parliament, we have four political formations. But is it really this result, as
it would seem to suggest our common sense, the prelude to an inevitable phase
of ungovernability? Can a Parliament with changing majorities in Senate to be
as efficient as a Parliament with a large majority in both the Houses? In this
short note we will try to answer these questions going beyond common sense and
analyzing the current political situation by means of a scientific, original
and innovative instrument, i.e. an &quot;agent-based simulation&quot;. We show that the
situation is not so dramatic as it sounds, but it contains within itself
potential positive aspects, as long as one makes the most appropriate choices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4257</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4257</id><created>2013-03-18</created><authors><author><keyname>Dunchev</keyname><forenames>Cvetan</forenames></author><author><keyname>Leitsch</keyname><forenames>Alexander</forenames></author><author><keyname>Rukhaia</keyname><forenames>Mikheil</forenames></author><author><keyname>Weller</keyname><forenames>Daniel</forenames></author></authors><title>CERES for First-Order Schemata</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cut-elimination method CERES (for first- and higher-order classical
logic) is based on the notion of a characteristic clause set, which is
extracted from an LK-proof and is always unsatisfiable. A resolution refutation
of this clause set can be used as a skeleton for a proof with atomic cuts only
(atomic cut normal form). This is achieved by replacing clauses from the
resolution refutation by the corresponding projections of the original proof.
  We present a generalization of CERES (called CERESs) to first-order proof
schemata and define a schematic version of the sequent calculus called LKS, and
a notion of proof schema based on primitive recursive definitions. A method is
developed to extract schematic characteristic clause sets and schematic
projections from these proof schemata. We also define a schematic resolution
calculus for refutation of schemata of clause sets, which can be applied to
refute the schematic characteristic clause sets. Finally the projection
schemata and resolution schemata are plugged together and a schematic
representation of the atomic cut normal forms is obtained. A major benefit of
CERESs is the extension of cut-elimination to inductively defined proofs: we
compare CERESs with standard calculi using induction rules and demonstrate that
CERESs is capable of performing cut-elimination where traditional methods fail.
The algorithmic handling of CERESs is supported by a recent extension of the
CERES system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4262</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4262</id><created>2013-03-18</created><authors><author><keyname>Alderman</keyname><forenames>James</forenames></author><author><keyname>Crampton</keyname><forenames>Jason</forenames></author></authors><title>On the Use of Key Assignment Schemes in Authentication Protocols</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Key Assignment Schemes (KASs) have been extensively studied in the context of
cryptographically-enforced access control, where derived keys are used to
decrypt protected resources. In this paper, we explore the use of KASs in
entity authentication protocols, where we use derived keys to encrypt
challenges. This novel use of KASs permits the efficient authentication of an
entity in accordance with an authentication policy by associating entities with
security labels representing specific services. Cryptographic keys are
associated with each security label and demonstrating knowledge of an
appropriate key is used as the basis for authentication. Thus, by controlling
the distribution of such keys, restrictions may be efficiently placed upon the
circumstances under which an entity may be authenticated and the services to
which they may gain access.
  In this work, we explore how both standardized protocols and novel
constructions may be developed to authenticate entities as members of a group
associated to a particular security label, whilst protecting the long-term
secrets in the system. We also see that such constructions may allow for
authentication whilst preserving anonymity, and that by including a trusted
third party we can achieve the authentication of individual identities and
authentication based on timestamps without the need for synchronized clocks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4264</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4264</id><created>2013-03-18</created><authors><author><keyname>Kopiczko</keyname><forenames>Pawel</forenames></author><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Szczypiorski</keyname><forenames>Krzysztof</forenames></author></authors><title>StegTorrent: a Steganographic Method for the P2P File Sharing Service</title><categories>cs.MM cs.CR</categories><comments>7 pages, 7 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper proposes StegTorrent a new network steganographic method for the
popular P2P file transfer service-BitTorrent. It is based on modifying the
order of data packets in the peer-peer data exchange protocol. Unlike other
existing steganographic methods that modify the packets' order it does not
require any synchronization. Experimental results acquired from prototype
implementation proved that it provides high steganographic bandwidth of up to
270 b/s while introducing little transmission distortion and providing
difficult detectability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4266</identifier>
 <datestamp>2013-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4266</id><created>2013-03-18</created><updated>2013-09-16</updated><authors><author><keyname>Vehkapera</keyname><forenames>Mikko</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author><author><keyname>Chatterjee</keyname><forenames>Saikat</forenames></author></authors><title>Statistical Mechanics Approach to Sparse Noise Denoising</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures; Special session: &quot;Trends in Sparse Signal
  Processing: Theory and Algorithm Design&quot;, Eusipco 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconstruction fidelity of sparse signals contaminated by sparse noise is
considered. Statistical mechanics inspired tools are used to show that the
l1-norm based convex optimization algorithm exhibits a phase transition between
the possibility of perfect and imperfect reconstruction. Conditions
characterizing this threshold are derived and the mean square error of the
estimate is obtained for the case when perfect reconstruction is not possible.
Detailed calculations are provided to expose the mathematical tools to a wide
audience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4277</identifier>
 <datestamp>2013-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4277</id><created>2013-03-18</created><updated>2013-06-20</updated><authors><author><keyname>Boneva</keyname><forenames>Iovka</forenames></author><author><keyname>Ciucanu</keyname><forenames>Radu</forenames></author><author><keyname>Staworko</keyname><forenames>Slawek</forenames></author></authors><title>Simple Schemas for Unordered XML</title><categories>cs.DB</categories><comments>16th International Workshop on the Web and Databases (WebDB 2013)
  http://webdb2013.lille.inria.fr/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider unordered XML, where the relative order among siblings is
ignored, and propose two simple yet practical schema formalisms: disjunctive
multiplicity schemas (DMS), and its restriction, disjunction-free multiplicity
schemas (MS). We investigate their computational properties and characterize
the complexity of the following static analysis problems: schema
satisfiability, membership of a tree to the language of a schema, schema
containment, twig query satisfiability, implication, and containment in the
presence of schema. Our research indicates that the proposed formalisms retain
much of the expressiveness of DTDs without an increase in computational
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4289</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4289</id><created>2013-03-18</created><authors><author><keyname>Katselis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Rojas</keyname><forenames>Cristian R.</forenames></author><author><keyname>Hjalmarsson</keyname><forenames>H&#xe5;kan</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author></authors><title>On the Design of Channel Estimators for given Signal Estimators and
  Detectors</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fundamental task of a digital receiver is to decide the transmitted
symbols in the best possible way, i.e., with respect to an appropriately
defined performance metric. Examples of usual performance metrics are the
probability of error and the Mean Square Error (MSE) of a symbol estimator. In
a coherent receiver, the symbol decisions are made based on the use of a
channel estimate. This paper focuses on examining the optimality of usual
estimators such as the minimum variance unbiased (MVU) and the minimum mean
square error (MMSE) estimators for these metrics and on proposing better
estimators whenever it is necessary. For illustration purposes, this study is
performed on a toy channel model, namely a single input single output (SISO)
flat fading channel with additive white Gaussian noise (AWGN). In this way,
this paper highlights the design dependencies of channel estimators on target
performance metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4293</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4293</id><created>2013-03-11</created><authors><author><keyname>Kaljurand</keyname><forenames>Kaarel</forenames></author><author><keyname>Kuhn</keyname><forenames>Tobias</forenames></author></authors><title>A Multilingual Semantic Wiki Based on Attempto Controlled English and
  Grammatical Framework</title><categories>cs.CL cs.HC</categories><comments>To appear in the Proceedings of the 10th Extended Semantic Web
  Conference (ESWC 2013)</comments><report-no>LNCS 7882</report-no><doi>10.1007/978-3-642-38288-8_29</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a semantic wiki system with an underlying controlled natural
language grammar implemented in Grammatical Framework (GF). The grammar
restricts the wiki content to a well-defined subset of Attempto Controlled
English (ACE), and facilitates a precise bidirectional automatic translation
between ACE and language fragments of a number of other natural languages,
making the wiki content accessible multilingually. Additionally, our approach
allows for automatic translation into the Web Ontology Language (OWL), which
enables automatic reasoning over the wiki content. The developed wiki
environment thus allows users to build, query and view OWL knowledge bases via
a user-friendly multilingual natural language interface. As a further feature,
the underlying multilingual grammar is integrated into the wiki and can be
collaboratively edited to extend the vocabulary of the wiki or even customize
its sentence structures. This work demonstrates the combination of the existing
technologies of Attempto Controlled English and Grammatical Framework, and is
implemented as an extension of the existing semantic wiki engine AceWiki.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4296</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4296</id><created>2013-03-18</created><authors><author><keyname>Ingl&#xe9;s-Romero</keyname><forenames>Juan Francisco</forenames></author><author><keyname>Lotz</keyname><forenames>Alex</forenames></author><author><keyname>Chicote</keyname><forenames>Cristina Vicente</forenames></author><author><keyname>Schlegel</keyname><forenames>Christian</forenames></author></authors><title>Dealing with Run-Time Variability in Service Robotics: Towards a DSL for
  Non-Functional Properties</title><categories>cs.RO</categories><comments>Presented at DSLRob 2012 (arXiv:cs/1302.5082)</comments><report-no>DSLRob/2012/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service robots act in open-ended, natural environments. Therefore, due to
combinatorial explosion of potential situations, it is not possible to foresee
all eventualities in advance during robot design. In addition, due to limited
resources on a mobile robot, it is not feasible to plan any action on demand.
Hence, it is necessary to provide a mechanism to express variability at
design-time that can be efficiently resolved on the robot at run-time based on
the then available information. In this paper, we introduce a DSL to express
run- time variability focused on the execution quality of the robot (in terms
of non-functional properties like safety and task efficiency) under changing
situations and limited resources. We underpin the applicability of our approach
by an example integrated into an overall robotics architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4312</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4312</id><created>2013-03-18</created><updated>2013-11-12</updated><authors><author><keyname>Siebert</keyname><forenames>Christian</forenames></author><author><keyname>Tr&#xe4;ff</keyname><forenames>Jesper Larsson</forenames></author></authors><title>Perfectly load-balanced, optimal, stable, parallel merge</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple, work-optimal and synchronization-free solution to the
problem of stably merging in parallel two given, ordered arrays of m and n
elements into an ordered array of m+n elements. The main contribution is a new,
simple, fast and direct algorithm that determines, for any prefix of the stably
merged output sequence, the exact prefixes of each of the two input sequences
needed to produce this output prefix. More precisely, for any given index
(rank) in the resulting, but not yet constructed output array representing an
output prefix, the algorithm computes the indices (co-ranks) in each of the two
input arrays representing the required input prefixes without having to merge
the input arrays. The co-ranking algorithm takes O(log min(m,n)) time steps.
The algorithm is used to devise a perfectly load-balanced, stable, parallel
merge algorithm where each of p processing elements has exactly the same number
of input elements to merge. Compared to other approaches to the parallel merge
problem, our algorithm is considerably simpler and can be faster up to a factor
of two. Compared to previous algorithms for solving the co-ranking problem, the
algorithm given here is direct and maintains stability in the presence of
repeated elements at no extra space or time cost. When the number of processing
elements p does not exceed (m+n)/log min(m,n), the parallel merge algorithm has
optimal speedup. It is easy to implement on both shared and distributed memory
parallel systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4315</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4315</id><created>2013-03-18</created><authors><author><keyname>Sood</keyname><forenames>Gaurav</forenames></author><author><keyname>Krishnan</keyname><forenames>K. Murali</forenames></author></authors><title>On the computational complexity of Data Flow Analysis</title><categories>cs.CC</categories><comments>7 pages 4 figures</comments><msc-class>68Q17</msc-class><acm-class>F.1.3; D.3.4; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of Data Flow Analysis over monotone data flow
frameworks with a finite lattice. The problem of computing the Maximum Fixed
Point (MFP) solution is shown to be P-complete even when the lattice has just
four elements. This shows that the problem is unlikely to be efficiently
parallelizable. It is also shown that the problem of computing the Meet Over
all Paths (MOP) solution is NL-complete (and hence efficiently parallelizable)
when the lattice is finite even for non-monotone data flow frameworks. These
results appear in contrast with the fact that when the lattice is not finite,
solving the MOP problem is undecidable and hence significantly harder than the
MFP problem which is polynomial time computable for lattices of finite height.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4324</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4324</id><created>2013-03-18</created><updated>2013-08-26</updated><authors><author><keyname>Labouze</keyname><forenames>Xavier</forenames></author></authors><title>About Inverse 3-SAT</title><categories>cs.DS cs.LO</categories><comments>8 pages</comments><msc-class>68Q15</msc-class><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Inverse 3-SAT problem is known to be coNP Complete. This article shows a
new interesting way to solve directly the problem by using closure under
resolution and partial assignment properties. An algorithm is proposed which
lets solve the (co)Inverse 3-SAT problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4336</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4336</id><created>2013-03-18</created><authors><author><keyname>Dove</keyname><forenames>Andrew P.</forenames></author><author><keyname>Griggs</keyname><forenames>Jerrold R.</forenames></author><author><keyname>Kang</keyname><forenames>Ross J.</forenames></author><author><keyname>Sereni</keyname><forenames>Jean-S&#xe9;bastien</forenames></author></authors><title>Supersaturation in the Boolean lattice</title><categories>math.CO cs.DM</categories><journal-ref>Integers 14A (2014): #A4</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a &quot;supersaturation-type&quot; extension of both Sperner's Theorem (1928)
and its generalization by Erdos (1945) to k-chains. Our result implies that a
largest family whose size is x more than the size of a largest k-chain free
family and that contains the minimum number of k-chains is the family formed by
taking the middle (k-1) rows of the Boolean lattice and x elements from the
k-th middle row. We prove our result using the symmetric chain decomposition
method of de Bruijn, van Ebbenhorst Tengbergen, and Kruyswijk (1951).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4347</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4347</id><created>2013-03-18</created><authors><author><keyname>Jafri</keyname><forenames>Mohsin Raza</forenames></author><author><keyname>Javaid</keyname><forenames>Nadeem</forenames></author><author><keyname>Javaid</keyname><forenames>Akmal</forenames></author><author><keyname>Khan</keyname><forenames>Zahoor Ali</forenames></author></authors><title>Maximizing the Lifetime of Multi-chain PEGASIS using Sink Mobility</title><categories>cs.NI</categories><journal-ref>World Applied Sciences Journal 21 (9): 1283-1289, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose the mobility of a sink in improved energy efficient
PEGASIS-based protocol (IEEPB) to advance the network lifetime of Wireless
Sensor Networks (WSNs). The multi-head chain, multi-chain concept and the sink
mobility affects largely in enhancing the network lifetime of wireless sensors.
Thus, we recommend Mobile sink improved energy-efficient PEGASIS-based routing
protocol (MIEEPB); a multi-chain model having a sink mobility, to achieve
proficient energy utilization of wireless sensors. As the motorized movement of
mobile sink is steered by petrol or current, there is a need to confine this
movement within boundaries and the trajectory of mobile sink should be fixed.
In our technique, the mobile sink moves along its trajectory and stays for a
sojourn time at sojourn location to guarantee complete data collection. We
develop an algorithm for trajectory of mobile sink. We ultimately perform
wide-ranging experiments to assess the performance of the proposed method. The
results reveal that the proposed way out is nearly optimal and also better than
IEEPB in terms of network lifetime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4348</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4348</id><created>2013-03-18</created><updated>2013-04-01</updated><authors><author><keyname>Tang</keyname><forenames>Gongguo</forenames></author><author><keyname>Bhaskar</keyname><forenames>Badri Narayan</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author></authors><title>Near Minimax Line Spectral Estimation</title><categories>cs.IT math.IT</categories><comments>25 pages, 12 figures. Added a numerical experiments section</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper establishes a nearly optimal algorithm for estimating the
frequencies and amplitudes of a mixture of sinusoids from noisy equispaced
samples. We derive our algorithm by viewing line spectral estimation as a
sparse recovery problem with a continuous, infinite dictionary. We show how to
compute the estimator via semidefinite programming and provide guarantees on
its mean-square error rate. We derive a complementary minimax lower bound on
this estimation rate, demonstrating that our approach nearly achieves the best
possible estimation error. Furthermore, we establish bounds on how well our
estimator localizes the frequencies in the signal, showing that the
localization error tends to zero as the number of samples grows. We verify our
theoretical results in an array of numerical experiments, demonstrating that
the semidefinite programming approach outperforms two classical spectral
estimation techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4349</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4349</id><created>2013-03-18</created><updated>2014-01-16</updated><authors><author><keyname>Glantz</keyname><forenames>Roland</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author></authors><title>Finding all Convex Cuts of a Plane Graph in Polynomial Time</title><categories>cs.DS cs.DM</categories><comments>23 pages. Submitted to Journal of Discrete Algorithms (JDA)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convexity is a notion that has been defined for subsets of $\RR^n$ and for
subsets of general graphs. A convex cut of a graph $G=(V, E)$ is a
$2$-partition $V_1 \dot{\cup} V_2=V$ such that both $V_1$ and $V_2$ are convex,
\ie shortest paths between vertices in $V_i$ never leave $V_i$, $i \in \{1,
2\}$. Finding convex cuts is $\mathcal{NP}$-hard for general graphs. To
characterize convex cuts, we employ the Djokovic relation, a reflexive and
symmetric relation on the edges of a graph that is based on shortest paths
between the edges' end vertices.
  It is known for a long time that, if $G$ is bipartite and the Djokovic
relation is transitive on $G$, \ie $G$ is a partial cube, then the cut-sets of
$G$'s convex cuts are precisely the equivalence classes of the Djokovic
relation. In particular, any edge of $G$ is contained in the cut-set of exactly
one convex cut. We first characterize a class of plane graphs that we call {\em
well-arranged}. These graphs are not necessarily partial cubes, but any edge of
a well-arranged graph is contained in the cut-set(s) of at least one convex
cut. We also present an algorithm that uses the Djokovic relation for computing
all convex cuts of a (not necessarily plane) bipartite graph in $\bigO(|E|^3)$
time. Specifically, a cut-set is the cut-set of a convex cut if and only if the
Djokovic relation holds for any pair of edges in the cut-set.
  We then characterize the cut-sets of the convex cuts of a general graph $H$
using two binary relations on edges: (i) the Djokovic relation on the edges of
a subdivision of $H$, where any edge of $H$ is subdivided into exactly two
edges and (ii) a relation on the edges of $H$ itself that is not the Djokovic
relation. Finally, we use this characterization to present the first algorithm
for finding all convex cuts of a plane graph in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4352</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4352</id><created>2013-03-18</created><authors><author><keyname>Chen</keyname><forenames>Jinyuan</forenames></author><author><keyname>Elia</keyname><forenames>Petros</forenames></author></authors><title>Optimal DoF Region of the Two-User MISO-BC with General Alternating CSIT</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the setting of the time-selective two-user multiple-input single-output
(MISO) broadcast channel (BC), recent work by Tandon et al. considered the case
where - in the presence of error-free delayed channel state information at the
transmitter (delayed CSIT) - the current CSIT for the channel of user 1 and of
user 2, alternate between the two extreme states of perfect current CSIT and of
no current CSIT.
  Motivated by the problem of having limited-capacity feedback links which may
not allow for perfect CSIT, as well as by the need to utilize any available
partial CSIT, we here deviate from this `all-or-nothing' approach and proceed -
again in the presence of error-free delayed CSIT - to consider the general
setting where current CSIT now alternates between any two qualities.
Specifically for $I_1$ and $I_2$ denoting the high-SNR asymptotic
rates-of-decay of the mean-square error of the CSIT estimates for the channel
of user~1 and of user~2 respectively, we consider the case where $I_1,I_2
\in\{\gamma,\alpha\}$ for any two positive current-CSIT quality exponents
$\gamma,\alpha$. In a fast-fading setting where we consider communication over
any number of coherence periods, and where each CSIT state $I_1I_2$ is present
for a fraction $\lambda_{I_1I_2}$ of this total duration, we focus on the
symmetric case of $\lambda_{\alpha\gamma}=\lambda_{\gamma\alpha}$, and derive
the optimal degrees-of-freedom (DoF) region. The result, which is supported by
novel communication protocols, naturally incorporates the aforementioned
`Perfect current' vs. `No current' setting by limiting $I_1,I_2\in\{0,1\}$.
  Finally, motivated by recent interest in frequency correlated channels with
unmatched CSIT, we also analyze the setting where there is no delayed CSIT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4366</identifier>
 <datestamp>2013-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4366</id><created>2013-03-18</created><updated>2013-04-30</updated><authors><author><keyname>Baumgartner</keyname><forenames>Susanne</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Group-Based Trajectory Modeling of Citations in Scholarly Literature:
  Dynamic Qualities of &quot;Transient&quot; and &quot;Sticky Knowledge Claims&quot;</title><categories>cs.DL</categories><comments>Journal of the American Society for Information Science and
  Technology (2013, in press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group-based Trajectory Modeling (GBTM) is applied to the citation curves of
articles in six journals and to all citable items in a single field of science
(Virology, 24 journals), in order to distinguish among the developmental
trajectories in subpopulations. Can highly-cited citation patterns be
distinguished in an early phase as &quot;fast-breaking&quot; papers? Can &quot;late bloomers&quot;
or &quot;sleeping beauties&quot; be identified? Most interesting, we find differences
between &quot;sticky knowledge claims&quot; that continue to be cited more than ten years
after publication, and &quot;transient knowledge claims&quot; that show a decay pattern
after reaching a peak within a few years. Only papers following the trajectory
of a &quot;sticky knowledge claim&quot; can be expected to have a sustained impact. These
findings raise questions about indicators of &quot;excellence&quot; that use aggregated
citation rates after two or three years (e.g., impact factors). Because
aggregated citation curves can also be composites of the two patterns,
5th-order polynomials (with four bending points) are needed to capture citation
curves precisely. For the journals under study, the most frequently cited
groups were furthermore much smaller than ten percent. Although GBTM has proved
a useful method for investigating differences among citation trajectories, the
methodology does not enable us to define a percentage of highly-cited papers
inductively across different fields and journals. Using multinomial logistic
regression, we conclude that predictor variables such as journal names, number
of authors, etc., do not affect the stickiness of knowledge claims in terms of
citations, but only the levels of aggregated citations (that are
field-specific).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4370</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4370</id><created>2013-03-18</created><authors><author><keyname>Badr</keyname><forenames>Ahmed</forenames></author><author><keyname>Lui</keyname><forenames>Devin</forenames></author><author><keyname>Khisti</keyname><forenames>Ashish</forenames></author></authors><title>Streaming-Codes for Multicast over Burst Erasure Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the capacity limits of real-time streaming over burst-erasure
channels. A stream of source packets must be sequentially encoded and the
resulting channel packets must be transmitted over a two-receiver burst-erasure
broadcast channel. The source packets must be sequentially reconstructed at
each receiver with a possibly different reconstruction deadline. We study the
associated capacity as a function of burst-lengths and delays at the two
receivers.
  We establish that the operation of the system can be divided into two main
regimes: a low-delay regime and a large-delay regime. We fully characterize the
capacity in the large delay regime. The key to this characterization is an
inherent slackness in the delay of one of the receivers. At every point in this
regime we can reduce the delay of at-least one of the users until a certain
critical value and thus it suffices to obtain code constructions for certain
critical delays. We partially characterize the capacity in the low-delay
regime. Our capacity results involve code constructions and converse techniques
that appear to be novel. We also provide a rigorous information theoretic
converse theorem in the point-to-point setting which was studied by Martinian
in an earlier work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4375</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4375</id><created>2013-03-18</created><authors><author><keyname>Askali</keyname><forenames>Mohamed</forenames></author><author><keyname>Azouaoui</keyname><forenames>Ahmed</forenames></author><author><keyname>Nouh</keyname><forenames>Sa&#xef;d</forenames></author><author><keyname>Belkasmi</keyname><forenames>Mostafa</forenames></author></authors><title>On the Computing of the Minimum Distance of Linear Block Codes by
  Heuristic Methods</title><categories>cs.IT math.IT</categories><journal-ref>Int. J. Communications, Network and System Sciences, 2012, 5,
  774-784</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evaluation of the minimum distance of linear block codes remains an open
problem in coding theory, and it is not easy to determine its true value by
classical methods, for this reason the problem has been solved in the
literature with heuristic techniques such as genetic algorithms and local
search algorithms. In this paper we propose two approaches to attack the
hardness of this problem. The first approach is based on genetic algorithms and
it yield to good results comparing to another work based also on genetic
algorithms. The second approach is based on a new randomized algorithm which we
call Multiple Impulse Method MIM, where the principle is to search codewords
locally around the all-zero codeword perturbed by a minimum level of noise,
anticipating that the resultant nearest nonzero codewords will most likely
contain the minimum Hamming-weight codeword whose Hamming weight is equal to
the minimum distance of the linear code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4376</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4376</id><created>2013-03-18</created><authors><author><keyname>Pierrot</keyname><forenames>Adeline</forenames><affiliation>LIAFA</affiliation></author><author><keyname>Rossin</keyname><forenames>Dominique</forenames><affiliation>LIX</affiliation></author></authors><title>2-stack pushall sortable permutations</title><categories>cs.DM cs.DS</categories><comments>41 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the 60's, Knuth introduced stack-sorting and serial compositions of
stacks. In particular, one significant question arise out of the work of Knuth:
how to decide efficiently if a given permutation is sortable with 2 stacks in
series? Whether this problem is polynomial or NP-complete is still unanswered
yet. In this article we introduce 2-stack pushall permutations which form a
subclass of 2-stack sortable permutations and show that these two classes are
closely related. Moreover, we give an optimal O(n^2) algorithm to decide if a
given permutation of size n is 2-stack pushall sortable and describe all its
sortings. This result is a step to the solve the general 2-stack sorting
problem in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4384</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4384</id><created>2013-03-17</created><authors><author><keyname>Peng</keyname><forenames>T.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Schmeink</keyname><forenames>A.</forenames></author></authors><title>Adaptive Distributed Space-Time Coding in Cooperative MIMO Relaying
  Systems using Limited Feedback</title><categories>cs.IT math.IT</categories><comments>5 figures. arXiv admin note: substantial text overlap with
  arXiv:1303.4120</comments><journal-ref>VTC 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An adaptive randomized distributed space-time coding (DSTC) scheme is
proposed for two-hop cooperative MIMO networks. Linear minimum mean square
error (MMSE) receiver filters and randomized matrices subject to a power
constraint are considered with an amplify-and-forward (AF) cooperation
strategy. In the proposed DSTC scheme, a randomized matrix obtained by a
feedback channel is employed to transform the space-time coded matrix at the
relay node. The effect of the limited feedback and feedback errors are
considered. Linear MMSE expressions are devised to compute the parameters of
the adaptive randomized matrix and the linear receive filters. A stochastic
gradient algorithm is also developed with reduced computational complexity. The
simulation results show that the proposed algorithms obtain significant
performance gains as compared to existing DSTC schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4402</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4402</id><created>2013-03-18</created><authors><author><keyname>McAuley</keyname><forenames>Julian</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>From Amateurs to Connoisseurs: Modeling the Evolution of User Expertise
  through Online Reviews</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>11 pages, 7 figures</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommending products to consumers means not only understanding their tastes,
but also understanding their level of experience. For example, it would be a
mistake to recommend the iconic film Seven Samurai simply because a user enjoys
other action movies; rather, we might conclude that they will eventually enjoy
it -- once they are ready. The same is true for beers, wines, gourmet foods --
or any products where users have acquired tastes: the `best' products may not
be the most `accessible'. Thus our goal in this paper is to recommend products
that a user will enjoy now, while acknowledging that their tastes may have
changed over time, and may change again in the future. We model how tastes
change due to the very act of consuming more products -- in other words, as
users become more experienced. We develop a latent factor recommendation system
that explicitly accounts for each user's level of experience. We find that such
a model not only leads to better recommendations, but also allows us to study
the role of user experience and expertise on a novel dataset of fifteen million
beer, wine, food, and movie reviews.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4408</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4408</id><created>2013-03-18</created><updated>2013-06-25</updated><authors><author><keyname>Van der Mude</keyname><forenames>Antony</forenames></author></authors><title>Computing in the Limit</title><categories>cs.CC</categories><comments>10 pages</comments><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a class of functions termed &quot;Computable in the Limit&quot;, based on the
Machine Learning paradigm of &quot;Identification in the Limit&quot;. A function is
Computable in the Limit if it defines a property P_p of a recursively
enumerable class A of recursively enumerable data sequences S in A, such that
each data sequence S is generated by a total recursive function s that
enumerates . Let the index s represent the data sequence S. The property
P_p(s)=x is computed by a partial recursive function f_p(s,t) such that there
exists a u where f_p(s,u)=x and for all t&gt;=u, f_p(s,t)=x if it converges. Since
the index s is known, this is not an identification problem - instead it is
computing a common property of the sequences in A. We give a Normal Form
Theorem for properties that are Computable in the Limit, similar to Kleene's
Normal Form Theorem. We also give some examples of sets that are Computable in
the Limit, and derive some properties of Canonical and Complexity Bound
Enumerations of classes of total functions, and show that no full enumeration
of all indices of Turing machines TM_i that compute a given total function can
be Computable in the Limit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4411</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4411</id><created>2013-03-18</created><authors><author><keyname>Barrat</keyname><forenames>Alain</forenames></author><author><keyname>Fernandez</keyname><forenames>Bastien</forenames></author><author><keyname>Lin</keyname><forenames>Kevin K</forenames></author><author><keyname>Young</keyname><forenames>Lai-Sang</forenames></author></authors><title>Modeling temporal networks using random itineraries</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><journal-ref>Phys. Rev. Lett. 110, 158702 (2013)</journal-ref><doi>10.1103/PhysRevLett.110.158702</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a procedure to generate dynamical networks with bursty, possibly
repetitive and correlated temporal behaviors. Regarding any weighted directed
graph as being composed of the accumulation of paths between its nodes, our
construction uses random walks of variable length to produce time-extended
structures with adjustable features. The procedure is first described in a
general framework. It is then illustrated in a case study inspired by a
transportation system for which the resulting synthetic network is shown to
accurately mimic the empirical phenomenology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4431</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4431</id><created>2013-03-18</created><authors><author><keyname>Ortega</keyname><forenames>Pedro A.</forenames></author><author><keyname>Braun</keyname><forenames>Daniel A.</forenames></author></authors><title>Generalized Thompson Sampling for Sequential Decision-Making and Causal
  Inference</title><categories>cs.AI stat.ML</categories><comments>28 pages, 5 figures</comments><journal-ref>Complex Adaptive Systems Modeling 2014, 2:2</journal-ref><doi>10.1186/2194-3206-2-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, it has been shown how sampling actions from the predictive
distribution over the optimal action-sometimes called Thompson sampling-can be
applied to solve sequential adaptive control problems, when the optimal policy
is known for each possible environment. The predictive distribution can then be
constructed by a Bayesian superposition of the optimal policies weighted by
their posterior probability that is updated by Bayesian inference and causal
calculus. Here we discuss three important features of this approach. First, we
discuss in how far such Thompson sampling can be regarded as a natural
consequence of the Bayesian modeling of policy uncertainty. Second, we show how
Thompson sampling can be used to study interactions between multiple adaptive
agents, thus, opening up an avenue of game-theoretic analysis. Third, we show
how Thompson sampling can be applied to infer causal relationships when
interacting with an environment in a sequential fashion. In summary, our
results suggest that Thompson sampling might not merely be a useful heuristic,
but a principled method to address problems of adaptive sequential
decision-making and causal inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4434</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4434</id><created>2013-03-18</created><authors><author><keyname>Gong</keyname><forenames>Pinghua</forenames></author><author><keyname>Zhang</keyname><forenames>Changshui</forenames></author><author><keyname>Lu</keyname><forenames>Zhaosong</forenames></author><author><keyname>Huang</keyname><forenames>Jianhua</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author></authors><title>A General Iterative Shrinkage and Thresholding Algorithm for Non-convex
  Regularized Optimization Problems</title><categories>cs.LG cs.NA stat.CO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-convex sparsity-inducing penalties have recently received considerable
attentions in sparse learning. Recent theoretical investigations have
demonstrated their superiority over the convex counterparts in several sparse
learning settings. However, solving the non-convex optimization problems
associated with non-convex penalties remains a big challenge. A commonly used
approach is the Multi-Stage (MS) convex relaxation (or DC programming), which
relaxes the original non-convex problem to a sequence of convex problems. This
approach is usually not very practical for large-scale problems because its
computational cost is a multiple of solving a single convex problem. In this
paper, we propose a General Iterative Shrinkage and Thresholding (GIST)
algorithm to solve the nonconvex optimization problem for a large class of
non-convex penalties. The GIST algorithm iteratively solves a proximal operator
problem, which in turn has a closed-form solution for many commonly used
penalties. At each outer iteration of the algorithm, we use a line search
initialized by the Barzilai-Borwein (BB) rule that allows finding an
appropriate step size quickly. The paper also presents a detailed convergence
analysis of the GIST algorithm. The efficiency of the proposed algorithm is
demonstrated by extensive experiments on large-scale data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4438</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4438</id><created>2013-03-18</created><authors><author><keyname>Alaei</keyname><forenames>Saeed</forenames></author><author><keyname>Malekian</keyname><forenames>Azarakhsh</forenames></author><author><keyname>Srinivasan</keyname><forenames>Aravind</forenames></author></authors><title>On Random Sampling Auctions for Digital Goods</title><categories>cs.GT</categories><comments>A preliminary version appeared in the 10th ACM conference on
  Electronic Commerce, 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of auctions for digital goods, an interesting random sampling
auction has been proposed by Goldberg, Hartline, and Wright [2001]. This
auction has been analyzed by Feige, Flaxman, Hartline, and Kleinberg [2005],
who have shown that it is 15-competitive in the worst case {which is
substantially better than the previously proven constant bounds but still far
from the conjectured competitive ratio of 4. In this paper, we prove that the
aforementioned random sampling auction is indeed 4-competitive for a large
class of instances where the number of bids above (or equal to) the optimal
sale price is at least 6. We also show that it is 4:68-competitive for the
small class of remaining instances thus leaving a negligible gap between the
lower and upper bound. We employ a mix of probabilistic techniques and dynamic
programming to compute these bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4439</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4439</id><created>2013-03-18</created><authors><author><keyname>Chen</keyname><forenames>Xu</forenames></author><author><keyname>Guo</keyname><forenames>Dongning</forenames></author><author><keyname>Grosspietsch</keyname><forenames>John</forenames></author></authors><title>The Public Safety Broadband Network: A Novel Architecture with Mobile
  Base Stations</title><categories>cs.NI cs.IT math.IT</categories><comments>ICC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A nationwide interoperable public safety broadband network is being planned
by the United States government. The network will be based on long term
evolution (LTE) standards and use recently designated spectrum in the 700 MHz
band. The public safety network has different objectives and traffic patterns
than commercial wireless networks. In particular, the public safety network
puts more emphasis on coverage, reliability and latency in the worst case
scenario. Moreover, the routine public safety traffic is relatively light,
whereas when a major incident occurs, the traffic demand at the incident scene
can be significantly heavier than that in a commercial network. Hence it is
prohibitively costly to build the public safety network using conventional
cellular network architecture consisting of an infrastructure of stationary
base transceiver stations. A novel architecture is proposed in this paper for
the public safety broadband network. The architecture deploys stationary base
stations sparsely to serve light routine traffic and dispatches mobile base
stations to incident scenes along with public safety personnel to support heavy
traffic. The analysis shows that the proposed architecture can potentially
offer more than 75% reduction in terms of the total number of base stations
needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4441</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4441</id><created>2013-03-18</created><updated>2014-04-21</updated><authors><author><keyname>Burch</keyname><forenames>Neil</forenames></author><author><keyname>Johanson</keyname><forenames>Michael</forenames></author><author><keyname>Bowling</keyname><forenames>Michael</forenames></author></authors><title>Solving Imperfect Information Games Using Decomposition</title><categories>cs.GT</categories><comments>7 pages by 2 columns, 5 figures; April 21 2014 - expand explanations
  and theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decomposition, i.e. independently analyzing possible subgames, has proven to
be an essential principle for effective decision-making in perfect information
games. However, in imperfect information games, decomposition has proven to be
problematic. To date, all proposed techniques for decomposition in imperfect
information games have abandoned theoretical guarantees. This work presents the
first technique for decomposing an imperfect information game into subgames
that can be solved independently, while retaining optimality guarantees on the
full-game solution. We can use this technique to construct theoretically
justified algorithms that make better use of information available at run-time,
overcome memory or disk limitations at run-time, or make a time/space trade-off
to overcome memory or disk limitations while solving a game. In particular, we
present an algorithm for subgame solving which guarantees performance in the
whole game, in contrast to existing methods which may have unbounded error. In
addition, we present an offline game solving algorithm, CFR-D, which can
produce a Nash equilibrium for a game that is larger than available storage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4443</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4443</id><created>2013-03-18</created><updated>2013-06-17</updated><authors><author><keyname>Oliveira</keyname><forenames>Mateus de Oliveira</forenames></author></authors><title>Subgraphs Satisfying MSO Properties on z-Topologically Orderable
  Digraphs</title><categories>cs.CC cs.DM cs.LO math.CO</categories><comments>12 pages body, 11 pages of references + appendix + 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of z-topological orderings for digraphs. We prove
that given a digraph G on n vertices admitting a z-topological order- ing,
together with such an ordering, one may count the number of subgraphs of G that
at the same time satisfy a monadic second order formula {\phi} and are the
union of k directed paths, in time f ({\phi}, k, z) * n^O(k*z) . Our result
implies the polynomial time solvability of many natural counting problems on
digraphs admitting z-topological orderings for constant values of z and k.
Concerning the relationship between z-topological orderability and other
digraph width measures, we observe that any digraph of directed path-width d
has a z- topological ordering for z &lt;= 2d + 1. On the other hand, there are
digraphs on n vertices admitting a z-topological order for z = 2, but whose
directed path-width is {\Theta}(log n). Since graphs of bounded directed
path-width can have both arbitrarily large undirected tree-width and
arbitrarily large clique width, our result provides for the first time a
suitable way of partially trans- posing metatheorems developed in the context
of the monadic second order logic of graphs of constant undirected tree-width
and constant clique width to the realm of digraph width measures that are
closed under taking subgraphs and whose constant levels incorporate families of
graphs of arbitrarily large undirected tree-width and arbitrarily large clique
width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4447</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4447</id><created>2013-03-18</created><authors><author><keyname>Yang</keyname><forenames>Ang</forenames></author><author><keyname>Fei</keyname><forenames>Zesong</forenames></author><author><keyname>Xing</keyname><forenames>Chengwen</forenames></author><author><keyname>Xiao</keyname><forenames>Ming</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author><author><keyname>Kuang</keyname><forenames>Jingming</forenames></author></authors><title>Design of Binary Network Codes for Multi-user Multi-way Relay Networks</title><categories>cs.IT math.IT</categories><comments>accepted by the IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study multi-user multi-way relay networks where $N$ user nodes exchange
their information through a single relay node. We use network coding in the
relay to increase the throughput. Due to the limitation of complexity, we only
consider the binary multi-user network coding (BMNC) in the relay. We study
BMNC matrix (in GF(2)) and propose several design criteria on the BMNC matrix
to improve the symbol error probability (SEP) performance. Closed-form
expressions of the SEP of the system are provided. Moreover, an upper bound of
the SEP is also proposed to provide further insights on system performance.
Then BMNC matrices are designed to minimize the error probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4448</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4448</id><created>2013-03-18</created><authors><author><keyname>Field</keyname><forenames>Rebecca E.</forenames></author><author><keyname>Jones</keyname><forenames>Brant C.</forenames></author></authors><title>Using carry-truncated addition to analyze add-rotate-xor hash algorithms</title><categories>cs.DM cs.CR</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a truncated addition operation on pairs of N-bit binary numbers
that interpolates between ordinary addition mod 2^N and bitwise addition in
(Z/2Z)^N. We use truncated addition to analyze hash functions that are built
from the bit operations add, rotate, and xor, such as Blake, Skein, and
Cubehash. Any ARX algorithm can be approximated by replacing ordinary addition
with truncated addition, and we define a metric on such algorithms which we
call the {\bf sensitivity}. This metric measures the smallest approximation
agreeing with the full algorithm a statistically useful portion of the time (we
use 0.1%). Because truncated addition greatly reduces the complexity of the
non-linear operation in ARX algorithms, the approximated algorithms are more
susceptible to both collision and pre-image attacks, and we outline a potential
collision attack explicitly. We particularize some of these observations to the
Skein hash function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4451</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4451</id><created>2013-03-18</created><authors><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author><author><keyname>Jain</keyname><forenames>Prachi</forenames></author><author><keyname>Ghosh</keyname><forenames>Rumi</forenames></author><author><keyname>Kang</keyname><forenames>Jeon-Hyung</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>Limited Attention and Centrality in Social Networks</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>in Proceedings of International Conference on Social Intelligence and
  Technology (SOCIETY2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How does one find important or influential people in an online social
network? Researchers have proposed a variety of centrality measures to identify
individuals that are, for example, often visited by a random walk, infected in
an epidemic, or receive many messages from friends. Recent research suggests
that a social media users' capacity to respond to an incoming message is
constrained by their finite attention, which they divide over all incoming
information, i.e., information sent by users they follow. We propose a new
measure of centrality --- limited-attention version of Bonacich's
Alpha-centrality --- that models the effect of limited attention on epidemic
diffusion. The new measure describes a process in which nodes broadcast
messages to their out-neighbors, but the neighbors' ability to receive the
message depends on the number of in-neighbors they have. We evaluate the
proposed measure on real-world online social networks and show that it can
better reproduce an empirical influence ranking of users than other popular
centrality measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4452</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4452</id><created>2013-03-18</created><authors><author><keyname>Wu</keyname><forenames>Jinhong</forenames></author><author><keyname>El-Khamy</keyname><forenames>Mostafa</forenames></author><author><keyname>Lee</keyname><forenames>Jungwon</forenames></author><author><keyname>Kang</keyname><forenames>Inyup</forenames></author></authors><title>BICM Performance Improvement via Online LLR Optimization</title><categories>cs.IT math.IT</categories><comments>IEEE WCNC, Apr. 2013, Shanghai</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider bit interleaved coded modulation (BICM) receiver performance
improvement based on the concept of generalized mutual information (GMI).
Increasing achievable rates of BICM receiver with GMI maximization by proper
scaling of the log likelihood ratio (LLR) is investigated. While it has been
shown in the literature that look-up table based LLR scaling functions matched
to each specific transmission scenario may provide close to optimal solutions,
this method is difficult to adapt to time-varying channel conditions. To solve
this problem, an online adaptive scaling factor searching algorithm is
developed. Uniform scaling factors are applied to LLRs from different bit
channels of each data frame by maximizing an approximate GMI that characterizes
the transmission conditions of current data frame. Numerical analysis on
effective achievable rates as well as link level simulation of realistic mobile
transmission scenarios indicate that the proposed method is simple yet
effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4458</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4458</id><created>2013-03-18</created><updated>2013-06-24</updated><authors><author><keyname>Bandeira</keyname><forenames>Afonso S.</forenames></author><author><keyname>Chen</keyname><forenames>Yutong</forenames></author><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author></authors><title>Phase retrieval from power spectra of masked signals</title><categories>math.FA cs.IT math.IT</categories><comments>18 pages, 3 figures</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In diffraction imaging, one is tasked with reconstructing a signal from its
power spectrum. To resolve the ambiguity in this inverse problem, one might
invoke prior knowledge about the signal, but phase retrieval algorithms in this
vein have found limited success. One alternative is to create redundancy in the
measurement process by illuminating the signal multiple times, distorting the
signal each time with a different mask. Despite several recent advances in
phase retrieval, the community has yet to construct an ensemble of masks which
uniquely determines all signals and admits an efficient reconstruction
algorithm. In this paper, we leverage the recently proposed polarization method
to construct such an ensemble. We also present numerical simulations to
illustrate the stability of the polarization method in this setting. In
comparison to a state-of-the-art phase retrieval algorithm known as PhaseLift,
we find that polarization is much faster with comparable stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4461</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4461</id><created>2013-03-18</created><authors><author><keyname>Solomakhin</keyname><forenames>Dmitry</forenames></author></authors><title>Medical Process Modeling: an Artifact-Centric Approach</title><categories>cs.OH</categories><comments>6 pages submission for the KR4HC and ProHealth workshops held
  together with AIME 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this position paper we argue that just as traditional business process
modeling has been adopted to deal with clinical pathways, also the
artifact-centric process modeling technique may be successfully used to model
various kinds of medical processes: physiological processes, disease behavior
and treatment processes. We also discuss how a proposed approach may be used to
deal with an interplay of all the processes a patient is subject to and what
are the queries that might be imposed over an overall patient model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4471</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4471</id><created>2013-03-18</created><authors><author><keyname>Kennedy</keyname><forenames>Oliver</forenames></author><author><keyname>Ziarek</keyname><forenames>Lukasz</forenames></author></authors><title>BarQL: Collaborating Through Change</title><categories>cs.DB</categories><comments>BarQL reference document</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications such as Google Docs, Office 365, and Dropbox show a growing
trend towards incorporating multi-user live collaboration functionality into
web applications. These collaborative applications share a need to efficiently
express shared state, and a common strategy for doing so is a shared log
abstraction. Extensive research efforts on log abstractions by the database,
programming languages, and distributed systems communities have identified a
variety of optimization techniques based on the algebraic properties of updates
(i.e., pairwise commutativity, subsumption, and idempotence). Although these
techniques have been applied to specific applications and use-cases, to the
best of our knowledge, no attempt has been made to create a general framework
for such optimizations in the context of a non-trivial update language. In this
paper, we introduce mutation languages, a low-level framework for reasoning
about the algebraic properties of state updates, or mutations. We define BarQL,
a general purpose state-update language, and show how mutation languages allow
us to reason about the algebraic properties of updates expressed in BarQ L .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4484</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4484</id><created>2013-03-19</created><authors><author><keyname>Wangmei</keyname><forenames>Guo</forenames><affiliation>IEEE Student Member</affiliation></author><author><keyname>Xiaomeng</keyname><forenames>Shi</forenames><affiliation>IEEE Student Member</affiliation></author><author><keyname>Ning</keyname><forenames>Cai</forenames><affiliation>EEE Senior Member</affiliation></author><author><keyname>M&#xe9;dard</keyname><forenames>Muriel</forenames><affiliation>IEEE Fellow</affiliation></author></authors><title>Localized Dimension Growth: A Convolutional Random Network Coding
  Approach to Managing Memory and Decoding Delay</title><categories>cs.IT math.IT</categories><comments>submit to IEEE Transactions on Communications.13 pages, 12 figures.
  arXiv admin note: text overlap with arXiv:1103.6258</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an \textit{Adaptive Random Convolutional Network Coding} (ARCNC)
algorithm to address the issue of field size in random network coding for
multicast, and study its memory and decoding delay performances through both
analysis and numerical simulations. ARCNC operates as a convolutional code,
with the coefficients of local encoding kernels chosen randomly over a small
finite field. The cardinality of local encoding kernels increases with time
until the global encoding kernel matrices at related sink nodes have full
rank.ARCNC adapts to unknown network topologies without prior knowledge, by
locally incrementing the dimensionality of the convolutional code. Because
convolutional codes of different constraint lengths can coexist in different
portions of the network, reductions in decoding delay and memory overheads can
be achieved. We show that this method performs no worse than random linear
network codes in terms of decodability, and can provide significant gains in
terms of average decoding delay or memory in combination, shuttle and random
geometric networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4498</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4498</id><created>2013-03-19</created><authors><author><keyname>James</keyname><forenames>Joshua I.</forenames></author><author><keyname>Gladyshev</keyname><forenames>Pavel</forenames></author></authors><title>Challenges with Automation in Digital Forensic Investigations</title><categories>cs.CY</categories><comments>17 pages, 1 figure</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The use of automation in digital forensic investigations is not only a
technological issue, but also has political and social implications. This work
discusses some challenges with the implementation and acceptance of automation
in digital forensic investigation, and possible implications for current
digital forensic investigators. Current attitudes towards the use of automation
in digital forensic investigations are examined, as well as the issue of
digital investigators knowledge acquisition and retention. The argument is made
for a well planned, careful use of automation going forward that allows for a
more efficient and effective use of automation in digital forensic
investigations while at the same time attempting to improve the overall quality
of expert investigators. Targeting and carefully controlling automated
solutions for beginning investigators may improve the speed and quality of
investigations while at the same time letting expert digital investigators
spend more time utilizing expert level knowledge required in manual phases of
investigations. By considering how automated solutions are being implemented
into digital investigations, investigation unit managers can increase the
efficiency of their unit while at the same time maximizing their return on
investment for expert level digital investigator training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4532</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4532</id><created>2013-03-19</created><updated>2013-03-20</updated><authors><author><keyname>Ganguly</keyname><forenames>Arnab</forenames></author><author><keyname>Petrov</keyname><forenames>Tatjana</forenames></author><author><keyname>Koeppl</keyname><forenames>Heinz</forenames></author></authors><title>Markov chain aggregation and its applications to combinatorial reaction
  networks</title><categories>cs.DM q-bio.QM</categories><comments>29 pages, 9 figures, 1 table; Ganguly and Petrov are authors with
  equal contribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a continuous-time Markov chain (CTMC) whose state space is
partitioned into aggregates, and each aggregate is assigned a probability
measure. A sufficient condition for defining a CTMC over the aggregates is
presented as a variant of weak lumpability, which also characterizes that the
measure over the original process can be recovered from that of the aggregated
one. We show how the applicability of de-aggregation depends on the initial
distribution. The application section is a major aspect of the article, where
we illustrate that the stochastic rule-based models for biochemical reaction
networks form an important area for usage of the tools developed in the paper.
For the rule-based models, the construction of the aggregates and computation
of the distribution over the aggregates are algorithmic. The techniques are
exemplified in three case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4533</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4533</id><created>2013-03-19</created><updated>2013-05-22</updated><authors><author><keyname>Benedikt</keyname><forenames>Michael</forenames></author><author><keyname>Lenhardt</keyname><forenames>Rastislav</forenames></author><author><keyname>Worrell</keyname><forenames>James</forenames></author></authors><title>Two Variable vs. Linear Temporal Logic in Model Checking and Games</title><categories>cs.LO cs.FL</categories><comments>37 pages, to be published in Logical Methods in Computer Science
  journal, includes material presented in Concur 2011 and QEST 2012 extended
  abstracts</comments><proxy>Logical Methods In Computer Science</proxy><acm-class>F.4.1; F.4.3; F.1.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 2 (May 23,
  2013) lmcs:1103</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model checking linear-time properties expressed in first-order logic has
non-elementary complexity, and thus various restricted logical languages are
employed. In this paper we consider two such restricted specification logics,
linear temporal logic (LTL) and two-variable first-order logic (FO2). LTL is
more expressive but FO2 can be more succinct, and hence it is not clear which
should be easier to verify. We take a comprehensive look at the issue, giving a
comparison of verification problems for FO2, LTL, and various sublogics thereof
across a wide range of models. In particular, we look at unary temporal logic
(UTL), a subset of LTL that is expressively equivalent to FO2; we also consider
the stutter-free fragment of FO2, obtained by omitting the successor relation,
and the expressively equivalent fragment of UTL, obtained by omitting the next
and previous connectives. We give three logic-to-automata translations which
can be used to give upper bounds for FO2 and UTL and various sublogics. We
apply these to get new bounds for both non-deterministic systems (hierarchical
and recursive state machines, games) and for probabilistic systems (Markov
chains, recursive Markov chains, and Markov decision processes). We couple
these with matching lower-bound arguments. Next, we look at combining FO2
verification techniques with those for LTL. We present here a language that
subsumes both FO2 and LTL, and inherits the model checking properties of both
languages. Our results give both a unified approach to understanding the
behaviour of FO2 and LTL, along with a nearly comprehensive picture of the
complexity of verification for these logics and their sublogics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4538</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4538</id><created>2013-03-19</created><authors><author><keyname>Scheit</keyname><forenames>Christoph</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Treibig</keyname><forenames>Jan</forenames></author><author><keyname>Becker</keyname><forenames>Stefan</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>Optimization of FASTEST-3D for Modern Multicore Systems</title><categories>cs.PF cs.DC</categories><comments>10 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FASTEST-3D is an MPI-parallel finite-volume flow solver based on
block-structured meshes that has been developed at the University of
Erlangen-Nuremberg since the early 1990s. It can be used to solve the laminar
or turbulent incompressible Navier-Stokes equations. Up to now its scalability
was strongly limited by a rather rigid communication infrastructure, which led
to a dominance of MPI time already at small process counts.
  This paper describes several optimizations to increase the performance,
scalability, and flexibility of FASTEST-3D. First, a node-level performance
analysis is carried out in order to pinpoint the main bottlenecks and identify
sweet spots for energy-efficient execution. In addition, a single-precision
version of the solver for the linear equation system arising from the
discretization of the governing equations is devised, which significantly
increases the single-core performance. Then the communication mechanisms in
FASTEST-3D are analyzed and a new communication strategy based on non-blocking
calls is implemented. Performance results with the revised version show
significantly increased single-node performance and considerably improved
communication patterns along with much better parallel scalability. In this
context we discuss the concept of &quot;acceptable parallel efficiency&quot; and how it
influences the real gain of the optimizations. Scaling measurements are carried
out on a modern petascale system. The obtained improvements are of major
importance for the use of FASTEST-3D on current high-performance computer
clusters and will help to perform simulations with much higher spatial and
temporal resolution to tackle turbulent flow in technical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4566</identifier>
 <datestamp>2013-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4566</id><created>2013-03-19</created><updated>2013-10-22</updated><authors><author><keyname>Harper</keyname><forenames>Marc</forenames></author></authors><title>Inferring Fitness in Finite Populations with Moran-like dynamics</title><categories>math.DS cs.NE q-bio.PE</categories><comments>Title update, minor edits, more references</comments><msc-class>91A22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biological fitness is not an observable quantity and must be inferred from
population dynamics. Bayesian inference applied to the Moran process and
variants yields a robust inference method that can infer fitness in populations
evolving via a Moran dynamic and generalizations. Information about fitness is
derived solely from birth-events in birth-death and death-birth processes in
which selection acts proportionally to fitness, which allows the method to be
applied to populations on a network where the network itself may be changing in
time. Populations may also be allowed to change size while still allowing
estimates for fitness to be inferred.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4567</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4567</id><created>2013-03-19</created><authors><author><keyname>Xu</keyname><forenames>Weiqiang</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author><author><keyname>Alshomrani</keyname><forenames>Saleh</forenames></author></authors><title>Probability-constrained Power Optimization for Multiuser MISO Systems
  with Imperfect CSI: A Bernstein Approximation Approach</title><categories>cs.IT math.IT</categories><comments>25 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider power allocations in downlink cellular wireless systems where the
basestations are equipped with multiple transmit antennas and the mobile users
are equipped with single receive antennas. Such systems can be modeled as
multiuser MISO systems. We assume that the multi-antenna transmitters employ
some fixed beamformers to transmit data, and the objective is to optimize the
power allocation for different users to satisfy certain QoS constraints, with
imperfect transmitter-side channel state information (CSI). Specifically, for
MISO interference channels, we consider the transmit power minimization problem
and the max-min SINR problem. For MISO broadcast channels, we consider the
MSE-constrained transmit power minimization problem. All these problems are
formulated as probability-constrained optimization problems. We make use of the
Bernstein approximation to conservatively transform the probabilistic
constraints into deterministic ones, and consequently convert the original
stochastic optimization problems into convex optimization problems. However,
the transformed problems cannot be straightforwardly solved using standard
solver, since one of the constraints is itself an optimization problem. We
employ the long-step logarithmic barrier cutting plane (LLBCP) algorithm to
overcome difficulty. Extensive simulation results are provided to demonstrate
the effectiveness of the proposed method, and the performance advantage over
some existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4614</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4614</id><created>2013-03-19</created><authors><author><keyname>Bela&#xef;d</keyname><forenames>Abdel</forenames><affiliation>LORIA</affiliation></author><author><keyname>Santosh</keyname><forenames>K. C.</forenames><affiliation>LORIA</affiliation></author><author><keyname>D'Andecy</keyname><forenames>Vincent Poulain</forenames></author></authors><title>Handwritten and Printed Text Separation in Real Document</title><categories>cs.CV</categories><comments>Machine Vision Applications (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of the paper is to separate handwritten and printed text from a real
document embedded with noise, graphics including annotations. Relying on
run-length smoothing algorithm (RLSA), the extracted pseudo-lines and
pseudo-words are used as basic blocks for classification. To handle this, a
multi-class support vector machine (SVM) with Gaussian kernel performs a first
labelling of each pseudo-word including the study of local neighbourhood. It
then propagates the context between neighbours so that we can correct possible
labelling errors. Considering running time complexity issue, we propose linear
complexity methods where we use k-NN with constraint. When using a kd-tree, it
is almost linearly proportional to the number of pseudo-words. The performance
of our system is close to 90%, even when very small learning dataset where
samples are basically composed of complex administrative documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4629</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4629</id><created>2013-03-19</created><authors><author><keyname>Ba&#xf1;os</keyname><forenames>Raquel A</forenames></author><author><keyname>Borge-Holthoefer</keyname><forenames>Javier</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author></authors><title>The role of hidden influentials in the diffusion of online information
  cascades</title><categories>physics.soc-ph cs.SI</categories><comments>Submitted to EPJ Data Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a diversified context with multiple social networking sites, heterogeneous
activity patterns and different user-user relations, the concept of
&quot;information cascade&quot; is all but univocal. Despite the fact that such
information cascades can be defined in different ways, it is important to check
whether some of the observed patterns are common to diverse contagion processes
that take place on modern social media. Here, we explore one type of
information cascades, namely, those that are time-constrained, related to two
kinds of socially-rooted topics on Twitter. Specifically, we show that in both
cases cascades sizes distribute following a fat tailed distribution and that
whether or not a cascade reaches system-wide proportions is mainly given by the
presence of so-called hidden influentials. These latter nodes are not the hubs,
which on the contrary, often act as firewalls for information spreading. Our
results are important for a better understanding of the dynamics of complex
contagion and, from a practical side, for the identification of efficient
spreaders in viral phenomena.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4632</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4632</id><created>2013-03-19</created><authors><author><keyname>Shakarian</keyname><forenames>Paulo</forenames></author><author><keyname>Subrahmanian</keyname><forenames>V. S.</forenames></author></authors><title>Geospatial Optimization Problems</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  There are numerous applications which require the ability to take certain
actions (e.g. distribute money, medicines, people etc.) over a geographic
region. A disaster relief organization must allocate people and supplies to
parts of a region after a disaster. A public health organization must allocate
limited vaccine to people across a region. In both cases, the organization is
trying to optimize something (e.g. minimize expected number of people with a
disease). We introduce &quot;geospatial optimization problems&quot; (GOPs) where an
organization has limited resources and budget to take actions in a geographic
area. The actions result in one or more properties changing for one or more
locations. There are also certain constraints on the combinations of actions
that can be taken. We study two types of GOPs - goal-based and
benefit-maximizing (GBGOP and BMGOP respectively). A GBGOP ensures that certain
properties must be true at specified locations after the actions are taken
while a BMGOP optimizes a linear benefit function. We show both problems to be
NP-hard (with membership in NP for the associated decision problems).
Additionally, we prove limits on approximation for both problems. We present
integer programs for both GOPs that provide exact solutions. We also correctly
reduce the number of variables in for the GBGOP integer constraints. For BMGOP,
we present the BMGOP-Compute algorithm that runs in PTIME and provides a
reasonable approximation guarantee in most cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4638</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4638</id><created>2013-03-13</created><authors><author><keyname>Chen</keyname><forenames>Xianfu</forenames></author><author><keyname>Zhang</keyname><forenames>Honggang</forenames></author><author><keyname>Chen</keyname><forenames>Tao</forenames></author><author><keyname>Lasanen</keyname><forenames>Mika</forenames></author><author><keyname>Palicot</keyname><forenames>Jacques</forenames></author></authors><title>On Improving Energy Efficiency within Green Femtocell Networks: A
  Hierarchical Reinforcement Learning Approach</title><categories>cs.LG cs.GT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1209.2790</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  One of the efficient solutions of improving coverage and increasing capacity
in cellular networks is the deployment of femtocells. As the cellular networks
are becoming more complex, energy consumption of whole network infrastructure
is becoming important in terms of both operational costs and environmental
impacts. This paper investigates energy efficiency of two-tier femtocell
networks through combining game theory and stochastic learning. With the
Stackelberg game formulation, a hierarchical reinforcement learning framework
is applied for studying the joint expected utility maximization of macrocells
and femtocells subject to the minimum signal-to-interference-plus-noise-ratio
requirements. In the learning procedure, the macrocells act as leaders and the
femtocells are followers. At each time step, the leaders commit to dynamic
strategies based on the best responses of the followers, while the followers
compete against each other with no further information but the leaders'
transmission parameters. In this paper, we propose two reinforcement learning
based intelligent algorithms to schedule each cell's stochastic power levels.
Numerical experiments are presented to validate the investigations. The results
show that the two learning algorithms substantially improve the energy
efficiency of the femtocell networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4645</identifier>
 <datestamp>2013-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4645</id><created>2013-03-19</created><updated>2013-09-07</updated><authors><author><keyname>Zhang</keyname><forenames>Hui</forenames></author><author><keyname>Yin</keyname><forenames>Wotao</forenames></author></authors><title>Gradient methods for convex minimization: better rates under weaker
  conditions</title><categories>math.OC cs.IT math.IT math.NA</categories><comments>20 pages, 4 figures, typos are corrected, Theorem 2 is new</comments><report-no>UCLA CAM Reports 13-17</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The convergence behavior of gradient methods for minimizing convex
differentiable functions is one of the core questions in convex optimization.
This paper shows that their well-known complexities can be achieved under
conditions weaker than the commonly accepted ones. We relax the common gradient
Lipschitz-continuity condition and strong convexity condition to ones that hold
only over certain line segments. Specifically, we establish complexities
$O(\frac{R}{\epsilon})$ and $O(\sqrt{\frac{R}{\epsilon}})$ for the ordinary and
accelerate gradient methods, respectively, assuming that $\nabla f$ is
Lipschitz continuous with constant $R$ over the line segment joining $x$ and
$x-\frac{1}{R}\nabla f$ for each $x\in\dom f$. Then we improve them to
$O(\frac{R}{\nu}\log(\frac{1}{\epsilon}))$ and
$O(\sqrt{\frac{R}{\nu}}\log(\frac{1}{\epsilon}))$ for function $f$ that also
satisfies the secant inequality $\ &lt; \nabla f(x), x- x^*\ &gt; \ge \nu\|x-x^*\|^2$
for each $x\in \dom f$ and its projection $x^*$ to the minimizer set of $f$.
The secant condition is also shown to be necessary for the geometric decay of
solution error. Not only are the relaxed conditions met by more functions, the
restrictions give smaller $R$ and larger $\nu$ than they are without the
restrictions and thus lead to better complexity bounds. We apply these results
to sparse optimization and demonstrate a faster algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4664</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4664</id><created>2013-03-19</created><authors><author><keyname>Golovin</keyname><forenames>Daniel</forenames></author><author><keyname>Sculley</keyname><forenames>D.</forenames></author><author><keyname>McMahan</keyname><forenames>H. Brendan</forenames></author><author><keyname>Young</keyname><forenames>Michael</forenames></author></authors><title>Large-Scale Learning with Less RAM via Randomization</title><categories>cs.LG</categories><comments>Extended version of ICML 2013 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We reduce the memory footprint of popular large-scale online learning methods
by projecting our weight vector onto a coarse discrete set using randomized
rounding. Compared to standard 32-bit float encodings, this reduces RAM usage
by more than 50% during training and by up to 95% when making predictions from
a fixed model, with almost no loss in accuracy. We also show that randomized
counting can be used to implement per-coordinate learning rates, improving
model quality with little additional RAM. We prove these memory-saving methods
achieve regret guarantees similar to their exact variants. Empirical evaluation
confirms excellent performance, dominating standard approaches across memory
versus accuracy tradeoffs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4672</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4672</id><created>2013-03-19</created><updated>2015-09-07</updated><authors><author><keyname>Rotolo</keyname><forenames>Daniele</forenames></author><author><keyname>Rafols</keyname><forenames>Ismael</forenames></author><author><keyname>Hopkins</keyname><forenames>Michael</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Strategic Intelligence on Emerging Technologies: Scientometric Overlay
  Mapping</title><categories>cs.DL</categories><comments>Journal of the Association for Information Science and Technology
  (2015)</comments><doi>10.1002/asi.23631</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the use of scientometric overlay mapping as a tool of
'strategic intelligence' to aid the governance of emerging technologies. We
develop an integrative synthesis of different overlay mapping techniques and
associated perspectives on technological emergence across the geographical,
social, and cognitive spaces. To do so, we longitudinally analyse (with
publication and patent data) three case-studies of emerging technologies in the
medical domain. These are: RNA interference (RNAi), Human Papilloma Virus (HPV)
testing technologies for cervical cancer, and Thiopurine Methyltransferase
(TPMT) genetic testing. Given the flexibility (i.e. adaptability to different
sources of data) and granularity (i.e. applicability across multiple levels of
data aggregation) of overlay mapping techniques, we argue that these techniques
can favour the integration and comparison of results from different contexts
and cases, thus potentially functioning as platform for a 'distributed'
strategic intelligence for analysts and decision-makers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4679</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4679</id><created>2013-03-19</created><authors><author><keyname>Khan</keyname><forenames>M. Y.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Khan</keyname><forenames>M. A.</forenames></author><author><keyname>Javaid</keyname><forenames>A.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author></authors><title>Hybrid DEEC: Towards Efficient Energy Utilization in Wireless Sensor
  Networks</title><categories>cs.NI</categories><comments>World Applied Sciences Journal (WASJ), 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The clustering algorithm are considered as a kind of key technique used to
reduce energy consumption. It can help in increasing the stability period and
network life time. Routing protocol for efficient energy utilization should be
designed for heterogeneous Wireless Sensor Networks (WSNs). We purpose
Hybrid-DEEC (H-DEEC), a chain and cluster based (hybrid) distributed scheme for
efficient energy utilization in WSNs. In H-DEEC,elected Cluster Heads (CHs)
communicate the Base Station (BS) through beta elected nodes, by using
multi-hopping. We logically divide the network into two parts, on the basis of
the residual energy of nodes. The normal nodes with high initial and residual
energy will behighlyprobable to be CHs than the nodes with lesser energy. To
overcome the deficiencies of H-DEEC, we propose Multi-Edged Hybrid-DEEC
(MH-DEEC). In MH-DEEC the criteria of chain construction is modified. Finally,
the comparison in simulation results with other heterogeneous protocols show
that, MH-DEEC and H-DEEC achieves longer stability time and network life time
due to efficient energy utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4683</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4683</id><created>2013-03-19</created><updated>2013-06-28</updated><authors><author><keyname>Mochaourab</keyname><forenames>Rami</forenames></author><author><keyname>Cao</keyname><forenames>Pan</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard</forenames></author></authors><title>Alternating Rate Profile Optimization in Single Stream MIMO Interference
  Channels</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Signal Processing Letters, 4 pages, 3 figures, 1
  table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multiple-input multiple-output interference channel is considered with
perfect channel information at the transmitters and single-user decoding
receivers. With all transmissions restricted to single stream beamforming, we
consider the problem of finding all Pareto optimal rate-tuples in the
achievable rate region. The problem is cast as a rate profile optimization
problem. Due to its nonconvexity, we resort to an alternating approach: For
fixed receivers, optimal transmission is known. For fixed transmitters, we show
that optimal receive beamforming is a solution to an inverse field of values
problem. We prove the solution's stationarity and compare it with existing
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4692</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4692</id><created>2013-03-15</created><authors><author><keyname>Almeida</keyname><forenames>Jo&#xe3;o E.</forenames></author><author><keyname>Rosseti</keyname><forenames>Rosaldo J. F.</forenames></author><author><keyname>Coelho</keyname><forenames>Ant&#xf3;nio Le&#xe7;a</forenames></author></authors><title>Crowd Simulation Modeling Applied to Emergency and Evacuation
  Simulations using Multi-Agent Systems</title><categories>cs.MA</categories><comments>DSIE'11 - 6th Doctoral Symposium on Informatics Engineering, FEUP -
  Engineering Faculty of Porto University, Porto</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years crowd modeling has become increasingly important both in the
computer games industry and in emergency simulation. This paper discusses some
aspects of what has been accomplished in this field, from social sciences to
the computer implementation of modeling and simulation. Problem overview is
described including some of the most common techniques used. Multi-Agent
Systems is stated as the preferred approach for emergency evacuation
simulations. A framework is proposed based on the work of Fangqin and Aizhu
with extensions to include some BDI aspects. Future work includes expansion of
the model's features and implementation of a prototype for validation of the
propose methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4693</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4693</id><created>2013-03-19</created><authors><author><keyname>Abbas</keyname><forenames>Z.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Javaid</keyname><forenames>A.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Khan</keyname><forenames>M. A.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author></authors><title>EAPESS: An Adaptive Transmission Scheme in Wireless Sensor Networks</title><categories>cs.NI</categories><comments>World Applied Sciences Journal (WASJ), 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reduced energy consumption in sensor nodes is one of the major challenges in
Wireless Sensor Networks (WSNs) deployment. In this regard, Error Control
Coding (ECC) is one of techniques used for energy optimization in WSNs.
Similarly, critical distance is another term being used for energy efficiency,
when used with ECC provides better results of energy saving. In this paper
three different critical distance values are used against different coding
gains for sake of energy saving. If distance lies below critical distance
values then particular encoders are selected with respect to their particular
coding gains. Coding gains are used for critical distances estimation of all
encoders. This adaptive encoder and transmit power selection scheme with
respect to their coding gain results in a significant energy saving in WSNs
environment. Simulations provide better results of energy saving achieved by
using this adaptive scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4694</identifier>
 <datestamp>2013-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4694</id><created>2013-03-12</created><updated>2013-09-20</updated><authors><author><keyname>Ramamurthy</keyname><forenames>Karthikeyan Natesan</forenames></author><author><keyname>Thiagarajan</keyname><forenames>Jayaraman J.</forenames></author><author><keyname>Spanias</keyname><forenames>Andreas</forenames></author></authors><title>Recovering Non-negative and Combined Sparse Representations</title><categories>math.NA cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The non-negative solution to an underdetermined linear system can be uniquely
recovered sometimes, even without imposing any additional sparsity constraints.
In this paper, we derive conditions under which a unique non-negative solution
for such a system can exist, based on the theory of polytopes. Furthermore, we
develop the paradigm of combined sparse representations, where only a part of
the coefficient vector is constrained to be non-negative, and the rest is
unconstrained (general). We analyze the recovery of the unique, sparsest
solution, for combined representations, under three different cases of
coefficient support knowledge: (a) the non-zero supports of non-negative and
general coefficients are known, (b) the non-zero support of general
coefficients alone is known, and (c) both the non-zero supports are unknown.
For case (c), we propose the combined orthogonal matching pursuit algorithm for
coefficient recovery and derive the deterministic sparsity threshold under
which recovery of the unique, sparsest coefficient vector is possible. We
quantify the order complexity of the algorithms, and examine their performance
in exact and approximate recovery of coefficients under various conditions of
noise. Furthermore, we also obtain their empirical phase transition
characteristics. We show that the basis pursuit algorithm, with partial
non-negative constraints, and the proposed greedy algorithm perform better in
recovering the unique sparse representation when compared to their
unconstrained counterparts. Finally, we demonstrate the utility of the proposed
methods in recovering images corrupted by saturation noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4695</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4695</id><created>2013-03-15</created><authors><author><keyname>Almeida</keyname><forenames>Jo&#xe3;o Em&#xed;lio</forenames></author><author><keyname>Kokkinogenis</keyname><forenames>Zafeiris</forenames></author><author><keyname>Rossetti</keyname><forenames>Rosaldo J. F.</forenames></author></authors><title>NetLogo Implementation of an Evacuation Scenario</title><categories>cs.MA</categories><comments>WISA'2012 (Fourth Workshop on Intelligent Systems and Applications),
  Madrid, Spain. ISBN: 978-1-4673-2843-2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of evacuating crowded closed spaces, such as discotheques, public
exhibition pavilions or concert houses, has become increasingly important and
gained attention both from practitioners and from public authorities. A
simulation implementation using NetLogo, an agent-based simulation framework
that permits the quickly creation of prototypes, is presented. Our aim is to
prove that this model developed using NetLogo, albeit simple can be expanded
and adapted for fire safety experts test various scenarios and validate the
outcome of their design. Some preliminary experiments are carried out, whose
results are presented, validated and discussed so as to illustrate their
efficiency. Finally, we draw some conclusions and point out ways in which this
work can be further extended.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4696</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4696</id><created>2013-03-15</created><authors><author><keyname>Vasconcelos</keyname><forenames>Gon&#xe7;alo</forenames></author><author><keyname>Petry</keyname><forenames>Marcelo</forenames></author><author><keyname>Almeida</keyname><forenames>Jo&#xe3;o Em&#xed;lio</forenames></author><author><keyname>Rossetti</keyname><forenames>Rosaldo J. F.</forenames></author><author><keyname>Coelho</keyname><forenames>Ant&#xf3;nio Le&#xe7;a</forenames></author></authors><title>Using UWB for Human Trajectory Extraction</title><categories>cs.OH</categories><comments>24th European Modeling &amp; Simulation Symposium - EMSS 2012, Vienna,
  Austria</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we report on a methodology to model pedestrian behaviours
whilst aggregate variables are concerned, with potential applications to
different situations, such as evacuating a building in emergency events. The
approach consists of using UWB (ultra-wide band) based data collection to
characterise behaviour in specific scenarios. From a number of experiments
carried out, we detail the single-file scenario to demonstrate the ability of
this approach to represent macroscopic characteristics of the pedestrian flow.
Results are discussed and we can conclude that UWB-based data collection shows
great potential and suitability for human trajectory extraction, when compared
to other traditional approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4699</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4699</id><created>2013-03-19</created><authors><author><keyname>He</keyname><forenames>Dongxiao</forenames></author><author><keyname>Liu</keyname><forenames>Dayou</forenames></author><author><keyname>Weixiongzhang</keyname></author><author><keyname>Jin</keyname><forenames>Di</forenames></author><author><keyname>Yang</keyname><forenames>Bo</forenames></author></authors><title>Discovering link communities in complex networks by exploiting link
  dynamics</title><categories>cs.SI cond-mat.stat-mech physics.soc-ph</categories><comments>18 pages,11 figures</comments><journal-ref>Journal of Statistical Mechanics: Theory and Experiment,P10015,
  2012</journal-ref><doi>10.1088/1742-5468/2012/10/P10015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discovery of communities in complex networks is a fundamental data analysis
problem with applications in various domains. Most of the existing approaches
have focused on discovering communities of nodes, while recent studies have
shown great advantages and utilities of the knowledge of communities of links
in networks. From this new perspective, we propose a link dynamics based
algorithm, called UELC, for identifying link communities of networks. In UELC,
the stochastic process of a link-node-link random walk is employed to unfold an
embedded bipartition structure of links in a network. The local mixing
properties of the Markov chain underlying the random walk are then utilized to
extract two emerged link communities. Further, the random walk and the
bipartitioning processes are wrapped in an iterative subdivision strategy to
recursively identify link partitions that segregate the network links into
multiple subdivisions. We evaluate the performance of the new method on
synthetic benchmarks and demonstrate its utility on real-world networks. Our
experimental results show that our method is highly effective for discovering
link communities in complex networks. As a comparison, we also extend UELC to
extracting communities of node, and show that it is effective for node
community identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4702</identifier>
 <datestamp>2014-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4702</id><created>2013-03-19</created><updated>2013-03-21</updated><authors><author><keyname>Steiner</keyname><forenames>Thomas</forenames></author><author><keyname>van Hooland</keyname><forenames>Seth</forenames></author><author><keyname>Summers</keyname><forenames>Ed</forenames></author></authors><title>MJ no more: Using Concurrent Wikipedia Edit Spikes with Social Network
  Plausibility Checks for Breaking News Detection</title><categories>cs.SI cs.IR physics.soc-ph</categories><journal-ref>Proceedings of the 22nd international conference on World Wide Web
  companion (WWW '13 Companion), 2013. International World Wide Web Conferences
  Steering Committee, Republic and Canton of Geneva, Switzerland, 791-794</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We have developed an application called Wikipedia Live Monitor that monitors
article edits on different language versions of Wikipedia, as they happen in
realtime. Wikipedia articles in different languages are highly interlinked. For
example, the English article en:2013_Russian_meteor_event on the topic of the
February 15 meteoroid that exploded over the region of Chelyabinsk Oblast,
Russia, is interlinked with the Russian article on the same topic. As we
monitor multiple language versions of Wikipedia in parallel, we can exploit
this fact to detect concurrent edit spikes of Wikipedia articles covering the
same topics, both in only one, and in different languages. We treat such
concurrent edit spikes as signals for potential breaking news events, whose
plausibility we then check with full-text cross-language searches on multiple
social networks. Unlike the reverse approach of monitoring social networks
first, and potentially checking plausibility on Wikipedia second, the approach
proposed in this paper has the advantage of being less prone to false-positive
alerts, while being equally sensitive to true-positive events, however, at only
a fraction of the processing cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4711</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4711</id><created>2013-03-19</created><authors><author><keyname>He</keyname><forenames>Dongxiao</forenames></author><author><keyname>Liu</keyname><forenames>Jie</forenames></author><author><keyname>Yang</keyname><forenames>Bo</forenames></author><author><keyname>Huang</keyname><forenames>Yuxiao</forenames></author><author><keyname>Liu</keyname><forenames>Dayou</forenames></author><author><keyname>Jin</keyname><forenames>Di</forenames></author></authors><title>An Ant-Based Algorithm with Local Optimization for Community Detection
  in Large-Scale Networks</title><categories>cs.SI physics.soc-ph</categories><comments>18 pages,7 figures, 4 tables. arXiv admin note: text overlap with
  arXiv:0803.0476, arXiv:cond-mat/0309508 by other authors</comments><journal-ref>Advances in Complex Systems, 2012, 15(08): 1250036</journal-ref><doi>10.1142/S0219525912500361</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a multi-layer ant-based algorithm MABA, which
detects communities from networks by means of locally optimizing modularity
using individual ants. The basic version of MABA, namely SABA, combines a
self-avoiding label propagation technique with a simulated annealing strategy
for ant diffusion in networks. Once the communities are found by SABA, this
method can be reapplied to a higher level network where each obtained community
is regarded as a new vertex. The aforementioned process is repeated
iteratively, and this corresponds to MABA. Thanks to the intrinsic multi-level
nature of our algorithm, it possesses the potential ability to unfold
multi-scale hierarchical structures. Furthermore, MABA has the ability that
mitigates the resolution limit of modularity. The proposed MABA has been
evaluated on both computer-generated benchmarks and widely used real-world
networks, and has been compared with a set of competitive algorithms.
Experimental results demonstrate that MABA is both effective and efficient (in
near linear time with respect to the size of network) for discovering
communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4733</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4733</id><created>2013-03-19</created><updated>2013-04-29</updated><authors><author><keyname>Reem</keyname><forenames>Daniel</forenames></author></authors><title>Topological properties of sets represented by an inequality involving
  distances</title><categories>math.FA cs.CG math.GT</categories><comments>13 pages; 4 figures; the font was enlarged; the introduction was
  extended; the figures and auxiliary results were improved; added remarks and
  references</comments><msc-class>46B20, 68U05, 46N99, 65D18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a set represented by an inequality. An interesting phenomenon which
occurs in various settings in mathematics is that the interior of this set is
the subset where strict inequality holds, the boundary is the subset where
equality holds, and the closure of the set is the closure of its interior. This
paper discusses this phenomenon assuming the set is a Voronoi cell induced by
given sites (subsets), a geometric object which appears in many fields of
science and technology and has diverse applications. Simple counterexamples
show that the discussed phenomenon does not hold in general, but it is
established in a wide class of cases. More precisely, the setting is a
(possibly infinite dimensional) uniformly convex normed space with arbitrary
positively separated sites. An important ingredient in the proof is a strong
version of the triangle inequality due to Clarkson (1936), an interesting
inequality which has been almost totally forgotten.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4756</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4756</id><created>2013-03-19</created><updated>2014-08-13</updated><authors><author><keyname>Meng</keyname><forenames>Zhaoshi</forenames></author><author><keyname>Wei</keyname><forenames>Dennis</forenames></author><author><keyname>Wiesel</keyname><forenames>Ami</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Marginal Likelihoods for Distributed Parameter Estimation of Gaussian
  Graphical Models</title><categories>stat.ML cs.LG</categories><doi>10.1109/TSP.2014.2350956</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider distributed estimation of the inverse covariance matrix, also
called the concentration or precision matrix, in Gaussian graphical models.
Traditional centralized estimation often requires global inference of the
covariance matrix, which can be computationally intensive in large dimensions.
Approximate inference based on message-passing algorithms, on the other hand,
can lead to unstable and biased estimation in loopy graphical models. In this
paper, we propose a general framework for distributed estimation based on a
maximum marginal likelihood (MML) approach. This approach computes local
parameter estimates by maximizing marginal likelihoods defined with respect to
data collected from local neighborhoods. Due to the non-convexity of the MML
problem, we introduce and solve a convex relaxation. The local estimates are
then combined into a global estimate without the need for iterative
message-passing between neighborhoods. The proposed algorithm is naturally
parallelizable and computationally efficient, thereby making it suitable for
high-dimensional problems. In the classical regime where the number of
variables $p$ is fixed and the number of samples $T$ increases to infinity, the
proposed estimator is shown to be asymptotically consistent and to improve
monotonically as the local neighborhood size increases. In the high-dimensional
scaling regime where both $p$ and $T$ increase to infinity, the convergence
rate to the true parameters is derived and is seen to be comparable to
centralized maximum likelihood estimation. Extensive numerical experiments
demonstrate the improved performance of the two-hop version of the proposed
estimator, which suffices to almost close the gap to the centralized maximum
likelihood estimator at a reduced computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4761</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4761</id><created>2013-03-19</created><authors><author><keyname>Thramboulidis</keyname><forenames>Kleanthis</forenames></author></authors><title>IEC 61499 vs. 61131: A Comparison Based on Misperceptions</title><categories>cs.SE</categories><comments>12 pages, 7 figures, submitted on 15 Jun 2012 to IEEE Transactions on
  Systems, Man, and Cybernetics--Part C: Applications and Reviews</comments><journal-ref>Journal of Software Engineering and Applications Vol. 6 No. 8,
  2013, pp. 405-415</journal-ref><doi>10.4236/jsea.2013.68050</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IEC 61131 has been widely accepted in the industrial automation domain.
However, it is claimed that the standard does not address today the new
requirements of complex industrial systems, which include among others,
portability, interoperability, increased reusability and distribution. To
address these restrictions, IEC has initiated the task of developing IEC 61499,
which is presented as a mature technology to enable intelligent automation in
various domains. This standard was not accepted by industry even though it is
highly promoted by the academic community. In this paper, a comparison between
the two standards is presented. We argue that IEC 61499 has been promoted by
academy based on unsubstantiated claims on its main features, i.e.,
reusability, portability, interoperability, event-driven execution. A number of
misperceptions are presented and discussed. Based on this, it is claimed that
IEC 61499 does not provide a solid framework for the next generation of
industrial automation systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4762</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4762</id><created>2013-03-17</created><authors><author><keyname>Peng</keyname><forenames>Tong</forenames></author><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Schmeink</keyname><forenames>Anke</forenames></author></authors><title>Minimum BER Power Adjustment and Receiver Design for Distributed
  Space-Time Coded Cooperative MIMO Relaying Systems</title><categories>cs.IT math.IT</categories><comments>3 figures; WSA 2012. arXiv admin note: text overlap with
  arXiv:1303.4120, arXiv:1303.4384</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An adaptive joint power allocation (JPA) and linear receiver design algorithm
using the minimum bit error rate (MBER) criterion for a cooperative
Multiple-Input Multiple-Output (MIMO) network is proposed. The system employs
multiple relays with Distributed Space-Time Coding (DSTC) schemes and an
Amplify-and-Forward (AF) strategy. It is designed according to a joint
constrained optimization algorithm to determine the MBER power allocation
parameters and the receive filter parameters for each transmitted symbol. The
simulation results indicate that the proposed algorithm obtains performance
gains compared to the equal power allocation systems and the minimum mean
square error (MMSE) designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4771</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4771</id><created>2013-03-19</created><authors><author><keyname>Baddi</keyname><forenames>Youssef</forenames></author><author><keyname>Kettani</keyname><forenames>Mohamed Dafir Ech-Cherif El</forenames></author></authors><title>Delay and Delay Variation Constrained Algorithm based on VNS Algorithm
  for RP Management in Mobile IPv6</title><categories>cs.NI</categories><journal-ref>International Journal of Computer Science Issues IJCSI, Vol. 9,
  Issue 6, No 1, November 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On account of the progress of network multimedia technology, more and more
real-time multimedia applications arrive with the need to transmit information
using multicast communication. These applications are more important with the
arrival of mobile IPv6 protocol through mobile receivers and sources. These
applications require a multicast routing protocol which has packets arriving at
multicast receptors within a specified QoS guaranteed and a quick recovery
mechanism. When multicasting with mobile IPv6, the mobility of a receivers and
senders may lead to serious problems. When the receiver or sender moves, the
quality of full multicast tree may degrade so that multicast datagrams cannot
be forwarded efficiently. D2V-RPM (delay and delay variation RP Manager)
problem consist in choosing an optimal multicast router in the network as the
root of the shared multicast tree (ST) within a specified delay and delay
variation bound for all multicast session, and recovering this RP if it's not
optimal or it has failed. This NP hard problem needs to be solved through a
heuristic algorithm. In this paper, we propose a new RP Management algorithm
based on Variable Neighborhood Search algorithm, based on a systematic
neighborhood changing. D2VVNS-RPM algorithm selects and recovers the RP router
by considering tree cost, delay and delay variation. Simulation results show
that good performance is achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4776</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4776</id><created>2013-03-19</created><authors><author><keyname>Ouyang</keyname><forenames>Wenzhuo</forenames></author><author><keyname>Prasad</keyname><forenames>Narayan</forenames></author><author><keyname>Rangarajan</keyname><forenames>Sampath</forenames></author></authors><title>Exploiting Hybrid Channel Information for Downlink Multi-User MIMO
  Scheduling</title><categories>cs.IT math.IT</categories><comments>Expanded version: Accepted WiOpt 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the downlink multi-user MIMO (MU-MIMO) scheduling problem in
the presence of imperfect Channel State Information at the transmitter (CSIT)
that comprises of coarse and current CSIT as well as finer but delayed CSIT.
This scheduling problem is characterized by an intricate `exploitation -
exploration tradeoff' between scheduling the users based on current CSIT for
immediate gains, and scheduling them to obtain finer albeit delayed CSIT and
potentially larger future gains. We solve this scheduling problem by
formulating a frame based joint scheduling and feedback approach, where in each
frame a policy is obtained as the solution to a Markov Decision Process. We
prove that our proposed approach can be made arbitrarily close to the optimal
and then demonstrate its significant gains over conventional MU-MIMO
scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4778</identifier>
 <datestamp>2015-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4778</id><created>2013-03-19</created><updated>2013-07-03</updated><authors><author><keyname>Dyer</keyname><forenames>Eva L.</forenames></author><author><keyname>Sankaranarayanan</keyname><forenames>Aswin C.</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Greedy Feature Selection for Subspace Clustering</title><categories>cs.LG math.NA stat.ML</categories><comments>32 pages, 7 figures, 1 table</comments><journal-ref>Vol.14, Issue 1, pp. 2487-2517, January 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unions of subspaces provide a powerful generalization to linear subspace
models for collections of high-dimensional data. To learn a union of subspaces
from a collection of data, sets of signals in the collection that belong to the
same subspace must be identified in order to obtain accurate estimates of the
subspace structures present in the data. Recently, sparse recovery methods have
been shown to provide a provable and robust strategy for exact feature
selection (EFS)--recovering subsets of points from the ensemble that live in
the same subspace. In parallel with recent studies of EFS with L1-minimization,
in this paper, we develop sufficient conditions for EFS with a greedy method
for sparse signal recovery known as orthogonal matching pursuit (OMP).
Following our analysis, we provide an empirical study of feature selection
strategies for signals living on unions of subspaces and characterize the gap
between sparse recovery methods and nearest neighbor (NN)-based approaches. In
particular, we demonstrate that sparse recovery methods provide significant
advantages over NN methods and the gap between the two approaches is
particularly pronounced when the sampling of subspaces in the dataset is
sparse. Our results suggest that OMP may be employed to reliably recover exact
feature sets in a number of regimes where NN approaches fail to reveal the
subspace membership of points in the ensemble.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4782</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4782</id><created>2013-03-19</created><authors><author><keyname>Park</keyname><forenames>Seok-Hwan</forenames><affiliation>Shitz</affiliation></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Sahin</keyname><forenames>Onur</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Multi-Layer Hybrid-ARQ for an Out-of-Band Relay Channel</title><categories>cs.IT math.IT</categories><comments>A shorter version of the paper has been submitted to PIMRC 13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses robust communication on a fading relay channel in which
the relay is connected to the decoder via an out-of-band digital link of
limited capacity. Both the source-to-relay and the source-to-destination links
are subject to fading gains, which are generally unknown to the encoder prior
to transmission. To overcome this impairment, a hybrid automatic retransmission
request (HARQ) protocol is combined with multi-layer broadcast transmission,
thus allowing for variable-rate decoding. Moreover, motivated by cloud radio
access network applications, the relay operation is limited to
compress-and-forward. The aim is maximizing the throughput performance as
measured by the average number of successfully received bits per channel use,
under either long-term static channel (LTSC) or short-term static channel
(STSC) models. In order to opportunistically leverage better channel states
based on the HARQ feedback from the decoder, an adaptive compression strategy
at the relay is also proposed. Numerical results confirm the effectiveness of
the proposed strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4788</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4788</id><created>2013-03-19</created><authors><author><keyname>Munoz-Canavate</keyname><forenames>Antonio</forenames></author><author><keyname>Hipola</keyname><forenames>Pedro</forenames></author></authors><title>Business information through Spain's Chambers of Commerce: meeting
  business needs</title><categories>cs.DL</categories><journal-ref>Munoz-Canavate, Antonio; Hipola Pedro. Business information
  through Spain's Chambers of Commerce: meeting business needs. Business
  Information Review. vol. 25, 4, December 2008, pp. 224-229. DOI:
  10.1177/0266382108098866</journal-ref><doi>10.1177/0266382108098866</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From different public and private instances, mechanisms have been set in
action that allow for companies to obtain information in order to make
decisions with a stronger foundation. This article is focused on the
description of an entire information system for the business world, developed
in the realm of the Chambers of Commerce of Spain, which have given rise to the
creation of an authentic network of inter-chamber information.
  In Spain, the obligatory membership of businesses to the Chambers of Commerce
in their geographic areas, and therefore the compulsory payment of member
quotas, has traditionally generated some polemics, above all because many firms
have not perceived a material usefulness of the services offered by these
Chambers.
  Notwithstanding, the 85 Chambers currently existing in Spain, as well as the
organism that coordinates them -the Upper Council or Consejo Superior de
Camaras de Comercio- and the company created expressly to commercialize
informational services online, Camerdata, have developed genuinely informative
tools that cover a good part of the informational demands that a business might
claim, described here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4803</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4803</id><created>2013-03-19</created><authors><author><keyname>Li</keyname><forenames>Xi</forenames></author><author><keyname>Hu</keyname><forenames>Weiming</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Zhang</keyname><forenames>Zhongfei</forenames></author><author><keyname>Dick</keyname><forenames>Anthony</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>A Survey of Appearance Models in Visual Object Tracking</title><categories>cs.CV</categories><comments>Appearing in ACM Transactions on Intelligent Systems and Technology,
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual object tracking is a significant computer vision task which can be
applied to many domains such as visual surveillance, human computer
interaction, and video compression. In the literature, researchers have
proposed a variety of 2D appearance models. To help readers swiftly learn the
recent advances in 2D appearance models for visual object tracking, we
contribute this survey, which provides a detailed review of the existing 2D
appearance models. In particular, this survey takes a module-based architecture
that enables readers to easily grasp the key points of visual object tracking.
In this survey, we first decompose the problem of appearance modeling into two
different processing stages: visual representation and statistical modeling.
Then, different 2D appearance models are categorized and discussed with respect
to their composition modules. Finally, we address several issues of interest as
well as the remaining challenges for future research on this topic. The
contributions of this survey are four-fold. First, we review the literature of
visual representations according to their feature-construction mechanisms
(i.e., local and global). Second, the existing statistical modeling schemes for
tracking-by-detection are reviewed according to their model-construction
mechanisms: generative, discriminative, and hybrid generative-discriminative.
Third, each type of visual representations or statistical modeling techniques
is analyzed and discussed from a theoretical or practical viewpoint. Fourth,
the existing benchmark resources (e.g., source code and video datasets) are
examined in this survey.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4808</identifier>
 <datestamp>2014-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4808</id><created>2013-03-19</created><updated>2013-11-01</updated><authors><author><keyname>Ooms</keyname><forenames>Jeroen</forenames></author></authors><title>The RAppArmor Package: Enforcing Security Policies in R Using Dynamic
  Sandboxing on Linux</title><categories>cs.CR cs.MS stat.CO</categories><journal-ref>Journal of Statistical Software, Vol. 55, Issue 7, Nov 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing availability of cloud computing and scientific super computers
brings great potential for making R accessible through public or shared
resources. This allows us to efficiently run code requiring lots of cycles and
memory, or embed R functionality into, e.g., systems and web services. However
some important security concerns need to be addressed before this can be put in
production. The prime use case in the design of R has always been a single
statistician running R on the local machine through the interactive console.
Therefore the execution environment of R is entirely unrestricted, which could
result in malicious behavior or excessive use of hardware resources in a shared
environment. Properly securing an R process turns out to be a complex problem.
We describe various approaches and illustrate potential issues using some of
our personal experiences in hosting public web services. Finally we introduce
the RAppArmor package: a Linux based reference implementation for dynamic
sandboxing in R on the level of the operating system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4814</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4814</id><created>2013-03-19</created><authors><author><keyname>Sen</keyname><forenames>Jaydip</forenames></author></authors><title>Security and Privacy Issues in Cloud Computing</title><categories>cs.CR cs.NI</categories><comments>42 pages, 2 Figures, and 5 Tables. The book chapter is accepted for
  publication and is expected to be published in the second half of 2013</comments><journal-ref>Book Chapter in the book: &quot;Architectures and Protocols for Secure
  Information Technology&quot;, Ruiz-Martinez, Pereniguez-Garcia, and Marin-Lopez
  (Eds.), IGI-Global, USA, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing transforms the way information technology (IT) is consumed
and managed, promising improved cost efficiencies, accelerated innovation,
faster time-to-market, and the ability to scale applications on demand
(Leighton, 2009). According to Gartner, while the hype grew exponentially
during 2008 and continued since, it is clear that there is a major shift
towards the cloud computing model and that the benefits may be substantial
(Gartner Hype-Cycle, 2012). However, as the shape of the cloud computing is
emerging and developing rapidly both conceptually and in reality, the
legal/contractual, economic, service quality, interoperability, security and
privacy issues still pose significant challenges. In this chapter, we describe
various service and deployment models of cloud computing and identify major
challenges. In particular, we discuss three critical challenges: regulatory,
security and privacy issues in cloud computing. Some solutions to mitigate
these challenges are also proposed along with a brief presentation on the
future trends in cloud computing deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4816</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4816</id><created>2013-03-19</created><updated>2013-03-20</updated><authors><author><keyname>Li</keyname><forenames>Yongkun</forenames></author><author><keyname>Lee</keyname><forenames>Patrick P. C.</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author></authors><title>Stochastic Modeling of Large-Scale Solid-State Storage Systems:
  Analysis, Design Tradeoffs and Optimization</title><categories>cs.PF</categories><comments>14 pages, Sigmetrics 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solid state drives (SSDs) have seen wide deployment in mobiles, desktops, and
data centers due to their high I/O performance and low energy consumption. As
SSDs write data out-of-place, garbage collection (GC) is required to erase and
reclaim space with invalid data. However, GC poses additional writes that
hinder the I/O performance, while SSD blocks can only endure a finite number of
erasures. Thus, there is a performance-durability tradeoff on the design space
of GC. To characterize the optimal tradeoff, this paper formulates an
analytical model that explores the full optimal design space of any GC
algorithm. We first present a stochastic Markov chain model that captures the
I/O dynamics of large-scale SSDs, and adapt the mean-field approach to derive
the asymptotic steady-state performance. We further prove the model convergence
and generalize the model for all types of workload. Inspired by this model, we
propose a randomized greedy algorithm (RGA) that can operate along the optimal
tradeoff curve with a tunable parameter. Using trace-driven simulation on
DiskSim with SSD add-ons, we demonstrate how RGA can be parameterized to
realize the performance-durability tradeoff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4823</identifier>
 <datestamp>2013-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4823</id><created>2013-03-19</created><updated>2013-08-01</updated><authors><author><keyname>Compagno</keyname><forenames>Alberto</forenames></author><author><keyname>Conti</keyname><forenames>Mauro</forenames></author><author><keyname>Gasti</keyname><forenames>Paolo</forenames></author><author><keyname>Tsudik</keyname><forenames>Gene</forenames></author></authors><title>Poseidon: Mitigating Interest Flooding DDoS Attacks in Named Data
  Networking</title><categories>cs.NI cs.CR</categories><comments>The IEEE Conference on Local Computer Networks (LCN 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content-Centric Networking (CCN) is an emerging networking paradigm being
considered as a possible replacement for the current IP-based host-centric
Internet infrastructure. In CCN, named content becomes a first-class entity.
CCN focuses on content distribution, which dominates current Internet traffic
and is arguably not well served by IP. Named-Data Networking (NDN) is an
example of CCN. NDN is also an active research project under the NSF Future
Internet Architectures (FIA) program. FIA emphasizes security and privacy from
the outset and by design. To be a viable Internet architecture, NDN must be
resilient against current and emerging threats. This paper focuses on
distributed denial-of-service (DDoS) attacks; in particular we address interest
flooding, an attack that exploits key architectural features of NDN. We show
that an adversary with limited resources can implement such attack, having a
significant impact on network performance. We then introduce Poseidon: a
framework for detecting and mitigating interest flooding attacks. Finally, we
report on results of extensive simulations assessing proposed countermeasure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4839</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4839</id><created>2013-03-20</created><authors><author><keyname>Parwej</keyname><forenames>Dr. Firoj</forenames></author></authors><title>The State of the Art Recognize in Arabic Script through Combination of
  Online and Offline</title><categories>cs.CV</categories><comments>Pages 7, Figure 6, Table 2. arXiv admin note: text overlap with
  arXiv:1110.1488 by other authors</comments><journal-ref>International Journal of Computer Science and Telecommunications
  (IJCST), UK, London (http://www.ijcst.org) , ISSN 2047-3338 , Vol. 4, Issue
  3, pages 60 - 66, March 2013. Link - http://www.ijcst.org/Volume4/Issue3.html</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Handwriting recognition refers to the identification of written characters.
Handwriting recognition has become an acute research area in recent years for
the ease of access of computer science. In this paper primarily discussed
On-line and Off-line handwriting recognition methods for Arabic words which are
often used among then across the Middle East and North Africa People. Arabic
word online handwriting recognition is a very challenging task due to its
cursive nature. Because of the characteristic of the whole body of the Arabic
script, namely connectivity between the characters, thereby the segmentation of
An Arabic script is very difficult. In this paper we introduced an Arabic
script multiple classifier system for recognizing notes written on a Starboard.
This Arabic script multiple classifier system combines one off-line and on-line
handwriting recognition systems. The Arabic script recognizers are all based on
Hidden Markov Models but vary in the way of preprocessing and normalization. To
combine the Arabic script output sequences of the recognizers, we incrementally
align the word sequences using a norm string matching algorithm. The Arabic
script combination we could increase the system performance over the excellent
character recognizer by about 3%. The proposed technique is also the necessary
step towards character recognition, person identification, personality
determination where input data is processed from all perspectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4840</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4840</id><created>2013-03-20</created><authors><author><keyname>Polkovnikov</keyname><forenames>Igor</forenames></author></authors><title>Asynchronous Cellular Operations on Gray Images Extracting Topographic
  Shape Features and Their Relations</title><categories>cs.CV</categories><comments>19 pages, 37 figures, 10 function classes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A variety of operations of cellular automata on gray images is presented. All
operations are of a wave-front nature finishing in a stable state. They are
used to extract shape descripting gray objects robust to a variety of pattern
distortions. Topographic terms are used: &quot;lakes&quot;, &quot;dales&quot;, &quot;dales of dales&quot;. It
is shown how mutual object relations like &quot;above&quot; can be presented in terms of
gray image analysis and how it can be used for character classification and for
gray pattern decomposition. Algorithms can be realized with a parallel
asynchronous architecture. Keywords: Pattern Recognition, Mathematical
Morphology, Cellular Automata, Wave-front Algorithms, Gray Image Analysis,
Topographical Shape Descriptors, Asynchronous Parallel Processors, Holes,
Cavities, Concavities, Graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4845</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4845</id><created>2013-03-20</created><updated>2013-03-27</updated><authors><author><keyname>Ho</keyname><forenames>Myong-Song</forenames></author><author><keyname>Ju</keyname><forenames>Gwang-Hui</forenames></author><author><keyname>O</keyname><forenames>Yong-Bom</forenames></author><author><keyname>Jong</keyname><forenames>Gwang-Ho</forenames></author></authors><title>On Constructing the Value Function for Optimal Trajectory Problem and
  its Application to Image Processing</title><categories>cs.CV</categories><comments>5 pages, presented in International Symposium in Commemoration of the
  65th Anniversary of the Foundation of Kim Il Sung University
  (Mathematics)20-21. Sep. Juche100(2011), Pyongyang DPR Korea; v2:added
  authors and revised introduction</comments><report-no>KISU-MATH-2011-E-C-016</report-no><journal-ref>International Symposium in Commemoration of the 65th Anniversary
  of the Foundation of Kim Il Sung University (Mathematics)20-21. Sep.
  Juche100(2011), Pyongyang DPR Korea, 120-124</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We proposed an algorithm for solving Hamilton-Jacobi equation associated to
an optimal trajectory problem for a vehicle moving inside the pre-specified
domain with the speed depending upon the direction of the motion and current
position of the vehicle. The dynamics of the vehicle is defined by an ordinary
differential equation, the right hand of which is given by product of control(a
time dependent fuction) and a function dependent on trajectory and control. At
some unspecified terminal time, the vehicle reaches the boundary of the
pre-specified domain and incurs a terminal cost. We also associate the
traveling cost with a type of integral to the trajectory followed by vehicle.
We are interested in a numerical method for finding a trajectory that minimizes
the sum of the traveling cost and terminal cost. We developed an algorithm
solving the value function for general trajectory optimization problem. Our
algorithm is closely related to the Tsitsiklis's Fast Marching Method and J. A.
Sethian's OUM and SLF-LLL[1-4] and is a generalization of them. On the basis of
these results, We applied our algorithm to the image processing such as
fingerprint verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4854</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4854</id><created>2013-03-20</created><authors><author><keyname>Kim</keyname><forenames>Chung-Song</forenames></author><author><keyname>Kim</keyname><forenames>Chol-Hun</forenames></author></authors><title>Study On Universal Lossless Data Compression by using Context Dependence
  Multilevel Pattern Matching Grammar Transform</title><categories>cs.DM cs.IT math.IT</categories><comments>5 pages, presented in International Symposium in Commemoration of the
  65th Anniversary of the Foundation of Kim Il Sung University (Mathematics),
  20-21. Sep. Juche100(2011) Pyongyang DPR Korea</comments><report-no>KISU-MATH-2011-E-C-015</report-no><journal-ref>International Symposium in Commemoration of the 65th Anniversary
  of the Foundation of Kim Il Sung University (Mathematics), 20-21. Sep.
  Juche100(2011) Pyongyang DPR Korea, 115-119</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the context dependence multilevel pattern matching(in short
CDMPM) grammar transform is proposed; based on this grammar transform, the
universal lossless data compression algorithm, CDMPM code is then developed.
Moreover we get a upper bound of this algorithms' worst case redundancy among
all individual sequences of length n from a finite alphabet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4866</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4866</id><created>2013-03-20</created><authors><author><keyname>Chadha</keyname><forenames>Ankit R.</forenames></author><author><keyname>Satam</keyname><forenames>Neha S.</forenames></author></authors><title>A Robust Rapid Approach to Image Segmentation with Optimal Thresholding
  and Watershed Transform</title><categories>cs.CV</categories><journal-ref>International Journal of Computer Applications (2013)</journal-ref><doi>10.5120/10949-5908</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a novel method for partitioning image into meaningful
segments. The proposed method employs watershed transform, a well-known image
segmentation technique. Along with that, it uses various auxiliary schemes such
as Binary Gradient Masking, dilation which segment the image in proper way. The
algorithm proposed in this paper considers all these methods in effective way
and takes little time. It is organized in such a manner so that it operates on
input image adaptively. Its robustness and efficiency makes it more convenient
and suitable for all types of images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4869</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4869</id><created>2013-03-20</created><authors><author><keyname>Niemann</keyname><forenames>Raik</forenames></author><author><keyname>Korfiatis</keyname><forenames>Nikolaos</forenames></author><author><keyname>Zicari</keyname><forenames>Roberto</forenames></author><author><keyname>G&#xf6;bel</keyname><forenames>Richard</forenames></author></authors><title>Does query performance optimization lead to energy efficiency? A
  comparative analysis of energy efficiency of database operations under
  different workload scenarios</title><categories>cs.DB</categories><comments>10 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the continuous increase of online services as well as energy costs,
energy consumption becomes a significant cost factor for the evaluation of data
center operations. A significant contributor to that is the performance of
database servers which are found to constitute the backbone of online services.
From a software approach, while a set of novel data management technologies
appear in the market e.g. key-value based or in-memory databases, classic
relational database management systems (RDBMS) are still widely used. In
addition from a hardware perspective, the majority of database servers is still
using standard magnetic hard drives (HDDs) instead of solid state drives (SSDs)
due to lower cost of storage per gigabyte, disregarding the performance boost
that might be given due to high cost.
  In this study we focus on a software based assessment of the energy
consumption of a database server by running three different and complete
database workloads namely TCP-H, Star Schema Benchmark -SSB as well a modified
benchmark we have derived for this study called W22. We profile the energy
distribution among the ost important server components and by using different
resource allocation we assess the energy consumption of a typical open source
RDBMS (PostgreSQL) on a standard server in relation with its performance
(measured by query time).
  Results confirm the well-known fact that even for complete workloads,
optimization of the RDBMS results to lower energy consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4892</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4892</id><created>2013-03-20</created><authors><author><keyname>Poss</keyname><forenames>Raphael</forenames></author></authors><title>On whether and how D-RISC and Microgrids can be kept relevant
  (self-assessment report)</title><categories>cs.AR</categories><comments>45 pages, 5 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report lays flat my personal views on D-RISC and Microgrids as of March
2013. It reflects the opinions and insights that I have gained from working on
this project during the period 2008-2013. This report is structed in two parts:
deconstruction and reconstruction. In the deconstruction phase, I review what I
believe are the fundamental motivation and goals of the D-RISC/Microgrids
enterprise, and identify what I judge are shortcomings: that the project did
not deliver on its expectations, that fundamental questions are left
unanswered, and that its original motivation may not even be relevant in
scientific research any more in this day and age. In the reconstruction phase,
I start by identifying the merits of the current D-RISC/Microgrids technology
and know-how taken at face value, re-motivate its existence from a different
angle, and suggest new, relevant research questions that could justify
continued scientific investment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4893</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4893</id><created>2013-03-20</created><updated>2014-01-10</updated><authors><author><keyname>Horvat</keyname><forenames>Marko</forenames></author><author><keyname>Popovi&#x107;</keyname><forenames>Sini&#x161;a</forenames></author><author><keyname>&#x106;osi&#x107;</keyname><forenames>Kre&#x161;imir</forenames></author></authors><title>Multimedia stimuli databases usage patterns: a survey report</title><categories>cs.MM cs.HC</categories><comments>5 pages, 2 figures</comments><journal-ref>Proceedings of the 36nd International ICT Convention MIPRO 2013,
  pp. 993-997, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multimedia documents such as images, sounds or videos can be used to elicit
emotional responses in exposed human subjects. These stimuli are stored in
affective multimedia databases and successfully used for a wide variety of
research in affective computing, human-computer interaction and cognitive
sciences. Affective multimedia databases are simple repositories of multimedia
documents with annotated high-level semantics and affective content. Although
important all affective multimedia databases have numerous deficiencies which
impair their applicability. To establish a better understanding of how experts
use affective multimedia databases an online survey was conducted into the
subject. The survey results are statistically significant and indicate that
contemporary databases lack stimuli with rich semantic and emotional content.
73.33% of survey participants find the databases lacking at least some
important semantic or emotion content. Most of the participants consider
stimuli descriptions to be inadequate. Overall, 1-2h or more than 24h are
generally needed to construct a single stimulation sequence. Almost 84% of the
survey participants would like to use real-life videos in their research.
Experts unequivocally recognize the need for an intelligent stimuli retrieval
application that would assist them in experimentation. Almost all experts agree
such applications could be useful in their work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4897</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4897</id><created>2013-03-20</created><authors><author><keyname>Chekuri</keyname><forenames>Chandra</forenames></author><author><keyname>Naves</keyname><forenames>Guyslain</forenames></author><author><keyname>Shepherd</keyname><forenames>F. Bruce</forenames></author></authors><title>Maximum Edge-Disjoint Paths in $k$-sums of Graphs</title><categories>cs.DM cs.DS math.CO</categories><msc-class>05C21, 05C83, 05C85, 68W25</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the approximability of the maximum edge-disjoint paths problem
(MEDP) in undirected graphs, and in particular, the integrality gap of the
natural multicommodity flow based relaxation for it. The integrality gap is
known to be $\Omega(\sqrt{n})$ even for planar graphs due to a simple
topological obstruction and a major focus, following earlier work, has been
understanding the gap if some constant congestion is allowed.
  In this context, it is natural to ask for which classes of graphs does a
constant-factor constant-congestion property hold. It is easy to deduce that
for given constant bounds on the approximation and congestion, the class of
&quot;nice&quot; graphs is nor-closed. Is the converse true? Does every proper
minor-closed family of graphs exhibit a constant factor, constant congestion
bound relative to the LP relaxation? We conjecture that the answer is yes.
  One stumbling block has been that such bounds were not known for bounded
treewidth graphs (or even treewidth 3). In this paper we give a polytime
algorithm which takes a fractional routing solution in a graph of bounded
treewidth and is able to integrally route a constant fraction of the LP
solution's value. Note that we do not incur any edge congestion. Previously
this was not known even for series parallel graphs which have treewidth 2. The
algorithm is based on a more general argument that applies to $k$-sums of
graphs in some graph family, as long as the graph family has a constant factor,
constant congestion bound. We then use this to show that such bounds hold for
the class of $k$-sums of bounded genus graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4899</identifier>
 <datestamp>2013-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4899</id><created>2013-03-20</created><updated>2013-07-04</updated><authors><author><keyname>Borello</keyname><forenames>Martino</forenames></author><author><keyname>Volta</keyname><forenames>Francesca Dalla</forenames></author><author><keyname>Nebe</keyname><forenames>Gabriele</forenames></author></authors><title>The automorphism group of a self-dual [72,36,16] code does not contain
  S_3, A_4, or D_8</title><categories>cs.IT math.CO math.IT</categories><comments>9 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A computer calculation with Magma shows that there is no extremal self-dual
binary code C of length 72, whose automorphism group contains the symmetric
group of degree 3, the alternating group of degree 4 or the dihedral group of
order 8. Combining this with the known results in the literature one obtains
that Aut(C) has order at most 5 or isomorphic to the elementary abelian group
of order 8.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4905</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4905</id><created>2013-03-20</created><updated>2013-03-24</updated><authors><author><keyname>Fionda</keyname><forenames>Valeria</forenames></author><author><keyname>Gutierrez</keyname><forenames>Claudio</forenames></author><author><keyname>Pirr&#xf3;</keyname><forenames>Giuseppe</forenames></author></authors><title>Web Maps and Their Algebra</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A map is an abstract visual representation of a region, taken from a given
space, usually designed for final human consumption. Traditional cartography
focuses on the mapping of Euclidean spaces by using some distance metric. In
this paper we aim at mapping the Web space by leveraging its relational nature.
We introduce a general mathematical framework for maps and an algebra and
discuss the feasibility of maps suitable for interpretation not only by humans
but also by machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4920</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4920</id><created>2013-03-20</created><updated>2013-03-25</updated><authors><author><keyname>Dougherty</keyname><forenames>Steven T</forenames></author><author><keyname>Karadeniz</keyname><forenames>Suat</forenames></author><author><keyname>Yildiz</keyname><forenames>Bahattin</forenames></author></authors><title>The automorphism group of the doubly-even [72,36,16] code can only be of
  order 1, 3 or 5</title><categories>math.CO cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  Lemma 2.2 that affects all the results</comments><msc-class>94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that a putative $[72,36,16]$ code is not the image of linear code
over $\ZZ_4$, $\FF_2 + u \FF_2$ or $\FF_2+v\FF_2$, thus proving that the
extremal doubly even $[72,36,16]$-binary code cannot have an automorphism group
containing a fixed point-free involution. Combining this with the previously
proved result by Bouyuklieva that such a code cannot have an automorphism group
containing an involution with fixed points, we conclude that the automorphism
group of the $[72,36,16]$-code cannot be of even order, leaving 3 and 5 as the
only possibilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4924</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4924</id><created>2013-03-20</created><updated>2013-12-17</updated><authors><author><keyname>Shi</keyname><forenames>Lei</forenames></author><author><keyname>Obregon</keyname><forenames>Evanny</forenames></author><author><keyname>Sung</keyname><forenames>Ki Won</forenames></author><author><keyname>Zander</keyname><forenames>Jens</forenames></author><author><keyname>Bostrom</keyname><forenames>Jan</forenames></author></authors><title>CellTV - on the Benefit of TV Distribution over Cellular Networks A Case
  Study</title><categories>cs.NI</categories><comments>To appear on Trans. Broadcasting 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As mobile IP-access is becoming the dominant technology for providing
wireless services, the demand for more spectrum for this type of access is
increasing rapidly. Since IP-access can be used for all types of services,
instead of a plethora of dedicated, single-service systems, there is a
significant potential to make spectrum use more efficient. In this paper, the
feasibility and potential benefit of replacing the current terrestrial UHF TV
broadcasting system with a mobile, cellular data (IP-) network is analyzed. In
the cellular network, TV content would be provided as {one} of the services,
here referred to as CellTV. In the investigation we consider typical Swedish
rural and urban environments. We use different models for TV viewing patterns
and cellular technologies as expected in the year 2020. Results of the
quantitative analysis indicate that CellTV distribution can be beneficial if
the TV consumption trend goes towards more specialized programming, more local
contents, and more on-demand requests. Mobile cellular systems, with their
flexible unicast capabilities, will be an ideal platform to provide these
services. However, the results also demonstrate that CellTV is not a
spectrum-efficient replacement for terrestrial TV broadcasting with current
viewing patterns (i.e. a moderate number of channels with each a high numbers
of viewers). In this case, it is doubtful whether the expected spectrum savings
can motivate the necessary investments in upgrading cellular sites and
developing advanced TV receiver required for the success of CellTV
distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4928</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4928</id><created>2013-03-20</created><updated>2013-04-09</updated><authors><author><keyname>Dierkes</keyname><forenames>Thomas</forenames></author><author><keyname>R&#xf6;blitz</keyname><forenames>Susanna</forenames></author><author><keyname>Wade</keyname><forenames>Moritz</forenames></author><author><keyname>Deuflhard</keyname><forenames>Peter</forenames></author></authors><title>Parameter identification in large kinetic networks with BioPARKIN</title><categories>cs.MS cs.CE q-bio.QM</categories><comments>20 pages, 7 figures, 4 tables; added 1 figure, and revised section 4</comments><msc-class>65L09 (Primary) 49M15, 65C20, 65L04, 65L80, 92C42 (Secondary)</msc-class><acm-class>G.1.6; G.1.7; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modelling, parameter identification, and simulation play an important role in
systems biology. Usually, the goal is to determine parameter values that
minimise the difference between experimental measurement values and model
predictions in a least-squares sense. Large-scale biological networks, however,
often suffer from missing data for parameter identification. Thus, the
least-squares problems are rank-deficient and solutions are not unique. Many
common optimisation methods ignore this detail because they do not take into
account the structure of the underlying inverse problem. These algorithms
simply return a &quot;solution&quot; without additional information on identifiability or
uniqueness. This can yield misleading results, especially if parameters are
co-regulated and data are noisy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4942</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4942</id><created>2013-03-20</created><authors><author><keyname>Eswaran</keyname><forenames>K.</forenames></author></authors><title>A Novel Algorithm for Linear Programming</title><categories>cs.NA math.OC</categories><msc-class>F.2.1</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of optimizing a linear objective function,given a number of
linear constraints has been a long standing problem ever since the times of
Kantorovich, Dantzig and von Neuman. These developments have been followed by a
different approach pioneered by Khachiyan and Karmarkar.
  In this paper we present an entirely new method for solving an old
optimization problem in a novel manner, a technique that reduces the dimension
of the problem step by step and interestingly is recursive. A theorem which
proves the correctness of the approach is given.
  The method can be extended to other types of optimization problems in convex
space, e.g. for solving a linear optimization problem subject to nonlinear
constraints in a convex region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4949</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4949</id><created>2013-03-20</created><authors><author><keyname>Varesano</keyname><forenames>Fabio</forenames></author></authors><title>FreeIMU: An Open Hardware Framework for Orientation and Motion Sensing</title><categories>cs.ET cs.AR cs.HC</categories><comments>10 pages, 1 figure</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Orientation and Motion Sensing are widely implemented on various consumer
products, such as mobile phones, tablets and cameras as they enable immediate
interaction with virtual information. The prototyping phase of any orientation
and motion sensing capable device is however a quite difficult process as it
may involve complex hardware designing, math algorithms and programming.
  In this paper, we present FreeIMU, an Open Hardware Framework for prototyping
orientation and motion sensing capable devices. The framework consists in a
small circuit board containing various sensors and a software library, built on
top of the Arduino platform. Both the hardware and library are released under
open licences and supported by an active community allowing to be implemented
into research and commercial projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4959</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4959</id><created>2013-03-20</created><authors><author><keyname>Otero-Espinar</keyname><forenames>Victoria</forenames></author><author><keyname>Seoane</keyname><forenames>Lu&#xed;s F.</forenames></author><author><keyname>Nieto</keyname><forenames>Juan J.</forenames></author><author><keyname>Mira</keyname><forenames>Jorge</forenames></author></authors><title>Analytic solution of a model of language competition with bilingualism
  and interlinguistic similarity</title><categories>physics.soc-ph cs.CL</categories><comments>3 pages, 5 figures</comments><doi>10.1016/j.physd.2013.08.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An in-depth analytic study of a model of language dynamics is presented: a
model which tackles the problem of the coexistence of two languages within a
closed community of speakers taking into account bilingualism and incorporating
a parameter to measure the distance between languages. After previous numerical
simulations, the model yielded that coexistence might lead to survival of both
languages within monolingual speakers along with a bilingual community or to
extinction of the weakest tongue depending on different parameters. In this
paper, such study is closed with thorough analytical calculations to settle the
results in a robust way and previous results are refined with some
modifications. From the present analysis it is possible to almost completely
assay the number and nature of the equilibrium points of the model, which
depend on its parameters, as well as to build a phase space based on them.
Also, we obtain conclusions on the way the languages evolve with time. Our
rigorous considerations also suggest ways to further improve the model and
facilitate the comparison of its consequences with those from other approaches
or with real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4969</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4969</id><created>2013-03-20</created><updated>2013-03-25</updated><authors><author><keyname>Jones</keyname><forenames>Jeff</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Computation of the Travelling Salesman Problem by a Shrinking Blob</title><categories>cs.ET cs.CG</categories><comments>27 Pages, 13 Figures. 25-03-13: Amended typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Travelling Salesman Problem (TSP) is a well known and challenging
combinatorial optimisation problem. Its computational intractability has
attracted a number of heuristic approaches to generate satisfactory, if not
optimal, candidate solutions. In this paper we demonstrate a simple
unconventional computation method to approximate the Euclidean TSP using a
virtual material approach. The morphological adaptation behaviour of the
material emerges from the low-level interactions of a population of particles
moving within a diffusive lattice. A `blob' of this material is placed over a
set of data points projected into the lattice, representing TSP city locations,
and the blob is reduced in size over time. As the blob shrinks it
morphologically adapts to the configuration of the cities. The shrinkage
process automatically stops when the blob no longer completely covers all
cities. By manually tracing the perimeter of the blob a path between cities is
elicited corresponding to a TSP tour. Over 6 runs on 20 randomly generated
datasets of 20 cities this simple and unguided method found tours with a mean
best tour length of 1.04, mean average tour length of 1.07 and mean worst tour
length of 1.09 when expressed as a fraction of the minimal tour computed by an
exact TSP solver. We examine the insertion mechanism by which the blob
constructs a tour, note some properties and limitations of its performance, and
discuss the relationship between the blob TSP and proximity graphs which group
points on the plane. The method is notable for its simplicity and the spatially
represented mechanical mode of its operation. We discuss similarities between
this method and previously suggested models of human performance on the TSP and
suggest possibilities for further improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4970</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4970</id><created>2013-03-20</created><authors><author><keyname>Hermenier</keyname><forenames>Romain</forenames></author><author><keyname>Rossetto</keyname><forenames>Francesco</forenames></author><author><keyname>Berioli</keyname><forenames>Matteo</forenames></author></authors><title>On the Behavior of RObust Header Compression U-mode in Channels with
  Memory</title><categories>cs.NI</categories><comments>submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existing studies of RObust Header Compression (ROHC) have provided some
understanding for memoryless channel, but the behavior of ROHC for correlated
wireless channels is not well investigated in spite of its practical
importance. In this paper, the dependence of ROHC against its design parameters
for the Gilbert Elliot channel is studied by means of three analytical models.
A first more elaborated approach accurately predicts the behavior of the
protocol for the single RTP flow profile, while a simpler, analytically
tractable model yields clear and insightful mathematical relationships that
explain the qualitative trends of ROHC. The results are validated against a
real world implementation of this protocol. Moreover, a third model studies
also the less conventional yet practically relevant setting of multiple RTP
flows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4986</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4986</id><created>2013-03-20</created><authors><author><keyname>Magnani</keyname><forenames>Matteo</forenames></author><author><keyname>Micenkova</keyname><forenames>Barbora</forenames></author><author><keyname>Rossi</keyname><forenames>Luca</forenames></author></authors><title>Combinatorial Analysis of Multiple Networks</title><categories>cs.SI physics.soc-ph</categories><comments>17 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of complex networks has been historically based on simple graph
data models representing relationships between individuals. However, often
reality cannot be accurately captured by a flat graph model. This has led to
the development of multi-layer networks. These models have the potential of
becoming the reference tools in network data analysis, but require the parallel
development of specific analysis methods explicitly exploiting the information
hidden in-between the layers and the availability of a critical mass of
reference data to experiment with the tools and investigate the real-world
organization of these complex systems. In this work we introduce a real-world
layered network combining different kinds of online and offline relationships,
and present an innovative methodology and related analysis tools suggesting the
existence of hidden motifs traversing and correlating different representation
layers. We also introduce a notion of betweenness centrality for multiple
networks. While some preliminary experimental evidence is reported, our
hypotheses are still largely unverified, and in our opinion this calls for the
availability of new analysis methods but also new reference multi-layer social
network data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4994</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4994</id><created>2013-03-20</created><authors><author><keyname>Wegener</keyname><forenames>Albert</forenames></author></authors><title>Universal Numerical Encoder and Profiler Reduces Computing's Memory Wall
  with Software, FPGA, and SoC Implementations</title><categories>cs.OH</categories><comments>10 pages, 4 figures, 3 tables, 19 references</comments><acm-class>E.4; B.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the multicore era, the time to computational results is increasingly
determined by how quickly operands are accessed by cores, rather than by the
speed of computation per operand. From high-performance computing (HPC) to
mobile application processors, low multicore utilization rates result from the
slowness of accessing off-chip operands, i.e. the memory wall. The APplication
AXcelerator (APAX) universal numerical encoder reduces computing's memory wall
by compressing numerical operands (integers and floats), thereby decreasing CPU
access time by 3:1 to 10:1 as operands stream between memory and cores. APAX
encodes numbers using a low-complexity algorithm designed both for time series
sensor data and for multi-dimensional data, including images. APAX encoding
parameters are determined by a profiler that quantifies the uncertainty
inherent in numerical datasets and recommends encoding parameters reflecting
this uncertainty. Compatible software, FPGA, and systemon-chip (SoC)
implementations efficiently support encoding rates between 150 MByte/sec and
1.5 GByte/sec at low power. On 25 integer and floating-point datasets, we
achieved encoding rates between 3:1 and 10:1, with average correlation of
0.999959, while accelerating computational &quot;time to results.&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.4996</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.4996</id><created>2013-03-20</created><updated>2013-08-03</updated><authors><author><keyname>Ohlsson</keyname><forenames>Henrik</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Yang</keyname><forenames>Allen Y.</forenames></author><author><keyname>Sastry</keyname><forenames>S. Shankar</forenames></author></authors><title>Compressive Shift Retrieval</title><categories>cs.SY cs.IT math.IT stat.ML</categories><comments>Submitted to IEEE Transactions on Signal Processing. Accepted to the
  38th International Conference on Acoustics, Speech, and Signal Processing
  (ICASSP), Vancouver, Canada, May 2013</comments><doi>10.1109/TSP.2014.2332974</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical shift retrieval problem considers two signals in vector form
that are related by a shift. The problem is of great importance in many
applications and is typically solved by maximizing the cross-correlation
between the two signals. Inspired by compressive sensing, in this paper, we
seek to estimate the shift directly from compressed signals. We show that under
certain conditions, the shift can be recovered using fewer samples and less
computation compared to the classical setup. Of particular interest is shift
estimation from Fourier coefficients. We show that under rather mild conditions
only one Fourier coefficient suffices to recover the true shift.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5003</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5003</id><created>2013-03-20</created><authors><author><keyname>La Guardia</keyname><forenames>Giuliano G.</forenames></author></authors><title>Convolutional Codes: Techniques of Construction</title><categories>cs.IT math.IT quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show how to construct new convolutional codes from old ones
by applying the well-known techniques: puncturing, extending, expanding, direct
sum, the (u|u + v) construction and the product code construction. By applying
these methods, several new families of convolutional codes can be constructed.
As an example of code expansion, families of convolutional codes derived from
classical Bose- Chaudhuri-Hocquenghem (BCH), character codes and Melas codes
are constructed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5009</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5009</id><created>2013-03-20</created><authors><author><keyname>Michalski</keyname><forenames>Rados&#x142;aw</forenames></author><author><keyname>Br&#xf3;dka</keyname><forenames>Piotr</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Juszczyszyn</keyname><forenames>Krzysztof</forenames></author></authors><title>Quantifying Social Network Dynamics</title><categories>cs.SI physics.soc-ph</categories><comments>In proceedings of the 4th International Conference on Computational
  Aspects of Social Networks, CASoN 2012</comments><journal-ref>Michalski, R., Brodka, P., Kazienko, P., Juszczyszyn, K.:
  Quantifying Social Network Dynamics. IEEE Computer Society, pp. 59-64 (2012)</journal-ref><doi>10.1109/CASoN.2012.6412380</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamic character of most social networks requires to model evolution of
networks in order to enable complex analysis of theirs dynamics. The following
paper focuses on the definition of differences between network snapshots by
means of Graph Differential Tuple. These differences enable to calculate the
diverse distance measures as well as to investigate the speed of changes. Four
separate measures are suggested in the paper with experimental study on real
social network data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5016</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5016</id><created>2013-03-20</created><authors><author><keyname>Gilio</keyname><forenames>Angelo</forenames></author><author><keyname>Sanfilippo</keyname><forenames>Giuseppe</forenames></author></authors><title>Quasi Conjunction, Quasi Disjunction, T-norms and T-conorms:
  Probabilistic Aspects</title><categories>math.PR cs.AI</categories><journal-ref>Information Sciences, vol. 245, pp. 146 - 167, 2013</journal-ref><doi>10.1016/j.ins.2013.03.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We make a probabilistic analysis related to some inference rules which play
an important role in nonmonotonic reasoning. In a coherence-based setting, we
study the extensions of a probability assessment defined on $n$ conditional
events to their quasi conjunction, and by exploiting duality, to their quasi
disjunction. The lower and upper bounds coincide with some well known t-norms
and t-conorms: minimum, product, Lukasiewicz, and Hamacher t-norms and their
dual t-conorms. On this basis we obtain Quasi And and Quasi Or rules. These are
rules for which any finite family of conditional events p-entails the
associated quasi conjunction and quasi disjunction. We examine some cases of
logical dependencies, and we study the relations among coherence, inclusion for
conditional events, and p-entailment. We also consider the Or rule, where quasi
conjunction and quasi disjunction of premises coincide with the conclusion. We
analyze further aspects of quasi conjunction and quasi disjunction, by
computing probabilistic bounds on premises from bounds on conclusions. Finally,
we consider biconditional events, and we introduce the notion of an
$n$-conditional event. Then we give a probabilistic interpretation for a
generalized Loop rule. In an appendix we provide explicit expressions for the
Hamacher t-norm and t-conorm in the unitary hypercube.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5029</identifier>
 <datestamp>2013-11-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5029</id><created>2013-03-20</created><authors><author><keyname>Bandini</keyname><forenames>Stefania</forenames></author><author><keyname>Gorrini</keyname><forenames>Andrea</forenames></author><author><keyname>Vizzari</keyname><forenames>Giuseppe</forenames></author></authors><title>Towards an Integrated Approach to Crowd Analysis and Crowd Synthesis: a
  Case Study and First Results</title><categories>cs.MA physics.soc-ph</categories><comments>Preprint submitted to Pattern Recognition Letters March 20, 2013</comments><msc-class>68T42, 68U20</msc-class><doi>10.1016/j.patrec.2013.10.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studies related to crowds of pedestrians, both those of theoretical nature
and application oriented ones, have generally focused on either the analysis or
the synthesis of the phenomena related to the interplay between individual
pedestrians, each characterised by goals, preferences and potentially relevant
relationships with others, and the environment in which they are situated. The
cases in which these activities have been systematically integrated for a
mutual benefit are still very few compared to the corpus of crowd related
literature. This paper presents a case study of an integrated approach to the
definition of an innovative model for pedestrian and crowd simulation (on the
side of synthesis) that was actually motivated and supported by the analyses of
empirical data acquired from both experimental settings and observations in
real world scenarios. In particular, we will introduce a model for the adaptive
behaviour of pedestrians that are also members of groups, that strive to
maintain their cohesion even in difficult (e.g. high density) situations. The
paper will show how the synthesis phase also provided inputs to the analysis of
empirical data, in a virtuous circle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5039</identifier>
 <datestamp>2014-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5039</id><created>2013-03-20</created><updated>2014-04-01</updated><authors><author><keyname>Cherevichenko</keyname><forenames>George</forenames></author></authors><title>Is alpha-conversion easy?</title><categories>cs.LO math.LO</categories><comments>Bibliography was expanded, nothing else</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new lambda-calculus with explicit substitutions and named
variables. Renaming of bound variables in this calculus is explicit (there is a
special rewrite rule) and can be delayed. Contexts (environments) are not sets
or lists without multiplicity, but have a more complicated structure. There is
a natural order on the set of contexts. A &quot;set&quot; of free variables is not a set,
but a context in the new sense. New definitions simplify working with
alpha-conversion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5041</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5041</id><created>2013-03-20</created><updated>2014-01-20</updated><authors><author><keyname>Bouzidi</keyname><forenames>Yacine</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Lazard</keyname><forenames>Sylvain</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Pouget</keyname><forenames>Marc</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Rouillier</keyname><forenames>Fabrice</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author></authors><title>Separating linear forms for bivariate systems</title><categories>cs.SC cs.CG</categories><proxy>ccsd</proxy><report-no>RR-8261</report-no><journal-ref>N&amp;deg; RR-8261 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for computing a separating linear form of a system of
bivariate polynomials with integer coefficients, that is a linear combination
of the variables that takes different values when evaluated at distinct
(complex) solutions of the system. In other words, a separating linear form
defines a shear of the coordinate system that sends the algebraic system in
generic position, in the sense that no two distinct solutions are vertically
aligned. The computation of such linear forms is at the core of most algorithms
that solve algebraic systems by computing rational parameterizations of the
solutions and, moreover, the computation a separating linear form is the
bottleneck of these algorithms, in terms of worst-case bit complexity. Given
two bivariate polynomials of total degree at most $d$ with integer coefficients
of bitsize at most~$\tau$, our algorithm computes a separating linear form in
$\sOB(d^{8}+d^7\tau)$ bit operations in the worst case, where the previously
known best bit complexity for this problem was $\sOB(d^{10}+d^9\tau)$ (where
$\sO$ refers to the complexity where polylogarithmic factors are omitted and
$O_B$ refers to the bit complexity).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5042</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5042</id><created>2013-03-20</created><updated>2013-11-25</updated><authors><author><keyname>Bouzidi</keyname><forenames>Yacine</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Lazard</keyname><forenames>Sylvain</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Pouget</keyname><forenames>Marc</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author><author><keyname>Rouillier</keyname><forenames>Fabrice</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author></authors><title>Rational Univariate Representations of Bivariate Systems and
  Applications</title><categories>cs.SC cs.CG</categories><comments>Changed the title of RR_paper_rur_bitsize to match the one of
  ISSAC'13</comments><proxy>ccsd</proxy><report-no>RR-8262</report-no><journal-ref>N&amp;deg; RR-8262 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of solving systems of two bivariate polynomials of
total degree at most $d$ with integer coefficients of maximum bitsize $\tau$.
It is known that a linear separating form, that is a linear combination of the
variables that takes different values at distinct solutions of the system, can
be computed in $\sOB(d^{8}+d^7\tau)$ bit operations (where $O_B$ refers to bit
complexities and $\sO$ to complexities where polylogarithmic factors are
omitted) and we focus here on the computation of a Rational Univariate
Representation (RUR) given a linear separating form. We present an algorithm
for computing a RUR with worst-case bit complexity in $\sOB(d^7+d^6\tau)$ and
bound the bitsize of its coefficients by $\sO(d^2+d\tau)$. We show in addition
that isolating boxes of the solutions of the system can be computed from the
RUR with $\sOB(d^{8}+d^7\tau)$ bit operations. Finally, we show how a RUR can
be used to evaluate the sign of a bivariate polynomial (of degree at most $d$
and bitsize at most $\tau$) at one real solution of the system in
$\sOB(d^{8}+d^7\tau)$ bit operations and at all the $\Theta(d^2)$ {real}
solutions in only $O(d)$ times that for one solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5050</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5050</id><created>2013-03-17</created><updated>2013-03-21</updated><authors><author><keyname>Cluzel</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>LGI</affiliation></author><author><keyname>Yannou</keyname><forenames>Bernard</forenames><affiliation>LGI</affiliation></author><author><keyname>Dihlmann</keyname><forenames>Markus</forenames><affiliation>US</affiliation></author></authors><title>Using evolutionary design to interactively sketch car silhouettes and
  stimulate designer's creativity</title><categories>cs.NE cs.HC physics.med-ph</categories><proxy>ccsd</proxy><journal-ref>Engineering Applications of Artificial Intelligence 25 (2012)
  1413-1424</journal-ref><doi>10.1016/j.engappai.2012.02.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An Interactive Genetic Algorithm is proposed to progressively sketch the
desired side-view of a car profile. It adopts a Fourier decomposition of a 2D
profile as the genotype, and proposes a cross-over mechanism. In addition, a
formula function of two genes' discrepancies is fitted to the perceived
dissimilarity between two car profiles. This similarity index is intensively
used, throughout a series of user tests, to highlight the added value of the
IGA compared to a systematic car shape exploration, to prove its ability to
create superior satisfactory designs and to stimulate designer's creativity.
These tests have involved six designers with a design goal defined by a
semantic attribute. The results reveal that if &quot;friendly&quot; is diversely
interpreted in terms of car shapes, &quot;sportive&quot; denotes a very conventional
representation which may be a limitation for shape renewal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5061</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5061</id><created>2013-03-17</created><authors><author><keyname>Yannou</keyname><forenames>Bernard</forenames><affiliation>LGI</affiliation></author></authors><title>Which research in design creativity and innovation? Let us not forget
  the reality of companies</title><categories>cs.CY</categories><proxy>ccsd</proxy><journal-ref>International Journal of Design Creativity and Innovation 2 (2013)
  1-21</journal-ref><doi>10.1080/21650349.2013.754647</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studying design creativity and innovation from practical perspectives for
companies requires both a good understanding of the company ecosystem and its
inner processes contributing to delivered innovations and a rigorous design
research methodology to provide effective design models, methods, platforms
that are truly effective in the context of company. Working in an Industrial
Engineering laboratory, we advocate a more systemic vision of design creativity
and innovation in company ecosystems. We present in this paper an attempt to
develop and make professional an innovation engineering. Our research works are
illustrated along the different research topics of an innovation process. We
start by a recent survey on innovation practice and organizational models led
in 28 large companies. The lessons learned about this survey reinforce our
belief that there is a need for a new method in agile management of radical
innovation projects in company contexts. We currently develop, test and apply
such a methodology named: Radical Innovation Design(RID). Its effectiveness has
been evaluated through a large scale evaluation of the project outcomes for the
company. Two extensions of RID have been proposed and deployed in company
contexts: a selection procedure for innovation clusters and a value-driven
process for airplane development projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5097</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5097</id><created>2013-03-20</created><authors><author><keyname>Jacques</keyname><forenames>Laurent</forenames></author></authors><title>On the optimality of a L1/L1 solver for sparse signal recovery from
  sparsely corrupted compressive measurements</title><categories>cs.IT math.IT</categories><comments>4 pages (all comments are welcome)</comments><report-no>TR-LJ-2013-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short note proves the $\ell_2-\ell_1$ instance optimality of a
$\ell_1/\ell_1$ solver, i.e a variant of \emph{basis pursuit denoising} with a
$\ell_1$ fidelity constraint, when applied to the estimation of sparse (or
compressible) signals observed by sparsely corrupted compressive measurements.
The approach simply combines two known results due to Y. Plan, R. Vershynin and
E. Cand\`es.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5107</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5107</id><created>2013-03-17</created><authors><author><keyname>Peng</keyname><forenames>T.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author><author><keyname>Schmeink</keyname><forenames>A.</forenames></author></authors><title>Joint Power Adjustment and Receiver Design for Distributed Space-Time
  Coded in Cooperative MIMO Systems</title><categories>cs.IT math.IT</categories><comments>3 figures; ISWCS 2011. arXiv admin note: substantial text overlap
  with arXiv:1303.4762</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a joint power allocation algorithm with minimum mean-squared
error (MMSE) receiver for a cooperative Multiple-Input and Multiple-Output
(MIMO) network which employs multiple relays and a Decode-and-Forward (DF)
strategy is proposed. A Distributed Space-Time Coding (DSTC) scheme is applied
in each relay node. We present a joint constrained optimization algorithm to
determine the power allocation parameters and the MMSE receive filter parameter
vectors for each transmitted symbol in each link, as well as the channel
coefficients matrix. A Stochastic Gradient (SG) algorithm is derived for the
calculation of the joint optimization in order to release the receiver from the
massive calculation complexity for the MMSE receive filter and power allocation
parameters. The simulation results indicate that the proposed algorithm obtains
gains compared to the equal power allocation system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5121</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5121</id><created>2013-03-20</created><authors><author><keyname>Fa</keyname><forenames>R.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Low-Rank STAP Algorithm for Airborne Radar Based on Basis-Function
  Approximation</title><categories>cs.IT math.IT</categories><comments>2 figures</comments><journal-ref>SSP 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a novel reduced-rank space-time adaptive processing
(STAP) algorithm based on adaptive basis function approximation (ABFA) for
airborne radar applications. The proposed algorithm employs the well-known
framework of the side-lobe canceller (SLC) structure and consists of selected
sets of basis functions that perform dimensionality reduction and an adaptive
reduced-rank filter. Compared to traditional reduced-rank techniques, the
proposed scheme works on an instantaneous basis, selecting the best suited set
of basis functions at each instant to minimize the squared error. Furthermore,
we derive stochastic gradient (SG) and recursive least squares (RLS) algorithm
for efficiently implementing the proposed ABFA scheme. Simulations for a
clutter-plus-jamming suppression application show that the proposed STAP
algorithm outperforms the state-of-the-art reduced-rank schemes in convergence
and tracking at significantly lower complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5132</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5132</id><created>2013-03-20</created><authors><author><keyname>Fontes</keyname><forenames>Vitor Cunha</forenames></author><author><keyname>Bogorny</keyname><forenames>Vania</forenames></author></authors><title>Discovering Semantic Spatial and Spatio-Temporal Outliers from Moving
  Object Trajectories</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several algorithms have been proposed for discovering patterns from
trajectories of moving objects, but only a few have concentrated on outlier
detection. Existing approaches, in general, discover spatial outliers, and do
not provide any further analysis of the patterns. In this paper we introduce
semantic spatial and spatio-temporal outliers and propose a new algorithm for
trajectory outlier detection. Semantic outliers are computed between regions of
interest, where objects have similar movement intention, and there exist
standard paths which connect the regions. We show with experiments on real data
that the method finds semantic outliers from trajectory data that are not
discovered by similar approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5134</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5134</id><created>2013-03-20</created><authors><author><keyname>Rao</keyname><forenames>Angeline</forenames></author><author><keyname>Liu</keyname><forenames>Ying</forenames></author><author><keyname>Feng</keyname><forenames>Yezhou</forenames></author><author><keyname>Shen</keyname><forenames>Jian</forenames></author></authors><title>Bounds on the Number of Huffman and Binary-Ternary Trees</title><categories>cs.IT math.IT</categories><comments>17 pages, 6 figures, 1 table data</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Huffman coding is a widely used method for lossless data compression because
it optimally stores data based on how often the characters occur in Huffman
trees. An $n$-ary Huffman tree is a connected, cycle-lacking graph where each
vertex can have either $n$ &quot;children&quot; vertices connecting to it, or 0 children.
Vertices with 0 children are called \textit{leaves}. We let $h_n(q)$ represent
the total number of $n$-ary Huffman trees with $q$ leaves. In this paper, we
use a recursive method to generate upper and lower bounds on $h_n(q)$ and get
$h_2(q) \approx (0.1418532)(1.7941471)^q+(0.0612410)(1.2795491)^q$ for $n=2$.
This matches the best results achieved by Elsholtz, Heuberger, and Prodinger in
August 2011. Our approach reveals patterns in Huffman trees that we used in our
analysis of the Binary-Ternary (BT) trees we created. Our research opens a
completely new door in data compression by extending the study of Huffman trees
to BT trees. Our study of BT trees paves the way for designing data-specific
trees, minimizing possible wasted storage space from Huffman coding. We prove a
recursive formula for the number of BT trees with $q$ leaves. Furthermore, we
provide analysis and further proofs to reach numeric bounds. Our discoveries
have broad applications in computer data compression. These results also
improve graphical representations of protein sequences that facilitate in-depth
genome analysis used in researching evolutionary patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5145</identifier>
 <datestamp>2014-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5145</id><created>2013-03-20</created><updated>2014-01-22</updated><authors><author><keyname>Mohan</keyname><forenames>Karthik</forenames></author><author><keyname>London</keyname><forenames>Palma</forenames></author><author><keyname>Fazel</keyname><forenames>Maryam</forenames></author><author><keyname>Witten</keyname><forenames>Daniela</forenames></author><author><keyname>Lee</keyname><forenames>Su-In</forenames></author></authors><title>Node-Based Learning of Multiple Gaussian Graphical Models</title><categories>stat.ML cs.LG math.OC</categories><comments>42 pages, 16 figures. Accepted to JMLR, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating high-dimensional Gaussian graphical
models corresponding to a single set of variables under several distinct
conditions. This problem is motivated by the task of recovering transcriptional
regulatory networks on the basis of gene expression data {containing
heterogeneous samples, such as different disease states, multiple species, or
different developmental stages}. We assume that most aspects of the conditional
dependence networks are shared, but that there are some structured differences
between them. Rather than assuming that similarities and differences between
networks are driven by individual edges, we take a node-based approach, which
in many cases provides a more intuitive interpretation of the network
differences. We consider estimation under two distinct assumptions: (1)
differences between the K networks are due to individual nodes that are
perturbed across conditions, or (2) similarities among the K networks are due
to the presence of common hub nodes that are shared across all K networks.
Using a row-column overlap norm penalty function, we formulate two convex
optimization problems that correspond to these two assumptions. We solve these
problems using an alternating direction method of multipliers algorithm, and we
derive a set of necessary and sufficient conditions that allows us to decompose
the problem into independent subproblems so that our algorithm can be scaled to
high-dimensional settings. Our proposal is illustrated on synthetic data, a
webpage data set, and a brain cancer gene expression data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5148</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5148</id><created>2013-03-20</created><authors><author><keyname>Karakos</keyname><forenames>Damianos</forenames></author><author><keyname>Dredze</keyname><forenames>Mark</forenames></author><author><keyname>Khudanpur</keyname><forenames>Sanjeev</forenames></author></authors><title>Estimating Confusions in the ASR Channel for Improved Topic-based
  Language Model Adaptation</title><categories>cs.CL cs.LG</categories><comments>Technical Report 8, Human Language Technology Center of Excellence,
  Johns Hopkins University</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human language is a combination of elemental languages/domains/styles that
change across and sometimes within discourses. Language models, which play a
crucial role in speech recognizers and machine translation systems, are
particularly sensitive to such changes, unless some form of adaptation takes
place. One approach to speech language model adaptation is self-training, in
which a language model's parameters are tuned based on automatically
transcribed audio. However, transcription errors can misguide self-training,
particularly in challenging settings such as conversational speech. In this
work, we propose a model that considers the confusions (errors) of the ASR
channel. By modeling the likely confusions in the ASR output instead of using
just the 1-best, we improve self-training efficacy by obtaining a more reliable
reference transcription estimate. We demonstrate improved topic-based language
modeling adaptation results over both 1-best and lattice self-training using
our ASR channel confusion estimates on telephone conversations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5157</identifier>
 <datestamp>2014-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5157</id><created>2013-03-21</created><authors><author><keyname>Yan</keyname><forenames>Shihao</forenames></author><author><keyname>Yang</keyname><forenames>Nan</forenames></author><author><keyname>Malaney</keyname><forenames>Robert</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author></authors><title>Transmit Antenna Selection with Alamouti Scheme in MIMO Wiretap Channels</title><categories>cs.IT cs.CR math.IT</categories><comments>6 pages, 6 figures, submitted to GlobeCom 2013</comments><doi>10.1109/GLOCOM.2013.6831148</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new transmit antenna selection (TAS) scheme which
provides enhanced physical layer security in multiple-input multiple-output
(MIMO) wiretap channels. The practical passive eavesdropping scenario we
consider is where channel state information (CSI) from the eavesdropper is not
available at the transmitter. Our new scheme is carried out in two steps.
First, the transmitter selects the first two strongest antennas based on the
feedback from the receiver, which maximizes the instantaneous signal-to-noise
ratio (SNR) of the transmitter-receiver channel. Second, the Alamouti scheme is
employed at the selected antennas in order to perform data transmission. At the
receiver and the eavesdropper, maximal-ratio combining is applied in order to
exploit the multiple antennas.We derive a new closed-form expression for the
secrecy outage probability in nonidentical Rayleigh fading, and using this
result, we then present the probability of non-zero secrecy capacity in closed
form and the {\epsilon}-outage secrecy capacity in numerical form. We
demonstrate that our proposed TAS-Alamouti scheme offers lower secrecy outage
probability than a single TAS scheme when the SNR of the transmitter-receiver
channel is above a specific value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5164</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5164</id><created>2013-03-21</created><authors><author><keyname>Zhong</keyname><forenames>Jianlong</forenames></author><author><keyname>He</keyname><forenames>Bingsheng</forenames></author></authors><title>Kernelet: High-Throughput GPU Kernel Executions with Dynamic Slicing and
  Scheduling</title><categories>cs.DC</categories><acm-class>I.3.1; D.1.3; C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphics processors, or GPUs, have recently been widely used as accelerators
in the shared environments such as clusters and clouds. In such shared
environments, many kernels are submitted to GPUs from different users, and
throughput is an important metric for performance and total ownership cost.
Despite the recently improved runtime support for concurrent GPU kernel
executions, the GPU can be severely underutilized, resulting in suboptimal
throughput. In this paper, we propose Kernelet, a runtime system with dynamic
slicing and scheduling techniques to improve the throughput of concurrent
kernel executions on the GPU. With slicing, Kernelet divides a GPU kernel into
multiple sub-kernels (namely slices). Each slice has tunable occupancy to allow
co-scheduling with other slices and to fully utilize the GPU resources. We
develop a novel and effective Markov chain based performance model to guide the
scheduling decision. Our experimental results demonstrate up to 31.1% and 23.4%
performance improvement on NVIDIA Tesla C2050 and GTX680 GPUs, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5175</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5175</id><created>2013-03-21</created><authors><author><keyname>Namiot</keyname><forenames>Dmitry</forenames></author></authors><title>Discovery of Convoys in Network Proximity Log</title><categories>cs.DB cs.NI</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes an algorithm for discovery of convoys in database with
proximity log. Traditionally, discovery of convoys covers trajectories
databases. This paper presents a model for context-aware browsing application
based on the network proximity. Our model uses mobile phone as proximity sensor
and proximity data replaces location information. As per our concept, any
existing or even especially created wireless network node could be used as
presence sensor that can discover access to some dynamic or user-generated
content. Content revelation in this model depends on rules based on the
proximity. Discovery of convoys in historical user's logs provides a new class
of rules for delivering local content to mobile subscribers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5177</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5177</id><created>2013-03-21</created><authors><author><keyname>Shikoun</keyname><forenames>Nabila</forenames></author><author><keyname>Nahas</keyname><forenames>Mohamed El</forenames></author><author><keyname>Kassim</keyname><forenames>Samar</forenames></author></authors><title>Model Based Framework for Estimating Mutation Rate of Hepatitis C Virus
  in Egypt</title><categories>cs.AI</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hepatitis C virus (HCV) is a widely spread disease all over the world. HCV
has very high mutation rate that makes it resistant to antibodies. Modeling HCV
to identify the virus mutation process is essential to its detection and
predicting its evolution. This paper presents a model based framework for
estimating mutation rate of HCV in two steps. Firstly profile hidden Markov
model (PHMM) architecture was builder to select the sequences which represents
sequence per year. Secondly mutation rate was calculated by using pair-wise
distance method between sequences. A pilot study is conducted on NS5B zone of
HCV dataset of genotype 4 subtype a (HCV4a) in Egypt.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5190</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5190</id><created>2013-03-21</created><authors><author><keyname>Rehman</keyname><forenames>O.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Manzoor</keyname><forenames>B.</forenames></author><author><keyname>Hafeez</keyname><forenames>A.</forenames></author><author><keyname>Iqbal</keyname><forenames>A.</forenames></author><author><keyname>Ishfaq</keyname><forenames>M.</forenames></author></authors><title>Energy Consumption Rate based Stable Election Protocol (ECRSEP) for WSNs</title><categories>cs.NI</categories><journal-ref>International Workshop on Body Area Sensor Networks (BASNet-2013)
  in conjunction with 4th International Conference on Ambient Systems, Networks
  and Technologies (ANT 2013), 2013, Halifax, Nova Scotia, Canada</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent few yearsWireless Sensor Networks (WSNs) have seen an increased
interest in various applications like border field security, disaster
management and medical applications. So large number of sensor nodes are
deployed for such applications, which can work autonomously. Due to small power
batteries in WSNs, efficient utilization of battery power is an important
factor. Clustering is an efficient technique to extend life time of sensor
networks by reducing the energy consumption. In this paper, we propose a new
protocol; Energy Consumption Rate based Stable Election Protocol (ECRSEP). Our
CH selection scheme is based on the weighted election probabilities of each
node according to the Energy Consumption Rate (ECR) of each node. We compare
results of our proposed protocol with Low Energy Adaptive Clustering Hierarchy
(LEACH), Distributed Energy Efficient Clustering (DEEC), Stable Election
Protocol (SEP), and Enhanced SEP(ESEP). Our simulation results show that our
proposed protocol, ECRSEP outperforms all these protocols in terms of network
stability and network lifetime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5194</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5194</id><created>2013-03-21</created><authors><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Full-Duplex Cooperative Cognitive Radio with Transmit Imperfections</title><categories>cs.IT math.IT</categories><comments>10 figures, to appear in the IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the cooperation between a primary system and a cognitive
system in a cellular network where the cognitive base station (CBS) relays the
primary signal using amplify-and-forward or decode-and-forward protocols, and
in return it can transmit its own cognitive signal. While the commonly used
half-duplex (HD) assumption may render the cooperation less efficient due to
the two orthogonal channel phases employed, we propose that the CBS can work in
a full-duplex (FD) mode to improve the system rate region. The problem of
interest is to find the achievable primary-cognitive rate region by studying
the cognitive rate maximization problem. For both modes, we explicitly consider
the CBS transmit imperfections, which lead to the residual self-interference
associated with the FD operation mode. We propose closed-form solutions or
efficient algorithms to solve the problem when the related residual
interference power is non-scalable or scalable with the transmit power.
Furthermore, we propose a simple hybrid scheme to select the HD or FD mode
based on zero-forcing criterion, and provide insights on the impact of system
parameters. Numerical results illustrate significant performance improvement by
using the FD mode and the hybrid scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5197</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5197</id><created>2013-03-21</created><updated>2015-03-10</updated><authors><author><keyname>Isaac</keyname><forenames>Yoann</forenames></author><author><keyname>Barth&#xe9;lemy</keyname><forenames>Quentin</forenames></author><author><keyname>Atif</keyname><forenames>Jamal</forenames></author><author><keyname>Gouy-Pailler</keyname><forenames>C&#xe9;dric</forenames></author><author><keyname>Sebag</keyname><forenames>Mich&#xe8;le</forenames></author></authors><title>Multi-dimensional sparse structured signal approximation using split
  Bregman iterations</title><categories>cs.DS stat.ML</categories><comments>5 pages, ICASSP 2013 preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper focuses on the sparse approximation of signals using overcomplete
representations, such that it preserves the (prior) structure of
multi-dimensional signals. The underlying optimization problem is tackled using
a multi-dimensional split Bregman optimization approach. An extensive empirical
evaluation shows how the proposed approach compares to the state of the art
depending on the signal features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5199</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5199</id><created>2013-03-21</created><authors><author><keyname>Ebadat</keyname><forenames>Afrooz</forenames></author><author><keyname>Annergren</keyname><forenames>Mariette</forenames></author><author><keyname>Larsson</keyname><forenames>Christian A.</forenames></author><author><keyname>Rojas</keyname><forenames>Cristian R.</forenames></author><author><keyname>Wahlberg</keyname><forenames>Bo</forenames></author></authors><title>Application Set Approximation in Optimal Input Design for Model
  Predictive Control</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This contribution considers one central aspect of experiment design in system
identification. When a control design is based on an estimated model, the
achievable performance is related to the quality of the estimate. The
degradation in control performance due to errors in the estimated model is
measured by an application cost function. In order to use an optimization based
input design method, a convex approximation of the set of models that atisfies
the control specification is required. The standard approach is to use a
quadratic approximation of the application cost function, where the main
computational effort is to find the corresponding Hessian matrix. Our main
contribution is an alternative approach for this problem, which uses the
structure of the underlying optimal control problem to considerably reduce the
computations needed to find the application set. This technique allows the use
of applications oriented input design for MPC on much more complex plants. The
approach is numerically evaluated on a distillation control problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5205</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5205</id><created>2013-03-21</created><updated>2014-06-26</updated><authors><author><keyname>Bousquet</keyname><forenames>Nicolas</forenames></author><author><keyname>Lagoutte</keyname><forenames>Aur&#xe9;lie</forenames></author><author><keyname>Thomass&#xe9;</keyname><forenames>St&#xe9;phan</forenames></author></authors><title>The Erd\H{o}s-Hajnal Conjecture for Paths and Antipaths</title><categories>math.CO cs.DM</categories><journal-ref>Journal of Combinatorial Theory, Series B, 113:261-264, 2015</journal-ref><doi>10.1016/j.jctb.2015.01.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that for every k, there exists $c_k&gt;0$ such that every graph G on n
vertices not inducing a path $P_k$ and its complement contains a clique or a
stable set of size $n^{c_k}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5217</identifier>
 <datestamp>2015-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5217</id><created>2013-03-21</created><updated>2015-10-13</updated><authors><author><keyname>Aum&#xfc;ller</keyname><forenames>Martin</forenames></author><author><keyname>Dietzfelbinger</keyname><forenames>Martin</forenames></author></authors><title>Optimal Partitioning for Dual-Pivot Quicksort</title><categories>cs.DS</categories><comments>Accepted for publication in ACM Transactions on Algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dual-pivot quicksort refers to variants of classical quicksort where in the
partitioning step two pivots are used to split the input into three segments.
This can be done in different ways, giving rise to different algorithms.
Recently, a dual-pivot algorithm proposed by Yaroslavskiy received much
attention, because a variant of it replaced the well-engineered quicksort
algorithm in Sun's Java 7 runtime library. Nebel and Wild (ESA 2012) analyzed
this algorithm and showed that on average it uses 1.9n ln n + O(n) comparisons
to sort an input of size n, beating standard quicksort, which uses 2n ln n +
O(n) comparisons. We introduce a model that captures all dual-pivot algorithms,
give a unified analysis, and identify new dual-pivot algorithms that minimize
the average number of key comparisons among all possible algorithms up to a
linear term. This minimum is 1.8n ln n + O(n). For the case that the pivots are
chosen from a small sample, we include a comparison of dual-pivot quicksort and
classical quicksort. Specifically, we show that dual-pivot quicksort benefits
from a skewed choice of pivots. We experimentally evaluate our algorithms and
compare them to Yaroslavskiy's algorithm and the recently described three-pivot
quicksort algorithm of Kushagra et al. (ALENEX 2014).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5223</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5223</id><created>2013-03-21</created><authors><author><keyname>Kumar</keyname><forenames>Ramesh</forenames></author><author><keyname>Hussain</keyname><forenames>Dilawar</forenames></author><author><keyname>Ruchita</keyname></author></authors><title>Optimization of PI Coefficients in DSTATCOM Nonlinear Controller for
  Regulating DC Voltage using Particle Swarm Optimization</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-linear controller is preferred to linear controller due to non-linear
operation of DSTATCOM. System dynamic can be improved by regulating and fixing
the capacitor DC voltage in DSTATCOM. The nonlinear control is based on exact
linearization via feedback. There is a PI controller in this system to regulate
DC voltage. In conventional scheme, the trial and error method is used to
determine PI values. Exact calculation to optimize PI coefficients can be
carried out to reduce disturbances in DC link voltage and thus, in this paper,
Particle Swarm Optimization is applied. As a result, Capacitor voltage tracks
the reference values which have less vibration than conventional status. Both
trial and error method and PSO are implemented. A set of corresponding diagrams
achieved by these two methods are offered to demonstrate the effectiveness of
new method. Optimizations and Simulations are worked out in MATLAB environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5226</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5226</id><created>2013-03-21</created><authors><author><keyname>Khadir</keyname><forenames>Omar</forenames></author></authors><title>Algorithm for factoring some RSA and Rabin moduli</title><categories>cs.CR math.NT</categories><msc-class>11B50 94A60</msc-class><journal-ref>Journal of Discrete Mathematical Sciences and Cryptography (in
  2008)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new efficient algorithm for factoring the RSA and
the Rabin moduli in the particular case when the difference between their two
prime factors is bounded. As an extension, we also give some theoretical
results on factoring integers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5234</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5234</id><created>2013-03-21</created><updated>2014-03-16</updated><authors><author><keyname>Dendek</keyname><forenames>Piotr Jan</forenames></author><author><keyname>Czeczko</keyname><forenames>Artur</forenames></author><author><keyname>Fedoryszak</keyname><forenames>Mateusz</forenames></author><author><keyname>Kawa</keyname><forenames>Adam</forenames></author><author><keyname>Wendykier</keyname><forenames>Piotr</forenames></author><author><keyname>Bolikowski</keyname><forenames>Lukasz</forenames></author></authors><title>How to perform research in Hadoop environment not losing mental
  equilibrium - case study</title><categories>cs.SE cs.DC</categories><comments>This paper (with changed content) appeared under the title &quot;Chrum:
  The Tool for Convenient Generation of Apache Oozie Workflows&quot; in &quot;Intelligent
  Tools for Building a Scientific Information Platform: From Research to
  Implementation&quot;, &quot;Studies in Computational Intelligence&quot;, Volume 541, 2014,
  http://link.springer.com/book/10.1007/978-3-319-04714-0</comments><acm-class>H.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conducting a research in an efficient, repetitive, evaluable, but also
convenient (in terms of development) way has always been a challenge. To
satisfy those requirements in a long term and simultaneously minimize costs of
the software engineering process, one has to follow a certain set of
guidelines. This article describes such guidelines based on the research
environment called Content Analysis System (CoAnSys) created in the Center for
Open Science (CeON). Best practices and tools for working in the Apache Hadoop
environment, as well as the process of establishing these rules are portrayed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5239</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5239</id><created>2013-03-21</created><authors><author><keyname>Kambites</keyname><forenames>Mark</forenames><affiliation>University of Manchester</affiliation></author></authors><title>Anisimov's Theorem for inverse semigroups</title><categories>math.GR cs.FL</categories><comments>8 pages</comments><msc-class>20M18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The idempotent problem of a finitely generated inverse semigroup is the
formal language of all words over the generators representing idempotent
elements. This note proves that a finitely generated inverse semigroup with
regular idempotent problem is necessarily finite. This answers a question of
Gilbert and Noonan Heale, and establishes a generalisation to inverse
semigroups of Anisimov's Theorem for groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5240</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5240</id><created>2013-03-21</created><authors><author><keyname>Manzoor</keyname><forenames>B.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Rehman</keyname><forenames>O.</forenames></author><author><keyname>Akbar</keyname><forenames>M.</forenames></author><author><keyname>Nadeem</keyname><forenames>Q.</forenames></author><author><keyname>Iqbal</keyname><forenames>A.</forenames></author><author><keyname>Ishfaq</keyname><forenames>M.</forenames></author></authors><title>Q-LEACH: A New Routing Protocol for WSNs</title><categories>cs.NI</categories><journal-ref>International Workshop on Body Area Sensor Networks (BASNet-2013)
  in conjunction with 4th International Conference on Ambient Systems, Networks
  and Technologies (ANT 2013), 2013, Halifax, Nova Scotia, Canada</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) with their dynamic applications gained a
tremendous attention of researchers. Constant monitoring of critical situations
attracted researchers to utilize WSNs at vast platforms. The main focus in WSNs
is to enhance network life-time as much as one could, for efficient and optimal
utilization of resources. Different approaches based upon clustering are
proposed for optimum functionality. Network life-time is always related with
energy of sensor nodes deployed at remote areas for constant and fault tolerant
monitoring. In this work, we propose Quadrature-LEACH (Q-LEACH) for homogenous
networks which enhances stability period, network life-time and throughput
quiet significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5243</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5243</id><created>2013-03-21</created><authors><author><keyname>Argyriou</keyname><forenames>Antonios</forenames></author></authors><title>Link Scheduling for Multiple Multicast Sessions in Distributed Wireless
  Networks</title><categories>cs.NI</categories><journal-ref>IEEE Wireless Communications Letters 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter we investigate link scheduling algorithms for throughput
maximization in multicast wireless networks. According to our system model,
each source node transmits to a multicast group that resides one hop away. We
adopt the physical interference model to reflect the aggregate signal to
interference and noise ratio (SINR) at each node of the multicast group. We
present an ILP formulation of the aforementioned problem. The basic feature of
the problem formulation is that it decomposes the single multicast session into
the corresponding point-to-point links. The rationale is that a solution
algorithm has more flexibility regarding the scheduling options for individual
nodes. The extended MILP problem that also considers power control is solved
with LP relaxation. Performance results for both the ILP and MILP problems are
obtained for different traffic loads and different number of nodes per
multicast group.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5244</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5244</id><created>2013-03-21</created><authors><author><keyname>Hawe</keyname><forenames>Simon</forenames></author><author><keyname>Seibert</keyname><forenames>Matthias</forenames></author><author><keyname>Kleinsteuber</keyname><forenames>Martin</forenames></author></authors><title>Separable Dictionary Learning</title><categories>cs.CV cs.LG stat.ML</categories><comments>12 pages, 2 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many techniques in computer vision, machine learning, and statistics rely on
the fact that a signal of interest admits a sparse representation over some
dictionary. Dictionaries are either available analytically, or can be learned
from a suitable training set. While analytic dictionaries permit to capture the
global structure of a signal and allow a fast implementation, learned
dictionaries often perform better in applications as they are more adapted to
the considered class of signals. In imagery, unfortunately, the numerical
burden for (i) learning a dictionary and for (ii) employing the dictionary for
reconstruction tasks only allows to deal with relatively small image patches
that only capture local image information. The approach presented in this paper
aims at overcoming these drawbacks by allowing a separable structure on the
dictionary throughout the learning process. On the one hand, this permits
larger patch-sizes for the learning phase, on the other hand, the dictionary is
applied efficiently in reconstruction tasks. The learning procedure is based on
optimizing over a product of spheres which updates the dictionary as a whole,
thus enforces basic dictionary properties such as mutual coherence explicitly
during the learning procedure. In the special case where no separable structure
is enforced, our method competes with state-of-the-art dictionary learning
methods like K-SVD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5248</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5248</id><created>2013-03-21</created><updated>2014-04-03</updated><authors><author><keyname>Salin</keyname><forenames>Boris M.</forenames></author><author><keyname>Salin</keyname><forenames>Mikhail B.</forenames></author></authors><title>Methods Of Measurement The Three-Dimensional Wind Waves Spectra, Based
  On The Processing Of Video Images Of The Sea Surface</title><categories>physics.ao-ph cs.CV</categories><comments>36 pages, 9 figures, 1 table. Grammar and spelling was carefully
  edited in version 2</comments><journal-ref>Radiophysics and Quantum Electronics. 58(2015), No.2, P.114-123</journal-ref><doi>10.1007/s11141-015-9586-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical instruments for measuring surface-wave characteristics provide a
better spatial and temporal resolution than other methods, but they face
difficulties while converting the results of indirect measurements into
absolute levels of the waves. We have solved this problem to some extent. In
this paper, we propose an optical method for measuring the 3D power spectral
density of the surface waves and spatio-temporal samples of the wave profiles.
The method involves, first, synchronous recording of the brightness field over
a patch of a rough surface and measurement of surface oscillations at one or
more points and, second, filtering of the spatial image spectrum. Filter
parameters are chosen to maximize the correlation of the surface oscillations
recovered and measured at one or two points. In addition to the measurement
procedure, the paper provides experimental results of measuring
multidimensional spectra of roughness, which generally agree with theoretical
expectations and the results of other authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5250</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5250</id><created>2013-03-21</created><authors><author><keyname>Sloan</keyname><forenames>Marc</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author></authors><title>Iterative Expectation for Multi Period Information Retrieval</title><categories>cs.IR</categories><comments>8 pages, 3 tables, published at the Workshop on Web Search Click Data
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many Information Retrieval (IR) models make use of offline statistical
techniques to score documents for ranking over a single period, rather than use
an online, dynamic system that is responsive to users over time. In this paper,
we explicitly formulate a general Multi Period Information Retrieval problem,
where we consider retrieval as a stochastic yet controllable process. The
ranking action during the process continuously controls the retrieval system's
dynamics, and an optimal ranking policy is found in order to maximise the
overall users' satisfaction over the multiple periods as much as possible. Our
derivations show interesting properties about how the posterior probability of
the documents relevancy evolves from users feedbacks through clicks, and
provides a plug-in framework for incorporating different click models. Based on
the Multi-Armed Bandit theory, we propose a simple implementation of our
framework using a dynamic ranking rule that takes rank bias and exploration of
documents into account. We use TREC data to learn a suitable exploration
parameter for our model, and then analyse its performance and a number of
variants using a search log data set; the experiments suggest an ability to
explore document relevance dynamically over time using user feedback in a way
that can handle rank bias.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5251</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5251</id><created>2013-03-21</created><authors><author><keyname>Reiter</keyname><forenames>Johannes G.</forenames></author><author><keyname>Bozic</keyname><forenames>Ivana</forenames></author><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Nowak</keyname><forenames>Martin A.</forenames></author></authors><title>TTP: Tool for Tumor Progression</title><categories>q-bio.PE cs.CE</categories><comments>A conference version of the paper will appear in CAV 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we present a flexible tool for tumor progression, which
simulates the evolutionary dynamics of cancer. Tumor progression implements a
multi-type branching process where the key parameters are the fitness
landscape, the mutation rate, and the average time of cell division. The
fitness of a cancer cell depends on the mutations it has accumulated. The input
to our tool could be any fitness landscape, mutation rate, and cell division
time, and the tool produces the growth dynamics and all relevant statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5259</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5259</id><created>2013-03-21</created><authors><author><keyname>Thom</keyname><forenames>Markus</forenames></author><author><keyname>Palm</keyname><forenames>G&#xfc;nther</forenames></author></authors><title>Efficient Sparseness-Enforcing Projections</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a linear time and constant space algorithm for computing Euclidean
projections onto sets on which a normalized sparseness measure attains a
constant value. These non-convex target sets can be characterized as
intersections of a simplex and a hypersphere. Some previous methods required
the vector to be projected to be sorted, resulting in at least quasilinear time
complexity and linear space complexity. We improve on this by adaptation of a
linear time algorithm for projecting onto simplexes. In conclusion, we propose
an efficient algorithm for computing the product of the gradient of the
projection with an arbitrary vector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5260</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5260</id><created>2013-03-21</created><authors><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Abbas</keyname><forenames>Z.</forenames></author><author><keyname>Fareed</keyname><forenames>M. S.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Alrajeh</keyname><forenames>N.</forenames></author></authors><title>M-ATTEMPT: A New Energy-Efficient Routing Protocol for Wireless Body
  Area Sensor Networks</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1208.6096</comments><journal-ref>4th International Conference on Ambient Systems, Networks and
  Technologies (ANT), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new routing protocol for heterogeneous Wireless
Body Area Sensor Networks (WBASNs); Mobility-supporting Adaptive
Threshold-based Thermal-aware Energy-efficientMulti-hop ProTocol (M-ATTEMPT). A
prototype is defined for employing heterogeneous sensors on human body. Direct
communication is used for real-time traffic (critical data) or on-demand data
while Multi-hop communication is used for normal data delivery. One of the
prime challenges in WBASNs is sensing of the heat generated by the implanted
sensor nodes. The proposed routing algorithm is thermal-aware which senses the
link Hot-spot and routes the data away from these links. Continuous mobility of
human body causes disconnection between previous established links. So,
mobility support and energy-management is introduced to overcome the problem.
Linear Programming (LP) model for maximum information extraction and minimum
energy consumption is presented in this study. MATLAB simulations of proposed
routing algorithm are performed for lifetime and successful packet delivery in
comparison with Multi-hop communication. The results show that the proposed
routing algorithm has less energy consumption and more reliable as compared to
Multi-hop communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5268</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5268</id><created>2013-03-21</created><authors><author><keyname>Latif</keyname><forenames>K.</forenames></author><author><keyname>Ahmad</keyname><forenames>A.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Alrajeh</keyname><forenames>N.</forenames></author></authors><title>Divide-and-Rule Scheme for Energy Efficient Routing in Wireless Sensor
  Networks</title><categories>cs.NI</categories><journal-ref>4th International Conference on Ambient Systems, Networks and
  Technologies (ANT), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From energy conservation perspective inWireless Sensor Networks (WSNs),
clustering of sensor nodes is a challenging task. Clustering technique in
routing protocols play a key role to prolong the stability period and lifetime
of the network. In this paper, we propose and evaluate a new routing protocol
for WSNs. Our protocol; Divide-and-Rule (DR) is based upon static clustering
and dynamic Cluster Head (CH) selection technique. This technique selects fixed
number of CHs in each round instead of probabilistic selection of CH.
Simulation results show that DR protocol outperform its counterpart routing
protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5269</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5269</id><created>2013-03-21</created><updated>2013-09-20</updated><authors><author><keyname>Louzada</keyname><forenames>V. H. P.</forenames></author><author><keyname>Daolio</keyname><forenames>F.</forenames></author><author><keyname>Herrmann</keyname><forenames>H. J.</forenames></author><author><keyname>Tomassini</keyname><forenames>M.</forenames></author></authors><title>Smart Rewiring for Network Robustness</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>Journal of Complex Networks, 2013</comments><doi>10.1093/comnet/cnt010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While new forms of attacks are developed every day to compromise essential
infrastructures, service providers are also expected to develop strategies to
mitigate the risk of extreme failures. In this context, tools of Network
Science have been used to evaluate network robustness and propose resilient
topologies against attacks. We present here a new rewiring method to modify the
network topology improving its robustness, based on the evolution of the
network largest component during a sequence of targeted attacks. In comparison
to previous strategies, our method lowers by several orders of magnitude the
computational effort necessary to improve robustness. Our rewiring also drives
the formation of layers of nodes with similar degree while keeping a highly
modular structure. This &quot;modular onion-like structure&quot; is a particular class of
the onion-like structure previously described in the literature. We apply our
rewiring strategy to an unweighted representation of the World Air
Transportation network and show that an improvement of 30% in its overall
robustness can be achieved through smart swaps of around 9% of its links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5274</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5274</id><created>2013-03-21</created><authors><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Qureshi</keyname><forenames>T. N.</forenames></author><author><keyname>Khan</keyname><forenames>A. H.</forenames></author><author><keyname>Iqbal</keyname><forenames>A.</forenames></author><author><keyname>Akhtar</keyname><forenames>E.</forenames></author><author><keyname>Ishfaq</keyname><forenames>M.</forenames></author></authors><title>EDDEEC: Enhanced Developed Distributed Energy-Efficient Clustering for
  Heterogeneous Wireless Sensor Networks</title><categories>cs.NI</categories><journal-ref>International Workshop on Body Area Sensor Networks (BASNet-2013)
  in conjunction with 4th International Conference on Ambient Systems, Networks
  and Technologies (ANT 2013), 2013, Halifax, Nova Scotia, Canada</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) consist of large number of randomly deployed
energy constrained sensor nodes. Sensor nodes have ability to sense and send
sensed data to Base Station (BS). Sensing as well as transmitting data towards
BS require high energy. In WSNs, saving energy and extending network lifetime
are great challenges. Clustering is a key technique used to optimize energy
consumption in WSNs. In this paper, we propose a novel clustering based routing
technique: Enhanced Developed Distributed Energy Efficient Clustering scheme
(EDDEEC) for heterogeneous WSNs. Our technique is based on changing dynamically
and with more efficiency the Cluster Head (CH) election probability. Simulation
results show that our proposed protocol achieves longer lifetime, stability
period and more effective messages to BS than Distributed Energy Efficient
Clustering (DEEC), Developed DEEC (DDEEC) and Enhanced DEEC (EDEEC) in
heterogeneous environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5275</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5275</id><created>2013-03-21</created><authors><author><keyname>Lange</keyname><forenames>Michael</forenames></author><author><keyname>Gorman</keyname><forenames>Gerard</forenames></author><author><keyname>Weiland</keyname><forenames>Michele</forenames></author><author><keyname>Mitchell</keyname><forenames>Lawrence</forenames></author><author><keyname>Southern</keyname><forenames>James</forenames></author></authors><title>Achieving Efficient Strong Scaling with PETSc using Hybrid MPI/OpenMP
  Optimisation</title><categories>cs.DC</categories><journal-ref>Lecture Notes in Computer Science 7905:97-108 (2013)</journal-ref><doi>10.1007/978-3-642-38750-0_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing number of processing elements and decreas- ing memory to core
ratio in modern high-performance platforms makes efficient strong scaling a key
requirement for numerical algorithms. In order to achieve efficient scalability
on massively parallel systems scientific software must evolve across the entire
stack to exploit the multiple levels of parallelism exposed in modern
architectures. In this paper we demonstrate the use of hybrid MPI/OpenMP
parallelisation to optimise parallel sparse matrix-vector multiplication in
PETSc, a widely used scientific library for the scalable solution of partial
differential equations. Using large matrices generated by Fluidity, an open
source CFD application code which uses PETSc as its linear solver engine, we
evaluate the effect of explicit communication overlap using task-based
parallelism and show how to further improve performance by explicitly load
balancing threads within MPI processes. We demonstrate a significant speedup
over the pure-MPI mode and efficient strong scaling of sparse matrix-vector
multiplication on Fujitsu PRIMEHPC FX10 and Cray XE6 systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5285</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5285</id><created>2013-03-21</created><authors><author><keyname>Qureshi</keyname><forenames>T. N.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Khan</keyname><forenames>A. H.</forenames></author><author><keyname>Iqbal</keyname><forenames>A.</forenames></author><author><keyname>Akhtar</keyname><forenames>E.</forenames></author><author><keyname>Ishfaq</keyname><forenames>M.</forenames></author></authors><title>BEENISH: Balanced Energy Efficient Network Integrated Super
  Heterogeneous Protocol for Wireless Sensor Networks</title><categories>cs.NI</categories><journal-ref>International Workshop on Body Area Sensor Networks (BASNet-2013)
  in conjunction with 4th International Conference on Ambient Systems, Networks
  and Technologies (ANT 2013), 2013, Halifax, Nova Scotia, Canada</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In past years there has been increasing interest in field of Wireless Sensor
Networks (WSNs). One of the major issue of WSNs is development of energy
efficient routing protocols. Clustering is an effective way to increase energy
efficiency. Mostly, heterogenous protocols consider two or three energy level
of nodes. In reality, heterogonous WSNs contain large range of energy levels.
By analyzing communication energy consumption of the clusters and large range
of energy levels in heterogenous WSN, we propose BEENISH (Balanced Energy
Efficient Network Integrated Super Heterogenous) Protocol. It assumes WSN
containing four energy levels of nodes. Here, Cluster Heads (CHs) are elected
on the bases of residual energy level of nodes. Simulation results show that it
performs better than existing clustering protocols in heterogeneous WSNs. Our
protocol achieve longer stability, lifetime and more effective messages than
Distributed Energy Efficient Clustering (DEEC), Developed DEEC (DDEEC) and
Enhanced DEEC (EDEEC).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5301</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5301</id><created>2013-03-21</created><authors><author><keyname>HosseinNia</keyname><forenames>S. Hassan</forenames></author><author><keyname>Tejado</keyname><forenames>In&#xe9;s</forenames></author><author><keyname>Vinagre</keyname><forenames>Blas M</forenames></author></authors><title>Basic Properties and Stability of Fractional-Order Reset Control Systems</title><categories>cs.SY nlin.AO</categories><comments>The 12th European Control Conference (ECC13), Switzerland, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reset control is introduced to overcome limitations of linear control. A
reset controller includes a linear controller which resets some of states to
zero when their input is zero or certain non-zero values. This paper studies
the application of the fractional-order Clegg integrator (FCI) and compares its
performance with both the commonly used first order reset element (FORE) and
traditional Clegg integrator (CI). Moreover, stability of reset control systems
is generalized for the fractional-order case. Two examples are given to
illustrate the application of the stability theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5302</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5302</id><created>2013-03-21</created><authors><author><keyname>Sartor</keyname><forenames>Pablo</forenames><affiliation>IRISA / INRIA Rennes</affiliation></author><author><keyname>Robledo</keyname><forenames>Franco</forenames><affiliation>IRISA / INRIA Rennes</affiliation></author></authors><title>A simulation Method for Network Performability Estimation using
  Heuristically-computed Pathsets and Cutsets</title><categories>cs.NI</categories><proxy>ccsd</proxy><report-no>RR-8267</report-no><journal-ref>N&amp;deg; RR-8267 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a set of terminal nodes K that belong to a network whose nodes are
connected by links that fail independently with known probabilities. We
introduce a method for estimating any performability measure that depends on
the hop distance between terminal nodes. It generalises previously introduced
Monte Carlo methods for estimation of the K-reliability of networks with
variance reduction compared to crude Monte Carlo. They are based on using sets
of edges named d-pathsets and d-cutsets for reducing the variance of the
estimator. These sets of edges, considered as a priori known in previous
literature, heaviliy affect the attained performance; we hereby introduce and
compare a family of heuristics for their selection. Numerical examples are
presented, showing the significant efficiency improvements that can be obtained
by chaining the edge set selection heuristics to the proposed Monte Carlo
sampling plan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5305</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5305</id><created>2013-03-21</created><updated>2014-03-11</updated><authors><author><keyname>Tillmann</keyname><forenames>Andreas M.</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Gribonval</keyname><forenames>R&#xe9;mi</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Pfetsch</keyname><forenames>Marc E.</forenames></author></authors><title>Projection onto the Cosparse Set is NP-Hard</title><categories>cs.CC</categories><comments>to appear in ICASSP 2014</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computational complexity of a problem arising in the context of sparse
optimization is considered, namely, the projection onto the set of $k$-cosparse
vectors w.r.t. some given matrix $\Omeg$. It is shown that this projection
problem is (strongly) \NP-hard, even in the special cases in which the matrix
$\Omeg$ contains only ternary or bipolar coefficients. Interestingly, this is
in contrast to the projection onto the set of $k$-sparse vectors, which is
trivially solved by keeping only the $k$ largest coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5310</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5310</id><created>2013-03-21</created><authors><author><keyname>Di Renzo</keyname><forenames>Marco</forenames></author><author><keyname>Iezzi</keyname><forenames>Michela</forenames></author><author><keyname>Graziosi</keyname><forenames>Fabio</forenames></author></authors><title>Error Performance and Diversity Analysis of Multi-Source Multi-Relay
  Wireless Networks with Binary Network Coding and Cooperative MRC</title><categories>cs.IT math.IT</categories><comments>accepted, IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we contribute to the theoretical understanding, the design,
and the performance evaluation of multi-source multi-relay network-coded
cooperative diversity protocols. These protocols are useful to counteract the
spectral inefficiency of repetition-based cooperation. We provide a general
analytical framework for analysis and design of wireless networks using the
Demodulate-and-Forward (DemF) protocol with binary Network Coding (NC) at the
relays and Cooperative Maximal Ratio Combining (C-MRC) at the destination. Our
system model encompasses an arbitrary number of relays which offer two
cooperation levels: i) full-cooperative relays, which postpone the transmission
of their own data frames to help the transmission of the sources via DemF
relaying and binary NC; and ii) partial-cooperative relays, which exploit NC to
transmit their own data frames along with the packets received from the
sources. The relays can apply NC on different subsets of sources, which is
shown to provide the sources with unequal diversity orders. Guidelines to
choose the packets to be combined, i.e., the network code, to achieve the
desired diversity order are given. Our study shows that partial-cooperative
relays provide no contribution to the diversity order of the sources.
Theoretical findings and design guidelines are validated through extensive
Monte Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5313</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5313</id><created>2013-03-21</created><authors><author><keyname>Veldhuizen</keyname><forenames>Todd L.</forenames></author></authors><title>Incremental Maintenance for Leapfrog Triejoin</title><categories>cs.DB cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an incremental maintenance algorithm for leapfrog triejoin. The
algorithm maintains rules in time proportional (modulo log factors) to the edit
distance between leapfrog triejoin traces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5315</identifier>
 <datestamp>2014-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5315</id><created>2013-03-21</created><updated>2014-07-02</updated><authors><author><keyname>Lokhov</keyname><forenames>Andrey Y.</forenames></author><author><keyname>M&#xe9;zard</keyname><forenames>Marc</forenames></author><author><keyname>Ohta</keyname><forenames>Hiroki</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Inferring the origin of an epidemic with a dynamic message-passing
  algorithm</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI q-bio.PE</categories><comments>9 pages, 8 figures. Revised version, new figures added</comments><journal-ref>Phys. Rev. E 90, 012801 (2014)</journal-ref><doi>10.1103/PhysRevE.90.012801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of estimating the origin of an epidemic outbreak --
given a contact network and a snapshot of epidemic spread at a certain time,
determine the infection source. Finding the source is important in different
contexts of computer or social networks. We assume that the epidemic spread
follows the most commonly used susceptible-infected-recovered model. We
introduce an inference algorithm based on dynamic message-passing equations,
and we show that it leads to significant improvement of performance compared to
existing approaches. Importantly, this algorithm remains efficient in the case
where one knows the state of only a fraction of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5319</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5319</id><created>2013-03-21</created><updated>2014-01-04</updated><authors><author><keyname>Lockhart</keyname><forenames>J.</forenames></author><author><keyname>Di Franco</keyname><forenames>C.</forenames></author><author><keyname>Paternostro</keyname><forenames>M.</forenames></author></authors><title>Glued trees algorithm under phase damping</title><categories>quant-ph cs.DS</categories><comments>6 pages, 7 figures, RevTeX4</comments><journal-ref>Phys. Lett. A 378, 338 (2014)</journal-ref><doi>10.1016/j.physleta.2013.11.034</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the behaviour of the glued trees algorithm described by Childs et
al. in [STOC `03, Proc. 35th ACM Symposium on Theory of Computing (2004) 59]
under decoherence. We consider a discrete time reformulation of the continuous
time quantum walk protocol and apply a phase damping channel to the coin state,
investigating the effect of such a mechanism on the probability of the walker
appearing on the target vertex of the graph. We pay particular attention to any
potential advantage coming from the use of weak decoherence for the spreading
of the walk across the glued trees graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5321</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5321</id><created>2013-03-21</created><authors><author><keyname>Dierks</keyname><forenames>Stefan</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author><author><keyname>Zirwas</keyname><forenames>Wolfgang</forenames></author></authors><title>Feasibility Conditions of Interference Alignment via Two Orthogonal
  Subcarriers</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditions are derived on line-of-sight channels to ensure the feasibility of
interference alignment. The conditions involve choosing only the spacing
between two subcarriers of an orthogonal frequency division multiplexing (OFDM)
scheme. The maximal degrees-of-freedom are achieved and even an upper bound on
the sum-rate of interference alignment is approached arbitrarily closely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5364</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5364</id><created>2013-03-21</created><updated>2013-03-24</updated><authors><author><keyname>Faisal</keyname><forenames>S.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Javaid</keyname><forenames>A.</forenames></author><author><keyname>Khan</keyname><forenames>M. A.</forenames></author><author><keyname>Bouk</keyname><forenames>S. H.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Z-SEP: Zonal-Stable Election Protocol for Wireless Sensor Networks</title><categories>cs.NI</categories><journal-ref>Journal of Basic and Applied Scientific Research (JBASR), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) are comprised of thousands of sensor nodes,
with restricted energy, that co-operate to accomplish a sensing task. Various
routing Protocols are designed for transmission in WSNs. In this paper, we
proposed a hybrid routing protocol: Zonal-Stable Election Protocol (Z-SEP) for
heterogeneous WSNs. In this protocol, some nodes transmit data directly to base
station while some use clustering technique to send data to base station as in
SEP. We implemented Z-SEP and compared it with traditional Low Energy adaptive
clustering hierarchy (LEACH) and SEP. Simulation results showed that Z-SEP
enhanced the stability period and throughput than existing protocols like LEACH
and SEP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5365</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5365</id><created>2013-03-21</created><updated>2013-03-24</updated><authors><author><keyname>Rasheedl</keyname><forenames>M. B.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Javaid</keyname><forenames>A.</forenames></author><author><keyname>Khan</keyname><forenames>M. A.</forenames></author><author><keyname>Bouk</keyname><forenames>S. H.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Improving Network Efficiency by Removing Energy Holes in WSNs</title><categories>cs.NI</categories><journal-ref>Journal of Basic and Applied Scientific Research (JBASR), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cluster based Wireless Sensor Networks (WSNs) have been widely used for
better performance in terms of energy efficiency. Efficient use of energy is
challenging task of designing these protocols. Energy holedare created due to
quickly drain the energy of a few nodes due to non-uniform distribution in the
network. Normally, energy holes make the data routing failure when nodes
transmit data back to the base station. We proposedEnergy-efficient
HOleRemoving Mechanism (E-HORM) technique to remove energy holes. In this
technique, we use sleep and awake mechanism for sensor nodes to save energy.
This approach finds the maximum distance node to calculate the maximum energy
for data transmission. We considered it as a threshold energy Eth. Every node
first checks its energy level for data transmission. If the energy level is
less than Eth, it cannot transmit data. We also explain mathematically the
energy consumption and average energy saving of sensor nodes in each round.
Extensive simulations showed that when use this approach for WSNs significantly
helps to extend the network lifetime and stability period.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5367</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5367</id><created>2013-03-21</created><updated>2014-03-16</updated><authors><author><keyname>Dendek</keyname><forenames>Piotr Jan</forenames></author><author><keyname>Czeczko</keyname><forenames>Artur</forenames></author><author><keyname>Fedoryszak</keyname><forenames>Mateusz</forenames></author><author><keyname>Kawa</keyname><forenames>Adam</forenames></author><author><keyname>Wendykier</keyname><forenames>Piotr</forenames></author><author><keyname>Bolikowski</keyname><forenames>Lukasz</forenames></author></authors><title>Taming the zoo - about algorithms implementation in the ecosystem of
  Apache Hadoop</title><categories>cs.IR cs.DL</categories><comments>This paper (with changed content) appeared under the title &quot;Content
  Analysis of Scientific Articles in Apache Hadoop Ecosystem&quot; in &quot;Intelligent
  Tools for Building a Scientific Information Platform: From Research to
  Implementation&quot;, &quot;Studies in Computational Intelligence&quot;, Volume 541, 2014,
  http://link.springer.com/book/10.1007/978-3-319-04714-0</comments><acm-class>H.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content Analysis System (CoAnSys) is a research framework for mining
scientific publications using Apache Hadoop. This article describes the
algorithms currently implemented in CoAnSys including classification,
categorization and citation matching of scientific publications. The size of
the input data classifies these algorithms in the range of big data problems,
which can be efficiently solved on Hadoop clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5387</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5387</id><created>2013-03-21</created><authors><author><keyname>Liu</keyname><forenames>Jianxing</forenames></author><author><keyname>Laghrouche</keyname><forenames>Salah</forenames></author><author><keyname>Wack</keyname><forenames>Maxime</forenames></author></authors><title>Adaptive High Order Sliding Mode Observer Based Fault Reconstruction for
  a Class of Nonlinear Uncertain Systems: Application to PEM Fuel Cell System</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on observer based fault reconstruction for a class of
nonlinear uncertain systems with Lipschitz nonlinearities. An adaptive-gain
Super-Twisting (STW) observer is developed for observing the system states,
where the adaptive law compensates the uncertainty in parameters. The inherent
equivalent output error injection feature of STW algorithm is then used to
reconstruct the fault signal. The performance of the proposed observer is
validated through a Hardware-In-Loop (HIL) simulator which consists of a
commercial twin screw compressor and a real time Polymer Electrolyte Membrane
fuel cell emulation system. The simulation results illustrate the feasibility
and effectiveness of the proposed approach for application to fuel cell
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5388</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5388</id><created>2013-03-21</created><authors><author><keyname>M&#xe9;rigot</keyname><forenames>Quentin</forenames><affiliation>LJK</affiliation></author></authors><title>Lower bounds for k-distance approximation</title><categories>cs.CG</categories><proxy>ccsd</proxy><journal-ref>ACM Symposium on Computational Geometry, Rio de Janeiro : France
  (2013)</journal-ref><doi>10.1145/2462356.2462367</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a set P of N random points on the unit sphere of dimension $d-1$,
and the symmetrized set S = P union (-P). The halving polyhedron of S is
defined as the convex hull of the set of centroids of N distinct points in S.
We prove that after appropriate rescaling this halving polyhedron is Hausdorff
close to the unit ball with high probability, as soon as the number of points
grows like $Omega(d log(d))$. From this result, we deduce probabilistic lower
bounds on the complexity of approximations of the distance to the empirical
measure on the point set by distance-like functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5391</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5391</id><created>2013-03-13</created><authors><author><keyname>An</keyname><forenames>Zhi</forenames></author><author><keyname>Bell</keyname><forenames>David A.</forenames></author><author><keyname>Hughes</keyname><forenames>John G.</forenames></author></authors><title>RES - a Relative Method for Evidential Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-1-8</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a novel method for evidential reasoning [1]. It
involves modelling the process of evidential reasoning in three steps, namely,
evidence structure construction, evidence accumulation, and decision making.
The proposed method, called RES, is novel in that evidence strength is
associated with an evidential support relationship (an argument) between a pair
of statements and such strength is carried by comparison between arguments.
This is in contrast to the onventional approaches, where evidence strength is
represented numerically and is associated with a statement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5392</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5392</id><created>2013-03-13</created><authors><author><keyname>Bouckaert</keyname><forenames>Remco R.</forenames></author></authors><title>Optimizing Causal Orderings for Generating DAGs from Data</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-9-16</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm for generating the structure of a directed acyclic graph from
data using the notion of causal input lists is presented. The algorithm
manipulates the ordering of the variables with operations which very much
resemble arc reversal. Operations are only applied if the DAG after the
operation represents at least the independencies represented by the DAG before
the operation until no more arcs can be removed from the DAG. The resulting DAG
is a minimal l-map.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5393</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5393</id><created>2013-03-13</created><authors><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author></authors><title>Modal Logics for Qualitative Possibility and Beliefs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-17-24</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Possibilistic logic has been proposed as a numerical formalism for reasoning
with uncertainty. There has been interest in developing qualitative accounts of
possibility, as well as an explanation of the relationship between possibility
and modal logics. We present two modal logics that can be used to represent and
reason with qualitative statements of possibility and necessity. Within this
modal framework, we are able to identify interesting relationships between
possibilistic logic, beliefs and conditionals. In particular, the most natural
conditional definable via possibilistic means for default reasoning is
identical to Pearl's conditional for e-semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5394</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5394</id><created>2013-03-13</created><authors><author><keyname>Chan</keyname><forenames>Brian Y.</forenames></author><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author></authors><title>Structural Controllability and Observability in Influence Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-25-32</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence diagram is a graphical representation of belief networks with
uncertainty. This article studies the structural properties of a probabilistic
model in an influence diagram. In particular, structural controllability
theorems and structural observability theorems are developed and algorithms are
formulated. Controllability and observability are fundamental concepts in
dynamic systems (Luenberger 1979). Controllability corresponds to the ability
to control a system while observability analyzes the inferability of its
variables. Both properties can be determined by the ranks of the system
matrices. Structural controllability and observability, on the other hand,
analyze the property of a system with its structure only, without the specific
knowledge of the values of its elements (tin 1974, Shields and Pearson 1976).
The structural analysis explores the connection between the structure of a
model and the functional dependence among its elements. It is useful in
comprehending problem and formulating solution by challenging the underlying
intuitions and detecting inconsistency in a model. This type of qualitative
reasoning can sometimes provide insight even when there is insufficient
numerical information in a model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5395</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5395</id><created>2013-03-13</created><authors><author><keyname>Chatalic</keyname><forenames>Philippe</forenames></author><author><keyname>Froidevaux</keyname><forenames>Christine</forenames></author></authors><title>Lattice-Based Graded Logic: a Multimodal Approach</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-33-40</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Experts do not always feel very, comfortable when they have to give precise
numerical estimations of certainty degrees. In this paper we present a
qualitative approach which allows for attaching partially ordered symbolic
grades to logical formulas. Uncertain information is expressed by means of
parameterized modal operators. We propose a semantics for this multimodal logic
and give a sound and complete axiomatization. We study the links with related
approaches and suggest how this framework might be used to manage both
uncertain and incomplere knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5396</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5396</id><created>2013-03-13</created><authors><author><keyname>Dagum</keyname><forenames>Paul</forenames></author><author><keyname>Galper</keyname><forenames>Adam</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author></authors><title>Dynamic Network Models for Forecasting</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-41-48</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have developed a probabilistic forecasting methodology through a synthesis
of belief network models and classical time-series analysis. We present the
dynamic network model (DNM) and describe methods for constructing, refining,
and performing inference with this representation of temporal probabilistic
knowledge. The DNM representation extends static belief-network models to more
general dynamic forecasting models by integrating and iteratively refining
contemporaneous and time-lagged dependencies. We discuss key concepts in terms
of a model for forecasting U.S. car sales in Japan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5397</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5397</id><created>2013-03-13</created><authors><author><keyname>Dagum</keyname><forenames>Paul</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author></authors><title>Reformulating Inference Problems Through Selective Conditioning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-49-54</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe how we selectively reformulate portions of a belief network that
pose difficulties for solution with a stochastic-simulation algorithm. With
employ the selective conditioning approach to target specific nodes in a belief
network for decomposition, based on the contribution the nodes make to the
tractability of stochastic simulation. We review previous work on BNRAS
algorithms- randomized approximation algorithms for probabilistic inference. We
show how selective conditioning can be employed to reformulate a single BNRAS
problem into multiple tractable BNRAS simulation problems. We discuss how we
can use another simulation algorithm-logic sampling-to solve a component of the
inference problem that provides a means for knitting the solutions of
individual subproblems into a final result. Finally, we analyze tradeoffs among
the computational subtasks associated with the selective conditioning approach
to reformulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5398</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5398</id><created>2013-03-13</created><authors><author><keyname>Dalkey</keyname><forenames>Norman C.</forenames></author></authors><title>Entropy and Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-55-58</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The product expansion of conditional probabilities for belief nets is not
maximum entropy. This appears to deny a desirable kind of assurance for the
model. However, a kind of guarantee that is almost as strong as maximum entropy
can be derived. Surprisingly, a variant model also exhibits the guarantee, and
for many cases obtains a higher performance score than the product expansion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5399</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5399</id><created>2013-03-13</created><authors><author><keyname>D'Ambrosio</keyname><forenames>Bruce</forenames></author><author><keyname>Fountain</keyname><forenames>Tony</forenames></author><author><keyname>Li</keyname><forenames>Zhaoyu</forenames></author></authors><title>Parallelizing Probabilistic Inference: Some Early Explorations</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-59-66</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on an experimental investigation into opportunities for parallelism
in beliefnet inference. Specifically, we report on a study performed of the
available parallelism, on hypercube style machines, of a set of randomly
generated belief nets, using factoring (SPI) style inference algorithms. Our
results indicate that substantial speedup is available, but that it is
available only through parallelization of individual conformal product
operations, and depends critically on finding an appropriate factoring. We find
negligible opportunity for parallelism at the topological, or clustering tree,
level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5400</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5400</id><created>2013-03-13</created><authors><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author></authors><title>Objection-Based Causal Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-67-73</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the notion of objection-based causal networks which
resemble probabilistic causal networks except that they are quantified using
objections. An objection is a logical sentence and denotes a condition under
which a, causal dependency does not exist. Objection-based causal networks
enjoy almost all the properties that make probabilistic causal networks
popular, with the added advantage that objections are, arguably more intuitive
than probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5401</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5401</id><created>2013-03-13</created><authors><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author><author><keyname>Godo</keyname><forenames>Lluis</forenames></author><author><keyname>de Mantaras</keyname><forenames>Ramon Lopez</forenames></author></authors><title>A Symbolic Approach to Reasoning with Linguistic Quantifiers</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-74-82</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the possibility of performing automated reasoning in
probabilistic logic when probabilities are expressed by means of linguistic
quantifiers. Each linguistic term is expressed as a prescribed interval of
proportions. Then instead of propagating numbers, qualitative terms are
propagated in accordance with the numerical interpretation of these terms. The
quantified syllogism, modelling the chaining of probabilistic rules, is studied
in this context. It is shown that a qualitative counterpart of this syllogism
makes sense, and is relatively independent of the threshold defining the
linguistically meaningful intervals, provided that these threshold values
remain in accordance with the intuition. The inference power is less than that
of a full-fledged probabilistic con-quaint propagation device but better
corresponds to what could be thought of as commonsense probabilistic reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5402</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5402</id><created>2013-03-13</created><authors><author><keyname>Monai</keyname><forenames>Francesco Fulvio</forenames></author><author><keyname>Chehire</keyname><forenames>Thomas</forenames></author></authors><title>Possibilistic Assumption based Truth Maintenance System, Validation in a
  Data Fusion Application</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-83-91</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data fusion allows the elaboration and the evaluation of a situation
synthesized from low level informations provided by different kinds of sensors.
The fusion of the collected data will result in fewer and higher level
informations more easily assessed by a human operator and that will assist him
effectively in his decision process. In this paper we present the suitability
and the advantages of using a Possibilistic Assumption based Truth Maintenance
System (n-ATMS) in a data fusion military application. We first describe the
problem, the needed knowledge representation formalisms and problem solving
paradigms. Then we remind the reader of the basic concepts of ATMSs,
Possibilistic Logic and 11-ATMSs. Finally we detail the solution to the given
data fusion problem and conclude with the results and comparison with a
non-possibilistic solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5403</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5403</id><created>2013-03-13</created><authors><author><keyname>Geiger</keyname><forenames>Dan</forenames></author></authors><title>An Entropy-based Learning Algorithm of Bayesian Conditional Trees</title><categories>cs.LG cs.AI cs.CV</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-92-97</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article offers a modification of Chow and Liu's learning algorithm in
the context of handwritten digit recognition. The modified algorithm directs
the user to group digits into several classes consisting of digits that are
hard to distinguish and then constructing an optimal conditional tree
representation for each class of digits instead of for each single digit as
done by Chow and Liu (1968). Advantages and extensions of the new method are
discussed. Related works of Wong and Wang (1977) and Wong and Poon (1989) which
offer a different entropy-based learning algorithm are shown to rest on
inappropriate assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5404</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5404</id><created>2013-03-13</created><authors><author><keyname>Gilio</keyname><forenames>Angelo</forenames></author><author><keyname>Spezzaferri</keyname><forenames>Fulvio</forenames></author></authors><title>Knowledge Integration for Conditional Probability Assessments</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-98-103</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the probabilistic approach to uncertainty management the input knowledge
is usually represented by means of some probability distributions. In this
paper we assume that the input knowledge is given by two discrete conditional
probability distributions, represented by two stochastic matrices P and Q. The
consistency of the knowledge base is analyzed. Coherence conditions and
explicit formulas for the extension to marginal distributions are obtained in
some special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5405</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5405</id><created>2013-03-13</created><authors><author><keyname>Goldman</keyname><forenames>Robert P.</forenames></author><author><keyname>Breese</keyname><forenames>John S.</forenames></author></authors><title>Integrating Model Construction and Evaluation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-104-111</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To date, most probabilistic reasoning systems have relied on a fixed belief
network constructed at design time. The network is used by an application
program as a representation of (in)dependencies in the domain. Probabilistic
inference algorithms operate over the network to answer queries. Recognizing
the inflexibility of fixed models has led researchers to develop automated
network construction procedures that use an expressive knowledge base to
generate a network that can answer a query. Although more flexible than fixed
model approaches, these construction procedures separate construction and
evaluation into distinct phases. In this paper we develop an approach to
combining incremental construction and evaluation of a partial probability
model. The combined method holds promise for improved methods for control of
model construction based on a trade-off between fidelity of results and cost of
construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5406</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5406</id><created>2013-03-13</created><authors><author><keyname>Goldszmidt</keyname><forenames>Moises</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Reasoning With Qualitative Probabilities Can Be Tractable</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-112-120</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We recently described a formalism for reasoning with if-then rules that re
expressed with different levels of firmness [18]. The formalism interprets
these rules as extreme conditional probability statements, specifying orders of
magnitude of disbelief, which impose constraints over possible rankings of
worlds. It was shown that, once we compute a priority function Z+ on the rules,
the degree to which a given query is confirmed or denied can be computed in
O(log n`) propositional satisfiability tests, where n is the number of rules in
the knowledge base. In this paper, we show that computing Z+ requires O(n2 X
log n) satisfiability tests, not an exponential number as was conjectured in
[18], which reduces to polynomial complexity in the case of Horn expressions.
We also show how reasoning with imprecise observations can be incorporated in
our formalism and how the popular notions of belief revision and epistemic
entrenchment are embodied naturally and tractably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5407</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5407</id><created>2013-03-13</created><authors><author><keyname>Kj&#xe6;rulff</keyname><forenames>Uffe</forenames></author></authors><title>A computational scheme for Reasoning in Dynamic Probabilistic Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-121-129</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A computational scheme for reasoning about dynamic systems using (causal)
probabilistic networks is presented. The scheme is based on the framework of
Lauritzen and Spiegelhalter (1988), and may be viewed as a generalization of
the inference methods of classical time-series analysis in the sense that it
allows description of non-linear, multivariate dynamic systems with complex
conditional independence structures. Further, the scheme provides a method for
efficient backward smoothing and possibilities for efficient, approximate
forecasting methods. The scheme has been implemented on top of the HUGIN shell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5408</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5408</id><created>2013-03-13</created><authors><author><keyname>Klawonn</keyname><forenames>Frank</forenames></author><author><keyname>Smets</keyname><forenames>Philippe</forenames></author></authors><title>The Dynamic of Belief in the Transferable Belief Model and
  Specialization-Generalization Matrices</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-130-137</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fundamental updating process in the transferable belief model is related
to the concept of specialization and can be described by a specialization
matrix. The degree of belief in the truth of a proposition is a degree of
justified support. The Principle of Minimal Commitment implies that one should
never give more support to the truth of a proposition than justified. We show
that Dempster's rule of conditioning corresponds essentially to the least
committed specialization, and that Dempster's rule of combination results
essentially from commutativity requirements. The concept of generalization,
dual to thc concept of specialization, is described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5409</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5409</id><created>2013-03-13</created><authors><author><keyname>Klir</keyname><forenames>George J.</forenames></author><author><keyname>Parviz</keyname><forenames>Behzad</forenames></author></authors><title>A Note on the Measure of Discord</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-138-141</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new entropy-like measure as well as a new measure of total uncertainty
pertaining to the Dempster-Shafer theory are introduced. It is argued that
these measures are better justified than any of the previously proposed
candidates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5410</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5410</id><created>2013-03-13</created><authors><author><keyname>Kyburg</keyname><forenames>Henry E.</forenames><suffix>Jr</suffix></author></authors><title>Semantics for Probabilistic Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-142-148</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of writers(Joseph Halpern and Fahiem Bacchus among them) have
offered semantics for formal languages in which inferences concerning
probabilities can be made. Our concern is different. This paper provides a
formalization of nonmonotonic inferences in which the conclusion is supported
only to a certain degree. Such inferences are clearly 'invalid' since they must
allow the falsity of a conclusion even when the premises are true.
Nevertheless, such inferences can be characterized both syntactically and
semantically. The 'premises' of probabilistic arguments are sets of statements
(as in a database or knowledge base), the conclusions categorical statements in
the language. We provide standards for both this form of inference, for which
high probability is required, and for an inference in which the conclusion is
qualified by an intermediate interval of support.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5411</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5411</id><created>2013-03-13</created><authors><author><keyname>Kyburg</keyname><forenames>Henry E.</forenames><suffix>Jr.</suffix></author><author><keyname>Pittarelli</keyname><forenames>Michael</forenames></author></authors><title>Some Problems for Convex Bayesians</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-149-154</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss problems for convex Bayesian decision making and uncertainty
representation. These include the inability to accommodate various natural and
useful constraints and the possibility of an analog of the classical Dutch Book
being made against an agent behaving in accordance with convex Bayesian
prescriptions. A more general set-based Bayesianism may be as tractable and
would avoid the difficulties we raise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5412</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5412</id><created>2013-03-13</created><authors><author><keyname>Laskey</keyname><forenames>Kathryn Blackmond</forenames></author></authors><title>Bayesian Meta-Reasoning: Determining Model Adequacy from Within a Small
  World</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-155-158</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a Bayesian framework for assessing the adequacy of a
model without the necessity of explicitly enumerating a specific alternate
model. A test statistic is developed for tracking the performance of the model
across repeated problem instances. Asymptotic methods are used to derive an
approximate distribution for the test statistic. When the model is rejected,
the individual components of the test statistic can be used to guide search for
an alternate model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5413</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5413</id><created>2013-03-13</created><authors><author><keyname>Laskey</keyname><forenames>Kathryn Blackmond</forenames></author></authors><title>The Bounded Bayesian</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-159-165</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ideal Bayesian agent reasons from a global probability model, but real
agents are restricted to simplified models which they know to be adequate only
in restricted circumstances. Very little formal theory has been developed to
help fallibly rational agents manage the process of constructing and revising
small world models. The goal of this paper is to present a theoretical
framework for analyzing model management approaches. For a probability
forecasting problem, a search process over small world models is analyzed as an
approximation to a larger-world model which the agent cannot explicitly
enumerate or compute. Conditions are given under which the sequence of
small-world models converges to the larger-world probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5414</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5414</id><created>2013-03-13</created><authors><author><keyname>Leong</keyname><forenames>Tze-Yun</forenames></author></authors><title>Representing Context-Sensitive Knowledge in a Network Formalism: A
  Preliminary Report</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-166-173</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated decision making is often complicated by the complexity of the
knowledge involved. Much of this complexity arises from the context sensitive
variations of the underlying phenomena. We propose a framework for representing
descriptive, context-sensitive knowledge. Our approach attempts to integrate
categorical and uncertain knowledge in a network formalism. This paper outlines
the basic representation constructs, examines their expressiveness and
efficiency, and discusses the potential applications of the framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5415</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5415</id><created>2013-03-13</created><authors><author><keyname>Lin</keyname><forenames>Dekang</forenames></author></authors><title>A Probabilistic Network of Predicates</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-174-181</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian networks are directed acyclic graphs representing independence
relationships among a set of random variables. A random variable can be
regarded as a set of exhaustive and mutually exclusive propositions. We argue
that there are several drawbacks resulting from the propositional nature and
acyclic structure of Bayesian networks. To remedy these shortcomings, we
propose a probabilistic network where nodes represent unary predicates and
which may contain directed cycles. The proposed representation allows us to
represent domain knowledge in a single static network even though we cannot
determine the instantiations of the predicates before hand. The ability to deal
with cycles also enables us to handle cyclic causal tendencies and to recognize
recursive plans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5416</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5416</id><created>2013-03-13</created><authors><author><keyname>Liu</keyname><forenames>Weiru</forenames></author><author><keyname>Hughes</keyname><forenames>John G.</forenames></author><author><keyname>McTear</keyname><forenames>Michael F.</forenames></author></authors><title>Representing Heuristic Knowledge in D-S Theory</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-182-190</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Dempster-Shafer theory of evidence has been used intensively to deal with
uncertainty in knowledge-based systems. However the representation of uncertain
relationships between evidence and hypothesis groups (heuristic knowledge) is
still a major research problem. This paper presents an approach to representing
such heuristic knowledge by evidential mappings which are defined on the basis
of mass functions. The relationships between evidential mappings and multi
valued mappings, as well as between evidential mappings and Bayesian multi-
valued causal link models in Bayesian theory are discussed. Following this the
detailed procedures for constructing evidential mappings for any set of
heuristic rules are introduced. Several situations of belief propagation are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5417</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5417</id><created>2013-03-13</created><authors><author><keyname>Matzkevich</keyname><forenames>Izhar</forenames></author><author><keyname>Abramson</keyname><forenames>Bruce</forenames></author></authors><title>The Topological Fusion of Bayes Nets</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-191-198</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayes nets are relatively recent innovations. As a result, most of their
theoretical development has focused on the simplest class of single-author
models. The introduction of more sophisticated multiple-author settings raises
a variety of interesting questions. One such question involves the nature of
compromise and consensus. Posterior compromises let each model process all data
to arrive at an independent response, and then split the difference. Prior
compromises, on the other hand, force compromise to be reached on all points
before data is observed. This paper introduces prior compromises in a Bayes net
setting. It outlines the problem and develops an efficient algorithm for fusing
two directed acyclic graphs into a single, consensus structure, which may then
be used as the basis of a prior compromise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5418</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5418</id><created>2013-03-13</created><authors><author><keyname>Moral</keyname><forenames>Serafin</forenames></author></authors><title>Calculating Uncertainty Intervals From Conditional Convex Sets of
  Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-199-206</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Moral, Campos (1991) and Cano, Moral, Verdegay-Lopez (1991) a new method
of conditioning convex sets of probabilities has been proposed. The result of
it is a convex set of non-necessarily normalized probability distributions. The
normalizing factor of each probability distribution is interpreted as the
possibility assigned to it by the conditioning information. From this, it is
deduced that the natural value for the conditional probability of an event is a
possibility distribution. The aim of this paper is to study methods of
transforming this possibility distribution into a probability (or uncertainty)
interval. These methods will be based on the use of Sugeno and Choquet
integrals. Their behaviour will be compared in basis to some selected examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5419</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5419</id><created>2013-03-13</created><authors><author><keyname>Nicholson</keyname><forenames>Ann</forenames></author><author><keyname>Brady</keyname><forenames>J. M.</forenames></author></authors><title>Sensor Validation Using Dynamic Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-207-214</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The trajectory of a robot is monitored in a restricted dynamic environment
using light beam sensor data. We have a Dynamic Belief Network (DBN), based on
a discrete model of the domain, which provides discrete monitoring analogous to
conventional quantitative filter techniques. Sensor observations are added to
the basic DBN in the form of specific evidence. However, sensor data is often
partially or totally incorrect. We show how the basic DBN, which infers only an
impossible combination of evidence, may be modified to handle specific types of
incorrect data which may occur in the domain. We then present an extension to
the DBN, the addition of an invalidating node, which models the status of the
sensor as working or defective. This node provides a qualitative explanation of
inconsistent data: it is caused by a defective sensor. The connection of
successive instances of the invalidating node models the status of a sensor
over time, allowing the DBN to handle both persistent and intermittent faults.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5420</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5420</id><created>2013-03-13</created><authors><author><keyname>Ng</keyname><forenames>Raymond T.</forenames></author><author><keyname>Subrahmanian</keyname><forenames>V. S.</forenames></author></authors><title>Empirical Probabilities in Monadic Deductive Databases</title><categories>cs.AI cs.DB</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-215-222</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of supporting empirical probabilities in monadic logic
databases. Though the semantics of multivalued logic programs has been studied
extensively, the treatment of probabilities as results of statistical findings
has not been studied in logic programming/deductive databases. We develop a
model-theoretic characterization of logic databases that facilitates such a
treatment. We present an algorithm for checking consistency of such databases
and prove its total correctness. We develop a sound and complete query
processing procedure for handling queries to such databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5421</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5421</id><created>2013-03-13</created><authors><author><keyname>Olesen</keyname><forenames>Kristian G.</forenames></author><author><keyname>Lauritzen</keyname><forenames>Steffen L.</forenames></author><author><keyname>Jensen</keyname><forenames>Finn Verner</forenames></author></authors><title>aHUGIN: A System Creating Adaptive Causal Probabilistic Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-223-229</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes aHUGIN, a tool for creating adaptive systems. aHUGIN is
an extension of the HUGIN shell, and is based on the methods reported by
Spiegelhalter and Lauritzen (1990a). The adaptive systems resulting from aHUGIN
are able to adjust the C011ditional probabilities in the model. A short
analysis of the adaptation task is given and the features of aHUGIN are
described. Finally a session with experiments is reported and the results are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5422</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5422</id><created>2013-03-13</created><authors><author><keyname>Paa&#xdf;</keyname><forenames>Gerhard</forenames></author></authors><title>MESA: Maximum Entropy by Simulated Annealing</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-230-237</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic reasoning systems combine different probabilistic rules and
probabilistic facts to arrive at the desired probability values of
consequences. In this paper we describe the MESA-algorithm (Maximum Entropy by
Simulated Annealing) that derives a joint distribution of variables or
propositions. It takes into account the reliability of probability values and
can resolve conflicts between contradictory statements. The joint distribution
is represented in terms of marginal distributions and therefore allows to
process large inference networks and to determine desired probability values
with high precision. The procedure derives a maximum entropy distribution
subject to the given constraints. It can be applied to inference networks of
arbitrary topology and may be extended into a number of directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5423</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5423</id><created>2013-03-13</created><authors><author><keyname>Paterson</keyname><forenames>Thomas S.</forenames></author><author><keyname>Fehling</keyname><forenames>Michael R.</forenames></author></authors><title>Decision Methods for Adaptive Task-Sharing in Associate Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-238-243</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes some results of research on associate systems:
knowledge-based systems that flexibly and adaptively support their human users
in carrying out complex, time-dependent problem-solving tasks under
uncertainty. Based on principles derived from decision theory and decision
analysis, a problem-solving approach is presented which can overcome many of
the limitations of traditional expert-systems. This approach implements an
explicit model of the human user's problem-solving capabilities as an integral
element in the overall problem solving architecture. This integrated model,
represented as an influence diagram, is the basis for achieving adaptive task
sharing behavior between the associate system and the human user. This
associate system model has been applied toward ongoing research on a Mars Rover
Manager's Associate (MRMA). MRMA's role would be to manage a small fleet of
robotic rovers on the Martian surface. The paper describes results for a
specific scenario where MRMA examines the benefits and costs of consulting
human experts on Earth to assist a Mars rover with a complex resource
management decision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5424</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5424</id><created>2013-03-13</created><authors><author><keyname>Portinale</keyname><forenames>Luigi</forenames></author></authors><title>Modeling Uncertain Temporal Evolutions in Model-Based Diagnosis</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-244-251</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the notion of diagnostic problem has been extensively investigated
in the context of static systems, in most practical applications the behavior
of the modeled system is significantly variable during time. The goal of the
paper is to propose a novel approach to the modeling of uncertainty about
temporal evolutions of time-varying systems and a characterization of
model-based temporal diagnosis. Since in most real world cases knowledge about
the temporal evolution of the system to be diagnosed is uncertain, we consider
the case when probabilistic temporal knowledge is available for each component
of the system and we choose to model it by means of Markov chains. In fact, we
aim at exploiting the statistical assumptions underlying reliability theory in
the context of the diagnosis of timevarying systems. We finally show how to
exploit Markov chain theory in order to discard, in the diagnostic process,
very unlikely diagnoses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5425</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5425</id><created>2013-03-13</created><authors><author><keyname>Qiu</keyname><forenames>Yuping</forenames></author><author><keyname>Cox,</keyname><forenames>Louis Anthony</forenames><suffix>Jr.</suffix></author><author><keyname>Davis</keyname><forenames>Lawrence</forenames></author></authors><title>Guess-And-Verify Heuristics for Reducing Uncertainties in Expert
  Classification Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-252-258</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An expert classification system having statistical information about the
prior probabilities of the different classes should be able to use this
knowledge to reduce the amount of additional information that it must collect,
e.g., through questions, in order to make a correct classification. This paper
examines how best to use such prior information and additional
information-collection opportunities to reduce uncertainty about the class to
which a case belongs, thus minimizing the average cost or effort required to
correctly classify new cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5426</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5426</id><created>2013-03-13</created><authors><author><keyname>Regan</keyname><forenames>Peter J.</forenames></author><author><keyname>Holtzman</keyname><forenames>Samuel</forenames></author></authors><title>R&amp;D Analyst: An Interactive Approach to Normative Decision System Model
  Construction</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-259-267</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the architecture of R&amp;D Analyst, a commercial
intelligent decision system for evaluating corporate research and development
projects and portfolios. In analyzing projects, R&amp;D Analyst interactively
guides a user in constructing an influence diagram model for an individual
research project. The system's interactive approach can be clearly explained
from a blackboard system perspective. The opportunistic reasoning emphasis of
blackboard systems satisfies the flexibility requirements of model
construction, thereby suggesting that a similar architecture would be valuable
for developing normative decision systems in other domains. Current research is
aimed at extending the system architecture to explicitly consider of sequential
decisions involving limited temporal, financial, and physical resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5427</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5427</id><created>2013-03-13</created><authors><author><keyname>Schiex</keyname><forenames>Thomas</forenames></author></authors><title>Possibilistic Constraint Satisfaction Problems or &quot;How to handle soft
  constraints?&quot;</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-268-275</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many AI synthesis problems such as planning or scheduling may be modelized as
constraint satisfaction problems (CSP). A CSP is typically defined as the
problem of finding any consistent labeling for a fixed set of variables
satisfying all given constraints between these variables. However, for many
real tasks such as job-shop scheduling, time-table scheduling, design?, all
these constraints have not the same significance and have not to be necessarily
satisfied. A first distinction can be made between hard constraints, which
every solution should satisfy and soft constraints, whose satisfaction has not
to be certain. In this paper, we formalize the notion of possibilistic
constraint satisfaction problems that allows the modeling of uncertainly
satisfied constraints. We use a possibility distribution over labelings to
represent respective possibilities of each labeling. Necessity-valued
constraints allow a simple expression of the respective certainty degrees of
each constraint. The main advantage of our approach is its integration in the
CSP technical framework. Most classical techniques, such as Backtracking (BT),
arcconsistency enforcing (AC) or Forward Checking have been extended to handle
possibilistics CSP and are effectively implemented. The utility of our approach
is demonstrated on a simple design problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5428</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5428</id><created>2013-03-13</created><authors><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author><author><keyname>Peot</keyname><forenames>Mark Alan</forenames></author></authors><title>Decision Making Using Probabilistic Inference Methods</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-276-283</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of decision making under uncertainty is closely related to the
analysis of probabilistic inference. Indeed, much of the research into
efficient methods for probabilistic inference in expert systems has been
motivated by the fundamental normative arguments of decision theory. In this
paper we show how the developments underlying those efficient methods can be
applied immediately to decision problems. In addition to general approaches
which need know nothing about the actual probabilistic inference method, we
suggest some simple modifications to the clustering family of algorithms in
order to efficiently incorporate decision making capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5429</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5429</id><created>2013-03-13</created><authors><author><keyname>Shenoy</keyname><forenames>Prakash P.</forenames></author></authors><title>Conditional Independence in Uncertainty Theories</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-284-291</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the notions of independence and conditional
independence in valuation-based systems (VBS). VBS is an axiomatic framework
capable of representing many different uncertainty calculi. We define
independence and conditional independence in terms of factorization of the
joint valuation. The definitions of independence and conditional independence
in VBS generalize the corresponding definitions in probability theory. Our
definitions apply not only to probability theory, but also to Dempster-Shafer's
belief-function theory, Spohn's epistemic-belief theory, and Zadeh's
possibility theory. In fact, they apply to any uncertainty calculi that fit in
the framework of valuation-based systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5430</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5430</id><created>2013-03-13</created><authors><author><keyname>Smets</keyname><forenames>Philippe</forenames></author></authors><title>The Nature of the Unnormalized Beliefs Encountered in the Transferable
  Belief Model</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-292-297</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the transferable belief model, positive basic belief masses can be
allocated to the empty set, leading to unnormalized belief functions. The
nature of these unnormalized beliefs is analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5431</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5431</id><created>2013-03-13</created><authors><author><keyname>Snow</keyname><forenames>Paul</forenames></author></authors><title>Intuitions about Ordered Beliefs Leading to Probabilistic Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-298-302</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The general use of subjective probabilities to model belief has been
justified using many axiomatic schemes. For example, ?consistent betting
behavior' arguments are well-known. To those not already convinced of the
unique fitness and generality of probability models, such justifications are
often unconvincing. The present paper explores another rationale for
probability models. ?Qualitative probability,' which is known to provide
stringent constraints on belief representation schemes, is derived from five
simple assumptions about relationships among beliefs. While counterparts of
familiar rationality concepts such as transitivity, dominance, and consistency
are used, the betting context is avoided. The gap between qualitative
probability and probability proper can be bridged by any of several additional
assumptions. The discussion here relies on results common in the recent AI
literature, introducing a sixth simple assumption. The narrative emphasizes
models based on unique complete orderings, but the rationale extends easily to
motivate set-valued representations of partial orderings as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5432</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5432</id><created>2013-03-13</created><authors><author><keyname>Sucar</keyname><forenames>Luis Enrique</forenames></author><author><keyname>Gillies</keyname><forenames>Duncan F.</forenames></author></authors><title>Expressing Relational and Temporal Knowledge in Visual Probabilistic
  Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-303-309</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian networks have been used extensively in diagnostic tasks such as
medicine, where they represent the dependency relations between a set of
symptoms and a set of diseases. A criticism of this type of knowledge
representation is that it is restricted to this kind of task, and that it
cannot cope with the knowledge required in other artificial intelligence
applications. For example, in computer vision, we require the ability to model
complex knowledge, including temporal and relational factors. In this paper we
extend Bayesian networks to model relational and temporal knowledge for
high-level vision. These extended networks have a simple structure which
permits us to propagate probability efficiently. We have applied them to the
domain of endoscopy, illustrating how the general modelling principles can be
used in specific cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5433</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5433</id><created>2013-03-13</created><authors><author><keyname>Tao</keyname><forenames>Chin-Wang</forenames></author><author><keyname>Thompson</keyname><forenames>Wiley E.</forenames></author></authors><title>A Fuzzy Logic Approach to Target Tracking</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-310-314</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses a target tracking problem in which no dynamic
mathematical model is explicitly assumed. A nonlinear filter based on the fuzzy
If-then rules is developed. A comparison with a Kalman filter is made, and
empirical results show that the performance of the fuzzy filter is better.
Intensive simulations suggest that theoretical justification of the empirical
results is possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5434</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5434</id><created>2013-03-13</created><authors><author><keyname>Thone</keyname><forenames>Helmut</forenames></author><author><keyname>Guntzer</keyname><forenames>Ulrich</forenames></author><author><keyname>Kiessling</keyname><forenames>Werner</forenames></author></authors><title>Towards Precision of Probabilistic Bounds Propagation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-315-322</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The DUCK-calculus presented here is a recent approach to cope with
probabilistic uncertainty in a sound and efficient way. Uncertain rules with
bounds for probabilities and explicit conditional independences can be
maintained incrementally. The basic inference mechanism relies on local bounds
propagation, implementable by deductive databases with a bottom-up fixpoint
evaluation. In situations, where no precise bounds are deducible, it can be
combined with simple operations research techniques on a local scope. In
particular, we provide new precise analytical bounds for probabilistic
entailment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5435</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5435</id><created>2013-03-13</created><authors><author><keyname>Verma</keyname><forenames>Tom S.</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>An Algorithm for Deciding if a Set of Observed Independencies Has a
  Causal Explanation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-323-330</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a previous paper [Pearl and Verma, 1991] we presented an algorithm for
extracting causal influences from independence information, where a causal
influence was defined as the existence of a directed arc in all minimal causal
models consistent with the data. In this paper we address the question of
deciding whether there exists a causal model that explains ALL the observed
dependencies and independencies. Formally, given a list M of conditional
independence statements, it is required to decide whether there exists a
directed acyclic graph (dag) D that is perfectly consistent with M, namely,
every statement in M, and no other, is reflected via dseparation in D. We
present and analyze an effective algorithm that tests for the existence of such
a day, and produces one, if it exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5436</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5436</id><created>2013-03-13</created><authors><author><keyname>Wagner</keyname><forenames>Carl G.</forenames></author></authors><title>Generalizing Jeffrey Conditionalization</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-331-335</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Jeffrey's rule has been generalized by Wagner to the case in which new
evidence bounds the possible revisions of a prior probability below by a
Dempsterian lower probability. Classical probability kinematics arises within
this generalization as the special case in which the evidentiary focal elements
of the bounding lower probability are pairwise disjoint. We discuss a twofold
extension of this generalization, first allowing the lower bound to be any
two-monotone capacity and then allowing the prior to be a lower envelope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5437</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5437</id><created>2013-03-13</created><authors><author><keyname>Wong</keyname><forenames>Michael S. K. M.</forenames></author><author><keyname>Wang</keyname><forenames>L. S.</forenames></author><author><keyname>Yao</keyname><forenames>Y. Y.</forenames></author></authors><title>Interval Structure: A Framework for Representing Uncertain Information</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-336-343</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a unified framework for representing uncertain information
based on the notion of an interval structure is proposed. It is shown that the
lower and upper approximations of the rough-set model, the lower and upper
bounds of incidence calculus, and the belief and plausibility functions all
obey the axioms of an interval structure. An interval structure can be used to
synthesize the decision rules provided by the experts. An efficient algorithm
to find the desirable set of rules is developed from a set of sound and
complete inference axioms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5438</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5438</id><created>2013-03-13</created><authors><author><keyname>Xiang</keyname><forenames>Yang</forenames></author><author><keyname>Poole</keyname><forenames>David L.</forenames></author><author><keyname>Beddoes</keyname><forenames>Michael P.</forenames></author></authors><title>Exploring Localization in Bayesian Networks for Large Expert Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-344-351</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current Bayesian net representations do not consider structure in the domain
and include all variables in a homogeneous network. At any time, a human
reasoner in a large domain may direct his attention to only one of a number of
natural subdomains, i.e., there is ?localization' of queries and evidence. In
such a case, propagating evidence through a homogeneous network is inefficient
since the entire network has to be updated each time. This paper presents
multiply sectioned Bayesian networks that enable a (localization preserving)
representation of natural subdomains by separate Bayesian subnets. The subnets
are transformed into a set of permanent junction trees such that evidential
reasoning takes place at only one of them at a time. Probabilities obtained are
identical to those that would be obtained from the homogeneous network. We
discuss attention shift to a different junction tree and propagation of
previously acquired evidence. Although the overall system can be large,
computational requirements are governed by the size of only one junction tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5439</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5439</id><created>2013-03-13</created><authors><author><keyname>Xu</keyname><forenames>Hong</forenames></author></authors><title>A Decision Calculus for Belief Functions in Valuation-Based Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-352-359</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Valuation-based system (VBS) provides a general framework for representing
knowledge and drawing inferences under uncertainty. Recent studies have shown
that the semantics of VBS can represent and solve Bayesian decision problems
(Shenoy, 1991a). The purpose of this paper is to propose a decision calculus
for Dempster-Shafer (D-S) theory in the framework of VBS. The proposed calculus
uses a weighting factor whose role is similar to the probabilistic
interpretation of an assumption that disambiguates decision problems
represented with belief functions (Strat 1990). It will be shown that with the
presented calculus, if the decision problems are represented in the valuation
network properly, we can solve the problems by using fusion algorithm (Shenoy
1991a). It will also be shown the presented decision calculus can be reduced to
the calculus for Bayesian probability theory when probabilities, instead of
belief functions, are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5440</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5440</id><created>2013-03-13</created><authors><author><keyname>Zhang</keyname><forenames>Nevin Lianwen</forenames></author><author><keyname>Poole</keyname><forenames>David L.</forenames></author></authors><title>Sidestepping the Triangulation Problem in Bayesian Net Computations</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighth Conference on Uncertainty in
  Artificial Intelligence (UAI1992)</comments><proxy>auai</proxy><report-no>UAI-P-1992-PG-360-367</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new approach for computing posterior probabilities in
Bayesian nets, which sidesteps the triangulation problem. The current state of
art is the clique tree propagation approach. When the underlying graph of a
Bayesian net is triangulated, this approach arranges its cliques into a tree
and computes posterior probabilities by appropriately passing around messages
in that tree. The computation in each clique is simply direct marginalization.
When the underlying graph is not triangulated, one has to first triangulated it
by adding edges. Referred to as the triangulation problem, the problem of
finding an optimal or even a ?good? triangulation proves to be difficult. In
this paper, we propose to first decompose a Bayesian net into smaller
components by making use of Tarjan's algorithm for decomposing an undirected
graph at all its minimal complete separators. Then, the components are arranged
into a tree and posterior probabilities are computed by appropriately passing
around messages in that tree. The computation in each component is carried out
by repeating the whole procedure from the beginning. Thus the triangulation
problem is sidestepped.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5441</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5441</id><created>2013-03-21</created><authors><author><keyname>Labatut</keyname><forenames>Vincent</forenames></author></authors><title>Generalized Measures for the Evaluation of Community Detection Methods</title><categories>cs.SI math.ST physics.soc-ph stat.TH</categories><proxy>ccsd</proxy><journal-ref>International Journal of Social Network Mining, 2(1):44-63, 2015</journal-ref><doi>10.1504/IJSNM.2015.069776</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection can be considered as a variant of cluster analysis
applied to complex networks. For this reason, all existing studies have been
using tools derived from this field when evaluating community detection
algorithms. However, those are not completely relevant in the context of
network analysis, because they ignore an essential part of the available
information: the network structure. Therefore, they can lead to incorrect
interpretations. In this article, we review these measures, and illustrate this
limitation. We propose a modification to solve this problem, and apply it to
the three most widespread measures: purity, Rand index and normalized mutual
information (NMI). We then perform an experimental evaluation on artificially
generated networks with realistic community structure. We assess the relevance
of the modified measures by comparison with their traditional counterparts, and
also relatively to the topological properties of the community structures. On
these data, the modified NMI turns out to provide the most relevant results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5442</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5442</id><created>2013-03-21</created><authors><author><keyname>HosseinNia</keyname><forenames>S. Hassan</forenames></author><author><keyname>Tejado</keyname><forenames>In&#xe9;s</forenames></author><author><keyname>Vinagre</keyname><forenames>Blas M.</forenames></author></authors><title>Fractional Order Hybrid Systems and Their Stability</title><categories>cs.SY nlin.AO</categories><comments>14th International Carpathian Control Conference, Krak\'ow-Rytro,
  Poland, May 26-29, 2013. arXiv admin note: text overlap with arXiv:1303.5301</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with hybrid systems (HS) with fractional order dynamics and
their stability. The stability of two particular types of fractional order
hybrid systems (FOHS), i.e., switching and reset control systems, is studied.
Common Lyapunov method, as well as its frequency domain equivalence, are
generalized for the former systems and, for the latter, H$_{\beta}$-condition
is used --frequency domain equivalence of Lyapunov-like method for reset
control systems. The applicability and efficiency of the proposed methods are
shown by some illustrative examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5452</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5452</id><created>2013-03-21</created><updated>2014-05-05</updated><authors><author><keyname>Patel</keyname><forenames>Utkarsh R.</forenames></author><author><keyname>Gustavsen</keyname><forenames>Bjorn</forenames></author><author><keyname>Triverio</keyname><forenames>Piero</forenames></author></authors><title>Fast Computation of the Series Impedance of Power Cables with Inclusion
  of Skin and Proximity Effects</title><categories>cs.CE</categories><comments>Submitted for publication to IEEE Transactions on Power Delivery.
  Update: Published in IEEE Transactions on Power Delivery with the revised
  title of &quot;An Equivalent Surface Current Approach for the Computation of the
  Series Impedance of Power Cables with Inclusion of Skin and Proximity
  Effects&quot;</comments><journal-ref>IEEE Trans. on Power Delivery, vol. 28, pp. 2474-2482, Oct. 2013</journal-ref><doi>10.1109/TPWRD.2013.2267098</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient numerical technique for calculating the series
impedance matrix of systems with round conductors. The method is based on a
surface admittance operator in combination with the method of moments and it
accurately predicts both skin and proximity effects. Application to a
three-phase armored cable with wire screens demonstrates a speed-up by a factor
of about 100 compared to a finite elements computation. The inclusion of
proximity effect in combination with the high efficiency makes the new method
very attractive for cable modeling within EMTP-type simulation tools.
Currently, these tools can only take skin effect into account.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5457</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5457</id><created>2013-03-21</created><updated>2013-11-10</updated><authors><author><keyname>Krivulin</keyname><forenames>Nikolai</forenames></author></authors><title>Explicit solution of a tropical optimization problem with application to
  project scheduling</title><categories>math.OC cs.SY</categories><comments>Mathematical Methods and Optimization Techniques in Engineering:
  Proc. 1st Intern. Conf. on Optimization Techniques in Engineering (OTENG
  '13), Antalya, Turkey, October 8-10, 2013, WSEAS Press, 2013, pp. 39-45. ISBN
  978-960-474-339-1</comments><msc-class>65K10 (Primary) 15A80, 65K05, 90C48, 90B35 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new multidimensional optimization problem is considered in the tropical
mathematics setting. The problem is to minimize a nonlinear function defined on
a finite-dimensional semimodule over an idempotent semifield and given by a
conjugate transposition operator. A special case of the problem, which arises
in just-in-time scheduling, serves as a motivation for the study. To solve the
general problem, we derive a sharp lower bound for the objective function and
then find vectors that yield the bound. Under general conditions, an explicit
solution is obtained in a compact vector form. This result is applied to
provide new solutions for scheduling problems under consideration. To
illustrate, numerical examples are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5464</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5464</id><created>2013-03-21</created><authors><author><keyname>Morales-Jimenez</keyname><forenames>D.</forenames></author><author><keyname>Lopez-Martinez</keyname><forenames>F. J.</forenames></author><author><keyname>Martos-Naya</keyname><forenames>E.</forenames></author><author><keyname>Paris</keyname><forenames>J. F.</forenames></author><author><keyname>Lozano</keyname><forenames>A.</forenames></author></authors><title>Connections between the Generalized Marcum Q-Function and a class of
  Hypergeometric Functions</title><categories>cs.IT math.IT</categories><comments>The manuscript has been submitted to IEEE Transactions on Information
  Theory. It contains 13 pages and 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new connection between the generalized Marcum-Q
function and the confluent hypergeometric function of two variables, phi3. This
result is then applied to the closed-form characterization of the bivariate
Nakagami-m distribution and of the distribution of the minimum eigenvalue of
correlated non-central Wishart matrices, both important in communication
theory. New expressions for the corresponding cumulative distributions are
obtained and a number of communication-theoretic problems involving them are
pointed out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5473</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5473</id><created>2013-03-21</created><authors><author><keyname>Barba</keyname><forenames>Luis</forenames></author><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>De Carufel</keyname><forenames>Jean-Lou</forenames></author><author><keyname>van Renssen</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Verdonschot</keyname><forenames>Sander</forenames></author></authors><title>On the stretch factor of the Theta-4 graph</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that the \theta-graph with 4 cones has constant stretch
factor, i.e., there is a path between any pair of vertices in this graph whose
length is at most a constant times the Euclidean distance between that pair of
vertices. This is the last \theta-graph for which it was not known whether its
stretch factor was bounded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5475</identifier>
 <datestamp>2014-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5475</id><created>2013-03-21</created><updated>2014-05-28</updated><authors><author><keyname>Dybizba&#x144;ski</keyname><forenames>Janusz</forenames></author><author><keyname>Dzido</keyname><forenames>Tomasz</forenames></author><author><keyname>Radziszowski</keyname><forenames>Stanis&#x142;aw</forenames></author></authors><title>On Some Zarankiewicz Numbers and Bipartite Ramsey Numbers for
  Quadrilateral</title><categories>math.CO cs.DM</categories><comments>13 pages</comments><msc-class>05C55, 05C35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Zarankiewicz number $z(m,n;s,t)$ is the maximum number of edges in a
subgraph of $K_{m,n}$ that does not contain $K_{s,t}$ as a subgraph. The
bipartite Ramsey number $b(n_1, \cdots, n_k)$ is the least positive integer $b$
such that any coloring of the edges of $K_{b,b}$ with $k$ colors will result in
a monochromatic copy of $K_{n_i,n_i}$ in the $i$-th color, for some $i$, $1 \le
i \le k$. If $n_i=m$ for all $i$, then we denote this number by $b_k(m)$. In
this paper we obtain the exact values of some Zarankiewicz numbers for
quadrilateral ($s=t=2$), and we derive new bounds for diagonal multicolor
bipartite Ramsey numbers avoiding quadrilateral. In particular, we prove that
$b_4(2)=19$, and establish new general lower and upper bounds on $b_k(2)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5479</identifier>
 <datestamp>2013-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5479</id><created>2013-03-21</created><updated>2013-06-11</updated><authors><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>Bottom-k and Priority Sampling, Set Similarity and Subset Sums with
  Minimal Independence</title><categories>cs.DS</categories><comments>A short version appeared at STOC'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider bottom-k sampling for a set X, picking a sample S_k(X) consisting
of the k elements that are smallest according to a given hash function h. With
this sample we can estimate the relative size f=|Y|/|X| of any subset Y as
|S_k(X) intersect Y|/k. A standard application is the estimation of the Jaccard
similarity f=|A intersect B|/|A union B| between sets A and B. Given the
bottom-k samples from A and B, we construct the bottom-k sample of their union
as S_k(A union B)=S_k(S_k(A) union S_k(B)), and then the similarity is
estimated as |S_k(A union B) intersect S_k(A) intersect S_k(B)|/k.
  We show here that even if the hash function is only 2-independent, the
expected relative error is O(1/sqrt(fk)). For fk=Omega(1) this is within a
constant factor of the expected relative error with truly random hashing.
  For comparison, consider the classic approach of kxmin-wise where we use k
hash independent functions h_1,...,h_k, storing the smallest element with each
hash function. For kxmin-wise there is an at least constant bias with constant
independence, and it is not reduced with larger k. Recently Feigenblat et al.
showed that bottom-k circumvents the bias if the hash function is 8-independent
and k is sufficiently large. We get down to 2-independence for any k. Our
result is based on a simply union bound, transferring generic concentration
bounds for the hashing scheme to the bottom-k sample, e.g., getting stronger
probability error bounds with higher independence.
  For weighted sets, we consider priority sampling which adapts efficiently to
the concrete input weights, e.g., benefiting strongly from heavy-tailed input.
This time, the analysis is much more involved, but again we show that generic
concentration bounds can be applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5481</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5481</id><created>2013-03-21</created><authors><author><keyname>Andreica</keyname><forenames>Mugurel Ionut</forenames></author></authors><title>Novel O(H(N)+N/H(N)) Algorithmic Techniques for Several Types of Queries
  and Updates on Rooted Trees and Lists</title><categories>cs.DS</categories><journal-ref>Metalurgia International, vol. 17, no. 12, pp. 216-223, 2012.
  (ISSN: 1582-2214)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present novel algorithmic techniques with a O(H(N)+N/H(N))
time complexity for performing several types of queries and updates on general
rooted trees, binary search trees and lists of size N. For rooted trees we
introduce a new compressed super-node tree representation which can be used for
efficiently addressing a wide range of applications. For binary search trees we
discuss the idea of globally rebuilding the entire tree in a fully balanced
manner whenever the height of the tree exceeds the value of a conveniently
chosen function of the number of tree nodes. In the end of the paper we
introduce the H-list data structure which supports concatenation, split and
several types of queries. Note that when choosing H(N)=sqrt(N) we obtain
O(H(N)+N/H(N))=O(sqrt(N)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5492</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5492</id><created>2013-03-21</created><updated>2013-07-29</updated><authors><author><keyname>Guo</keyname><forenames>Chunli</forenames></author><author><keyname>Davies</keyname><forenames>Mike E.</forenames></author></authors><title>Sample Distortion for Compressed Imaging</title><categories>cs.CV cs.IT math.IT</categories><comments>12 pages, 10 figures</comments><doi>10.1109/TSP.2013.2286775</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the notion of a sample distortion (SD) function for independent
and identically distributed (i.i.d) compressive distributions to fundamentally
quantify the achievable reconstruction performance of compressed sensing for
certain encoder-decoder pairs at a given sampling ratio. Two lower bounds on
the achievable performance and the intrinsic convexity property is derived. A
zeroing procedure is then introduced to improve non convex SD functions. The SD
framework is then applied to analyse compressed imaging with a multi-resolution
statistical image model using both the generalized Gaussian distribution and
the two-state Gaussian mixture distribution. We subsequently focus on the
Gaussian encoder-Bayesian optimal approximate message passing (AMP) decoder
pair, whose theoretical SD function is provided by the rigorous analysis of the
AMP algorithm. Given the image statistics, analytic bandwise sample allocation
for bandwise independent model is derived as a reverse water-filling scheme.
Som and Schniter's turbo message passing approach is further deployed to
integrate the bandwise sampling with the exploitation of the hidden Markov tree
structure of wavelet coefficients. Natural image simulations confirm that with
oracle image statistics, the SD function associated with the optimized sample
allocation can accurately predict the possible compressed sensing gains.
Finally, a general sample allocation profile based on average image statistics
not only illustrates preferable performance but also makes the scheme
practical.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5503</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5503</id><created>2013-03-21</created><authors><author><keyname>Ji</keyname><forenames>Zhenyi</forenames></author><author><keyname>Wu</keyname><forenames>Wenyuan</forenames></author><author><keyname>Li</keyname><forenames>Yi</forenames></author><author><keyname>Feng</keyname><forenames>Yong</forenames></author></authors><title>Numerical method for real root isolation of semi-algebraic system and
  its applications</title><categories>math.NA cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, based on the homotopy continuation method and the interval
Newton method, an efficient algorithm is introduced to isolate the real roots
of semi-algebraic system.
  Tests on some random examples and a variety of problems including
transcendental functions arising in many applications show that the new
algorithm reduces the cost substantially compared with the traditional symbolic
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5508</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5508</id><created>2013-03-21</created><updated>2013-03-28</updated><authors><author><keyname>Chen</keyname><forenames>George H.</forenames></author><author><keyname>Wachinger</keyname><forenames>Christian</forenames></author><author><keyname>Golland</keyname><forenames>Polina</forenames></author></authors><title>Sparse Projections of Medical Images onto Manifolds</title><categories>cs.CV cs.LG stat.ML</categories><comments>International Conference on Information Processing in Medical Imaging
  (IPMI 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manifold learning has been successfully applied to a variety of medical
imaging problems. Its use in real-time applications requires fast projection
onto the low-dimensional space. To this end, out-of-sample extensions are
applied by constructing an interpolation function that maps from the input
space to the low-dimensional manifold. Commonly used approaches such as the
Nystr\&quot;{o}m extension and kernel ridge regression require using all training
points. We propose an interpolation function that only depends on a small
subset of the input training data. Consequently, in the testing phase each new
point only needs to be compared against a small number of input training data
in order to project the point onto the low-dimensional space. We interpret our
method as an out-of-sample extension that approximates kernel ridge regression.
Our method involves solving a simple convex optimization problem and has the
attractive property of guaranteeing an upper bound on the approximation error,
which is crucial for medical applications. Tuning this error bound controls the
sparsity of the resulting interpolation function. We illustrate our method in
two clinical applications that require fast mapping of input images onto a
low-dimensional space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5513</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5513</id><created>2013-03-22</created><authors><author><keyname>Shrawankar</keyname><forenames>Urmila</forenames></author><author><keyname>Thakare</keyname><forenames>Vilas</forenames></author></authors><title>Parameters Optimization for Improving ASR Performance in Adverse Real
  World Noisy Environmental Conditions</title><categories>cs.CL cs.SD</categories><comments>13 pages, 3 figures, 5 tables</comments><journal-ref>International Journal of Human Computer Interaction (IJHCI) 3(3),
  58-70, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From the existing research it has been observed that many techniques and
methodologies are available for performing every step of Automatic Speech
Recognition (ASR) system, but the performance (Minimization of Word Error
Recognition-WER and Maximization of Word Accuracy Rate- WAR) of the methodology
is not dependent on the only technique applied in that method. The research
work indicates that, performance mainly depends on the category of the noise,
the level of the noise and the variable size of the window, frame, frame
overlap etc is considered in the existing methods. The main aim of the work
presented in this paper is to use variable size of parameters like window size,
frame size and frame overlap percentage to observe the performance of
algorithms for various categories of noise with different levels and also train
the system for all size of parameters and category of real world noisy
environment to improve the performance of the speech recognition system. This
paper presents the results of Signal-to-Noise Ratio (SNR) and Accuracy test by
applying variable size of parameters. It is observed that, it is really very
hard to evaluate test results and decide parameter size for ASR performance
improvement for its resultant optimization. Hence, this study further suggests
the feasible and optimum parameter size using Fuzzy Inference System (FIS) for
enhancing resultant accuracy in adverse real world noisy environmental
conditions. This work will be helpful to give discriminative training of
ubiquitous ASR system for better Human Computer Interaction (HCI).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5515</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5515</id><created>2013-03-22</created><authors><author><keyname>Shrawankar</keyname><forenames>Urmila</forenames></author><author><keyname>Thakare</keyname><forenames>VM</forenames></author></authors><title>Adverse Conditions and ASR Techniques for Robust Speech User Interface</title><categories>cs.CL cs.SD</categories><comments>10 pages 2 Tables</comments><journal-ref>International Journal of Computer Science Issues (IJCSI), 8(5),
  440-449, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main motivation for Automatic Speech Recognition (ASR) is efficient
interfaces to computers, and for the interfaces to be natural and truly useful,
it should provide coverage for a large group of users. The purpose of these
tasks is to further improve man-machine communication. ASR systems exhibit
unacceptable degradations in performance when the acoustical environments used
for training and testing the system are not the same. The goal of this research
is to increase the robustness of the speech recognition systems with respect to
changes in the environment. A system can be labeled as environment-independent
if the recognition accuracy for a new environment is the same or higher than
that obtained when the system is retrained for that environment. Attaining such
performance is the dream of the researchers. This paper elaborates some of the
difficulties with Automatic Speech Recognition (ASR). These difficulties are
classified into Speakers characteristics and environmental conditions, and
tried to suggest some techniques to compensate variations in speech signal.
This paper focuses on the robustness with respect to speakers variations and
changes in the acoustical environment. We discussed several different external
factors that change the environment and physiological differences that affect
the performance of a speech recognition system followed by techniques that are
helpful to design a robust ASR system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5526</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5526</id><created>2013-03-22</created><authors><author><keyname>Obst</keyname><forenames>Oliver</forenames></author><author><keyname>Boedecker</keyname><forenames>Joschka</forenames></author><author><keyname>Schmidt</keyname><forenames>Benedikt</forenames></author><author><keyname>Asada</keyname><forenames>Minoru</forenames></author></authors><title>On active information storage in input-driven systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information theory and the framework of information dynamics have been used
to provide tools to characterise complex systems. In particular, we are
interested in quantifying information storage, information modification and
information transfer as characteristic elements of computation. Although these
quantities are defined for autonomous dynamical systems, information dynamics
can also help to get a &quot;wholistic&quot; understanding of input-driven systems such
as neural networks. In this case, we do not distinguish between the system
itself, and the effects the input has to the system. This may be desired in
some cases, but it will change the questions we are able to answer, and is
consequently an important consideration, for example, for biological systems
which perform non-trivial computations and also retain a short-term memory of
past inputs. Many other real world systems like cortical networks are also
heavily input-driven, and application of tools designed for autonomous dynamic
systems may not necessarily lead to intuitively interpretable results.
  The aim of our work is to extend the measurements used in the information
dynamics framework for input-driven systems. Using the proposed input-corrected
information storage we hope to better quantify system behaviour, which will be
important for heavily input-driven systems like artificial neural networks to
abstract from specific benchmarks, or for brain networks, where intervention is
difficult, individual components cannot be tested in isolation or with
arbitrary input data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5541</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5541</id><created>2013-03-22</created><authors><author><keyname>Janjic</keyname><forenames>Werner</forenames></author><author><keyname>Stoll</keyname><forenames>Dietmar</forenames></author><author><keyname>Bostan</keyname><forenames>Philipp</forenames></author><author><keyname>Atkinson</keyname><forenames>Colin</forenames></author></authors><title>Lowering the Barrier to Reuse through Test-Driven Search</title><categories>cs.SE</categories><comments>4 pages, ICSE Workshop on Search-Driven Development-Users,
  Infrastructure, Tools and Evaluation, 2009. SUITE '09, Vancouver, BC</comments><doi>10.1109/SUITE.2009.5070015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dedicated software search engines that index open source software
repositories or in-house software assets significantly enhance the chance of
finding software components suitable for reuse. However, they still leave the
work of evaluating and testing components to the developer. To significantly
change the risk-cost-benefit tradeoff involved in software reuse, search
engines need to be supported by user friendly environments that deliver code
search functionality non-intrusively right to developers' fingertips.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5560</identifier>
 <datestamp>2013-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5560</id><created>2013-03-22</created><updated>2013-05-31</updated><authors><author><keyname>Gerlach</keyname><forenames>Jens</forenames></author></authors><title>Sorting in Lattices</title><categories>cs.DM math.CO</categories><comments>9 pages, 1 figure, 1 table, change name for sequence from &quot;a&quot; to &quot;x&quot;</comments><msc-class>18B35</msc-class><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a totally ordered set the notion of sorting a finite sequence is defined
through a suitable permutation of the sequence's indices. In this paper we
prove a simple formula that explicitly describes how the elements of a sequence
are related to those of its sorted counterpart. As this formula relies only on
the minimum and maximum functions we use it to define the notion of sorting for
lattices. A major difference of sorting in lattices is that it does not
guarantee that sequence elements are only rearranged. However, we can show that
other fundamental properties that are associated with sorting are preserved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5596</identifier>
 <datestamp>2013-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5596</id><created>2013-03-22</created><authors><author><keyname>Wei</keyname><forenames>Tian</forenames></author><author><keyname>Li</keyname><forenames>Menghui</forenames></author><author><keyname>Wu</keyname><forenames>Chensheng</forenames></author><author><keyname>Yan</keyname><forenames>XiaoYong</forenames></author><author><keyname>Fan</keyname><forenames>Ying</forenames></author><author><keyname>Di</keyname><forenames>Zengru</forenames></author><author><keyname>Wu</keyname><forenames>Jinshan</forenames></author></authors><title>Do scientists trace hot topics?</title><categories>physics.soc-ph cs.DL cs.SI</categories><journal-ref>Scientific Reports 3, 2207 (2013)</journal-ref><doi>10.1038/srep02207</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Do scientists follow hot topics in their scientific investigations? In this
paper, by performing analysis to papers published in the American Physical
Society (APS) Physical Review journals, it is found that papers are more likely
to be attracted by hot fields, where the hotness of a field is measured by the
number of papers belonging to the field. This indicates that scientists
generally do follow hot topics. However, there are qualitative differences
among scientists from various countries, among research works regarding
different number of authors, different number of affiliations and different
number of references. These observations could be valuable for policy makers
when deciding research funding and also for individual researchers when
searching for scientific projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5601</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5601</id><created>2013-03-22</created><authors><author><keyname>Adamaszek</keyname><forenames>Michal</forenames></author></authors><title>The smallest nonevasive graph property</title><categories>math.CO cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A property of n-vertex graphs is called evasive if every algorithm testing
this property by asking questions of the form &quot;is there an edge between
vertices u and v&quot; requires, in the worst case, to ask about all pairs of
vertices. Most &quot;natural&quot; graph properties are either evasive or conjectured to
be such, and of the few examples of nontrivial nonevasive properties scattered
in the literature the smallest one has n=6. We exhibit a nontrivial, nonevasive
property of 5-vertex graphs and show that it is essentially the unique such
with n at most 5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5613</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5613</id><created>2013-03-22</created><authors><author><keyname>Smith</keyname><forenames>Steven T.</forenames></author><author><keyname>Senne</keyname><forenames>Kenneth D.</forenames></author><author><keyname>Philips</keyname><forenames>Scott</forenames></author><author><keyname>Kao</keyname><forenames>Edward K.</forenames></author><author><keyname>Bernstein</keyname><forenames>Garrett</forenames></author></authors><title>Network Detection Theory and Performance</title><categories>cs.SI cs.LG math.ST physics.soc-ph stat.ML stat.TH</categories><comments>Submitted to IEEE Trans. Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network detection is an important capability in many areas of applied
research in which data can be represented as a graph of entities and
relationships. Oftentimes the object of interest is a relatively small subgraph
in an enormous, potentially uninteresting background. This aspect characterizes
network detection as a &quot;big data&quot; problem. Graph partitioning and network
discovery have been major research areas over the last ten years, driven by
interest in internet search, cyber security, social networks, and criminal or
terrorist activities. The specific problem of network discovery is addressed as
a special case of graph partitioning in which membership in a small subgraph of
interest must be determined. Algebraic graph theory is used as the basis to
analyze and compare different network detection methods. A new Bayesian network
detection framework is introduced that partitions the graph based on prior
information and direct observations. The new approach, called space-time threat
propagation, is proved to maximize the probability of detection and is
therefore optimum in the Neyman-Pearson sense. This optimality criterion is
compared to spectral community detection approaches which divide the global
graph into subsets or communities with optimal connectivity properties. We also
explore a new generative stochastic model for covert networks and analyze using
receiver operating characteristics the detection performance of both classes of
optimal detection techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5636</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5636</id><created>2013-03-22</created><updated>2013-06-28</updated><authors><author><keyname>Cardinali</keyname><forenames>Ilaria</forenames></author><author><keyname>Giuzzi</keyname><forenames>Luca</forenames></author></authors><title>Codes and caps from orthogonal Grassmannians</title><categories>math.AG cs.IT math.CO math.IT</categories><comments>Keywords: Polar Grassmannian; dual polar space; embedding; error
  correcting code; cap; Hadamard matrix; Sylvester construction (this is a
  slightly revised version of v2, with updated bibliography)</comments><msc-class>51A50, 51E22, 51A45</msc-class><journal-ref>Finite Fields Appl. 24 (2013), 148-169</journal-ref><doi>10.1016/j.ffa.2013.07.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate linear error correcting codes and projective
caps related to the Grassmann embedding $\varepsilon_k^{gr}$ of an orthogonal
Grassmannian $\Delta_k$. In particular, we determine some of the parameters of
the codes arising from the projective system determined by
$\varepsilon_k^{gr}(\Delta_k)$. We also study special sets of points of
$\Delta_k$ which are met by any line of $\Delta_k$ in at most 2 points and we
show that their image under the Grassmann embedding $\varepsilon_k^{gr}$ is a
projective cap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5655</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5655</id><created>2013-03-22</created><authors><author><keyname>Giryes</keyname><forenames>Raja</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author></authors><title>Can we allow linear dependencies in the dictionary in the sparse
  synthesis framework?</title><categories>cs.IT math.IT</categories><comments>2 figures, to appear in ICASSP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signal recovery from a given set of linear measurements using a sparsity
prior has been a major subject of research in recent years. In this model, the
signal is assumed to have a sparse representation under a given dictionary.
Most of the work dealing with this subject has focused on the reconstruction of
the signal's representation as the means for recovering the signal itself. This
approach forced the dictionary to be of low coherence and with no linear
dependencies between its columns. Recently, a series of contributions that
focus on signal recovery using the analysis model find that linear dependencies
in the analysis dictionary are in fact permitted and beneficial. In this paper
we show theoretically that the same holds also for signal recovery in the
synthesis case for the l0- synthesis minimization problem. In addition, we
demonstrate empirically the relevance of our conclusions for recovering the
signal using an l1-relaxation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5656</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5656</id><created>2013-03-22</created><updated>2013-08-12</updated><authors><author><keyname>Juul</keyname><forenames>Jeppe</forenames></author><author><keyname>Kianercy</keyname><forenames>Ardeshir</forenames></author><author><keyname>Bernhardsson</keyname><forenames>Sebastian</forenames></author><author><keyname>Pigolotti</keyname><forenames>Simone</forenames></author></authors><title>Replicator dynamics with turnover of players</title><categories>cs.GT physics.soc-ph q-bio.PE</categories><comments>14 pages, 7 figures</comments><journal-ref>Physical Review E 88, 022806 (2013)</journal-ref><doi>10.1103/PhysRevE.88.022806</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study adaptive dynamics in games where players abandon the population at a
given rate, and are replaced by naive players characterized by a prior
distribution over the admitted strategies. We demonstrate how such process
leads macroscopically to a variant of the replicator equation, with an
additional term accounting for player turnover. We study how Nash equilibria
and the dynamics of the system are modified by this additional term, for
prototypical examples such as the rock-scissor-paper game and different classes
of two-action games played between two distinct populations. We conclude by
showing how player turnover can account for non-trivial departures from Nash
equilibria observed in data from lowest unique bid auctions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5659</identifier>
 <datestamp>2014-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5659</id><created>2013-03-22</created><updated>2013-11-28</updated><authors><author><keyname>Sato</keyname><forenames>Taisuke</forenames></author><author><keyname>Kubota</keyname><forenames>Keiichi</forenames></author></authors><title>Viterbi training in PRISM</title><categories>cs.AI</categories><comments>23 pages, 1 figure</comments><doi>10.1017/S1471068413000677</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  VT (Viterbi training), or hard EM, is an efficient way of parameter learning
for probabilistic models with hidden variables. Given an observation $y$, it
searches for a state of hidden variables $x$ that maximizes $p(x,y \mid
\theta)$ by coordinate ascent on parameters $\theta$ and $x$. In this paper we
introduce VT to PRISM, a logic-based probabilistic modeling system for
generative models. VT improves PRISM in three ways. First VT in PRISM converges
faster than EM in PRISM due to the VT's termination condition. Second,
parameters learned by VT often show good prediction performance compared to
those learned by EM. We conducted two parsing experiments with probabilistic
grammars while learning parameters by a variety of inference methods, i.e.\ VT,
EM, MAP and VB. The result is that VT achieved the best parsing accuracy among
them in both experiments. Also we conducted a similar experiment for
classification tasks where a hidden variable is not a prediction target unlike
probabilistic grammars. We found that in such a case VT does not necessarily
yield superior performance. Third since VT always deals with a single
probability of a single explanation, Viterbi explanation, the exclusiveness
condition that is imposed on PRISM programs is no more required if we learn
parameters by VT.
  Last but not least we can say that as VT in PRISM is general and applicable
to any PRISM program, it largely reduces the need for the user to develop a
specific VT algorithm for a specific model. Furthermore since VT in PRISM can
be used just by setting a PRISM flag appropriately, it makes VT easily
accessible to (probabilistic) logic programmers. To appear in Theory and
Practice of Logic Programming (TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5673</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5673</id><created>2013-03-22</created><authors><author><keyname>He</keyname><forenames>Dongxiao</forenames></author><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Yang</keyname><forenames>Bin</forenames></author><author><keyname>Zhou</keyname><forenames>Chunguang</forenames></author></authors><title>Genetic Algorithm with Ensemble Learning for Detecting Community
  Structure in Complex Networks</title><categories>cs.SI physics.soc-ph</categories><comments>6 pages, 2 figures, 1 table 2009 Fourth International Conference on
  Computer Sciences and Convergence Information Technology(ICCIT '09)</comments><doi>10.1109/ICCIT.2009.189</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection in complex networks is a topic of considerable recent
interest within the scientific community. For dealing with the problem that
genetic algorithm are hardly applied to community detection, we propose a
genetic algorithm with ensemble learning (GAEL) for detecting community
structure in complex networks. GAEL replaces its traditional crossover operator
with a multi-individual crossover operator based on ensemble learning.
Therefore, GAEL can avoid the problems that are brought by traditional
crossover operator which is only able to mix string blocks of different
individuals, but not able to recombine clustering contexts of different
individuals into new better ones. In addition, the local search strategy, which
makes mutated node be placed into the community where most of its neighbors
are, is used in mutation operator. At last, a Markov random walk based method
is used to initialize population in this paper, and it can provide us a
population of accurate and diverse clustering solutions. Those diverse and
accurate individuals are suitable for ensemble learning based multi-individual
crossover operator. The proposed GAEL is tested on both computer-generated and
real-world networks, and compared with current representative algorithms for
community detection in complex networks. Experimental results demonstrate that
GAEL is highly effective at discovering community structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5675</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5675</id><created>2013-03-22</created><authors><author><keyname>Jin</keyname><forenames>Di</forenames></author><author><keyname>Yang</keyname><forenames>Bo</forenames></author><author><keyname>Baquero</keyname><forenames>Carlos</forenames></author><author><keyname>Liu</keyname><forenames>Dayou</forenames></author><author><keyname>He</keyname><forenames>Dongxiao</forenames></author><author><keyname>Liu</keyname><forenames>Jie</forenames></author></authors><title>Markov random walk under constraint for discovering overlapping
  communities in complex networks</title><categories>cs.SI cond-mat.stat-mech physics.soc-ph</categories><comments>21 pages, 8 pages, 2 tables</comments><journal-ref>Journal of Statistical Mechanics: Theory and Experiment, P05031,
  2011</journal-ref><doi>10.1088/1742-5468/2011/05/P05031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detection of overlapping communities in complex networks has motivated recent
research in the relevant fields. Aiming this problem, we propose a Markov
dynamics based algorithm, called UEOC, which means, 'unfold and extract
overlapping communities'. In UEOC, when identifying each natural community that
overlaps, a Markov random walk method combined with a constraint strategy,
which is based on the corresponding annealed network (degree conserving random
network), is performed to unfold the community. Then, a cutoff criterion with
the aid of a local community function, called conductance, which can be thought
of as the ratio between the number of edges inside the community and those
leaving it, is presented to extract this emerged community from the entire
network. The UEOC algorithm depends on only one parameter whose value can be
easily set, and it requires no prior knowledge on the hidden community
structures. The proposed UEOC has been evaluated both on synthetic benchmarks
and on some real-world networks, and was compared with a set of competing
algorithms. Experimental result has shown that UEOC is highly effective and
efficient for discovering overlapping communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5678</identifier>
 <datestamp>2014-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5678</id><created>2013-03-22</created><updated>2014-08-16</updated><authors><author><keyname>Bresler</keyname><forenames>Guy</forenames></author><author><keyname>Cartwright</keyname><forenames>Dustin</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>Interference alignment for the MIMO interference channel</title><categories>cs.IT math.IT</categories><comments>16 pages, 7 figures, final submitted version</comments><report-no>Mittag-Leffler-2011spring</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study vector space interference alignment for the MIMO interference
channel with no time or frequency diversity, and no symbol extensions. We prove
both necessary and sufficient conditions for alignment. In particular, we
characterize the feasibility of alignment for the symmetric three-user channel
where all users transmit along d dimensions, all transmitters have M antennas
and all receivers have N antennas, as well as feasibility of alignment for the
fully symmetric (M=N) channel with an arbitrary number of users.
  An implication of our results is that the total degrees of freedom available
in a K-user interference channel, using only spatial diversity from the
multiple antennas, is at most 2. This is in sharp contrast to the K/2 degrees
of freedom shown to be possible by Cadambe and Jafar with arbitrarily large
time or frequency diversity.
  Moving beyond the question of feasibility, we additionally discuss
computation of the number of solutions using Schubert calculus in cases where
there are a finite number of solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5685</identifier>
 <datestamp>2015-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5685</id><created>2013-03-22</created><updated>2013-07-19</updated><authors><author><keyname>Lan</keyname><forenames>Andrew S.</forenames></author><author><keyname>Waters</keyname><forenames>Andrew E.</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard G.</forenames></author></authors><title>Sparse Factor Analysis for Learning and Content Analytics</title><categories>stat.ML cs.LG math.OC stat.AP</categories><journal-ref>Journal of Machine Learning Research, vol. 15, pp. 1959-2008,
  June, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new model and algorithms for machine learning-based learning
analytics, which estimate a learner's knowledge of the concepts underlying a
domain, and content analytics, which estimate the relationships among a
collection of questions and those concepts. Our model represents the
probability that a learner provides the correct response to a question in terms
of three factors: their understanding of a set of underlying concepts, the
concepts involved in each question, and each question's intrinsic difficulty.
We estimate these factors given the graded responses to a collection of
questions. The underlying estimation problem is ill-posed in general,
especially when only a subset of the questions are answered. The key
observation that enables a well-posed solution is the fact that typical
educational domains of interest involve only a small number of key concepts.
Leveraging this observation, we develop both a bi-convex maximum-likelihood and
a Bayesian solution to the resulting SPARse Factor Analysis (SPARFA) problem.
We also incorporate user-defined tags on questions to facilitate the
interpretability of the estimated factors. Experiments with synthetic and
real-world data demonstrate the efficacy of our approach. Finally, we make a
connection between SPARFA and noisy, binary-valued (1-bit) dictionary learning
that is of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5691</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5691</id><created>2013-03-22</created><authors><author><keyname>Berkels</keyname><forenames>Benjamin</forenames></author><author><keyname>Cabrilo</keyname><forenames>Ivan</forenames></author><author><keyname>Haller</keyname><forenames>Sven</forenames></author><author><keyname>Rumpf</keyname><forenames>Martin</forenames></author><author><keyname>Schaller</keyname><forenames>Carlo</forenames></author></authors><title>Cortical Surface Co-Registration based on MRI Images and Photos</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Brain shift, i.e. the change in configuration of the brain after opening the
dura mater, is a key problem in neuronavigation. We present an approach to
co-register intra-operative microscope images with pre-operative MRI to adapt
and optimize intra-operative neuronavigation. The tools are a robust
classification of sulci on MRI extracted cortical surfaces, guided user marking
of most prominent sulci on a microscope image, and the actual variational
registration method with a fidelity energy for 3D deformations of the cortical
surface combined with a higher order, linear elastica type prior energy.
Furthermore, the actual registration is validated on an artificial testbed with
known ground truth deformation and on real data of a neuro clinical patient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5694</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5694</id><created>2013-03-22</created><updated>2013-06-27</updated><authors><author><keyname>Akemann</keyname><forenames>Gernot</forenames></author><author><keyname>Kieburg</keyname><forenames>Mario</forenames></author><author><keyname>Wei</keyname><forenames>Lu</forenames></author></authors><title>Singular value correlation functions for products of Wishart random
  matrices</title><categories>math-ph cond-mat.stat-mech cs.IT math.IT math.MP</categories><comments>23 pages, 4 figures, PACS: 02.10.Yn, 02.30.Cj, 02.30.Ik, 02.50.Sk,
  84.40.Ba, 84.40.Ua</comments><msc-class>15B52, 33C20, 33C45, 94Axx</msc-class><journal-ref>J. Phys. A: Math. Theor. 46 (2013) 275205</journal-ref><doi>10.1088/1751-8113/46/27/275205</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the product of $M$ quadratic random matrices with complex elements
and no further symmetry, where all matrix elements of each factor have a
Gaussian distribution. This generalises the classical Wishart-Laguerre Gaussian
Unitary Ensemble with M=1. In this paper we first compute the joint probability
distribution for the singular values of the product matrix when the matrix size
$N$ and the number $M$ are fixed but arbitrary. This leads to a determinantal
point process which can be realised in two different ways. First, it can be
written as a one-matrix singular value model with a non-standard Jacobian, or
second, for $M\geq2$, as a two-matrix singular value model with a set of
auxiliary singular values and a weight proportional to the Meijer $G$-function.
For both formulations we determine all singular value correlation functions in
terms of the kernels of biorthogonal polynomials which we explicitly construct.
They are given in terms of hypergeometric and Meijer $G$-functions,
generalising the Laguerre polynomials. Our investigation was motivated from
applications in telecommunication of multi-layered scattering MIMO channels. We
present the ergodic mutual information for finite-$N$ for such a channel model
with $M-1$ layers of scatterers as an example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5698</identifier>
 <datestamp>2013-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5698</id><created>2013-03-22</created><authors><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Simsek</keyname><forenames>Meryem</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Valentin</keyname><forenames>Stefan</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author><author><keyname>Czylwik</keyname><forenames>Andreas</forenames></author></authors><title>When Cellular Meets WiFi in Wireless Small Cell Networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The deployment of small cell base stations(SCBSs) overlaid on existing
macro-cellular systems is seen as a key solution for offloading traffic,
optimizing coverage, and boosting the capacity of future cellular wireless
systems. The next-generation of SCBSs is envisioned to be multi-mode, i.e.,
capable of transmitting simultaneously on both licensed and unlicensed bands.
This constitutes a cost-effective integration of both WiFi and cellular radio
access technologies (RATs) that can efficiently cope with peak wireless data
traffic and heterogeneous quality-of-service requirements. To leverage the
advantage of such multi-mode SCBSs, we discuss the novel proposed paradigm of
cross-system learning by means of which SCBSs self-organize and autonomously
steer their traffic flows across different RATs. Cross-system learning allows
the SCBSs to leverage the advantage of both the WiFi and cellular worlds. For
example, the SCBSs can offload delay-tolerant data traffic to WiFi, while
simultaneously learning the probability distribution function of their
transmission strategy over the licensed cellular band. This article will first
introduce the basic building blocks of cross-system learning and then provide
preliminary performance evaluation in a Long-Term Evolution (LTE) simulator
overlaid with WiFi hotspots. Remarkably, it is shown that the proposed
cross-system learning approach significantly outperforms a number of benchmark
traffic steering policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5703</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5703</id><created>2013-03-20</created><authors><author><keyname>Abramson</keyname><forenames>Bruce</forenames></author></authors><title>ARCO1: An Application of Belief Networks to the Oil Market</title><categories>cs.AI q-fin.GN</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-1-8</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Belief networks are a new, potentially important, class of knowledge-based
models. ARCO1, currently under development at the Atlantic Richfield Company
(ARCO) and the University of Southern California (USC), is the most advanced
reported implementation of these models in a financial forecasting setting.
ARCO1's underlying belief network models the variables believed to have an
impact on the crude oil market. A pictorial market model-developed on a MAC II-
facilitates consensus among the members of the forecasting team. The system
forecasts crude oil prices via Monte Carlo analyses of the network. Several
different models of the oil market have been developed; the system's ability to
be updated quickly highlights its flexibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5704</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5704</id><created>2013-03-20</created><authors><author><keyname>Agosta</keyname><forenames>John Mark</forenames></author></authors><title>&quot;Conditional Inter-Causally Independent&quot; Node Distributions, a Property
  of &quot;Noisy-Or&quot; Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-9-16</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the interdependence generated between two parent nodes
with a common instantiated child node, such as two hypotheses sharing common
evidence. The relation so generated has been termed &quot;intercausal.&quot; It is shown
by construction that inter-causal independence is possible for binary
distributions at one state of evidence. For such &quot;CICI&quot; distributions, the two
measures of inter-causal effect, &quot;multiplicative synergy&quot; and &quot;additive
synergy&quot; are equal. The well known &quot;noisy-or&quot; model is an example of such a
distribution. This introduces novel semantics for the noisy-or, as a model of
the degree of conflict among competing hypotheses of a common observation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5705</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5705</id><created>2013-03-20</created><authors><author><keyname>Agust&#xed;-Cullell</keyname><forenames>Jaume</forenames></author><author><keyname>Esteva</keyname><forenames>Francesc</forenames></author><author><keyname>Garcia</keyname><forenames>Pere</forenames></author><author><keyname>Godo</keyname><forenames>Lluis</forenames></author><author><keyname>Sierra</keyname><forenames>Carles</forenames></author></authors><title>Combining Multiple-Valued Logics in Modular Expert Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-17-25</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The way experts manage uncertainty usually changes depending on the task they
are performing. This fact has lead us to consider the problem of communicating
modules (task implementations) in a large and structured knowledge based system
when modules have different uncertainty calculi. In this paper, the analysis of
the communication problem is made assuming that (i) each uncertainty calculus
is an inference mechanism defining an entailment relation, and therefore the
communication is considered to be inference-preserving, and (ii) we restrict
ourselves to the case which the different uncertainty calculi are given by a
class of truth functional Multiple-valued Logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5706</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5706</id><created>2013-03-20</created><authors><author><keyname>Amarger</keyname><forenames>Stephane</forenames></author><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>Constraint Propagation with Imprecise Conditional Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-26-34</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An approach to reasoning with default rules where the proportion of
exceptions, or more generally the probability of encountering an exception, can
be at least roughly assessed is presented. It is based on local uncertainty
propagation rules which provide the best bracketing of a conditional
probability of interest from the knowledge of the bracketing of some other
conditional probabilities. A procedure that uses two such propagation rules
repeatedly is proposed in order to estimate any simple conditional probability
of interest from the available knowledge. The iterative procedure, that does
not require independence assumptions, looks promising with respect to the
linear programming method. Improved bounds for conditional probabilities are
given when independence assumptions hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5707</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5707</id><created>2013-03-20</created><authors><author><keyname>Berzuini</keyname><forenames>Carlo</forenames></author><author><keyname>Spiegelhalter</keyname><forenames>David J.</forenames></author><author><keyname>Bellazzi</keyname><forenames>Riccardo</forenames></author></authors><title>Bayesian Networks Aplied to Therapy Monitoring</title><categories>cs.AI stat.AP</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-35-43</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general Bayesian network model for application in a wide class
of problems of therapy monitoring. We discuss the use of stochastic simulation
as a computational approach to inference on the proposed class of models. As an
illustration we present an application to the monitoring of cytotoxic
chemotherapy in breast cancer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5708</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5708</id><created>2013-03-20</created><authors><author><keyname>Buntine</keyname><forenames>Wray L.</forenames></author></authors><title>Some Properties of Plausible Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-44-51</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a plausible reasoning system to illustrate some broad
issues in knowledge representation: dualities between different reasoning
forms, the difficulty of unifying complementary reasoning styles, and the
approximate nature of plausible reasoning. These issues have a common
underlying theme: there should be an underlying belief calculus of which the
many different reasoning forms are special cases, sometimes approximate. The
system presented allows reasoning about defaults, likelihood, necessity and
possibility in a manner similar to the earlier work of Adams. The system is
based on the belief calculus of subjective Bayesian probability which itself is
based on a few simple assumptions about how belief should be manipulated.
Approximations, semantics, consistency and consequence results are presented
for the system. While this puts these often discussed plausible reasoning forms
on a probabilistic footing, useful application to practical problems remains an
issue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5709</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5709</id><created>2013-03-20</created><authors><author><keyname>Buntine</keyname><forenames>Wray L.</forenames></author></authors><title>Theory Refinement on Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-52-60</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Theory refinement is the task of updating a domain theory in the light of new
cases, to be done automatically or with some expert assistance. The problem of
theory refinement under uncertainty is reviewed here in the context of Bayesian
statistics, a theory of belief revision. The problem is reduced to an
incremental learning task as follows: the learning system is initially primed
with a partial theory supplied by a domain expert, and thereafter maintains its
own internal representation of alternative theories which is able to be
interrogated by the domain expert and able to be incrementally refined from
data. Algorithms for refinement of Bayesian networks are presented to
illustrate what is meant by &quot;partial theory&quot;, &quot;alternative theory
representation&quot;, etc. The algorithms are an incremental variant of batch
learning algorithms from the literature so can work well in batch and
incremental mode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5710</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5710</id><created>2013-03-20</created><authors><author><keyname>Cano</keyname><forenames>Jose E.</forenames></author><author><keyname>Moral</keyname><forenames>Serafin</forenames></author><author><keyname>Verdegay-Lopez</keyname><forenames>Juan F.</forenames></author></authors><title>Combination of Upper and Lower Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-61-68</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider several types of information and methods of
combination associated with incomplete probabilistic systems. We discriminate
between 'a priori' and evidential information. The former one is a description
of the whole population, the latest is a restriction based on observations for
a particular case. Then, we propose different combination methods for each one
of them. We also consider conditioning as the heterogeneous combination of 'a
priori' and evidential information. The evidential information is represented
as a convex set of likelihood functions. These will have an associated
possibility distribution with behavior according to classical Possibility
Theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5711</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5711</id><created>2013-03-20</created><authors><author><keyname>Carroll</keyname><forenames>Glenn</forenames></author><author><keyname>Charniak</keyname><forenames>Eugene</forenames></author></authors><title>A Probabilistic Analysis of Marker-Passing Techniques for
  Plan-Recognition</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-69-76</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Useless paths are a chronic problem for marker-passing techniques. We use a
probabilistic analysis to justify a method for quickly identifying and
rejecting useless paths. Using the same analysis, we identify key conditions
and assumptions necessary for marker-passing to perform well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5712</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5712</id><created>2013-03-20</created><authors><author><keyname>Chang</keyname><forenames>Kuo-Chu</forenames></author><author><keyname>Fung</keyname><forenames>Robert</forenames></author></authors><title>Symbolic Probabilistic Inference with Continuous Variables</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-77-81</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on Symbolic Probabilistic Inference (SPI) [2, 3] has provided an
algorithm for resolving general queries in Bayesian networks. SPI applies the
concept of dependency directed backward search to probabilistic inference, and
is incremental with respect to both queries and observations. Unlike
traditional Bayesian network inferencing algorithms, SPI algorithm is goal
directed, performing only those calculations that are required to respond to
queries. Research to date on SPI applies to Bayesian networks with
discrete-valued variables and does not address variables with continuous
values. In this papers, we extend the SPI algorithm to handle Bayesian networks
made up of continuous variables where the relationships between the variables
are restricted to be ?linear gaussian?. We call this variation of the SPI
algorithm, SPI Continuous (SPIC). SPIC modifies the three basic SPI operations:
multiplication, summation, and substitution. However, SPIC retains the
framework of the SPI algorithm, namely building the search tree and recursive
query mechanism and therefore retains the goal-directed and incrementality
features of SPI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5713</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5713</id><created>2013-03-20</created><authors><author><keyname>Chang</keyname><forenames>Kuo-Chu</forenames></author><author><keyname>Fung</keyname><forenames>Robert</forenames></author></authors><title>Symbolic Probabilistic Inference with Evidence Potential</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-82-85</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research on the Symbolic Probabilistic Inference (SPI) algorithm[2]
has focused attention on the importance of resolving general queries in
Bayesian networks. SPI applies the concept of dependency-directed backward
search to probabilistic inference, and is incremental with respect to both
queries and observations. In response to this research we have extended the
evidence potential algorithm [3] with the same features. We call the extension
symbolic evidence potential inference (SEPI). SEPI like SPI can handle generic
queries and is incremental with respect to queries and observations. While in
SPI, operations are done on a search tree constructed from the nodes of the
original network, in SEPI, a clique-tree structure obtained from the evidence
potential algorithm [3] is the basic framework for recursive query processing.
In this paper, we describe the systematic query and caching procedure of SEPI.
SEPI begins with finding a clique tree from a Bayesian network-the standard
procedure of the evidence potential algorithm. With the clique tree, various
probability distributions are computed and stored in each clique. This is the
?pre-processing? step of SEPI. Once this step is done, the query can then be
computed. To process a query, a recursive process similar to the SPI algorithm
is used. The queries are directed to the root clique and decomposed into
queries for the clique's subtrees until a particular query can be answered at
the clique at which it is directed. The algorithm and the computation are
simple. The SEPI algorithm will be presented in this paper along with several
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5714</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5714</id><created>2013-03-20</created><authors><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author><author><keyname>Herskovits</keyname><forenames>Edward H.</forenames></author></authors><title>A Bayesian Method for Constructing Bayesian Belief Networks from
  Databases</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-86-94</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a Bayesian method for constructing Bayesian belief
networks from a database of cases. Potential applications include
computer-assisted hypothesis testing, automated scientific discovery, and
automated construction of probabilistic expert systems. Results are presented
of a preliminary evaluation of an algorithm for constructing a belief network
from a database of cases. We relate the methods in this paper to previous work,
and we discuss open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5715</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5715</id><created>2013-03-20</created><authors><author><keyname>D'Ambrosio</keyname><forenames>Bruce</forenames></author></authors><title>Local Expression Languages for Probabilistic Dependence: a Preliminary
  Report</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-95-102</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a generalization of the local expression language used in the
Symbolic Probabilistic Inference (SPI) approach to inference in belief nets
[1l, [8]. The local expression language in SPI is the language in which the
dependence of a node on its antecedents is described. The original language
represented the dependence as a single monolithic conditional probability
distribution. The extended language provides a set of operators (*, +, and -)
which can be used to specify methods for combining partial conditional
distributions. As one instance of the utility of this extension, we show how
this extended language can be used to capture the semantics, representational
advantages, and inferential complexity advantages of the &quot;noisy or&quot;
relationship.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5716</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5716</id><created>2013-03-20</created><authors><author><keyname>Fox</keyname><forenames>John</forenames></author><author><keyname>Krause</keyname><forenames>Paul J.</forenames></author></authors><title>Symbolic Decision Theory and Autonomous Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-103-110</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to reason under uncertainty and with incomplete information is a
fundamental requirement of decision support technology. In this paper we argue
that the concentration on theoretical techniques for the evaluation and
selection of decision options has distracted attention from many of the wider
issues in decision making. Although numerical methods of reasoning under
uncertainty have strong theoretical foundations, they are representationally
weak and only deal with a small part of the decision process. Knowledge based
systems, on the other hand, offer greater flexibility but have not been
accompanied by a clear decision theory. We describe here work which is under
way towards providing a theoretical framework for symbolic decision procedures.
A central proposal is an extended form of inference which we call
argumentation; reasoning for and against decision options from generalised
domain theories. The approach has been successfully used in several decision
support applications, but it is argued that a comprehensive decision theory
must cover autonomous decision making, where the agent can formulate questions
as well as take decisions. A major theoretical challenge for this theory is to
capture the idea of reflection to permit decision agents to reason about their
goals, what they believe and why, and what they need to know or do in order to
achieve their goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5717</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5717</id><created>2013-03-20</created><authors><author><keyname>Fringuelli</keyname><forenames>B.</forenames></author><author><keyname>Marcugini</keyname><forenames>S.</forenames></author><author><keyname>Milani</keyname><forenames>A.</forenames></author><author><keyname>Rivoira</keyname><forenames>S.</forenames></author></authors><title>A Reason Maintenace System Dealing with Vague Data</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-111-117</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A reason maintenance system which extends an ATMS through Mukaidono's fuzzy
logic is described. It supports a problem solver in situations affected by
incomplete information and vague data, by allowing nonmonotonic inferences and
the revision of previous conclusions when contradictions are detected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5718</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5718</id><created>2013-03-20</created><updated>2015-05-16</updated><authors><author><keyname>Geiger</keyname><forenames>Dan</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>Advances in Probabilistic Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1991-PG-118-126</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discuses multiple Bayesian networks representation paradigms for
encoding asymmetric independence assertions. We offer three contributions: (1)
an inference mechanism that makes explicit use of asymmetric independence to
speed up computations, (2) a simplified definition of similarity networks and
extensions of their theory, and (3) a generalized representation scheme that
encodes more types of asymmetric independence assertions than do similarity
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5719</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5719</id><created>2013-03-20</created><authors><author><keyname>Grove</keyname><forenames>Adam J.</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Probability Estimation in Face of Irrelevant Information</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-127-134</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider one aspect of the problem of applying decision
theory to the design of agents that learn how to make decisions under
uncertainty. This aspect concerns how an agent can estimate probabilities for
the possible states of the world, given that it only makes limited observations
before committing to a decision. We show that the naive application of
statistical tools can be improved upon if the agent can determine which of his
observations are truly relevant to the estimation problem at hand. We give a
framework in which such determinations can be made, and define an estimation
procedure to use them. Our framework also suggests several extensions, which
show how additional knowledge can be used to improve tile estimation procedure
still further.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5720</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5720</id><created>2013-03-20</created><updated>2015-05-16</updated><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author><author><keyname>Middleton</keyname><forenames>Blackford</forenames></author></authors><title>An Approximate Nonmyopic Computation for Value of Information</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1991-PG-135-141</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Value-of-information analyses provide a straightforward means for selecting
the best next observation to make, and for determining whether it is better to
gather additional information or to act immediately. Determining the next best
test to perform, given a state of uncertainty about the world, requires a
consideration of the value of making all possible sequences of observations. In
practice, decision analysts and expert-system designers have avoided the
intractability of exact computation of the value of information by relying on a
myopic approximation. Myopic analyses are based on the assumption that only one
additional test will be performed, even when there is an opportunity to make a
large number of observations. We present a nonmyopic approximation for value of
information that bypasses the traditional myopic analyses by exploiting the
statistical properties of large samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5721</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5721</id><created>2013-03-20</created><authors><author><keyname>Henrion</keyname><forenames>Max</forenames></author></authors><title>Search-based Methods to Bound Diagnostic Probabilities in Very Large
  Belief Nets</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-142-150</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since exact probabilistic inference is intractable in general for large
multiply connected belief nets, approximate methods are required. A promising
approach is to use heuristic search among hypotheses (instantiations of the
network) to find the most probable ones, as in the TopN algorithm. Search is
based on the relative probabilities of hypotheses which are efficient to
compute. Given upper and lower bounds on the relative probability of partial
hypotheses, it is possible to obtain bounds on the absolute probabilities of
hypotheses. Best-first search aimed at reducing the maximum error progressively
narrows the bounds as more hypotheses are examined. Here, qualitative
probabilistic analysis is employed to obtain bounds on the relative probability
of partial hypotheses for the BN20 class of networks networks and a
generalization replacing the noisy OR assumption by negative synergy. The
approach is illustrated by application to a very large belief network, QMR-BN,
which is a reformulation of the Internist-1 system for diagnosis in internal
medicine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5722</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5722</id><created>2013-03-20</created><authors><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author><author><keyname>Rutledge</keyname><forenames>Geoffrey</forenames></author></authors><title>Time-Dependent Utility and Action Under Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-151-158</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss representing and reasoning with knowledge about the time-dependent
utility of an agent's actions. Time-dependent utility plays a crucial role in
the interaction between computation and action under bounded resources. We
present a semantics for time-dependent utility and describe the use of
time-dependent information in decision contexts. We illustrate our discussion
with examples of time-pressured reasoning in Protos, a system constructed to
explore the ideal control of inference by reasoners with limit abilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5723</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5723</id><created>2013-03-20</created><authors><author><keyname>Hunter</keyname><forenames>Daniel</forenames></author></authors><title>Non-monotonic Reasoning and the Reversibility of Belief Change</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-159-164</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional approaches to non-monotonic reasoning fail to satisfy a number of
plausible axioms for belief revision and suffer from conceptual difficulties as
well. Recent work on ranked preferential models (RPMs) promises to overcome
some of these difficulties. Here we show that RPMs are not adequate to handle
iterated belief change. Specifically, we show that RPMs do not always allow for
the reversibility of belief change. This result indicates the need for
numerical strengths of belief.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5724</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5724</id><created>2013-03-20</created><authors><author><keyname>Hsia</keyname><forenames>Yen-Teh</forenames></author></authors><title>Belief and Surprise - A Belief-Function Formulation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-165-173</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We motivate and describe a theory of belief in this paper. This theory is
developed with the following view of human belief in mind. Consider the belief
that an event E will occur (or has occurred or is occurring). An agent either
entertains this belief or does not entertain this belief (i.e., there is no
&quot;grade&quot; in entertaining the belief). If the agent chooses to exercise &quot;the will
to believe&quot; and entertain this belief, he/she/it is entitled to a degree of
confidence c (1 &gt; c &gt; 0) in doing so. Adopting this view of human belief, we
conjecture that whenever an agent entertains the belief that E will occur with
c degree of confidence, the agent will be surprised (to the extent c) upon
realizing that E did not occur.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5725</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5725</id><created>2013-03-20</created><authors><author><keyname>Kennes</keyname><forenames>Robert</forenames></author></authors><title>Evidential Reasoning in a Categorial Perspective: Conjunction and
  Disjunction of Belief Functions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-174-181</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The categorial approach to evidential reasoning can be seen as a combination
of the probability kinematics approach of Richard Jeffrey (1965) and the
maximum (cross-) entropy inference approach of E. T. Jaynes (1957). As a
consequence of that viewpoint, it is well known that category theory provides
natural definitions for logical connectives. In particular, disjunction and
conjunction are modelled by general categorial constructions known as products
and coproducts. In this paper, I focus mainly on Dempster-Shafer theory of
belief functions for which I introduce a category I call Dempster?s category. I
prove the existence of and give explicit formulas for conjunction and
disjunction in the subcategory of separable belief functions. In Dempster?s
category, the new defined conjunction can be seen as the most cautious
conjunction of beliefs, and thus no assumption about distinctness (of the
sources) of beliefs is needed as opposed to Dempster?s rule of combination,
which calls for distinctness (of the sources) of beliefs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5726</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5726</id><created>2013-03-20</created><authors><author><keyname>Kruse</keyname><forenames>Rudolf</forenames></author><author><keyname>Nauck</keyname><forenames>Detlef</forenames></author><author><keyname>Klawonn</keyname><forenames>Frank</forenames></author></authors><title>Reasoning with Mass Distributions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-182-187</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of movable evidence masses that flow from supersets to subsets as
specified by experts represents a suitable framework for reasoning under
uncertainty. The mass flow is controlled by specialization matrices. New
evidence is integrated into the frame of discernment by conditioning or
revision (Dempster's rule of conditioning), for which special specialization
matrices exist. Even some aspects of non-monotonic reasoning can be represented
by certain specialization matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5727</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5727</id><created>2013-03-20</created><authors><author><keyname>Lang</keyname><forenames>Jerome</forenames></author><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>A Logic of Graded Possibility and Certainty Coping with Partial
  Inconsistency</title><categories>cs.AI cs.LO</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-188-196</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A semantics is given to possibilistic logic, a logic that handles weighted
classical logic formulae, and where weights are interpreted as lower bounds on
degrees of certainty or possibility, in the sense of Zadeh's possibility
theory. The proposed semantics is based on fuzzy sets of interpretations. It is
tolerant to partial inconsistency. Satisfiability is extended from
interpretations to fuzzy sets of interpretations, each fuzzy set representing a
possibility distribution describing what is known about the state of the world.
A possibilistic knowledge base is then viewed as a set of possibility
distributions that satisfy it. The refutation method of automated deduction in
possibilistic logic, based on previously introduced generalized resolution
principle is proved to be sound and complete with respect to the proposed
semantics, including the case of partial inconsistency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5728</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5728</id><created>2013-03-20</created><authors><author><keyname>Laskey</keyname><forenames>Kathryn Blackmond</forenames></author></authors><title>Conflict and Surprise: Heuristics for Model Revision</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-197-204</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Any probabilistic model of a problem is based on assumptions which, if
violated, invalidate the model. Users of probability based decision aids need
to be alerted when cases arise that are not covered by the aid's model.
Diagnosis of model failure is also necessary to control dynamic model
construction and revision. This paper presents a set of decision theoretically
motivated heuristics for diagnosing situations in which a model is likely to
provide an inadequate representation of the process being modeled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5729</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5729</id><created>2013-03-20</created><authors><author><keyname>Lehner</keyname><forenames>Paul E.</forenames></author><author><keyname>Sadigh</keyname><forenames>Azar</forenames></author></authors><title>Reasoning under Uncertainty: Some Monte Carlo Results</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-205-211</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A series of monte carlo studies were performed to compare the behavior of
some alternative procedures for reasoning under uncertainty. The behavior of
several Bayesian, linear model and default reasoning procedures were examined
in the context of increasing levels of calibration error. The most interesting
result is that Bayesian procedures tended to output more extreme posterior
belief values (posterior beliefs near 0.0 or 1.0) than other techniques, but
the linear models were relatively less likely to output strong support for an
erroneous conclusion. Also, accounting for the probabilistic dependencies
between evidence items was important for both Bayesian and linear updating
procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5730</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5730</id><created>2013-03-20</created><authors><author><keyname>Leong</keyname><forenames>Tze-Yun</forenames></author></authors><title>Representation Requirements for Supporting Decision Model Formulation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-212-219</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper outlines a methodology for analyzing the representational support
for knowledge-based decision-modeling in a broad domain. A relevant set of
inference patterns and knowledge types are identified. By comparing the
analysis results to existing representations, some insights are gained into a
design approach for integrating categorical and uncertain knowledge in a
context sensitive manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5731</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5731</id><created>2013-03-20</created><authors><author><keyname>Martin</keyname><forenames>Nathaniel G.</forenames></author><author><keyname>Allen</keyname><forenames>James F.</forenames></author></authors><title>A Language for Planning with Statistics</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-220-227</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When a planner must decide whether it has enough evidence to make a decision
based on probability, it faces the sample size problem. Current planners using
probabilities need not deal with this problem because they do not generate
their probabilities from observations. This paper presents an event based
language in which the planner's probabilities are calculated from the binomial
random variable generated by the observed ratio of one type of event to
another. Such probabilities are subject to error, so the planner must
introspect about their validity. Inferences about the probability of these
events can be made using statistics. Inferences about the validity of the
approximations can be made using interval estimation. Interval estimation
allows the planner to avoid making choices that are only weakly supported by
the planner's evidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5732</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5732</id><created>2013-03-20</created><authors><author><keyname>Murtezao&#x11f;lu</keyname><forenames>B&#xfc;lent</forenames></author><author><keyname>Kyburg</keyname><forenames>Henry E.</forenames><suffix>Jr</suffix></author></authors><title>A Modification to Evidential Probability</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-228-231</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Selecting the right reference class and the right interval when faced with
conflicting candidates and no possibility of establishing subset style
dominance has been a problem for Kyburg's Evidential Probability system.
Various methods have been proposed by Loui and Kyburg to solve this problem in
a way that is both intuitively appealing and justifiable within Kyburg's
framework. The scheme proposed in this paper leads to stronger statistical
assertions without sacrificing too much of the intuitive appeal of Kyburg's
latest proposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5733</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5733</id><created>2013-03-20</created><authors><author><keyname>Neapolitan</keyname><forenames>Richard E.</forenames></author><author><keyname>Kenevan</keyname><forenames>James</forenames></author></authors><title>Investigation of Variances in Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-232-241</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The belief network is a well-known graphical structure for representing
independences in a joint probability distribution. The methods, which perform
probabilistic inference in belief networks, often treat the conditional
probabilities which are stored in the network as certain values. However, if
one takes either a subjectivistic or a limiting frequency approach to
probability, one can never be certain of probability values. An algorithm
should not only be capable of reporting the probabilities of the alternatives
of remaining nodes when other nodes are instantiated; it should also be capable
of reporting the uncertainty in these probabilities relative to the uncertainty
in the probabilities which are stored in the network. In this paper a method
for determining the variances in inferred probabilities is obtained under the
assumption that a posterior distribution on the uncertainty variables can be
approximated by the prior distribution. It is shown that this assumption is
plausible if their is a reasonable amount of confidence in the probabilities
which are stored in the network. Furthermore in this paper, a surprising upper
bound for the prior variances in the probabilities of the alternatives of all
nodes is obtained in the case where the probability distributions of the
probabilities of the alternatives are beta distributions. It is shown that the
prior variance in the probability at an alternative of a node is bounded above
by the largest variance in an element of the conditional probability
distribution for that node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5734</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5734</id><created>2013-03-20</created><authors><author><keyname>Ng</keyname><forenames>Keung-Chi</forenames></author><author><keyname>Abramson</keyname><forenames>Bruce</forenames></author></authors><title>A Sensitivity Analysis of Pathfinder: A Follow-up Study</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-242-248</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At last year?s Uncertainty in AI Conference, we reported the results of a
sensitivity analysis study of Pathfinder. Our findings were quite
unexpected-slight variations to Pathfinder?s parameters appeared to lead to
substantial degradations in system performance. A careful look at our first
analysis, together with the valuable feedback provided by the participants of
last year?s conference, led us to conduct a follow-up study. Our follow-up
differs from our initial study in two ways: (i) the probabilities 0.0 and 1.0
remained unchanged, and (ii) the variations to the probabilities that are close
to both ends (0.0 or 1.0) were less than the ones close to the middle (0.5).
The results of the follow-up study look more reasonable-slight variations to
Pathfinder?s parameters now have little effect on its performance. Taken
together, these two sets of results suggest a viable extension of a common
decision analytic sensitivity analysis to the larger, more complex settings
generally encountered in artificial intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5735</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5735</id><created>2013-03-20</created><authors><author><keyname>Ng</keyname><forenames>Raymond T.</forenames></author><author><keyname>Subrahmanian</keyname><forenames>V. S.</forenames></author></authors><title>Non-monotonic Negation in Probabilistic Deductive Databases</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-249-256</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the uses and the semantics of non-monotonic negation
in probabilistic deductive data bases. Based on the stable semantics for
classical logic programming, we introduce the notion of stable formula,
functions. We show that stable formula, functions are minimal fixpoints of
operators associated with probabilistic deductive databases with negation.
Furthermore, since a. probabilistic deductive database may not necessarily have
a stable formula function, we provide a stable class semantics for such
databases. Finally, we demonstrate that the proposed semantics can handle
default reasoning naturally in the context of probabilistic deduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5736</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5736</id><created>2013-03-20</created><authors><author><keyname>Paasch</keyname><forenames>Robert K.</forenames></author><author><keyname>Agogino</keyname><forenames>Alice M.</forenames></author></authors><title>Management of Uncertainty in the Multi-Level Monitoring and Diagnosis of
  the Time of Flight Scintillation Array</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-257-263</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general architecture for the monitoring and diagnosis of large
scale sensor-based systems with real time diagnostic constraints. This
architecture is multileveled, combining a single monitoring level based on
statistical methods with two model based diagnostic levels. At each level,
sources of uncertainty are identified, and integrated methodologies for
uncertainty management are developed. The general architecture was applied to
the monitoring and diagnosis of a specific nuclear physics detector at Lawrence
Berkeley National Laboratory that contained approximately 5000 components and
produced over 500 channels of output data. The general architecture is
scalable, and work is ongoing to apply it to detector systems one and two
orders of magnitude more complex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5737</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5737</id><created>2013-03-20</created><authors><author><keyname>Paass</keyname><forenames>Gerhard</forenames></author></authors><title>Integrating Probabilistic Rules into Neural Networks: A Stochastic EM
  Learning Algorithm</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-264-270</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The EM-algorithm is a general procedure to get maximum likelihood estimates
if part of the observations on the variables of a network are missing. In this
paper a stochastic version of the algorithm is adapted to probabilistic neural
networks describing the associative dependency of variables. These networks
have a probability distribution, which is a special case of the distribution
generated by probabilistic inference networks. Hence both types of networks can
be combined allowing to integrate probabilistic rules as well as unspecified
associations in a sound way. The resulting network may have a number of
interesting features including cycles of probabilistic rules, hidden
'unobservable' variables, and uncertain and contradictory evidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5738</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5738</id><created>2013-03-20</created><authors><author><keyname>Poole</keyname><forenames>David L.</forenames></author></authors><title>Representing Bayesian Networks within Probabilistic Horn Abduction</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-271-278</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a simple framework for Horn clause abduction, with
probabilities associated with hypotheses. It is shown how this representation
can represent any probabilistic knowledge representable in a Bayesian belief
network. The main contributions are in finding a relationship between logical
and probabilistic notions of evidential reasoning. This can be used as a basis
for a new way to implement Bayesian Networks that allows for approximations to
the value of the posterior probabilities, and also points to a way that
Bayesian networks can be extended beyond a propositional language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5739</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5739</id><created>2013-03-20</created><authors><author><keyname>Provan</keyname><forenames>Gregory M.</forenames></author></authors><title>Dynamic Network Updating Techniques For Diagnostic Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-279-286</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new probabilistic network construction system, DYNASTY, is proposed for
diagnostic reasoning given variables whose probabilities change over time.
Diagnostic reasoning is formulated as a sequential stochastic process, and is
modeled using influence diagrams. Given a set O of observations, DYNASTY
creates an influence diagram in order to devise the best action given O.
Sensitivity analyses are conducted to determine if the best network has been
created, given the uncertainty in network parameters and topology. DYNASTY uses
an equivalence class approach to provide decision thresholds for the
sensitivity analysis. This equivalence-class approach to diagnostic reasoning
differentiates diagnoses only if the required actions are different. A set of
network-topology updating algorithms are proposed for dynamically updating the
network when necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5740</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5740</id><created>2013-03-20</created><authors><author><keyname>Qi</keyname><forenames>Runping</forenames></author><author><keyname>Poole</keyname><forenames>David L.</forenames></author></authors><title>High Level Path Planning with Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-287-294</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For high level path planning, environments are usually modeled as distance
graphs, and path planning problems are reduced to computing the shortest path
in distance graphs. One major drawback of this modeling is the inability to
model uncertainties, which are often encountered in practice. In this paper, a
new tool, called U-yraph, is proposed for environment modeling. A U-graph is an
extension of distance graphs with the ability to handle a kind of uncertainty.
By modeling an uncertain environment as a U-graph, and a navigation problem as
a Markovian decision process, we can precisely define a new optimality
criterion for navigation plans, and more importantly, we can come up with a
general algorithm for computing optimal plans for navigation tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5741</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5741</id><created>2013-03-20</created><authors><author><keyname>Ramer</keyname><forenames>Arthur</forenames></author></authors><title>Formal Model of Uncertainty for Possibilistic Rules</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-295-299</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a universe of discourse X-a domain of possible outcomes-an experiment
may consist of selecting one of its elements, subject to the operation of
chance, or of observing the elements, subject to imprecision. A priori
uncertainty about the actual result of the experiment may be quantified,
representing either the likelihood of the choice of :r_X or the degree to which
any such X would be suitable as a description of the outcome. The former case
corresponds to a probability distribution, while the latter gives a possibility
assignment on X. The study of such assignments and their properties falls
within the purview of possibility theory [DP88, Y80, Z783. It, like probability
theory, assigns values between 0 and 1 to express likelihoods of outcomes.
Here, however, the similarity ends. Possibility theory uses the maximum and
minimum functions to combine uncertainties, whereas probability theory uses the
plus and times operations. This leads to very dissimilar theories in terms of
analytical framework, even though they share several semantic concepts. One of
the shared concepts consists of expressing quantitatively the uncertainty
associated with a given distribution. In probability theory its value
corresponds to the gain of information that would result from conducting an
experiment and ascertaining an actual result. This gain of information can
equally well be viewed as a decrease in uncertainty about the outcome of an
experiment. In this case the standard measure of information, and thus
uncertainty, is Shannon entropy [AD75, G77]. It enjoys several advantages-it is
characterized uniquely by a few, very natural properties, and it can be
conveniently used in decision processes. This application is based on the
principle of maximum entropy; it has become a popular method of relating
decisions to uncertainty. This paper demonstrates that an equally integrated
theory can be built on the foundation of possibility theory. We first show how
to define measures of in formation and uncertainty for possibility assignments.
Next we construct an information-based metric on the space of all possibility
distributions defined on a given domain. It allows us to capture the notion of
proximity in information content among the distributions. Lastly, we show that
all the above constructions can be carried out for continuous
distributions-possibility assignments on arbitrary measurable domains. We
consider this step very significant-finite domains of discourse are but
approximations of the real-life infinite domains. If possibility theory is to
represent real world situations, it must handle continuous distributions both
directly and through finite approximations. In the last section we discuss a
principle of maximum uncertainty for possibility distributions. We show how
such a principle could be formalized as an inference rule. We also suggest it
could be derived as a consequence of simple assumptions about combining
information. We would like to mention that possibility assignments can be
viewed as fuzzy sets and that every fuzzy set gives rise to an assignment of
possibilities. This correspondence has far reaching consequences in logic and
in control theory. Our treatment here is independent of any special
interpretation; in particular we speak of possibility distributions and
possibility measures, defining them as measurable mappings into the interval
[0, 1]. Our presentation is intended as a self-contained, albeit terse summary.
Topics discussed were selected with care, to demonstrate both the completeness
and a certain elegance of the theory. Proofs are not included; we only offer
illustrative examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5742</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5742</id><created>2013-03-20</created><authors><author><keyname>Rao</keyname><forenames>Anand S.</forenames></author><author><keyname>Georgeff</keyname><forenames>Michael P.</forenames></author></authors><title>Deliberation and its Role in the Formation of Intentions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-300-307</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deliberation plays an important role in the design of rational agents
embedded in the real-world. In particular, deliberation leads to the formation
of intentions, i.e., plans of action that the agent is committed to achieving.
In this paper, we present a branching time possible-worlds model for
representing and reasoning about, beliefs, goals, intentions, time, actions,
probabilities, and payoffs. We compare this possible-worlds approach with the
more traditional decision tree representation and provide a transformation from
decision trees to possible worlds. Finally, we illustrate how an agent can
perform deliberation using a decision-tree representation and then use a
possible-worlds model to form and reason about his intentions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5743</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5743</id><created>2013-03-20</created><authors><author><keyname>Raskutti</keyname><forenames>Bhavani</forenames></author><author><keyname>Zukerman</keyname><forenames>Ingrid</forenames></author></authors><title>Handling Uncertainty during Plan Recognition in Task-Oriented
  Consultation Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-308-315</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During interactions with human consultants, people are used to providing
partial and/or inaccurate information, and still be understood and assisted. We
attempt to emulate this capability of human consultants; in computer
consultation systems. In this paper, we present a mechanism for handling
uncertainty in plan recognition during task-oriented consultations. The
uncertainty arises while choosing an appropriate interpretation of a user?s
statements among many possible interpretations. Our mechanism handles this
uncertainty by using probability theory to assess the probabilities of the
interpretations, and complements this assessment by taking into account the
information content of the interpretations. The information content of an
interpretation is a measure of how well defined an interpretation is in terms
of the actions to be performed on the basis of the interpretation. This measure
is used to guide the inference process towards interpretations with a higher
information content. The information content for an interpretation depends on
the specificity and the strength of the inferences in it, where the strength of
an inference depends on the reliability of the information on which the
inference is based. Our mechanism has been developed for use in task-oriented
consultation systems. The domain that we have chosen for exploration is that of
a travel agency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5744</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5744</id><created>2013-03-20</created><authors><author><keyname>Ruspini</keyname><forenames>Enrique H.</forenames></author></authors><title>Truth as Utility: A Conceptual Synthesis</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-316-322</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces conceptual relations that synthesize utilitarian and
logical concepts, extending the logics of preference of Rescher. We define
first, in the context of a possible worlds model, constraint-dependent measures
that quantify the relative quality of alternative solutions of reasoning
problems or the relative desirability of various policies in control, decision,
and planning problems. We show that these measures may be interpreted as truth
values in a multi valued logic and propose mechanisms for the representation of
complex constraints as combinations of simpler restrictions. These extended
logical operations permit also the combination and aggregation of goal-specific
quality measures into global measures of utility. We identify also relations
that represent differential preferences between alternative solutions and
relate them to the previously defined desirability measures. Extending
conventional modal logic formulations, we introduce structures for the
representation of ignorance about the utility of alternative solutions.
Finally, we examine relations between these concepts and similarity based
semantic models of fuzzy logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5745</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5745</id><created>2013-03-20</created><authors><author><keyname>Saffiotti</keyname><forenames>Alessandro</forenames></author><author><keyname>Umkehrer</keyname><forenames>Elisabeth</forenames></author></authors><title>Pulcinella: A General Tool for Propagating Uncertainty in Valuation
  Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-323-331</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present PULCinella and its use in comparing uncertainty theories.
PULCinella is a general tool for Propagating Uncertainty based on the Local
Computation technique of Shafer and Shenoy. It may be specialized to different
uncertainty theories: at the moment, Pulcinella can propagate probabilities,
belief functions, Boolean values, and possibilities. Moreover, Pulcinella
allows the user to easily define his own specializations. To illustrate
Pulcinella, we analyze two examples by using each of the four theories above.
In the first one, we mainly focus on intrinsic differences between theories. In
the second one, we take a knowledge engineer viewpoint, and check the adequacy
of each theory to a given problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5746</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5746</id><created>2013-03-20</created><authors><author><keyname>Sandri</keyname><forenames>Sandra</forenames></author></authors><title>Structuring Bodies of Evidence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-332-338</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we present two ways of structuring bodies of evidence, which
allow us to reduce the complexity of the operations usually performed in the
framework of evidence theory. The first structure just partitions the focal
elements in a body of evidence by their cardinality. With this structure we are
able to reduce the complexity on the calculation of the belief functions Bel,
Pl, and Q. The other structure proposed here, the Hierarchical Trees, permits
us to reduce the complexity of the calculation of Bel, Pl, and Q, as well as of
the Dempster's rule of combination in relation to the brute-force algorithm.
Both these structures do not require the generation of all the subsets of the
reference domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5747</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5747</id><created>2013-03-20</created><authors><author><keyname>Santos</keyname><forenames>Eugene</forenames><suffix>Jr</suffix></author></authors><title>On the Generation of Alternative Explanations with Implications for
  Belief Revision</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-339-347</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In general, the best explanation for a given observation makes no promises on
how good it is with respect to other alternative explanations. A major
deficiency of message-passing schemes for belief revision in Bayesian networks
is their inability to generate alternatives beyond the second best. In this
paper, we present a general approach based on linear constraint systems that
naturally generates alternative explanations in an orderly and highly efficient
manner. This approach is then applied to cost-based abduction problems as well
as belief revision in Bayesian net works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5748</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5748</id><created>2013-03-20</created><authors><author><keyname>Schill</keyname><forenames>Kerstin</forenames></author><author><keyname>Poppel</keyname><forenames>Ernst</forenames></author><author><keyname>Zetzsche</keyname><forenames>Christoph</forenames></author></authors><title>Completing Knowledge by Competing Hierarchies</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-348-352</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A control strategy for expert systems is presented which is based on Shafer's
Belief theory and the combination rule of Dempster. In contrast to well known
strategies it is not sequentially and hypotheses-driven, but parallel and self
organizing, determined by the concept of information gain. The information
gain, calculated as the maximal difference between the actual evidence
distribution in the knowledge base and the potential evidence determines each
consultation step. Hierarchically structured knowledge is an important
representation form and experts even use several hierarchies in parallel for
constituting their knowledge. Hence the control strategy is applied to a
layered set of distinct hierarchies. Depending on the actual data one of these
hierarchies is chosen by the control strategy for the next step in the
reasoning process. Provided the actual data are well matched to the structure
of one hierarchy, this hierarchy remains selected for a longer consultation
time. If no good match can be achieved, a switch from the actual hierarchy to a
competing one will result, very similar to the phenomenon of restructuring in
problem solving tasks. Up to now the control strategy is restricted to multi
hierarchical knowledge bases with disjunct hierarchies. It is implemented in
the expert system IBIG (inference by information gain), being presently applied
to acquired speech disorders (aphasia).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5749</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5749</id><created>2013-03-20</created><authors><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author></authors><title>A Graph-Based Inference Method for Conditional Independence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-353-360</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The graphoid axioms for conditional independence, originally described by
Dawid [1979], are fundamental to probabilistic reasoning [Pearl, 19881. Such
axioms provide a mechanism for manipulating conditional independence assertions
without resorting to their numerical definition. This paper explores a
representation for independence statements using multiple undirected graphs and
some simple graphical transformations. The independence statements derivable in
this system are equivalent to those obtainable by the graphoid axioms.
Therefore, this is a purely graphical proof technique for conditional
independence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5750</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5750</id><created>2013-03-20</created><authors><author><keyname>Shenoy</keyname><forenames>Prakash P.</forenames></author></authors><title>A Fusion Algorithm for Solving Bayesian Decision Problems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-361-369</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new method for solving Bayesian decision problems. The
method consists of representing a Bayesian decision problem as a
valuation-based system and applying a fusion algorithm for solving it. The
fusion algorithm is a hybrid of local computational methods for computation of
marginals of joint probability distributions and the local computational
methods for discrete optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5751</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5751</id><created>2013-03-20</created><authors><author><keyname>Shimony</keyname><forenames>Solomon Eyal</forenames></author></authors><title>Algorithms for Irrelevance-Based Partial MAPs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-370-377</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Irrelevance-based partial MAPs are useful constructs for domain-independent
explanation using belief networks. We look at two definitions for such partial
MAPs, and prove important properties that are useful in designing algorithms
for computing them effectively. We make use of these properties in modifying
our standard MAP best-first algorithm, so as to handle irrelevance-based
partial MAPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5752</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5752</id><created>2013-03-20</created><authors><author><keyname>Smets</keyname><forenames>Philippe</forenames></author></authors><title>About Updating</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-378-385</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Survey of several forms of updating, with a practical illustrative example.
We study several updating (conditioning) schemes that emerge naturally from a
common scenarion to provide some insights into their meaning. Updating is a
subtle operation and there is no single method, no single 'good' rule. The
choice of the appropriate rule must always be given due consideration. Planchet
(1989) presents a mathematical survey of many rules. We focus on the practical
meaning of these rules. After summarizing the several rules for conditioning,
we present an illustrative example in which the various forms of conditioning
can be explained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5753</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5753</id><created>2013-03-20</created><authors><author><keyname>Snow</keyname><forenames>Paul</forenames></author></authors><title>Compressed Constraints in Probabilistic Logic and Their Revision</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-386-391</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In probabilistic logic entailments, even moderate size problems can yield
linear constraint systems with so many variables that exact methods are
impractical. This difficulty can be remedied in many cases of interest by
introducing a three valued logic (true, false, and &quot;don't care&quot;). The
three-valued approach allows the construction of &quot;compressed&quot; constraint
systems which have the same solution sets as their two-valued counterparts, but
which may involve dramatically fewer variables. Techniques to calculate point
estimates for the posterior probabilities of entailed sentences are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5754</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5754</id><created>2013-03-20</created><authors><author><keyname>Spirtes</keyname><forenames>Peter L.</forenames></author></authors><title>Detecting Causal Relations in the Presence of Unmeasured Variables</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-392-397</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The presence of latent variables can greatly complicate inferences about
causal relations between measured variables from statistical data. In many
cases, the presence of latent variables makes it impossible to determine for
two measured variables A and B, whether A causes B, B causes A, or there is
some common cause. In this paper I present several theorems that state
conditions under which it is possible to reliably infer the causal relation
between two measured variables, regardless of whether latent variables are
acting or not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5755</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5755</id><created>2013-03-20</created><authors><author><keyname>Thurston</keyname><forenames>Deborah L.</forenames></author><author><keyname>Tian</keyname><forenames>Yun Qi</forenames></author></authors><title>A Method for Integrating Utility Analysis into an Expert System for
  Design Evaluation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-398-405</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In mechanical design, there is often unavoidable uncertainty in estimates of
design performance. Evaluation of design alternatives requires consideration of
the impact of this uncertainty. Expert heuristics embody assumptions regarding
the designer's attitude towards risk and uncertainty that might be reasonable
in most cases but inaccurate in others. We present a technique to allow
designers to incorporate their own unique attitude towards uncertainty as
opposed to those assumed by the domain expert's rules. The general approach is
to eliminate aspects of heuristic rules which directly or indirectly include
assumptions regarding the user's attitude towards risk, and replace them with
explicit, user-specified probabilistic multi attribute utility and probability
distribution functions. We illustrate the method in a system for material
selection for automobile bumpers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5756</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5756</id><created>2013-03-20</created><authors><author><keyname>Wen</keyname><forenames>Wilson X.</forenames></author></authors><title>From Relational Databases to Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-406-413</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The relationship between belief networks and relational databases is
examined. Based on this analysis, a method to construct belief networks
automatically from statistical relational data is proposed. A comparison
between our method and other methods shows that our method has several
advantages when generalization or prediction is deeded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5757</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5757</id><created>2013-03-20</created><authors><author><keyname>Wilson</keyname><forenames>Nic</forenames></author></authors><title>A Monte-Carlo Algorithm for Dempster-Shafer Belief</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-414-417</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A very computationally-efficient Monte-Carlo algorithm for the calculation of
Dempster-Shafer belief is described. If Bel is the combination using Dempster's
Rule of belief functions Bel, ..., Bel,7, then, for subset b of the frame C),
Bel(b) can be calculated in time linear in 1(31 and m (given that the weight of
conflict is bounded). The algorithm can also be used to improve the complexity
of the Shenoy-Shafer algorithms on Markov trees, and be generalised to
calculate Dempster-Shafer Belief over other logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5758</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5758</id><created>2013-03-20</created><authors><author><keyname>Wong</keyname><forenames>Michael S. K. M.</forenames></author><author><keyname>Yao</keyname><forenames>Y. Y.</forenames></author><author><keyname>Lingras</keyname><forenames>P.</forenames></author></authors><title>Compatibility of Quantitative and Qualitative Representations of Belief</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-418-424</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The compatibility of quantitative and qualitative representations of beliefs
was studied extensively in probability theory. It is only recently that this
important topic is considered in the context of belief functions. In this
paper, the compatibility of various quantitative belief measures and
qualitative belief structures is investigated. Four classes of belief measures
considered are: the probability function, the monotonic belief function,
Shafer's belief function, and Smets' generalized belief function. The analysis
of their individual compatibility with different belief structures not only
provides a sound b&lt;msis for these quantitative measures, but also alleviates
some of the difficulties in the acquisition and interpretation of numeric
belief numbers. It is shown that the structure of qualitative probability is
compatible with monotonic belief functions. Moreover, a belief structure
slightly weaker than that of qualitative belief is compatible with Smets'
generalized belief functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5759</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5759</id><created>2013-03-20</created><authors><author><keyname>Xu</keyname><forenames>Hong</forenames></author></authors><title>An Efficient Implementation of Belief Function Propagation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-425-432</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The local computation technique (Shafer et al. 1987, Shafer and Shenoy 1988,
Shenoy and Shafer 1986) is used for propagating belief functions in so called a
Markov Tree. In this paper, we describe an efficient implementation of belief
function propagation on the basis of the local computation technique. The
presented method avoids all the redundant computations in the propagation
process, and so makes the computational complexity decrease with respect to
other existing implementations (Hsia and Shenoy 1989, Zarley et al. 1988). We
also give a combined algorithm for both propagation and re-propagation which
makes the re-propagation process more efficient when one or more of the prior
belief functions is changed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5760</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5760</id><created>2013-03-20</created><authors><author><keyname>Yager</keyname><forenames>Ronald R.</forenames></author></authors><title>A Non-Numeric Approach to Multi-Criteria/Multi-Expert Aggregation Based
  on Approximate Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-433-437</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a technique that can be used for the fusion of multiple sources
of information as well as for the evaluation and selection of alternatives
under multi-criteria. Three important properties contribute to the uniqueness
of the technique introduced. The first is the ability to do all necessary
operations and aggregations with information that is of a nonnumeric linguistic
nature. This facility greatly reduces the burden on the providers of
information, the experts. A second characterizing feature is the ability
assign, again linguistically, differing importance to the criteria or in the
case of information fusion to the individual sources of information. A third
significant feature of the approach is its ability to be used as method to find
a consensus of the opinion of multiple experts on the issue of concern. The
techniques used in this approach are base on ideas developed from the theory of
approximate reasoning. We illustrate the approach with a problem of project
selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5761</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5761</id><created>2013-03-20</created><authors><author><keyname>Kyburg</keyname><forenames>Henry E.</forenames><suffix>Jr</suffix></author></authors><title>Why Do We Need Foundations for Modelling Uncertainties?</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Seventh Conference on Uncertainty in
  Artificial Intelligence (UAI1991)</comments><proxy>auai</proxy><report-no>UAI-P-1991-PG-438-442</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Surely we want solid foundations. What kind of castle can we build on sand?
What is the point of devoting effort to balconies and minarets, if the
foundation may be so weak as to allow the structure to collapse of its own
weight? We want our foundations set on bedrock, designed to last for
generations. Who would want an architect who cannot certify the soundness of
the foundations of his buildings?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5762</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5762</id><created>2013-03-21</created><authors><author><keyname>Kallel</keyname><forenames>Emna</forenames></author><author><keyname>Aoudni</keyname><forenames>Yassine</forenames></author><author><keyname>Abid</keyname><forenames>Mohamed</forenames></author></authors><title>Object-oriented approach to Rapid Custom Instruction design</title><categories>cs.AR</categories><comments>IEEE 2012</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Due to continuous evolution of Systems-on-Chip (SoC), the complexity of their
design and development has augmented exponentially. To deal with the
ever-growing complexity of such embedded systems, we introduce, in this paper,
an object-oriented approach to rapid SoC design using auto-generation of
hardware custom instructions to simplify and accelerate the SoC design process.
In our approach, a Data Flow Graph (DFG) is adopted as a representation of the
arithmetic operation to convert it to a custom instruction. Then VHDL code will
be automatically generated. The input C code is automatically updated for
calling the new hardware components. To prove the effectiveness of the proposed
approach, a Java source code framework named Automatic Custom Architecture
generator (ACAgen) is developed. Experimental results on 3D sample application
validate our approach and demonstrate how the proposed framework facilitates
and accelerates the SoC design process at low costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5768</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5768</id><created>2013-03-22</created><authors><author><keyname>Thielemann</keyname><forenames>Henning</forenames></author></authors><title>Live music programming in Haskell</title><categories>cs.PL cs.SD</categories><comments>10 pages, 2 figures, Linux Audio Conference 2013. This is a
  translation and update of the ATPS-2012 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We aim for composing algorithmic music in an interactive way with multiple
participants. To this end we have developed an interpreter for a sub-language
of the non-strict functional programming language Haskell that allows the
modification of a program during its execution. Our system can be used both for
musical live-coding and for demonstration and education of functional
programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5778</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5778</id><created>2013-03-22</created><authors><author><keyname>Graves</keyname><forenames>Alex</forenames></author><author><keyname>Mohamed</keyname><forenames>Abdel-rahman</forenames></author><author><keyname>Hinton</keyname><forenames>Geoffrey</forenames></author></authors><title>Speech Recognition with Deep Recurrent Neural Networks</title><categories>cs.NE cs.CL</categories><comments>To appear in ICASSP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent neural networks (RNNs) are a powerful model for sequential data.
End-to-end training methods such as Connectionist Temporal Classification make
it possible to train RNNs for sequence labelling problems where the
input-output alignment is unknown. The combination of these methods with the
Long Short-term Memory RNN architecture has proved particularly fruitful,
delivering state-of-the-art results in cursive handwriting recognition. However
RNN performance in speech recognition has so far been disappointing, with
better results returned by deep feedforward networks. This paper investigates
\emph{deep recurrent neural networks}, which combine the multiple levels of
representation that have proved so effective in deep networks with the flexible
use of long range context that empowers RNNs. When trained end-to-end with
suitable regularisation, we find that deep Long Short-term Memory RNNs achieve
a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to
our knowledge is the best recorded score.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5800</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5800</id><created>2013-03-22</created><authors><author><keyname>Andreica</keyname><forenames>Mugurel Ionut</forenames></author><author><keyname>Tirsa</keyname><forenames>Eliana-Dina</forenames></author></authors><title>Line-Constrained Geometric Server Placement</title><categories>cs.DS cs.CG cs.DC</categories><acm-class>E.1; E.2; F.2.2; H.3.4</acm-class><journal-ref>Metalurgia International, vol. 16, no. 11, pp. 106-110, 2011.
  (ISSN: 1582-2214)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present new algorithmic solutions for several constrained
geometric server placement problems. We consider the problems of computing the
1-center and obnoxious 1-center of a set of line segments, constrained to lie
on a line segment, and the problem of computing the K-median of a set of
points, constrained to lie on a line. The presented algorithms have
applications in many types of distributed systems, as well as in various fields
which make use of distributed systems for running some of their applications
(like chemistry, metallurgy, physics, etc.).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5802</identifier>
 <datestamp>2014-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5802</id><created>2013-03-22</created><updated>2014-01-31</updated><authors><author><keyname>Dall'Anese</keyname><forenames>Emiliano</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Sparsity-leveraging Reconfiguration of Smart Distribution Systems</title><categories>math.OC cs.SY</categories><comments>This is a longer version of a paper that will be published on IEEE
  Transactions of Power Delivery. This longer version contains proofs and
  additional numerical results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A system reconfiguration problem is considered for three-phase power
distribution networks featuring distributed generation. In lieu of binary line
selection variables, the notion of group sparsity is advocated to re-formulate
the nonconvex distribution system reconfiguration (DSR) problem into a convex
one. Using the duality theory, it is shown that the line selection task boils
down to a shrinkage and thresholding operation on the line currents. Further,
numerical tests illustrate the ability of the proposed scheme to identify
meshed, weakly-meshed, or even radial configurations by adjusting a
sparsity-tuning parameter in the DSR cost. Constraints on the voltages are
investigated, and incorporated in the novel DSR problem to effect voltage
regulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5803</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5803</id><created>2013-03-22</created><authors><author><keyname>Andreica</keyname><forenames>Mugurel Ionut</forenames></author><author><keyname>Tapus</keyname><forenames>Nicolae</forenames></author></authors><title>Efficient Online Algorithmic Strategies for Several Two-Player Games
  with Different or Identical Player Roles</title><categories>cs.GT cs.DS</categories><msc-class>91A05, 91A10, 91A12, 91A20, 91A35, 91A40, 91A46, 91A50, 91A80</msc-class><journal-ref>Acta Universitatis Apulensis - Mathematics-Informatics, no. 25,
  pp. 77-97, 2011. (ISSN: 1582-5329)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce novel algorithmic strategies for effciently
playing two-player games in which the players have different or identical
player roles. In the case of identical roles, the players compete for the same
objective (that of winning the game). The case with different player roles
assumes that one of the players asks questions in order to identify a secret
pattern and the other one answers them. The purpose of the first player is to
ask as few questions as possible (or that the questions and their number
satisfy some previously known constraints) and the purpose of the secret player
is to answer the questions in a way that will maximize the number of questions
asked by the first player (or in a way which forces the first player to break
the constraints of the game). We consider both previously known games (or
extensions of theirs) and new types of games, introduced in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5805</identifier>
 <datestamp>2014-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5805</id><created>2013-03-22</created><updated>2014-08-15</updated><authors><author><keyname>Thrampoulidis</keyname><forenames>Christos</forenames></author><author><keyname>Bose</keyname><forenames>Subhonmesh</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Optimal Placement of Distributed Energy Storage in Power Networks</title><categories>math.OC cs.SY</categories><comments>15 pages, 9 figures, generalized result to include line losses in
  Section 4B</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate the optimal placement, sizing and control of storage devices in
a power network to minimize generation costs with the intent of load shifting.
We assume deterministic demand, a linearized DC approximated power flow model
and a fixed available storage budget. Our main result proves that when the
generation costs are convex and nondecreasing, there always exists an optimal
storage capacity allocation that places zero storage at generation-only buses
that connect to the rest of the network via single links. This holds regardless
of the demand profiles, generation capacities, line-flow limits and
characteristics of the storage technologies. Through a counterexample, we
illustrate that this result is not generally true for generation buses with
multiple connections. For specific network topologies, we also characterize the
dependence of the optimal generation cost on the available storage budget,
generation capacities and flow constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5837</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5837</id><created>2013-03-23</created><authors><author><keyname>Grigori</keyname><forenames>Laura</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Jacquelin</keyname><forenames>Mathias</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Khabou</keyname><forenames>Amal</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author></authors><title>Multilevel communication optimal LU and QR factorizations for
  hierarchical platforms</title><categories>cs.DC</categories><proxy>ccsd</proxy><report-no>RR-8270</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study focuses on the performance of two classical dense linear algebra
algorithms, the LU and the QR factorizations, on multilevel hierarchical
platforms. We first introduce a new model called Hierarchical Cluster Platform
(HCP), encapsulating the characteristics of such platforms. The focus is set on
reducing the communication requirements of studied algorithms at each level of
the hierarchy. Lower bounds on communications are therefore extended with
respect to the HCP model. We then introduce multilevel LU and QR algorithms
tailored for those platforms, and provide a detailed performance analysis. We
also provide a set of numerical experiments and performance predictions
demonstrating the need for such algorithms on large platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5839</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5839</id><created>2013-03-23</created><authors><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Jain</keyname><forenames>Satbir</forenames></author></authors><title>Clustering Based Lifetime Maximizing Aggregation Tree for Wireless
  Sensor Networks</title><categories>cs.NI</categories><comments>7 pages</comments><journal-ref>International Journal of Advanced Studies in Computers, Science
  and Engineering (IJASCSE), Feb, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy efficiency is the most important issue in all facets of wireless
sensor networks (WSNs) operations because of the limited and non-replenish able
energy supply. Data aggregation mechanism is one of the possible solutions to
prolong the life time of sensor nodes and on the other hand it also helps in
eliminating the data redundancy and improving the accuracy of information
gathering, is essential for WSNs. In this paper we propose a Clustering based
lifetime maximizing aggregation tree (CLMAT) in which we create aggregation
tree which aim to reduce energy consumption, minimizing the distance traversed
and minimizing the cost in terms of energy consumption. In CLMAT the node
having maximum available energy is used as parent node/ aggregator node. We
concluded with the best possible aggregation tree minimizing energy
utilization, minimizing cost and hence maximizing network lifetime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5841</identifier>
 <datestamp>2013-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5841</id><created>2013-03-23</created><updated>2013-05-30</updated><authors><author><keyname>Liu</keyname><forenames>Jianxing</forenames></author><author><keyname>Laghrouche</keyname><forenames>Salah</forenames></author><author><keyname>Harmouche</keyname><forenames>M.</forenames></author><author><keyname>Wack</keyname><forenames>Maxime</forenames></author></authors><title>Adaptive-Gain Second Order Sliding Mode Observer Design for Switching
  Power Converters</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel adaptive-gain Second Order Sliding Mode (SOSM)
observer is proposed for multicell converters by considering it as a class of
hybrid systems. The aim is to reduce the number of voltage sensors by
estimating the capacitor voltages only from the measurement of load current.
The proposed observer is proven to be robust in the presence of perturbations
with \emph{unknown} boundary. However, the states of the system are only
partially observable in the sense of observability rank condition. Due to its
switching behavior, a recent concept of $Z(T_N)$ observability is used to
analysis its hybrid observability, since its observability depends upon the
switching control signals. Under certain condition of the switching sequences,
the voltage across each capacitor becomes observable. Simulation results and
comparisons with Luenberger switched observer highlight the effectiveness and
robustness of the proposed observer with respect to output measurement noise
and system uncertainties (load variations).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5842</identifier>
 <datestamp>2013-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5842</id><created>2013-03-23</created><updated>2013-05-30</updated><authors><author><keyname>Liu</keyname><forenames>Jianxing</forenames></author><author><keyname>Laghrouche</keyname><forenames>Salah</forenames></author><author><keyname>Wack</keyname><forenames>Maxime</forenames></author></authors><title>Observer-Based High Order Sliding Mode Control of Unity Power Factor in
  Three-Phase AC/DC Converter for Hybrid Electric Vehicle Applications</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a full-bridge boost power converter topology is studied for
power factor control, using output high order sliding mode control. The AC/DC
converters are used for charging the battery and super-capacitor in hybrid
electric vehicles from the utility. The proposed control forces the input
currents to track the desired values, which can controls the output voltage
while keeping the power factor close to one. Super-twisting sliding mode
observer is employed to estimate the input currents and load resistance only
from the measurement of output voltage. Lyapunov analysis shows the asymptotic
convergence of the closed loop system to zero. Simulation results show the
effectiveness and robustness of the proposed controller.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5844</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5844</id><created>2013-03-23</created><updated>2013-05-22</updated><authors><author><keyname>Wang</keyname><forenames>Xiang-Wen</forenames></author><author><keyname>Han</keyname><forenames>Xiao-Pu</forenames></author><author><keyname>Wang</keyname><forenames>Bing-Hong</forenames></author></authors><title>Correlations and Scaling Laws in Human Mobility</title><categories>physics.soc-ph cs.SI</categories><comments>10 pages, 9 figures</comments><doi>10.1371/journal.pone.0084954</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human mobility patterns deeply affect the dynamics of many social systems. In
this paper, we empirically analyze the real-world human movements based GPS
records, and observe rich scaling properties in the temporal-spatial patterns
as well as an abnormal transition in the speed-displacement patterns. We notice
that the displacements at the population level show significant positive
correlation, indicating a cascade-like nature in human movements. Furthermore,
our analysis at the individual level finds that the displacement distributions
of users with strong correlation of displacements are closer to power laws,
implying a relationship between the positive correlation of the series of
displacements and the form of an individual's displacement distribution. These
findings from our empirical analysis show a factor directly relevant to the
origin of the scaling properties in human mobility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5855</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5855</id><created>2013-03-23</created><authors><author><keyname>Zhang</keyname><forenames>Zhong-Yuan</forenames></author><author><keyname>Wang</keyname><forenames>Yong</forenames></author><author><keyname>Ahn</keyname><forenames>Yong-Yeol</forenames></author></authors><title>Overlapping Community Detection in Complex Networks using Symmetric
  Binary Matrix Factorization</title><categories>cs.SI physics.soc-ph</categories><doi>10.1103/PhysRevE.87.062803</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discovering overlapping community structures is a crucial step to
understanding the structure and dynamics of many networks. In this paper we
develop a symmetric binary matrix factorization model (SBMF) to identify
overlapping communities. Our model allows us not only to assign community
memberships explicitly to nodes, but also to distinguish outliers from
overlapping nodes. In addition, we propose a modified partition density to
evaluate the quality of community structures. We use this to determine the most
appropriate number of communities. We evaluate our methods using both synthetic
benchmarks and real world networks, demonstrating the effectiveness of our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5857</identifier>
 <datestamp>2013-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5857</id><created>2013-03-23</created><authors><author><keyname>&#x160;ubelj</keyname><forenames>Lovro</forenames></author><author><keyname>Bajec</keyname><forenames>Marko</forenames></author></authors><title>Model of complex networks based on citation dynamics</title><categories>cs.SI physics.soc-ph</categories><journal-ref>Proceedings of the WWW Workshop on Large Scale Network Analysis
  2013 (LSNA '13), pp. 527-530</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex networks of real-world systems are believed to be controlled by
common phenomena, producing structures far from regular or random. These
include scale-free degree distributions, small-world structure and assortative
mixing by degree, which are also the properties captured by different random
graph models proposed in the literature. However, many (non-social) real-world
networks are in fact disassortative by degree. Thus, we here propose a simple
evolving model that generates networks with most common properties of
real-world networks including degree disassortativity. Furthermore, the model
has a natural interpretation for citation networks with different practical
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5862</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5862</id><created>2013-03-23</created><updated>2013-03-26</updated><authors><author><keyname>Ouali</keyname><forenames>Mourad El</forenames></author><author><keyname>Sauerland</keyname><forenames>Volkmar</forenames></author></authors><title>Improved Approximation Algorithm for the Number of Queries Necessary to
  Identify a Permutation</title><categories>cs.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past three decades, deductive games have become interesting from the
algorithmic point of view. Deductive games are two players zero sum games of
imperfect information. The first player, called &quot;codemaker&quot;, chooses a secret
code and the second player, called &quot;codebreaker&quot;, tries to break the secret
code by making as few guesses as possible, exploiting information that is given
by the codemaker after each guess. A well known deductive game is the famous
Mastermind game. In this paper, we consider the so called Black-Peg variant of
Mastermind, where the only information concerning a guess is the number of
positions in which the guess coincides with the secret code. More precisely, we
deal with a special version of the Black-Peg game with n holes and k &gt;= n
colors where no repetition of colors is allowed. We present a strategy that
identifies the secret code in O(n log n) queries. Our algorithm improves the
previous result of Ker-I Ko and Shia-Chung Teng (1985) by almost a factor of 2
for the case k = n. To our knowledge there is no previous work dealing with the
case k &gt; n.
  Keywords: Mastermind; combinatorial problems; permutations; algorithms
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5867</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5867</id><created>2013-03-23</created><authors><author><keyname>C</keyname><forenames>Srikantaiah K</forenames></author><author><keyname>M</keyname><forenames>Suraj</forenames></author><author><keyname>R</keyname><forenames>Venugopal K</forenames></author><author><keyname>Patnaik</keyname><forenames>L M</forenames></author></authors><title>Similarity based Dynamic Web Data Extraction and Integration System from
  Search Engine Result Pages for Web Content Mining</title><categories>cs.IR cs.DB</categories><comments>8 pages</comments><journal-ref>ACEEE International Journal on Information Technology, Volume 3,
  Issue 1, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is an explosive growth of information in the World Wide Web thus posing
a challenge to Web users to extract essential knowledge from the Web. Search
engines help us to narrow down the search in the form of Search Engine Result
Pages (SERP). Web Content Mining is one of the techniques that help users to
extract useful information from these SERPs. In this paper, we propose two
similarity based mechanisms; WDES, to extract desired SERPs and store them in
the local depository for offline browsing and WDICS, to integrate the requested
contents and enable the user to perform the intended analysis and extract the
desired information. Our experimental results show that WDES and WDICS
outperform DEPTA [1] in terms of Precision and Recall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5870</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5870</id><created>2013-03-23</created><authors><author><keyname>Lopez-Cozar</keyname><forenames>Emilio Delgado</forenames></author><author><keyname>Cabezas-Clavijo</keyname><forenames>Alvaro</forenames></author></authors><title>Ranking journals: Could Google Scholar Metrics be an alternative to
  Journal Citation Reports and Scimago Journal Rank?</title><categories>cs.DL</categories><comments>26 pages, 5 tables</comments><journal-ref>Delgado-Lopez-Cozar, Emilio and Cabezas-Clavijo, Alvaro (2013).
  Ranking journals: could Google Scholar Metrics be an alternative to Journal
  Citation Reports and Scimago Journal Rank? Learned Publishing, 26(2), 101-114</journal-ref><doi>10.1087/20130206</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The launch of Google Scholar Metrics as a tool for assessing scientific
journals may be serious competition for Thomson Reuters Journal Citation
Reports, and for Scopus powered Scimago Journal Rank. A review of these
bibliometric journal evaluation products is performed. We compare their main
characteristics from different approaches: coverage, indexing policies, search
and visualization, bibliometric indicators, results analysis options, economic
cost and differences in their ranking of journals. Despite its shortcomings,
Google Scholar Metrics is a helpful tool for authors and editors in identifying
core journals. As an increasingly useful tool for ranking scientific journals,
it may also challenge established journals products
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5874</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5874</id><created>2013-03-23</created><authors><author><keyname>Mora</keyname><forenames>Oscar</forenames></author><author><keyname>Bisbal</keyname><forenames>Jes&#xfa;s</forenames></author></authors><title>BIMS: Biomedical Information Management System</title><categories>cs.SE</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present BIMS (Biomedical Information Management System).
BIMS is a software architecture designed to provide a flexible computational
framework to manage the information needs of a wide range of biomedical
research projects. The main goal is to facilitate the clinicians' job in data
entry, and researcher's tasks in data management, in high data quality
biomedical research projects. The BIMS architecture has been designed following
the two-level modeling paradigm, a promising methodology to model rich and
dynamic information environments. In addition, a functional implementation of
BIMS architecture has been developed as a web-based application. The result is
a highly flexible web application which allows modeling and managing large
amounts of heterogeneous biomedical data sets, both textual as well as visual
(medical images) information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5887</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5887</id><created>2013-03-23</created><updated>2013-06-17</updated><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author></authors><title>A Behavioural Foundation for Natural Computing and a Programmability
  Test</title><categories>cs.IT cs.AI cs.CC math.IT</categories><comments>37 pages, 4 figures. Based on an invited Talk at the Symposium on
  Natural/Unconventional Computing and its Philosophical Significance, Alan
  Turing World Congress 2012, Birmingham, UK.
  http://link.springer.com/article/10.1007/s13347-012-0095-2 Ref. glitch fixed
  in 2nd. version; Philosophy &amp; Technology (special issue on History and
  Philosophy of Computing), Springer, 2013</comments><doi>10.1007/s13347-012-0095-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What does it mean to claim that a physical or natural system computes? One
answer, endorsed here, is that computing is about programming a system to
behave in different ways. This paper offers an account of what it means for a
physical system to compute based on this notion. It proposes a behavioural
characterisation of computing in terms of a measure of programmability, which
reflects a system's ability to react to external stimuli. The proposed measure
of programmability is useful for classifying computers in terms of the apparent
algorithmic complexity of their evolution in time. I make some specific
proposals in this connection and discuss this approach in the context of other
behavioural approaches, notably Turing's test of machine intelligence. I also
anticipate possible objections and consider the applicability of these
proposals to the task of relating abstract computation to nature-like
computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5891</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5891</id><created>2013-03-23</created><authors><author><keyname>Balasubramanian</keyname><forenames>Bharath</forenames></author><author><keyname>Garg</keyname><forenames>Vijay K.</forenames></author></authors><title>Fault Tolerance in Distributed Systems using Fused State Machines</title><categories>cs.DC</categories><comments>This is under review with the Distributed Computing journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Replication is a standard technique for fault tolerance in distributed
systems modeled as deterministic finite state machines (DFSMs or machines). To
correct f crash or f/2 Byzantine faults among n different machines, replication
requires nf additional backup machines. We present a solution called fusion
that requires just f additional backup machines. First, we build a framework
for fault tolerance in DFSMs based on the notion of Hamming distances. We
introduce the concept of an (f,m)-fusion, which is a set of m backup machines
that can correct f crash faults or f/2 Byzantine faults among a given set of
machines. Second, we present an algorithm to generate an (f,f)-fusion for a
given set of machines. We ensure that our backups are efficient in terms of the
size of their state and event sets. Our evaluation of fusion on the widely used
MCNC'91 benchmarks for DFSMs show that the average state space savings in
fusion (over replication) is 38% (range 0-99%). To demonstrate the practical
use of fusion, we describe its potential application to the MapReduce
framework. Using a simple case study, we compare replication and fusion as
applied to this framework. While a pure replication-based solution requires 1.8
million map tasks, our fusion-based solution requires only 1.4 million map
tasks with minimal overhead during normal operation or recovery. Hence, fusion
results in considerable savings in state space and other resources such as the
power needed to run the backup tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5903</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5903</id><created>2013-03-23</created><authors><author><keyname>Sarkar</keyname><forenames>Kaushik</forenames></author><author><keyname>Sundaram</keyname><forenames>Hari</forenames></author></authors><title>How Do We Find Early Adopters Who Will Guide a Resource Constrained
  Network Towards a Desired Distribution of Behaviors?</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We identify influential early adopters that achieve a target behavior
distribution for a resource constrained social network with multiple costly
behaviors. This problem is important for applications ranging from collective
behavior change to corporate viral marketing campaigns. In this paper, we
propose a model of diffusion of multiple behaviors when individual participants
have resource constraints. Individuals adopt the set of behaviors that maximize
their utility subject to available resources. We show that the problem of
influence maximization for multiple behaviors is NP-complete. Thus we propose
heuristics, which are based on node degree and expected immediate adoption, to
select early adopters. We evaluate the effectiveness under three metrics:
unique number of participants, total number of active behaviors and network
resource utilization. We also propose heuristics to distribute the behaviors
amongst the early adopters to achieve a target distribution in the population.
We test our approach on synthetic and real-world topologies with excellent
results. Our heuristics produce 15-51\% increase in resource utilization over
the na\&quot;ive approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5907</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5907</id><created>2013-03-23</created><authors><author><keyname>Zinoviev</keyname><forenames>Dmitry</forenames></author><author><keyname>Benbrahim</keyname><forenames>Hamid</forenames></author><author><keyname>Meszoely</keyname><forenames>Greta</forenames></author><author><keyname>Stefanescu</keyname><forenames>Dan</forenames></author></authors><title>Simulating Resilience in Transaction-Oriented Networks</title><categories>cs.DC cs.NI</categories><comments>5 pages, 5 figures. Accepted to HPC-2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The power of networks manifests itself in a highly non-linear amplification
of a number of effects, and their weakness - in propagation of cascading
failures. The potential systemic risk effects can be either exacerbated or
mitigated, depending on the resilience characteristics of the network. The
goals of this paper are to study some characteristics of network amplification
and resilience. We simulate random Erdos-Renyi networks and measure
amplification by varying node capacity, transaction volume, and expected
failure rates. We discover that network throughput scales almost quadratically
with respect to the node capacity and that the effects of excessive network
load and random and irreparable node faults are equivalent and almost perfectly
anticorrelated. This knowledge can be used by capacity planners to determine
optimal reliability requirements that maximize the optimal operational regions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5909</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5909</id><created>2013-03-23</created><authors><author><keyname>Liu</keyname><forenames>Dayou</forenames></author><author><keyname>Jin</keyname><forenames>Di</forenames></author><author><keyname>Baquero</keyname><forenames>Carlos</forenames></author><author><keyname>He</keyname><forenames>Dongxiao</forenames></author><author><keyname>Yang</keyname><forenames>Bo</forenames></author><author><keyname>Yu</keyname><forenames>Qiangyuan</forenames></author></authors><title>Genetic Algorithm with a Local Search Strategy for Discovering
  Communities in Complex Networks</title><categories>cs.SI physics.soc-ph</categories><comments>17 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:1303.4711</comments><journal-ref>International Journal of Computational Intelligence Systems, Vol.
  6, No. 2 (March, 2013), 354-369</journal-ref><doi>10.1080/18756891.2013.773175</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to further improve the performance of current genetic algorithms
aiming at discovering communities, a local search based genetic algorithm GALS
is here proposed. The core of GALS is a local search based mutation technique.
In order to overcome the drawbacks of traditional mutation methods, the paper
develops the concept of marginal gene and then the local monotonicity of
modularity function Q is deduced from each nodes local view. Based on these two
elements, a new mutation method combined with a local search strategy is
presented. GALS has been evaluated on both synthetic benchmarks and several
real networks, and compared with some presently competing algorithms.
Experimental results show that GALS is highly effective and efficient for
discovering community structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5910</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5910</id><created>2013-03-23</created><authors><author><keyname>Jin</keyname><forenames>Di</forenames></author><author><keyname>Liu</keyname><forenames>Dayou</forenames></author><author><keyname>Yang</keyname><forenames>Bo</forenames></author><author><keyname>Liu</keyname><forenames>Jie</forenames></author><author><keyname>He</keyname><forenames>Dongxiao</forenames></author></authors><title>Ant Colony Optimization with a New Random Walk Model for Community
  Detection in Complex Networks</title><categories>cs.SI physics.soc-ph</categories><comments>14 pages, 4 figures. arXiv admin note: substantial text overlap with
  arXiv:1303.5675</comments><journal-ref>Advances in Complex Systems, Vol. 14, No. 5 (2011) 795-815</journal-ref><doi>10.1142/S0219525911003219</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting communities from complex networks has recently triggered great
interest. Aiming at this problem, a new ant colony optimization strategy
building on the Markov random walks theory, which is named as MACO, is proposed
in this paper. The framework of ant colony optimization is taken as the basic
framework in this algorithm. In each iteration, a Markov random walk model is
employed as heuristic rule; all of the ants local solutions are aggregated to a
global one through an idea of clustering ensemble, which then will be used to
update a pheromone matrix. The strategy relies on the progressive strengthening
of within-community links and the weakening of between-community links.
Gradually this converges to a solution where the underlying community structure
of the complex network will become clearly visible. The proposed MACO has been
evaluated both on synthetic benchmarks and on some real-world networks, and
compared with some present competing algorithms. Experimental result has shown
that MACO is highly effective for discovering communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5912</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5912</id><created>2013-03-23</created><authors><author><keyname>Jin</keyname><forenames>Di</forenames></author><author><keyname>Liu</keyname><forenames>Dayou</forenames></author><author><keyname>Yang</keyname><forenames>Bo</forenames></author><author><keyname>Liu</keyname><forenames>Jie</forenames></author></authors><title>Fast Complex Network Clustering Algorithm Using Agents</title><categories>cs.SI physics.soc-ph</categories><comments>5 pages, 2 figures</comments><journal-ref>The 8th IEEE International Symposium on Dependable, Autonomic and
  Secure Computing (DASC 2009), pp.615-619</journal-ref><doi>10.1109/DASC.2009.91</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the sizes of networks are always very huge, and they take on
distributed nature. Aiming at this kind of network clustering problem, in the
sight of local view, this paper proposes a fast network clustering algorithm in
which each node is regarded as an agent, and each agent tries to maximize its
local function in order to optimize network modularity defined by function Q,
rather than optimize function Q from the global view as traditional methods.
Both the efficiency and effectiveness of this algorithm are tested against
computer-generated and real-world networks. Experimental result shows that this
algorithm not only has the ability of clustering large-scale networks, but also
can attain very good clustering quality compared with the existing algorithms.
Furthermore, the parameters of this algorithm are analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5913</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5913</id><created>2013-03-24</created><authors><author><keyname>Chen</keyname><forenames>Marcus</forenames></author><author><keyname>Jen</keyname><forenames>Cham Tat</forenames></author><author><keyname>Kim</keyname><forenames>Pang Sze</forenames></author><author><keyname>Goh</keyname><forenames>Alvina</forenames></author></authors><title>A Diffusion Process on Riemannian Manifold for Visual Tracking</title><categories>cs.CV cs.LG cs.RO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust visual tracking for long video sequences is a research area that has
many important applications. The main challenges include how the target image
can be modeled and how this model can be updated. In this paper, we model the
target using a covariance descriptor, as this descriptor is robust to problems
such as pixel-pixel misalignment, pose and illumination changes, that commonly
occur in visual tracking. We model the changes in the template using a
generative process. We introduce a new dynamical model for the template update
using a random walk on the Riemannian manifold where the covariance descriptors
lie in. This is done using log-transformed space of the manifold to free the
constraints imposed inherently by positive semidefinite matrices. Modeling
template variations and poses kinetics together in the state space enables us
to jointly quantify the uncertainties relating to the kinematic states and the
template in a principled way. Finally, the sequential inference of the
posterior distribution of the kinematic states and the template is done using a
particle filter. Our results shows that this principled approach can be robust
to changes in illumination, poses and spatial affine transformation. In the
experiments, our method outperformed the current state-of-the-art algorithm -
the incremental Principal Component Analysis method, particularly when a target
underwent fast poses changes and also maintained a comparable performance in
stable target tracking cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5919</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5919</id><created>2013-03-24</created><authors><author><keyname>Jabbar</keyname><forenames>M. Akhil</forenames></author><author><keyname>Deekshatulu</keyname><forenames>B L</forenames></author><author><keyname>Chandra</keyname><forenames>Priti</forenames></author></authors><title>Heart Disease Prediction System using Associative Classification and
  Genetic Algorithm</title><categories>cs.AI stat.AP</categories><comments>International Conference on Emerging Trends in Electrical,
  Electronics and Communication Technologies-ICECIT, 2012</comments><journal-ref>Vol no1 pp 183-192, Elsevier Dec 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Associative classification is a recent and rewarding technique which
integrates association rule mining and classification to a model for prediction
and achieves maximum accuracy. Associative classifiers are especially fit to
applications where maximum accuracy is desired to a model for prediction. There
are many domains such as medical where the maximum accuracy of the model is
desired. Heart disease is a single largest cause of death in developed
countries and one of the main contributors to disease burden in developing
countries. Mortality data from the registrar general of India shows that heart
disease are a major cause of death in India, and in Andhra Pradesh coronary
heart disease cause about 30%of deaths in rural areas. Hence there is a need to
develop a decision support system for predicting heart disease of a patient. In
this paper we propose efficient associative classification algorithm using
genetic approach for heart disease prediction. The main motivation for using
genetic algorithm in the discovery of high level prediction rules is that the
discovered rules are highly comprehensible, having high predictive accuracy and
of high interestingness values. Experimental Results show that most of the
classifier rules help in the best prediction of heart disease which even helps
doctors in their diagnosis decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5926</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5926</id><created>2013-03-24</created><authors><author><keyname>Dasgupta</keyname><forenames>Sourish</forenames></author><author><keyname>Bhat</keyname><forenames>Satish</forenames></author><author><keyname>Lee</keyname><forenames>Yugyung</forenames></author></authors><title>STC: Semantic Taxonomical Clustering for Service Category Learning</title><categories>cs.SE</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service discovery is one of the key problems that has been widely researched
in the area of Service Oriented Architecture (SOA) based systems. Service
category learning is a technique for efficiently facilitating service
discovery. Most approaches for service category learning are based on suitable
similarity distance measures using thresholds. Threshold selection is
essentially difficult and often leads to unsatisfactory accuracy. In this
paper, we have proposed a self-organizing based clustering algorithm called
Semantic Taxonomical Clustering (STC) for taxonomically organizing services
with self-organizing information and knowledge. We have tested the STC
algorithm on both randomly generated data and the standard OWL-S TC dataset. We
have observed promising results both in terms of classification accuracy and
runtime performance compared to existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5929</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5929</id><created>2013-03-24</created><authors><author><keyname>Dasgupta</keyname><forenames>Sourish</forenames></author><author><keyname>Padia</keyname><forenames>Ankur</forenames></author><author><keyname>Shah</keyname><forenames>Kushal</forenames></author><author><keyname>KaPatel</keyname><forenames>Rupali</forenames></author><author><keyname>Majumder</keyname><forenames>Prasenjit</forenames></author></authors><title>DLOLIS-A: Description Logic based Text Ontology Learning</title><categories>cs.AI</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ontology Learning has been the subject of intensive study for the past
decade. Researchers in this field have been motivated by the possibility of
automatically building a knowledge base on top of text documents so as to
support reasoning based knowledge extraction. While most works in this field
have been primarily statistical (known as light-weight Ontology Learning) not
much attempt has been made in axiomatic Ontology Learning (called heavy-weight
Ontology Learning) from Natural Language text documents. Heavy-weight Ontology
Learning supports more precise formal logic-based reasoning when compared to
statistical ontology learning. In this paper we have proposed a sound Ontology
Learning tool DLOL_(IS-A) that maps English language IS-A sentences into their
equivalent Description Logic (DL) expressions in order to automatically
generate a consistent pair of T-box and A-box thereby forming both regular
(definitional form) and generalized (axiomatic form) DL ontology. The current
scope of the paper is strictly limited to IS-A sentences that exclude the
possible structures of: (i) implicative IS-A sentences, and (ii) &quot;Wh&quot; IS-A
questions. Other linguistic nuances that arise out of pragmatics and epistemic
of IS-A sentences are beyond the scope of this present work. We have adopted
Gold Standard based Ontology Learning evaluation on chosen IS-A rich Wikipedia
documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5934</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5934</id><created>2013-03-24</created><authors><author><keyname>Hezarkhani</keyname><forenames>Behzad</forenames></author><author><keyname>Kubiak</keyname><forenames>Wieslaw</forenames></author><author><keyname>Hartman</keyname><forenames>Bruce</forenames></author></authors><title>On Transshipment Games with Identical Newsvendors</title><categories>math.OC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a transshipment game, supply chain agents cooperate to transship surplus
products. This note studies the effect of size of transshipment coalitions on
the optimal production/order quantities. It characterizes these quantities for
transshipment games with identical newsvendors and normally distributed market
demands. It also gives a closed form formula for equal allocation in their
cores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5938</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5938</id><created>2013-03-24</created><authors><author><keyname>Ralph</keyname><forenames>Paul</forenames></author></authors><title>The Two Paradigms of Software Design</title><categories>cs.SE</categories><comments>38 pages, 3 tables, 4 figures. A previous version of this paper was
  published at the 2010 Mediterranean Conference on Information Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dominant view of design in information systems and software engineering,
the Rational Design Paradigm, views software development as a methodical,
plan-centered, approximately rational process of optimizing a design candidate
for known constraints and objectives. This paper synthesizes an Alternative
Design Paradigm, which views software development as an amethodical,
improvisational, emotional process of simultaneously framing the problem and
building artifacts to address it. These conflicting paradigms are
manifestations of a deeper philosophical conflict between rationalism and
empiricism. The paper clarifies the nature, components and assumptions of each
paradigm and explores the implications of the paradigmatic conflict for
research, practice and education.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5942</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5942</id><created>2013-03-24</created><updated>2015-05-17</updated><authors><author><keyname>Brassard</keyname><forenames>Gilles</forenames></author><author><keyname>Devroye</keyname><forenames>Luc</forenames></author><author><keyname>Gravel</keyname><forenames>Claude</forenames></author></authors><title>Exact simulation of the GHZ distribution</title><categories>cs.IT math.IT quant-ph</categories><comments>Improved in a variety of ways, including new results. 27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  John Bell has shown that the correlations entailed by quantum mechanics
cannot be reproduced by a classical process involving non-communicating
parties. But can they be simulated with the help of bounded communication? This
problem has been studied for more than two decades and it is now well
understood in the case of bipartite entanglement. However, the issue was still
widely open for multipartite entanglement, even for the simplest case, which is
the tripartite Greenberger-Horne-Zeilinger (GHZ) state. We give an exact
simulation of arbitrary independent von Neumann measurements on general
n-partite GHZ states. Our protocol requires O(n^2) bits of expected
communication between the parties, and O(n log n) expected time is sufficient
to carry it out in parallel. Furthermore, we need only an expectation of O(n)
independent unbiased random bits, with no need for the generation of continuous
real random variables nor prior shared random variables. In the case of
equatorial measurements, we improve on the prior art with a protocol that needs
only O(n log n) bits of communication and O(log^2 n) parallel time. At the cost
of a slight increase in the number of bits communicated, these tasks can be
accomplished with a constant expected number of rounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5943</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5943</id><created>2013-03-24</created><authors><author><keyname>Namiot</keyname><forenames>Dmitry</forenames></author><author><keyname>Sneps-Sneppe</keyname><forenames>Manfred</forenames></author></authors><title>Geofence and Network Proximity</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many of modern location-based services are often based on an area or place as
opposed to an accurate determination of the precise location. Geo-fencing
approach is based on the observation that users move from one place to another
and then stay at that place for a while. These places can be, for example,
commercial properties, homes, office centers and so on. As per geo-fencing
approach they could be described (defined) as some geographic areas bounded by
polygons. It assumes users simply move from fence to fence and stay inside
fences for a while. In this article we replace geo-based boundaries with
network proximity rules. This new approach let us effectively deploy location
based services indoor and provide a significant energy saving for mobile
devices comparing with the traditional methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5947</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5947</id><created>2013-03-24</created><updated>2013-06-19</updated><authors><author><keyname>Nguyen</keyname><forenames>Hieu Duy</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Hui</keyname><forenames>Hon Tat</forenames></author></authors><title>Effect of Receive Spatial Diversity on the Degrees of Freedom Region in
  Multi-Cell Random Beamforming</title><categories>cs.IT math.IT</categories><comments>33 pages, 7 figures, a longer version of the paper submitted to IEEE
  Transactions on Wireless Communcations. This work was presented in part at
  IEEE Wireless Communications and Networking Conference (WCNC), Shanghai,
  China, April 07-10, 2013. The authors are with the Department of Electrical
  and Computer Engineering, National University of Singapore (emails: {hieudn,
  elezhang, elehht}@nus.edu.sg)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The random beamforming (RBF) scheme, jointly applied with multi-user
diversity based scheduling, is able to achieve virtually interference-free
downlink transmissions with only partial channel state information (CSI)
available at the transmitter. However, the impact of receive spatial diversity
on the rate performance of RBF is not fully characterized yet even in a
single-cell setup. In this paper, we study a multi-cell multiple-input
multiple-output (MIMO) broadcast system with RBF applied at each base station
(BS) and either the minimum-mean-square-error (MMSE), matched filter (MF), or
antenna selection (AS) based spatial receiver employed at each mobile terminal.
We investigate the effect of different spatial diversity receivers on the
achievable sum-rate of multi-cell RBF systems subject to both the intra- and
inter-cell interferences. We first derive closed-form expressions for the
distributions of the receiver signal-to-interference-plus-noise ratio (SINR)
with different spatial diversity techniques, based on which we compare their
rate performances at finite signal-to-noise ratios (SNRs). We then investigate
the asymptotically high-SNR regime and for a tractable analysis assume that the
number of users in each cell scales in a certain order with the per-cell SNR as
SNR goes to infinity. Under this setup, we characterize the degrees of freedom
(DoF) region for multi-cell RBF systems with different types of spatial
receivers, which consists of all the achievable DoF tuples for the individual
sum-rate of all the cells. The DoF region analysis provides a succinct
characterization of the interplays among the receive spatial diversity,
multiuser diversity, spatial multiplexing gain, inter-/intra-cell
interferences, and BSs' collaborative transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5950</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5950</id><created>2013-03-24</created><authors><author><keyname>Dinesh</keyname><forenames>C.</forenames></author></authors><title>High Quality Requirement Engineering and Applying Priority Based Tools
  for QoS Standardization in Web Service Architecture</title><categories>cs.SE</categories><comments>9 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Even though there are more development to improving the Quality of Service
and requirement engineering in web services yet there is a big scarcity for its
related standardization in day to day progress leading to vast needs in its
area. Also in web service environment it always has been a big challenge to
raise the standard of Quality of Service in requirement engineering analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5956</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5956</id><created>2013-03-24</created><updated>2013-06-20</updated><authors><author><keyname>Sebastian</keyname><forenames>Preugschat</forenames><affiliation>Christian-Albrechts-Universit&#xe4;t zu Kiel</affiliation></author><author><keyname>Wilke</keyname><forenames>Thomas</forenames><affiliation>Christian-Albrechts-Universit&#xe4;t zu Kiel</affiliation></author></authors><title>Effective Characterizations of Simple Fragments of Temporal Logic Using
  Carton--Michel Automata</title><categories>cs.FL cs.LO</categories><comments>22 pages, to appear in LMCS Special Issue: Selected papers of the
  conference on &quot;Foundations of Software Science and Computation Structures&quot;:
  FOSSACS 2012</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 2 (June 21,
  2013) lmcs:909</journal-ref><doi>10.2168/LMCS-9(2:8)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for obtaining effective characterizations of simple
fragments of future temporal logic (LTL) with the natural numbers as time
domain. The framework is based on a form of strongly unambiguous automata, also
known as prophetic automata or complete unambiguous B\&quot;uchi automata and
referred to as Carton-Michel automata in this paper. These automata enjoy
strong structural properties, in particular, they separate the &quot;finitary
fraction&quot; of a regular language of infinite words from its &quot;infinitary
fraction&quot; in a natural fashion. Within our framework, we provide
characterizations of several natural fragments of temporal logic, where, in
some cases, no effective characterization had been known previously, and give
lower and upper bounds for their computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5958</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5958</id><created>2013-03-24</created><authors><author><keyname>Vigneron</keyname><forenames>Antoine</forenames></author><author><keyname>Yan</keyname><forenames>Lie</forenames></author></authors><title>A Faster Algorithm for Computing Motorcycle Graphs</title><categories>cs.CG</categories><comments>22 page, 28 figures, extended abstract to appear in the proceedings
  of the Symposium on Computational Geometry (SoCG 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for computing motorcycle graphs that runs in
O(n^(4/3+e)) time for any e&gt;0, improving on all previously known algorithms.
The main application of this result is to computing the straight skeleton of a
polygon. It allows us to compute the straight skeleton of a non-degenerate
polygon with h holes in O(n.sqrt(h+1)log^2(n)+n^(4/3+e)) expected time. If all
input coordinates are O(log n)-bit rational numbers, we can compute the
straight skeleton of a (possibly degenerate) polygon with h holes in
O(n.sqrt(h+1)log^3(n)) expected time.
  In particular, it means that we can compute the straight skeleton of a simple
polygon in O(n.log^3(n)) expected time if all input coordinates are O(\log
n)-bit rationals, while all previously known algorithms have worst-case running
time larger than n^(3/2).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5960</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5960</id><created>2013-03-24</created><updated>2016-01-21</updated><authors><author><keyname>Christen</keyname><forenames>Daniel</forenames></author></authors><title>SYNTAGMA. A Linguistic Approach to Parsing</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SYNTAGMA is a rule-based parsing system, structured on two levels: a general
parsing engine and a language specific grammar. The parsing engine is a
language independent program, while grammar and language specific rules and
resources are given as text files, consisting in a list of constituent
structuresand a lexical database with word sense related features and
constraints. Since its theoretical background is principally Tesniere's
Elements de syntaxe, SYNTAGMA's grammar emphasizes the role of argument
structure (valency) in constraint satisfaction, and allows also horizontal
bounds, for instance treating coordination. Notions such as Pro, traces, empty
categories are derived from Generative Grammar and some solutions are close to
Government&amp;Binding Theory, although they are the result of an autonomous
research. These properties allow SYNTAGMA to manage complex syntactic
configurations and well known weak points in parsing engineering. An important
resource is the semantic network, which is used in disambiguation tasks.
Parsing process follows a bottom-up, rule driven strategy. Its behavior can be
controlled and fine-tuned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5965</identifier>
 <datestamp>2015-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5965</id><created>2013-03-24</created><updated>2015-01-05</updated><authors><author><keyname>Salvia</keyname><forenames>Raffaele</forenames></author></authors><title>A catalog of matchstick graphs</title><categories>math.CO cs.DM</categories><comments>88 pages, 2 tables, 7 plots</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Classification of planar unit-distance graphs with up to 9 edges, by
homeomorphism and isomorphism classes. With exactly nine edges, there are 633
nonisomorphic connected matchstick graphs, of which 196 are topologically
distinct from each other. Increasing edges' number, their quantities rise more
than exponentially, in a still unclear way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5966</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5966</id><created>2013-03-24</created><updated>2014-02-17</updated><authors><author><keyname>Karsai</keyname><forenames>M&#xe1;rton</forenames></author><author><keyname>Perra</keyname><forenames>Nicola</forenames></author><author><keyname>Vespignani</keyname><forenames>Alessandro</forenames></author></authors><title>Time varying networks and the weakness of strong ties</title><categories>physics.soc-ph cs.SI</categories><comments>22 pages, 15 figures</comments><journal-ref>Scientific Reports 4, 4001 (2014)</journal-ref><doi>10.1038/srep04001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In most social and information systems the activity of agents generates
rapidly evolving time-varying networks. The temporal variation in networks'
connectivity patterns and the ongoing dynamic processes are usually coupled in
ways that still challenge our mathematical or computational modelling. Here we
analyse a mobile call dataset and find a simple statistical law that
characterize the temporal evolution of users' egocentric networks. We encode
this observation in a reinforcement process defining a time-varying network
model that exhibits the emergence of strong and weak ties. We study the effect
of time-varying and heterogeneous interactions on the classic rumour spreading
model in both synthetic, and real-world networks. We observe that strong ties
severely inhibit information diffusion by confining the spreading process among
agents with recurrent communication patterns. This provides the
counterintuitive evidence that strong ties may have a negative role in the
spreading of information across networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5972</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5972</id><created>2013-03-24</created><authors><author><keyname>Pal</keyname><forenames>Arijit Kumar</forenames></author><author><keyname>Das</keyname><forenames>Poulami</forenames></author><author><keyname>Dey</keyname><forenames>Nilanjan</forenames></author></authors><title>Odd-Even Embedding Scheme Based Modified Reversible Watermarking
  Technique using Blueprint</title><categories>cs.MM cs.CR</categories><comments>10 Pages, Figure 2, Table 1, FOSET, Academic Meet, Kolkata, India,
  22-23 March 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital watermarking is a technique of information adding or information
hiding in order to identify the owner of the data in multimedia content. It
seems that a signal or digital image can permanently embed over another digital
data providing a good way to protect intellectual property from illegal
replication. The cover data that is transmitted through the internet hides the
watermark in a computer aided assertion method such that it becomes
undetectable. Finally it stands as a hindrance over many operations without
harming the embedded host document. Unfortunately, many owners of the digital
materials such as images, text, audio and video are reluctant to the spreading
of their documents on the web or other networked environment, because the ease
of duplicating digital materials facilitates copyright violation. Digital media
distribution occurs through various channels. The cover data may or may not
hold any relation with the watermark information. In the last two decades, a
considerable amount of research has been done on the digital watermarking of
multimedia files such as audio, video, images and text. Different type of
watermarking algorithms has been proposed by the researchers to achieve high
level of security and authenticity. In our proposed method, a modified
reversible watermarking technique is introduced, which employs a blueprint
generation of original image based on odd-even embedding methodology to yield
large data hiding capacity, security as well as high watermarked quality. The
experimental results demonstrate that, no matter how much secret data is
embedded, the watermarked quality is about 51dB in this proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5974</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5974</id><created>2013-03-24</created><updated>2013-06-15</updated><authors><author><keyname>Ganesan</keyname><forenames>Ashwin</forenames></author></authors><title>Automorphisms of Cayley graphs generated by transposition sets</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $S$ be a set of transpositions such that the girth of the transposition
graph of $S$ is at least 5. It is shown that the automorphism group of the
Cayley graph of the permutation group $H$ generated by $S$ is the semidirect
product $R(H) \rtimes \Aut(H,S)$, where $R(H)$ is the right regular
representation of $H$ and $\Aut(H,S)$ is the set of automorphisms of $H$ that
fixes $S$ setwise. Furthermore, if the connected components of the
transposition graph of $S$ are isomorphic to each other, then $\Aut(H,S)$ is
isomorphic to the automorphism group of the line graph of the transposition
graph of $S$. This result is a common generalization of previous results by
Feng, Ganesan, Harary, Mirafzal, and Zhang and Huang. As another special case,
we obtain the automorphism group of the extended cube graph that was proposed
as a topology for interconnection networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5976</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5976</id><created>2013-03-24</created><authors><author><keyname>Villa</keyname><forenames>Silvia</forenames></author><author><keyname>Rosasco</keyname><forenames>Lorenzo</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author></authors><title>On Learnability, Complexity and Stability</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the fundamental question of learnability of a hypotheses class in
the supervised learning setting and in the general learning setting introduced
by Vladimir Vapnik. We survey classic results characterizing learnability in
term of suitable notions of complexity, as well as more recent results that
establish the connection between learnability and stability of a learning
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5984</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5984</id><created>2013-03-24</created><authors><author><keyname>Ibrahimi</keyname><forenames>Morteza</forenames></author><author><keyname>Javanmard</keyname><forenames>Adel</forenames></author><author><keyname>Van Roy</keyname><forenames>Benjamin</forenames></author></authors><title>Efficient Reinforcement Learning for High Dimensional Linear Quadratic
  Systems</title><categories>stat.ML cs.LG math.OC</categories><comments>16 pages</comments><journal-ref>Advances in Neural Information Processing Systems (NIPS) 2012:
  2645-2653</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of adaptive control of a high dimensional linear
quadratic (LQ) system. Previous work established the asymptotic convergence to
an optimal controller for various adaptive control schemes. More recently, for
the average cost LQ problem, a regret bound of ${O}(\sqrt{T})$ was shown, apart
form logarithmic factors. However, this bound scales exponentially with $p$,
the dimension of the state space. In this work we consider the case where the
matrices describing the dynamic of the LQ system are sparse and their
dimensions are large. We present an adaptive control scheme that achieves a
regret bound of ${O}(p \sqrt{T})$, apart from logarithmic factors. In
particular, our algorithm has an average cost of $(1+\eps)$ times the optimum
cost after $T = \polylog(p) O(1/\eps^2)$. This is in comparison to previous
work on the dense dynamics where the algorithm requires time that scales
exponentially with dimension in order to achieve regret of $\eps$ times the
optimal cost.
  We believe that our result has prominent applications in the emerging area of
computational advertising, in particular targeted online advertising and
advertising in social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.5988</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.5988</id><created>2013-03-24</created><authors><author><keyname>Yao</keyname><forenames>Hengshuai</forenames></author><author><keyname>Schuurmans</keyname><forenames>Dale</forenames></author></authors><title>Reinforcement Ranking</title><categories>cs.IR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new framework for web page ranking -- reinforcement ranking --
that improves the stability and accuracy of Page Rank while eliminating the
need for computing the stationary distribution of random walks. Instead of
relying on teleportation to ensure a well defined Markov chain, we develop a
reverse-time reinforcement learning framework that determines web page
authority based on the solution of a reverse Bellman equation. In particular,
for a given reward function and surfing policy we recover a well defined
authority score from a reverse-time perspective: looking back from a web page,
what is the total incoming discounted reward brought by the surfer from the
page's predecessors? This results in a novel form of reverse-time
dynamic-programming/reinforcement-learning problem that achieves several
advantages over Page Rank based methods: First, stochasticity, ergodicity, and
irreducibility of the underlying Markov chain is no longer required for
well-posedness. Second, the method is less sensitive to graph topology and more
stable in the presence of dangling pages. Third, not only does the reverse
Bellman iteration yield a more efficient power iteration, it allows for faster
updating in the presence of graph changes. Finally, our experiments demonstrate
improvements in ranking quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6001</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6001</id><created>2013-03-24</created><authors><author><keyname>Szalkai</keyname><forenames>Bal&#xe1;zs</forenames></author></authors><title>Generalizing k-means for an arbitrary distance matrix</title><categories>cs.LG cs.CV stat.ML</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The original k-means clustering method works only if the exact vectors
representing the data points are known. Therefore calculating the distances
from the centroids needs vector operations, since the average of abstract data
points is undefined. Existing algorithms can be extended for those cases when
the sole input is the distance matrix, and the exact representing vectors are
unknown. This extension may be named relational k-means after a notation for a
similar algorithm invented for fuzzy clustering. A method is then proposed for
generalizing k-means for scenarios when the data points have absolutely no
connection with a Euclidean space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6017</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6017</id><created>2013-03-24</created><updated>2013-03-30</updated><authors><author><keyname>Wu</keyname><forenames>Zhouyun</forenames></author><author><keyname>Huang</keyname><forenames>Aiping</forenames></author><author><keyname>Chen</keyname><forenames>Hsiao-Hwa</forenames></author></authors><title>Scrambling Code Planning in TD-SCDMA Systems</title><categories>cs.IT cs.NI math.IT</categories><comments>This paper has been withdrawn</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn by the author due to a crucial sign error in
equation 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6020</identifier>
 <datestamp>2014-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6020</id><created>2013-03-24</created><updated>2014-12-17</updated><authors><author><keyname>Chang</keyname><forenames>Fei-Huang</forenames></author><author><keyname>Chen</keyname><forenames>Hong-Bin</forenames></author><author><keyname>Guo</keyname><forenames>Jun-Yi</forenames></author><author><keyname>Huang</keyname><forenames>Yu-Pei</forenames></author></authors><title>Multi-Group Testing for Items with Real-Valued Status under Standard
  Arithmetic</title><categories>cs.IT math.CO math.IT</categories><comments>presented in part at 2nd Japan-Taiwan Conference of Combinatorics and
  its Applications, Nagoya University, Japan, 2012</comments><msc-class>05C35, 05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel generalization of group testing, called
multi-group testing, which relaxes the notion of &quot;testing subset&quot; in group
testing to &quot;testing multi-set&quot;. The generalization aims to learn more
information of each item to be tested rather than identify only defectives as
was done in conventional group testing. This paper provides efficient
nonadaptive strategies for the multi-group testing problem. The major tool is a
new structure, $q$-ary additive $(w,d)$-disjunct matrix, which is a
generalization of the well-known binary disjunct matrix introduced by Kautz and
Singleton in 1964.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6021</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6021</id><created>2013-03-24</created><authors><author><keyname>Sanin</keyname><forenames>Andres</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Harandi</keyname><forenames>Mehrtash T.</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>Spatio-Temporal Covariance Descriptors for Action and Gesture
  Recognition</title><categories>cs.CV cs.HC</categories><acm-class>I.5.1; I.5.4; I.4.7; I.4.8; I.2.10</acm-class><journal-ref>IEEE Workshop on Applications of Computer Vision, pp. 103-110,
  2013</journal-ref><doi>10.1109/WACV.2013.6475006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new action and gesture recognition method based on
spatio-temporal covariance descriptors and a weighted Riemannian locality
preserving projection approach that takes into account the curved space formed
by the descriptors. The weighted projection is then exploited during boosting
to create a final multiclass classification algorithm that employs the most
useful spatio-temporal regions. We also show how the descriptors can be
computed quickly through the use of integral video representations. Experiments
on the UCF sport, CK+ facial expression and Cambridge hand gesture datasets
indicate superior performance of the proposed method compared to several recent
state-of-the-art techniques. The proposed method is robust and does not require
additional processing of the videos, such as foreground detection,
interest-point detection or tracking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6022</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6022</id><created>2013-03-24</created><authors><author><keyname>Zhang</keyname><forenames>Zhe</forenames></author><author><keyname>Liu</keyname><forenames>Xudong</forenames></author><author><keyname>Sun</keyname><forenames>Hailong</forenames></author><author><keyname>Zhang</keyname><forenames>Richong</forenames></author><author><keyname>Wang</keyname><forenames>Fei</forenames></author></authors><title>A Cooperation Model Towards the Federated Internet of Applications</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As Internet is changing from network of data into network of functionalities,
a federated Internet of applications, that every application can cooperate with
each other smoothly, is a natural trending topic. However, existing integration
techniques did not pay enough attention to multiple control domains for
participants, i.e. application providers and end-users. In this study, we
advocate a global cooperation model for all the participants counts. In
particular, we propose a hybrid model to manage the cooperation among
applications to achieve more optimized allocation of efforts, which means users
perform lighter actions and application providers concerning less
uncontrollable information. In addition, we implement the required system and
show a case study which demonstrates the effectiveness of this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6025</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6025</id><created>2013-03-25</created><authors><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Robust Stability Analysis of an Optical Parametric Amplifier Quantum
  System</title><categories>quant-ph cs.SY math.OC</categories><comments>A shortened version will appear in the Proceedings of the 2013 Asian
  Control Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of robust stability for a class of uncertain
nonlinear quantum systems subject to unknown perturbations in the system
Hamiltonian. The case of a nominal linear quantum system is considered with
non-quadratic perturbations to the system Hamiltonian. The paper extends recent
results on the robust stability of nonlinear quantum systems to allow for
non-quadratic perturbations to the Hamiltonian which depend on multiple
parameters. A robust stability condition is given in terms of a strict bounded
real condition. This result is then applied to the robust stability analysis of
a nonlinear quantum system which is a model of an optical parametric amplifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6034</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6034</id><created>2013-03-25</created><updated>2013-04-15</updated><authors><author><keyname>SaiToh</keyname><forenames>Akira</forenames></author></authors><title>ZKCM: a C++ library for multiprecision matrix computation with
  applications in quantum information</title><categories>cs.MS physics.comp-ph quant-ph</categories><comments>19 pages, 5 figures, to appear in Comput. Phys. Comm.; this is an
  extended version of arXiv:1111.3124, v2: typographical corrections only</comments><msc-class>97N80, 81-01</msc-class><acm-class>G.4</acm-class><journal-ref>Comput. Phys. Comm. 184, 2005-2020 (2013)</journal-ref><doi>10.1016/j.cpc.2013.03.022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ZKCM is a C++ library developed for the purpose of multiprecision matrix
computation, on the basis of the GNU MP and MPFR libraries. It provides an
easy-to-use syntax and convenient functions for matrix manipulations including
those often used in numerical simulations in quantum physics. Its extension
library, ZKCM_QC, is developed for simulating quantum computing using the
time-dependent matrix-product-state simulation method. This paper gives an
introduction about the libraries with practical sample programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6046</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6046</id><created>2013-03-25</created><authors><author><keyname>Gerami</keyname><forenames>Majid</forenames></author><author><keyname>Xiao</keyname><forenames>Ming</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author><author><keyname>Shum</keyname><forenames>Kenneth W.</forenames></author><author><keyname>Lin</keyname><forenames>Dengsheng</forenames></author></authors><title>Optimized-Cost Repair in Multi-hop Distributed Storage Systems with
  Network Coding</title><categories>cs.IT math.IT</categories><comments>(Submitted)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In distributed storage systems reliability is achieved through redundancy
stored at different nodes in the network. Then a data collector can reconstruct
source information even though some nodes fail. To maintain reliability, an
autonomous and efficient protocol should be used to repair the failed node. The
repair process causes traffic and consequently transmission cost in the
network. Recent results found the optimal trafficstorage tradeoff, and proposed
regenerating codes to achieve the optimality. We aim at minimizing the
transmission cost in the repair process. We consider the network topology in
the repair, and accordingly modify information flow graphs. Then we analyze the
cut requirement and based on the results, we formulate the minimum-cost as a
linear programming problem for linear costs. We show that the solution of the
linear problem establishes a fundamental lower bound of the repair-cost. We
also show that this bound is achievable for minimum storage regenerating, which
uses the optimal-cost minimum-storage regenerating (OCMSR) code. We propose
surviving node cooperation which can efficiently reduce the repair cost.
Further, the field size for the construction of OCMSR codes is discussed. We
show the gain of optimal-cost repair in tandem, star, grid and fully connected
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6066</identifier>
 <datestamp>2014-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6066</id><created>2013-03-25</created><updated>2014-02-25</updated><authors><author><keyname>Paisitkriangkrai</keyname><forenames>Sakrapee</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>Asymmetric Pruning for Learning Cascade Detectors</title><categories>cs.CV</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cascade classifiers are one of the most important contributions to real-time
object detection. Nonetheless, there are many challenging problems arising in
training cascade detectors. One common issue is that the node classifier is
trained with a symmetric classifier. Having a low misclassification error rate
does not guarantee an optimal node learning goal in cascade classifiers, i.e.,
an extremely high detection rate with a moderate false positive rate. In this
work, we present a new approach to train an effective node classifier in a
cascade detector. The algorithm is based on two key observations: 1) Redundant
weak classifiers can be safely discarded; 2) The final detector should satisfy
the asymmetric learning objective of the cascade architecture. To achieve this,
we separate the classifier training into two steps: finding a pool of
discriminative weak classifiers/features and training the final classifier by
pruning weak classifiers which contribute little to the asymmetric learning
criterion (asymmetric classifier construction). Our model reduction approach
helps accelerate the learning time while achieving the pre-determined learning
objective. Experimental results on both face and car data sets verify the
effectiveness of the proposed algorithm. On the FDDB face data sets, our
approach achieves the state-of-the-art performance, which demonstrates the
advantage of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6071</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6071</id><created>2013-03-25</created><updated>2014-01-24</updated><authors><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Shi</keyname><forenames>Tianlin</forenames></author></authors><title>A Fully Polynomial-Time Approximation Scheme for Approximating a Sum of
  Random Variables</title><categories>cs.DS</categories><comments>11 pages, new title, proofs polished, several typos revised. Also
  added a section about the bit complexity</comments><doi>10.1016/j.orl.2014.02.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given $n$ independent random variables $X_1, X_2, ..., X_n$ and an integer
$C$, we study the fundamental problem of computing the probability that the sum
$X=X_1+X_2+...+X_n$ is at most $C$. We assume that each random variable $X_i$
is implicitly given by an oracle which, given an input value $k$, returns the
probability $X_i\leq k$. We give the first deterministic fully polynomial-time
approximation scheme (FPTAS) to estimate the probability up to a relative error
of $1\pm \epsilon$. Our algorithm is based on the idea developed for
approximately counting knapsack solutions in [Gopalan et al. FOCS11].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6075</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6075</id><created>2013-03-25</created><updated>2013-03-29</updated><authors><author><keyname>M&#xfc;ller</keyname><forenames>Sebastian</forenames><affiliation>Charles University, Prague</affiliation></author></authors><title>Polylogarithmic Cuts in Models of V^0</title><categories>cs.LO</categories><comments>16 pages</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 1 (April 1,
  2013) lmcs:1123</journal-ref><doi>10.2168/LMCS-9(1:16)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study initial cuts of models of weak two-sorted Bounded Arithmetics with
respect to the strength of their theories and show that these theories are
stronger than the original one. More explicitly we will see that
polylogarithmic cuts of models of $\mathbf{V}^0$ are models of $\mathbf{VNC}^1$
by formalizing a proof of Nepomnjascij's Theorem in such cuts. This is a
strengthening of a result by Paris and Wilkie. We can then exploit our result
in Proof Complexity to observe that Frege proof systems can be sub
exponentially simulated by bounded depth Frege proof systems. This result has
recently been obtained by Filmus, Pitassi and Santhanam in a direct proof. As
an interesting observation we also obtain an average case separation of
Resolution from AC0-Frege by applying a recent result with Tzameret.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6076</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6076</id><created>2013-03-25</created><authors><author><keyname>Wu</keyname><forenames>Yu</forenames></author><author><keyname>Wu</keyname><forenames>Chuan</forenames></author><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Lau</keyname><forenames>Francis C. M.</forenames></author></authors><title>vSkyConf: Cloud-assisted Multi-party Mobile Video Conferencing</title><categories>cs.NI</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an important application in the busy world today, mobile video
conferencing facilitates virtual face-to-face communication with friends,
families and colleagues, via their mobile devices on the move. However, how to
provision high-quality, multi-party video conferencing experiences over mobile
devices is still an open challenge. The fundamental reason behind is the lack
of computation and communication capacities on the mobile devices, to scale to
large conferencing sessions. In this paper, we present vSkyConf, a
cloud-assisted mobile video conferencing system to fundamentally improve the
quality and scale of multi-party mobile video conferencing. By novelly
employing a surrogate virtual machine in the cloud for each mobile user, we
allow fully scalable communication among the conference participants via their
surrogates, rather than directly. The surrogates exchange conferencing streams
among each other, transcode the streams to the most appropriate bit rates, and
buffer the streams for the most efficient delivery to the mobile recipients. A
fully decentralized, optimal algorithm is designed to decide the best paths of
streams and the most suitable surrogates for video transcoding along the paths,
such that the limited bandwidth is fully utilized to deliver streams of the
highest possible quality to the mobile recipients. We also carefully tailor a
buffering mechanism on each surrogate to cooperate with optimal stream
distribution. We have implemented vSkyConf based on Amazon EC2 and verified the
excellent performance of our design, as compared to the widely adopted unicast
solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6086</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6086</id><created>2013-03-25</created><authors><author><keyname>Argyriou</keyname><forenames>Andreas</forenames></author><author><keyname>Baldassarre</keyname><forenames>Luca</forenames></author><author><keyname>Micchelli</keyname><forenames>Charles A.</forenames></author><author><keyname>Pontil</keyname><forenames>Massimiliano</forenames></author></authors><title>On Sparsity Inducing Regularization Methods for Machine Learning</title><categories>cs.LG stat.ML</categories><comments>12 pages. arXiv admin note: text overlap with arXiv:1104.1436</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the past years there has been an explosion of interest in learning
methods based on sparsity regularization. In this paper, we discuss a general
class of such methods, in which the regularizer can be expressed as the
composition of a convex function $\omega$ with a linear function. This setting
includes several methods such the group Lasso, the Fused Lasso, multi-task
learning and many more. We present a general approach for solving
regularization problems of this kind, under the assumption that the proximity
operator of the function $\omega$ is available. Furthermore, we comment on the
application of this approach to support vector machines, a technique pioneered
by the groundbreaking work of Vladimir Vapnik.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6088</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6088</id><created>2013-03-25</created><updated>2013-07-23</updated><authors><author><keyname>Gliwa</keyname><forenames>Bogdan</forenames></author><author><keyname>Zygmunt</keyname><forenames>Anna</forenames></author><author><keyname>Byrski</keyname><forenames>Aleksander</forenames></author></authors><title>Graphical Analysis of Social Group Dynamics</title><categories>cs.SI physics.soc-ph</categories><comments>Fourth International Conference on Computational Aspects of Social
  Networks, CASoN 2012, Sao Carlos, Brazil, November 21-23, 2012, pp. 41-46;
  IEEE Computer Society, 2012</comments><doi>10.1109/CASoN.2012.6412375</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying communities in social networks becomes an increasingly important
research problem. Several methods for identifying such groups have been
developed, however, qualitative analysis (taking into account the scale of the
problem) still poses serious problems. This paper describes a tool for
facilitating such an analysis, allowing to visualize the dynamics and
supporting localization of different events (such as creation or merging of
groups). In the final part of the paper, the experimental results performed
using the benchmark data (Enron emails) provide an insight into usefulness of
the proposed tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6091</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6091</id><created>2013-03-25</created><authors><author><keyname>Ko&#x17a;lak</keyname><forenames>Jaros&#x142;aw</forenames></author><author><keyname>Zygmunt</keyname><forenames>Anna</forenames></author></authors><title>Agent-based modelling of social organisations</title><categories>cs.SI cs.MA physics.soc-ph</categories><comments>International Conference on Complex, Intelligent and Software
  Intensive Systems, CISIS 2011, June 30 - July 2, 2011, Korean Bible
  University, Seoul, Korea, pp. 467-472</comments><journal-ref>IEEE Computer Society, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper, the model of the society represented by a social network and
the model of a multi-agent system built on the basis of this, is presented. The
particular aim of the system is to predict the evolution of a society and an
analysis of the communities that appear, their characteristic features and
reasons for coming into being. As an example of application, an analysis was
made of a social portal which makes it possible to o?er and reserve places in
rooms for travelling tourists
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6092</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6092</id><created>2013-03-25</created><authors><author><keyname>B&#xfc;rger</keyname><forenames>Mathias</forenames></author><author><keyname>Notarstefano</keyname><forenames>Giuseppe</forenames></author><author><keyname>Allg&#xf6;wer</keyname><forenames>Frank</forenames></author></authors><title>A Polyhedral Approximation Framework for Convex and Robust Distributed
  Optimization</title><categories>cs.SY cs.DC math.OC</categories><comments>submitted to IEEE Transactions on Automatic Control</comments><doi>10.1109/TAC.2013.2281883</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a general problem set-up for a wide class of convex
and robust distributed optimization problems in peer-to-peer networks. In this
set-up convex constraint sets are distributed to the network processors who
have to compute the optimizer of a linear cost function subject to the
constraints. We propose a novel fully distributed algorithm, named
cutting-plane consensus, to solve the problem, based on an outer polyhedral
approximation of the constraint sets. Processors running the algorithm compute
and exchange linear approximations of their locally feasible sets.
Independently of the number of processors in the network, each processor stores
only a small number of linear constraints, making the algorithm scalable to
large networks. The cutting-plane consensus algorithm is presented and analyzed
for the general framework. Specifically, we prove that all processors running
the algorithm agree on an optimizer of the global problem, and that the
algorithm is tolerant to node and link failures as long as network connectivity
is preserved. Then, the cutting plane consensus algorithm is specified to three
different classes of distributed optimization problems, namely (i) inequality
constrained problems, (ii) robust optimization problems, and (iii) almost
separable optimization problems with separable objective functions and coupling
constraints. For each one of these problem classes we solve a concrete problem
that can be expressed in that framework and present computational results. That
is, we show how to solve: position estimation in wireless sensor networks, a
distributed robust linear program and, a distributed microgrid control problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6094</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6094</id><created>2013-03-25</created><authors><author><keyname>Ko&#x17a;lak</keyname><forenames>Jaros&#x142;aw</forenames></author><author><keyname>Zygmunt</keyname><forenames>Anna</forenames></author><author><keyname>Nawarecki</keyname><forenames>Edward</forenames></author></authors><title>Modelling and analysing relations between entities using the
  multi-agents and social network approaches</title><categories>cs.SI cs.MA physics.soc-ph</categories><journal-ref>IEEE 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, the concept of a system for analysing social relations between
entities using the social network analysis and multi-agent system approaches is
presented. The following problems especially appear within the domain of our
interests: identification of the most influential individuals in a given
society, identification of roles played by the given individuals in that
society and the recognition of groups of individuals strongly connected with
one another. For the analysis of these problems, two application domains are
selected: an analysis of data regarding phone calls and analysis of Internet
Weblogs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6106</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6106</id><created>2013-03-25</created><authors><author><keyname>Zygmunt</keyname><forenames>Anna</forenames></author><author><keyname>Ko&#x17a;lak</keyname><forenames>Jaros&#x142;aw</forenames></author><author><keyname>Siwik</keyname><forenames>Leszek</forenames></author></authors><title>Agent-based environment for knowledge integration</title><categories>cs.MA</categories><comments>Computational Science - ICCS 2009, 9th International Conference,
  Baton Rouge, LA, USA, May 25-27, 2009, Proceedings, Part II, pp. 885-894</comments><journal-ref>ICCS 2009, LNCS 5545, 2009</journal-ref><doi>10.1007/978-3-642-01973-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representing knowledge with the use of ontology description languages offers
several advantages arising from knowledge reusability, possibilities of
carrying out reasoning processes and the use of existing concepts of knowledge
integration. In this work we are going to present an environment for the
integration of knowledge expressed in such a way. Guaranteeing knowledge
integration is an important element during the development of the Semantic Web.
Thanks to this, it is possible to obtain access to services which offer
knowledge contained in various distributed databases associated with
semantically described web portals. We will present the advantages of the
multi-agent approach while solving this problem. Then, we will describe an
example of its application in systems supporting company management knowledge
in the process of constructing supply-chains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6107</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6107</id><created>2013-03-25</created><authors><author><keyname>Narodytska</keyname><forenames>Nina</forenames><affiliation>NICTA and UNSW Sydney Australia</affiliation></author><author><keyname>Skocovsky</keyname><forenames>Peter</forenames><affiliation>Universidade Nova de Lisboa Portugal</affiliation></author><author><keyname>Walsh</keyname><forenames>Toby</forenames><affiliation>NICTA and UNSW Sydney Australia</affiliation></author></authors><title>Global SPACING Constraint (Technical Report)</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new global SPACING constraint that is useful in modeling events
that are distributed over time, like learning units scheduled over a study
program or repeated patterns in music compositions. First, we investigate
theoretical properties of the constraint and identify tractable special cases.
We propose efficient DC filtering algorithms for these cases. Then, we
experimentally evaluate performance of the proposed algorithms on a music
composition problem and demonstrate that our filtering algorithms outperform
the state-of-the-art approach for solving this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6120</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6120</id><created>2013-03-25</created><authors><author><keyname>Naimi</keyname><forenames>Yaghoob</forenames></author><author><keyname>Naimi</keyname><forenames>Mohammad</forenames></author></authors><title>Reliability and efficiency of generalized rumor spreading model on
  complex social networks</title><categories>physics.soc-ph cs.SI</categories><comments>11 pages, 7 figures, Accepted for publication in Communications in
  Theoretical Physics (CTP). arXiv admin note: text overlap with
  arXiv:cond-mat/0312131, arXiv:0807.1458, arXiv:physics/0609124 by other
  authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the generalized rumor spreading model and investigate some
properties of this model on different complex social networks. Despite pervious
rumor models that both the spreader-spreader ($SS$) and the spreader-stifler
($SR$) interactions have the same rate $\alpha$, we define $\alpha^{(1)}$ and
$\alpha^{(2)}$ for $SS$ and $SR$ interactions, respectively. The effect of
variation of $\alpha^{(1)}$ and $\alpha^{(2)}$ on the final density of stiflers
is investigated. Furthermore, the influence of the topological structure of the
network in rumor spreading is studied by analyzing the behavior of several
global parameters such as reliability and efficiency. Our results show that
while networks with homogeneous connectivity patterns reach a higher
reliability, scale-free topologies need a less time to reach a steady state
with respect the rumor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6127</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6127</id><created>2013-03-25</created><authors><author><keyname>Buchin</keyname><forenames>Kevin</forenames></author><author><keyname>Buchin</keyname><forenames>Maike</forenames></author><author><keyname>van Kreveld</keyname><forenames>Marc</forenames></author><author><keyname>Speckmann</keyname><forenames>Bettina</forenames></author><author><keyname>Staals</keyname><forenames>Frank</forenames></author></authors><title>Trajectory Grouping Structures</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The collective motion of a set of moving entities like people, birds, or
other animals, is characterized by groups arising, merging, splitting, and
ending. Given the trajectories of these entities, we define and model a
structure that captures all of such changes using the Reeb graph, a concept
from topology. The trajectory grouping structure has three natural parameters
that allow more global views of the data in group size, group duration, and
entity inter-distance. We prove complexity bounds on the maximum number of
maximal groups that can be present, and give algorithms to compute the grouping
structure efficiently. We also study how the trajectory grouping structure can
be made robust, that is, how brief interruptions of groups can be disregarded
in the global structure, adding a notion of persistence to the structure.
Furthermore, we showcase the results of experiments using data generated by the
NetLogo flocking model and from the Starkey project. The Starkey data describe
the movement of elk, deer, and cattle. Although there is no ground truth for
the grouping structure in this data, the experiments show that the trajectory
grouping structure is plausible and has the desired effects when changing the
essential parameters. Our research provides the first complete study of
trajectory group evolvement, including combinatorial, algorithmic, and
experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6129</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6129</id><created>2013-03-25</created><authors><author><keyname>Salehi</keyname><forenames>&#xd6;zlem</forenames></author><author><keyname>Yakary\ilmaz</keyname><forenames>Abuzer</forenames></author><author><keyname>Say</keyname><forenames>A. C. Cem</forenames></author></authors><title>Real-Time Vector Automata</title><categories>cs.FL cs.CC</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational power of real-time finite automata that have been
augmented with a vector of dimension k, and programmed to multiply this vector
at each step by an appropriately selected $k \times k$ matrix. Only one entry
of the vector can be tested for equality to 1 at any time. Classes of languages
recognized by deterministic, nondeterministic, and &quot;blind&quot; versions of these
machines are studied and compared with each other, and the associated classes
for multicounter automata, automata with multiplication, and generalized finite
automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6135</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6135</id><created>2013-03-25</created><authors><author><keyname>Pankiewicz</keyname><forenames>Pawel Jerzy</forenames></author><author><keyname>Arildsen</keyname><forenames>Thomas</forenames></author><author><keyname>Larsen</keyname><forenames>Torben</forenames></author></authors><title>Model-Based Calibration of Filter Imperfections in the Random
  Demodulator for Compressive Sensing</title><categories>cs.IT math.IT</categories><comments>10 pages, 8 figures, submitted to IEEE Transactions on Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The random demodulator is a recent compressive sensing architecture providing
efficient sub-Nyquist sampling of sparse band-limited signals. The compressive
sensing paradigm requires an accurate model of the analog front-end to enable
correct signal reconstruction in the digital domain. In practice, hardware
devices such as filters deviate from their desired design behavior due to
component variations. Existing reconstruction algorithms are sensitive to such
deviations, which fall into the more general category of measurement matrix
perturbations. This paper proposes a model-based technique that aims to
calibrate filter model mismatches to facilitate improved signal reconstruction
quality. The mismatch is considered to be an additive error in the discretized
impulse response. We identify the error by sampling a known calibrating signal,
enabling least-squares estimation of the impulse response error. The error
estimate and the known system model are used to calibrate the measurement
matrix. Numerical analysis demonstrates the effectiveness of the calibration
method even for highly deviating low-pass filter responses. The proposed method
performance is also compared to a state of the art method based on discrete
Fourier transform trigonometric interpolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6138</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6138</id><created>2013-03-25</created><authors><author><keyname>Monakhov</keyname><forenames>Yuri</forenames></author><author><keyname>Kostina</keyname><forenames>Natalia</forenames></author><author><keyname>Medvednikova</keyname><forenames>Maria</forenames></author><author><keyname>Makarov</keyname><forenames>Oleg</forenames></author><author><keyname>Semenova</keyname><forenames>Irina</forenames></author></authors><title>About the survey of propagandistic messages in contemporary social media</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>2 pages</comments><acm-class>H.1.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the research results that have identified a set of
characteristic parameters of propagandistic messages. Later these parameters
can be used in the algorithm creating special user-oriented propagandistic
messages to improve distribution and assimilation of information by users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6145</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6145</id><created>2013-03-25</created><updated>2013-08-08</updated><authors><author><keyname>Schmitt</keyname><forenames>Manuel</forenames></author><author><keyname>Wanka</keyname><forenames>Rolf</forenames></author></authors><title>Particles Prefer Walking Along the Axes: Experimental Insights into the
  Behavior of a Particle Swarm</title><categories>cs.NE cs.AI</categories><comments>Full version of poster on Genetic and Evolutionary Computation
  Conference (GECCO) 13</comments><acm-class>I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Particle swarm optimization (PSO) is a widely used nature-inspired
meta-heuristic for solving continuous optimization problems. However, when
running the PSO algorithm, one encounters the phenomenon of so-called
stagnation, that means in our context, the whole swarm starts to converge to a
solution that is not (even a local) optimum. The goal of this work is to point
out possible reasons why the swarm stagnates at these non-optimal points. To
achieve our results, we use the newly defined potential of a swarm. The total
potential has a portion for every dimension of the search space, and it drops
when the swarm approaches the point of convergence. As it turns out
experimentally, the swarm is very likely to come sometimes into &quot;unbalanced&quot;
states, i. e., almost all potential belongs to one axis. Therefore, the swarm
becomes blind for improvements still possible in any other direction. Finally,
we show how in the light of the potential and these observations, a slightly
adapted PSO rebalances the potential and therefore increases the quality of the
solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6149</identifier>
 <datestamp>2014-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6149</id><created>2013-03-25</created><updated>2014-03-16</updated><authors><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author></authors><title>Adaptivity of averaged stochastic gradient descent to local strong
  convexity for logistic regression</title><categories>math.ST cs.LG math.OC stat.TH</categories><proxy>ccsd</proxy><journal-ref>Journal of Machine Learning Research 15 (2014) 595-627</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider supervised learning problems such as logistic
regression and study the stochastic gradient method with averaging, in the
usual stochastic approximation setting where observations are used only once.
We show that after $N$ iterations, with a constant step-size proportional to
$1/R^2 \sqrt{N}$ where $N$ is the number of observations and $R$ is the maximum
norm of the observations, the convergence rate is always of order
$O(1/\sqrt{N})$, and improves to $O(R^2 / \mu N)$ where $\mu$ is the lowest
eigenvalue of the Hessian at the global optimum (when this eigenvalue is
greater than $R^2/\sqrt{N}$). Since $\mu$ does not need to be known in advance,
this shows that averaged stochastic gradient is adaptive to \emph{unknown
local} strong convexity of the objective function. Our proof relies on the
generalized self-concordance properties of the logistic loss and thus extends
to all generalized linear models with uniformly bounded features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6163</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6163</id><created>2013-03-25</created><updated>2013-07-23</updated><authors><author><keyname>Nunez-Iglesias</keyname><forenames>Juan</forenames></author><author><keyname>Kennedy</keyname><forenames>Ryan</forenames></author><author><keyname>Parag</keyname><forenames>Toufiq</forenames></author><author><keyname>Shi</keyname><forenames>Jianbo</forenames></author><author><keyname>Chklovskii</keyname><forenames>Dmitri B.</forenames></author></authors><title>Machine learning of hierarchical clustering to segment 2D and 3D images</title><categories>cs.CV cs.LG</categories><comments>15 pages, 8 figures</comments><journal-ref>PLoS ONE, 2013, 8(8): e71715</journal-ref><doi>10.1371/journal.pone.0071715</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We aim to improve segmentation through the use of machine learning tools
during region agglomeration. We propose an active learning approach for
performing hierarchical agglomerative segmentation from superpixels. Our method
combines multiple features at all scales of the agglomerative process, works
for data with an arbitrary number of dimensions, and scales to very large
datasets. We advocate the use of variation of information to measure
segmentation accuracy, particularly in 3D electron microscopy (EM) images of
neural tissue, and using this metric demonstrate an improvement over competing
algorithms in EM and natural images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6166</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6166</id><created>2013-03-25</created><updated>2014-03-04</updated><authors><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author><author><keyname>Martinez</keyname><forenames>Alfonso</forenames></author><author><keyname>F&#xe0;bregas</keyname><forenames>Albert Guill&#xe9;n i</forenames></author></authors><title>Mismatched Decoding: Error Exponents, Second-Order Rates and Saddlepoint
  Approximations</title><categories>cs.IT math.IT</categories><comments>Accepted to IEEE Transactions on Information Theory. (v2) Major
  revisions made, Saddlepoint Approximation section extended significantly,
  title changed (v3) Final version uploaded</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of channel coding with a given (possibly
suboptimal) maximum-metric decoding rule. A cost-constrained random-coding
ensemble with multiple auxiliary costs is introduced, and is shown to achieve
error exponents and second-order coding rates matching those of
constant-composition random coding, while being directly applicable to channels
with infinite or continuous alphabets. The number of auxiliary costs required
to match the error exponents and second-order rates of constant-composition
coding is studied, and is shown to be at most two. For i.i.d. random coding,
asymptotic estimates of two well-known non-asymptotic bounds are given using
saddlepoint approximations. Each expression is shown to characterize the
asymptotic behavior of the corresponding random-coding bound at both fixed and
varying rates, thus unifying the regimes characterized by error exponents,
second-order rates and moderate deviations. For fixed rates, novel exact
asymptotics expressions are obtained to within a multiplicative 1+o(1) term.
Using numerical examples, it is shown that the saddlepoint approximations are
highly accurate even at short block lengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6167</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6167</id><created>2013-03-25</created><updated>2014-11-16</updated><authors><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author><author><keyname>Martinez</keyname><forenames>Alfonso</forenames></author><author><keyname>F&#xe0;bregas</keyname><forenames>Albert Guill&#xe9;n i</forenames></author></authors><title>Second-Order Rate Region of Constant-Composition Codes for the
  Multiple-Access Channel</title><categories>cs.IT math.IT</categories><comments>(v2) Results/proofs given in matrix notation, det(V)=0 handled more
  rigorously, Berry-Esseen derivation given. (v3) Gaussian case added (v4)
  Significant change of presentation; added local dispersion results; added new
  method to obtain non-standard tangent vector terms using coded time-sharing;
  (v5) Final version (IEEE Transactions on Information Theory)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the second-order asymptotics of coding rates for the
discrete memoryless multiple-access channel with a fixed target error
probability. Using constant-composition random coding, coded time-sharing, and
a variant of Hoeffding's combinatorial central limit theorem, an inner bound on
the set of locally achievable second-order coding rates is given for each point
on the boundary of the capacity region. It is shown that the inner bound for
constant-composition random coding includes that recovered by i.i.d. random
coding, and that the inclusion may be strict. The inner bound is extended to
the Gaussian multiple-access channel via an increasingly fine quantization of
the inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6170</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6170</id><created>2013-03-25</created><authors><author><keyname>Jones</keyname><forenames>Brandon</forenames></author><author><keyname>Campbell</keyname><forenames>Mark</forenames></author><author><keyname>Tong</keyname><forenames>Lang</forenames></author></authors><title>Maximum Likelihood Fusion of Stochastic Maps</title><categories>stat.AP cs.RO</categories><comments>10 pages, 8 figures, submitted to IEEE Transactions on Signal
  Processing on 24-March-2013</comments><doi>10.1109/TSP.2014.2304435</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fusion of independently obtained stochastic maps by collaborating mobile
agents is considered. The proposed approach includes two parts: matching of
stochastic maps and maximum likelihood alignment. In particular, an affine
invariant hypergraph is constructed for each stochastic map, and a bipartite
matching via a linear program is used to establish landmark correspondence
between stochastic maps. A maximum likelihood alignment procedure is proposed
to determine rotation and translation between common landmarks in order to
construct a global map within a common frame of reference. A main feature of
the proposed approach is its scalability with respect to the number of
landmarks: the matching step has polynomial complexity and the maximum
likelihood alignment is obtained in closed form. Experimental validation of the
proposed fusion approach is performed using the Victoria Park benchmark
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6175</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6175</id><created>2013-03-25</created><authors><author><keyname>Ferrer-i-Cancho</keyname><forenames>R.</forenames></author><author><keyname>Hern&#xe1;ndez-Fern&#xe1;ndez</keyname><forenames>A.</forenames></author><author><keyname>Lusseau</keyname><forenames>D.</forenames></author><author><keyname>Agoramoorthy</keyname><forenames>G.</forenames></author><author><keyname>Hsu</keyname><forenames>M. J.</forenames></author><author><keyname>Semple</keyname><forenames>S.</forenames></author></authors><title>Compression as a universal principle of animal behavior</title><categories>q-bio.NC cs.CL cs.IT math.IT physics.data-an q-bio.QM</categories><comments>This is the pre-proofed version. The published version will be
  available at
  http://onlinelibrary.wiley.com/journal/10.1111/%28ISSN%291551-6709</comments><doi>10.1111/cogs.12061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key aim in biology and psychology is to identify fundamental principles
underpinning the behavior of animals, including humans. Analyses of human
language and the behavior of a range of non-human animal species have provided
evidence for a common pattern underlying diverse behavioral phenomena: words
follow Zipf's law of brevity (the tendency of more frequently used words to be
shorter), and conformity to this general pattern has been seen in the behavior
of a number of other animals. It has been argued that the presence of this law
is a sign of efficient coding in the information theoretic sense. However, no
strong direct connection has been demonstrated between the law and compression,
the information theoretic principle of minimizing the expected length of a
code. Here we show that minimizing the expected code length implies that the
length of a word cannot increase as its frequency increases. Furthermore, we
show that the mean code length or duration is significantly small in human
language, and also in the behavior of other species in all cases where
agreement with the law of brevity has been found. We argue that compression is
a general principle of animal behavior, that reflects selection for efficiency
of coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6186</identifier>
 <datestamp>2013-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6186</id><created>2013-03-25</created><updated>2013-10-31</updated><authors><author><keyname>Griebl</keyname><forenames>Ludwig</forenames></author><author><keyname>Schuster</keyname><forenames>Johann</forenames></author></authors><title>Some notes on the abstraction operation for Multi-Terminal Binary
  Decision Diagrams</title><categories>cs.LO</categories><comments>Added link to springer.com</comments><msc-class>68P05</msc-class><doi>10.1007/s10703-013-0198-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The starting point of this work are inaccurate statements found in the
literature for Multi-terminal Binary Decision Diagrams (MTBDDs) regarding the
well-definedness of the MTBDD abstraction operation. The statements try to
relate an operation * on a set of terminal values M to the property that the
abstraction over this operation does depend on the order of the abstracted
variables. This paper gives a necessary and sufficient condition for the
independence of the abstraction operation of the order of the abstracted
variables in the case of an underlying monoid and it treats the more general
setting of a magma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6200</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6200</id><created>2013-03-25</created><authors><author><keyname>Cao</keyname><forenames>Zhigang</forenames></author><author><keyname>Chen</keyname><forenames>Xujin</forenames></author><author><keyname>Wang</keyname><forenames>Changjun</forenames></author></authors><title>How to Schedule the Marketing of Products with Negative Externalities</title><categories>cs.GT cs.DS</categories><comments>16 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In marketing products with negative externalities, a schedule which specifies
an order of consumer purchase decisions is crucial, since in the social network
of consumers, the decision of each consumer is negatively affected by the
choices of her neighbors. In this paper, we study the problems of finding a
marketing schedule for two asymmetric products with negative externalites. The
goals are two-fold: maximizing the sale of one product and ensuring regret-free
purchase decisions. We show that the maximization is NP-hard, and provide
efficient algorithms with satisfactory performance guarantees. Two of these
algorithms give regret-proof schedules, i.e. they reach Nash equilibria where
no consumers regret their previous decisions. Our work is the first attempt to
address these marketing problems from an algorithmic point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6224</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6224</id><created>2013-03-25</created><authors><author><keyname>Rossi</keyname><forenames>Wilbert Samuel</forenames></author><author><keyname>Frasca</keyname><forenames>Paolo</forenames></author><author><keyname>Fagnani</keyname><forenames>Fabio</forenames></author></authors><title>Limited benefit of cooperation in distributed relative localization</title><categories>cs.SY math.OC</categories><comments>11 pages, 2 figures, submitted to conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Important applications in robotic and sensor networks require distributed
algorithms to solve the so-called relative localization problem: a node-indexed
vector has to be reconstructed from measurements of differences between
neighbor nodes. In a recent note, we have studied the estimation error of a
popular gradient descent algorithm showing that the mean square error has a
minimum at a finite time, after which the performance worsens. This paper
proposes a suitable modification of this algorithm incorporating more realistic
&quot;a priori&quot; information on the position. The new algorithm presents a
performance monotonically decreasing to the optimal one. Furthermore, we show
that the optimal performance is approximated, up to a 1 + \eps factor, within a
time which is independent of the graph and of the number of nodes. This
convergence time is very much related to the minimum exhibited by the previous
algorithm and both lead to the following conclusion: in the presence of noisy
data, cooperation is only useful till a certain limit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6241</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6241</id><created>2013-03-25</created><updated>2014-04-07</updated><authors><author><keyname>Schaub</keyname><forenames>Michael T.</forenames></author><author><keyname>Lehmann</keyname><forenames>J&#xf6;rg</forenames></author><author><keyname>Yaliraki</keyname><forenames>Sophia N.</forenames></author><author><keyname>Barahona</keyname><forenames>Mauricio</forenames></author></authors><title>Structure of complex networks: Quantifying edge-to-edge relations by
  failure-induced flow redistribution</title><categories>physics.soc-ph cs.SI</categories><comments>24 pages, 6 figures</comments><journal-ref>Network Science, 2014, 2(1), pp. 66--89</journal-ref><doi>10.1017/nws.2014.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis of complex networks has so far revolved mainly around the role
of nodes and communities of nodes. However, the dynamics of interconnected
systems is commonly focalised on edge processes, and a dual edge-centric
perspective can often prove more natural. Here we present graph-theoretical
measures to quantify edge-to-edge relations inspired by the notion of flow
redistribution induced by edge failures. Our measures, which are related to the
pseudo-inverse of the Laplacian of the network, are global and reveal the
dynamical interplay between the edges of a network, including potentially
non-local interactions. Our framework also allows us to define the embeddedness
of an edge, a measure of how strongly an edge features in the weighted cuts of
the network. We showcase the general applicability of our edge-centric
framework through analyses of the Iberian Power grid, traffic flow in road
networks, and the C. elegans neuronal network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6242</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6242</id><created>2013-03-25</created><authors><author><keyname>Tahir</keyname><forenames>M.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Ishfaq</keyname><forenames>M.</forenames></author></authors><title>EAST: Energy Efficient Adaptive Scheme for Transmission in Wireless
  Sensor Networks</title><categories>cs.NI</categories><journal-ref>26th IEEE Canadian Conference on Electrical and Computer
  Engineering (CCECE2013), Regina, Saskatchewan, Canada, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose Energy-efficient Adaptive Scheme for Transmission
(EAST) in WSNs. EAST is IEEE 802.15.4 standard compliant. In this approach,
open-loop is used for temperature-aware link quality estimation and
compensation. Whereas, closed-loop feedback helps to divide network into three
logical regions to minimize overhead of control packets on basis of Threshold
transmitter power loss (RSSIloss) for each region and current number of
neighbor nodes that help to adapt transmit power according to link quality
changes due to temperature variation. Simulation results show that propose
scheme; EAST effectively adapts transmission power to changing link quality
with less control packets overhead and energy consumption compared to classical
approach with single region in which maximum transmitter power assigned to
compensate temperature variation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6249</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6249</id><created>2013-03-25</created><updated>2014-02-18</updated><authors><author><keyname>Campo</keyname><forenames>Adri&#xe0; Tauste</forenames></author><author><keyname>Vazquez-Vilar</keyname><forenames>Gonzalo</forenames></author><author><keyname>F&#xe0;bregas</keyname><forenames>Albert Guill&#xe9;n i</forenames></author><author><keyname>Koch</keyname><forenames>Tobias</forenames></author><author><keyname>Martinez</keyname><forenames>Alfonso</forenames></author></authors><title>A Derivation of the Source-Channel Error Exponent using Non-identical
  Product Distributions</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the random-coding exponent of joint source-channel coding
for a scheme where source messages are assigned to disjoint subsets (referred
to as classes), and codewords are independently generated according to a
distribution that depends on the class index of the source message. For
discrete memoryless systems, two optimally chosen classes and product
distributions are found to be sufficient to attain the sphere-packing exponent
in those cases where it is tight.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6257</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6257</id><created>2013-03-25</created><updated>2014-06-10</updated><authors><author><keyname>Karney</keyname><forenames>Charles F. F.</forenames></author></authors><title>Sampling exactly from the normal distribution</title><categories>physics.comp-ph cs.MS math.PR</categories><comments>LaTeX, 8 pages, 1 figure. Revision includes algorithm for sampling
  discrete normal distribution. An implementation of the algorithms is
  available at http://exrandom.sf.net</comments><acm-class>G.3</acm-class><journal-ref>ACM Trans. Mathematical Software 42(1), 3:1-14 (Jan. 2016)</journal-ref><doi>10.1145/2710016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm for sampling exactly from the normal distribution is given. The
algorithm reads some number of uniformly distributed random digits in a given
base and generates an initial portion of the representation of a normal deviate
in the same base. Thereafter, uniform random digits are copied directly into
the representation of the normal deviate. Thus, in contrast to existing
methods, it is possible to generate normal deviates exactly rounded to any
precision with a mean cost that scales linearly in the precision. The method
performs no extended precision arithmetic, calls no transcendental functions,
and, indeed, uses no floating point arithmetic whatsoever; it uses only simple
integer operations. It can easily be adapted to sample exactly from the
discrete normal distribution whose parameters are rational numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6260</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6260</id><created>2013-03-25</created><authors><author><keyname>Rasheed</keyname><forenames>M. B.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Ishfaq</keyname><forenames>M.</forenames></author></authors><title>E-HORM: An Energy-efficient Hole Removing Mechanism in Wireless Senor
  Networks</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1303.5365</comments><journal-ref>26th IEEE Canadian Conference on Electrical and Computer
  Engineering (CCECE2013), Regina, Saskatchewan, Canada, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cluster based routing protocols forWireless Sensor Networks (WSNs) have been
widely used for better performance in terms of energy efficiency. Efficient use
of energy is challenging task of designing these protocols. Energy holes are
created due to quickly drain the energy of a few nodes due to nonuniform node
distribution in the network. Normally, energy holes make the data routing
failure when nodes transmit data back to the sink. We propose
Energy-efficientHOle Removing Mechanism (E-HORM) technique to remove energy
holes. In this technique, we use sleep and awake mechanism for sensor nodes to
save energy. This approach finds the maximum distance nodes to calculate the
maximum energy for data transmission. We consider it as a threshold energy Eth.
Every node first checks its energy level for data transmission. If the energy
level of node is less than Eth, it cannot transmit data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6271</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6271</id><created>2013-03-23</created><authors><author><keyname>Kunegis</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Blattner</keyname><forenames>Marcel</forenames></author><author><keyname>Moser</keyname><forenames>Christine</forenames></author></authors><title>Preferential Attachment in Online Networks: Measurement and Explanations</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>10 pages, 5 figures, Accepted for the WebSci'13 Conference, Paris,
  2013</comments><acm-class>H.4.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We perform an empirical study of the preferential attachment phenomenon in
temporal networks and show that on the Web, networks follow a nonlinear
preferential attachment model in which the exponent depends on the type of
network considered. The classical preferential attachment model for networks by
Barab\'asi and Albert (1999) assumes a linear relationship between the number
of neighbors of a node in a network and the probability of attachment. Although
this assumption is widely made in Web Science and related fields, the
underlying linearity is rarely measured. To fill this gap, this paper performs
an empirical longitudinal (time-based) study on forty-seven diverse Web network
datasets from seven network categories and including directed, undirected and
bipartite networks. We show that contrary to the usual assumption, preferential
attachment is nonlinear in the networks under consideration. Furthermore, we
observe that the deviation from linearity is dependent on the type of network,
giving sublinear attachment in certain types of networks, and superlinear
attachment in others. Thus, we introduce the preferential attachment exponent
$\beta$ as a novel numerical network measure that can be used to discriminate
different types of networks. We propose explanations for the behavior of that
network measure, based on the mechanisms that underly the growth of the network
in question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6296</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6296</id><created>2013-03-25</created><authors><author><keyname>D&#xfc;ck</keyname><forenames>Natalia</forenames></author><author><keyname>Zimmermann</keyname><forenames>Karl-Heinz</forenames></author></authors><title>A Variant of the Gr\&quot;obner Basis Algorithm for Computing Hilbert Bases</title><categories>math.AG cs.SC math.AC</categories><comments>11 pages</comments><msc-class>13P10 (Primary), 94B05 (Secondary)</msc-class><journal-ref>IJPAM, Vol. 81, 2012, 145-155</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gr\&quot;obner bases can be used for computing the Hilbert basis of a numerical
submonoid. By using these techniques, we provide an algorithm that calculates a
basis of a subspace of a finite-dimensional vector space over a finite prime
field given as a matrix kernel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6310</identifier>
 <datestamp>2013-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6310</id><created>2013-03-25</created><updated>2013-06-05</updated><authors><author><keyname>Fister</keyname><forenames>Iztok</forenames><suffix>Jr.</suffix></author><author><keyname>Fister</keyname><forenames>Du&#x161;an</forenames></author><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author></authors><title>A hybrid bat algorithm</title><categories>cs.NE</categories><comments>Electrotechnical review, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Swarm intelligence is a very powerful technique to be used for optimization
purposes. In this paper we present a new swarm intelligence algorithm, based on
the bat algorithm. The Bat algorithm is hybridized with differential evolution
strategies. Besides showing very promising results of the standard benchmark
functions, this hybridization also significantly improves the original bat
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6311</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6311</id><created>2013-03-25</created><authors><author><keyname>Valeyev</keyname><forenames>Rustem</forenames></author></authors><title>Principle &quot;synthesis&quot; for the solution of tasks of class NP</title><categories>cs.OH</categories><comments>7 pages, 6 figures</comments><msc-class>68.11Y16</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Initial contours of the non-standard approach to reception of the answer of
any task on discrete structures are considered: the algorithm independently
creates such answer from separate fragments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6314</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6314</id><created>2013-03-25</created><updated>2013-11-28</updated><authors><author><keyname>Zemanov&#xe1;</keyname><forenames>Alena</forenames></author><author><keyname>Zeman</keyname><forenames>Jan</forenames></author><author><keyname>&#x160;ejnoha</keyname><forenames>Michal</forenames></author></authors><title>Numerical model of elastic laminated glass beams under finite strain</title><categories>cs.CE</categories><comments>Moderate revisions; 17 pages, 6 figures, 8 tables</comments><journal-ref>Archives of Civil and Mechanical Engineering, 14 (4), 734--744,
  (2014)</journal-ref><doi>10.1016/j.acme.2014.03.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Laminated glass structures are formed by stiff layers of glass connected with
a compliant plastic interlayer. Due to their slenderness and heterogeneity,
they exhibit a complex mechanical response that is difficult to capture by
single-layer models even in the elastic range. The purpose of this paper is to
introduce an efficient and reliable finite element approach to the simulation
of the immediate response of laminated glass beams. It proceeds from a refined
plate theory due to Mau (1973), as we treat each layer independently and
enforce the compatibility by the Lagrange multipliers. At the layer level, we
adopt the finite-strain shear deformable formulation of Reissner (1972) and the
numerical framework by Ibrahimbegovi\'{c} and Frey (1993). The resulting system
is solved by the Newton method with consistent linearization. By comparing the
model predictions against available experimental data, analytical methods and
two-dimensional finite element simulations, we demonstrate that the proposed
formulation is reliable and provides accuracy comparable to the detailed
two-dimensional finite element analyzes. As such, it offers a convenient basis
to incorporate more refined constitutive description of the interlayer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6323</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6323</id><created>2013-03-25</created><authors><author><keyname>Bulut</keyname><forenames>Eyuphan</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author></authors><title>Constructing Limited Scale-Free Topologies Over Peer-to-Peer Networks</title><categories>cs.NI</categories><comments>10 pages</comments><journal-ref>IEEE Transactions on Parallel and Distributed Systems, vol. 24,
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Overlay network topology together with peer/data organization and search
algorithm are the crucial components of unstructured peer-to-peer (P2P)
networks as they directly affect the efficiency of search on such networks.
Scale-free (powerlaw) overlay network topologies are among structures that
offer high performance for these networks. A key problem for these topologies
is the existence of hubs, nodes with high connectivity. Yet, the peers in a
typical unstructured P2P network may not be willing or able to cope with such
high connectivity and its associated load. Therefore, some hard cutoffs are
often imposed on the number of edges that each peer can have, restricting
feasible overlays to limited or truncated scale-free networks. In this paper,
we analyze the growth of such limited scale-free networks and propose two
different algorithms for constructing perfect scale-free overlay network
topologies at each instance of such growth. Our algorithms allow the user to
define the desired scalefree exponent (gamma). They also induce low
communication overhead when network grows from one size to another. Using
extensive simulations, we demonstrate that these algorithms indeed generate
perfect scale free networks (at each step of network growth) that provide
better search efficiency in various search algorithms than the networks
generated by the existing solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6361</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6361</id><created>2013-03-25</created><authors><author><keyname>Mau</keyname><forenames>Sandra</forenames></author><author><keyname>Chen</keyname><forenames>Shaokang</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>Video Face Matching using Subset Selection and Clustering of
  Probabilistic Multi-Region Histograms</title><categories>cs.CV cs.IR</categories><acm-class>I.5.4; I.4.7; I.5.3; G.3</acm-class><journal-ref>International Conference of Image and Vision Computing New Zealand
  (IVCNZ), 2010</journal-ref><doi>10.1109/IVCNZ.2010.6148860</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Balancing computational efficiency with recognition accuracy is one of the
major challenges in real-world video-based face recognition. A significant
design decision for any such system is whether to process and use all possible
faces detected over the video frames, or whether to select only a few &quot;best&quot;
faces. This paper presents a video face recognition system based on
probabilistic Multi-Region Histograms to characterise performance trade-offs
in: (i) selecting a subset of faces compared to using all faces, and (ii)
combining information from all faces via clustering. Three face selection
metrics are evaluated for choosing a subset: face detection confidence, random
subset, and sequential selection. Experiments on the recently introduced MOBIO
dataset indicate that the usage of all faces through clustering always
outperformed selecting only a subset of faces. The experiments also show that
the face selection metric based on face detection confidence generally provides
better recognition performance than random or sequential sampling. Moreover,
the optimal number of faces varies drastically across selection metric and
subsets of MOBIO. Given the trade-offs between computational effort,
recognition accuracy and robustness, it is recommended that face feature
clustering would be most advantageous in batch processing (particularly for
video-based watchlists), whereas face selection methods should be limited to
applications with significant computational restrictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6369</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6369</id><created>2013-03-25</created><authors><author><keyname>Zhang</keyname><forenames>Qian-Ming</forenames></author><author><keyname>Zeng</keyname><forenames>An</forenames></author><author><keyname>Shang</keyname><forenames>Ming-Sheng</forenames></author></authors><title>Extracting the information backbone in online system</title><categories>cs.IR cs.SI physics.soc-ph</categories><doi>10.1371/journal.pone.0062624</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information overload is a serious problem in modern society and many
solutions such as recommender system have been proposed to filter out
irrelevant information. In the literature, researchers mainly dedicated to
improve the recommendation performance (accuracy and diversity) of the
algorithms while overlooked the influence of topology of the online user-object
bipartite networks. In this paper, we find that some information provided by
the bipartite networks is not only redundant but also misleading. With such
&quot;less can be more&quot; feature, we design some algorithms to improve the
recommendation performance by eliminating some links from the original
networks. Moreover, we propose a hybrid method combining the time-aware and
topology-aware link removal algorithms to extract the backbone which contains
the essential information for the recommender systems. From the practical point
of view, our method can improve the performance and reduce the computational
time of the recommendation system, thus improve both of their effectiveness and
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6370</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6370</id><created>2013-03-25</created><authors><author><keyname>Tomioka</keyname><forenames>Ryota</forenames></author><author><keyname>Suzuki</keyname><forenames>Taiji</forenames></author></authors><title>Convex Tensor Decomposition via Structured Schatten Norm Regularization</title><categories>stat.ML cs.LG cs.NA</categories><comments>12 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss structured Schatten norms for tensor decomposition that includes
two recently proposed norms (&quot;overlapped&quot; and &quot;latent&quot;) for
convex-optimization-based tensor decomposition, and connect tensor
decomposition with wider literature on structured sparsity. Based on the
properties of the structured Schatten norms, we mathematically analyze the
performance of &quot;latent&quot; approach for tensor decomposition, which was
empirically found to perform better than the &quot;overlapped&quot; approach in some
settings. We show theoretically that this is indeed the case. In particular,
when the unknown true tensor is low-rank in a specific mode, this approach
performs as good as knowing the mode with the smallest rank. Along the way, we
show a novel duality result for structures Schatten norms, establish the
consistency, and discuss the identifiability of this approach. We confirm
through numerical simulations that our theoretical prediction can precisely
predict the scaling behavior of the mean squared error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6372</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6372</id><created>2013-03-25</created><authors><author><keyname>Merritt</keyname><forenames>Sears</forenames></author><author><keyname>Jacobs</keyname><forenames>Abigail Z.</forenames></author><author><keyname>Mason</keyname><forenames>Winter</forenames></author><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author></authors><title>Detecting Friendship Within Dynamic Online Interaction Networks</title><categories>cs.SI cs.CY cs.HC physics.soc-ph</categories><comments>To Appear at the 7th International AAAI Conference on Weblogs and
  Social Media (ICWSM '13), 11 pages, 1 table, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many complex social systems, the timing and frequency of interactions
between individuals are observable but friendship ties are hidden. Recovering
these hidden ties, particularly for casual users who are relatively less
active, would enable a wide variety of friendship-aware applications in domains
where labeled data are often unavailable, including online advertising and
national security. Here, we investigate the accuracy of multiple statistical
features, based either purely on temporal interaction patterns or on the
cooperative nature of the interactions, for automatically extracting latent
social ties. Using self-reported friendship and non-friendship labels derived
from an anonymous online survey, we learn highly accurate predictors for
recovering hidden friendships within a massive online data set encompassing 18
billion interactions among 17 million individuals of the popular online game
Halo: Reach. We find that the accuracy of many features improves as more data
accumulates, and cooperative features are generally reliable. However,
periodicities in interaction time series are sufficient to correctly classify
95% of ties, even for casual users. These results clarify the nature of
friendship in online social environments and suggest new opportunities and new
privacy concerns for friendship-aware applications that do not require the
disclosure of private friendship information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6377</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6377</id><created>2013-03-25</created><authors><author><keyname>Gelbaum</keyname><forenames>Zachary</forenames></author><author><keyname>Titus</keyname><forenames>Mathew</forenames></author></authors><title>Simulation of Fractional Brownian Surfaces via Spectral Synthesis on
  Manifolds</title><categories>cs.CG cs.CV math.PR</categories><doi>10.1109/TIP.2014.2348793</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the spectral decomposition of the Laplace-Beltrami operator we simulate
fractal surfaces as random series of eigenfunctions. This approach allows us to
generate random fields over smooth manifolds of arbitrary dimension,
generalizing previous work with fractional Brownian motion with
multi-dimensional parameter. We give examples of surfaces with and without
boundary and discuss implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6378</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6378</id><created>2013-03-25</created><authors><author><keyname>Sun</keyname><forenames>Yuhua</forenames></author><author><keyname>Yan</keyname><forenames>Tongjiang</forenames></author><author><keyname>Li</keyname><forenames>Hui</forenames></author></authors><title>Cyclic code from the first class Whiteman's generalized cyclotomic
  sequence with order 4</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic codes are a subclass of linear codes and have important applications
in data storage systems, and communication systems because of their e?cient
encoding and decoding algorithms. Employing the First Class Whiteman's
generalized cyclotomic sequences with order 4, we describe several cyclic codes
over the ?nite ?eld GF(q) and give their generator polynomials. Additionally,
Lower bounds on the minimum weight of these cyclic codes are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6385</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6385</id><created>2013-03-26</created><updated>2013-04-18</updated><authors><author><keyname>Singhal</keyname><forenames>Ayush</forenames></author><author><keyname>Subbian</keyname><forenames>Karthik</forenames></author><author><keyname>Srivastava</keyname><forenames>Jaideep</forenames></author><author><keyname>Kolda</keyname><forenames>Tamara G.</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author></authors><title>Dynamics of Trust Reciprocation in Heterogenous MMOG Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Understanding the dynamics of reciprocation is of great interest in sociology
and computational social science. The recent growth of Massively Multi-player
Online Games (MMOGs) has provided unprecedented access to large-scale data
which enables us to study such complex human behavior in a more systematic
manner. In this paper, we consider three different networks in the EverQuest2
game: chat, trade, and trust. The chat network has the highest level of
reciprocation (33%) because there are essentially no barriers to it. The trade
network has a lower rate of reciprocation (27%) because it has the obvious
barrier of requiring more goods or money for exchange; morever, there is no
clear benefit to returning a trade link except in terms of social connections.
The trust network has the lowest reciprocation (14%) because this equates to
sharing certain within-game assets such as weapons, and so there is a high
barrier for such connections because they require faith in the players that are
granted such high access. In general, we observe that reciprocation rate is
inversely related to the barrier level in these networks. We also note that
reciprocation has connections across the heterogeneous networks. Our
experiments indicate that players make use of the medium-barrier reciprocations
to strengthen a relationship. We hypothesize that lower-barrier interactions
are an important component to predicting higher-barrier ones. We verify our
hypothesis using predictive models for trust reciprocations using features from
trade interactions. Using the number of trades (both before and after the
initial trust link) boosts our ability to predict if the trust will be
reciprocated up to 11% with respect to the AUC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6387</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6387</id><created>2013-03-26</created><authors><author><keyname>Wen</keyname><forenames>Chao-Kai</forenames></author><author><keyname>Chen</keyname><forenames>Jung-Chieh</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author><author><keyname>Ting</keyname><forenames>Pangan</forenames></author></authors><title>Message Passing Algorithm for Distributed Downlink Regularized
  Zero-forcing Beamforming with Cooperative Base Stations</title><categories>cs.IT math.IT</categories><comments>20 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Base station (BS) cooperation can turn unwanted interference to useful signal
energy for enhancing system performance. In the cooperative downlink,
zero-forcing beamforming (ZFBF) with a simple scheduler is well known to obtain
nearly the performance of the capacity-achieving dirty-paper coding. However,
the centralized ZFBF approach is prohibitively complex as the network size
grows. In this paper, we devise message passing algorithms for realizing the
regularized ZFBF (RZFBF) in a distributed manner using belief propagation. In
the proposed methods, the overall computational cost is decomposed into many
smaller computation tasks carried out by groups of neighboring BSs and
communications is only required between neighboring BSs. More importantly, some
exchanged messages can be computed based on channel statistics rather than
instantaneous channel state information, leading to significant reduction in
computational complexity. Simulation results demonstrate that the proposed
algorithms converge quickly to the exact RZFBF and much faster compared to
conventional methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6388</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6388</id><created>2013-03-26</created><updated>2013-04-08</updated><authors><author><keyname>Kang</keyname><forenames>Jaewook</forenames></author><author><keyname>Lee</keyname><forenames>Heung-No</forenames></author><author><keyname>Kim</keyname><forenames>Kiseon</forenames></author></authors><title>Phase Transition Analysis of Sparse Support Detection from Noisy
  Measurements</title><categories>cs.IT math.IT</categories><comments>7 pages, 5 figures, submitted to IEEE transaction on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of sparse support detection (SSD) via a
detection-oriented algorithm named Bayesian hypothesis test via belief
propagation (BHT-BP). Our main focus is to compare BHT-BP to an
estimation-based algorithm, called CS-BP, and show its superiority in the SSD
problem. For this investigation, we perform a phase transition (PT) analysis
over the plain of the noise level and signal magnitude on the signal support.
This PT analysis sharply specifies the required signal magnitude for the
detection under a certain noise level. In addition, we provide an experimental
validation to assure the PT analysis. Our analytical and experimental results
show the fact that BHT-BP detects the signal support against additive noise
more robustly than CS-BP does.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6390</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6390</id><created>2013-03-26</created><updated>2013-03-27</updated><authors><author><keyname>Blaschko</keyname><forenames>Matthew</forenames><affiliation>INRIA Saclay - Ile de France, CVN</affiliation></author></authors><title>A Note on k-support Norm Regularized Risk Minimization</title><categories>cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The k-support norm has been recently introduced to perform correlated
sparsity regularization. Although Argyriou et al. only reported experiments
using squared loss, here we apply it to several other commonly used settings
resulting in novel machine learning algorithms with interesting and familiar
limit cases. Source code for the algorithms described here is available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6397</identifier>
 <datestamp>2014-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6397</id><created>2013-03-26</created><authors><author><keyname>Ugrinovskii</keyname><forenames>V.</forenames></author></authors><title>Conditions for detectability in distributed consensus-based observer
  networks</title><categories>cs.SY</categories><comments>Accepted for publication in the IEEE Transactions on Automatic
  Control</comments><journal-ref>IEEE Transactions on Automatic Control, 58, pp.2659-2664, 2013</journal-ref><doi>10.1109/TAC.2013.2256675</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper discusses fundamental detectability properties associated with the
problem of distributed state estimation using networked observers. The main
result of the paper establishes connections between detectability of the plant
through measurements, observability of the node filters through
interconnections, and algebraic properties of the underlying communication
graph, to ensure the interconnected filtering error dynamics are stabilizable
via output injection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6409</identifier>
 <datestamp>2013-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6409</id><created>2013-03-26</created><updated>2013-04-17</updated><authors><author><keyname>Geiger</keyname><forenames>Bernhard C.</forenames></author><author><keyname>Kubin</keyname><forenames>Gernot</forenames></author></authors><title>Information Measures for Deterministic Input-Output Systems</title><categories>cs.IT math.IT</categories><comments>23 pages, 12 figures; submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work the information loss in deterministic, memoryless systems is
investigated by evaluating the conditional entropy of the input random variable
given the output random variable. It is shown that for a large class of systems
the information loss is finite, even if the input is continuously distributed.
Based on this finiteness, the problem of perfectly reconstructing the input is
addressed and Fano-type bounds between the information loss and the
reconstruction error probability are derived.
  For systems with infinite information loss a relative measure is defined and
shown to be tightly related to R\'{e}nyi information dimension. Employing
another Fano-type argument, the reconstruction error probability is bounded by
the relative information loss from below.
  In view of developing a system theory from an information-theoretic
point-of-view, the theoretical results are illustrated by a few example
systems, among them a multi-channel autocorrelation receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6422</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6422</id><created>2013-03-26</created><updated>2013-11-27</updated><authors><author><keyname>Benedetti</keyname><forenames>Bruno</forenames></author><author><keyname>Lutz</keyname><forenames>Frank H.</forenames></author></authors><title>Random Discrete Morse Theory and a New Library of Triangulations</title><categories>cs.CG math.AT math.CO math.GT</categories><comments>35 pages, 5 figures, 7 tables</comments><report-no>CPH-SYM-DNRF92</report-no><msc-class>57Q15, 57Q05, 57M25, 57N10, 57N13, 52B70, 52B05, 52B22, 55N35</msc-class><journal-ref>Experimental Mathematics, Vol. 23, Issue 1 (2014), 66-94</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  1) We introduce random discrete Morse theory as a computational scheme to
measure the complicatedness of a triangulation. The idea is to try to quantify
the frequence of discrete Morse matchings with a certain number of critical
cells. Our measure will depend on the topology of the space, but also on how
nicely the space is triangulated.
  (2) The scheme we propose looks for optimal discrete Morse functions with an
elementary random heuristic. Despite its na\&quot;ivet\'e, this approach turns out
to be very successful even in the case of huge inputs.
  (3) In our view the existing libraries of examples in computational topology
are `too easy' for testing algorithms based on discrete Morse theory. We
propose a new library containing more complicated (and thus more meaningful)
test examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6424</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6424</id><created>2013-03-26</created><authors><author><keyname>M&#xfc;ller</keyname><forenames>Julian-Steffen</forenames></author><author><keyname>Vollmer</keyname><forenames>Heribert</forenames></author></authors><title>Model Checking for Modal Dependence Logic: An Approach Through Post's
  Lattice</title><categories>cs.CC cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate an extended version of modal dependence logic by
allowing arbitrary Boolean connectives. Modal dependence logic was recently
introduced by Jouko V\&quot;a\&quot;an\&quot;anen by extending modal logic by a the dependence
atom dep(.). In this paper we study the computational complexity of the model
checking problem. For a complete classification of arbitrary Boolean functions
we are using a Lattice approach introduced by Emil Post. This classification is
done for all fragments of the logical language allowing modalities $\Diamond$
and $\Box$, the dependence atom, and logical symbols for arbitrary Boolean
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6428</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6428</id><created>2013-03-26</created><updated>2015-04-22</updated><authors><author><keyname>Pavlovic</keyname><forenames>Dusko</forenames></author></authors><title>Bicompletions of distance matrices</title><categories>cs.LO math.CT math.MG</categories><comments>20 pages, 5 figures; appeared in Springer LNCS vol 7860 in 2013; v2
  fixes an error in Sec. 2.3, noticed by Toshiki Kataoka</comments><msc-class>18D20, 68Q55</msc-class><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the practice of information extraction, the input data are usually
arranged into pattern matrices, and analyzed by the methods of linear algebra
and statistics, such as principal component analysis. In some applications, the
tacit assumptions of these methods lead to wrong results. The usual reason is
that the matrix composition of linear algebra presents information as flowing
in waves, whereas it sometimes flows in particles, which seek the shortest
paths. This wave-particle duality in computation and information processing has
been originally observed by Abramsky. In this paper we pursue a particle view
of information, formalized in *distance spaces*, which generalize metric
spaces, but are slightly less general than Lawvere's *generalized metric
spaces*. In this framework, the task of extracting the 'principal components'
from a given matrix of data boils down to a bicompletio}, in the sense of
enriched category theory. We describe the bicompletion construction for
distance matrices. The practical goal that motivates this research is to
develop a method to estimate the hardness of attack constructions in security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6437</identifier>
 <datestamp>2013-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6437</id><created>2013-03-26</created><updated>2013-06-10</updated><authors><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author><author><keyname>Lampis</keyname><forenames>Michael</forenames></author><author><keyname>Schmied</keyname><forenames>Richard</forenames></author></authors><title>New Inapproximability Bounds for TSP</title><categories>cs.CC cs.DM cs.DS math.CO math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the approximability of the metric Traveling Salesman
Problem (TSP) and prove new explicit inapproximability bounds for that problem.
The best up to now known hardness of approximation bounds were 185/184 for the
symmetric case (due to Lampis) and 117/116 for the asymmetric case (due to
Papadimitriou and Vempala). We construct here two new bounded occurrence CSP
reductions which improve these bounds to 123/122 and 75/74, respectively. The
latter bound is the first improvement in more than a decade for the case of the
asymmetric TSP. One of our main tools, which may be of independent interest, is
a new construction of a bounded degree wheel amplifier used in the proof of our
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6453</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6453</id><created>2013-03-26</created><authors><author><keyname>Fern&#xe1;ndez</keyname><forenames>Ariel</forenames></author><author><keyname>Soltys</keyname><forenames>Michael</forenames></author></authors><title>Feasible combinatorial matrix theory</title><categories>cs.LO math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the well-known Konig's Min-Max Theorem (KMM), a fundamental
result in combinatorial matrix theory, can be proven in the first order theory
$\LA$ with induction restricted to $\Sigma_1^B$ formulas. This is an
improvement over the standard textbook proof of KMM which requires $\Pi_2^B$
induction, and hence does not yield feasible proofs --- while our new approach
does. $\LA$ is a weak theory that essentially captures the ring properties of
matrices; however, equipped with $\Sigma_1^B$ induction $\LA$ is capable of
proving KMM, and a host of other combinatorial properties such as Menger's,
Hall's and Dilworth's Theorems. Therefore, our result formalizes Min-Max type
of reasoning within a feasible framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6454</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6454</id><created>2013-03-26</created><authors><author><keyname>Kugiumtzis</keyname><forenames>Dimitris</forenames></author></authors><title>Partial Transfer Entropy on Rank Vectors</title><categories>stat.ME cs.IT math.IT nlin.CD physics.data-an</categories><comments>21 pages, 6 figures, 3 tables, accepted in EPJ/ST</comments><doi>10.1140/epjst/e2013-01849-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the evaluation of information flow in bivariate time series, information
measures have been employed, such as the transfer entropy (TE), the symbolic
transfer entropy (STE), defined similarly to TE but on the ranks of the
components of the reconstructed vectors, and the transfer entropy on rank
vectors (TERV), similar to STE but forming the ranks for the future samples of
the response system with regard to the current reconstructed vector. Here we
extend TERV for multivariate time series, and account for the presence of
confounding variables, called partial transfer entropy on ranks (PTERV). We
investigate the asymptotic properties of PTERV, and also partial STE (PSTE),
construct parametric significance tests under approximations with Gaussian and
gamma null distributions, and show that the parametric tests cannot achieve the
power of the randomization test using time-shifted surrogates. Using
simulations on known coupled dynamical systems and applying parametric and
randomization significance tests, we show that PTERV performs better than PSTE
but worse than the partial transfer entropy (PTE). However, PTERV, unlike PTE,
is robust to the presence of drifts in the time series and it is also not
affected by the level of detrending.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6455</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6455</id><created>2013-03-26</created><authors><author><keyname>Yu</keyname><forenames>Shaode</forenames></author><author><keyname>Zhu</keyname><forenames>Qingsong</forenames></author><author><keyname>Wu</keyname><forenames>Shibin</forenames></author><author><keyname>Xie</keyname><forenames>Yaoqin</forenames></author></authors><title>Performance Evaluation of Edge-Directed Interpolation Methods for Images</title><categories>cs.CV</categories><comments>9 pages, 5 figures, 2 tables</comments><acm-class>I.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many interpolation methods have been developed for high visual quality, but
fail for inability to preserve image structures. Edges carry heavy structural
information for detection, determination and classification. Edge-adaptive
interpolation approaches become a center of focus. In this paper, performance
of four edge-directed interpolation methods comparing with two traditional
methods is evaluated on two groups of images. These methods include new
edge-directed interpolation (NEDI), edge-guided image interpolation (EGII),
iterative curvature-based interpolation (ICBI), directional cubic convolution
interpolation (DCCI) and two traditional approaches, bi-linear and bi-cubic.
Meanwhile, no parameters are mentioned to measure edge-preserving ability of
edge-adaptive interpolation approaches and we proposed two. One evaluates
accuracy and the other measures robustness of edge-preservation ability.
Performance evaluation is based on six parameters. Objective assessment and
visual analysis are illustrated and conclusions are drawn from theoretical
backgrounds and practical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6460</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6460</id><created>2013-03-26</created><updated>2013-07-01</updated><authors><author><keyname>Brown</keyname><forenames>Chlo&#xeb;</forenames></author><author><keyname>Nicosia</keyname><forenames>Vincenzo</forenames></author><author><keyname>Scellato</keyname><forenames>Salvatore</forenames></author><author><keyname>Noulas</keyname><forenames>Anastasios</forenames></author><author><keyname>Mascolo</keyname><forenames>Cecilia</forenames></author></authors><title>Social and place-focused communities in location-based online social
  networks</title><categories>physics.soc-ph cs.SI</categories><comments>11 pages, 5 figures</comments><journal-ref>C. Brown, V. Nicosia, S. Scellato, A. Noulas, C. Mascolo &quot;Social
  and place-focused communities in location-based online social networks&quot;, Eur.
  Phys. J. B 86 (6), 290 (2013)</journal-ref><doi>10.1140/epjb/e2013-40253-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thanks to widely available, cheap Internet access and the ubiquity of
smartphones, millions of people around the world now use online location-based
social networking services. Understanding the structural properties of these
systems and their dependence upon users' habits and mobility has many potential
applications, including resource recommendation and link prediction. Here, we
construct and characterise social and place-focused graphs by using
longitudinal information about declared social relationships and about users'
visits to physical places collected from a popular online location-based social
service. We show that although the social and place-focused graphs are
constructed from the same data set, they have quite different structural
properties. We find that the social and location-focused graphs have different
global and meso-scale structure, and in particular that social and
place-focused communities have negligible overlap. Consequently, group
inference based on community detection performed on the social graph alone
fails to isolate place-focused groups, even though these do exist in the
network. By studying the evolution of tie structure within communities, we show
that the time period over which location data are aggregated has a substantial
impact on the stability of place-focused communities, and that information
about place-based groups may be more useful for user-centric applications than
that obtained from the analysis of social communities alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6481</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6481</id><created>2013-03-26</created><authors><author><keyname>Gog</keyname><forenames>Simon</forenames></author><author><keyname>Moffat</keyname><forenames>Alistair</forenames></author><author><keyname>Culpepper</keyname><forenames>J. Shane</forenames></author><author><keyname>Turpin</keyname><forenames>Andrew</forenames></author><author><keyname>Wirth</keyname><forenames>Anthony</forenames></author></authors><title>Large-Scale Pattern Search Using Reduced-Space On-Disk Suffix Arrays</title><categories>cs.DS</categories><acm-class>H.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The suffix array is an efficient data structure for in-memory pattern search.
Suffix arrays can also be used for external-memory pattern search, via
two-level structures that use an internal index to identify the correct block
of suffix pointers. In this paper we describe a new two-level suffix
array-based index structure that requires significantly less disk space than
previous approaches. Key to the saving is the use of disk blocks that are based
on prefixes rather than the more usual uniform-sampling approach, allowing
reductions between blocks and subparts of other blocks. We also describe a new
in-memory structure based on a condensed BWT string, and show that it allows
common patterns to be resolved without access to the text. Experiments using 64
GB of English web text and a laptop computer with just 4 GB of main memory
demonstrate the speed and versatility of the new approach. For this data the
index is around one- third the size of previous two-level mechanisms; and the
memory footprint of as little as 1% of the text size means that queries can be
processed more quickly than is possible with a compact FM-INDEX.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6485</identifier>
 <datestamp>2014-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6485</id><created>2013-03-26</created><updated>2013-08-27</updated><authors><author><keyname>Pallister</keyname><forenames>James</forenames></author><author><keyname>Hollis</keyname><forenames>Simon</forenames></author><author><keyname>Bennett</keyname><forenames>Jeremy</forenames></author></authors><title>Identifying Compiler Options to Minimise Energy Consumption for Embedded
  Platforms</title><categories>cs.PF</categories><comments>14 pages, 7 figures</comments><doi>10.1093/comjnl/bxt129</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an analysis of the energy consumption of an extensive
number of the optimisations a modern compiler can perform. Using GCC as a test
case, we evaluate a set of ten carefully selected benchmarks for five different
embedded platforms.
  A fractional factorial design is used to systematically explore the large
optimisation space (2^82 possible combinations), whilst still accurately
determining the effects of optimisations and optimisation combinations.
Hardware power measurements on each platform are taken to ensure all
architectural effects on the energy consumption are captured.
  We show that fractional factorial design can find more optimal combinations
than relying on built in compiler settings. We explore the relationship between
run-time and energy consumption, and identify scenarios where they are and are
not correlated.
  A further conclusion of this study is the structure of the benchmark has a
larger effect than the hardware architecture on whether the optimisation will
be effective, and that no single optimisation is universally beneficial for
execution time or energy consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6493</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6493</id><created>2013-03-26</created><authors><author><keyname>Boissonnat</keyname><forenames>Jean-Daniel</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Dyer</keyname><forenames>Ramsay</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Ghosh</keyname><forenames>Arijit</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author></authors><title>Constructing Intrinsic Delaunay Triangulations of Submanifolds</title><categories>cs.CG</categories><proxy>ccsd</proxy><report-no>RR-8273</report-no><journal-ref>N&amp;deg; RR-8273 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an algorithm to construct an intrinsic Delaunay triangulation of
a smooth closed submanifold of Euclidean space. Using results established in a
companion paper on the stability of Delaunay triangulations on $\delta$-generic
point sets, we establish sampling criteria which ensure that the intrinsic
Delaunay complex coincides with the restricted Delaunay complex and also with
the recently introduced tangential Delaunay complex. The algorithm generates a
point set that meets the required criteria while the tangential complex is
being constructed. In this way the computation of geodesic distances is
avoided, the runtime is only linearly dependent on the ambient dimension, and
the Delaunay complexes are guaranteed to be triangulations of the manifold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6512</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6512</id><created>2013-03-26</created><authors><author><keyname>Thukral</keyname><forenames>Ruchika</forenames></author><author><keyname>Goel</keyname><forenames>Anita</forenames></author></authors><title>Web Service Interface for Data Collection</title><categories>cs.CY</categories><comments>6 pages, 3 figures, International Journal in computer Science and
  issues</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 3, No 3, May 2012, IJCSI-9-3-3-525-530</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data collection is a key component of an information system. The widespread
penetration of ICT tools in organizations and institutions has resulted in a
shift in the way the data is collected. Data may be collected in printed-form,
by e-mails, on a compact disk, or, by direct upload on the management
information system. Since web services are platform-independent, it can access
data stored in the XML format from any platform. In this paper, we present an
interface which uses web services for data collection. It requires interaction
between a web service deployed for the purposes of data collection, and the web
address where the data is stored. Our interface requires that the web service
has pre-knowledge of the address from where the data is to be collected. Also,
the data to be accessed must be stored in XML format. Since our interface uses
computer-supported interaction on both sides, it eases the task of regular and
ongoing data collection. We apply our framework to the Education Management
Information System, which collects data from schools spread across the country.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6518</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6518</id><created>2013-03-26</created><authors><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Khan</keyname><forenames>A. A.</forenames></author><author><keyname>Akbar</keyname><forenames>M.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author></authors><title>SRP-MS: A New Routing Protocol for Delay Tolerant Wireless Sensor
  Networks</title><categories>cs.NI</categories><comments>26th IEEE Canadian Conference on Electrical and Computer Engineering
  (CCECE2013), Regina, Saskatchewan, Canada, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sink Mobility is becoming popular due to excellent load balancing between
nodes and ultimately resulting in prolonged network lifetime and throughput. A
major challenge is to provide reliable and energy-efficient operations are to
be taken into consideration for differentmobility patterns of sink. Aim of this
paper is lifetime maximization of Delay TolerantWireless Sensor Networks (WSNs)
through the manipulation of Mobile Sink (MS) on different trajectories. We
propose Square Routing Protocol with MS (SRP-MS) based on existing SEP (Stable
Election Protocol) by making it Cluster Less (CL) and introducing sink
mobility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6541</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6541</id><created>2013-03-26</created><authors><author><keyname>Su</keyname><forenames>Kai</forenames></author><author><keyname>Zhang</keyname><forenames>Dan</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan B.</forenames></author></authors><title>Dynamic Radio Resource Management for Random Network Coding: Power
  Control and CSMA Backoff Control</title><categories>cs.NI</categories><comments>28 pages, 9 figures. Submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resource allocation in wireless networks typically occurs at PHY/MAC layers,
while random network coding (RNC) is a network layer strategy. An interesting
question is how resource allocation mechanisms can be tuned to improve RNC
performance. By means of a differential equation framework which models RNC
throughput in terms of lower layer parameters, we propose a gradient based
approach that can dynamically allocate MAC and PHY layer resources with the
goal of maximizing the minimum network coding throughput among all the
destination nodes in a RNC multicast. We exemplify this general approach with
two resource allocation problems: (i) power control to improve network coding
throughput, and (ii) CSMA mean backoff delay control to improve network coding
throughput. We design both centralized algorithms and online algorithms for
power control and CSMA backoff control. Our evaluations, including numerically
solving the differential equations in the centralized algorithm and an
event-driven simulation for the online algorithm, show that such gradient based
dynamic resource allocation yields significant throughput improvement of the
destination nodes in RNC. Further, our numerical results reveal that network
coding aware power control can regain the broadcast advantage of wireless
transmissions to improve the throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6544</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6544</id><created>2013-03-26</created><authors><author><keyname>Dasarathy</keyname><forenames>Gautam</forenames></author><author><keyname>Shah</keyname><forenames>Parikshit</forenames></author><author><keyname>Bhaskar</keyname><forenames>Badri Narayan</forenames></author><author><keyname>Nowak</keyname><forenames>Robert</forenames></author></authors><title>Sketching Sparse Matrices</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of recovering an unknown sparse p\times p
matrix X from an m\times m matrix Y=AXB^T, where A and B are known m \times p
matrices with m &lt;&lt; p.
  The main result shows that there exist constructions of the &quot;sketching&quot;
matrices A and B so that even if X has O(p) non-zeros, it can be recovered
exactly and efficiently using a convex program as long as these non-zeros are
not concentrated in any single row/column of X. Furthermore, it suffices for
the size of Y (the sketch dimension) to scale as m = O(\sqrt{# nonzeros in X}
\times log p). The results also show that the recovery is robust and stable in
the sense that if X is equal to a sparse matrix plus a perturbation, then the
convex program we propose produces an approximation with accuracy proportional
to the size of the perturbation. Unlike traditional results on sparse recovery,
where the sensing matrix produces independent measurements, our sensing
operator is highly constrained (it assumes a tensor product structure).
Therefore, proving recovery guarantees require non-standard techniques. Indeed
our approach relies on a novel result concerning tensor products of bipartite
graphs, which may be of independent interest.
  This problem is motivated by the following application, among others.
Consider a p\times n data matrix D, consisting of n observations of p
variables. Assume that the correlation matrix X:=DD^{T} is (approximately)
sparse in the sense that each of the p variables is significantly correlated
with only a few others. Our results show that these significant correlations
can be detected even if we have access to only a sketch of the data S=AD with A
\in R^{m\times p}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6555</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6555</id><created>2013-03-26</created><authors><author><keyname>Cenzer</keyname><forenames>D.</forenames></author><author><keyname>Marek</keyname><forenames>V. W.</forenames></author><author><keyname>Remmel</keyname><forenames>J. B.</forenames></author></authors><title>Index sets for Finite Normal Predicate Logic Programs</title><categories>cs.LO cs.CC</categories><comments>55 pages</comments><acm-class>F.1.3; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &lt;Q&gt;_e is the effective list of all finite predicate logic programs. &lt;T_e&gt; is
the list of recursive trees. We modify constructions of Marek, Nerode, and
Remmel [25] to construct recursive functions f and g such that for all indices
e, (i) there is a one-to-one degree preserving correspondence between the set
of stable models of Q_e and the set of infinite paths through T_{f(e)} and (ii)
there is a one-to-one degree preserving correspondence between the set of
infinite paths through T_e and the set of stable models of Q_{g(e)}. We use
these two recursive functions to reduce the problem of finding the complexity
of the index set I_P for various properties P of normal finite predicate logic
programs to the problem of computing index sets for primitive recursive trees
for which there is a large variety of results [6], [8], [16], [17], [18], [19].
We use our correspondences to determine the complexity of the index sets of all
programs and of certain special classes of finite predicate logic programs of
properties such as (i) having no stable models, (ii) having at least one stable
model, (iii) having exactly c stable models for any given positive integer c,
(iv) having only finitely many stable models, or (vi) having infinitely many
stable models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6573</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6573</id><created>2013-03-26</created><authors><author><keyname>Ahmad</keyname><forenames>A.</forenames></author><author><keyname>Latif</keyname><forenames>K.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author></authors><title>Density Controlled Divide-and-Rule Scheme for Energy Efficient Routing
  in Wireless Sensor Networks</title><categories>cs.NI</categories><comments>26th IEEE Canadian Conference on Electrical and Computer Engineering
  (CCECE2013), Regina, Saskatchewan, Canada, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cluster based routing technique is most popular routing technique in Wireless
Sensor Networks (WSNs). Due to varying need of WSN applications efficient
energy utilization in routing protocols is still a potential area of research.
In this research work we introduced a new energy efficient cluster based
routing technique. In this technique we tried to overcome the problem of
coverage hole and energy hole. In our technique we controlled these problems by
introducing density controlled uniform distribution of nodes and fixing optimum
number of Cluster Heads (CHs) in each round. Finally we verified our technique
by experimental results of MATLAB simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6609</identifier>
 <datestamp>2013-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6609</id><created>2013-03-26</created><updated>2013-12-10</updated><authors><author><keyname>LeFevre</keyname><forenames>Jeff</forenames></author><author><keyname>Sankaranarayanan</keyname><forenames>Jagan</forenames></author><author><keyname>Hacigumus</keyname><forenames>Hakan</forenames></author><author><keyname>Tatemura</keyname><forenames>Junichi</forenames></author><author><keyname>Polyzotis</keyname><forenames>Neoklis</forenames></author><author><keyname>Carey</keyname><forenames>Michael J.</forenames></author></authors><title>Exploiting Opportunistic Physical Design in Large-scale Data Analytics</title><categories>cs.DB cs.DC cs.DS</categories><comments>15 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Large-scale systems, such as MapReduce and Hadoop, perform aggressive
materialization of intermediate job results in order to support fault
tolerance. When jobs correspond to exploratory queries submitted by data
analysts, these materializations yield a large set of materialized views that
typically capture common computation among successive queries from the same
analyst, or even across queries of different analysts who test similar
hypotheses. We propose to treat these views as an opportunistic physical design
and use them for the purpose of query optimization. We develop a novel
query-rewrite algorithm that addresses the two main challenges in this context:
how to search the large space of rewrites, and how to reason about views that
contain UDFs (a common feature in large-scale data analytics). The algorithm,
which provably finds the minimum-cost rewrite, is inspired by nearest-neighbor
searches in non-metric spaces. We present an extensive experimental study on
real-world datasets with a prototype data-analytics system based on Hive. The
results demonstrate that our approach can result in dramatic performance
improvements on complex data-analysis queries, reducing total execution time by
an average of 61% and up to two orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6619</identifier>
 <datestamp>2013-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6619</id><created>2013-03-26</created><authors><author><keyname>p</keyname><forenames>Arun</forenames><suffix>V</suffix></author><author><keyname>Katiyar</keyname><forenames>S. K.</forenames></author></authors><title>An N-dimensional approach towards object based classification of
  remotely sensed imagery</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Remote sensing techniques are widely used for land cover classification and
urban analysis. The availability of high resolution remote sensing imagery
limits the level of classification accuracy attainable from pixel-based
approach. In this paper object-based classification scheme based on a
hierarchical support vector machine is introduced. By combining spatial and
spectral information, the amount of overlap between classes can be decreased;
thereby yielding higher classification accuracy and more accurate land cover
maps. We have adopted certain automatic approaches based on the advanced
techniques as Cellular automata and Genetic Algorithm for kernel and tuning
parameter selection. Performance evaluation of the proposed methodology in
comparison with the existing approaches is performed with reference to the
Bhopal city study area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6659</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6659</id><created>2013-03-26</created><updated>2015-11-24</updated><authors><author><keyname>Dumitrescu</keyname><forenames>Adrian</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>Csaba D.</forenames></author></authors><title>The traveling salesman problem for lines, balls and planes</title><categories>cs.CG math.MG</categories><comments>30 pages, 9 figures; final version to appear in ACM Transactions on
  Algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the traveling salesman problem with neighborhoods (TSPN) and
propose several new approximation algorithms. These constitute either first
approximations (for hyperplanes, lines, and balls in $\mathbb{R}^d$, for $d\geq
3$) or improvements over previous approximations achievable in comparable times
(for unit disks in the plane).
  \smallskip (I) Given a set of $n$ hyperplanes in $\mathbb{R}^d$, a TSP tour
whose length is at most $O(1)$ times the optimal can be computed in $O(n)$
time, when $d$ is constant.
  \smallskip (II) Given a set of $n$ lines in $\mathbb{R}^d$, a TSP tour whose
length is at most $O(\log^3 n)$ times the optimal can be computed in polynomial
time for all $d$.
  \smallskip (III) Given a set of $n$ unit balls in $\mathbb{R}^d$, a TSP tour
whose length is at most $O(1)$ times the optimal can be computed in polynomial
time, when $d$ is constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6672</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6672</id><created>2013-03-26</created><updated>2014-04-25</updated><authors><author><keyname>Amelunxen</keyname><forenames>Dennis</forenames></author><author><keyname>Lotz</keyname><forenames>Martin</forenames></author><author><keyname>McCoy</keyname><forenames>Michael B.</forenames></author><author><keyname>Tropp</keyname><forenames>Joel A.</forenames></author></authors><title>Living on the edge: Phase transitions in convex programs with random
  data</title><categories>cs.IT math.IT</categories><comments>First version, 26 March 2013. This version, 24 April 2014. 52 pages,
  12 figures, 3 tables</comments><msc-class>Primary: 90C25, 52A22, 60D05. Secondary: 52A20, 62C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research indicates that many convex optimization problems with random
constraints exhibit a phase transition as the number of constraints increases.
For example, this phenomenon emerges in the $\ell_1$ minimization method for
identifying a sparse vector from random linear measurements. Indeed, the
$\ell_1$ approach succeeds with high probability when the number of
measurements exceeds a threshold that depends on the sparsity level; otherwise,
it fails with high probability.
  This paper provides the first rigorous analysis that explains why phase
transitions are ubiquitous in random convex optimization problems. It also
describes tools for making reliable predictions about the quantitative aspects
of the transition, including the location and the width of the transition
region. These techniques apply to regularized linear inverse problems with
random measurements, to demixing problems under a random incoherence model, and
also to cone programs with random affine constraints.
  The applied results depend on foundational research in conic geometry. This
paper introduces a summary parameter, called the statistical dimension, that
canonically extends the dimension of a linear subspace to the class of convex
cones. The main technical result demonstrates that the sequence of intrinsic
volumes of a convex cone concentrates sharply around the statistical dimension.
This fact leads to accurate bounds on the probability that a randomly rotated
cone shares a ray with a fixed cone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6682</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6682</id><created>2013-03-26</created><authors><author><keyname>Grahne</keyname><forenames>Gosta</forenames></author><author><keyname>Onet</keyname><forenames>Adrian</forenames></author></authors><title>Anatomy of the chase</title><categories>cs.DB</categories><comments>18 pages, 2 figures</comments><acm-class>H.2.5</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  A lot of research activity has recently taken place around the chase
procedure, due to its usefulness in data integration, data exchange, query
optimization, peer data exchange and data correspondence, to mention a few. As
the chase has been investigated and further developed by a number of research
groups and authors, many variants of the chase have emerged and associated
results obtained. Due to the heterogeneous nature of the area it is frequently
difficult to verify the scope of each result. In this paper we take closer look
at recent developments, and provide additional results. Our analysis allows us
create a taxonomy of the chase variations and the properties they satisfy.
  Two of the most central problems regarding the chase is termination, and
discovery of restricted classes of sets of dependencies that guarantee
termination of the chase. The search for the restricted classes has been
motivated by a fairly recent result that shows that it is undecidable to
determine whether the chase with a given dependency set will terminate on a
given instance. There is a small dissonance here, since the quest has been for
classes of sets of dependencies guaranteeing termination of the chase on all
instances, even though the latter problem was not known to be undecidable. We
resolve the dissonance in this paper by showing that determining whether the
chase with a given set of dependencies terminates on all instances is
coRE-complete. Our reduction also gives us the aforementioned
instance-dependent RE-completeness result as a byproduct. For one of the
restricted classes, the stratified sets dependencies, we provide new complexity
results for the problem of testing whether a given set of dependencies belongs
to it. These results rectify some previous claims that have occurred in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6683</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6683</id><created>2013-03-26</created><authors><author><keyname>da Silva</keyname><forenames>Josildo Pereira</forenames></author><author><keyname>J&#xfa;nior</keyname><forenames>Ant&#xf4;nio Lopes Apolin&#xe1;rio</forenames></author><author><keyname>Giraldi</keyname><forenames>Gilson A.</forenames></author></authors><title>A Review of Dynamic NURBS Approach</title><categories>cs.CG cs.NA physics.comp-ph</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Dynamic NURBS, also called D-NURBS, is a known dynamic version of the
nonuniform rational B-spline (NURBS) which integrates free-form shape
representation and a physically-based model in a unified framework. More
recently, computer aided design (CAD) and finite element (FEM) community
realized the need to unify CAD and FEM descriptions which motivates a review of
D-NURBS concepts. Therefore, in this paper we describe D-NURBS theory in the
context of 1D shape deformations. We start with a revision of NURBS for
parametric representation of curve spaces. Then, the Lagrangian mechanics is
introduced in order to complete the theoretical background. Next, the D-NURBS
framework for 1D curve spaces is presented as well as some details about
constraints and numerical implementations. In the experimental results, we
focus on parameters choice and computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6704</identifier>
 <datestamp>2013-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6704</id><created>2013-03-26</created><updated>2013-05-02</updated><authors><author><keyname>Worrell</keyname><forenames>James</forenames></author></authors><title>Revisiting the Equivalence Problem for Finite Multitape Automata</title><categories>cs.FL</categories><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The decidability of determining equivalence of deterministic multitape
automata (or transducers) was a longstanding open problem until it was resolved
by Harju and Karhum\&quot;{a}ki in the early 1990s. Their proof of decidability
yields a co_NP upper bound, but apparently not much more is known about the
complexity of the problem. In this paper we give an alternative proof of
decidability, which follows the basic strategy of Harju and Karhumaki but
replaces their use of group theory with results on matrix algebras. From our
proof we obtain a simple randomised algorithm for deciding language equivalence
of deterministic multitape automata and, more generally, multiplicity
equivalence of nondeterministic multitape automata. The algorithm involves only
matrix exponentiation and runs in polynomial time for each fixed number of
tapes. If the two input automata are inequivalent then the algorithm outputs a
word on which they differ.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6711</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6711</id><created>2013-03-26</created><authors><author><keyname>Arun</keyname><forenames>P. V.</forenames></author><author><keyname>Katiyar</keyname><forenames>S. K.</forenames></author></authors><title>An intelligent approach towards automatic shape modeling and object
  extraction from satellite images using cellular automata based algorithm</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic feature extraction domain has witnessed the application of many
intelligent methodologies over past decade; however detection accuracy of these
approaches were limited as object geometry and contextual knowledge were not
given enough consideration. In this paper, we propose a frame work for accurate
detection of features along with automatic interpolation, and interpretation by
modeling feature shape as well as contextual knowledge using advanced
techniques such as SVRF, Cellular Neural Network, Core set, and MACA. Developed
methodology has been compared with contemporary methods using different
statistical measures. Investigations over various satellite images revealed
that considerable success was achieved with the CNN approach. CNN has been
effective in modeling different complex features effectively and complexity of
the approach has been considerably reduced using corset optimization. The
system has dynamically used spectral and spatial information for representing
contextual knowledge using CNN-prolog approach. System has been also proved to
be effective in providing intelligent interpolation and interpretation of
random features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6719</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6719</id><created>2013-03-26</created><authors><author><keyname>Ohlsson</keyname><forenames>Henrik</forenames></author><author><keyname>Ratliff</keyname><forenames>Lillian</forenames></author><author><keyname>Dong</keyname><forenames>Roy</forenames></author><author><keyname>Sastry</keyname><forenames>S. Shankar</forenames></author></authors><title>Blind Identification of ARX Models with Piecewise Constant Inputs</title><categories>cs.SY</categories><comments>Submitted to the 52nd IEEE Conference on Decision and Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blind system identification is known to be a hard ill-posed problem and
without further assumptions, no unique solution is at hand. In this
contribution, we are concerned with the task of identifying an ARX model from
only output measurements. Driven by the task of identifying systems that are
turned on and off at unknown times, we seek a piecewise constant input and a
corresponding ARX model which approximates the measured outputs. We phrase this
as a rank minimization problem and present a relaxed convex formulation to
approximate its solution. The proposed method was developed to model power
consumption of electrical appliances and is now a part of a bigger energy
disaggregation framework. Code will be made available online.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6729</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6729</id><created>2013-03-26</created><updated>2013-03-28</updated><authors><author><keyname>Cai</keyname><forenames>Jin-Yi</forenames></author><author><keyname>Gorenstein</keyname><forenames>Aaron</forenames></author></authors><title>Matchgates Revisited</title><categories>cs.CC</categories><comments>28 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a collection of concepts and theorems that laid the foundation of
matchgate computation. This includes the signature theory of planar matchgates,
and the parallel theory of characters of not necessarily planar matchgates. Our
aim is to present a unified and, whenever possible, simplified account of this
challenging theory. Our results include: (1) A direct proof that Matchgate
Identities (MGI) are necessary and sufficient conditions for matchgate
signatures. This proof is self-contained and does not go through the character
theory. More importantly it rectifies a gap in the existing proof. (2) A proof
that Matchgate Identities already imply the Parity Condition. (3) A simplified
construction of a crossover gadget. This is used in the proof of sufficiency of
MGI for matchgate signatures. This is also used to give a proof of equivalence
between the signature theory and the character theory which permits omittable
nodes. (4) A direct construction of matchgates realizing all
matchgate-realizable symmetric signatures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6746</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6746</id><created>2013-03-27</created><updated>2013-11-11</updated><authors><author><keyname>Hoffman</keyname><forenames>Matthew W.</forenames></author><author><keyname>Shahriari</keyname><forenames>Bobak</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author></authors><title>Exploiting correlation and budget constraints in Bayesian multi-armed
  bandit optimization</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of finding the maximizer of a nonlinear smooth
function, that can only be evaluated point-wise, subject to constraints on the
number of permitted function evaluations. This problem is also known as
fixed-budget best arm identification in the multi-armed bandit literature. We
introduce a Bayesian approach for this problem and show that it empirically
outperforms both the existing frequentist counterpart and other Bayesian
optimization methods. The Bayesian approach places emphasis on detailed
modelling, including the modelling of correlations among the arms. As a result,
it can perform well in situations where the number of arms is much larger than
the number of allowed function evaluation, whereas the frequentist counterpart
is inapplicable. This feature enables us to develop and deploy practical
applications, such as automatic machine learning toolboxes. The paper presents
comprehensive comparisons of the proposed approach, Thompson sampling,
classical Bayesian optimization techniques, more recent Bayesian bandit
approaches, and state-of-the-art best arm identification methods. This is the
first comparison of many of these methods in the literature and allows us to
examine the relative merits of their different features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6750</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6750</id><created>2013-03-27</created><authors><author><keyname>Thakur</keyname><forenames>Gaurav</forenames></author></authors><title>Sequential testing over multiple stages and performance analysis of data
  fusion</title><categories>stat.ML cs.LG</categories><comments>SPIE Signal Processing, Sensor Fusion and Target Recognition XXII</comments><journal-ref>Proc. SPIE Vol 8745, 87450S (2013)</journal-ref><doi>10.1117/12.2017754</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a methodology for modeling the performance of decision-level data
fusion between different sensor configurations, implemented as part of the
JIEDDO Analytic Decision Engine (JADE). We first discuss a Bayesian network
formulation of classical probabilistic data fusion, which allows elementary
fusion structures to be stacked and analyzed efficiently. We then present an
extension of the Wald sequential test for combining the outputs of the Bayesian
network over time. We discuss an algorithm to compute its performance
statistics and illustrate the approach on some examples. This variant of the
sequential test involves multiple, distinct stages, where the evidence
accumulated from each stage is carried over into the next one, and is motivated
by a need to keep certain sensors in the network inactive unless triggered by
other sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6753</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6753</id><created>2013-03-27</created><authors><author><keyname>Abarca</keyname><forenames>Ernesto</forenames></author><author><keyname>Grassler</keyname><forenames>Johannes</forenames></author><author><keyname>Schaffrath</keyname><forenames>Gregor</forenames></author><author><keyname>Schmid</keyname><forenames>Stefan</forenames></author></authors><title>A Federated CloudNet Architecture: The PIP and the VNP Role</title><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a generic and flexible architecture to realize CloudNets: virtual
networks connecting cloud resources with resource guarantees. Our architecture
is federated and supports different (and maybe even competing) economical
roles, by providing explicit negotiation and provisioning interfaces.
Contract-based interactions and a resource description language that allows for
aggregation and abstraction, preserve the different roles' autonomy without
sacrificing flexibility. Moreover, since our CloudNet architecture is plugin
based, essentially all cloud operating systems (e.g., OpenStack) or link
technologies (e.g., VLANs, OpenFlow, VPLS) can be used within the framework.
  This paper describes two roles in more detail: The Physical Infrastructure
Providers (PIP) which own the substrate network and resources, and the Virtual
Network Providers (VNP) which can act as resource and CloudNet brokers and
resellers. Both roles are fully implemented in our wide-area prototype that
spans remote sites and resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6761</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6761</id><created>2013-03-27</created><authors><author><keyname>Wu</keyname><forenames>Qinghua</forenames></author><author><keyname>Hao</keyname><forenames>Jin-Kao</forenames></author></authors><title>Improved Lower Bounds for Sum Coloring via Clique Decomposition</title><categories>cs.DM cs.DS</categories><comments>Pre-print</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph $G = (V,E)$ with a set $V$ of vertices and a set
$E$ of edges, the minimum sum coloring problem (MSCP) is to find a legal vertex
coloring of $G$, using colors represented by natural numbers $1, 2, . . .$ such
that the total sum of the colors assigned to the vertices is minimized. This
paper describes an approach based on the decomposition of the original graph
into disjoint cliques for computing lower bounds for the MSCP. Basically, the
proposed approach identifies and removes at each extraction iteration a maximum
number of cliques of the same size (the largest possible) from the graph.
Computational experiments show that this approach is able to improve on the
current best lower bounds for 14 benchmark instances, and to prove optimality
for the first time for 4 instances. We also report lower bounds for 24 more
instances for which no such bounds are available in the literature. These new
lower bounds are useful to estimate the quality of the upper bounds obtained
with various heuristic approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6771</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6771</id><created>2013-03-27</created><authors><author><keyname>Li</keyname><forenames>Jiaming</forenames></author><author><keyname>Tang</keyname><forenames>Junhua</forenames></author><author><keyname>Krishnamachari</keyname><forenames>Bhaskar</forenames></author></authors><title>Optimal Power Allocation over Multiple Identical Gilbert-Elliott
  Channels</title><categories>cs.IT math.IT</categories><comments>10 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the fundamental problem of power allocation over multiple
Gilbert-Elliott communication channels. In a communication system with time
varying channel qualities, it is important to allocate the limited transmission
power to channels that will be in good state. However, it is very challenging
to do so because channel states are usually unknown when the power allocation
decision is made. In this paper, we derive an optimal power allocation policy
that can maximize the expected discounted number of bits transmitted over an
infinite time span by allocating the transmission power only to those channels
that are believed to be good in the coming time slot. We use the concept belief
to represent the probability that a channel will be good and derive an optimal
power allocation policy that establishes a mapping from the channel belief to
an allocation decision.
  Specifically, we first model this problem as a partially observable Markov
decision processes (POMDP), and analytically investigate the structure of the
optimal policy. Then a simple threshold-based policy is derived for a
three-channel communication system. By formulating and solving a linear
programming formulation of this power allocation problem, we further verified
the derived structure of the optimal policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6775</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6775</id><created>2013-03-27</created><updated>2013-04-09</updated><authors><author><keyname>Tu</keyname><forenames>Jinlong</forenames></author><author><keyname>Lu</keyname><forenames>Lian</forenames></author><author><keyname>Chen</keyname><forenames>Minghua</forenames></author><author><keyname>Sitaraman</keyname><forenames>Ramesh K.</forenames></author></authors><title>Dynamic Provisioning in Next-Generation Data Centers with On-site Power
  Production</title><categories>cs.DS cs.SY</categories><acm-class>F.1.2; G.1.6; I.1.2; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The critical need for clean and economical sources of energy is transforming
data centers that are primarily energy consumers to also energy producers. We
focus on minimizing the operating costs of next-generation data centers that
can jointly optimize the energy supply from on-site generators and the power
grid, and the energy demand from servers as well as power conditioning and
cooling systems. We formulate the cost minimization problem and present an
offline optimal algorithm. For &quot;on-grid&quot; data centers that use only the grid,
we devise a deterministic online algorithm that achieves the best possible
competitive ratio of $2-\alpha_{s}$, where $\alpha_{s}$ is a normalized
look-ahead window size. For &quot;hybrid&quot; data centers that have on-site power
generation in addition to the grid, we develop an online algorithm that
achieves a competitive ratio of at most \textmd{\normalsize {\small
$\frac{P_{\max} (2-\alpha_{s})}{c_{o}+c_{m}/L}
[1+2\frac{P_{\max}-c_{o}}{P_{\max}(1+\alpha_{g})}]$}}, where $\alpha_{s}$ and
$\alpha_{g}$ are normalized look-ahead window sizes, $P_{\max}$ is the maximum
grid power price, and $L$, $c_{o}$, and $c_{m}$ are parameters of an on-site
generator.
  Using extensive workload traces from Akamai with the corresponding grid power
prices, we simulate our offline and online algorithms in a realistic setting.
Our offline (resp., online) algorithm achieves a cost reduction of 25.8%
(resp., 20.7%) for a hybrid data center and 12.3% (resp., 7.3%) for an on-grid
data center. The cost reductions are quite significant and make a strong case
for a joint optimization of energy supply and energy demand in a data center. A
hybrid data center provides about 13% additional cost reduction over an on-grid
data center representing the additional cost benefits that on-site power
generation provides over using the grid alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6777</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6777</id><created>2013-03-27</created><authors><author><keyname>Angerer</keyname><forenames>Andreas</forenames></author><author><keyname>Smirra</keyname><forenames>Remi</forenames></author><author><keyname>Hoffmann</keyname><forenames>Alwin</forenames></author><author><keyname>Schierl</keyname><forenames>Andreas</forenames></author><author><keyname>Vistein</keyname><forenames>Michael</forenames></author><author><keyname>Reif</keyname><forenames>Wolfgang</forenames></author></authors><title>A Graphical Language for Real-Time Critical Robot Commands</title><categories>cs.RO cs.PL cs.SE</categories><comments>Presented at DSLRob 2012 (arXiv:cs/1302.5082)</comments><report-no>DSLRob/2012/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Industrial robotics is characterized by sophisticated mechanical components
and highly-developed real-time control algorithms. However, the efficient use
of robotic systems is very much limited by existing proprietary programming
methods. In the research project SoftRobot, a software architecture was
developed that enables the programming of complex real-time critical robot
tasks with an object-oriented general purpose language. On top of this
architecture, a graphical language was developed to ease the specification of
complex robot commands, which can then be used as part of robot application
workflows. This paper gives an overview about the design and implementation of
this graphical language and illustrates its usefulness with some examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6782</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6782</id><created>2013-03-27</created><authors><author><keyname>da Silva</keyname><forenames>Carlo Marcelo Revoredo</forenames></author><author><keyname>da Silva</keyname><forenames>Jose Lutiano Costa</forenames></author><author><keyname>Rodrigues</keyname><forenames>Ricardo Batista</forenames></author><author><keyname>Nascimento</keyname><forenames>Leandro Marques do</forenames></author><author><keyname>Garcia</keyname><forenames>Vinicius Cardoso</forenames></author></authors><title>Systematic Mapping Study On Security Threats in Cloud Computing</title><categories>cs.CR</categories><comments>10 pages</comments><journal-ref>(IJCSIS) International Journal of Computer Science and Information
  Security, Vol. 11, No. 3, March 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Today, Cloud Computing is rising strongly, presenting itself to the market by
its main service models, known as IaaS, PaaS and SaaS, that offer advantages in
operational investments by means of on-demand costs, where consumers pay by
resources used. In face of this growth, security threats also rise,
compromising the Confidentiality, Integrity and Availability of the services
provided. Our work is a Systematic Mapping where we hope to present metrics
about publications available in literature that deal with some of the seven
security threats in Cloud Computing, based in the guide entitled &quot;Top Threats
to Cloud Computing&quot; from the Cloud Security Alliance (CSA). In our research we
identified the more explored threats, distributed the results between fifteen
Security Domains and identified the types of solutions proposed for the
threats. In face of those results, we highlight the publications that are
concerned to fulfill some standard of compliance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6784</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6784</id><created>2013-03-27</created><authors><author><keyname>Clegg</keyname><forenames>Richard G.</forenames></author><author><keyname>Landa</keyname><forenames>Raul</forenames></author><author><keyname>Haddadi</keyname><forenames>Hamed</forenames></author><author><keyname>Rio</keyname><forenames>M.</forenames></author></authors><title>Measuring the likelihood of models for network evolution</title><categories>stat.AP cs.SI</categories><comments>Published in INFOCOM NetSciCom Workshop 2009.
  http://dl.acm.org/citation.cfm?id=1719896</comments><journal-ref>INFOCOM NetSciCom Workshop, Pages 272-277 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many researchers have hypothesised models which explain the evolution of the
topology of a target network. The framework described in this paper gives the
likelihood that the target network arose from the hypothesised model. This
allows rival hypothesised models to be compared for their ability to explain
the target network. A null model (of random evolution) is proposed as a
baseline for comparison. The framework also considers models made from linear
combinations of model components. A method is given for the automatic
optimisation of component weights. The framework is tested on simulated
networks with known parameters and also on real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6785</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6785</id><created>2013-03-27</created><updated>2014-04-17</updated><authors><author><keyname>Cicalese</keyname><forenames>Ferdinando</forenames></author><author><keyname>Cordasco</keyname><forenames>Gennaro</forenames></author><author><keyname>Gargano</keyname><forenames>Luisa</forenames></author><author><keyname>Milanic</keyname><forenames>M.</forenames></author><author><keyname>Vaccaro</keyname><forenames>Ugo</forenames></author></authors><title>Latency-Bounded Target Set Selection in Social Networks</title><categories>cs.DS cs.SI math.CO</categories><comments>An extended version of this paper will appear in Theoretical Computer
  Science, Elsevier. See also Proceedings of Computability in Europe 2013 (CiE
  2013), The Nature of Computation: Logic, Algorithms, Applications, Lectures
  Notes in Computer Science, Springer</comments><doi>10.1016/j.tcs.2014.02.027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by applications in sociology, economy and medicine, we study
variants of the Target Set Selection problem, first proposed by Kempe,
Kleinberg and Tardos. In our scenario one is given a graph $G=(V,E)$, integer
values $t(v)$ for each vertex $v$ (\emph{thresholds}), and the objective is to
determine a small set of vertices (\emph{target set}) that activates a given
number (or a given subset) of vertices of $G$ \emph{within} a prescribed number
of rounds. The activation process in $G$ proceeds as follows: initially, at
round 0, all vertices in the target set are activated; subsequently at each
round $r\geq 1$ every vertex of $G$ becomes activated if at least $t(v)$ of its
neighbors are already active by round $r-1$. It is known that the problem of
finding a minimum cardinality Target Set that eventually activates the whole
graph $G$ is hard to approximate to a factor better than
$O(2^{\log^{1-\epsilon}|V|})$. In this paper we give \emph{exact} polynomial
time algorithms to find minimum cardinality Target Sets in graphs of bounded
clique-width, and \emph{exact} linear time algorithms for trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6794</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6794</id><created>2013-03-27</created><authors><author><keyname>Clegg</keyname><forenames>R. G.</forenames></author><author><keyname>Landa</keyname><forenames>R.</forenames></author><author><keyname>Harder</keyname><forenames>U.</forenames></author><author><keyname>Rio</keyname><forenames>M.</forenames></author></authors><title>A likelihood based framework for assessing network evolution models
  tested on real network data</title><categories>cs.SI physics.soc-ph</categories><comments>Presented at ACM/SIMPLEX '09 Workshop, ACM/SIMPLEX '09 Proceedings of
  the 1st Annual Workshop on Simplifying Complex Network for Practitioners 2009</comments><doi>10.1145/1610304.1610306</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a statistically sound method for using likelihood to
assess potential models of network evolution. The method is tested on data from
five real networks. Data from the internet autonomous system network, from two
photo sharing sites and from a co-authorship network are tested using this
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6799</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6799</id><created>2013-03-27</created><authors><author><keyname>Bixby</keyname><forenames>Eliot</forenames></author><author><keyname>Flint</keyname><forenames>Toby</forenames></author><author><keyname>Mikl&#xf3;s</keyname><forenames>Istv&#xe1;n</forenames></author></authors><title>Proving the Pressing Game Conjecture on Linear Graphs</title><categories>cs.DM math.CO</categories><msc-class>05A05, 05Cxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pressing game on black-and-white graphs is the following: Given a graph
$G(V,E)$ with its vertices colored with black and white, any black vertex $v$
can be pressed, which has the following effect: (a) all neighbors of $v$ change
color, i.e. white neighbors become black and \emph{vice versa}, (b) all pairs
of neighbors of $v$ change connectivity, i.e. connected pairs become
unconnected, unconnected ones become connected, (c) and finally, $v$ becomes a
separated white vertex. The aim of the game is to transform $G$ into an all
white, empty graph. It is a known result that the all white empty graph is
reachable in the pressing game if each component of $G$ contains at least one
black vertex, and for a fixed graph, any successful transformation has the same
number of pressed vertices.
  The pressing game conjecture is that any successful pressing path can be
transformed into any other successful pressing path with small alterations.
Here we prove the conjecture for linear graphs. The connection to genome
rearrangement and sorting signed permutations with reversals is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6801</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6801</id><created>2013-03-27</created><authors><author><keyname>Anil</keyname><forenames>Srijan</forenames></author><author><keyname>Gupta</keyname><forenames>Manish K.</forenames></author><author><keyname>Gulliver</keyname><forenames>T. Aaron</forenames></author></authors><title>Enumerating Some Fractional Repetition Codes</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, Submitted to IEEE Netcod 2013, Code List at
  http://www.ece.uvic.ca/~agullive/manish/List.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a distributed storage systems (DSS), regenerating codes are used to
optimize bandwidth in the repair process of a failed node. To optimize other
DSS parameters such as computation and disk I/O, Distributed Replication-based
Simple Storage (Dress) Codes consisting of an inner Fractional Repetition (FR)
code and an outer MDS code are commonly used. Thus constructing FR codes is an
important research problem, and several constructions using graphs and designs
have been proposed. In this paper, we present an algorithm for constructing the
node-packet distribution matrix of FR codes and thus enumerate some FR codes up
to a given number of nodes n. We also present algorithms for constructing
regular graphs which give rise to FR codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6803</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6803</id><created>2013-03-27</created><updated>2013-06-17</updated><authors><author><keyname>Hellmuth</keyname><forenames>Marc</forenames></author><author><keyname>Imrich</keyname><forenames>Wilfried</forenames></author><author><keyname>Kupka</keyname><forenames>Tomas</forenames></author></authors><title>Partial Star Products: A Local Covering Approach for the Recognition of
  Approximate Cartesian Product Graphs</title><categories>cs.DM math.CO</categories><doi>10.1007/s11786-013-0156-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the recognition of approximate graph products
with respect to the Cartesian product. Most graphs are prime, although they can
have a rich product-like structure. The proposed algorithms are based on a
local approach that covers a graph by small subgraphs, so-called partial star
products, and then utilizes this information to derive the global factors and
an embedding of the graph under investigation into Cartesian product graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6807</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6807</id><created>2013-03-27</created><authors><author><keyname>Clegg</keyname><forenames>R. G.</forenames></author><author><keyname>Landa</keyname><forenames>R.</forenames></author><author><keyname>Griffin</keyname><forenames>D.</forenames></author><author><keyname>Mykoniati</keyname><forenames>E.</forenames></author><author><keyname>Rio</keyname><forenames>M.</forenames></author></authors><title>The performance of locality-aware topologies for peer-to-peer live
  streaming</title><categories>cs.NI</categories><comments>This is an expanded version of a paper presented at the 2008 UK
  Performance Engineering Workshop</comments><journal-ref>Clegg, R.G.; Landa, R.; Griffin, D.; Mykoniati, E.; Rio, M.:
  'Performance of locality-aware topologies for peer-to-peer live streaming',
  IET Software, 2009, 3, (6), p. 470-479</journal-ref><doi>10.1049/iet-sen.2009.0005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the effect of overlay network topology on the
performance of live streaming peer-to-peer systems. The paper focuses on the
evaluation of topologies which are aware of the delays experienced between
different peers on the network. Metrics are defined which assess the topologies
in terms of delay, bandwidth usage and resilience to peer drop-out. Several
topology creation algorithms are tested and the metrics are measured in a
simple simulation testbed. This gives an assessment of the type of gains which
might be expected from locality awareness in peer-to-peer networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6817</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6817</id><created>2013-03-27</created><authors><author><keyname>Gong</keyname><forenames>YiXi</forenames></author><author><keyname>Rossi</keyname><forenames>Dario</forenames></author><author><keyname>Leonardi</keyname><forenames>Emilio</forenames></author></authors><title>Modeling the interdependency of low-priority congestion control and
  active queue management</title><categories>cs.NI</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a negative interplay has been shown to arise when scheduling/AQM
techniques and low-priority congestion control protocols are used together:
namely, AQM resets the relative level of priority among congestion control
protocols. This work explores this issue by (i) studying a fluid model that
describes system dynamics of heterogeneous congestion control protocols
competing on a bottleneck link governed by AQM and (ii) proposing a system
level solution able to reinstate priorities among protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6837</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6837</id><created>2013-03-27</created><authors><author><keyname>Demirel</keyname><forenames>Burak</forenames></author><author><keyname>Briat</keyname><forenames>Corentin</forenames></author><author><keyname>Johansson</keyname><forenames>Mikael</forenames></author></authors><title>Deterministic and Stochastic Approaches to Supervisory Control Design
  for Networked Systems with Time-Varying Communication Delays</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a supervisory control structure for networked systems
with time-varying delays. The control structure, in which a supervisor triggers
the most appropriate controller from a multi-controller unit, aims at improving
the closed-loop performance relative to what can be obtained using a single
robust controller. Our analysis considers average dwell-time switching and is
based on a novel multiple Lyapunov-Krasovskii functional. We develop stability
conditions that can be verified by semi-definite programming, and show that the
associated state feedback synthesis problem also can be solved using convex
optimization tools. Extensions of the analysis and synthesis procedures to the
case when the evolution of the delay mode is described by a Markov chain are
also developed. Simulations on small and large-scale networked control systems
are used to illustrate the effectiveness of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6839</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6839</id><created>2013-03-27</created><authors><author><keyname>Woldeselasie</keyname><forenames>M.</forenames></author><author><keyname>Clegg</keyname><forenames>R. G.</forenames></author><author><keyname>Rio</keyname><forenames>M.</forenames></author></authors><title>Forecasting Full-Path Network Congestion Using One Bit Signalling</title><categories>cs.NI</categories><comments>Paper presented at IEEE/ICC conference 2010</comments><journal-ref>IEEE International Conference on Communications (ICC), pp.1,5,
  23-27 May 2010</journal-ref><doi>10.1109/ICC.2010.5502644</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a mechanism for packet marking called Probabilistic
Congestion Notification (PCN). This scheme makes use of the 1-bit Explicit
Congestion Notification (ECN) field in the Internet Protocol (IP) header. It
allows the source to estimate the exact level of congestion at each
intermediate queue. By knowing this, the source could take avoiding action
either by adapting its sending rate or by using alternate routes. The
estimation mechanism makes use of time series analysis both to improve the
quality of the congestion estimation and to predict, ahead of time, the
congestion level which subsequent packets will encounter.
  The proposed protocol is tested in ns-2 simulator using a background of real
Internet traffic traces. Results show that the methods can successfully
calculate the congestion at any queue along the path with low error levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6841</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6841</id><created>2013-03-27</created><authors><author><keyname>Clegg</keyname><forenames>R. G.</forenames></author><author><keyname>Landa</keyname><forenames>R.</forenames></author><author><keyname>Rio</keyname><forenames>M.</forenames></author></authors><title>Criticisms of modelling packet traffic using long-range dependence
  (extended version)</title><categories>cs.NI math.ST stat.TH</categories><comments>This is an extended version of the conference paper
  http://arxiv.org/abs/0910.0144</comments><journal-ref>Journal of Computer and System Sciences, 77(5) pp 861--868 2010</journal-ref><doi>10.1016/j.jcss.2010.08.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper criticises the notion that long-range dependence is an important
contributor to the queuing behaviour of real Internet traffic. The idea is
questioned in two different ways. Firstly, a class of models used to simulate
Internet traffic is shown to have important theoretical flaws. It is shown that
this behaviour is inconsistent with the behaviour of real traffic traces.
Secondly, the notion that long-range correlations significantly affects the
queuing performance of traffic is investigated by destroying those correlations
in real traffic traces (by reordering). It is shown that the longer ranges of
correlations are not important except in one case with an extremely high load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6849</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6849</id><created>2013-03-27</created><authors><author><keyname>Shen</keyname><forenames>Qi</forenames></author><author><keyname>Zhao</keyname><forenames>Lei</forenames></author><author><keyname>Liu</keyname><forenames>Shubin</forenames></author><author><keyname>Liao</keyname><forenames>Shengkai</forenames></author><author><keyname>Qi</keyname><forenames>Binxiang</forenames></author><author><keyname>Hu</keyname><forenames>Xueye</forenames></author><author><keyname>Peng</keyname><forenames>Chengzhi</forenames></author><author><keyname>An</keyname><forenames>Qi</forenames></author></authors><title>A Fast Improved Fat Tree Encoder for Wave Union TDC in an FPGA</title><categories>physics.ins-det cs.AR</categories><comments>Submitted to &quot;Chinese Physics C&quot;</comments><doi>10.1088/1674-1137/37/10/106102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Up to the present, the wave union method can achieve the best timing
performance in FPGA based TDC designs. However, it should be guaranteed in such
a structure that the non-thermometer code to binary code (NTH2B) encoding
process should be finished within just one system clock cycle. So the
implementation of the NTH2B encoder is quite challenging considering the high
speed requirement. Besides, the high resolution wave union TDC also demands the
encoder to convert an ultra-wide input code to a binary code. We present a fast
improved fat tree encoder (IFTE) to fulfill such requirements, in which bubble
error suppression is also integrated. With this encoder scheme, a wave union
TDC with 7.7 ps RMS and 3.8 ps effective bin size was implemented in an FPGA
from Xilinx Virtex 5 family. An encoding time of 8.33 ns was achieved for a
276-bit non-thermometer code to a 9-bit binary code conversion. We conducted a
series of tests on the oscillating period of the wave union launcher, as well
as the overall performance of the TDC; test results indicate that the IFTE
works well. In fact, in the implementation of this encoder, no manual routing
or special constrains were required; therefore, this IFTE structure could also
be further applied in other delay chain based FPGA TDCs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6859</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6859</id><created>2013-03-27</created><authors><author><keyname>Clegg</keyname><forenames>R. G.</forenames></author><author><keyname>Isam</keyname><forenames>S.</forenames></author><author><keyname>Kanaris</keyname><forenames>I.</forenames></author><author><keyname>Darwazeh</keyname><forenames>I.</forenames></author></authors><title>A practical system for improved efficiency in frequency division
  multiplexed wireless networks</title><categories>cs.NI cs.IT math.IT</categories><comments>24 pages, 6 figures</comments><journal-ref>IET Communications, Volume 6, issue 4, 2012, p. 449-457</journal-ref><doi>10.1049/iet-com.2011.0365</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral efficiency is a key design issue for all wireless communication
systems. Orthogonal frequency division multiplexing (OFDM) is a very well-known
technique for efficient data transmission over many carriers overlapped in
frequency. Recently, several papers have appeared which describe spectrally
efficient variations of multi-carrier systems where the condition of
orthogonality is dropped. Proposed techniques suffer from two weaknesses:
Firstly, the complexity of generating the signal is increased. Secondly, the
signal detection is computationally demanding. Known methods suffer either
unusably high complexity or high error rates because of the inter-carrier
interference. This work addresses both problems by proposing new transmitter
and receiver arch itectures whose design is based on using the simplification
that a rational Spectrally Efficient Frequency Division Multiplexing (SEFDM)
system can be treated as a set of overlapped and interleaving OFDM systems.
  The efficacy of the proposed designs is shown through detailed simulation of
sys tems with different signal types and carrier dimensions. The decoder is
heuristic but in practice produces very good results which are close to the
theoretical best performance in a variety of settings. The system is able to
produce efficiency gains of up to 20% with negligible impact on the required
signal to noise ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6862</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6862</id><created>2013-03-27</created><authors><author><keyname>Krupski</keyname><forenames>Vladimir</forenames></author></authors><title>Primal implication as encryption</title><categories>cs.LO</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a &quot;cryptographic&quot; interpretation for the propositional connectives
of primal infon logic introduced by Y. Gurevich and I. Neeman and prove the
corresponding soundness and completeness results. Primal implication
$\imp{\varphi}{\psi}$ corresponds to the encryption of $\psi$ with a secret key
$\varphi$, primal disjunction $\vp{\varphi}{\psi}$ is a group key and $\bot$
reflects some backdoor constructions such as full superuser permissions or a
universal decryption key. For the logic of $\bot$ as a universal key (it was
never considered before) we prove that the derivability problem has linear time
complexity. We also show that the universal key can be emulated using primal
disjunction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6867</identifier>
 <datestamp>2014-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6867</id><created>2013-03-27</created><updated>2014-02-05</updated><authors><author><keyname>Wu</keyname><forenames>Bang Ye</forenames></author><author><keyname>Chen</keyname><forenames>Li-Hsuan</forenames></author></authors><title>Parameterized algorithms for the 2-clustering problem with minimum sum
  and minimum sum of squares objective functions</title><categories>cs.DS</categories><comments>journal version</comments><msc-class>65W05, 68R10, 68Q25, 05C85, 91C20</msc-class><acm-class>G.2.2; I.1.2</acm-class><doi>10.1007/s00453-014-9874-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the {\sc Min-Sum 2-Clustering} problem, we are given a graph and a
parameter $k$, and the goal is to determine if there exists a 2-partition of
the vertex set such that the total conflict number is at most $k$, where the
conflict number of a vertex is the number of its non-neighbors in the same
cluster and neighbors in the different cluster. The problem is equivalent to
{\sc 2-Cluster Editing} and {\sc 2-Correlation Clustering} with an additional
multiplicative factor two in the cost function. In this paper we show an
algorithm for {\sc Min-Sum 2-Clustering} with time complexity $O(n\cdot
2.619^{r/(1-4r/n)}+n^3)$, where $n$ is the number of vertices and $r=k/n$.
Particularly, the time complexity is $O^*(2.619^{k/n})$ for $k\in o(n^2)$ and
polynomial for $k\in O(n\log n)$, which implies that the problem can be solved
in subexponential time for $k\in o(n^2)$. We also design a parameterized
algorithm for a variant in which the cost is the sum of the squared
conflict-numbers. For $k\in o(n^3)$, the algorithm runs in subexponential
$O(n^3\cdot 5.171^{\theta})$ time, where $\theta=\sqrt{k/n}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6872</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6872</id><created>2013-03-27</created><authors><author><keyname>Crochemore</keyname><forenames>Maxime</forenames></author><author><keyname>Iliopoulos</keyname><forenames>Costas S.</forenames></author><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author><author><keyname>Kubica</keyname><forenames>Marcin</forenames></author><author><keyname>Langiu</keyname><forenames>Alessio</forenames></author><author><keyname>Pissis</keyname><forenames>Solon P.</forenames></author><author><keyname>Radoszewski</keyname><forenames>Jakub</forenames></author><author><keyname>Rytter</keyname><forenames>Wojciech</forenames></author><author><keyname>Walen</keyname><forenames>Tomasz</forenames></author></authors><title>Order-Preserving Suffix Trees and Their Algorithmic Applications</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently Kubica et al. (Inf. Process. Let., 2013) and Kim et al. (submitted
to Theor. Comp. Sci.) introduced order-preserving pattern matching. In this
problem we are looking for consecutive substrings of the text that have the
same &quot;shape&quot; as a given pattern. These results include a linear-time
order-preserving pattern matching algorithm for polynomially-bounded alphabet
and an extension of this result to pattern matching with multiple patterns. We
make one step forward in the analysis and give an
$O(\frac{n\log{n}}{\log\log{n}})$ time randomized algorithm constructing suffix
trees in the order-preserving setting. We show a number of applications of
order-preserving suffix trees to identify patterns and repetitions in time
series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6880</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6880</id><created>2013-03-27</created><authors><author><keyname>Ghozlan</keyname><forenames>Hassan</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author></authors><title>Multi-sample Receivers Increase Information Rates for Wiener Phase Noise
  Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to Globecom 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A waveform channel is considered where the transmitted signal is corrupted by
Wiener phase noise and additive white Gaussian noise (AWGN). A discrete-time
channel model is introduced that is based on a multi-sample receiver. Tight
lower bounds on the information rates achieved by the multi-sample receiver are
computed by means of numerical simulations. The results show that oversampling
at the receiver is beneficial for both strong and weak phase noise at high
signal-to-noise ratios. The results are compared with results obtained when
using other discrete-time models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6881</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6881</id><created>2013-03-27</created><authors><author><keyname>Mykoniati</keyname><forenames>Eleni</forenames></author><author><keyname>Latif</keyname><forenames>Laurence</forenames></author><author><keyname>Landa</keyname><forenames>Raul</forenames></author><author><keyname>Yang</keyname><forenames>Ben</forenames></author><author><keyname>Clegg</keyname><forenames>Richard G.</forenames></author><author><keyname>Griffin</keyname><forenames>David</forenames></author><author><keyname>Rio</keyname><forenames>Miguel</forenames></author></authors><title>Distributed Overlay Anycast Table using Space filling curves</title><categories>cs.NI</categories><comments>7 pages, 4 figures</comments><journal-ref>Proceedings of IEEE INFOCOM Workshop, Global Internet Symposium GI
  2009</journal-ref><doi>10.1109/INFCOMW.2009.5072131</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the \emph{Distributed Overlay Anycast Table}, a
structured overlay that implements application-layer anycast, allowing the
discovery of the closest host that is a member of a given group. One
application is in locality-aware peer-to-peer networks, where peers need to
discover low-latency peers participating in the distribution of a particular
file or stream. The DOAT makes use of network delay coordinates and a space
filling curve to achieve locality-aware routing across the overlay, and Bloom
filters to aggregate group identifiers. The solution is designed to optimise
both accuracy and query time, which are essential for real-time applications.
We simulated DOAT using both random and realistic node distributions. The
results show that accuracy is high and query time is low.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6885</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6885</id><created>2013-03-27</created><authors><author><keyname>Kong</keyname><forenames>Hui</forenames></author><author><keyname>He</keyname><forenames>Fei</forenames></author><author><keyname>Song</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Hung</keyname><forenames>William N. N.</forenames></author><author><keyname>Gu</keyname><forenames>Ming</forenames></author></authors><title>Exponential-Condition-Based Barrier Certificate Generation for Safety
  Verification of Hybrid Systems</title><categories>cs.SE math.OC</categories><comments>18 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A barrier certificate is an inductive invariant function which can be used
for the safety verification of a hybrid system. Safety verification based on
barrier certificate has the benefit of avoiding explicit computation of the
exact reachable set which is usually intractable for nonlinear hybrid systems.
In this paper, we propose a new barrier certificate condition, called
Exponential Condition, for the safety verification of semi-algebraic hybrid
systems. The most important benefit of Exponential Condition is that it has a
lower conservativeness than the existing convex condition and meanwhile it
possesses the property of convexity. On the one hand, a less conservative
barrier certificate forms a tighter over-approximation for the reachable set
and hence is able to verify critical safety properties. On the other hand, the
property of convexity guarantees its solvability by semidefinite programming
method. Some examples are presented to illustrate the effectiveness and
practicality of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6887</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6887</id><created>2013-03-27</created><authors><author><keyname>Mykoniati</keyname><forenames>Eleni</forenames></author><author><keyname>Landa</keyname><forenames>Raul</forenames></author><author><keyname>Spirou</keyname><forenames>Spiros</forenames></author><author><keyname>Clegg</keyname><forenames>Richard G.</forenames></author><author><keyname>Latif</keyname><forenames>Lawrence</forenames></author><author><keyname>Griffin</keyname><forenames>David</forenames></author><author><keyname>Rio</keyname><forenames>Miguel</forenames></author></authors><title>Scalable peer-to-peer streaming for live entertainment content</title><categories>cs.NI</categories><comments>11 pages, 6 figures</comments><journal-ref>IEEE Communications 46(12) pp40-46 2008</journal-ref><doi>10.1109/MCOM.2008.4689206</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a system for streaming live entertainment content over the
Internet originating from a single source to a scalable number of consumers
without resorting to centralised or provider- provisioned resources. The system
creates a peer-to-peer overlay network, which attempts to optimise use of
existing capacity to ensure quality of service, delivering low start-up delay
and lag in playout of the live content. There are three main aspects of our
solution. Firstly, a swarming mechanism that constructs an overlay topology for
minimising propagation delays from the source to end consumers. Secondly, a
distributed overlay anycast system that uses a location-based search algorithm
for peers to quickly find the closest peers in a given stream. Finally, a novel
incentives mechanism that encourages peers to donate capacity even when the
user is not actively consuming content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6905</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6905</id><created>2013-03-27</created><authors><author><keyname>Bushev</keyname><forenames>Andrey</forenames></author><author><keyname>Vlasenko</keyname><forenames>Sergey</forenames></author><author><keyname>Glotov</keyname><forenames>Ilya</forenames></author><author><keyname>Monakhov</keyname><forenames>Yuri</forenames></author><author><keyname>Tishin</keyname><forenames>Aleksey</forenames></author></authors><title>The development of the architecture of Distributed Network Intrusion
  Detection System (D-NIDS)</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the development of the architecture of Distributed
Network Intrusion Detection System.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6906</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6906</id><created>2013-03-26</created><authors><author><keyname>Fedoryszak</keyname><forenames>Mateusz</forenames></author><author><keyname>Tkaczyk</keyname><forenames>Dominika</forenames></author><author><keyname>Bolikowski</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>Large scale citation matching using Apache Hadoop</title><categories>cs.IR cs.DL</categories><comments>11 pages, 4 figures</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the process of citation matching links from bibliography entries to
referenced publications are created. Such links are indicators of topical
similarity between linked texts, are used in assessing the impact of the
referenced document and improve navigation in the user interfaces of digital
libraries. In this paper we present a citation matching method and show how to
scale it up to handle great amounts of data using appropriate indexing and a
MapReduce paradigm in the Hadoop environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6907</identifier>
 <datestamp>2014-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6907</id><created>2013-03-27</created><updated>2014-08-17</updated><authors><author><keyname>Bazgan</keyname><forenames>Cristina</forenames></author><author><keyname>Chopin</keyname><forenames>Morgan</forenames></author><author><keyname>Nichterlein</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Sikora</keyname><forenames>Florian</forenames></author></authors><title>Parameterized Approximability of Maximizing the Spread of Influence in
  Networks</title><categories>cs.DS cs.SI</categories><journal-ref>Journal of Discrete Algorithms (27), 2014, 54--65</journal-ref><doi>10.1016/j.jda.2014.05.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of maximizing the spread of influence
through a social network. Given a graph with a threshold value~$thr(v)$
attached to each vertex~$v$, the spread of influence is modeled as follows: A
vertex~$v$ becomes &quot;active&quot; (influenced) if at least $thr(v)$ of its neighbors
are active. In the corresponding optimization problem the objective is then to
find a fixed number of vertices to activate such that the number of activated
vertices at the end of the propagation process is maximum. We show that this
problem is strongly inapproximable in fpt-time with respect to (w.r.t.)
parameter $k$ even for very restrictive thresholds. In the case that the
threshold of each vertex equals its degree, we prove that the problem is
inapproximable in polynomial time and it becomes $r(n)$-approximable in
fpt-time w.r.t. parameter $k$ for any strictly increasing function $r$.
  Moreover, we show that the decision version is W[1]-hard w.r.t. parameter $k$
but becomes fixed-parameter tractable on bounded degree graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6908</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6908</id><created>2013-03-27</created><authors><author><keyname>Clegg</keyname><forenames>R. G.</forenames></author><author><keyname>Withall</keyname><forenames>M. S.</forenames></author><author><keyname>Moore</keyname><forenames>A. W.</forenames></author><author><keyname>Phillips</keyname><forenames>I. W.</forenames></author><author><keyname>Parish</keyname><forenames>D. J.</forenames></author><author><keyname>Rio</keyname><forenames>M.</forenames></author><author><keyname>Landa</keyname><forenames>R.</forenames></author><author><keyname>Haddadi</keyname><forenames>H.</forenames></author><author><keyname>Kyriakopoulos</keyname><forenames>K.</forenames></author><author><keyname>Auge</keyname><forenames>J.</forenames></author><author><keyname>Clayton</keyname><forenames>R.</forenames></author><author><keyname>Salmon</keyname><forenames>D.</forenames></author></authors><title>Challenges in the capture and dissemination of measurements from
  high-speed networks</title><categories>cs.NI</categories><comments>21 pages, 4 figures</comments><journal-ref>IET Communications, Vol 3, Issue 6, June 2009 pp 957-966</journal-ref><doi>10.1049/iet-com.2008.0068</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The production of a large-scale monitoring system for a high-speed network
leads to a number of challenges. These challenges are not purely techinical but
also socio-political and legal. The number of stakeholders in a such a
monitoring activity is large including the network operators, the users, the
equipment manufacturers and of course the monitoring researchers. The MASTS
project (Measurement at All Scales in Time and Space) was created to instrument
the high-speed JANET Lightpath network, and has been extended to incorporate
other paths supported by JANET(UK).
  Challenges the project has faced have included: simple access to the network;
legal issues involved in the storage and dissemination of the captured
information, which may be personal; the volume of data captured and the rate at
which this data appears at store. To this end the MASTS system will have
established four monitoring points each capturing packets on a high speed link.
Traffic header data will be continuously collected, anonymised, indexed, stored
and made available to the research community. A legal framework for the capture
and storage of network measurement data has been developed which allows the
anonymised IP traces to be used for research pur poses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6919</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6919</id><created>2013-03-27</created><authors><author><keyname>Tang</keyname><forenames>Yao</forenames></author><author><keyname>Vu</keyname><forenames>Mai</forenames></author></authors><title>A Partial Decode-Forward Scheme For A Network with N relays</title><categories>cs.IT math.IT</categories><comments>Presented in 47th Annual Conference on Information Sciences and
  Systems (CISS) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a discrete-memoryless relay network consisting of one source, one
destination and N relays, and design a scheme based on partial decode-forward
relaying. The source splits its message into one common and N+1 private parts,
one intended for each relay. It encodes these message parts using Nth-order
block Markov coding, in which each private message part is independently
superimposed on the common parts of the current and N previous blocks. Using
simultaneous sliding window decoding, each relay fully recovers the common
message and its intended private message with the same block index, then
forwards them to the following nodes in the next block. This scheme can be
applied to any network topology. We derive its achievable rate in a compact
form. The result reduces to a known decode-forward lower bound for an N-relay
network and partial decode-forward lower bound for a two-level relay network.
We then apply the scheme to a Gaussian two-level relay network and obtain its
capacity lower bound considering power constraints at the transmitting nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6926</identifier>
 <datestamp>2014-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6926</id><created>2013-03-27</created><authors><author><keyname>Katiyar</keyname><forenames>Dr. S. K.</forenames></author><author><keyname>V.</keyname><forenames>Arun P.</forenames></author></authors><title>A Comparative Analysis on the Applicability of Entropy in remote sensing</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entropy is the measure of uncertainty in any data and is adopted for
maximisation of mutual information in many remote sensing operations. The
availability of wide entropy variations motivated us for an investigation over
the suitability preference of these versions to specific operations.
Methodologies were implemented in Matlab and were enhanced with entropy
variations. Evaluation of various implementations was based on different
statistical parameters with reference to the study area The popular available
versions like Tsalli's, Shanon's, and Renyi's entropies were analysed in
context of various remote sensing operations namely thresholding, clustering
and registration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6927</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6927</id><created>2013-03-27</created><authors><author><keyname>V.</keyname><forenames>Arun P.</forenames></author><author><keyname>Katiyar</keyname><forenames>Dr. S. K.</forenames></author></authors><title>An investigation towards wavelet based optimization of automatic image
  registration techniques</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image registration is the process of transforming different sets of data into
one coordinate system and is required for various remote sensing applications
like change detection, image fusion, and other related areas. The effect of
increased relief displacement, requirement of more control points, and
increased data volume are the challenges associated with the registration of
high resolution image data. The objective of this research work is to study the
most efficient techniques and to investigate the extent of improvement
achievable by enhancing them with Wavelet transform. The SIFT feature based
method uses the Eigen value for extracting thousands of key points based on
scale invariant features and these feature points when further enhanced by the
wavelet transform yields the best results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6932</identifier>
 <datestamp>2014-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6932</id><created>2013-03-23</created><authors><author><keyname>Aslam</keyname><forenames>Muhammad</forenames></author><author><keyname>Abdullah</keyname><forenames>Saleem</forenames></author><author><keyname>ullah</keyname><forenames>Kifayat</forenames></author></authors><title>Bipolar Fuzzy Soft sets and its applications in decision making problem</title><categories>cs.AI</categories><doi>10.3233/IFS-131031</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we combine the concept of a bipolar fuzzy set and a soft
set. We introduce the notion of bipolar fuzzy soft set and study fundamental
properties. We study basic operations on bipolar fuzzy soft set. We define
exdended union, intersection of two bipolar fuzzy soft set. We also give an
application of bipolar fuzzy soft set into decision making problem. We give a
general algorithm to solve decision making problems by using bipolar fuzzy soft
set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6935</identifier>
 <datestamp>2013-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6935</id><created>2013-03-27</created><authors><author><keyname>Tang</keyname><forenames>Xiaocheng</forenames></author><author><keyname>Scheinberg</keyname><forenames>Katya</forenames></author></authors><title>Efficiently Using Second Order Information in Large l1 Regularization
  Problems</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel general algorithm LHAC that efficiently uses second-order
information to train a class of large-scale l1-regularized problems. Our method
executes cheap iterations while achieving fast local convergence rate by
exploiting the special structure of a low-rank matrix, constructed via
quasi-Newton approximation of the Hessian of the smooth loss function. A greedy
active-set strategy, based on the largest violations in the dual constraints,
is employed to maintain a working set that iteratively estimates the complement
of the optimal active set. This allows for smaller size of subproblems and
eventually identifies the optimal active set. Empirical comparisons confirm
that LHAC is highly competitive with several recently proposed state-of-the-art
specialized solvers for sparse logistic regression and sparse inverse
covariance matrix selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6976</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6976</id><created>2013-03-27</created><authors><author><keyname>Patriche</keyname><forenames>Monica</forenames></author></authors><title>The reduction of qualitative games</title><categories>cs.GT math.OC</categories><comments>21 pages</comments><msc-class>91B52, 91B50, 91A80</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We extend the study of the iterated elimination of strictly dominated
strategies (IESDS) from Nash strategic games to a class of qualitative games.
Also in this case, the IESDS process leads us to a kind of 'rationalizable'
result. We define several types of dominance relation and game reduction and
establish conditions under which a unique and nonempty maximal reduction
exists. We generalize, in this way, some results due to Dufwenberg and Stegeman
(2002) and Apt (2007).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.6977</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.6977</id><created>2013-03-27</created><updated>2013-06-28</updated><authors><author><keyname>Dimitrakakis</keyname><forenames>Christos</forenames></author><author><keyname>Tziortziotis</keyname><forenames>Nikolaos</forenames></author></authors><title>ABC Reinforcement Learning</title><categories>stat.ML cs.LG</categories><comments>Corrected version of paper appearing in ICML 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a simple, general framework for likelihood-free
Bayesian reinforcement learning, through Approximate Bayesian Computation
(ABC). The main advantage is that we only require a prior distribution on a
class of simulators (generative models). This is useful in domains where an
analytical probabilistic model of the underlying process is too complex to
formulate, but where detailed simulation models are available. ABC-RL allows
the use of any Bayesian reinforcement learning technique, even in this case. In
addition, it can be seen as an extension of rollout algorithms to the case
where we do not know what the correct model to draw rollouts from is. We
experimentally demonstrate the potential of this approach in a comparison with
LSPI. Finally, we introduce a theorem showing that ABC is a sound methodology
in principle, even when non-sufficient statistics are used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7000</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7000</id><created>2013-03-27</created><authors><author><keyname>Sun</keyname><forenames>Hua</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>Index Coding Capacity: How far can one go with only Shannon
  Inequalities?</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An interference alignment perspective is used to identify the simplest
instances (minimum possible number of edges in the alignment graph, no more
than 2 interfering messages at any destination) of index coding problems where
non-Shannon information inequalities are necessary for capacity
characterization. In particular, this includes the first known example of a
multiple unicast (one destination per message) index coding problem where
non-Shannon information inequalities are shown to be necessary. The simplest
multiple unicast example has 7 edges in the alignment graph and 11 messages.
The simplest multiple groupcast (multiple destinations per message) example has
6 edges in the alignment graph, 6 messages, and 10 receivers. For both the
simplest multiple unicast and multiple groupcast instances, the best outer
bound based on only Shannon inequalities is $\frac{2}{5}$, which is tightened
to $\frac{11}{28}$ by the use of the Zhang-Yeung non-Shannon type information
inequality, and the linear capacity is shown to be $\frac{5}{13}$ using the
Ingleton inequality. Conversely, identifying the minimal challenging aspects of
the index coding problem allows an expansion of the class of solved index
coding problems up to (but not including) these instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7012</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7012</id><created>2013-03-27</created><authors><author><keyname>Mohaisen</keyname><forenames>Abedelaziz</forenames></author><author><keyname>Alrawi</keyname><forenames>Omar</forenames></author></authors><title>Unveiling Zeus</title><categories>cs.CR</categories><comments>Accepted to SIMPLEX 2013 (a workshop held in conjunction with WWW
  2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Malware family classification is an age old problem that many Anti-Virus (AV)
companies have tackled. There are two common techniques used for
classification, signature based and behavior based. Signature based
classification uses a common sequence of bytes that appears in the binary code
to identify and detect a family of malware. Behavior based classification uses
artifacts created by malware during execution for identification. In this paper
we report on a unique dataset we obtained from our operations and classified
using several machine learning techniques using the behavior-based approach.
Our main class of malware we are interested in classifying is the popular Zeus
malware. For its classification we identify 65 features that are unique and
robust for identifying malware families. We show that artifacts like file
system, registry, and network features can be used to identify distinct malware
families with high accuracy---in some cases as high as 95%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7015</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7015</id><created>2013-03-27</created><authors><author><keyname>Zhou</keyname><forenames>Xiaojun</forenames></author></authors><title>A Multiobjective State Transition Algorithm for Single Machine
  Scheduling</title><categories>math.OC cs.IT math.CO math.IT</categories><comments>10 pages, 4 figures</comments><journal-ref>Advances in Global Optimization, 2015, 95: 79-88</journal-ref><doi>10.1007/978-3-319-08377-3_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a discrete state transition algorithm is introduced to solve a
multiobjective single machine job shop scheduling problem. In the proposed
approach, a non-dominated sort technique is used to select the best from a
candidate state set, and a Pareto archived strategy is adopted to keep all the
non-dominated solutions. Compared with the enumeration and other heuristics,
experimental results have demonstrated the effectiveness of the multiobjective
state transition algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7020</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7020</id><created>2013-03-27</created><updated>2013-04-08</updated><authors><author><keyname>Beigi</keyname><forenames>Salman</forenames></author><author><keyname>Chen</keyname><forenames>Jianxin</forenames></author><author><keyname>Grassl</keyname><forenames>Markus</forenames></author><author><keyname>Ji</keyname><forenames>Zhengfeng</forenames></author><author><keyname>Wang</keyname><forenames>Qiang</forenames></author><author><keyname>Zeng</keyname><forenames>Bei</forenames></author></authors><title>Symmetries of Codeword Stabilized Quantum Codes</title><categories>quant-ph cs.IT math.IT</categories><comments>15 pages, 1 figure. Accepted by TQC 2013. Version 2: Funding
  information added; typos corrected</comments><doi>10.4230/LIPIcs.TQC.2013.192</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symmetry is at the heart of coding theory. Codes with symmetry, especially
cyclic codes, play an essential role in both theory and practical applications
of classical error-correcting codes. Here we examine symmetry properties for
codeword stabilized (CWS) quantum codes, which is the most general framework
for constructing quantum error-correcting codes known to date. A CWS code Q can
be represented by a self-dual additive code S and a classical code C, i.,e.,
Q=(S,C), however this representation is in general not unique. We show that for
any CWS code Q with certain permutation symmetry, one can always find a
self-dual additive code S with the same permutation symmetry as Q such that
Q=(S,C). As many good CWS codes have been found by starting from a chosen S,
this ensures that when trying to find CWS codes with certain permutation
symmetry, the choice of S with the same symmetry will suffice. A key step for
this result is a new canonical representation for CWS codes, which is given in
terms of a unique decomposition as union stabilizer codes. For CWS codes, so
far mainly the standard form (G,C) has been considered, where G is a graph
state. We analyze the symmetry of the corresponding graph of G, which in
general cannot possess the same permutation symmetry as Q. We show that it is
indeed the case for the toric code on a square lattice with translational
symmetry, even if its encoding graph can be chosen to be translational
invariant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7026</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7026</id><created>2013-03-27</created><authors><author><keyname>Hessar</keyname><forenames>Farzad</forenames></author><author><keyname>Roy</keyname><forenames>Sumit</forenames></author></authors><title>Minimum Energy Source Coding for Asymmetric Modulation with Application
  to RFID</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimum energy (ME) source coding is an effective technique for efficient
communication with energy-constrained devices, such as sensor network nodes. In
this paper, the principles of generalized ME source coding is developed that is
broadly applicable. Two scenarios - fixed and variable length codewords - are
analyzed. The application of this technique to RFID systems where ME source
coding is particularly advantageous due to the asymmetric nature of data
communications is demonstrated, a first to the best of our knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7030</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7030</id><created>2013-03-27</created><updated>2013-03-29</updated><authors><author><keyname>Rini</keyname><forenames>Stefano</forenames></author><author><keyname>Kurniawany</keyname><forenames>Ernest</forenames></author><author><keyname>Ghaghanidze</keyname><forenames>Levan</forenames></author><author><keyname>Goldsmithy</keyname><forenames>Andrea</forenames></author></authors><title>Energy Efficient Cooperative Strategies for Relay-Assisted Downlink
  Cellular Systems, Part I: Theoretical Framework</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The impact of cognition on the energy efficiency of a downlink cellular
system in which multiple relays assist the transmission of the base station is
considered. The problem is motivated by the practical importance of
relay-assisted solutions in mobile networks, such as LTE-A, in which
cooperation among relays holds the promise of greatly improving the energy
efficiency of the system. We study the fundamental tradeoff between the power
consumption at the base station and the level of cooperation and cognition at
the relay nodes. By distributing the same message to multiple relays, the base
station consumes more power but it enables cooperation among the relays, thus
making the transmission between relays to destination a multiuser cognitive
channel. Cooperation among the relays allows for a reduction of the power used
to transmit from the relays to the end users due to interference management and
the coherent combining gains. These gain are present even in the case of
partial or unidirectional transmitter cooperation, which is the case in
cognitive channels such as the cognitive interference channel and the
interference channel with a cognitive relay. We therefore address the problem
of determining the optimal level of cooperation at the relays which results in
the smallest total power consumption when accounting for the power reduction
due to cognition. A practical design examples and numerical simulation are
presented in a companion paper (part II).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7032</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7032</id><created>2013-03-27</created><updated>2013-07-21</updated><authors><author><keyname>Yao</keyname><forenames>Zhe</forenames></author><author><keyname>Gripon</keyname><forenames>Vincent</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael G.</forenames></author></authors><title>A Massively Parallel Associative Memory Based on Sparse Neural Networks</title><categories>cs.AI cs.DC cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Associative memories store content in such a way that the content can be
later retrieved by presenting the memory with a small portion of the content,
rather than presenting the memory with an address as in more traditional
memories. Associative memories are used as building blocks for algorithms
within database engines, anomaly detection systems, compression algorithms, and
face recognition systems. A classical example of an associative memory is the
Hopfield neural network. Recently, Gripon and Berrou have introduced an
alternative construction which builds on ideas from the theory of error
correcting codes and which greatly outperforms the Hopfield network in
capacity, diversity, and efficiency. In this paper we implement a variation of
the Gripon-Berrou associative memory on a general purpose graphical processing
unit (GPU). The work of Gripon and Berrou proposes two retrieval rules,
sum-of-sum and sum-of-max. The sum-of-sum rule uses only matrix-vector
multiplication and is easily implemented on the GPU. The sum-of-max rule is
much less straightforward to implement because it involves non-linear
operations. However, the sum-of-max rule gives significantly better retrieval
error rates. We propose a hybrid rule tailored for implementation on a GPU
which achieves a 880-fold speedup without sacrificing any accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7034</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7034</id><created>2013-03-28</created><authors><author><keyname>Riniy</keyname><forenames>Stefano</forenames></author><author><keyname>Kurniawan</keyname><forenames>Ernest</forenames></author><author><keyname>Ghaghanidze</keyname><forenames>Levan</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author></authors><title>Energy Efficient Cooperative Strategies for Relay-Assisted Downlink
  Cellular Systems Part II: Practical Design</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a companion paper [1], we present a general approach to evaluate the
impact of cognition in a downlink cellular system in which multiple relays
assist the transmission of the base station. This approach is based on a novel
theoretical tool which produces transmission schemes involving rate-splitting,
superposition coding and interference decoding for a network with any number of
relays and receivers. This second part focuses on a practical design example
for a network in which a base station transmits to three receivers with the aid
of two relay nodes. For this simple network, we explicitly evaluate the impact
of relay cognition and precisely characterize the trade offs between the total
energy consumption and the rate improvements provided by relay cooperation.
These closedform expressions provide important insights on the role of
cognition in larger networks and highlights interesting interference management
strategies. We also present a numerical simulation setup in which we fully
automate the derivation of achievable rate region for a general relay-assisted
downlink cellular network. Our simulations clearly show the great advantages
provided by cooperative strategies at the relays as compared to the
uncoordinated scenario under varying channel conditions and target rates. These
results are obtained by considering a large number of transmission strategies
for different levels of relay cognition and numerically determining one that is
the most energy efficient. The limited computational complexity of the
numerical evaluations makes this approach suitable for the optimization of
transmission strategies for larger networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7037</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7037</id><created>2013-03-28</created><authors><author><keyname>Burton</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Lewiner</keyname><forenames>Thomas</forenames></author><author><keyname>Paix&#xe3;o</keyname><forenames>Jo&#xe3;o</forenames></author><author><keyname>Spreer</keyname><forenames>Jonathan</forenames></author></authors><title>Parameterized Complexity of Discrete Morse Theory</title><categories>cs.CG cs.CC math.GT</categories><comments>To appear in Proceedings of the Twenty-Ninth Annual Symposium on
  Computational Geometry (SoCG). 25 pages, 8 figures, 2 tables</comments><msc-class>68Q17, 68Q15, 57Q15, 58E05, 68R01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal Morse matchings reveal essential structures of cell complexes which
lead to powerful tools to study discrete geometrical objects, in particular
discrete 3-manifolds. However, such matchings are known to be NP-hard to
compute on 3-manifolds, through a reduction to the erasability problem.
  Here, we refine the study of the complexity of problems related to discrete
Morse theory in terms of parameterized complexity. On the one hand we prove
that the erasability problem is W[P]-complete on the natural parameter. On the
other hand we propose an algorithm for computing optimal Morse matchings on
triangulations of 3-manifolds which is fixed-parameter tractable in the
treewidth of the bipartite graph representing the adjacency of the 1- and
2-simplexes. This algorithm also shows fixed parameter tractability for
problems such as erasability and maximum alternating cycle-free matching. We
further show that these results are also true when the treewidth of the dual
graph of the triangulated 3-manifold is bounded. Finally, we investigate the
respective treewidths of simplicial and generalized triangulations of
3-manifolds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7039</identifier>
 <datestamp>2013-08-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7039</id><created>2013-03-28</created><updated>2013-08-20</updated><authors><author><keyname>Singh</keyname><forenames>Sarabjot</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Joint Resource Partitioning and Offloading in Heterogeneous Cellular
  Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In heterogeneous cellular networks (HCNs), it is desirable to offload mobile
users to small cells, which are typically significantly less congested than the
macrocells. To achieve sufficient load balancing, the offloaded users often
have much lower SINR than they would on the macrocell. This SINR degradation
can be partially alleviated through interference avoidance, for example time or
frequency resource partitioning, whereby the macrocell turns off in some
fraction of such resources. Naturally, the optimal offloading strategy is
tightly coupled with resource partitioning; the optimal amount of which in turn
depends on how many users have been offloaded. In this paper, we propose a
general and tractable framework for modeling and analyzing joint resource
partitioning and offloading in a two-tier cellular network. With it, we are
able to derive the downlink rate distribution over the entire network, and an
optimal strategy for joint resource partitioning and offloading. We show that
load balancing, by itself, is insufficient, and resource partitioning is
required in conjunction with offloading to improve the rate of cell edge users
in co-channel heterogeneous networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7043</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7043</id><created>2013-03-28</created><authors><author><keyname>Shen</keyname><forenames>Fumin</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Shi</keyname><forenames>Qinfeng</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Tang</keyname><forenames>Zhenmin</forenames></author></authors><title>Inductive Hashing on Manifolds</title><categories>cs.LG</categories><comments>Appearing in IEEE Conf. Computer Vision and Pattern Recognition, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning based hashing methods have attracted considerable attention due to
their ability to greatly increase the scale at which existing algorithms may
operate. Most of these methods are designed to generate binary codes that
preserve the Euclidean distance in the original space. Manifold learning
techniques, in contrast, are better able to model the intrinsic structure
embedded in the original high-dimensional data. The complexity of these models,
and the problems with out-of-sample data, have previously rendered them
unsuitable for application to large-scale embedding, however. In this work, we
consider how to learn compact binary embeddings on their intrinsic manifolds.
In order to address the above-mentioned difficulties, we describe an efficient,
inductive solution to the out-of-sample data problem, and a process by which
non-parametric manifold learning may be used as the basis of a hashing method.
Our proposed approach thus allows the development of a range of new hashing
techniques exploiting the flexibility of the wide variety of manifold learning
approaches available. We particularly show that hashing on the basis of t-SNE .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7048</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7048</id><created>2013-03-28</created><authors><author><keyname>Hou</keyname><forenames>Thomas Y.</forenames></author><author><keyname>Shi</keyname><forenames>Zuoqiang</forenames></author><author><keyname>Tavallali</keyname><forenames>Peyman</forenames></author></authors><title>Convergence of a data-driven time-frequency analysis method</title><categories>math.NA cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent paper, Hou and Shi introduced a new adaptive data analysis method
to analyze nonlinear and non-stationary data. The main idea is to look for the
sparsest representation of multiscale data within the largest possible
dictionary consisting of intrinsic mode functions of the form $\{a(t)
\cos(\theta(t))\}$, where $a \in V(\theta)$, $V(\theta)$ consists of the
functions smoother than $\cos(\theta(t))$ and $\theta'\ge 0$. This problem was
formulated as a nonlinear $L^0$ optimization problem and an iterative nonlinear
matching pursuit method was proposed to solve this nonlinear optimization
problem. In this paper, we prove the convergence of this nonlinear matching
pursuit method under some sparsity assumption on the signal. We consider both
well-resolved and sparse sampled signals. In the case without noise, we prove
that our method gives exact recovery of the original signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7054</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7054</id><created>2013-03-28</created><updated>2013-08-04</updated><authors><author><keyname>Feng</keyname><forenames>Shen</forenames></author><author><keyname>Liew</keyname><forenames>Soung C.</forenames></author></authors><title>Wireless Broadcast with Physical-Layer Network Coding</title><categories>cs.IT cs.NI math.IT</categories><comments>23 pages, 18 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates the maximum broadcast throughput and its achievability
in multi-hop wireless networks with half-duplex node constraint. We allow the
use of physical-layer network coding (PNC). Although the use of PNC for unicast
has been extensively studied, there has been little prior work on PNC for
broadcast. Our specific results are as follows: 1) For single-source broadcast,
the theoretical throughput upper bound is n/(n+1), where n is the &quot;min
vertex-cut&quot; size of the network. 2) In general, the throughput upper bound is
not always achievable. 3) For grid and many other networks, the throughput
upper bound n/(n+1) is achievable. Our work can be considered as an attempt to
understand the relationship between max-flow and min-cut in half-duplex
broadcast networks with cycles (there has been prior work on networks with
cycles, but not half-duplex broadcast networks).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7075</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7075</id><created>2013-03-28</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Debajyoti</forenames></author><author><keyname>Sonawane</keyname><forenames>Gitesh</forenames></author><author><keyname>Gupta</keyname><forenames>Parth Sarthi</forenames></author><author><keyname>Bhavsar</keyname><forenames>Sagar</forenames></author><author><keyname>Mittal</keyname><forenames>Vibha</forenames></author></authors><title>Enhanced Security for Cloud Storage using File Encryption</title><categories>cs.CR</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is a term coined to a network that offers incredible
processing power, a wide array of storage space and unbelievable speed of
computation. Social media channels, corporate structures and individual
consumers are all switching to the magnificent world of cloud computing. The
flip side to this coin is that with cloud storage emerges the security issues
of confidentiality, data integrity and data availability. Since the cloud is a
mere collection of tangible super computers spread across the world,
authentication and authorization for data access is more than a necessity. Our
work attempts to overcome these security threats. The proposed methodology
suggests the encryption of the files to be uploaded on the cloud. The integrity
and confidentiality of the data uploaded by the user is ensured doubly by not
only encrypting it but also providing access to the data only on successful
authentication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7077</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7077</id><created>2013-03-28</created><authors><author><keyname>Berkholz</keyname><forenames>Christoph</forenames></author><author><keyname>Verbitsky</keyname><forenames>Oleg</forenames></author></authors><title>On the speed of constraint propagation and the time complexity of arc
  consistency testing</title><categories>cs.LO cs.AI</categories><comments>19 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Establishing arc consistency on two relational structures is one of the most
popular heuristics for the constraint satisfaction problem. We aim at
determining the time complexity of arc consistency testing. The input
structures $G$ and $H$ can be supposed to be connected colored graphs, as the
general problem reduces to this particular case. We first observe the upper
bound $O(e(G)v(H)+v(G)e(H))$, which implies the bound $O(e(G)e(H))$ in terms of
the number of edges and the bound $O((v(G)+v(H))^3)$ in terms of the number of
vertices. We then show that both bounds are tight up to a constant factor as
long as an arc consistency algorithm is based on constraint propagation (like
any algorithm currently known).
  Our argument for the lower bounds is based on examples of slow constraint
propagation. We measure the speed of constraint propagation observed on a pair
$G,H$ by the size of a proof, in a natural combinatorial proof system, that
Spoiler wins the existential 2-pebble game on $G,H$. The proof size is bounded
from below by the game length $D(G,H)$, and a crucial ingredient of our
analysis is the existence of $G,H$ with $D(G,H)=\Omega(v(G)v(H))$. We find one
such example among old benchmark instances for the arc consistency problem and
also suggest a new, different construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7083</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7083</id><created>2013-03-28</created><updated>2015-01-29</updated><authors><author><keyname>Goldfeld</keyname><forenames>Ziv</forenames></author><author><keyname>Permuter</keyname><forenames>Haim H.</forenames></author><author><keyname>Zaidel</keyname><forenames>Benjamin M.</forenames></author></authors><title>The Finite State MAC with Cooperative Encoders and Delayed CSI</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the finite-state multiple access channel (MAC)
with partially cooperative encoders and delayed channel state information
(CSI). Here partial cooperation refers to the communication between the
encoders via finite-capacity links. The channel states are assumed to be
governed by a Markov process. Full CSI is assumed at the receiver, while at the
transmitters, only delayed CSI is available. The capacity region of this
channel model is derived by first solving the case of the finite-state MAC with
a common message. Achievability for the latter case is established using the
notion of strategies, however, we show that optimal codes can be constructed
directly over the input alphabet. This results in a single codebook
construction that is then leveraged to apply simultaneous joint decoding.
Simultaneous decoding is crucial here because it circumvents the need to rely
on the capacity region's corner points, a task that becomes increasingly
cumbersome with the growth in the number of messages to be sent. The common
message result is then used to derive the capacity region for the case with
partially cooperating encoders. Next, we apply this general result to the
special case of the Gaussian vector MAC with diagonal channel transfer
matrices, which is suitable for modeling, e.g., orthogonal frequency division
multiplexing (OFDM)-based communication systems. The capacity region of the
Gaussian channel is presented in terms of a convex optimization problem that
can be solved efficiently using numerical tools. The region is derived by first
presenting an outer bound on the general capacity region and then suggesting a
specific input distribution that achieves this bound. Finally, numerical
results are provided that give valuable insight into the practical implications
of optimally using conferencing to maximize the transmission rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7085</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7085</id><created>2013-03-28</created><authors><author><keyname>Benammar</keyname><forenames>Othman</forenames></author><author><keyname>Elasri</keyname><forenames>Hicham</forenames></author><author><keyname>Sekkaki</keyname><forenames>Abderrahim</forenames></author></authors><title>Semantic Matching of Security Policies to Support Security Experts</title><categories>cs.CR cs.AI</categories><comments>SECURWARE 2012 : The Sixth International Conference on Emerging
  Security Information, Systems and Technologies</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Management of security policies has become increasingly difficult given the
number of domains to manage, taken into consideration their extent and their
complexity. Security experts has to deal with a variety of frameworks and
specification languages used in different domains that may belong to any Cloud
Computing or Distributed Systems. This wealth of frameworks and languages make
the management task and the interpretation of the security policies so
difficult. Each approach provides its own conflict management method or tool,
the security expert will be forced to manage all these tools, which makes the
field maintenance and time consuming expensive. In order to hide this
complexity and to facilitate some security experts tasks and automate the
others, we propose a security policies aligning based on ontologies process;
this process enables to detect and resolve security policies conflicts and to
support security experts in managing tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7093</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7093</id><created>2013-03-28</created><updated>2013-04-08</updated><authors><author><keyname>Gopalakrishna</keyname><forenames>Aravind Kota</forenames></author><author><keyname>Ozcelebi</keyname><forenames>Tanir</forenames></author><author><keyname>Liotta</keyname><forenames>Antonio</forenames></author><author><keyname>Lukkien</keyname><forenames>Johan J.</forenames></author></authors><title>Relevance As a Metric for Evaluating Machine Learning Algorithms</title><categories>stat.ML cs.LG</categories><comments>To Appear at International Conference on Machine Learning and Data
  Mining (MLDM 2013), 14 pages, 6 figures</comments><doi>10.1007/978-3-642-39712-7_15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In machine learning, the choice of a learning algorithm that is suitable for
the application domain is critical. The performance metric used to compare
different algorithms must also reflect the concerns of users in the application
domain under consideration. In this work, we propose a novel probability-based
performance metric called Relevance Score for evaluating supervised learning
algorithms. We evaluate the proposed metric through empirical analysis on a
dataset gathered from an intelligent lighting pilot installation. In comparison
to the commonly used Classification Accuracy metric, the Relevance Score proves
to be more appropriate for a certain class of applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7103</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7103</id><created>2013-03-28</created><authors><author><keyname>Penna</keyname><forenames>Federico</forenames></author><author><keyname>Stanczak</keyname><forenames>Slawomir</forenames></author></authors><title>Decentralized Eigenvalue Algorithms for Distributed Signal Detection in
  Cognitive Networks</title><categories>cs.DC cs.MA</categories><comments>Submitted to IEEE JSAC Cognitive Radio Series</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we derive and analyze two algorithms -- referred to as
decentralized power method (DPM) and decentralized Lanczos algorithm (DLA) --
for distributed computation of one (the largest) or multiple eigenvalues of a
sample covariance matrix over a wireless network. The proposed algorithms,
based on sequential average consensus steps for computations of matrix-vector
products and inner vector products, are first shown to be equivalent to their
centralized counterparts in the case of exact distributed consensus. Then,
closed-form expressions of the error introduced by non-ideal consensus are
derived for both algorithms. The error of the DPM is shown to vanish
asymptotically under given conditions on the sequence of consensus errors.
Finally, we consider applications to spectrum sensing in cognitive radio
networks, and we show that virtually all eigenvalue-based tests proposed in the
literature can be implemented in a distributed setting using either the DPM or
the DLA. Simulation results are presented that validate the effectiveness of
the proposed algorithms in conditions of practical interest (large-scale
networks, small number of samples, and limited number of iterations).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7115</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7115</id><created>2013-03-28</created><authors><author><keyname>Namiot</keyname><forenames>Dmitry</forenames></author><author><keyname>Schneps-Schneppe</keyname><forenames>Manfred</forenames></author></authors><title>Smart Cities Software from the developer's point of view</title><categories>cs.CY cs.NI</categories><comments>8 pages, submitted to 6-th Conference Applied Information and
  Communication Technology AICT2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper discusses the current state and development proposals for Smart
Cities and Future Internet projects. Definitions of a Smart City can vary but
usually tend to suggest the use of innovative Info-Communication technologies
such as the Internet of Things and Web 2.0 to deliver more effective and
efficient public services that improve living and working conditions and create
more sustainable urban environments. Our goal is to analyze the current
proposals from the developer's point of view, highlight the really new
elements, the positions borrowed from the existing tools as well as propose
some new extensions. We would like to discuss the possible extensions for the
existing proposals and describe add-ons that, by our opinion, let keep the
future research inline with the modern approaches in the web development
domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7117</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7117</id><created>2013-03-28</created><updated>2014-11-20</updated><authors><author><keyname>Fasy</keyname><forenames>Brittany Terese</forenames></author><author><keyname>Lecci</keyname><forenames>Fabrizio</forenames></author><author><keyname>Rinaldo</keyname><forenames>Alessandro</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author><author><keyname>Balakrishnan</keyname><forenames>Sivaraman</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author></authors><title>Confidence sets for persistence diagrams</title><categories>math.ST cs.CG cs.LG stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/14-AOS1252 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1252</report-no><journal-ref>Annals of Statistics 2014, Vol. 42, No. 6, 2301-2339</journal-ref><doi>10.1214/14-AOS1252</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Persistent homology is a method for probing topological properties of point
clouds and functions. The method involves tracking the birth and death of
topological features (2000) as one varies a tuning parameter. Features with
short lifetimes are informally considered to be &quot;topological noise,&quot; and those
with a long lifetime are considered to be &quot;topological signal.&quot; In this paper,
we bring some statistical ideas to persistent homology. In particular, we
derive confidence sets that allow us to separate topological signal from
topological noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7122</identifier>
 <datestamp>2013-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7122</id><created>2013-03-28</created><updated>2013-07-09</updated><authors><author><keyname>Polym&#xe9;ris</keyname><forenames>Andreas</forenames></author><author><keyname>Riquelme</keyname><forenames>Fabi&#xe1;n</forenames></author></authors><title>On the Complexity of the Decisive Problem in Simple, Regular and
  Weighted Games</title><categories>cs.GT cs.CC</categories><comments>12 pages</comments><msc-class>05C65, 91A35, 91A12, 68Q25</msc-class><acm-class>G.2.2; F.1.1; F.1.3; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of an important property of simple,
regular and weighted games, which is decisiveness. We show that this concept
can naturally be represented in the context of hypergraph theory, and that
decisiveness can be decided for simple games in quasi-polynomial time, and for
regular and weighted games in polynomial time. The strongness condition poses
the main difficulties, while properness reduces the complexity of the problem,
especially if it is amplified by regularity. On the other hand, regularity also
allows to specify the problem instances much more economically, implying a
reconsideration of the corresponding complexity measure that, as we prove, has
important structural as well as algorithmic consequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7127</identifier>
 <datestamp>2014-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7127</id><created>2013-03-28</created><updated>2014-02-27</updated><authors><author><keyname>Balatsoukas-Stimming</keyname><forenames>A.</forenames></author><author><keyname>Raymond</keyname><forenames>A. J.</forenames></author><author><keyname>Gross</keyname><forenames>W. J.</forenames></author><author><keyname>Burg</keyname><forenames>A.</forenames></author></authors><title>Hardware Architecture for List SC Decoding of Polar Codes</title><categories>cs.IT cs.AR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a hardware architecture and algorithmic improvements for list SC
decoding of polar codes. More specifically, we show how to completely avoid
copying of the likelihoods, which is algorithmically the most cumbersome part
of list SC decoding. The hardware architecture was synthesized for a
blocklength of N = 1024 bits and list sizes L = 2, 4 using a UMC 90nm VLSI
technology. The resulting decoder can achieve a coded throughput of 181 Mbps at
a frequency of 459 MHz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7136</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7136</id><created>2013-03-28</created><authors><author><keyname>Kesner</keyname><forenames>Delia</forenames><affiliation>Universit&#xe9; Paris-Diderot</affiliation></author><author><keyname>Viana</keyname><forenames>Petrucio</forenames><affiliation>Universidade Federal Fluminense</affiliation></author></authors><title>Proceedings Seventh Workshop on Logical and Semantic Frameworks, with
  Applications</title><categories>cs.LO cs.PL</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 113, 2013</journal-ref><doi>10.4204/EPTCS.113</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document contains the proceedings of the Seventh International Workshop
on Logical and Semantic Frameworks, with Applications, which was held on
September 29 and 30, 2012, in Rio de Janeiro, Brazil. It contains 11 regular
papers (9 long and 2 short) accepted for presentation at the meeting, as well
as extended abstracts of invited talks by Torben Bra\&quot;uner (Roskilde
University, Denmark), Maribel Fern\'andez (King's College London, United
Kingdom), Edward Hermann Haeusler (PUC-Rio, Brazil) and Alexandre Miquel
(\'Ecole Normale Sup\'erieure de Lyon, France).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7137</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7137</id><created>2013-03-28</created><authors><author><keyname>Andronov</keyname><forenames>A.</forenames></author><author><keyname>Fioshin</keyname><forenames>M.</forenames></author></authors><title>Discrete Optimization of Statistical Sample Sizes in Simulation by Using
  the Hierarchical Bootstrap Method</title><categories>cs.AI</categories><comments>9 pages</comments><journal-ref>proceedings of the 6th Tartu Conference, 1999</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bootstrap method application in simulation supposes that value of random
variables are not generated during the simulation process but extracted from
available sample populations. In the case of Hierarchical Bootstrap the
function of interest is calculated recurrently using the calculation tree. In
the present paper we consider the optimization of sample sizes in each vertex
of the calculation tree. The dynamic programming method is used for this aim.
Proposed method allows to decrease a variance of system characteristic
estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7144</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7144</id><created>2013-03-28</created><authors><author><keyname>Lin</keyname><forenames>Yu-Ru</forenames></author><author><keyname>Margolin</keyname><forenames>Drew</forenames></author><author><keyname>Keegan</keyname><forenames>Brian</forenames></author><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author><author><keyname>Lazer</keyname><forenames>David</forenames></author></authors><title>#Bigbirds Never Die: Understanding Social Dynamics of Emergent Hashtag</title><categories>cs.SI physics.data-an physics.soc-ph</categories><comments>Proceedings of the 7th International AAAI Conference on Weblogs and
  Social Media (ICWSM 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the growth, survival, and context of 256 novel hashtags during the
2012 U.S. presidential debates. Our analysis reveals the trajectories of
hashtag use fall into two distinct classes: &quot;winners&quot; that emerge more quickly
and are sustained for longer periods of time than other &quot;also-rans&quot; hashtags.
We propose a &quot;conversational vibrancy&quot; framework to capture dynamics of
hashtags based on their topicality, interactivity, diversity, and prominence.
Statistical analyses of the growth and persistence of hashtags reveal novel
relationships between features of this framework and the relative success of
hashtags. Specifically, retweets always contribute to faster hashtag adoption,
replies extend the life of &quot;winners&quot; while having no effect on &quot;also-rans.&quot;
This is the first study on the lifecycle of hashtag adoption and use in
response to purely exogenous shocks. We draw on theories of uses and
gratification, organizational ecology, and language evolution to discuss these
findings and their implications for understanding social influence and
collective action in social media more generally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7149</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7149</id><created>2013-03-28</created><updated>2013-03-28</updated><authors><author><keyname>Vellino</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Usage-based vs. Citation-based Methods for Recommending Scholarly
  Research Articles</title><categories>cs.DL cs.IR</categories><comments>4 pages, 4 figures, ACM Recommender Systems Workshop 2012, Dublin
  Ireland</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are two principal data sources for collaborative filtering recommenders
in scholarly digital libraries: usage data obtained from harvesting a large,
distributed collection of Open URL web logs and citation data obtained from the
journal articles. This study explores the characteristics of recommendations
generated by implementations of these two methods: the 'bX' system by ExLibris
and an experimental citation-based recommender, Sarkanto. Recommendations from
each system were compared according to their semantic similarity to the seed
article that was used to generate them. Since the full text of the articles was
not available for all the recommendations in both systems, the semantic
similarity between the seed article and the recommended articles was deemed to
be the semantic distance between the journals in which the articles were
published. The semantic distance between journals was computed from the
&quot;semantic vectors&quot; distance between all the terms in the full-text of the
available articles in that journal and this study shows that citation-based
recommendations are more semantically diverse than usage-based ones. These
recommenders are complementary since most of the time, when one recommender
produces recommendations the other does not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7186</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7186</id><created>2013-03-28</created><authors><author><keyname>Kaynig</keyname><forenames>Verena</forenames></author><author><keyname>Vazquez-Reina</keyname><forenames>Amelio</forenames></author><author><keyname>Knowles-Barley</keyname><forenames>Seymour</forenames></author><author><keyname>Roberts</keyname><forenames>Mike</forenames></author><author><keyname>Jones</keyname><forenames>Thouis R.</forenames></author><author><keyname>Kasthuri</keyname><forenames>Narayanan</forenames></author><author><keyname>Miller</keyname><forenames>Eric</forenames></author><author><keyname>Lichtman</keyname><forenames>Jeff</forenames></author><author><keyname>Pfister</keyname><forenames>Hanspeter</forenames></author></authors><title>Large-Scale Automatic Reconstruction of Neuronal Processes from Electron
  Microscopy Images</title><categories>q-bio.NC cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated sample preparation and electron microscopy enables acquisition of
very large image data sets. These technical advances are of special importance
to the field of neuroanatomy, as 3D reconstructions of neuronal processes at
the nm scale can provide new insight into the fine grained structure of the
brain. Segmentation of large-scale electron microscopy data is the main
bottleneck in the analysis of these data sets. In this paper we present a
pipeline that provides state-of-the art reconstruction performance while
scaling to data sets in the GB-TB range. First, we train a random forest
classifier on interactive sparse user annotations. The classifier output is
combined with an anisotropic smoothing prior in a Conditional Random Field
framework to generate multiple segmentation hypotheses per image. These
segmentations are then combined into geometrically consistent 3D objects by
segmentation fusion. We provide qualitative and quantitative evaluation of the
automatic segmentation and demonstrate large-scale 3D reconstructions of
neuronal processes from a $\mathbf{27,000}$ $\mathbf{\mu m^3}$ volume of brain
tissue over a cube of $\mathbf{30 \; \mu m}$ in each dimension corresponding to
1000 consecutive image sections. We also introduce Mojo, a proofreading tool
including semi-automated correction of merge errors based on sparse user
scribbles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7195</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7195</id><created>2013-03-28</created><authors><author><keyname>Bux</keyname><forenames>Marc</forenames></author><author><keyname>Leser</keyname><forenames>Ulf</forenames></author></authors><title>Parallelization in Scientific Workflow Management Systems</title><categories>cs.DC</categories><comments>24 pages, 17 figures (13 PDF, 4 PNG)</comments><msc-class>68N19</msc-class><acm-class>C.1.4; D.1.3; D.3.2; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last two decades, scientific workflow management systems (SWfMS)
have emerged as a means to facilitate the design, execution, and monitoring of
reusable scientific data processing pipelines. At the same time, the amounts of
data generated in various areas of science outpaced enhancements in
computational power and storage capabilities. This is especially true for the
life sciences, where new technologies increased the sequencing throughput from
kilobytes to terabytes per day. This trend requires current SWfMS to adapt:
Native support for parallel workflow execution must be provided to increase
performance; dynamically scalable &quot;pay-per-use&quot; compute infrastructures have to
be integrated to diminish hardware costs; adaptive scheduling of workflows in
distributed compute environments is required to optimize resource utilization.
In this survey we give an overview of parallelization techniques for SWfMS,
both in theory and in their realization in concrete systems. We find that
current systems leave considerable room for improvement and we propose key
advancements to the landscape of SWfMS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7197</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7197</id><created>2013-03-28</created><updated>2014-05-12</updated><authors><author><keyname>Le</keyname><forenames>Anh</forenames></author><author><keyname>Tehrani</keyname><forenames>Arash S.</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Markopoulou</keyname><forenames>Athina</forenames></author></authors><title>Network Codes for Real-Time Applications</title><categories>cs.NI cs.IT math.IT</categories><comments>ToN 2013 Submission Version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the scenario of broadcasting for real-time applications and loss
recovery via instantly decodable network coding. Past work focused on
minimizing the completion delay, which is not the right objective for real-time
applications that have strict deadlines. In this work, we are interested in
finding a code that is instantly decodable by the maximum number of users.
First, we prove that this problem is NP-Hard in the general case. Then we
consider the practical probabilistic scenario, where users have i.i.d. loss
probability and the number of packets is linear or polynomial in the number of
users. In this scenario, we provide a polynomial-time (in the number of users)
algorithm that finds the optimal coded packet. The proposed algorithm is
evaluated using both simulation and real network traces of a real-time Android
application. Both results show that the proposed coding scheme significantly
outperforms the state-of-the-art baselines: an optimal repetition code and a
COPE-like greedy scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7200</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7200</id><created>2013-03-28</created><authors><author><keyname>Fernando</keyname><forenames>Chrisantha</forenames></author></authors><title>Design for a Darwinian Brain: Part 1. Philosophy and Neuroscience</title><categories>cs.AI q-bio.NC</categories><comments>Darwinian Neurodynamics. Submitted as a two part paper to Living
  Machines 2013 Natural History Museum, London</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Physical symbol systems are needed for open-ended cognition. A good way to
understand physical symbol systems is by comparison of thought to chemistry.
Both have systematicity, productivity and compositionality. The state of the
art in cognitive architectures for open-ended cognition is critically assessed.
I conclude that a cognitive architecture that evolves symbol structures in the
brain is a promising candidate to explain open-ended cognition. Part 2 of the
paper presents such a cognitive architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7201</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7201</id><created>2013-03-28</created><authors><author><keyname>Fernando</keyname><forenames>Chrisantha</forenames></author><author><keyname>Vasas</keyname><forenames>Vera</forenames></author></authors><title>Design for a Darwinian Brain: Part 2. Cognitive Architecture</title><categories>cs.AI</categories><comments>Submitted as Part 2 to Living Machines 2013, Natural History Museum,
  London. Code available on github as it is being developed to implement the
  cognitive architecture above, here...
  https://github.com/ctf20/DarwinianNeurodynamics</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The accumulation of adaptations in an open-ended manner during lifetime
learning is a holy grail in reinforcement learning, intrinsic motivation,
artificial curiosity, and developmental robotics. We present a specification
for a cognitive architecture that is capable of specifying an unlimited range
of behaviors. We then give examples of how it can stochastically explore an
interesting space of adjacent possible behaviors. There are two main novelties;
the first is a proper definition of the fitness of self-generated games such
that interesting games are expected to evolve. The second is a modular and
evolvable behavior language that has systematicity, productivity, and
compositionality, i.e. it is a physical symbol system. A part of the
architecture has already been implemented on a humanoid robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7205</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7205</id><created>2013-03-28</created><authors><author><keyname>Doerr</keyname><forenames>Benjamin</forenames></author></authors><title>Winkler's Hat Guessing Game: Better Results for Imbalanced Hat
  Distributions</title><categories>math.CO cs.DM cs.GT</categories><comments>8 pages. Mostly unchanged version from the author's habilitation
  thesis (2005)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we give an explicit polynomial-time executable strategy for
Peter Winkler's hat guessing game that gives superior results if the
distribution of hats is imbalanced. While Winkler's strategy guarantees in any
case that $\lfloor n/2 \rfloor$ of the $n$ player guess their hat color
correct, our strategy ensures that the players produce $\max\{r,b\} - 1.2
n^{2/3} -2$ correct guesses for any distribution of $r$ red and $b = n - r$
blue hats. We also show that any strategy ensuring $\max\{r,b\} - f(n)$ correct
guesses necessarily has $f(n) = \Omega(\sqrt n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7217</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7217</id><created>2013-03-28</created><authors><author><keyname>Kapoor</keyname><forenames>Sanjiv</forenames></author><author><keyname>Li</keyname><forenames>XiangYang</forenames></author></authors><title>Efficient Construction of Spanners in $d$-Dimensions</title><categories>cs.CG cs.DS</categories><comments>29 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of efficiently constructing $k$-vertex
fault-tolerant geometric $t$-spanners in $\dspace$ (for $k \ge 0$ and $t &gt;1$).
Vertex fault-tolerant spanners were introduced by Levcopoulus et. al in 1998.
For $k=0$, we present an $O(n \log n)$ method using the algebraic computation
tree model to find a $t$-spanner with degree bound O(1) and weight
$O(\weight(MST))$. This resolves an open problem. For $k \ge 1$, we present an
efficient method that, given $n$ points in $\dspace$, constructs $k$-vertex
fault-tolerant $t$-spanners with the maximum degree bound O(k) and weight bound
$O(k^2 \weight(MST))$ in time $O(n \log n)$. Our method achieves the best
possible bounds on degree, total edge length, and the time complexity, and
solves the open problem of efficient construction of (fault-tolerant)
$t$-spanners in $\dspace$ in time $O(n \log n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7220</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7220</id><created>2013-03-26</created><authors><author><keyname>Bull</keyname><forenames>Larry</forenames></author></authors><title>On Mobile DNA in Artificial Regulatory Networks: Evolving Functional and
  Structural Dynamism</title><categories>cs.ET</categories><comments>24 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a growing body of work considering the use of representations based
upon genetic regulatory networks. This paper uses a recently presented
abstract, tunable Boolean regulatory network model to explore aspects of mobile
DNA, such as transposons, within these dynamical systems. The significant role
of mobile DNA in the evolution of natural systems is becoming increasingly
clear. Whilst operators loosely based upon transposons have previously been
used within evolutionary computation, their use within regulatory network
representations enables the potential exploitation of numerous new mechanisms.
This paper shows how dynamically controlling network node connectivity and
function via transposon-inspired mechanisms can be selected for under
non-stationary and coevolutionary scenarios, including when such changes are
heritable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7225</identifier>
 <datestamp>2013-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7225</id><created>2013-03-28</created><authors><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Xie</keyname><forenames>Neng-Gang</forenames></author><author><keyname>Ye</keyname><forenames>Ye</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Evolution of emotions on networks leads to the evolution of cooperation
  in social dilemmas</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI q-bio.PE</categories><comments>7 two-column pages, 7 figures; accepted for publication in Physical
  Review E</comments><journal-ref>Phys. Rev. E 87 (2013) 042805</journal-ref><doi>10.1103/PhysRevE.87.042805</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the resolution of social dilemmas on random graphs and
scale-free networks is facilitated by imitating not the strategy of better
performing players but rather their emotions. We assume sympathy and envy as
the two emotions that determine the strategy of each player by any given
interaction, and we define them as probabilities to cooperate with players
having a lower and higher payoff, respectively. Starting with a population
where all possible combinations of the two emotions are available, the
evolutionary process leads to a spontaneous fixation to a single emotional
profile that is eventually adopted by all players. However, this emotional
profile depends not only on the payoffs but also on the heterogeneity of the
interaction network. Homogeneous networks, such as lattices and regular random
graphs, lead to fixations that are characterized by high sympathy and high
envy, while heterogeneous networks lead to low or modest sympathy but also low
envy. Our results thus suggest that public emotions and the propensity to
cooperate at large depend, and are in fact determined by the properties of the
interaction network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7226</identifier>
 <datestamp>2013-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7226</id><created>2013-03-28</created><authors><author><keyname>Chen</keyname><forenames>Yudong</forenames></author><author><keyname>Kawadia</keyname><forenames>Vikas</forenames></author><author><keyname>Urgaonkar</keyname><forenames>Rahul</forenames></author></authors><title>Detecting Overlapping Temporal Community Structure in Time-Evolving
  Networks</title><categories>cs.SI cs.LG physics.soc-ph stat.ML</categories><comments>12 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a principled approach for detecting overlapping temporal community
structure in dynamic networks. Our method is based on the following framework:
find the overlapping temporal community structure that maximizes a quality
function associated with each snapshot of the network subject to a temporal
smoothness constraint. A novel quality function and a smoothness constraint are
proposed to handle overlaps, and a new convex relaxation is used to solve the
resulting combinatorial optimization problem. We provide theoretical guarantees
as well as experimental results that reveal community structure in real and
synthetic networks. Our main insight is that certain structures can be
identified only when temporal correlation is considered and when communities
are allowed to overlap. In general, discovering such overlapping temporal
community structure can enhance our understanding of real-world complex
networks by revealing the underlying stability behind their seemingly chaotic
evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7264</identifier>
 <datestamp>2014-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7264</id><created>2013-03-28</created><authors><author><keyname>Zhu</keyname><forenames>Yaojia</forenames></author><author><keyname>Yan</keyname><forenames>Xiaoran</forenames></author><author><keyname>Getoor</keyname><forenames>Lise</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author></authors><title>Scalable Text and Link Analysis with Mixed-Topic Link Models</title><categories>cs.LG cs.IR cs.SI physics.data-an stat.ML</categories><comments>11 pages, 4 figures</comments><acm-class>G.3; H.3.3; H.4; I.2</acm-class><journal-ref>Proc. 19th SIGKDD Conference on Knowledge Discovery and Data
  Mining (KDD) 2013, 473-481</journal-ref><doi>10.1145/2487575.2487693</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many data sets contain rich information about objects, as well as pairwise
relations between them. For instance, in networks of websites, scientific
papers, and other documents, each node has content consisting of a collection
of words, as well as hyperlinks or citations to other nodes. In order to
perform inference on such data sets, and make predictions and recommendations,
it is useful to have models that are able to capture the processes which
generate the text at each node and the links between them. In this paper, we
combine classic ideas in topic modeling with a variant of the mixed-membership
block model recently developed in the statistical physics community. The
resulting model has the advantage that its parameters, including the mixture of
topics of each document and the resulting overlapping communities, can be
inferred with a simple and scalable expectation-maximization algorithm. We test
our model on three data sets, performing unsupervised topic classification and
link prediction. For both tasks, our model outperforms several existing
state-of-the-art methods, achieving higher accuracy with significantly less
computation, analyzing a data set with 1.3 million words and 44 thousand links
in a few minutes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7270</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7270</id><created>2013-03-28</created><authors><author><keyname>Moraveji</keyname><forenames>Reza</forenames></author><author><keyname>Taheri</keyname><forenames>Javid</forenames></author><author><keyname>HosseinyFarahabady</keyname><forenames>MohammadReza</forenames></author><author><keyname>Rizvandi</keyname><forenames>Nikzad Babaii</forenames></author><author><keyname>Zomaya</keyname><forenames>Albert Y.</forenames></author></authors><title>Data-Intensive Workload Consolidation on Hadoop Distributed File System</title><categories>cs.DC</categories><comments>Published at IEEE Grid 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Workload consolidation, sharing physical resources among multiple workloads,
is a promising technique to save cost and energy in cluster computing systems.
This paper highlights a few challenges of workload consolidation for Hadoop as
one of the current state-of-the-art data-intensive cluster computing system.
Through a systematic step-by-step procedure, we investigate challenges for
efficient server consolidation in Hadoop environments. To this end, we first
investigate the inter-relationship between last level cache (LLC) contention
and throughput degradation for consolidated workloads on a single physical
server employing Hadoop distributed file system (HDFS). We then investigate the
general case of consolidation on multiple physical servers so that their
throughput never falls below a desired/predefined utilization level. We use our
empirical results to model consolidation as a classic two-dimensional bin
packing problem and then design a computationally efficient greedy algorithm to
achieve minimum throughput degradation on multiple servers. Results are very
promising and show that our greedy approach is able to achieve near optimal
solution in all experimented cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7274</identifier>
 <datestamp>2014-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7274</id><created>2013-03-28</created><updated>2014-10-07</updated><authors><author><keyname>Petersen</keyname><forenames>Alexander M.</forenames></author><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author><author><keyname>Pan</keyname><forenames>Raj K.</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author><author><keyname>Penner</keyname><forenames>Orion</forenames></author><author><keyname>Rungi</keyname><forenames>Armando</forenames></author><author><keyname>Riccaboni</keyname><forenames>Massimo</forenames></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames></author><author><keyname>Pammolli</keyname><forenames>Fabio</forenames></author></authors><title>Reputation and Impact in Academic Careers</title><categories>physics.soc-ph cs.DL physics.data-an</categories><comments>Final published version of the main manuscript including additional
  analysis: 9 pages, 4 figures, 1 table, and full reference list, including
  those in the Supplementary Information. For the SI Appendix, see
  http://physics.bu.edu/~amp17/webpage_files/MyPapers/Reputation_SI.pdf</comments><journal-ref>Proceedings of the National Academy of Sciences 111, 15316-15321
  (2014)</journal-ref><doi>10.1073/pnas.1323111111</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reputation is an important social construct in science, which enables
informed quality assessments of both publications and careers of scientists in
the absence of complete systemic information. However, the relation between
reputation and career growth of an individual remains poorly understood,
despite recent proliferation of quantitative research evaluation methods. Here
we develop an original framework for measuring how a publication's citation
rate $\Delta c$ depends on the reputation of its central author $i$, in
addition to its net citation count $c$. To estimate the strength of the
reputation effect, we perform a longitudinal analysis on the careers of 450
highly-cited scientists, using the total citations $C_{i}$ of each scientist as
his/her reputation measure. We find a citation crossover $c_{\times}$ which
distinguishes the strength of the reputation effect. For publications with $c &lt;
c_{\times}$, the author's reputation is found to dominate the annual citation
rate. Hence, a new publication may gain a significant early advantage
corresponding to roughly a 66% increase in the citation rate for each tenfold
increase in $C_{i}$. However, the reputation effect becomes negligible for
highly cited publications meaning that for $c\geq c_{\times}$ the citation rate
measures scientific impact more transparently. In addition we have developed a
stochastic reputation model, which is found to reproduce numerous statistical
observations for real careers, thus providing insight into the microscopic
mechanisms underlying cumulative advantage in science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7286</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7286</id><created>2013-03-28</created><updated>2014-01-22</updated><authors><author><keyname>Nielsen</keyname><forenames>Frank</forenames></author></authors><title>On the symmetrical Kullback-Leibler Jeffreys centroids</title><categories>cs.IT cs.LG math.IT stat.ML</categories><comments>17 pages, 1 figure, source code in R</comments><journal-ref>IEEE Signal Processing Letters (Volume:20 , Issue: 7 ), pp.
  657-660, 2013</journal-ref><doi>10.1109/LSP.2013.2260538</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the success of the bag-of-word modeling paradigm, clustering
histograms has become an important ingredient of modern information processing.
Clustering histograms can be performed using the celebrated $k$-means
centroid-based algorithm. From the viewpoint of applications, it is usually
required to deal with symmetric distances. In this letter, we consider the
Jeffreys divergence that symmetrizes the Kullback-Leibler divergence, and
investigate the computation of Jeffreys centroids. We first prove that the
Jeffreys centroid can be expressed analytically using the Lambert $W$ function
for positive histograms. We then show how to obtain a fast guaranteed
approximation when dealing with frequency histograms. Finally, we conclude with
some remarks on the $k$-means histogram clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7287</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7287</id><created>2013-03-28</created><authors><author><keyname>Stojnic</keyname><forenames>Mihailo</forenames></author></authors><title>A rigorous geometry-probability equivalence in characterization of
  $\ell_1$-optimization</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider under-determined systems of linear equations that
have sparse solutions. This subject attracted enormous amount of interest in
recent years primarily due to influential works \cite{CRT,DonohoPol}. In a
statistical context it was rigorously established for the first time in
\cite{CRT,DonohoPol} that if the number of equations is smaller than but still
linearly proportional to the number of unknowns then a sparse vector of
sparsity also linearly proportional to the number of unknowns can be recovered
through a polynomial $\ell_1$-optimization algorithm (of course, this assuming
that such a sparse solution vector exists). Moreover, the geometric approach of
\cite{DonohoPol} produced the exact values for the proportionalities in
question. In our recent work \cite{StojnicCSetam09} we introduced an
alternative statistical approach that produced attainable values of the
proportionalities. Those happened to be in an excellent numerical agreement
with the ones of \cite{DonohoPol}. In this paper we give a rigorous analytical
confirmation that the results of \cite{StojnicCSetam09} indeed match those from
\cite{DonohoPol}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7288</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7288</id><created>2013-03-28</created><updated>2014-05-28</updated><authors><author><keyname>Zhao</keyname><forenames>Zhongyuan</forenames></author><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Wang</keyname><forenames>Wenbo</forenames></author></authors><title>A Full-Diversity Beamforming Scheme in Two-Way Amplified-and-Forward
  Relay Systems</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to the unprecise
  proof of lemma 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a simple two-way relaying channel where two single-antenna sources
exchange information via a multiple-antenna relay. To such a scenario, all the
existing works which can achieve full diversity order are based on the
antenna/relay selection, where the difficulty to design the beamforming lies in
the fact that a single beamformer needs to serve two destinations. In this
paper, we propose a new full-diversity beamforming scheme which ensures that
the relay signals are coherently combined at both destinations. Both analytical
and numerical results are provided to demonstrate that this proposed scheme can
outperform the existing one based on the antenna selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7289</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7289</id><created>2013-03-28</created><authors><author><keyname>Stojnic</keyname><forenames>Mihailo</forenames></author></authors><title>Upper-bounding $\ell_1$-optimization weak thresholds</title><categories>cs.IT math.IT math.OC</categories><comments>arXiv admin note: text overlap with arXiv:0907.3666</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our recent work \cite{StojnicCSetam09} we considered solving
under-determined systems of linear equations with sparse solutions. In a large
dimensional and statistical context we proved that if the number of equations
in the system is proportional to the length of the unknown vector then there is
a sparsity (number of non-zero elements of the unknown vector) also
proportional to the length of the unknown vector such that a polynomial
$\ell_1$-optimization technique succeeds in solving the system. We provided
lower bounds on the proportionality constants that are in a solid numerical
agreement with what one can observe through numerical experiments. Here we
create a mechanism that can be used to derive the upper bounds on the
proportionality constants. Moreover, the upper bounds obtained through such a
mechanism match the lower bounds from \cite{StojnicCSetam09} and ultimately
make the latter ones optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7291</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7291</id><created>2013-03-28</created><authors><author><keyname>Stojnic</keyname><forenames>Mihailo</forenames></author></authors><title>A framework to characterize performance of LASSO algorithms</title><categories>cs.IT math.IT math.OC math.PR math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider solving \emph{noisy} under-determined systems of
linear equations with sparse solutions. A noiseless equivalent attracted
enormous attention in recent years, above all, due to work of
\cite{CRT,CanRomTao06,DonohoPol} where it was shown in a statistical and large
dimensional context that a sparse unknown vector (of sparsity proportional to
the length of the vector) can be recovered from an under-determined system via
a simple polynomial $\ell_1$-optimization algorithm. \cite{CanRomTao06} further
established that even when the equations are \emph{noisy}, one can, through an
SOCP noisy equivalent of $\ell_1$, obtain an approximate solution that is (in
an $\ell_2$-norm sense) no further than a constant times the noise from the
sparse unknown vector. In our recent works
\cite{StojnicCSetam09,StojnicUpper10}, we created a powerful mechanism that
helped us characterize exactly the performance of $\ell_1$ optimization in the
noiseless case (as shown in \cite{StojnicEquiv10} and as it must be if the
axioms of mathematics are well set, the results of
\cite{StojnicCSetam09,StojnicUpper10} are in an absolute agreement with the
corresponding exact ones from \cite{DonohoPol}). In this paper we design a
mechanism, as powerful as those from \cite{StojnicCSetam09,StojnicUpper10},
that can handle the analysis of a LASSO type of algorithm (and many others)
that can be (or typically are) used for &quot;solving&quot; noisy under-determined
systems. Using the mechanism we then, in a statistical context, compute the
exact worst-case $\ell_2$ norm distance between the unknown sparse vector and
the approximate one obtained through such a LASSO. The obtained results match
the corresponding exact ones obtained in \cite{BayMon10,DonMalMon10}. Moreover,
as a by-product of our analysis framework we recognize existence of an SOCP
type of algorithm that achieves the same performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7295</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7295</id><created>2013-03-29</created><authors><author><keyname>Stojnic</keyname><forenames>Mihailo</forenames></author></authors><title>Regularly random duality</title><categories>cs.IT math.IT math.OC math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we look at a class of random optimization problems. We discuss
ways that can help determine typical behavior of their solutions. When the
dimensions of the optimization problems are large such an information often can
be obtained without actually solving the original problems. Moreover, we also
discover that fairly often one can actually determine many quantities of
interest (such as, for example, the typical optimal values of the objective
functions) completely analytically. We present a few general ideas and
emphasize that the range of applications is enormous.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7296</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7296</id><created>2013-03-29</created><authors><author><keyname>Venugopal</keyname><forenames>Kiran</forenames></author><author><keyname>Namboodiri</keyname><forenames>Vishnu</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>On Constellations for Physical Layer Network Coded Two-Way Relaying</title><categories>cs.IT math.IT</categories><comments>9 pages, 17 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modulation schemes for two-way bidirectional relay network employing two
phases: Multiple access (MA) phase and Broadcast (BC) phase and using physical
layer network coding are currently studied intensively. Recently, adaptive
modulation schemes using Latin Squares to obtain network coding maps with the
denoise and forward protocol have been reported with good end-to-end
performance. These schemes work based on avoiding the detrimental effects of
distance shortening in the effective receive constellation at the end of the MA
phase at the relay. The channel fade states that create such distance
shortening called singular fade states, are effectively removed using
appropriate Latin squares. This scheme as well as all other known schemes
studied so far use conventional regular PSK or QAM signal sets for the end
users which lead to the relay using different sized constellations for the BC
phase depending upon the fade state. In this work, we propose a 4-point signal
set that would always require a 4-ary constellation for the BC phase for all
the channel fade conditions. We also propose an 8-point constellation that
gives better SER performance (gain of 1 dB) than 8-PSK while still using 8-ary
constellation for BC phase like the case with 8-PSK. This is in spite of the
fact that the proposed 8-point signal set has more number of singular fade
states than for 8-PSK.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7300</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7300</id><created>2013-03-29</created><authors><author><keyname>Madhavi</keyname><forenames>Giddaluru</forenames></author><author><keyname>Kaushik</keyname><forenames>M. K.</forenames></author></authors><title>Queuing Methodology Based Power Efficient Routing Protocol for Reliable
  Data Communications in Manets</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mobile ad hoc network (MANET) is a wireless network that uses multi-hop
peer-to- peer routing instead of static network infrastructure to provide
network connectivity. MANETs have applications in rapidly deployed and dynamic
military and civilian systems. The network topology in a MANET usually changes
with time. Therefore, there are new challenges for routing protocols in MANETs
since traditional routing protocols may not be suitable for MANETs. In recent
years, a variety of new routing protocols targeted specifically at this
environment have been developed, but little performance information on each
protocol and no realistic performance comparison between them is available.
This paper presents the results of a detailed packet-level simulation comparing
three multi-hop wireless ad hoc network routing protocols that cover a range of
design choices: DSR, NFPQR, and clustered NFPQR. By applying queuing
methodology to the introduced routing protocol the reliability and throughput
of the network is increased.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7310</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7310</id><created>2013-03-29</created><authors><author><keyname>Kumar</keyname><forenames>Niraj</forenames></author><author><keyname>Gangadharaiah</keyname><forenames>Rashmi</forenames></author><author><keyname>Srinathan</keyname><forenames>Kannan</forenames></author><author><keyname>Varma</keyname><forenames>Vasudeva</forenames></author></authors><title>Exploring the Role of Logically Related Non-Question Phrases for
  Answering Why-Questions</title><categories>cs.CL cs.IR</categories><comments>Got accepted in NLDB-2013; as Paper ID: 23; Title: &quot;Exploring the
  Role of Logically Related Non-Question Phrases for Answering Why-Questions&quot;,
  Withdrawn</comments><acm-class>H.3.m</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we show that certain phrases although not present in a given
question/query, play a very important role in answering the question. Exploring
the role of such phrases in answering questions not only reduces the dependency
on matching question phrases for extracting answers, but also improves the
quality of the extracted answers. Here matching question phrases means phrases
which co-occur in given question and candidate answers. To achieve the above
discussed goal, we introduce a bigram-based word graph model populated with
semantic and topical relatedness of terms in the given document. Next, we apply
an improved version of ranking with a prior-based approach, which ranks all
words in the candidate document with respect to a set of root words (i.e.
non-stopwords present in the question and in the candidate document). As a
result, terms logically related to the root words are scored higher than terms
that are not related to the root words. Experimental results show that our
devised system performs better than state-of-the-art for the task of answering
Why-questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7326</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7326</id><created>2013-03-29</created><authors><author><keyname>Accattoli</keyname><forenames>Beniamino</forenames><affiliation>INRIA &amp; Ecole Polytechnique</affiliation></author></authors><title>Proof nets and the call-by-value lambda-calculus</title><categories>cs.LO cs.PL</categories><comments>In Proceedings LSFA 2012, arXiv:1303.7136</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 113, 2013, pp. 11-26</journal-ref><doi>10.4204/EPTCS.113.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives a detailed account of the relationship between (a variant
of) the call-by-value lambda calculus and linear logic proof nets. The
presentation is carefully tuned in order to realize a strong bisimulation
between the two systems: every single rewriting step on the calculus maps to a
single step on the nets, and viceversa. In this way, we obtain an algebraic
reformulation of proof nets. Moreover, we provide a simple correctness
criterion for our proof nets, which employ boxes in an unusual way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7327</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7327</id><created>2013-03-29</created><authors><author><keyname>Areces</keyname><forenames>Carlos</forenames><affiliation>FaMAF - Universidad Nacional de C&#xf3;rdoba, CONICET</affiliation></author><author><keyname>Hoffmann</keyname><forenames>Guillaume</forenames><affiliation>FaMAF - Universidad Nacional de C&#xf3;rdoba</affiliation></author><author><keyname>Orbe</keyname><forenames>Ezequiel</forenames><affiliation>FaMAF - Universidad Nacional de C&#xf3;rdoba, CONICET</affiliation></author></authors><title>Symmetries in Modal Logics</title><categories>cs.LO cs.AI</categories><comments>In Proceedings LSFA 2012, arXiv:1303.7136</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 113, 2013, pp. 27-44</journal-ref><doi>10.4204/EPTCS.113.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the notion of symmetries of propositional formulas in
conjunctive normal form to modal formulas. Our framework uses the coinductive
models and, hence, the results apply to a wide class of modal logics including,
for example, hybrid logics. Our main result shows that the symmetries of a
modal formula preserve entailment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7328</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7328</id><created>2013-03-29</created><authors><author><keyname>Ayala-Rinc&#xf3;n</keyname><forenames>Mauricio</forenames><affiliation>Universidade de Bras&#xed;lia</affiliation></author><author><keyname>Fern&#xe1;ndez</keyname><forenames>Maribel</forenames><affiliation>King's College London</affiliation></author><author><keyname>Nantes-Sobrinho</keyname><forenames>Daniele</forenames><affiliation>Universidade de Bras&#xed;lia</affiliation></author></authors><title>Elementary Deduction Problem for Locally Stable Theories with Normal
  Forms</title><categories>cs.LO cs.CC cs.CR</categories><comments>In Proceedings LSFA 2012, arXiv:1303.7136</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 113, 2013, pp. 45-60</journal-ref><doi>10.4204/EPTCS.113.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm to decide the intruder deduction problem (IDP) for a
class of locally stable theories enriched with normal forms. Our result relies
on a new and efficient algorithm to solve a restricted case of higher-order
associative-commutative matching, obtained by combining the Distinct
Occurrences of AC- matching algorithm and a standard algorithm to solve systems
of linear Diophantine equations. A translation between natural deduction and
sequent calculus allows us to use the same approach to decide the
\emphelementary deduction problem for locally stable theories. As an
application, we model the theory of blind signatures and derive an algorithm to
decide IDP in this context, extending previous decidability results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7329</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7329</id><created>2013-03-29</created><authors><author><keyname>Bucciarelli</keyname><forenames>Antonio</forenames><affiliation>PPS, Universit&#xe9; Paris Diderot</affiliation></author><author><keyname>Carraro</keyname><forenames>Alberto</forenames><affiliation>PPS, Universit&#xe9; Paris Diderot</affiliation></author><author><keyname>Salibra</keyname><forenames>Antonino</forenames><affiliation>DAIS, Universit&#xe0; Ca' Foscari Venezia</affiliation></author></authors><title>Minimal lambda-theories by ultraproducts</title><categories>cs.LO</categories><comments>In Proceedings LSFA 2012, arXiv:1303.7136</comments><proxy>EPTCS</proxy><acm-class>F.4.1; F.3.2</acm-class><journal-ref>EPTCS 113, 2013, pp. 61-76</journal-ref><doi>10.4204/EPTCS.113.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A longstanding open problem in lambda calculus is whether there exist
continuous models of the untyped lambda calculus whose theory is exactly the
least lambda-theory lambda-beta or the least sensible lambda-theory H
(generated by equating all the unsolvable terms). A related question is
whether, given a class of lambda models, there is a minimal lambda-theory
represented by it. In this paper, we give a general tool to answer positively
to this question and we apply it to a wide class of webbed models: the
i-models. The method then applies also to graph models, Krivine models,
coherent models and filter models. In particular, we build an i-model whose
theory is the set of equations satisfied in all i-models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7330</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7330</id><created>2013-03-29</created><authors><author><keyname>Carraro</keyname><forenames>Alberto</forenames><affiliation>PPS, Universit&#xe9; Paris Diderot</affiliation></author></authors><title>The untyped stack calculus and Bohm's theorem</title><categories>cs.LO</categories><comments>In Proceedings LSFA 2012, arXiv:1303.7136</comments><proxy>EPTCS</proxy><acm-class>F.4.1; F.3.2</acm-class><journal-ref>EPTCS 113, 2013, pp. 77-92</journal-ref><doi>10.4204/EPTCS.113.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stack calculus is a functional language in which is in a Curry-Howard
correspondence with classical logic. It enjoys confluence but, as well as
Parigot's lambda-mu, does not admit the Bohm Theorem, typical of the
lambda-calculus. We present a simple extension of stack calculus which is for
the stack calculus what Saurin's Lambda-mu is for lambda-mu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7331</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7331</id><created>2013-03-29</created><authors><author><keyname>Carraro</keyname><forenames>Alberto</forenames><affiliation>PPS, Universit&#xe9; Paris Diderot</affiliation></author><author><keyname>Ehrhard</keyname><forenames>Thomas</forenames><affiliation>PPS, Universit&#xe9; Paris Diderot</affiliation></author><author><keyname>Salibra</keyname><forenames>Antonino</forenames><affiliation>DAIS, Universit&#xe0; Ca' Foscari Venezia</affiliation></author></authors><title>The stack calculus</title><categories>cs.LO</categories><comments>In Proceedings LSFA 2012, arXiv:1303.7136</comments><proxy>EPTCS</proxy><acm-class>F.4.1; F.3.2</acm-class><journal-ref>EPTCS 113, 2013, pp. 93-108</journal-ref><doi>10.4204/EPTCS.113.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a functional calculus with simple syntax and operational
semantics in which the calculi introduced so far in the Curry-Howard
correspondence for Classical Logic can be faithfully encoded. Our calculus
enjoys confluence without any restriction. Its type system enforces strong
normalization of expressions and it is a sound and complete system for full
implicational Classical Logic. We give a very simple denotational semantics
which allows easy calculations of the interpretation of expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7332</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7332</id><created>2013-03-29</created><authors><author><keyname>Ciaffaglione</keyname><forenames>Alberto</forenames><affiliation>Dipartimento di Matematica e Informatica, Universit&#xe0; di Udine, Italia</affiliation></author><author><keyname>Scagnetto</keyname><forenames>Ivan</forenames><affiliation>Dipartimento di Matematica e Informatica, Universit&#xe0; di Udine, Italia</affiliation></author></authors><title>A weak HOAS approach to the POPLmark Challenge</title><categories>cs.LO cs.PL</categories><comments>In Proceedings LSFA 2012, arXiv:1303.7136</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 113, 2013, pp. 109-124</journal-ref><doi>10.4204/EPTCS.113.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Capitalizing on previous encodings and formal developments about nominal
calculi and type systems, we propose a weak Higher-Order Abstract Syntax
formalization of the type language of pure System F&lt;: within Coq, a proof
assistant based on the Calculus of Inductive Constructions.
  Our encoding allows us to accomplish the proof of the transitivity property
of algorithmic subtyping, which is in fact the first of the three tasks stated
by the POPLmark Challenge, a set of problems that capture the most critical
issues in formalizing programming language metatheory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7333</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7333</id><created>2013-03-29</created><authors><author><keyname>Coniglio</keyname><forenames>Marcelo E.</forenames><affiliation>State University of Campinas</affiliation></author><author><keyname>Corbal&#xe1;n</keyname><forenames>Mar&#xed;a I.</forenames><affiliation>State University of Campinas</affiliation></author></authors><title>Sequent Calculi for the classical fragment of Bochvar and Halld\'en's
  Nonsense Logics</title><categories>cs.LO</categories><comments>In Proceedings LSFA 2012, arXiv:1303.7136</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 113, 2013, pp. 125-136</journal-ref><doi>10.4204/EPTCS.113.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper sequent calculi for the classical fragment (that is, the
conjunction-disjunction-implication-negation fragment) of the nonsense logics
B3, introduced by Bochvar, and H3, introduced by Halld\'en, are presented.
These calculi are obtained by restricting in an appropriate way the application
of the rules of a sequent calculus for classical propositional logic CPL. The
nice symmetry between the provisos in the rules reveal the semantical
relationship between these logics. The Soundness and Completeness theorems for
both calculi are obtained, as well as the respective Cut elimination theorems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7334</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7334</id><created>2013-03-29</created><authors><author><keyname>D&#xed;az-Caro</keyname><forenames>Alejandro</forenames></author><author><keyname>Dowek</keyname><forenames>Gilles</forenames></author></authors><title>Non determinism through type isomorphism</title><categories>cs.LO</categories><comments>In Proceedings LSFA 2012, arXiv:1303.7136</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 113, 2013, pp. 137-144</journal-ref><doi>10.4204/EPTCS.113.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define an equivalence relation on propositions and a proof system where
equivalent propositions have the same proofs. The system obtained this way
resembles several known non-deterministic and algebraic lambda-calculi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7335</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7335</id><created>2013-03-29</created><authors><author><keyname>Oliveira</keyname><forenames>Ana Cristina Rocha</forenames><affiliation>Universidade de Brasilia</affiliation></author><author><keyname>Ayala-Rinc&#xf3;n</keyname><forenames>Mauricio</forenames><affiliation>Universidade de Brasilia</affiliation></author></authors><title>Formalizing the Confluence of Orthogonal Rewriting Systems</title><categories>cs.LO cs.AI cs.PL</categories><comments>In Proceedings LSFA 2012, arXiv:1303.7136</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 113, 2013, pp. 145-152</journal-ref><doi>10.4204/EPTCS.113.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orthogonality is a discipline of programming that in a syntactic manner
guarantees determinism of functional specifications. Essentially, orthogonality
avoids, on the one side, the inherent ambiguity of non determinism, prohibiting
the existence of different rules that specify the same function and that may
apply simultaneously (non-ambiguity), and, on the other side, it eliminates the
possibility of occurrence of repetitions of variables in the left-hand side of
these rules (left linearity). In the theory of term rewriting systems (TRSs)
determinism is captured by the well-known property of confluence, that
basically states that whenever different computations or simplifications from a
term are possible, the computed answers should coincide. Although the proofs
are technically elaborated, confluence is well-known to be a consequence of
orthogonality. Thus, orthogonality is an important mathematical discipline
intrinsic to the specification of recursive functions that is naturally applied
in functional programming and specification. Starting from a formalization of
the theory of TRSs in the proof assistant PVS, this work describes how
confluence of orthogonal TRSs has been formalized, based on axiomatizations of
properties of rules, positions and substitutions involved in parallel steps of
reduction, in this proof assistant. Proofs for some similar but restricted
properties such as the property of confluence of non-ambiguous and (left and
right) linear TRSs have been fully formalized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7336</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7336</id><created>2013-03-29</created><authors><author><keyname>Veloso</keyname><forenames>Paulo A. S.</forenames><affiliation>COPPE-UFRJ</affiliation></author><author><keyname>Veloso</keyname><forenames>Sheila R. M.</forenames><affiliation>FEN-UERJ</affiliation></author></authors><title>A Graph Calculus for Predicate Logic</title><categories>cs.LO</categories><comments>In Proceedings LSFA 2012, arXiv:1303.7136</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 113, 2013, pp. 153-168</journal-ref><doi>10.4204/EPTCS.113.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a refutation graph calculus for classical first-order predicate
logic, which is an extension of previous ones for binary relations. One reduces
logical consequence to establishing that a constructed graph has empty
extension, i. e. it represents bottom. Our calculus establishes that a graph
has empty extension by converting it to a normal form, which is expanded to
other graphs until we can recognize conflicting situations (equivalent to a
formula and its negation).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7353</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7353</id><created>2013-03-29</created><authors><author><keyname>Sharma</keyname><forenames>Nisha</forenames></author><author><keyname>Sharma</keyname><forenames>Kamlesh</forenames></author></authors><title>A Modified LSB Technique of Digital Watermarking in Spatial Domain</title><categories>cs.MM</categories><comments>4 pages, 2 images, Proceedings International Conference on Innovative
  Technologies Reserch &amp; Development in Science, Technology and Management
  organized by PDM collage of Engg &amp; IEEE-IMS/EMB(Delhi), 2009. arXiv admin
  note: text overlap with arXiv:0802.3746 by other authors without attribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital watermarking is a technique of embedding pieces of information into
digital data such as text, audio, video, and still images that can be detected
or extracted later to show authentication about the data. Watermark is hidden
information in the image(s) and is so designed that it does not degrade/distort
the quality of the image and still keeps the information. Digital watermarking
is basically to protect ownership rights and to control of making illicit
copies of digital data. In this paper, we have discussed various watermarking
techniques and properties and have proposed a modified LSB technique. We have
implemented the proposed technique by following: 2-bits of 8-bit gray image is
replaced by luminance part, next 2-bits by red component, next 2-bits by green
component and next 2-bits by blue component of 32-bit image using secret key.
The advantage is that watermarking capacity has been increased and unaffected
by various attacks e.g. zero out LSB bits, cropping etc. Watermark image is
imperceptible in resultant image. We have tested this technique on several
images and found that it is quite satisfactory. This technique is secured as
unauthorized user can not extract the watermarked contents easily from the
original image and works well in adverse situations. We have implemented this
technique on platform java 1.5.0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7361</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7361</id><created>2013-03-29</created><authors><author><keyname>Fu</keyname><forenames>Zhiguo</forenames></author><author><keyname>Yang</keyname><forenames>Fengqin</forenames></author></authors><title>Holographaic Alogorithms on Bases of Rank 2</title><categories>cs.CC</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An essential problem in the design of holographic algorithms is to decide
whether the required signatures can be realized by matchgates under a suitable
basis transformation (SRP). For holographic algorithms on domain size 2, [1, 2,
4, 5] have built a systematical theory. In this paper, we reduce SRP on domain
size k&gt;2 to SRP on domain size 2 for holographic algorithms on bases of rank 2.
Furthermore, we generalize the collapse theorem of [3] to domain size k&gt;2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7377</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7377</id><created>2013-03-29</created><authors><author><keyname>Gaur</keyname><forenames>Vibha</forenames></author><author><keyname>Sharma</keyname><forenames>Neeraj Kumar</forenames></author><author><keyname>Bedi</keyname><forenames>Punam</forenames></author></authors><title>Evaluating Reputation Systems for Agent Mediated e-Commerce</title><categories>cs.MA</categories><comments>5 pages. arXiv admin note: text overlap with arXiv:1110.3961</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agent mediated e-commerce involves buying and selling on Internet through
software agents. The success of an agent mediated e-commerce system lies in the
underlying reputation management system which is used to improve the quality of
services in e-market environment. A reputation system encourages the honest
behaviour of seller agents and discourages the malicious behaviour of dishonest
seller agents in the e-market where actual traders never meet each other. This
paper evaluates various reputation systems for assigning reputation rating to
software agents acting on behalf of buyers and sellers in e-market. These
models are analysed on the basis of a number of features viz. reputation
computation and their defence mechanisms against different attacks. To address
the problems of traditional reputation systems which are relatively static in
nature, this paper identifies characteristics of a dynamic reputation framework
which ensures judicious use of information sharing for inter-agent cooperation
and also associates the reputation of an agent with the value of a transaction
so that the market approaches an equilibrium state and dishonest agents are
weeded out of the market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7378</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7378</id><created>2013-03-29</created><updated>2014-12-02</updated><authors><author><keyname>Gupta</keyname><forenames>Ashutosh</forenames></author><author><keyname>Popeea</keyname><forenames>Corneliu</forenames></author><author><keyname>Rybalchenko</keyname><forenames>Andrey</forenames></author></authors><title>Generalised Interpolation by Solving Recursion-Free Horn Clauses</title><categories>cs.LO</categories><comments>In Proceedings HCVS 2014, arXiv:1412.0825</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 169, 2014, pp. 31-38</journal-ref><doi>10.4204/EPTCS.169.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present InterHorn, a solver for recursion-free Horn clauses.
The main application domain of InterHorn lies in solving interpolation problems
arising in software verification. We show how a range of interpolation
problems, including path, transition, nested, state/transition and well-founded
interpolation can be handled directly by InterHorn. By detailing these
interpolation problems and their Horn clause representations, we hope to
encourage the emergence of a common back-end interpolation interface useful for
diverse verification tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7379</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7379</id><created>2013-03-29</created><authors><author><keyname>Barnat</keyname><forenames>Jiri</forenames></author><author><keyname>Bauch</keyname><forenames>Petr</forenames></author></authors><title>Control Explicit---Data Symbolic Model Checking: An Introduction</title><categories>cs.SE</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A comprehensive verification of parallel software imposes three crucial
requirements on the procedure that implements it. Apart from accepting real
code as program input and temporal formulae as specification input, the
verification should be exhaustive, with respect to both control and data flows.
This paper is concerned with the third requirement, proposing to combine
explicit model checking to handle the control with symbolic set representations
to handle the data. The combination of explicit and symbolic approaches is
first investigated theoretically and we report the requirements on the symbolic
representation and the changes to the model checking process the combination
entails. The feasibility and efficiency of the combination is demonstrated on a
case study using the DVE modelling language and we report a marked improvement
in scalability compared to previous solutions. The results described in this
paper show the potential to meet all three requirements for automatic
verification in a single procedure combining explicit model checking with
symbolic set representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7390</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7390</id><created>2013-03-29</created><updated>2013-04-08</updated><authors><author><keyname>Feragen</keyname><forenames>Aasa</forenames></author><author><keyname>Petersen</keyname><forenames>Jens</forenames></author><author><keyname>Grimm</keyname><forenames>Dominik</forenames></author><author><keyname>Dirksen</keyname><forenames>Asger</forenames></author><author><keyname>Pedersen</keyname><forenames>Jesper Holst</forenames></author><author><keyname>Borgwardt</keyname><forenames>Karsten</forenames></author><author><keyname>de Bruijne</keyname><forenames>Marleen</forenames></author></authors><title>Geometric tree kernels: Classification of COPD from airway tree geometry</title><categories>cs.CV</categories><comments>12 pages</comments><msc-class>68T10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Methodological contributions: This paper introduces a family of kernels for
analyzing (anatomical) trees endowed with vector valued measurements made along
the tree. While state-of-the-art graph and tree kernels use combinatorial
tree/graph structure with discrete node and edge labels, the kernels presented
in this paper can include geometric information such as branch shape, branch
radius or other vector valued properties. In addition to being flexible in
their ability to model different types of attributes, the presented kernels are
computationally efficient and some of them can easily be computed for large
datasets (N of the order 10.000) of trees with 30-600 branches. Combining the
kernels with standard machine learning tools enables us to analyze the relation
between disease and anatomical tree structure and geometry. Experimental
results: The kernels are used to compare airway trees segmented from low-dose
CT, endowed with branch shape descriptors and airway wall area percentage
measurements made along the tree. Using kernelized hypothesis testing we show
that the geometric airway trees are significantly differently distributed in
patients with Chronic Obstructive Pulmonary Disease (COPD) than in healthy
individuals. The geometric tree kernels also give a significant increase in the
classification accuracy of COPD from geometric tree structure endowed with
airway wall thickness measurements in comparison with state-of-the-art methods,
giving further insight into the relationship between airway wall thickness and
COPD. Software: Software for computing kernels and statistical tests is
available at http://image.diku.dk/aasa/software.php.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7397</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7397</id><created>2013-03-29</created><authors><author><keyname>Kordy</keyname><forenames>Barbara</forenames></author><author><keyname>Pi&#xe8;tre-Cambac&#xe9;d&#xe8;s</keyname><forenames>Ludovic</forenames></author><author><keyname>Schweitzer</keyname><forenames>Patrick</forenames></author></authors><title>DAG-Based Attack and Defense Modeling: Don't Miss the Forest for the
  Attack Trees</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the current state of the art on attack and defense
modeling approaches that are based on directed acyclic graphs (DAGs). DAGs
allow for a hierarchical decomposition of complex scenarios into simple, easily
understandable and quantifiable actions. Methods based on threat trees and
Bayesian networks are two well-known approaches to security modeling. However
there exist more than 30 DAG-based methodologies, each having different
features and goals. The objective of this survey is to present a complete
overview of graphical attack and defense modeling techniques based on DAGs.
This consists of summarizing the existing methodologies, comparing their
features and proposing a taxonomy of the described formalisms. This article
also supports the selection of an adequate modeling technique depending on user
requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7425</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7425</id><created>2013-03-29</created><authors><author><keyname>Gastineau</keyname><forenames>Mickael</forenames></author><author><keyname>Laskar</keyname><forenames>Jacques</forenames></author></authors><title>Highly Scalable Multiplication for Distributed Sparse Multivariate
  Polynomials on Many-core Systems</title><categories>cs.SC astro-ph.IM cs.DC cs.MS</categories><comments>15 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a highly scalable algorithm for multiplying sparse multivariate
polynomials represented in a distributed format. This algo- rithm targets not
only the shared memory multicore computers, but also computers clusters or
specialized hardware attached to a host computer, such as graphics processing
units or many-core coprocessors. The scal- ability on the large number of cores
is ensured by the lacks of synchro- nizations, locks and false-sharing during
the main parallel step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7427</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7427</id><created>2013-03-29</created><authors><author><keyname>Chambers</keyname><forenames>Erin Wolf</forenames></author><author><keyname>Wang</keyname><forenames>Yusu</forenames></author></authors><title>Measuring Similarity Between Curves on 2-Manifolds via Homotopy Area</title><categories>cs.CG</categories><comments>25 pages, full version of SoCG 2013 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring the similarity of curves is a fundamental problem arising in many
application fields. There has been considerable interest in several such
measures, both in Euclidean space and in more general setting such as curves on
Riemannian surfaces or curves in the plane minus a set of obstacles. However,
so far, efficiently computable similarity measures for curves on general
surfaces remain elusive. This paper aims at developing a natural curve
similarity measure that can be easily extended and computed for curves on
general orientable 2-manifolds. Specifically, we measure similarity between
homotopic curves based on how hard it is to deform one curve into the other one
continuously, and define this &quot;hardness&quot; as the minimum possible surface area
swept by a homotopy between the curves. We consider cases where curves are
embedded in the plane or on a triangulated orientable surface with genus $g$,
and we present efficient algorithms (which are either quadratic or near linear
time, depending on the setting) for both cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7430</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7430</id><created>2013-03-29</created><updated>2013-04-01</updated><authors><author><keyname>Stefanoni</keyname><forenames>Giorgio</forenames></author><author><keyname>Motik</keyname><forenames>Boris</forenames></author><author><keyname>Horrocks</keyname><forenames>Ian</forenames></author></authors><title>Introducing Nominals to the Combined Query Answering Approaches for EL</title><categories>cs.AI cs.DB cs.LO</categories><comments>Extended version of a paper to appear on AAAI-13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  So-called combined approaches answer a conjunctive query over a description
logic ontology in three steps: first, they materialise certain consequences of
the ontology and the data; second, they evaluate the query over the data; and
third, they filter the result of the second phase to eliminate unsound answers.
Such approaches were developed for various members of the DL-Lite and the EL
families of languages, but none of them can handle ontologies containing
nominals. In our work, we bridge this gap and present a combined query
answering approach for ELHO---a logic that contains all features of the OWL 2
EL standard apart from transitive roles and complex role inclusions. This
extension is nontrivial because nominals require equality reasoning, which
introduces complexity into the first and the third step. Our empirical
evaluation suggests that our technique is suitable for practical application,
and so it provides a practical basis for conjunctive query answering in a large
fragment of OWL 2 EL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7434</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7434</id><created>2013-03-29</created><authors><author><keyname>Shi</keyname><forenames>Feng</forenames></author><author><keyname>Mucha</keyname><forenames>Peter J.</forenames></author><author><keyname>Durrett</keyname><forenames>Rick</forenames></author></authors><title>A multi-opinion evolving voter model with infinitely many phase
  transitions</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI math.PR nlin.AO</categories><comments>15 pages, 6 figures</comments><doi>10.1103/PhysRevE.88.062818</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an idealized model in which individuals' changing opinions and
their social network coevolve, with disagreements between neighbors in the
network resolved either through one imitating the opinion of the other or by
reassignment of the discordant edge. Specifically, an interaction between $x$
and one of its neighbors $y$ leads to $x$ imitating $y$ with probability
$(1-\alpha)$ and otherwise (i.e., with probability $\alpha$) $x$ cutting its
tie to $y$ in order to instead connect to a randomly chosen individual.
Building on previous work about the two-opinion case, we study the
multiple-opinion situation, finding that the model has infinitely many phase
transitions. Moreover, the formulas describing the end states of these
processes are remarkably simple when expressed as a function of $\beta =
\alpha/(1-\alpha)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7435</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7435</id><created>2013-03-29</created><authors><author><keyname>Bennett</keyname><forenames>Charles H.</forenames></author><author><keyname>Riedel</keyname><forenames>C. Jess</forenames></author></authors><title>On the security of key distribution based on Johnson-Nyquist noise</title><categories>quant-ph cs.CR cs.IT math.IT</categories><comments>4 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We point out that arguments for the security of Kish's noise-based
cryptographic protocol have relied on an unphysical no-wave limit, which if
taken seriously would prevent any correlation from developing between the
users. We introduce a noiseless version of the protocol, also having illusory
security in the no-wave limit, to show that noise and thermodynamics play no
essential role. Then we prove generally that classical electromagnetic
protocols cannot establish a secret key between two parties separated by a
spacetime region perfectly monitored by an eavesdropper. We note that the
original protocol of Kish is vulnerable to passive time-correlation attacks
even in the quasi-static limit. Finally we show that protocols of this type can
be secure in practice against an eavesdropper with noisy monitoring equipment.
In this case the security is a straightforward consequence of Maurer and Wolf's
discovery that key can be distilled by public discussion from correlated random
variables in a wide range of situations where the eavesdropper's noise is at
least partly independent from the users' noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7445</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7445</id><created>2013-03-29</created><authors><author><keyname>Khan</keyname><forenames>Saad Ahmad</forenames></author><author><keyname>Boloni</keyname><forenames>Ladislau</forenames></author></authors><title>Agent-based modeling of a price information trading business</title><categories>cs.AI q-fin.GN</categories><comments>Extended version of the paper published at Computer and Information
  Sciences, Proc. of ISCIS-26, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an agent-based simulation of a fictional (but feasible)
information trading business. The Gas Price Information Trader (GPIT) buys
information about real-time gas prices in a metropolitan area from drivers and
resells the information to drivers who need to refuel their vehicles.
  Our simulation uses real world geographic data, lifestyle-dependent driving
patterns and vehicle models to create an agent-based model of the drivers. We
use real world statistics of gas price fluctuation to create scenarios of
temporal and spatial distribution of gas prices. The price of the information
is determined on a case-by-case basis through a simple negotiation model. The
trader and the customers are adapting their negotiation strategies based on
their historical profits.
  We are interested in the general properties of the emerging information
market: the amount of realizable profit and its distribution between the trader
and customers, the business strategies necessary to keep the market operational
(such as promotional deals), the price elasticity of demand and the impact of
pricing strategies on the profit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7454</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7454</id><created>2013-03-29</created><authors><author><keyname>Christopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Ottersten</keyname><forenames>Bjorn</forenames></author></authors><title>Constructive Interference in Linear Precoding Systems: Power Allocation
  and User Selection</title><categories>cs.IT math.IT</categories><comments>7 pages, 6 figures, Conference Paper submitted to IEEE GlobeCom 2013
  conf</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The exploitation of interference in a constructive manner has recently been
proposed for the downlink of multiuser, multi-antenna transmitters. This novel
linear precoding technique, herein referred to as constructive interference
zero forcing (CIZF) precoding, has exhibited substantial gains over
conventional approaches; the concept is to cancel, on a symbol-by-symbol basis,
only the interfering users that do not add to the intended signal power. In
this paper, the power allocation problem towards maximizing the performance of
a CIZF system with respect to some metric (throughput or fairness) is
investigated. What is more, it is shown that the performance of the novel
precoding scheme can be further boosted by choosing some of the constructive
multiuser interference terms in the precoder design. Finally, motivated by the
significant effect of user selection on conventional, zero forcing (ZF)
precoding, the problem of user selection for the novel precoding method is
tackled. A new iterative, low complexity algorithm for user selection in CIZF
is developed. Simulation results are provided to display the gains of the
algorithm compared to known user selection approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7455</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7455</id><created>2013-03-29</created><authors><author><keyname>Lim</keyname><forenames>Lek-Heng</forenames></author></authors><title>Self-concordance is NP-hard</title><categories>math.OC cs.CC</categories><msc-class>15A69, 68Q17, 90C25, 90C51, 90C60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an elementary proof of a somewhat curious result, namely, that
deciding whether a convex function is self-concordant is in general an
intractable problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7457</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7457</id><created>2013-03-29</created><authors><author><keyname>Sukumar</keyname><forenames>Suraj</forenames></author></authors><title>Computational Analysis of Modified Blom's Scheme</title><categories>cs.CR</categories><comments>12 pages. arXiv admin note: substantial text overlap with
  arXiv:1103.5712 by other authors without attribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To achieve security in wireless networks, it is important to be able to
encrypt and authenticate messages sent among sensor nodes. Due to resource
constraints, achieving such key agreement in wireless sensor networks in
nontrivial. Blom's scheme is a symmetric key exchange protocol used in
cryptography. In this paper, we propose a new key pre-distribution scheme by
modifying Blom's scheme which reduces the computational complexity as well as
memory usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7459</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7459</id><created>2013-03-29</created><updated>2013-04-15</updated><authors><author><keyname>Smith</keyname><forenames>James</forenames></author></authors><title>State/event based versus purely Action or State based Logics</title><categories>cs.LO</categories><comments>9 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although less studied than purely action or state based logics, state/event
based logics are becoming increasingly important. Some systems are best studied
using structures with information on both states and transitions, and it is
these structures over which state/event based logics are defined. The logic
UCTL and its variants are perhaps the most widely studied and implemented of
these logics to date. As yet, however, no-one seems to have defined UCTL*, a
trivial step but a worthwhile one. Here we do just that, but prove in the cases
of both UCTL and UCTL* that these logics are no more expressive than their more
commonplace fragments. Also, acknowledging the importance of modal transition
systems, we define a state/event based logic over a modified modal transition
system as a precursor to further work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7460</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7460</id><created>2013-03-29</created><authors><author><keyname>Ernvall-Hyt&#xf6;nen</keyname><forenames>Anne-Maria</forenames></author></authors><title>Some results related to the conjecture by Belfiore and Sol\'e</title><categories>cs.IT math.IT math.NT</categories><comments>This paper contains the note http://arxiv.org/abs/1209.3573. However,
  there are several new results, including the results concerning a conjecture
  by Elkies</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the first part of the paper, we consider the relation between kissing
number and the secrecy gain. We show that on an $n=24m+8k$-dimensional even
unimodular lattice, if the shortest vector length is $\geq 2m$, then as the
number of vectors of length $2m$ decreases, the secrecy gain increases. We will
also prove a similar result on general unimodular lattices. We will also
consider the situations with shorter vectors. Furthermore, assuming the
conjecture by Belfiore and Sol\'e, we will calculate the difference between
inverses of secrecy gains as the number of vectors varies. We will show by an
example that there exist two lattices in the same dimension with the same
shortest vector length and the same kissing number, but different secrecy
gains. Finally, we consider some cases of a question by Elkies by providing an
answer for a special class of lattices assuming the conjecture of Belfiore and
Sol\'e. We will also get a conditional improvement on some Gaulter's results
concerning the conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7461</identifier>
 <datestamp>2014-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7461</id><created>2013-03-29</created><updated>2014-01-28</updated><authors><author><keyname>Mont&#xfa;far</keyname><forenames>Guido F.</forenames></author></authors><title>Universal Approximation Depth and Errors of Narrow Belief Networks with
  Discrete Units</title><categories>stat.ML cs.LG math.PR</categories><comments>19 pages, 5 figures, 1 table</comments><msc-class>82C32, 60C05, 68Q32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize recent theoretical work on the minimal number of layers of
narrow deep belief networks that can approximate any probability distribution
on the states of their visible units arbitrarily well. We relax the setting of
binary units (Sutskever and Hinton, 2008; Le Roux and Bengio, 2008, 2010;
Mont\'ufar and Ay, 2011) to units with arbitrary finite state spaces, and the
vanishing approximation error to an arbitrary approximation error tolerance.
For example, we show that a $q$-ary deep belief network with $L\geq
2+\frac{q^{\lceil m-\delta \rceil}-1}{q-1}$ layers of width $n \leq m +
\log_q(m) + 1$ for some $m\in \mathbb{N}$ can approximate any probability
distribution on $\{0,1,\ldots,q-1\}^n$ without exceeding a Kullback-Leibler
divergence of $\delta$. Our analysis covers discrete restricted Boltzmann
machines and na\&quot;ive Bayes models as special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7462</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7462</id><created>2013-03-29</created><updated>2016-02-20</updated><authors><author><keyname>Smith</keyname><forenames>James</forenames></author></authors><title>Concur: An Algorithm for Merging Concurrent Changes without Conflicts</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose you and I are both editing a document. You make and change and I make
a change, concurrently. Now if we want to still be seeing the same document
then I need to apply your change after mine and you mine after yours. But we
can't just apply them willy-nilly. I must amend yours somehow and you mine. If
my change is written D, yours d, and the amended changes D.d and d.D, we get
*D*D.d=*d*d.D as long as applying is written * and we don't care about what
we're applying the changes to. We start by proving this identity for single
changes and finish by proving it for many.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7467</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7467</id><created>2013-03-29</created><updated>2013-06-21</updated><authors><author><keyname>Freemon</keyname><forenames>D. Michael</forenames></author></authors><title>Optimizing Throughput on Guaranteed-Bandwidth WAN Networks for the Large
  Synoptic Survey Telescope (LSST)</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Large Synoptic Survey Telescope (LSST) is a proposed 8.4-meter telescope
that will be located in the Andes mountains in Chile. Every 17 seconds, a 6.4
GB image is transferred to Illinois for immediate processing. That transfer
needs to complete within approximately five seconds. LSST is provisioning an
international WAN with a 10Gbps bandwidth guarantee for this and other
project-related data transfers. The stringent latency requirement drives a
re-examination of TCP congestion control for this use case. Specifically, prior
work on dedicated Long Fat Networks (LFNs) does not go far enough in fully
leveraging the opportunity provided by the bandwidth guarantee. This paper
presents an approach for how optimal network throughput can be obtained for the
LSST use case, and the conditions under which any project can achieve data
throughput rates on long-distance networks approaching wire speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1303.7474</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1303.7474</id><created>2013-03-29</created><authors><author><keyname>Anderson</keyname><forenames>Matthew</forenames></author><author><keyname>Fu</keyname><forenames>Geng-Shen</forenames></author><author><keyname>Phlypo</keyname><forenames>Ronald</forenames></author><author><keyname>Adal&#x131;</keyname><forenames>T&#xfc;lay</forenames></author></authors><title>Independent Vector Analysis: Identification Conditions and Performance
  Bounds</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>14 pages, 5 figures, in review for IEEE Trans. on Signal Processing</comments><doi>10.1109/TSP.2014.2333554</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, an extension of independent component analysis (ICA) from one to
multiple datasets, termed independent vector analysis (IVA), has been the
subject of significant research interest. IVA has also been shown to be a
generalization of Hotelling's canonical correlation analysis. In this paper, we
provide the identification conditions for a general IVA formulation, which
accounts for linear, nonlinear, and sample-to-sample dependencies. The
identification conditions are a generalization of previous results for ICA and
for IVA when samples are independently and identically distributed.
Furthermore, a principal aim of IVA is the identification of dependent sources
between datasets. Thus, we provide the additional conditions for when the
arbitrary ordering of the sources within each dataset is common. Performance
bounds in terms of the Cramer-Rao lower bound are also provided for the
demixing matrices and interference to source ratio. The performance of two IVA
algorithms are compared to the theoretical bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0001</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0001</id><created>2013-03-28</created><authors><author><keyname>Stojnic</keyname><forenames>Mihailo</forenames></author></authors><title>Optimality of $\ell_2/\ell_1$-optimization block-length dependent
  thresholds</title><categories>cs.IT math.IT math.OC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1303.7289, and
  text overlap with arXiv:0907.3679</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent work of \cite{CRT,DonohoPol} rigorously proved (in a large
dimensional and statistical context) that if the number of equations
(measurements in the compressed sensing terminology) in the system is
proportional to the length of the unknown vector then there is a sparsity
(number of non-zero elements of the unknown vector) also proportional to the
length of the unknown vector such that $\ell_1$-optimization algorithm succeeds
in solving the system. In more recent papers
\cite{StojnicCSetamBlock09,StojnicICASSP09block,StojnicJSTSP09} we considered
under-determined systems with the so-called \textbf{block}-sparse solutions. In
a large dimensional and statistical context in \cite{StojnicCSetamBlock09} we
determined lower bounds on the values of allowable sparsity for any given
number (proportional to the length of the unknown vector) of equations such
that an $\ell_2/\ell_1$-optimization algorithm succeeds in solving the system.
These lower bounds happened to be in a solid numerical agreement with what one
can observe through numerical experiments. Here we derive the corresponding
upper bounds. Moreover, the upper bounds that we obtain in this paper match the
lower bounds from \cite{StojnicCSetamBlock09} and ultimately make them optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0002</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0002</id><created>2013-03-29</created><authors><author><keyname>Stojnic</keyname><forenames>Mihailo</forenames></author></authors><title>A performance analysis framework for SOCP algorithms in noisy compressed
  sensing</title><categories>cs.IT math.IT math.OC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1303.7291</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving under-determined systems of linear equations with sparse solutions
attracted enormous amount of attention in recent years, above all, due to work
of \cite{CRT,CanRomTao06,DonohoPol}. In \cite{CRT,CanRomTao06,DonohoPol} it was
rigorously shown for the first time that in a statistical and large dimensional
context a linear sparsity can be recovered from an under-determined system via
a simple polynomial $\ell_1$-optimization algorithm. \cite{CanRomTao06} went
even further and established that in \emph{noisy} systems for any linear level
of under-determinedness there is again a linear sparsity that can be
\emph{approximately} recovered through an SOCP (second order cone programming)
noisy equivalent to $\ell_1$. Moreover, the approximate solution is (in an
$\ell_2$-norm sense) guaranteed to be no further from the sparse unknown vector
than a constant times the noise. In this paper we will also consider solving
\emph{noisy} linear systems and present an alternative statistical framework
that can be used for their analysis. To demonstrate how the framework works we
will show how one can use it to precisely characterize the approximation error
of a wide class of SOCP algorithms. We will also show that our theoretical
predictions are in a solid agrement with the results one can get through
numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0003</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0003</id><created>2013-03-29</created><authors><author><keyname>Stojnic</keyname><forenames>Mihailo</forenames></author></authors><title>Meshes that trap random subspaces</title><categories>cs.IT math.IT math.OC math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our recent work \cite{StojnicCSetam09,StojnicUpper10} we considered
solving under-determined systems of linear equations with sparse solutions. In
a large dimensional and statistical context we proved results related to
performance of a polynomial $\ell_1$-optimization technique when used for
solving such systems. As one of the tools we used a probabilistic result of
Gordon \cite{Gordon88}. In this paper we revisit this classic result in its
core form and show how it can be reused to in a sense prove its own optimality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0004</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0004</id><created>2013-03-29</created><updated>2015-07-16</updated><authors><author><keyname>Stojnic</keyname><forenames>Mihailo</forenames></author></authors><title>Linear under-determined systems with sparse solutions: Redirecting a
  challenge?</title><categories>cs.IT math.IT math.OC</categories><comments>acknowledgement footnote added arXiv admin note: text overlap with
  arXiv:1303.7289</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Seminal works \cite{CRT,DonohoUnsigned,DonohoPol} generated a massive
interest in studying linear under-determined systems with sparse solutions. In
this paper we give a short mathematical overview of what was accomplished in
last 10 years in a particular direction of such a studying. We then discuss
what we consider were the main challenges in last 10 years and give our own
view as to what are the main challenges that lie ahead. Through the
presentation we arrive to a point where the following natural rhetoric question
arises: is it a time to redirect the main challenges? While we can not provide
the answer to such a question we hope that our small discussion will stimulate
further considerations in this direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0012</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0012</id><created>2013-03-29</created><authors><author><keyname>Power</keyname><forenames>Russell</forenames></author></authors><title>Using Memory-Protection to Simplify Zero-copy Operations</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High performance networks (e.g. Infiniband) rely on zero-copy operations for
performance. Zero-copy operations, as the name implies, avoid copying buffers
for sending and receiving data. Instead, hardware devices directly read and
write to application specified areas of memory. Since these networks can send
and receive at nearly the same speed as the memory bus inside machines,
zero-copy operations are necessary to achieve peak performance for many
applications.
  Unfortunately, programming with zero-copy APIs is a *giant pain*. Users must
carefully avoid using buffers that may be accessed by a device. Typically this
either results in spaghetti code (where every access to a buffer is checked
before usage), or blocking operations (which pretty much defeat the whole point
of zero-copy).
  We show that by abusing memory protection hardware, we can offer the best of
both worlds: a simple zero-copy mechanism which allows for non-blocking send
and receives while protecting against incorrect accesses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0018</identifier>
 <datestamp>2015-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0018</id><created>2013-03-29</created><authors><author><keyname>Antulov-Fantulin</keyname><forenames>Nino</forenames></author><author><keyname>Lancic</keyname><forenames>Alen</forenames></author><author><keyname>Stefancic</keyname><forenames>Hrvoje</forenames></author><author><keyname>Sikic</keyname><forenames>Mile</forenames></author><author><keyname>Smuc</keyname><forenames>Tomislav</forenames></author></authors><title>Statistical inference framework for source detection of contagion
  processes on arbitrary network structures</title><categories>cs.SI physics.soc-ph</categories><doi>10.1109/SASOW.2014.35</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a statistical inference framework for estimating
the contagion source from a partially observed contagion spreading process on
an arbitrary network structure. The framework is based on a maximum likelihood
estimation of a partial epidemic realization and involves large scale
simulation of contagion spreading processes from the set of potential source
locations. We present a number of different likelihood estimators that are used
to determine the conditional probabilities associated to observing partial
epidemic realization with particular source location candidates. This
statistical inference framework is also applicable for arbitrary compartment
contagion spreading processes on networks. We compare estimation accuracy of
these approaches in a number of computational experiments performed with the
SIR (susceptible-infected-recovered), SI (susceptible-infected) and ISS
(ignorant-spreading-stifler) contagion spreading models on synthetic and
real-world complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0019</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0019</id><created>2013-03-29</created><authors><author><keyname>Shewaye</keyname><forenames>Tizita Nesibu</forenames></author></authors><title>Age group and gender recognition from human facial images</title><categories>cs.CV</categories><comments>8 pages, October, 2012</comments><journal-ref>Ethiopian Society of Electrical Engineers 6th Scientific
  Conference on Electrical Engineering (CEE-2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents an automatic human gender and age group recognition system
based on human facial images. It makes an extensive experiment with row pixel
intensity valued features and Discrete Cosine Transform (DCT) coefficient
features with Principal Component Analysis and k-Nearest Neighbor
classification to identify the best recognition approach. The final results
show approaches using DCT coefficient outperform their counter parts resulting
in a 99% correct gender recognition rate and 68% correct age group recognition
rate (considering four distinct age groups) in unseen test images. Detailed
experimental settings and obtained results are clearly presented and explained
in this report.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0023</identifier>
 <datestamp>2014-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0023</id><created>2013-03-29</created><updated>2014-09-23</updated><authors><author><keyname>Loxley</keyname><forenames>Peter</forenames></author></authors><title>The two-dimensional Gabor function adapted to natural image statistics:
  An analytical model of simple-cell responses in the early visual system</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The two-dimensional Gabor function is adapted to natural image statistics by
learning the joint distribution of the Gabor function parameters. The joint
distribution is then approximated to yield an analytical model of simple-cell
receptive fields. Adapting a basis of Gabor functions is found to take an order
of magnitude less computation than learning an equivalent non-parameterized
basis. Derived learning rules are shown to be capable of adapting Gabor
parameters to the statistics of images of man-made and natural environments.
Learning is found to be most pronounced in three Gabor parameters that
represent the size, aspect-ratio, and spatial frequency of the two-dimensional
Gabor function. These three parameters are characterized by non-uniform
marginal distributions with heavy tails -- most likely due to scale invariance
in natural images -- and all three parameters are strongly correlated:
resulting in a basis of multiscale Gabor functions with similar aspect-ratios,
and size-dependent spatial frequencies. The Gabor orientation and phase
parameters do not appear to gain anything from learning over natural images.
Different tuning strategies are found by controlling learning through the Gabor
parameter learning rates. Two opposing strategies include well-resolved
orientation and well-resolved spatial frequency. On image reconstruction, a
basis of Gabor functions with fitted marginal distributions is shown to
significantly outperform a basis of Gabor functions generated from uniformly
sampled parameters. An additional increase in performance results when the
strong correlations are included. However, the best analytical model does not
yet achieve the performance of the learned model. A comparison with estimates
for biological simple cells shows that the Gabor function adapted to natural
image statistics correctly predicts some key receptive field properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0030</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0030</id><created>2013-03-29</created><authors><author><keyname>Levin</keyname><forenames>Mark Sh.</forenames></author></authors><title>Note on Combinatorial Engineering Frameworks for Hierarchical Modular
  Systems</title><categories>math.OC cs.AI cs.SY</categories><comments>11 pages, 7 figures, 3 tables</comments><msc-class>68T20, 90C27, 90C39, 90C59, 90C90, 90-02, 93B50, 93B51, 93-2, 93A13</msc-class><acm-class>G.1.6; G.2.1; G.2.3; G.4; H.1.1; I.2.8; J.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper briefly describes a basic set of special combinatorial engineering
frameworks for solving complex problems in the field of hierarchical modular
systems. The frameworks consist of combinatorial problems (and corresponding
models), which are interconnected/linked (e.g., by preference relation).
Mainly, hierarchical morphological system model is used. The list of basic
standard combinatorial engineering (technological) frameworks is the following:
(1) design of system hierarchical model, (2) combinatorial synthesis
('bottom-up' process for system design), (3) system evaluation, (4) detection
of system bottlenecks, (5) system improvement (re-design, upgrade), (6)
multi-stage design (design of system trajectory), (7) combinatorial modeling of
system evolution/development and system forecasting. The combinatorial
engineering frameworks are targeted to maintenance of some system life cycle
stages. The list of main underlaying combinatorial optimization problems
involves the following: knapsack problem, multiple-choice problem, assignment
problem, spanning trees, morphological clique problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0035</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0035</id><created>2013-03-29</created><authors><author><keyname>Chen</keyname><forenames>Po-Yu</forenames></author><author><keyname>Selesnick</keyname><forenames>Ivan W.</forenames></author></authors><title>Translation-Invariant Shrinkage/Thresholding of Group Sparse Signals</title><categories>cs.CV cs.LG cs.SD</categories><comments>33 pages, 7 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses signal denoising when large-amplitude coefficients form
clusters (groups). The L1-norm and other separable sparsity models do not
capture the tendency of coefficients to cluster (group sparsity). This work
develops an algorithm, called 'overlapping group shrinkage' (OGS), based on the
minimization of a convex cost function involving a group-sparsity promoting
penalty function. The groups are fully overlapping so the denoising method is
translation-invariant and blocking artifacts are avoided. Based on the
principle of majorization-minimization (MM), we derive a simple iterative
minimization algorithm that reduces the cost function monotonically. A
procedure for setting the regularization parameter, based on attenuating the
noise to a specified level, is also described. The proposed approach is
illustrated on speech enhancement, wherein the OGS approach is applied in the
short-time Fourier transform (STFT) domain. The denoised speech produced by OGS
does not suffer from musical noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0036</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0036</id><created>2013-03-29</created><updated>2015-03-13</updated><authors><author><keyname>Reeb</keyname><forenames>David</forenames></author><author><keyname>Wolf</keyname><forenames>Michael M.</forenames></author></authors><title>Tight bound on relative entropy by entropy difference</title><categories>quant-ph cond-mat.stat-mech cs.IT math.IT</categories><comments>v2: 27 pages, 1 figure, gap in proof of Theorem 1 fixed, other minor
  changes, references updated; v3: 27 pages, 1 figure, small changes and
  improvements, one-column version of published paper</comments><journal-ref>IEEE Trans. Inf. Theory 61, 1458-1473 (2015)</journal-ref><doi>10.1109/TIT.2014.2387822</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a lower bound on the relative entropy between two finite-dimensional
states in terms of their entropy difference and the dimension of the underlying
space. The inequality is tight in the sense that equality can be attained for
any prescribed value of the entropy difference, both for quantum and classical
systems. We outline implications for information theory and thermodynamics,
such as a necessary condition for a process to be close to thermodynamic
reversibility, or an easily computable lower bound on the classical channel
capacity. Furthermore, we derive a tight upper bound, uniform for all states of
a given dimension, on the variance of the surprisal, whose thermodynamic
meaning is that of heat capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0053</identifier>
 <datestamp>2014-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0053</id><created>2013-03-29</created><updated>2014-08-05</updated><authors><author><keyname>Neary</keyname><forenames>Turlough</forenames></author><author><keyname>Woods</keyname><forenames>Damien</forenames></author><author><keyname>Murphy</keyname><forenames>Niall</forenames></author><author><keyname>Glaschick</keyname><forenames>Rainer</forenames></author></authors><title>Wang's B machines are efficiently universal, as is Hasenjaeger's small
  universal electromechanical toy</title><categories>cs.CC cs.DS</categories><comments>18 pages, 1 figure, 1 table, Conference: Turing in context II -
  History and Philosophy of Computing, 2012</comments><msc-class>68Q05,</msc-class><acm-class>F.1.1</acm-class><journal-ref>Journal of Complexity, Volume 30, Issue 5, October 2014, pages
  634-646</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the 1960's Gisbert Hasenjaeger built Turing Machines from
electromechanical relays and uniselectors. Recently, Glaschick reverse
engineered the program of one of these machines and found that it is a
universal Turing machine. In fact, its program uses only four states and two
symbols, making it a very small universal Turing machine. (The machine has
three tapes and a number of other features that are important to keep in mind
when comparing it to other small universal machines.) Hasenjaeger's machine
simulates Hao Wang's B machines, which were proved universal by Wang.
Unfortunately, Wang's original simulation algorithm suffers from an exponential
slowdown when simulating Turing machines. Hence, via this simulation,
Hasenjaeger's machine also has an exponential slowdown when simulating Turing
machines. In this work, we give a new efficient simulation algorithm for Wang's
B machines by showing that they simulate Turing machines with only a polynomial
slowdown. As a second result, we find that Hasenjaeger's machine also
efficiently simulates Turing machines in polynomial time. Thus, Hasenjaeger's
machine is both small and fast. In another application of our result, we show
that Hooper's small universal Turing machine simulates Turing machines in
polynomial time, an exponential improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0055</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0055</id><created>2013-03-29</created><authors><author><keyname>Khanafer</keyname><forenames>Ali</forenames></author><author><keyname>Touri</keyname><forenames>Behrouz</forenames></author><author><keyname>Ba&#x15f;ar</keyname><forenames>Tamer</forenames></author></authors><title>Robust Distributed Averaging on Networks with Adversarial Intervention</title><categories>math.OC cs.SY</categories><comments>8 pages, 2 figures, submitted to CDC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the interaction between a network designer and an adversary over a
dynamical network. The network consists of nodes performing continuous-time
distributed averaging. The goal of the network designer is to assist the nodes
reach consensus by changing the weights of a limited number of links in the
network. Meanwhile, an adversary strategically disconnects a set of links to
prevent the nodes from converging. We formulate two problems to describe this
competition where the order in which the players act is reversed in the two
problems. We utilize Pontryagin's Maximum Principle (MP) to tackle both
problems and derive the optimal strategies. Although the canonical equations
provided by the MP are intractable, we provide an alternative characterization
for the optimal strategies that highlights a connection with potential theory.
Finally, we provide a sufficient condition for the existence of a saddle-point
equilibrium (SPE) for this zero-sum game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0062</identifier>
 <datestamp>2013-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0062</id><created>2013-03-29</created><updated>2013-12-29</updated><authors><author><keyname>Shi</keyname><forenames>Qingjiang</forenames></author><author><keyname>Liu</keyname><forenames>Liang</forenames></author><author><keyname>Xu</keyname><forenames>Weiqiang</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Joint Transmit Beamforming and Receive Power Splitting for MISO SWIPT
  Systems</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE TWC for second review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a multi-user multiple-input single-output (MISO) downlink
system for simultaneous wireless information and power transfer (SWIPT), in
which a set of single-antenna mobile stations (MSs) receive information and
energy simultaneously via power splitting (PS) from the signal sent by a
multi-antenna base station (BS). We aim to minimize the total transmission
power at BS by jointly designing transmit beamforming vectors and receive PS
ratios for all MSs under their given signal-to-interference-plus-noise ratio
(SINR) constraints for information decoding and harvested power constraints for
energy harvesting. First, we derive the sufficient and necessary condition for
the feasibility of our formulated problem. Next, we solve this non-convex
problem by applying the technique of semidefinite relaxation (SDR). We prove
that SDR is indeed tight for our problem and thus achieves its global optimum.
Finally, we propose two suboptimal solutions of lower complexity than the
optimal solution based on the principle of separating the optimization of
transmit beamforming and receive PS, where the zero-forcing (ZF) and the
SINR-optimal based transmit beamforming schemes are applied, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0076</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0076</id><created>2013-03-30</created><authors><author><keyname>Furtmueller</keyname><forenames>Florian G.</forenames></author></authors><title>An Approach to Secure Mobile Enterprise Architectures</title><categories>cs.NI cs.CR</categories><comments>8 pages, 9 figures, tutorial paper</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 10,
  Issue 1, No 1, January 2013, ISSN (Print): 1694-0784 | ISSN (Online):
  1694-0814</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to increased security awareness of enterprises for mobile applications
operating with sensitive or personal data as well as extended regulations form
legislative (the principle of proportionality) various approaches, how to
implement (extended) two-factor authentication, multi-factor authentication or
virtual private network within enterprise mobile environments to ensure
delivery of secure applications, have been developed. Within mobile
applications it will not be sufficient to rely on security measures of the
individual components or interested parties, an overall concept of a security
solution has to be established which requires the interaction of several
technologies, standards and system components. These include the physical fuses
on the device itself as well as on the network layer (such as integrated
security components), security measures (such as employee agreements, contract
clauses), insurance coverage, but also software technical protection at the
application level (e.g. password protection, encryption, secure container). The
purpose of this paper is to summarize the challenges and practical successes,
providing best practices to fulfill appropriate risk coverage of mobile
applications. I present a use case, in order to proof the concept in actual
work settings, and to demonstrate the adaptability of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0084</identifier>
 <datestamp>2013-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0084</id><created>2013-03-30</created><updated>2013-10-30</updated><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>&#x141;\kacki</keyname><forenames>Jakub</forenames></author></authors><title>Faster Algorithms for Markov Decision Processes with Low Treewidth</title><categories>cs.DS cs.LO</categories><comments>Conference version will appear in CAV 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two core algorithmic problems for probabilistic verification: the
maximal end-component decomposition and the almost-sure reachability set
computation for Markov decision processes (MDPs). For MDPs with treewidth $k$,
we present two improved static algorithms for both the problems that run in
time $O(n \cdot k^{2.38} \cdot 2^k)$ and $O(m \cdot \log n \cdot k)$,
respectively, where $n$ is the number of states and $m$ is the number of edges,
significantly improving the previous known $O(n\cdot k \cdot \sqrt{n\cdot k})$
bound for low treewidth. We also present decremental algorithms for both
problems for MDPs with constant treewidth that run in amortized logarithmic
time, which is a huge improvement over the previously known algorithms that
require amortized linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0090</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0090</id><created>2013-03-30</created><authors><author><keyname>Azghadi</keyname><forenames>Mostafa Rahimi</forenames></author><author><keyname>Al-Sarawi</keyname><forenames>Said</forenames></author><author><keyname>Abbott</keyname><forenames>Derek</forenames></author><author><keyname>Iannella</keyname><forenames>Nicolangelo</forenames></author></authors><title>A Neuromorphic VLSI Design for Spike Timing and Rate Based Synaptic
  Plasticity</title><categories>cs.NE</categories><comments>Accepted for publication in Neural Networks and is allowed to be
  distributed on Arxiv, according to the journal website at
  http://www.elsevier.com/authors/author-rights-and-responsibilities
  **Voluntary posting of Accepted Author Manuscripts in the arXiv subject
  repository is permitted</comments><journal-ref>Mostafa Rahimi Azghadi, Said Al-Sarawi, Derek Abbott, Nicolangelo
  Iannella, A Neuromorphic VLSI Design for Spike Timing and Rate Based Synaptic
  Plasticity, Neural Networks (2013), 10.1016/j.neunet.2013.03.003</journal-ref><doi>10.1016/j.neunet.2013.03.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Triplet-based Spike Timing Dependent Plasticity (TSTDP) is a powerful
synaptic plasticity rule that acts beyond conventional pair-based STDP (PSTDP).
Here, the TSTDP is capable of reproducing the outcomes from a variety of
biological experiments, while the PSTDP rule fails to reproduce them.
Additionally, it has been shown that the behaviour inherent to the spike
rate-based Bienenstock-Cooper-Munro (BCM) synaptic plasticity rule can also
emerge from the TSTDP rule. This paper proposes an analog implementation of the
TSTDP rule. The proposed VLSI circuit has been designed using the AMS 0.35 um
CMOS process and has been simulated using design kits for Synopsys and Cadence
tools. Simulation results demonstrate how well the proposed circuit can alter
synaptic weights according to the timing difference amongst a set of different
patterns of spikes. Furthermore, the circuit is shown to give rise to a
BCM-like learning rule, which is a rate-based rule. To mimic implementation
environment, a 1000 run Monte Carlo (MC) analysis was conducted on the proposed
circuit. The presented MC simulation analysis and the simulation result from
fine-tuned circuits show that, it is possible to mitigate the effect of process
variations in the proof of concept circuit, however, a practical variation
aware design technique is required to promise a high circuit performance in a
large scale neural network. We believe that the proposed design can play a
significant role in future VLSI implementations of both spike timing and rate
based neuromorphic learning systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0100</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0100</id><created>2013-03-30</created><updated>2013-09-05</updated><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author><author><keyname>Sozzo</keyname><forenames>Sandro</forenames></author></authors><title>Entanglement Zoo I: Foundational and Structural Aspects</title><categories>cs.AI quant-ph</categories><comments>11 pages</comments><journal-ref>Quantum Interaction. Lecture Notes in Computer Science, 8369, pp.
  84-96, 2014</journal-ref><doi>10.1007/978-3-642-54943-4_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We put forward a general classification for a structural description of the
entanglement present in compound entities experimentally violating Bell's
inequalities, making use of a new entanglement scheme that we developed
recently. Our scheme, although different from the traditional one, is
completely compatible with standard quantum theory, and enables quantum
modeling in complex Hilbert space for different types of situations. Namely,
situations where entangled states and product measurements appear ('customary
quantum modeling'), and situations where states and measurements and evolutions
between measurements are entangled ('nonlocal box modeling', 'nonlocal
non-marginal box modeling'). The role played by Tsirelson's bound and marginal
distribution law is emphasized. Specific quantum models are worked out in
detail in complex Hilbert space within this new entanglement scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0102</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0102</id><created>2013-03-30</created><updated>2013-09-05</updated><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author><author><keyname>Sozzo</keyname><forenames>Sandro</forenames></author></authors><title>Entanglement Zoo II: Examples in Physics and Cognition</title><categories>cs.AI quant-ph</categories><comments>11 pages</comments><journal-ref>Quantum Interaction. Lecture Notes in Computer Science, 8369, pp.
  97-109, 2014</journal-ref><doi>10.1007/978-3-642-54943-4_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have recently presented a general scheme enabling quantum modeling of
different types of situations that violate Bell's inequalities. In this paper,
we specify this scheme for a combination of two concepts. We work out a quantum
Hilbert space model where 'entangled measurements' occur in addition to the
expected 'entanglement between the component concepts', or 'state
entanglement'. We extend this result to a macroscopic physical entity, the
'connected vessels of water', which maximally violates Bell's inequalities. We
enlighten the structural and conceptual analogies between the cognitive and
physical situations which are both examples of a nonlocal non-marginal box
modeling in our classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0104</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0104</id><created>2013-03-30</created><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author><author><keyname>Broekaert</keyname><forenames>Jan</forenames></author><author><keyname>Sozzo</keyname><forenames>Sandro</forenames></author><author><keyname>Veloz</keyname><forenames>Tomas</forenames></author></authors><title>Meaning-focused and Quantum-inspired Information Retrieval</title><categories>cs.IR cs.CL quant-ph</categories><comments>11 pages</comments><journal-ref>Quantum Interaction. Lecture Notes in Computer Science, 8369, pp.
  71-83, 2014</journal-ref><doi>10.1007/978-3-642-54943-4_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, quantum-based methods have promisingly integrated the
traditional procedures in information retrieval (IR) and natural language
processing (NLP). Inspired by our research on the identification and
application of quantum structures in cognition, more specifically our work on
the representation of concepts and their combinations, we put forward a
'quantum meaning based' framework for structured query retrieval in text
corpora and standardized testing corpora. This scheme for IR rests on
considering as basic notions, (i) 'entities of meaning', e.g., concepts and
their combinations and (ii) traces of such entities of meaning, which is how
documents are considered in this approach. The meaning content of these
'entities of meaning' is reconstructed by solving an 'inverse problem' in the
quantum formalism, consisting of reconstructing the full states of the entities
of meaning from their collapsed states identified as traces in relevant
documents. The advantages with respect to traditional approaches, such as
Latent Semantic Analysis (LSA), are discussed by means of concrete examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0110</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0110</id><created>2013-03-30</created><authors><author><keyname>Shayovitz</keyname><forenames>Shachar</forenames></author><author><keyname>Raphaeli</keyname><forenames>Dan</forenames></author></authors><title>A Signal Constellation for Pilotless Communications Over Wiener Phase
  Noise Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to Globecom 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many satellite communication systems operating today employ low cost
upconverters or downconverters which create phase noise. This noise can
severely limit the information rate of the system and pose a serious challenge
for the detection systems. Moreover, simple solutions for phase noise tracking
such as PLL either require low phase noise or otherwise require many pilot
symbols which reduce the effective data rate. In order to increase the
effective information rate, we propose a signal constellation which does not
require pilots, at all, in order to converge in the decoding process. In this
contribution, we will present a signal constellation which does not require
pilot sequences, but we require a signal that does not present rotational
symmetry. For example a simple MPSK cannot be used.Moreover, we will provide a
method to analyze the proposed constellations and provide a figure of merit for
their performance when iterative decoding algorithms are used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0116</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0116</id><created>2013-03-30</created><authors><author><keyname>Ralph</keyname><forenames>Paul</forenames></author></authors><title>The Illusion of Requirements in Software Development</title><categories>cs.SE</categories><comments>5 pages, 1 table, 1 figure; accepted for publication in Requirements
  Engineering: http://link.springer.com/article/10.1007%2Fs00766-012-0161-4</comments><acm-class>D.2.1</acm-class><doi>10.1007/s00766-012-0161-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is widely accepted that understanding system requirements is important for
software development project success. However, this paper presents two novel
challenges to the requirements concept. First, where many plausible approaches
to achieving a goal are evident, there may be insufficient overlap between
approaches to form requirements. Second, while all plausible approaches may
have sufficient overlap to state requirements, we cannot know that unless all
approaches are identified and we are sure that none have been missed. This
suggest that many, if not most, software projects may have too few requirements
drive the design process, and that analysts may misrepresent design decisions
as requirements to compensate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0133</identifier>
 <datestamp>2013-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0133</id><created>2013-03-30</created><updated>2013-06-08</updated><authors><author><keyname>Hussein</keyname><forenames>Ramy</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author></authors><title>Adaptive Energy-aware Encoding for DWT-Based Wireless EEG Monitoring
  System</title><categories>cs.IT math.IT</categories><comments>This paper has been modified to republished</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Electroencephalography (EEG) tele-monitoring systems performing
encoding and streaming over energy-hungry wireless channels are limited in
energy supply. However, excessive power consumption either in encoding or radio
channel may render some applications inapplicable. Hence, energy efficient
methods are needed to improve such applications. In this work, an embedded EEG
encoding system should be able to adjust its computational complexity, hence,
energy consumption according to the channel variations. To analyze the
distortion-compression ratio (PRD-CR) behavior of the wireless EEG system under
energy constraints, both encoding and transmission power should be taken into
consideration. In this paper, we propose a power-distortion- compression ratio
(P-PRD-CR) framework, which extends the traditional PRD-CR to P-PRD-CR model.
We analyze the computational complexity for a typical discrete wavelet
transform (DWT)-based encoding system. Using our developed P-PRD-CR framework,
the encoder effectively reconfigures the complexity control parameters to match
the energy constraints while retaining maximum reconstruction quality. Results
show that using the proposed framework, we can obtain higher reconstruction
accuracy for the same power constrained-portable device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0140</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0140</id><created>2013-03-30</created><authors><author><keyname>Foukalas</keyname><forenames>F.</forenames></author><author><keyname>Khattab</keyname><forenames>T.</forenames></author><author><keyname>Poor</keyname><forenames>H. V.</forenames></author></authors><title>Packet Relaying Control in Sensing-based Spectrum Sharing Systems</title><categories>cs.NI cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive relaying has been introduced for opportunistic spectrum access
systems by which a secondary node forwards primary packets whenever the primary
link faces an outage condition. For spectrum sharing systems, cognitive
relaying is parametrized by an interference power constraint level imposed on
the transmit power of the secondary user. For sensing-based spectrum sharing,
the probability of detection is also involved in packet relaying control. This
paper considers the choice of these two parameters so as to maximize the
secondary nodes' throughput under certain constraints. The analysis leads to a
Markov decision process using dynamic programming approach. The problem is
solved using value iteration. Finally, the structural properties of the
resulting optimal control are highlighted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0141</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0141</id><created>2013-03-30</created><updated>2013-09-19</updated><authors><author><keyname>De Leo</keyname><forenames>Vincenzo</forenames></author><author><keyname>Santoboni</keyname><forenames>Giovanni</forenames></author><author><keyname>Cerina</keyname><forenames>Federica</forenames></author><author><keyname>Mureddu</keyname><forenames>Mario</forenames></author><author><keyname>Secchi</keyname><forenames>Luca</forenames></author><author><keyname>Chessa</keyname><forenames>Alessandro</forenames></author></authors><title>Community core detection in transportation networks</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 13 figures</comments><doi>10.1103/PhysRevE.88.042810</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work analyses methods for the identification and the stability under
perturbation of a territorial community structure with specific reference to
transportation networks. We considered networks of commuters for a city and an
insular region. In both cases, we have studied the distribution of commuters'
trips (i.e., home-to-work trips and viceversa). The identification and
stability of the communities' cores are linked to the land-use distribution
within the zone system, and therefore their proper definition may be useful to
transport planners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0145</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0145</id><created>2013-03-30</created><authors><author><keyname>Kambhampati</keyname><forenames>Soumya C.</forenames></author><author><keyname>Liu</keyname><forenames>Thomas</forenames></author></authors><title>Phase Transition and Network Structure in Realistic SAT Problems</title><categories>cs.AI</categories><journal-ref>Published as student abstract in Proceedings of AAAI 2013
  (National Conference on Artificial Intelligence)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental question in Computer Science is understanding when a specific
class of problems go from being computationally easy to hard. Because of its
generality and applications, the problem of Boolean Satisfiability (aka SAT) is
often used as a vehicle for investigating this question. A signal result from
these studies is that the hardness of SAT problems exhibits a dramatic
easy-to-hard phase transition with respect to the problem constrainedness. Past
studies have however focused mostly on SAT instances generated using uniform
random distributions, where all constraints are independently generated, and
the problem variables are all considered of equal importance. These assumptions
are unfortunately not satisfied by most real problems. Our project aims for a
deeper understanding of hardness of SAT problems that arise in practice. We
study two key questions: (i) How does easy-to-hard transition change with more
realistic distributions that capture neighborhood sensitivity and
rich-get-richer aspects of real problems and (ii) Can these changes be
explained in terms of the network properties (such as node centrality and
small-worldness) of the clausal networks of the SAT problems. Our results,
based on extensive empirical studies and network analyses, provide important
structural and computational insights into realistic SAT problems. Our
extensive empirical studies show that SAT instances from realistic
distributions do exhibit phase transition, but the transition occurs sooner (at
lower values of constrainedness) than the instances from uniform random
distribution. We show that this behavior can be explained in terms of their
clausal network properties such as eigenvector centrality and small-worldness
(measured indirectly in terms of the clustering coefficients and average node
distance).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0153</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0153</id><created>2013-03-31</created><authors><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Khan</keyname><forenames>M. A.</forenames></author><author><keyname>Latif</keyname><forenames>K.</forenames></author><author><keyname>Javaid</keyname><forenames>A.</forenames></author></authors><title>On Energy Efficiency and Delay Minimization in Reactive Protocols in
  Wireless Multi-hop Networks</title><categories>cs.NI</categories><journal-ref>2nd IEEE Saudi International Electronics, Communications and
  Photonics Conference (SIECPC 13), 2013, Riyadh, Saudi Arabia</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Wireless Multi-hop Networks (WMhNs), routing protocols with energy
efficient and delay reduction techniques are needed to fulfill users demands.
In this paper, we present Linear Programming models (LP_models) to assess and
enhance reactive routing protocols. To practically examine constraints of
respective LP_models over reactive protocols, we select AODV, DSR and DYMO. It
is deduced from analytical simulations of LP_models in MATLAB that quick route
repair reduces routing latency and optimizations of retransmission attempts
results efficient energy utilization. To provide quick repair, we enhance AODV
and DSR. To practically examine the efficiency of enhanced protocols in
different scenarios of WMhNs, we conduct simulations using NS- 2. From
simulation results, enhanced DSR and AODV achieve efficient output by
optimizing routing latencies and routing load in terms of retransmission
attempts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0154</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0154</id><created>2013-03-31</created><authors><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Khan</keyname><forenames>M. A.</forenames></author><author><keyname>Latif</keyname><forenames>K.</forenames></author><author><keyname>Javaid</keyname><forenames>A.</forenames></author></authors><title>Towards LP Modeling for Maximizing Throughput and Minimizing Routing
  Delay in Proactive Protocols in Wireless Multi-hop Networks</title><categories>cs.NI</categories><journal-ref>2nd IEEE Saudi International Electronics, Communications and
  Photonics Conference (SIECPC 13), 2013, Riyadh, Saudi Arabia</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Multi-hop Networks (WMhNs) provide users with the facility to
communicate while moving with whatever the node speed, the node density and the
number of traffic flows they want, without any unwanted delay and/or
disruption. This paper contributes Linear Programming models (LP_models) for
WMhNs. In WMhNs, different routing protocols are used to facilitate users
demand(s). To practically examine constraints of respective LP_models over
different routing techniques, we select three proactive routing protocols;
Destination Sequence Distance Vector (DSDV), Fish-eye State Routing (FSR) and
Optimized Link State Routing (OLSR). These protocols are simulated in two
important scenarios regarding to user demands; mobilities and different network
flows. To evaluate the performance, we further relate the protocols strategy
effects on respective constraints in selected network scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0156</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0156</id><created>2013-03-31</created><authors><author><keyname>Rahman</keyname><forenames>Mohammad Ashekur</forenames></author><author><keyname>Barai</keyname><forenames>Atanu</forenames></author><author><keyname>Islam</keyname><forenames>Md. Asadul</forenames></author><author><keyname>Hashem</keyname><forenames>M. M. A</forenames></author></authors><title>Development of a Device for Remote Monitoring of Heart Rate and Body
  Temperature</title><categories>cs.OH</categories><journal-ref>Procs. of the IEEE 2012 15th International Conference on Computer
  &amp; Information Technology (ICCIT 2012), pp.411-416, Chittagong, Bangladesh,
  December 22-24, (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new integrated, portable device to provide a convenient solution
for remote monitoring heart rate at the fingertip and body temperature using
Ethernet technology and widely spreading internet. Now a days, heart related
disease is rising. Most of the times in these cases, patients may not realize
their actual conditions and even it is a common fact that there are no doctors
by their side, especially in rural areas, but now a days most of the diseases
are curable if detected in time.
  We have tried to make a system which may give information about one's
physical condition and help him or her to detect these deadly but curable
diseases. The system gives information of heart rate and body temperature
simultaneously acquired on the portable side in real time and transmits results
to web. In this system, the condition of heart and body temperature can be
monitored from remote places. Eventually, this device provides a low cost,
easily accessible human health monitor solution bridging the gaps between
patients and doctors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0160</identifier>
 <datestamp>2013-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0160</id><created>2013-03-31</created><updated>2013-12-25</updated><authors><author><keyname>Mondal</keyname><forenames>Nabarun</forenames></author><author><keyname>Ghosh</keyname><forenames>Partha P.</forenames></author></authors><title>Parallel Computation Is ESS</title><categories>cs.LG cs.AI cs.GT</categories><comments>Submitted to Theoretical Computer Science - Elsevier</comments><msc-class>68Q32, 68Q05, 68T05, 68Q10, 62P10, 97M60, 92D15, 91A80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are enormous amount of examples of Computation in nature, exemplified
across multiple species in biology. One crucial aim for these computations
across all life forms their ability to learn and thereby increase the chance of
their survival. In the current paper a formal definition of autonomous learning
is proposed. From that definition we establish a Turing Machine model for
learning, where rule tables can be added or deleted, but can not be modified.
Sequential and parallel implementations of this model are discussed. It is
found that for general purpose learning based on this model, the
implementations capable of parallel execution would be evolutionarily stable.
This is proposed to be of the reasons why in Nature parallelism in computation
is found in abundance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0166</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0166</id><created>2013-03-31</created><authors><author><keyname>Charpentier</keyname><forenames>Cl&#xe9;ment</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Sopena</keyname><forenames>Eric</forenames><affiliation>LaBRI</affiliation></author></authors><title>Incidence coloring game and arboricity of graphs</title><categories>cs.DM math.CO</categories><comments>10 pages</comments><proxy>ccsd</proxy><journal-ref>International Workshop on Combinatorial Algorithms, Rouen : France
  (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An incidence of a graph $G$ is a pair $(v,e)$ where $v$ is a vertex of $G$
and $e$ an edge incident to $v$. Two incidences $(v,e)$ and $(w,f)$ are
adjacent whenever $v = w$, or $e = f$, or $vw = e$ or $f$. The incidence
coloring game [S.D. Andres, The incidence game chromatic number, Discrete Appl.
Math. 157 (2009), 1980-1987] is a variation of the ordinary coloring game where
the two players, Alice and Bob, alternately color the incidences of a graph,
using a given number of colors, in such a way that adjacent incidences get
distinct colors. If the whole graph is colored then Alice wins the game
otherwise Bob wins the game. The incidence game chromatic number $i_g(G)$ of a
graph $G$ is the minimum number of colors for which Alice has a winning
strategy when playing the incidence coloring game on $G$. Andres proved that
%$\lceil 3/2 \Delta(G)\rceil \le $i_g(G) \le 2\Delta(G) + 4k - 2$ for every
$k$-degenerate graph $G$. %The arboricity $a(G)$ of a graph $G$ is the minimum
number of forests into which its set of edges can be partitioned. %If $G$ is
$k$-degenerate, then $a(G) \le k \le 2a(G) - 1$. We show in this paper that
$i_g(G) \le \lfloor\frac{3\Delta(G) - a(G)}{2}\rfloor + 8a(G) - 2$ for every
graph $G$, where $a(G)$ stands for the arboricity of $G$, thus improving the
bound given by Andres since $a(G) \le k$ for every $k$-degenerate graph $G$.
Since there exists graphs with $i_g(G) \ge \lceil\frac{3\Delta(G)}{2}\rceil$,
the multiplicative constant of our bound is best possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0183</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0183</id><created>2013-03-31</created><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>On the data processing theorem in the semi-deterministic setting</title><categories>cs.IT math.IT</categories><comments>20 pages; submitted to IEEE Trans. on Inform. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data processing lower bounds on the expected distortion are derived in the
finite-alphabet semi-deterministic setting, where the source produces a
deterministic, individual sequence, but the channel model is probabilistic, and
the decoder is subjected to various kinds of limitations, e.g., decoders
implementable by finite-state machines, with or without counters, and with or
without a restriction of common reconstruction with high probability. Some of
our bounds are given in terms of the Lempel-Ziv complexity of the source
sequence or the reproduction sequence. We also demonstrate how some analogous
results can be obtained for classes of linear encoders and linear decoders in
the continuous alphabet case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0185</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0185</id><created>2013-03-31</created><authors><author><keyname>Abdo</keyname><forenames>Hosam</forenames></author><author><keyname>Dimitrov</keyname><forenames>Darko</forenames></author></authors><title>The Total Irregularity of Graphs under Graph Operations</title><categories>cs.DM math.CO</categories><comments>14 pages, 3 figures, Journal number</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The total irregularity of a graph $G$ is defined as $\irr_t(G)=1/2 \sum_{u,v
\in V(G)}$ $|d_G(u)-d_G(v)|$, where $d_G(u)$ denotes the degree of a vertex $u
\in V(G)$. In this paper we give (sharp) upper bounds on the total irregularity
of graphs under several graph operations including join, lexicographic product,
Cartesian product, strong product, direct product, corona product, disjunction
and symmetric difference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0188</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0188</id><created>2013-03-31</created><updated>2013-04-03</updated><authors><author><keyname>Shparlinski</keyname><forenames>Igor</forenames></author></authors><title>Evasive Properties of Sparse Graphs and Some Linear Equations in Primes</title><categories>cs.CC math.CO math.NT</categories><comments>This version corrects a mistake made in the previous version, which
  was pointed out to the author by Laszlo Babai</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an unconditional version of a conditional, on the Extended Riemann
Hypothesis, result of L. Babai, A. Banerjee, R. Kulkarni and V. Naik (2010) on
the evasiveness of sparse graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0193</identifier>
 <datestamp>2014-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0193</id><created>2013-03-31</created><updated>2014-01-06</updated><authors><author><keyname>Yu</keyname><forenames>Z.</forenames></author><author><keyname>Baxley</keyname><forenames>R. J.</forenames></author><author><keyname>Zhou</keyname><forenames>G. T.</forenames></author></authors><title>Brightness Control in Dynamic Range Constrained Visible Light OFDM
  Systems</title><categories>cs.IT math.IT</categories><doi>10.1109/WOCC.2014.6839941</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visible light communication (VLC) systems can provide illumination and
communication simultaneously via light emitting diodes (LEDs). Orthogonal
frequency division multiplexing (OFDM) waveforms transmitted in a VLC system
will have high peak-to-average power ratios (PAPRs). Since the transmitting LED
is dynamic-range limited, OFDM signal has to be scaled and biased to avoid
nonlinear distortion. Brightness control is an essential feature for the
illumination function. In this paper, we will analyze the performance of
dynamic range constrained visible light OFDM systems with biasing adjustment
and pulse width modulation (PWM) methods. We will investigate the trade-off
between duty cycle and forward ratio of PWM and find the optimum forward ratio
to maximize the achievable ergodic rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0207</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0207</id><created>2013-03-31</created><authors><author><keyname>Anwar</keyname><forenames>Ahmed H.</forenames></author><author><keyname>Seddik</keyname><forenames>Karim G.</forenames></author><author><keyname>ElBatt</keyname><forenames>Tamer</forenames></author><author><keyname>Zahran</keyname><forenames>Ahmed H.</forenames></author></authors><title>Effective Capacity of Delay Constrained Cognitive Radio Links Exploiting
  Primary Feedback</title><categories>cs.IT math.IT</categories><comments>8 pages, 7 figures, wiopt 2013. arXiv admin note: text overlap with
  arXiv:0906.3888, arXiv:1004.0907 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the performance of a secondary link in a cognitive
radio (CR) system operating under statistical quality of service (QoS) delay
constraints. In particular, we quantify analytically the performance
improvement for the secondary user (SU) when applying a feedback based sensing
scheme under the &quot;SINR Interference&quot; model. We leverage the concept of
effective capacity (EC) introduced earlier in the literature to quantify the
wireless link performance under delay constraints, in an attempt to
opportunistically support real-time applications. Towards this objective, we
study a two-link network, a single secondary link and a primary network
abstracted to a single primary link, with and without primary feedback
exploitation. We analytically prove that exploiting primary feedback at the
secondary transmitter improves the EC of the secondary user and decreases the
secondary user average transmitted power. Finally, we present numerical results
that support our analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0243</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0243</id><created>2013-03-31</created><authors><author><keyname>A&#xdf;mann</keyname><forenames>Marc</forenames></author><author><keyname>Bayer</keyname><forenames>Manfred</forenames></author></authors><title>Compressive adaptive computational ghost imaging</title><categories>physics.optics cs.CV</categories><journal-ref>Scientific Reports 3, 1545 (2013)</journal-ref><doi>10.1038/srep01545</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive sensing is considered a huge breakthrough in signal acquisition.
It allows recording an image consisting of $N^2$ pixels using much fewer than
$N^2$ measurements if it can be transformed to a basis where most pixels take
on negligibly small values. Standard compressive sensing techniques suffer from
the computational overhead needed to reconstruct an image with typical
computation times between hours and days and are thus not optimal for
applications in physics and spectroscopy. We demonstrate an adaptive
compressive sampling technique that performs measurements directly in a sparse
basis. It needs much fewer than $N^2$ measurements without any computational
overhead, so the result is available instantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0247</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0247</id><created>2013-03-31</created><authors><author><keyname>Giannopoulos</keyname><forenames>Panos</forenames></author><author><keyname>Knauer</keyname><forenames>Christian</forenames></author></authors><title>Finding a largest empty convex subset in space is W[1]-hard</title><categories>cs.CG</categories><comments>9 pages, 8 figures</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following problem: Given a point set in space find a largest
subset that is in convex position and whose convex hull is empty. We show that
the (decision version of the) problem is W[1]-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0254</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0254</id><created>2013-03-31</created><authors><author><keyname>Nurassyl</keyname><forenames>Kerimbayev</forenames></author></authors><title>Virtual learning: possibilities and realization</title><categories>cs.CY</categories><comments>7 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The virtual learning in University Education is the learning which is
presented by set of integrated information and pedagogical technologies, in a
process of interaction between subjects and objects as the virtual educational
resources. This interaction characterize as the set of dialectically
interconnected fields of human activity (intellectual, emotional and
figurative, cultural, social). The virtual educational resources, possibility
of their adaptation to student subjectivity, and realization in the conditions
of University education are the main issues of this article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0260</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0260</id><created>2013-03-31</created><authors><author><keyname>Xie</keyname><forenames>Qiuliang</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author><author><keyname>Yang</keyname><forenames>Zhixing</forenames></author></authors><title>Polar Decomposition of Mutual Information over Complex-Valued Channels</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A polar decomposition of mutual information between a complex-valued
channel's input and output is proposed for a input whose amplitude and phase
are independent of each other. The mutual information is symmetrically
decomposed into three terms: an amplitude term, a phase term, and a cross term,
whereby the cross term is negligible at high signal-to-noise ratio. Theoretical
bounds of the amplitude and phase terms are derived for additive white Gaussian
noise channels with Gaussian inputs. This decomposition is then applied to the
recently proposed amplitude phase shift keying with product constellation
(product-APSK) inputs. It shows from an information theoretical perspective
that coded modulation schemes using product-APSK are able to outperform those
using conventional quadrature amplitude modulation (QAM), meanwhile maintain a
low complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0263</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0263</id><created>2013-03-31</created><authors><author><keyname>Velimirovic</keyname><forenames>Lazar</forenames></author><author><keyname>Peric</keyname><forenames>Zoran</forenames></author><author><keyname>Stankovic</keyname><forenames>Miomir</forenames></author><author><keyname>Nikolic</keyname><forenames>Jelena</forenames></author></authors><title>Numerical determination of the optimal value of quantizer's segment
  threshold using quadratic spline functions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an approximation of the optimal compressor function using the
quadratic spline functions has been presented. The coefficients of the
quadratic spline functions are determined by minimizing the mean-square error
(MSE). Based on the obtained approximative quadratic spline functions, the
design for companding quantizer for Gaussian source is done. The support region
of proposed companding quantizer is divided on segments of unequal size, where
the optimal value of segment threshold is numerically determined depending on
maximal value of the signal to quantization noise ratio (SQNR). It is shown
that by the companding quantizer proposed in this paper, the SQNR that is very
close to SQNR of nonlinear optimal companding quantizer is achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0267</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0267</id><created>2013-03-31</created><authors><author><keyname>Goncalves</keyname><forenames>Alexandre Domingues</forenames></author><author><keyname>Drummond</keyname><forenames>Lucia Maria</forenames></author><author><keyname>Pessoa</keyname><forenames>Artur Alves</forenames></author><author><keyname>Hahn</keyname><forenames>Peter</forenames></author></authors><title>Improving Lower Bounds for the Quadratic Assignment Problem by applying
  a Distributed Dual Ascent Algorithm</title><categories>cs.DC</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The application of the Reformulation Linearization Technique (RLT) to the
Quadratic Assignment Problem (QAP) leads to a tight linear relaxation with huge
dimensions that is hard to solve. Previous works found in the literature show
that these relaxations combined with branch-and-bound algorithms belong to the
state-of-the-art of exact methods for the QAP. For the level 3 RLT (RLT3),
using this relaxation is prohibitive in conventional machines for instances
with more than 22 locations due to memory limitations. This paper presents a
distributed version of a dual ascent algorithm for the RLT3 QAP relaxation that
approximately solves it for instances with up to 30 locations for the first
time. Although, basically, the distributed algorithm has been implemented on
top of its sequential conterpart, some changes, which improved not only the
parallel performance but also the quality of solutions, were proposed here.
When compared to other lower bounding methods found in the literature, our
algorithm generates the best known lower bounds for 26 out of the 28 tested
instances, reaching the optimal solution in 18 of them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0270</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0270</id><created>2013-03-31</created><authors><author><keyname>Wang</keyname><forenames>Fan</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Lin</forenames></author></authors><title>An optimal problem for relative entropy</title><categories>cs.IT math.IT</categories><comments>10 page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relative entropy is an essential tool in quantum information theory. There
are so many problems which are related to relative entropy. In this article,
the optimal values which are defined by $\displaystyle\max_{U\in{U(\cX_{d})}}
S(U\rho{U^{\ast}}\parallel\sigma)$ and $\displaystyle\min_{U\in{U(\cX_{d})}}
S(U\rho{U^{\ast}}\parallel\sigma)$ for two positive definite operators
$\rho,\sigma\in{\textmd{Pd}(\cX)}$ are obtained. And the set of
$S(U\rho{U^{\ast}}\parallel\sigma)$ for every unitary operator $U$ is full of
the interval $[\displaystyle\min_{U\in{U(\cX_{d})}}
S(U\rho{U^{\ast}}\parallel\sigma),\displaystyle\max_{U\in{U(\cX_{d})}}
S(U\rho{U^{\ast}}\parallel\sigma)]$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0272</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0272</id><created>2013-03-31</created><authors><author><keyname>Vizca&#xed;no</keyname><forenames>Pedro Francisco Valencia</forenames></author></authors><title>Relations between ex falso, tertium non datur, and double negation
  elimination</title><categories>math.LO cs.LO</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show which implicational relations hold between the three principles ex
falso sequitur quodlibet, tertium non datur, and double negation elimination,
on the basis of minimal logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0274</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0274</id><created>2013-03-31</created><updated>2015-08-25</updated><authors><author><keyname>Wang</keyname><forenames>Tao</forenames></author></authors><title>On the diameter of total domination vertex critical graphs</title><categories>math.CO cs.DM</categories><comments>8 pages in Bulletin of the Malaysian Mathematical Sciences Society,
  2015</comments><msc-class>05C12, 05C69</msc-class><doi>10.1007/s40840-015-0200-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider various types of domination vertex critical
graphs, including total domination vertex critical graphs, independent
domination vertex critical graphs and connected domination vertex critical
graphs. We provide upper bounds on the diameter of them, two of which are
sharp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0285</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0285</id><created>2013-03-31</created><updated>2015-08-25</updated><authors><author><keyname>Wang</keyname><forenames>Tao</forenames></author></authors><title>Strong chromatic index of k-degenerate graphs</title><categories>math.CO cs.DM</categories><comments>3 pages in Discrete Mathematics, 2015</comments><msc-class>05C15</msc-class><doi>10.1016/j.disc.2014.04.016</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A {\em strong edge coloring} of a graph $G$ is a proper edge coloring in
which every color class is an induced matching. The {\em strong chromatic
index} $\chiup_{s}'(G)$ of a graph $G$ is the minimum number of colors in a
strong edge coloring of $G$. In this note, we improve a result by D{\k e}bski
\etal [Strong chromatic index of sparse graphs, arXiv:1301.1992v1] and show
that the strong chromatic index of a $k$-degenerate graph $G$ is at most
$(4k-2) \cdot \Delta(G) - 2k^{2} + 1$. As a direct consequence, the strong
chromatic index of a $2$-degenerate graph $G$ is at most $6\Delta(G) - 7$,
which improves the upper bound $10\Delta(G) - 10$ by Chang and Narayanan
[Strong chromatic index of 2-degenerate graphs, J. Graph Theory 73 (2013) (2)
119--126]. For a special subclass of $2$-degenerate graphs, we obtain a better
upper bound, namely if $G$ is a graph such that all of its $3^{+}$-vertices
induce a forest, then $\chiup_{s}'(G) \leq 4 \Delta(G) -3$; as a corollary,
every minimally $2$-connected graph $G$ has strong chromatic index at most $4
\Delta(G) - 3$. Moreover, all the results in this note are best possible in
some sense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0299</identifier>
 <datestamp>2013-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0299</id><created>2013-04-01</created><updated>2013-06-10</updated><authors><author><keyname>Mach</keyname><forenames>Lukas</forenames></author><author><keyname>Toufar</keyname><forenames>Tomas</forenames></author></authors><title>Amalgam width of matroids</title><categories>cs.DM math.CO</categories><comments>arXiv admin note: text overlap with arXiv:0904.2785 by other authors</comments><acm-class>G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new matroid width parameter based on the operation of matroid
amalgamation, which we call amalgam-width. The parameter is linearly related to
branch-width on finitely representable matroids (which is not possible for
branch-width). In particular, any property expressible in the monadic second
order logic can be decided in linear time for matroids with bounded
amalgam-width. We also prove that the Tutte polynomial can be computed in
polynomial time for matroids with bounded amalgam width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0321</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0321</id><created>2013-04-01</created><authors><author><keyname>Rhif</keyname><forenames>Ahmed</forenames></author><author><keyname>Kardous</keyname><forenames>Zohra</forenames></author><author><keyname>Ben</keyname><forenames>Naceur</forenames></author></authors><title>First and High Order Sliding Mode-Multimodel Stabilizing Control
  Synthesis using Single and Several Sliding Surfaces for Nonlinear Systems:
  Simulation on an Autonomous Underwater Vehicles (AUV)</title><categories>cs.SY cs.CE math.DS math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides new analytic tools for a rigorous control formulation and
stability analysis of sliding mode-multimodel controller (SM-MMC). In this way
to minimise the chattering effect we will adopt as a starting point the
multimodel approach to change the commutation of the sliding mode control (SMC)
into fusion using a first order then a high order sliding mode control with
single sliding surface and, then, with several sliding surfaces. For that the
stability conditions invoke the existence of two Lyapunov-type functions, the
first associated to the passage to the sliding set in finite time, and the
second with convergence to the desired state. The approaches presented in this
work are simulated on the immersion control of a submarine mobile which
presents a problem for the actuators because of the high level of system non
linearity and because of the external disturbances. Simulation results show
that this control strategy can attain excellent performances with no chattering
problem and low control level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0353</identifier>
 <datestamp>2013-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0353</id><created>2013-04-01</created><updated>2013-05-01</updated><authors><author><keyname>Sher</keyname><forenames>Galen</forenames></author><author><keyname>Vitoria</keyname><forenames>Pedro</forenames></author></authors><title>An Information-Theoretic Test for Dependence with an Application to the
  Temporal Structure of Stock Returns</title><categories>q-fin.ST cs.IT math.IT stat.ME</categories><comments>22 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information theory provides ideas for conceptualising information and
measuring relationships between objects. It has found wide application in the
sciences, but economics and finance have made surprisingly little use of it. We
show that time series data can usefully be studied as information -- by noting
the relationship between statistical redundancy and dependence, we are able to
use the results of information theory to construct a test for joint dependence
of random variables. The test is in the same spirit of those developed by
Ryabko and Astola (2005, 2006b,a), but differs from these in that we add extra
randomness to the original stochatic process. It uses data compression to
estimate the entropy rate of a stochastic process, which allows it to measure
dependence among sets of random variables, as opposed to the existing
econometric literature that uses entropy and finds itself restricted to
pairwise tests of dependence. We show how serial dependence may be detected in
S&amp;P500 and PSI20 stock returns over different sample periods and frequencies.
We apply the test to synthetic data to judge its ability to recover known
temporal dependence structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0355</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0355</id><created>2013-04-01</created><authors><author><keyname>Muralidharan</keyname><forenames>Vijayvaradharaj T.</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Linear Fractional Network Coding and Representable Discrete Polymatroids</title><categories>cs.IT math.IT</categories><comments>8 pages, 5 figures, 2 tables. arXiv admin note: substantial text
  overlap with arXiv:1301.3003</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A linear Fractional Network Coding (FNC) solution over $\mathbb{F}_q$ is a
linear network coding solution over $\mathbb{F}_q$ in which the message
dimensions need not necessarily be the same and need not be the same as the
edge vector dimension. Scalar linear network coding, vector linear network
coding are special cases of linear FNC. In this paper, we establish the
connection between the existence of a linear FNC solution for a network over
$\mathbb{F}_q$ and the representability over $\mathbb{F}_q$ of discrete
polymatroids, which are the multi-set analogue of matroids. All previously
known results on the connection between the scalar and vector linear
solvability of networks and representations of matroids and discrete
polymatroids follow as special cases. An algorithm is provided to construct
networks which admit FNC solution over $\mathbb{F}_q,$ from discrete
polymatroids representable over $\mathbb{F}_q.$ Example networks constructed
from discrete polymatroids using the algorithm are provided, which do not admit
any scalar and vector solution, and for which FNC solutions with the message
dimensions being different provide a larger throughput than FNC solutions with
the message dimensions being equal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0356</identifier>
 <datestamp>2014-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0356</id><created>2013-04-01</created><authors><author><keyname>Parandehgheibi</keyname><forenames>Marzieh</forenames></author><author><keyname>Modiano</keyname><forenames>Eytan</forenames></author></authors><title>Robustness of Interdependent Networks: The case of communication
  networks and the power grid</title><categories>math.OC cs.NI</categories><comments>6 pages, submitted to GlobeCom 2013</comments><doi>10.1109/GLOCOM.2013.6831395</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the robustness of interdependent networks, in which
the state of one network depends on the state of the other network and vice
versa. In particular, we focus on the interdependency between the power grid
and communication networks, where the grid depends on communications for its
control, and the communication network depends on the grid for power. A
real-world example is the Italian blackout of 2003, when a small failure in the
power grid cascaded between the two networks and led to a massive blackout. In
this paper, we study the minimum number of node failures needed to cause total
blackout (i.e., all nodes in both networks to fail). In the case of
unidirectional interdependency between the networks we show that the problem is
NP-hard, and develop heuristics to find a near-optimal solution. On the other
hand, we show that in the case of bidirectional interdependency this problem
can be solved in polynomial time. We believe that this new interdependency
model gives rise to important, yet unexplored, robust network design problems
for interdependent networked infrastructures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0357</identifier>
 <datestamp>2014-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0357</id><created>2013-04-01</created><authors><author><keyname>Stopczynski</keyname><forenames>Arkadiusz</forenames></author><author><keyname>Stahlhut</keyname><forenames>Carsten</forenames></author><author><keyname>Larsen</keyname><forenames>Jakob Eg</forenames></author><author><keyname>Petersen</keyname><forenames>Michael Kai</forenames></author><author><keyname>Hansen</keyname><forenames>Lars Kai</forenames></author></authors><title>The Smartphone Brain Scanner: A Mobile Real-time Neuroimaging System</title><categories>cs.HC</categories><doi>10.1371/journal.pone.0086733</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Combining low cost wireless EEG sensors with smartphones offers novel
opportunities for mobile brain imaging in an everyday context. We present a
framework for building multi-platform, portable EEG applications with real-time
3D source reconstruction. The system - Smartphone Brain Scanner - combines an
off-the-shelf neuroheadset or EEG cap with a smartphone or tablet, and as such
represents the first fully mobile system for real-time 3D EEG imaging. We
discuss the benefits and challenges of a fully portable system, including
technical limitations as well as real-time reconstruction of 3D images of brain
activity. We present examples of the brain activity captured in a simple
experiment involving imagined finger tapping, showing that the acquired signal
in a relevant brain region is similar to that obtained with standard EEG lab
equipment. Although the quality of the signal in a mobile solution using a
off-the-shelf consumer neuroheadset is lower compared to that obtained using
high density standard EEG equipment, we propose that mobile application
development may offset the disadvantages and provide completely new
opportunities for neuroimaging in natural settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0371</identifier>
 <datestamp>2013-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0371</id><created>2013-04-01</created><updated>2013-05-22</updated><authors><author><keyname>Shpilka</keyname><forenames>Amir</forenames></author><author><keyname>Tal</keyname><forenames>Avishay</forenames></author><author><keyname>Volk</keyname><forenames>Ben lee</forenames></author></authors><title>On the Structure of Boolean Functions with Small Spectral Norm</title><categories>cs.CC math.CA math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we prove results regarding Boolean functions with small
spectral norm (the spectral norm of f is
$\|\hat{f}\|_1=\sum_{\alpha}|\hat{f}(\alpha)|$). Specifically, we prove the
following results for functions $f:\{0,1\}^n \to \{0,1\}$ with
$\|\hat{f}\|_1=A$.
  1. There is a subspace $V$ of co-dimension at most $A^2$ such that $f|_V$ is
constant.
  2. f can be computed by a parity decision tree of size $2^{A^2}n^{2A}$. (a
parity decision tree is a decision tree whose nodes are labeled with arbitrary
linear functions.)
  3. If in addition f has at most s nonzero Fourier coefficients, then f can be
computed by a parity decision tree of depth $A^2 \log s$.
  4. For every $0&lt;\epsilon$ there is a parity decision tree of depth $O(A^2 +
\log(1/\epsilon))$ and size $2^{O(A^2)} \cdot
\min\{1/\epsilon^2,O(\log(1/\epsilon))^{2A}\}$ that \epsilon-approximates f.
Furthermore, this tree can be learned, with probability $1-\delta$, using
$\poly(n,\exp(A^2),1/\epsilon,\log(1/\delta))$ membership queries.
  All the results above also hold (with a slight change in parameters) to
functions $f:Z_p^n\to \{0,1\}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0374</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0374</id><created>2013-04-01</created><authors><author><keyname>Choe</keyname><forenames>Yong-Hwa</forenames></author><author><keyname>Jong</keyname><forenames>Chol-Yong</forenames></author><author><keyname>Han</keyname><forenames>Song</forenames></author></authors><title>Software Cognitive Information Measure based on Relation Between
  Structures</title><categories>cs.SE</categories><comments>12 pages</comments><report-no>KIUS-MATH-2013-E-R-010</report-no><msc-class>68Q25, 68W40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive complexity measures quantify human difficulty in understanding the
source code based on cognitive informatics foundation. The discipline derives
cognitive complexity on a basis of fundamental software factors i.e, inputs,
outputs, and internal processing architecture. An approach to integrating
Granular Computing into the new measure called Structured Cognitive Information
Measure or SCIM. The proposed measure unifies and re-organizes complexity
factors analogous to human cognitive process. However, according to the
methodology of software and the scope of the variables, Information Complexity
Number(ICN) of variables is depended on change of variable value and cognitive
complexity is measured in several ways. In this paper, we define the Scope
Information Complexity Number (SICN) and present the cognitive complexity based
on functional decomposition of software, including theoretical validation
through nine Weyuker's properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0378</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0378</id><created>2013-04-01</created><updated>2013-04-10</updated><authors><author><keyname>Gupta</keyname><forenames>Manoj</forenames></author><author><keyname>Peng</keyname><forenames>Richard</forenames></author></authors><title>Fully Dynamic $(1+\epsilon)$-Approximate Matchings</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first data structures that maintain near optimal maximum
cardinality and maximum weighted matchings on sparse graphs in sublinear time
per update. Our main result is a data structure that maintains a $(1+\epsilon)$
approximation of maximum matching under edge insertions/deletions in worst case
$O(\sqrt{m}\epsilon^{-2})$ time per update. This improves the 3/2 approximation
given in [Neiman,Solomon,STOC 2013] which runs in similar time. The result is
based on two ideas. The first is to re-run a static algorithm after a chosen
number of updates to ensure approximation guarantees. The second is to
judiciously trim the graph to a smaller equivalent one whenever possible.
  We also study extensions of our approach to the weighted setting, and combine
it with known frameworks to obtain arbitrary approximation ratios. For a
constant $\epsilon$ and for graphs with edge weights between 1 and N, we design
an algorithm that maintains an $(1+\epsilon)$-approximate maximum weighted
matching in $O(\sqrt{m} \log N)$ time per update. The only previous result for
maintaining weighted matchings on dynamic graphs has an approximation ratio of
4.9108, and was shown in [Anand,Baswana,Gupta,Sen, FSTTCS 2012, arXiv 2012].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0383</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0383</id><created>2013-04-01</created><updated>2013-07-05</updated><authors><author><keyname>Kim</keyname><forenames>Yong-Jin</forenames></author><author><keyname>Kim</keyname><forenames>Yong-Min</forenames></author><author><keyname>Choe</keyname><forenames>Yong-Jin</forenames></author><author><keyname>O</keyname><forenames>Hyong-Chol</forenames></author></authors><title>An Efficient Bilinear Pairing-Free Certificateless Two-Party
  Authenticated Key Agreement Protocol in the eCK Model</title><categories>cs.CR cs.IT math.IT</categories><comments>15 pages. 1 figure and 1 table, ver. 2 revised according to
  reviewers' advice, this version is the new development of [19] which is the
  development of [12](arXiv:1106.3898) of Debiao He who was the second academic
  advisor and colleague of the first author during visit to Wuhan university,
  ver. 4 accepted in JTPC</comments><report-no>KISU-MATH-2013-E-R-016</report-no><msc-class>94A62(primary), 68M12(secondary)</msc-class><journal-ref>Journal of Theoretical Physics and Cryptography, Vol.3, July 2013,
  pp1-10</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent study on certificateless authenticated key agreement focuses on
bilinear pairing-free certificateless authenticated key agreement protocol. Yet
it has got limitations in the aspect of computational amount. So it is
important to reduce the number of the scalar multiplication over elliptic curve
group in bilinear pairing-free protocols. This paper proposed a new bilinear
pairing-free certificateless two-party authenticated key agreement protocol,
providing more efficiency among related work and proof under the random oracle
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0393</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0393</id><created>2013-04-01</created><updated>2013-04-02</updated><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Kumar</keyname><forenames>Nirman</forenames></author></authors><title>Approximating Minimization Diagrams and Generalized Proximity Search</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the classes of functions whose minimization diagrams can be
approximated efficiently in \Re^d. We present a general framework and a
data-structure that can be used to approximate the minimization diagram of such
functions. The resulting data-structure has near linear size and can answer
queries in logarithmic time. Applications include approximating the Voronoi
diagram of (additively or multiplicatively) weighted points. Our technique also
works for more general distance functions, such as metrics induced by convex
bodies, and the nearest furthest-neighbor distance to a set of point sets.
Interestingly, our framework works also for distance functions that do not
comply with the triangle inequality. For many of these functions no near-linear
size approximation was known before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0419</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0419</id><created>2013-04-01</created><authors><author><keyname>Das</keyname><forenames>Mahashweta</forenames></author><author><keyname>Das</keyname><forenames>Gautam</forenames></author><author><keyname>Hristidis</keyname><forenames>Vagelis</forenames></author></authors><title>Top-K Product Design Based on Collaborative Tagging Data</title><categories>cs.SI cs.DS cs.IR</categories><comments>The conference version appeared under the title &quot;Leveraging
  collaborative tagging for web item design&quot; in SIGKDD 2011, pages 538-546</comments><acm-class>H.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The widespread use and popularity of collaborative content sites (e.g., IMDB,
Amazon, Yelp, etc.) has created rich resources for users to consult in order to
make purchasing decisions on various products such as movies, e-commerce
products, restaurants, etc. Products with desirable tags (e.g., modern,
reliable, etc.) have higher chances of being selected by prospective customers.
This creates an opportunity for product designers to design better products
that are likely to attract desirable tags when published. In this paper, we
investigate how to mine collaborative tagging data to decide the attribute
values of new products and to return the top-k products that are likely to
attract the maximum number of desirable tags when published. Given a training
set of existing products with their features and user-submitted tags, we first
build a Naive Bayes Classifier for each tag. We show that the problem of is
NP-complete even if simple Naive Bayes Classifiers are used for tag prediction.
We present a suite of algorithms for solving this problem: (a) an exact two
tier algorithm(based on top-k querying techniques), which performs much better
than the naive brute-force algorithm and works well for moderate problem
instances, and (b) a set of approximation algorithms for larger problem
instances: a novel polynomial-time approximation algorithm with provable error
bound and a practical hill-climbing heuristic. We conduct detailed experiments
on synthetic and real data crawled from the web to evaluate the efficiency and
quality of our proposed algorithms, as well as show how product designers can
benefit by leveraging collaborative tagging information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0421</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0421</id><created>2013-04-01</created><authors><author><keyname>Santosh</keyname><forenames>K. C.</forenames><affiliation>LORIA</affiliation></author><author><keyname>Iwata</keyname><forenames>E.</forenames></author></authors><title>Stroke-Based Cursive Character Recognition</title><categories>cs.CV</categories><proxy>ccsd</proxy><journal-ref>Advances in Character Recognition INTECH (Ed.) (2012) 175-192</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human eye can see and read what is written or displayed either in natural
handwriting or in printed format. The same work in case the machine does is
called handwriting recognition. Handwriting recognition can be broken down into
two categories: off-line and on-line. ...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0422</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0422</id><created>2013-04-01</created><authors><author><keyname>Kairouz</keyname><forenames>Peter</forenames></author><author><keyname>Singer</keyname><forenames>Andrew</forenames></author></authors><title>MIMO Communications over Multi-Mode Optical Fibers: Capacity Analysis
  and Input-Output Coupling Schemes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider multi-input multi-output (MIMO) communications over multi-mode
fibers (MMFs). Current MMF standards, such as OM3 and OM4, use fibers with core
radii of 50 \mu m, allowing hundreds of modes to propagate. Unfortunately, due
to physical and computational complexity limitations, we cannot couple and
detect hundreds of data streams into and out of the fiber. In order to
circumvent this issue, we present input-output coupling schemes that allow the
user to couple and extract a reasonable number of signals from a fiber with
many modes. This approach is particularly attractive as it is scalable; i.e.,
the fibers do not have to be replaced every time the number of transmitters or
receivers is increased, a phenomenon that is likely to happen in the near
future.
  We present a statistical channel model that incorporates intermodal
dispersion, chromatic dispersion, mode dependent losses, mode coupling, and
input-output coupling. We show that the statistics of the fiber's frequency
response are independent of frequency. This simplifies the computation of the
average Shannon capacity of the fiber. We also provide an input-output coupling
strategy that leads to an increase in the overall capacity. This strategy can
be used whenever channel state information (CSI) is available at the
transmitter. We show that the capacity of an Nt by Nt MIMO system over a fiber
with M&gt;&gt;Nt modes can approach the capacity of an Nt-mode fiber with no
mode-dependent losses. We finally present a statistical input-output coupling
model in order to quantify the loss in capacity when CSI is not available at
the transmitter. It turns out that the loss, relative to Nt-mode fibers, is
minimal (less than 0.5 dB) for a wide range of signal-to-noise ratios (SNRs)
and a reasonable range of MDLs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0432</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0432</id><created>2013-04-01</created><authors><author><keyname>Saeedi</keyname><forenames>Mehdi</forenames></author><author><keyname>Shafaei</keyname><forenames>Alireza</forenames></author><author><keyname>Pedram</keyname><forenames>Massoud</forenames></author></authors><title>Constant-Factor Optimization of Quantum Adders on 2D Quantum
  Architectures</title><categories>quant-ph cs.ET</categories><comments>10 pages, 11 figures, 3 tables, Conference on Reversible Computation
  (2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum arithmetic circuits have practical applications in various quantum
algorithms. In this paper, we address quantum addition on 2-dimensional
nearest-neighbor architectures based on the work presented by Choi and Van
Meter (JETC 2012). To this end, we propose new circuit structures for some
basic blocks in the adder, and reduce communication overhead by adding
concurrency to consecutive blocks and also by parallel execution of expensive
Toffoli gates. The proposed optimizations reduce total depth from $140\sqrt
n+k_1$ to $92\sqrt n+k_2$ for constants $k_1,k_2$ and affect the computation
fidelity considerably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0470</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0470</id><created>2013-04-01</created><authors><author><keyname>Ajmone-Marsan</keyname><forenames>Marco</forenames></author><author><keyname>Arrowsmith</keyname><forenames>David</forenames></author><author><keyname>Breymann</keyname><forenames>Wolfgang</forenames></author><author><keyname>Fritz</keyname><forenames>Oliver</forenames></author><author><keyname>Masera</keyname><forenames>Marcelo</forenames></author><author><keyname>Mengolini</keyname><forenames>Anna</forenames></author><author><keyname>Carbone</keyname><forenames>Anna</forenames></author></authors><title>The Emerging Energy Web</title><categories>physics.soc-ph cs.SI</categories><journal-ref>The European Physical Journal Special Topics, Vol. 214, Issue 1,
  pp 547-569 (2012)</journal-ref><doi>10.1140/epjst/e2012-01705-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a general need of elaborating energy-effective solutions for
managing our increasingly dense interconnected world. The problem should be
tackled in multiple dimensions -technology, society, economics, law,
regulations, and politics- at different temporal and spatial scales. Holistic
approaches will enable technological solutions to be supported by
socio-economic motivations, adequate incentive regulation to foster investment
in green infrastructures coherently integrated with adequate energy
provisioning schemes. In this article, an attempt is made to describe such
multidisciplinary challenges with a coherent set of solutions to be identified
to significantly impact the way our interconnected energy world is designed and
operated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0473</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0473</id><created>2013-04-01</created><authors><author><keyname>Martin</keyname><forenames>Travis</forenames></author><author><keyname>Ball</keyname><forenames>Brian</forenames></author><author><keyname>Karrer</keyname><forenames>Brian</forenames></author><author><keyname>Newman</keyname><forenames>M. E. J.</forenames></author></authors><title>Coauthorship and citation in scientific publishing</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>10 pages, 11 figures, 3 tables</comments><doi>10.1103/PhysRevE.88.012814</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large number of published studies have examined the properties of either
networks of citation among scientific papers or networks of coauthorship among
scientists. Here, using an extensive data set covering more than a century of
physics papers published in the Physical Review, we study a hybrid
coauthorship/citation network that combines the two, which we analyze to gain
insight into the correlations and interactions between authorship and citation.
Among other things, we investigate the extent to which individuals tend to cite
themselves or their collaborators more than others, the extent to which they
cite themselves or their collaborators more quickly after publication, and the
extent to which they tend to return the favor of a citation from another
scientist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0480</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0480</id><created>2013-03-29</created><authors><author><keyname>Stojnic</keyname><forenames>Mihailo</forenames></author></authors><title>A problem dependent analysis of SOCP algorithms in noisy compressed
  sensing</title><categories>cs.IT math.IT stat.ML</categories><comments>arXiv admin note: text overlap with arXiv:1304.0002</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under-determined systems of linear equations with sparse solutions have been
the subject of an extensive research in last several years above all due to
results of \cite{CRT,CanRomTao06,DonohoPol}. In this paper we will consider
\emph{noisy} under-determined linear systems. In a breakthrough
\cite{CanRomTao06} it was established that in \emph{noisy} systems for any
linear level of under-determinedness there is a linear sparsity that can be
\emph{approximately} recovered through an SOCP (second order cone programming)
optimization algorithm so that the approximate solution vector is (in an
$\ell_2$-norm sense) guaranteed to be no further from the sparse unknown vector
than a constant times the noise. In our recent work \cite{StojnicGenSocp10} we
established an alternative framework that can be used for statistical
performance analysis of the SOCP algorithms. To demonstrate how the framework
works we then showed in \cite{StojnicGenSocp10} how one can use it to precisely
characterize the \emph{generic} (worst-case) performance of the SOCP. In this
paper we present a different set of results that can be obtained through the
framework of \cite{StojnicGenSocp10}. The results will relate to \emph{problem
dependent} performance analysis of SOCP's. We will consider specific types of
unknown sparse vectors and characterize the SOCP performance when used for
recovery of such vectors. We will also show that our theoretical predictions
are in a solid agreement with the results one can get through numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0494</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0494</id><created>2013-04-01</created><authors><author><keyname>Sundar</keyname><forenames>Kaarthik</forenames></author><author><keyname>Rathinam</keyname><forenames>Sivakumar</forenames></author></authors><title>Algorithms for Routing an Unmanned Aerial Vehicle in the presence of
  Refueling Depots</title><categories>cs.DS</categories><comments>9 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a single Unmanned Aerial Vehicle (UAV) routing problem where
there are multiple depots and the vehicle is allowed to refuel at any depot.
The objective of the problem is to find a path for the UAV such that each
target is visited at least once by the vehicle, the fuel constraint is never
violated along the path for the UAV, and the total fuel required by the UAV is
a minimum. We develop an approximation algorithm for the problem, and propose
fast construction and improvement heuristics to solve the same. Computational
results show that solutions whose costs are on an average within 1.4% of the
optimum can be obtained relatively fast for the problem involving 5 depots and
25 targets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0501</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0501</id><created>2013-04-01</created><authors><author><keyname>Morrison</keyname><forenames>Katherine</forenames></author></authors><title>Equivalence for Rank-metric and Matrix Codes and Automorphism Groups of
  Gabidulin Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a growing number of applications such as cellular, peer-to-peer, and
sensor networks, efficient error-free transmission of data through a network is
essential. Toward this end, K\&quot;{o}tter and Kschischang propose the use of
subspace codes to provide error correction in the network coding context. The
primary construction for subspace codes is the lifting of rank-metric or matrix
codes, a process that preserves the structural and distance properties of the
underlying code. Thus, to characterize the structure and error-correcting
capability of these subspace codes, it is valuable to perform such a
characterization of the underlying rank-metric and matrix codes. This paper
lays a foundation for this analysis through a framework for classifying
rank-metric and matrix codes based on their structure and distance properties.
  To enable this classification, we extend work by Berger on equivalence for
rank-metric codes to define a notion of equivalence for matrix codes, and we
characterize the group structure of the collection of maps that preserve such
equivalence. We then compare the notions of equivalence for these two related
types of codes and show that matrix equivalence is strictly more general than
rank-metric equivalence. Finally, we characterize the set of equivalence maps
that fix the prominent class of rank-metric codes known as Gabidulin codes. In
particular, we give a complete characterization of the rank-metric automorphism
group of Gabidulin codes, correcting work by Berger, and give a partial
characterization of the matrix-automorphism group of the expanded matrix codes
that arise from Gabidulin codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0502</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0502</id><created>2013-04-01</created><updated>2013-06-06</updated><authors><author><keyname>Fujiwara</keyname><forenames>Yuichiro</forenames></author><author><keyname>Tonchev</keyname><forenames>Vladimir D.</forenames></author><author><keyname>Wong</keyname><forenames>Tony W. H.</forenames></author></authors><title>Algebraic techniques in designing quantum synchronizable codes</title><categories>quant-ph cs.IT math.IT</categories><comments>9 pages, no figures. The framework presented in this article
  supersedes the one given in arXiv:1206.0260 by the first author</comments><journal-ref>Physical Review A 88, 012318 (2013)</journal-ref><doi>10.1103/PhysRevA.88.012318</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum synchronizable codes are quantum error-correcting codes that can
correct the effects of quantum noise as well as block synchronization errors.
We improve the previously known general framework for designing quantum
synchronizable codes through more extensive use of the theory of finite fields.
This makes it possible to widen the range of tolerable magnitude of block
synchronization errors while giving mathematical insight into the algebraic
mechanism of synchronization recovery. Also given are families of quantum
synchronizable codes based on punctured Reed-Muller codes and their ambient
spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0513</identifier>
 <datestamp>2013-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0513</id><created>2013-04-01</created><updated>2013-04-22</updated><authors><author><keyname>Find</keyname><forenames>Magnus</forenames></author><author><keyname>G&#xf6;&#xf6;s</keyname><forenames>Mika</forenames></author><author><keyname>J&#xe4;rvisalo</keyname><forenames>Matti</forenames></author><author><keyname>Kaski</keyname><forenames>Petteri</forenames></author><author><keyname>Koivisto</keyname><forenames>Mikko</forenames></author><author><keyname>Korhonen</keyname><forenames>Janne H.</forenames></author></authors><title>Separating OR, SUM, and XOR Circuits</title><categories>cs.CC</categories><comments>1 + 16 pages, 2 figures. In this version we have improved the
  presentation following comments made by Stasys Jukna and Igor Sergeev</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a boolean n by n matrix A we consider arithmetic circuits for computing
the transformation x-&gt;Ax over different semirings. Namely, we study three
circuit models: monotone OR-circuits, monotone SUM-circuits (addition of
non-negative integers), and non-monotone XOR-circuits (addition modulo 2). Our
focus is on \emph{separating} these models in terms of their circuit
complexities. We give three results towards this goal:
  (1) We prove a direct sum type theorem on the monotone complexity of tensor
product matrices. As a corollary, we obtain matrices that admit OR-circuits of
size O(n), but require SUM-circuits of size \Omega(n^{3/2}/\log^2n).
  (2) We construct so-called \emph{k-uniform} matrices that admit XOR-circuits
of size O(n), but require OR-circuits of size \Omega(n^2/\log^2n).
  (3) We consider the task of \emph{rewriting} a given OR-circuit as a
XOR-circuit and prove that any subquadratic-time algorithm for this task
violates the strong exponential time hypothesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0524</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0524</id><created>2013-04-01</created><authors><author><keyname>Miller</keyname><forenames>Gary L.</forenames></author><author><keyname>Sheehy</keyname><forenames>Donald R.</forenames></author><author><keyname>Velingker</keyname><forenames>Ameya</forenames></author></authors><title>A Fast Algorithm for Well-Spaced Points and Approximate Delaunay Graphs</title><categories>cs.CG</categories><comments>Full version</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm that produces a well-spaced superset of points
conforming to a given input set in any dimension with guaranteed optimal output
size. We also provide an approximate Delaunay graph on the output points. Our
algorithm runs in expected time $O(2^{O(d)}(n\log n + m))$, where $n$ is the
input size, $m$ is the output point set size, and $d$ is the ambient dimension.
The constants only depend on the desired element quality bounds.
  To gain this new efficiency, the algorithm approximately maintains the
Voronoi diagram of the current set of points by storing a superset of the
Delaunay neighbors of each point. By retaining quality of the Voronoi diagram
and avoiding the storage of the full Voronoi diagram, a simple exponential
dependence on $d$ is obtained in the running time. Thus, if one only wants the
approximate neighbors structure of a refined Delaunay mesh conforming to a set
of input points, the algorithm will return a size $2^{O(d)}m$ graph in
$2^{O(d)}(n\log n + m)$ expected time. If $m$ is superlinear in $n$, then we
can produce a hierarchically well-spaced superset of size $2^{O(d)}n$ in
$2^{O(d)}n\log n$ expected time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0528</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0528</id><created>2013-04-02</created><authors><author><keyname>Becher</keyname><forenames>Veronica</forenames></author><author><keyname>Deymonnaz</keyname><forenames>Alejandro</forenames></author><author><keyname>Heiber</keyname><forenames>Pablo Ariel</forenames></author></authors><title>Efficient repeat finding via suffix arrays</title><categories>cs.DS</categories><comments>14 pages</comments><journal-ref>Bioinformatics, 25(14):1746-1753, 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve the problem of finding interspersed maximal repeats using a suffix
array construction. As it is well known, all the functionality of suffix trees
can be handled by suffix arrays, gaining practicality. Our solution improves
the suffix tree based approaches for the repeat finding problem, being
particularly well suited for very large inputs. We prove the corrrectness and
complexity of the algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0538</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0538</id><created>2013-04-02</created><authors><author><keyname>Ma</keyname><forenames>Sen</forenames></author></authors><title>OESPA:A Theory of Programming that Support Software Engineering</title><categories>cs.PL</categories><comments>18 pages for FOCS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new theory of programming is proposed. The theory consists of OE (Operation
Expression), SP (Semantic Predicate) and A (Axiom), abbreviated as OESPA. OE is
for programming: its syntax is given by BNF formulas and its semantics is
defined by axioms on these formulas. Similar to predicates in logic, SP is for
describing properties of OE (i.e. programs) and for program property analysis.
But SP is different from predicates, it directly relates the final values of
variables upon termination of a given OE with initial values of these variables
before the same OE. As such, it is feasible to prove or disprove whether a
given SP is a property of a given OE by computation based on A (Axioms). SP
calculus is proposed for program specification and specification analysis, that
is missing in software engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0539</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0539</id><created>2013-04-02</created><authors><author><keyname>Lee</keyname><forenames>Chonho</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author></authors><title>A Real-time Group Auction System for Efficient Allocation of Cloud
  Internet Applications</title><categories>cs.GT cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing number of the cloud-based Internet applications demands for
efficient resource and cost management. This paper proposes a real-time group
auction system for the cloud instance market. The system is designed based on a
combinatorial double auction, and its applicability and effectiveness are
evaluated in terms of resource efficiency and monetary benefits to auction
participants (e.g., cloud users and providers). The proposed auction system
assists them to decide when and how providers allocate their resources to which
users. Furthermore, we propose a distributed algorithm using a group formation
game that determines which users and providers will trade resources by their
cooperative decisions. To find how to allocate the resources, the utility
optimization problem is formulated as a binary integer programming problem, and
the nearly optimal solution is obtained by a heuristic algorithm with quadratic
time complexity. In comparison studies, the proposed real-time group auction
system with cooperation outperforms an individual auction in terms of the
resource efficiency (e.g., the request acceptance rate for users and resource
utilization for providers) and monetary benefits (e.g., average payments for
users and total profits for providers).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0552</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0552</id><created>2013-04-02</created><updated>2014-07-02</updated><authors><author><keyname>Maillard</keyname><forenames>Pascal</forenames></author><author><keyname>Zeitouni</keyname><forenames>Ofer</forenames></author></authors><title>Performance of the Metropolis algorithm on a disordered tree: The
  Einstein relation</title><categories>math.PR cond-mat.dis-nn cs.DS</categories><comments>Published in at http://dx.doi.org/10.1214/13-AAP972 the Annals of
  Applied Probability (http://www.imstat.org/aap/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AAP-AAP972</report-no><journal-ref>Annals of Applied Probability 2014, Vol. 24, No. 5, 2070-2090</journal-ref><doi>10.1214/13-AAP972</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a $d$-ary rooted tree ($d\geq3$) where each edge $e$ is assigned an
i.i.d. (bounded) random variable $X(e)$ of negative mean. Assign to each vertex
$v$ the sum $S(v)$ of $X(e)$ over all edges connecting $v$ to the root, and
assume that the maximum $S_n^*$ of $S(v)$ over all vertices $v$ at distance $n$
from the root tends to infinity (necessarily, linearly) as $n$ tends to
infinity. We analyze the Metropolis algorithm on the tree and show that under
these assumptions there always exists a temperature $1/\beta$ of the algorithm
so that it achieves a linear (positive) growth rate in linear time. This
confirms a conjecture of Aldous [Algorithmica 22 (1998) 388-412]. The proof is
obtained by establishing an Einstein relation for the Metropolis algorithm on
the tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0553</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0553</id><created>2013-04-02</created><updated>2013-10-08</updated><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Kountouris</keyname><forenames>Marios</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Massive MIMO and Small Cells: Improving Energy Efficiency by Optimal
  Soft-Cell Coordination</title><categories>cs.IT math.IT</categories><comments>Published at International Conference on Telecommunications (ICT
  2013), 6-8 May 2013, Casablanca, Morocco, 5 pages, 4 figures, 2 tables. This
  version includes the Matlab code necessary to reproduce the simulations; see
  the ancillary files. This version also corrects errors in Table 1 and in the
  simulations, which affected Figs. 3-4</comments><doi>10.1109/ICTEL.2013.6632074</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To improve the cellular energy efficiency, without sacrificing
quality-of-service (QoS) at the users, the network topology must be densified
to enable higher spatial reuse. We analyze a combination of two densification
approaches, namely &quot;massive&quot; multiple-input multiple-output (MIMO) base
stations and small-cell access points. If the latter are operator-deployed, a
spatial soft-cell approach can be taken where the multiple transmitters serve
the users by joint non-coherent multiflow beamforming. We minimize the total
power consumption (both dynamic emitted power and static hardware power) while
satisfying QoS constraints. This problem is proved to have a hidden convexity
that enables efficient solution algorithms. Interestingly, the optimal solution
promotes exclusive assignment of users to transmitters. Furthermore, we provide
promising simulation results showing how the total power consumption can be
greatly improved by combining massive MIMO and small cells; this is possible
with both optimal and low-complexity beamforming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0555</identifier>
 <datestamp>2013-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0555</id><created>2013-04-02</created><updated>2013-06-28</updated><authors><author><keyname>Zhou</keyname><forenames>Rui-Rui</forenames></author><author><keyname>Yang</keyname><forenames>Li</forenames></author></authors><title>Distributed quantum election scheme</title><categories>quant-ph cs.CR</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an electronic voting protocol, a distributed scheme can be used for
forbidding the malicious acts of the voting administrator and the counter
during the election, but it cannot prevent them from collaborating to trace the
ballots and destroy their privacy after the election. We present a distributed
anonymous quantum key distribution scheme and further construct a distributed
quantum election scheme with a voting administrator made up of more than one
part. This quantum election scheme can resist the malicious acts of the voting
administrator and the counter after the election and can work in a system with
lossy and noisy quantum channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0558</identifier>
 <datestamp>2013-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0558</id><created>2013-04-02</created><updated>2013-10-24</updated><authors><author><keyname>Salikhmetov</keyname><forenames>Anton</forenames></author></authors><title>Lambda Calculus Synopsis</title><categories>cs.LO math.GN math.LO</categories><comments>8 pages, in Russian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This text gives a rough, but linear summary covering some key definitions,
notations, and propositions from Lambda Calculus: Its Syntax and Semantics, the
classical monograph by Barendregt. First, we define a theory of untyped
extensional lambda calculus. Then, some syntactic sugar, a system of
combinatory logic, and the fixed point theorem are described. The final section
introduces a topology on the set of lambda terms which is meant to explain an
illusory contradiction. Namely, functions defined on the set of lambda terms
are in the set of lambda terms itself, the latter being a countable set.
However, the functions on the set of lambda terms appear to be continuous with
respect to a topology of trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0564</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0564</id><created>2013-04-02</created><authors><author><keyname>VanderWeele</keyname><forenames>Tyler J.</forenames></author><author><keyname>Shpitser</keyname><forenames>Ilya</forenames></author></authors><title>On the definition of a confounder</title><categories>stat.ME cs.AI</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOS1058 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1058</report-no><journal-ref>Annals of Statistics 2013, Vol. 41, No. 1, 196-220</journal-ref><doi>10.1214/12-AOS1058</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The causal inference literature has provided a clear formal definition of
confounding expressed in terms of counterfactual independence. The literature
has not, however, come to any consensus on a formal definition of a confounder,
as it has given priority to the concept of confounding over that of a
confounder. We consider a number of candidate definitions arising from various
more informal statements made in the literature. We consider the properties
satisfied by each candidate definition, principally focusing on (i) whether
under the candidate definition control for all &quot;confounders&quot; suffices to
control for &quot;confounding&quot; and (ii) whether each confounder in some context
helps eliminate or reduce confounding bias. Several of the candidate
definitions do not have these two properties. Only one candidate definition of
those considered satisfies both properties. We propose that a &quot;confounder&quot; be
defined as a pre-exposure covariate C for which there exists a set of other
covariates X such that effect of the exposure on the outcome is unconfounded
conditional on (X,C) but such that for no proper subset of (X,C) is the effect
of the exposure on the outcome unconfounded given the subset. We also provide a
conditional analogue of the above definition; and we propose a variable that
helps reduce bias but not eliminate bias be referred to as a &quot;surrogate
confounder.&quot; These definitions are closely related to those given by Robins and
Morgenstern [Comput. Math. Appl. 14 (1987) 869-916]. The implications that hold
among the various candidate definitions are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0567</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0567</id><created>2013-04-02</created><authors><author><keyname>Loizou</keyname><forenames>Antonis</forenames></author><author><keyname>Groth</keyname><forenames>Paul</forenames></author></authors><title>On the Formulation of Performant SPARQL Queries</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The combination of the flexibility of RDF and the expressiveness of SPARQL
provides a powerful mechanism to model, integrate and query data. However,
these properties also mean that it is nontrivial to write performant SPARQL
queries. Indeed, it is quite easy to create queries that tax even the most
optimised triple stores. Currently, application developers have little concrete
guidance on how to write &quot;good&quot; queries. The goal of this paper is to begin to
bridge this gap. It describes 5 heuristics that can be applied to create
optimised queries. The heuristics are informed by formal results in the
literature on the semantics and complexity of evaluating SPARQL queries, which
ensures that queries following these rules can be optimised effectively by an
underlying RDF store. Moreover, we empirically verify the efficacy of the
heuristics using a set of openly available datasets and corresponding SPARQL
queries developed by a large pharmacology data integration project. The
experimental results show improvements in performance across 6 state-of-the-art
RDF stores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0588</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0588</id><created>2013-04-02</created><authors><author><keyname>Hale</keyname><forenames>Scott A.</forenames></author><author><keyname>Margetts</keyname><forenames>Helen</forenames></author><author><keyname>Yasseri</keyname><forenames>Taha</forenames></author></authors><title>Petition Growth and Success Rates on the UK No. 10 Downing Street
  Website</title><categories>cs.CY cs.SI physics.data-an physics.soc-ph</categories><comments>To appear in proceeding of WebSci'13, May 1-5, 2013, Paris, France</comments><journal-ref>WebSci '13 Proceedings of the 5th Annual ACM Web Science
  Conference, Pages 132-138</journal-ref><doi>10.1145/2464464.2464518</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Now that so much of collective action takes place online, web-generated data
can further understanding of the mechanics of Internet-based mobilisation. This
trace data offers social science researchers the potential for new forms of
analysis, using real-time transactional data based on entire populations,
rather than sample-based surveys of what people think they did or might do.
This paper uses a `big data' approach to track the growth of over 8,000
petitions to the UK Government on the No. 10 Downing Street website for two
years, analysing the rate of growth per day and testing the hypothesis that the
distribution of daily change will be leptokurtic (rather than normal) as
previous research on agenda setting would suggest. This hypothesis is
confirmed, suggesting that Internet-based mobilisation is characterized by
tipping points (or punctuated equilibria) and explaining some of the volatility
in online collective action. We find also that most successful petitions grow
quickly and that the number of signatures a petition receives on its first day
is a significant factor in explaining the overall number of signatures a
petition receives during its lifetime. These findings have implications for the
strategies of those initiating petitions and the design of web sites with the
aim of maximising citizen engagement with policy issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0589</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0589</id><created>2013-04-02</created><authors><author><keyname>Kassou</keyname><forenames>Meryem</forenames></author><author><keyname>Kjiri</keyname><forenames>Laila</forenames></author></authors><title>A Goal Question Metric Approach for Evaluating Security in a Service
  Oriented Architecture Context</title><categories>cs.SE cs.CR</categories><comments>12 pages</comments><journal-ref>IJCSI(International Journal of Computer Science Issues)Journal,
  Volume 9, Issue 4, No 1, July 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For interactions to be possible within the Service Oriented Architecture
(SOA) ecosystem, each actor must be enough confident of other actors to engage
safely in the interactions. Therefore, the establishing of objective metrics
tailored to the context of SOA that show security of a system and lead to
enhancements is very attractive. The purpose of our paper is to present a GQM
(Goal Question Metric) approach based on Standard security metrics and on SOA
maturity that can be a support for organizations to assess SOA Security and to
ensure the safety of their SOA based collaborations
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0600</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0600</id><created>2013-04-02</created><authors><author><keyname>Vadimovich</keyname><forenames>Bezhentcev Roman</forenames></author></authors><title>Software for creating pictures in the LaTeX environment</title><categories>cs.GR</categories><comments>8 pages, 1 figure, 2 formulas, sourcecode</comments><msc-class>68U05</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  To create a text with graphic instructions for output pictures into LATEX
document, we offer software that allows us to build a picture in WIZIWIG mode
and for setting the text with these graphical instructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0604</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0604</id><created>2013-04-02</created><updated>2014-02-07</updated><authors><author><keyname>Cardone</keyname><forenames>Martina</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author><author><keyname>Knopp</keyname><forenames>Raymond</forenames></author><author><keyname>Salim</keyname><forenames>Umer</forenames></author></authors><title>On the Gaussian Interference Channel with Half-Duplex Causal Cognition</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Journal on Selected Areas in
  Communications: Cognitive Radio Series</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the two-user Gaussian interference channel with
half-duplex causal cognition. This channel model consists of two
source-destination pairs sharing a common wireless channel. One of the sources,
referred to as the cognitive, overhears the other source, referred to as the
primary, through a noisy link and can therefore assist in sending the primary's
data. Due to practical constraints, the cognitive source is assumed to work in
half-duplex mode, that is, it cannot simultaneously transmit and receive. This
model is more relevant for practical cognitive radio systems than the classical
information theoretic cognitive channel model, where the cognitive source is
assumed to have a non-causal knowledge of the primary's message. Different
network topologies are considered, corresponding to different interference
scenarios: (i) the interference-symmetric scenario, where both destinations are
in the coverage area of the two sources and hence experience interference, and
(ii) the interference-asymmetric scenario, where one destination does not
suffer from interference. For each topology the sum-rate performance is studied
by first deriving the generalized Degrees of Freedom (gDoF), or &quot;sum-capacity
pre-log&quot; in the high-SNR regime, and then showing relatively simple coding
schemes that achieve a sum-rate upper bound to within a constant number of bits
for any SNR. Finally, the gDoF of the channel is compared to that of the
non-cooperative interference channel and to that of the non-causal cognitive
channel to identify the parameter regimes where half-duplex causal cognition is
useless in practice or attains its ideal ultimate limit, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0606</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0606</id><created>2013-04-02</created><authors><author><keyname>Bahrak</keyname><forenames>Behnam</forenames></author><author><keyname>Park</keyname><forenames>Jung-Min</forenames></author></authors><title>Security of Spectrum Learning in Cognitive Radios</title><categories>cs.CR</categories><comments>25 pages, 9 figures. arXiv admin note: text overlap with
  arXiv:0712.0035 by other authors</comments><journal-ref>SK Telecom Telecommunications Review, Vol. 22, Issue 6, Dec. 2012,
  pp. 850-864</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to delay and energy constraints, a cognitive radio may not be able to
perform spectrum sensing in all available channels. Therefore, a sensing policy
is needed to decide which channels to sense. The channel selection problem is
the problem of designing such a sensing policy to maximize throughput while
avoiding interference to primary users. The channel selection problem can be
formulated as a reinforcement learning problem. Channel selection schemes that
employ reinforcement machine learning algorithms are vulnerable to belief
manipulation attacks that contaminate the knowledge base of the learning
algorithms. In this paper, we analyze the security of channel selection
algorithms that are based on reinforcement learning and propose mitigation
techniques that make these algorithms more robust against belief manipulation
attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0608</identifier>
 <datestamp>2013-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0608</id><created>2013-04-02</created><updated>2013-04-03</updated><authors><author><keyname>Lee</keyname><forenames>Jung Hoon</forenames></author><author><keyname>Choi</keyname><forenames>Wan</forenames></author></authors><title>Optimal Feedback Rate Sharing Strategy in Zero-Forcing MIMO Broadcast
  Channels</title><categories>cs.IT math.IT</categories><comments>To appear, IEEE Transactions on Wireless Communications</comments><doi>10.1109/TWC.2013.050713.121410</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a multiple-input multiple-output broadcast channel
with limited feedback where all users share the feedback rates. Firstly, we
find the optimal feedback rate sharing strategy using zero-forcing transmission
scheme at the transmitter and random vector quantization at each user. We
mathematically prove that equal sharing of sum feedback size among all users is
the optimal strategy in the low signal-to-noise ratio (SNR) region, while
allocating whole feedback size to a single user is the optimal strategy in the
high SNR region. For the mid-SNR region, we propose a simple numerical method
to find the optimal feedback rate sharing strategy based on our analysis and
show that the equal allocation of sum feedback rate to a partial number of
users is the optimal strategy. It is also shown that the proposed simple
numerical method can be applicable to finding the optimal feedback rate sharing
strategy when different path losses of the users are taken into account. We
show that our proposed feedback rate sharing scheme can be extended to the
system with stream control and is still useful for the systems with other
techniques such as regularized zero-forcing and spherical cap codebook.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0611</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0611</id><created>2013-04-02</created><authors><author><keyname>Engstr&#xf6;m</keyname><forenames>Fredrik</forenames></author><author><keyname>Kontinen</keyname><forenames>Juha</forenames></author><author><keyname>V&#xe4;&#xe4;n&#xe4;nen</keyname><forenames>Jouko</forenames></author></authors><title>Dependence Logic with Generalized Quantifiers: Axiomatizations</title><categories>math.LO cs.LO</categories><comments>17 pages</comments><msc-class>03C80</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove two completeness results, one for the extension of dependence logic
by a monotone generalized quantifier Q with weak interpretation, weak in the
meaning that the interpretation of Q varies with the structures. The second
result considers the extension of dependence logic where Q is interpreted as
&quot;there exists uncountable many.&quot; Both of the axiomatizations are shown to be
sound and complete for FO(Q) consequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0617</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0617</id><created>2013-04-02</created><authors><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Mohammad</keyname><forenames>S. N.</forenames></author><author><keyname>Latif</keyname><forenames>K.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Khan</keyname><forenames>M. A.</forenames></author></authors><title>HEER: Hybrid Energy Efficient Reactive Protocol for Wireless Sensor
  Networks</title><categories>cs.NI</categories><comments>2nd IEEE Saudi International Electronics, Communications and
  Photonics Conference (SIECPC 13), 2013, Riyadh, Saudi Arabia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) consist of numerous sensors which send sensed
data to base station. Energy conservation is an important issue for sensor
nodes as they have limited power.Many routing protocols have been proposed
earlier for energy efficiency of both homogeneous and heterogeneous
environments. We can prolong our stability and network lifetime by reducing our
energy consumption. In this research paper, we propose a protocol designed for
the characteristics of a reactive homogeneous WSNs, HEER (Hybrid Energy
Efficient Reactive) protocol. In HEER, Cluster Head(CH) selection is based on
the ratio of residual energy of node and average energy of network. Moreover,
to conserve more energy, we introduce Hard Threshold (HT) and Soft Threshold
(ST). Finally, simulations show that our protocol has not only prolonged the
network lifetime but also significantly increased stability period.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0620</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0620</id><created>2013-04-02</created><authors><author><keyname>Zhang</keyname><forenames>Heng</forenames></author><author><keyname>Zhang</keyname><forenames>Yan</forenames></author></authors><title>Disjunctive Logic Programs versus Normal Logic Programs</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the expressive power of disjunctive and normal logic
programs under the stable model semantics over finite, infinite, or arbitrary
structures. A translation from disjunctive logic programs into normal logic
programs is proposed and then proved to be sound over infinite structures. The
equivalence of expressive power of two kinds of logic programs over arbitrary
structures is shown to coincide with that over finite structures, and coincide
with whether or not NP is closed under complement. Over finite structures, the
intranslatability from disjunctive logic programs to normal logic programs is
also proved if arities of auxiliary predicates and functions are bounded in a
certain way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0627</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0627</id><created>2013-04-02</created><authors><author><keyname>Penner</keyname><forenames>Orion</forenames></author><author><keyname>Pan</keyname><forenames>Raj K.</forenames></author><author><keyname>Petersen</keyname><forenames>Alexander M.</forenames></author><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author></authors><title>The case for caution in predicting scientists' future impact</title><categories>physics.soc-ph cs.DL physics.data-an</categories><comments>2 pages, 1 figure</comments><journal-ref>Physics Today 66, 8-9 (2013)</journal-ref><doi>10.1063/PT.3.1928</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We stress-test the career predictability model proposed by Acuna et al.
[Nature 489, 201-202 2012] by applying their model to a longitudinal career
data set of 100 Assistant professors in physics, two from each of the top 50
physics departments in the US. The Acuna model claims to predict h(t+\Delta t),
a scientist's h-index \Delta t years into the future, using a linear
combination of 5 cumulative career measures taken at career age t. Here we
investigate how the &quot;predictability&quot; depends on the aggregation of career data
across multiple age cohorts. We confirm that the Acuna model does a respectable
job of predicting h(t+\Delta t) up to roughly 6 years into the future when
aggregating all age cohorts together. However, when calculated using subsets of
specific age cohorts (e.g. using data for only t=3), we find that the model's
predictive power significantly decreases, especially when applied to early
career years. For young careers, the model does a much worse job of predicting
future impact, and hence, exposes a serious limitation. The limitation is
particularly concerning as early career decisions make up a significant
portion, if not the majority, of cases where quantitative approaches are likely
to be applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0635</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0635</id><created>2013-04-02</created><authors><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Waseem</keyname><forenames>M.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Latif</keyname><forenames>K.</forenames></author><author><keyname>Javaid</keyname><forenames>A.</forenames></author></authors><title>ACH: Away Cluster Heads Scheme for Energy Efficient Clustering Protocols
  in WSNs</title><categories>cs.NI</categories><comments>2nd IEEE Saudi International Electronics, Communications and
  Photonics Conference (SIECPC 13), 2013, Riyadh, Saudi Arabia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the routing protocols for distributed wireless sensor
networks. The conventional protocols for WSNs like Low Energy adaptive
Clustering Hierarchy (LEACH), Stable Election Protocol (SEP), Threshold
Sensitive Energy Efficient Network (TEEN), Distributed Energy Efficient
Clustering Protocol (DEEC) may not be optimal. We propose a scheme called Away
Cluster Head (ACH) which effectively increases the efficiency of conventional
clustering based protocols in terms of stability period and number of packets
sent to base station (BS). We have implemented ACH scheme on LEACH, SEP, TEEN
and DEEC. Simulation results show that LEACHACH, SEP-ACH, TEEN-ACH and DEEC-ACH
performs better than LEACH, SEP, TEEN and DEEC respectively in terms of
stability period and number of packets sent to BS. The stability period of the
existing protocols prolongs by implementing ACH on them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0637</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0637</id><created>2013-04-02</created><authors><author><keyname>Chen</keyname><forenames>Xiaomin</forenames></author><author><keyname>Jukan</keyname><forenames>Admela</forenames></author><author><keyname>M&#xe9;dard</keyname><forenames>Muriel</forenames></author></authors><title>A Novel Network Coded Parallel Transmission Framework for High-Speed
  Ethernet</title><categories>cs.NI</categories><comments>6 pages, 8 figures, Submitted to Globecom2013</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Parallel transmission, as defined in high-speed Ethernet standards, enables
to use less expensive optoelectronics and offers backwards compatibility with
legacy Optical Transport Network (OTN) infrastructure. However, optimal
parallel transmission does not scale to large networks, as it requires
computationally expensive multipath routing algorithms to minimize differential
delay, and thus the required buffer size, optimize traffic splitting ratio, and
ensure frame synchronization. In this paper, we propose a novel framework for
high-speed Ethernet, which we refer to as network coded parallel transmission,
capable of effective buffer management and frame synchronization without the
need for complex multipath algorithms in the OTN layer. We show that using
network coding can reduce the delay caused by packet reordering at the
receiver, thus requiring a smaller overall buffer size, while improving the
network throughput. We design the framework in full compliance with high-speed
Ethernet standards specified in IEEE802.3ba and present solutions for network
encoding, data structure of coded parallel transmission, buffer management and
decoding at the receiver side. The proposed network coded parallel transmission
framework is simple to implement and represents a potential major breakthrough
in the system design of future high-speed Ethernet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0640</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0640</id><created>2013-04-02</created><authors><author><keyname>Caron</keyname><forenames>Louis-Charles</forenames></author><author><keyname>D'Haene</keyname><forenames>\and Michiel</forenames></author><author><keyname>Mailhot</keyname><forenames>\and Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Schrauwen</keyname><forenames>\and Benjamin</forenames></author><author><keyname>Rouat</keyname><forenames>\and Jean</forenames></author></authors><title>Event management for large scale event-driven digital hardware spiking
  neural networks</title><categories>cs.NE cs.AI cs.DC</categories><doi>10.1016/j.neunet.2013.02.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interest in brain-like computation has led to the design of a plethora of
innovative neuromorphic systems. Individually, spiking neural networks (SNNs),
event-driven simulation and digital hardware neuromorphic systems get a lot of
attention. Despite the popularity of event-driven SNNs in software, very few
digital hardware architectures are found. This is because existing hardware
solutions for event management scale badly with the number of events. This
paper introduces the structured heap queue, a pipelined digital hardware data
structure, and demonstrates its suitability for event management. The
structured heap queue scales gracefully with the number of events, allowing the
efficient implementation of large scale digital hardware event-driven SNNs. The
scaling is linear for memory, logarithmic for logic resources and constant for
processing time. The use of the structured heap queue is demonstrated on
field-programmable gate array (FPGA) with an image segmentation experiment and
a SNN of 65~536 neurons and 513~184 synapses. Events can be processed at the
rate of 1 every 7 clock cycles and a 406$\times$158 pixel image is segmented in
200 ms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0646</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0646</id><created>2013-04-02</created><authors><author><keyname>Holtkamp</keyname><forenames>Hauke</forenames></author></authors><title>Decentralized Synchronization for Wireless Sensor Networks</title><categories>cs.NI</categories><comments>59 pages, 22 figures, master thesis 2008</comments><acm-class>C.2.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Due to their heavy restrictions on the hardware side, Wireless Sensor
Networks (WSN) require specially adapted synchronization protocols to maximize
measurement precision and minimize computation efforts and energy costs. A
promising approach is given by the 'Firefly Protocol'. Inspired by the behavior
of fireflies it is intrinsically robust, specific to the wireless broadcast
nature of WSNs and promises high precision. So far only theoretically
evaluated, this thesis implements the 'Firefly Protocol' on a system of MICAz
Berkeley motes using TinyOS 2.x. In order to implement the theoretical
framework on actual hardware, several adaptations were made to compensate
hardware delays. Although Berkeley motes have the advantage of being readily
available and highly flexible, they bear many delay sources which have to be
addressed. In small networks, the protocol was found to deliver precisions up
to three microseconds over one hop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0660</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0660</id><created>2013-04-02</created><authors><author><keyname>Garg</keyname><forenames>Pranav</forenames></author><author><keyname>Madhusudan</keyname><forenames>P.</forenames></author><author><keyname>Parlato</keyname><forenames>Gennaro</forenames></author></authors><title>Quantified Data Automata on Skinny Trees: an Abstract Domain for Lists</title><categories>cs.PL cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new approach to heap analysis through an abstract domain of
automata, called automatic shapes. The abstract domain uses a particular kind
of automata, called quantified data automata on skinny trees (QSDAs), that
allows to define universally quantified properties of singly-linked lists. To
ensure convergence of the abstract fixed-point computation, we introduce a
sub-class of QSDAs called elastic QSDAs, which also form an abstract domain. We
evaluate our approach on several list manipulating programs and we show that
the proposed domain is powerful enough to prove a large class of these programs
correct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0662</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0662</id><created>2013-04-02</created><authors><author><keyname>Dey</keyname><forenames>Tamal K.</forenames></author><author><keyname>Fan</keyname><forenames>Fengtao</forenames></author><author><keyname>Wang</keyname><forenames>Yusu</forenames></author></authors><title>Graph Induced Complex on Point Data</title><categories>cs.CG math.AT</categories><comments>29th Annual Symposium on Computational Geometry, 2013 (to appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The efficiency of extracting topological information from point data depends
largely on the complex that is built on top of the data points. From a
computational viewpoint, the most favored complexes for this purpose have so
far been Vietoris-Rips and witness complexes. While the Vietoris-Rips complex
is simple to compute and is a good vehicle for extracting topology of sampled
spaces, its size is huge--particularly in high dimensions. The witness complex
on the other hand enjoys a smaller size because of a subsampling, but fails to
capture the topology in high dimensions unless imposed with extra structures.
We investigate a complex called the {\em graph induced complex} that, to some
extent, enjoys the advantages of both. It works on a subsample but still
retains the power of capturing the topology as the Vietoris-Rips complex. It
only needs a graph connecting the original sample points from which it builds a
complex on the subsample thus taming the size considerably. We show that, using
the graph induced complex one can (i) infer the one dimensional homology of a
manifold from a very lean subsample, (ii) reconstruct a surface in three
dimension from a sparse subsample without computing Delaunay triangulations,
(iii) infer the persistent homology groups of compact sets from a sufficiently
dense sample. We provide experimental evidences in support of our theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0664</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0664</id><created>2013-04-02</created><authors><author><keyname>Dey</keyname><forenames>Tamal K.</forenames></author><author><keyname>Hirani</keyname><forenames>Anil N.</forenames></author><author><keyname>Krishnamoorthy</keyname><forenames>Bala</forenames></author><author><keyname>Smith</keyname><forenames>Gavin</forenames></author></authors><title>Edge Contractions and Simplicial Homology</title><categories>cs.CG math.AT math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the effect of edge contractions on simplicial homology because these
contractions have turned to be useful in various applications involving
topology. It was observed previously that contracting edges that satisfy the so
called link condition preserves homeomorphism in low dimensional complexes, and
homotopy in general. But, checking the link condition involves computation in
all dimensions, and hence can be costly, especially in high dimensional
complexes. We define a weaker and more local condition called the p-link
condition for each dimension p, and study its effect on edge contractions. We
prove the following: (i) For homology groups, edges satisfying the p- and
(p-1)-link conditions can be contracted without disturbing the p-dimensional
homology group. (ii) For relative homology groups, the (p-1)-, and the
(p-2)-link conditions suffice to guarantee that the contraction does not
introduce any new class in any of the resulting relative homology groups,
though some of the existing classes can be destroyed. Unfortunately, the
surjection in relative homolgy groups does not guarantee that no new relative
torsion is created. (iii) For torsions, edges satisfying the p-link condition
alone can be contracted without creating any new relative torsion and the
p-link condition cannot be avoided. The results on relative homology and
relative torsion are motivated by recent results on computing optimal
homologous chains, which state that such problems can be solved by linear
programming if the complex has no relative torsion. Edge contractions that do
not introduce new relative torsions, can safely be availed in these contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0678</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0678</id><created>2013-04-02</created><updated>2014-07-21</updated><authors><author><keyname>Alamo</keyname><forenames>T.</forenames></author><author><keyname>Tempo</keyname><forenames>R.</forenames></author><author><keyname>Luque</keyname><forenames>A.</forenames></author><author><keyname>Ramirez</keyname><forenames>D. R.</forenames></author></authors><title>Randomized Methods for Design of Uncertain Systems: Sample Complexity
  and Sequential Algorithms</title><categories>cs.SY math.OC</categories><comments>16 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study randomized methods for feedback design of uncertain
systems. The first contribution is to derive the sample complexity of various
constrained control problems. In particular, we show the key role played by the
binomial distribution and related tail inequalities, and compute the sample
complexity. This contribution significantly improves the existing results by
reducing the number of required samples in the randomized algorithm. These
results are then applied to the analysis of worst-case performance and design
with robust optimization. The second contribution of the paper is to introduce
a general class of sequential algorithms, denoted as Sequential Probabilistic
Validation (SPV). In these sequential algorithms, at each iteration, a
candidate solution is probabilistically validated, and corrected if necessary,
to meet the required specifications. The results we derive provide the sample
complexity which guarantees that the solutions obtained with SPV algorithms
meet some pre-specified probabilistic accuracy and confidence. The performance
of these algorithms is illustrated and compared with other existing methods
using a numerical example dealing with robust system identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0680</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0680</id><created>2013-04-02</created><updated>2015-07-07</updated><authors><author><keyname>Avigad</keyname><forenames>Jeremy</forenames></author><author><keyname>Kapulkin</keyname><forenames>Chris</forenames></author><author><keyname>Lumsdaine</keyname><forenames>Peter LeFanu</forenames></author></authors><title>Homotopy limits in type theory</title><categories>math.LO cs.LO math.CT</categories><comments>33 pages; v3: theorem numbering changed since v2 to match journal
  version</comments><msc-class>03B70, 55U35</msc-class><acm-class>D.2.4; F.4.1</acm-class><doi>10.1017/S0960129514000498</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Working in homotopy type theory, we provide a systematic study of homotopy
limits of diagrams over graphs, formalized in the Coq proof assistant. We
discuss some of the challenges posed by this approach to formalizing
homotopy-theoretic material. We also compare our constructions with the more
classical approach to homotopy limits via fibration categories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0681</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0681</id><created>2013-04-02</created><authors><author><keyname>Mart&#xed;nez</keyname><forenames>H&#xe9;ctor</forenames><affiliation>Dpto. de Ingenier&#xed;a y Ciencia de los Computadores, Universidad Jaume I, Castell&#xf3;n, Spain</affiliation></author><author><keyname>T&#xe1;rraga</keyname><forenames>Joaqu&#xed;n</forenames><affiliation>Computational Genomics Institute, Centro de Investigaci&#xf3;n Pr&#xed;ncipe Felipe, Valencia, Spain</affiliation></author><author><keyname>Medina</keyname><forenames>Ignacio</forenames><affiliation>Computational Genomics Institute, Centro de Investigaci&#xf3;n Pr&#xed;ncipe Felipe, Valencia, Spain</affiliation></author><author><keyname>Barrachina</keyname><forenames>Sergio</forenames><affiliation>Dpto. de Ingenier&#xed;a y Ciencia de los Computadores, Universidad Jaume I, Castell&#xf3;n, Spain</affiliation></author><author><keyname>Castillo</keyname><forenames>Maribel</forenames><affiliation>Dpto. de Ingenier&#xed;a y Ciencia de los Computadores, Universidad Jaume I, Castell&#xf3;n, Spain</affiliation></author><author><keyname>Dopazo</keyname><forenames>Joaqu&#xed;n</forenames><affiliation>Computational Genomics Institute, Centro de Investigaci&#xf3;n Pr&#xed;ncipe Felipe, Valencia, Spain</affiliation></author><author><keyname>Quintana-Ort&#xed;</keyname><forenames>Enrique S.</forenames><affiliation>Dpto. de Ingenier&#xed;a y Ciencia de los Computadores, Universidad Jaume I, Castell&#xf3;n, Spain</affiliation></author></authors><title>Concurrent and Accurate RNA Sequencing on Multicore Platforms</title><categories>q-bio.GN cs.DC q-bio.QM</categories><report-no>UJI ICC 2013-03-01</report-no><acm-class>D.1.3; J.3</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we introduce a novel parallel pipeline for fast and accurate
mapping of RNA sequences on servers equipped with multicore processors. Our
software, named HPG-Aligner, leverages the speed of the Burrows-Wheeler
Transform to map a large number of RNA fragments (reads) rapidly, as well as
the accuracy of the Smith-Waterman algorithm, that is employed to deal with
conflictive reads. The aligner is complemented with a careful strategy to
detect splice junctions based on the division of RNA reads into short segments
(or seeds), which are then mapped onto a number of candidate alignment
locations, providing useful information for the successful alignment of the
complete reads.
  Experimental results on platforms with AMD and Intel multicore processors
report the remarkable parallel performance of HPG-Aligner, on short and long
RNA reads, which excels in both execution time and sensitivity to an
state-of-the-art aligner such as TopHat 2 built on top of Bowtie and Bowtie 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0682</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0682</id><created>2013-04-02</created><updated>2016-01-18</updated><authors><author><keyname>Aksoylar</keyname><forenames>Cem</forenames></author><author><keyname>Atia</keyname><forenames>George</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Sparse Signal Processing with Linear and Non-Linear Observations: A
  Unified Shannon Theoretic Approach</title><categories>cs.IT cs.LG math.IT math.ST stat.ML stat.TH</categories><comments>Revised version, submitted to Trans. IT. Tighter bounds, analysis for
  more applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive fundamental sample complexity bounds for recovering sparse and
structured signals for linear and nonlinear observation models including sparse
regression, group testing, multivariate regression and problems with missing
features. In general, sparse signal processing problems can be characterized in
terms of the following Markovian property. We are given a set of $N$ variables
$X_1,X_2,\ldots,X_N$, and there is an unknown subset of variables $S \subset
[N]$ that are \emph{relevant} for predicting outcomes $Y$. More specifically,
when $Y$ is conditioned on $\{X_n\}_{n\in S}$ it is conditionally independent
of the other variables, $\{X_n\}_{n \not \in S}$. Our goal is to identify the
set $S$ from samples of the variables $X$ and the associated outcomes $Y$. We
characterize this problem as a version of the noisy channel coding problem.
Using asymptotic information theoretic analyses, we establish mutual
information formulas that provide sufficient and necessary conditions on the
number of samples required to successfully recover the salient variables. These
mutual information expressions unify conditions for both linear and nonlinear
observations. We then compute sample complexity bounds for the aforementioned
models, based on the mutual information expressions in order to demonstrate the
applicability and flexibility of our results in general sparse signal
processing models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0713</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0713</id><created>2013-04-02</created><authors><author><keyname>Beck</keyname><forenames>Chris</forenames></author><author><keyname>Li</keyname><forenames>Yuan</forenames></author></authors><title>Represent MOD function by low degree polynomial with unbounded one-sided
  error</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we prove tight lower bounds on the smallest degree of a
nonzero polynomial in the ideal generated by $MOD_q$ or $\neg MOD_q$ in the
polynomial ring $F_p[x_1, \ldots, x_n]/(x_1^2 = x_1, \ldots, x_n^2 = x_n)$,
$p,q$ are coprime, which is called \emph{immunity} over $F_p$. The immunity of
$MOD_q$ is lower bounded by $\lfloor (n+1)/2 \rfloor$, which is achievable when
$n$ is a multiple of $2q$; the immunity of $\neg MOD_q$ is exactly $\lfloor
(n+q-1)/q \rfloor$ for every $q$ and $n$. Our result improves the previous
bound $\lfloor \frac{n}{2(q-1)} \rfloor$ by Green.
  We observe how immunity over $F_p$ is related to $\acc$ circuit lower bound.
For example, if the immunity of $f$ over $F_p$ is lower bounded by $n/2 -
o(\sqrt{n})$, and $|1_f| = \Omega(2^n)$, then $f$ requires $\acc$ circuit of
exponential size to compute.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0715</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0715</id><created>2013-03-31</created><authors><author><keyname>B&#xf6;l&#xf6;ni</keyname><forenames>Ladislau</forenames></author></authors><title>A cookbook of translating English to Xapi</title><categories>cs.AI cs.CL</categories><report-no>XTR-001</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Xapagy cognitive architecture had been designed to perform narrative
reasoning: to model and mimic the activities performed by humans when
witnessing, reading, recalling, narrating and talking about stories. Xapagy
communicates with the outside world using Xapi, a simplified, &quot;pidgin&quot; language
which is strongly tied to the internal representation model (instances, scenes
and verb instances) and reasoning techniques (shadows and headless shadows).
While not fully a semantic equivalent of natural language, Xapi can represent a
wide range of complex stories. We illustrate the representation technique used
in Xapi through examples taken from folk physics, folk psychology as well as
some more unusual literary examples. We argue that while the Xapi model
represents a conceptual shift from the English representation, the mapping is
logical and consistent, and a trained knowledge engineer can translate between
English and Xapi at near-native speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0722</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0722</id><created>2013-02-23</created><authors><author><keyname>Diouf</keyname><forenames>Mamadou Diallo</forenames></author><author><keyname>Kora</keyname><forenames>Ahmed D.</forenames></author><author><keyname>Ringar</keyname><forenames>Octave</forenames></author><author><keyname>Aupetit-Berthelemot</keyname><forenames>C.</forenames></author></authors><title>Evolution to 200G Passive Optical Network</title><categories>cs.NI</categories><comments>http://www.davidpublishing.com/davidpublishing/Upfile/2/7/2013/2013020707494407.pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New generation passive optical network aims at providing more than 100 Gb/s
capacity. Thanks to recent progress enabling a variety of optical transceivers
up to 40 Gb/s, many evolution possibilities to 200G PONs (passive optical
network) could be investigated. This work proposes two directly deployable
cases of evolution to 200G PON based on the combination of these improved
optical transceivers and WDM (wavelength division multiplexing). The physical
layer of the optical network has been simulated with OptiSystem software to
show the communication links performances behavior when considering key
components parameters in order to achieve good network design for a given area.
The complexity of the proposed architectures and financial cost comparisons are
also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0725</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0725</id><created>2013-03-11</created><authors><author><keyname>Ashok</keyname><forenames>P.</forenames></author><author><keyname>Nawaz</keyname><forenames>G. M Kadhar</forenames></author><author><keyname>Elayaraja</keyname><forenames>E.</forenames></author><author><keyname>Vadivel</keyname><forenames>V.</forenames></author></authors><title>Improved Performance of Unsupervised Method by Renovated K-Means</title><categories>cs.LG cs.CV stat.ML</categories><comments>7 pages, to strengthen the k means algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is a separation of data into groups of similar objects. Every
group called cluster consists of objects that are similar to one another and
dissimilar to objects of other groups. In this paper, the K-Means algorithm is
implemented by three distance functions and to identify the optimal distance
function for clustering methods. The proposed K-Means algorithm is compared
with K-Means, Static Weighted K-Means (SWK-Means) and Dynamic Weighted K-Means
(DWK-Means) algorithm by using Davis Bouldin index, Execution Time and
Iteration count methods. Experimental results show that the proposed K-Means
algorithm performed better on Iris and Wine dataset when compared with other
three clustering methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0726</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0726</id><created>2013-03-15</created><authors><author><keyname>Silva</keyname><forenames>Jos&#xe9; Fernando M.</forenames></author><author><keyname>Almeida</keyname><forenames>Jo&#xe3;o Em&#xed;lio</forenames></author><author><keyname>Pereira</keyname><forenames>Ant&#xf3;nio</forenames></author><author><keyname>Rossetti</keyname><forenames>Rosaldo J. F.</forenames></author><author><keyname>Coelho</keyname><forenames>Ant&#xf3;nio Le&#xe7;a</forenames></author></authors><title>Preliminary Experiments with EVA - Serious Games Virtual Fire Drill
  Simulator</title><categories>cs.CY</categories><comments>ECMS 2013 - 27th European Conference on Modelling and Simulation,
  {\AA}lesund, Norway May 27-30, 2013 (accepted)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fire keeps claiming a large number of victims in building fires. Although
there are ways to minimize such events, fire drills are used to train the
building occupants for emergency situations. However, organizing and implement
these exercises is a complex task, and sometimes not sucessfull. Furthermore,
fire drills require the mobilization of some finantial resources and time, and
affect the normal functioning of the site where they occur. To overcome the
aforementioned issues, computer games have a set of features that might
overcome this problem. They offer engagement to their players, keeping them
focused, and providing training to real life situations. The game evaluate
users, providing them some feedback, making possible for the players to improve
their performance. The proposed methodology aims to study the viability of
using a game that recreates a fire drill in a 3D environment using Serious
Games. The information acquired through the player's performance is very
valuable and will be later used to implement an artificial population. A sample
of 20 subjects was selected to test the application. Preliminary results are
promising, showing that the exercise had a positive impact on users. Moreover,
the data acquired is of great important and will be later used to demonstrate
the possibility of creating an artificial population based on human behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0727</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0727</id><created>2013-04-01</created><authors><author><keyname>Leznik</keyname><forenames>Michael</forenames></author></authors><title>Hubs and Authorities of the English Premier League for 2010-2011</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work author applies well known web search algorithm Hyperlink -
Induced Topic Search (HITS) to problem of ranking football teams in English
Premier League (EPL). The algorithm allows the ranking of the teams using the
notions of hubs and authorities well known for ranking pages in the World Wide
Web. Results of the games introduced as a graph where losing team 'gives a
link' to a winning team and, if draw registered both team give links to each
other. In case of a win link is weighted as three points in adjacent matrix and
in case of draw as one point. Author uses notion of authority in order to
define team which win a game and hub as a team which lose a game, the winner of
the competition defined as the 'worst' hub, team that didn't reinforced any
other team. Using this ranking system, the champion's team, which is a 'worst
hub' must not lose, or draw games to other 'good authorities' teams. If by the
end of the competition there are teams with an equal number of wins and losses
then the team which has beaten more teams with higher authority ranks, wins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0728</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0728</id><created>2013-04-02</created><authors><author><keyname>Dvorak</keyname><forenames>Zdenek</forenames></author><author><keyname>Klimosova</keyname><forenames>Tereza</forenames></author></authors><title>Strong immersions and maximum degree</title><categories>math.CO cs.DM</categories><comments>10 pages</comments><msc-class>05C38 (Primary) 05C40, 05C75, 05C83 (Secondary)</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph H is strongly immersed in G if G is obtained from H by a sequence of
vertex splittings (i.e., lifting some pairs of incident edges and removing the
vertex) and edge removals. Equivalently, vertices of H are mapped to distinct
vertices of G (branch vertices) and edges of H are mapped to pairwise
edge-disjoint paths in G, each of them joining the branch vertices
corresponding to the ends of the edge and not containing any other branch
vertices. We show that there exists a function d:N-&gt;N such that for all graphs
H and G, if G contains a strong immersion of the star K_{1,d(Delta(H))|V(H)|}
whose branch vertices are Delta(H)-edge-connected to one another, then H is
strongly immersed in G. This has a number of structural consequences for graphs
avoiding a strong immersion of H. In particular, a class C of simple
4-edge-connected graphs contains all graphs of maximum degree 4 as strong
immersions if and only if C has either unbounded maximum degree or unbounded
tree-width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0729</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0729</id><created>2013-03-31</created><authors><author><keyname>Hayajneh</keyname><forenames>Mohammad</forenames></author><author><keyname>AbuAli</keyname><forenames>Najah</forenames></author></authors><title>Closed-Form Rate Outage Probability for OFDMA Multi-Hop Broadband
  Wireless Networks under Nakagami-m Channels</title><categories>cs.NI</categories><comments>10 pages, 5 figures, 1 table, Journal</comments><journal-ref>IJASCSE, VOL 1, ISSUE 4, Dec. 31, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rate outage probability is an important performance metric to measure the
level of quality of service (QoS) in the 4th Generation (4G) broadband access
networks. Thus, in this paper, we calculate a closed form expression of the
rate outage probability for a given user in a down-link multi-hop OFDMA-based
system encountered as a result of links channel variations. The channel random
behavior on different subcarriers allocated to a given user is assumed to
follow independent non-identical Nakagami-m distributions. Besides the rate
outage probability formulas for single hop and multi-hop networks, we also
derive a novel closed form formulas for the moment generating function,
probability distribution function (pdf), and the cumulative distribution
function (cdf) of a product of independent non-identical Gamma distributed
random variables (RVs). These RVs are functions of the attainable
signal-to-noise power ratio (SNR) on the allocated group of subcarriers. For
single-hop scenario, inspired by the rate outage probability closed formula, we
formulate an optimization problem in which we allocate subcarriers to users
such that the total transmission rate is maximized while catering for fairness
for all users. In the proposed formulation, fairness is considered by
guaranteeing a minimum rate outage probability for each admitted user
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0730</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0730</id><created>2013-04-02</created><authors><author><keyname>Feldman</keyname><forenames>Vitaly</forenames></author><author><keyname>Kothari</keyname><forenames>Pravesh</forenames></author><author><keyname>Vondrak</keyname><forenames>Jan</forenames></author></authors><title>Representation, Approximation and Learning of Submodular Functions Using
  Low-rank Decision Trees</title><categories>cs.LG cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of approximate representation and learning of
submodular functions over the uniform distribution on the Boolean hypercube
$\{0,1\}^n$. Our main result is the following structural theorem: any
submodular function is $\epsilon$-close in $\ell_2$ to a real-valued decision
tree (DT) of depth $O(1/\epsilon^2)$. This immediately implies that any
submodular function is $\epsilon$-close to a function of at most
$2^{O(1/\epsilon^2)}$ variables and has a spectral $\ell_1$ norm of
$2^{O(1/\epsilon^2)}$. It also implies the closest previous result that states
that submodular functions can be approximated by polynomials of degree
$O(1/\epsilon^2)$ (Cheraghchi et al., 2012). Our result is proved by
constructing an approximation of a submodular function by a DT of rank
$4/\epsilon^2$ and a proof that any rank-$r$ DT can be $\epsilon$-approximated
by a DT of depth $\frac{5}{2}(r+\log(1/\epsilon))$.
  We show that these structural results can be exploited to give an
attribute-efficient PAC learning algorithm for submodular functions running in
time $\tilde{O}(n^2) \cdot 2^{O(1/\epsilon^{4})}$. The best previous algorithm
for the problem requires $n^{O(1/\epsilon^{2})}$ time and examples (Cheraghchi
et al., 2012) but works also in the agnostic setting. In addition, we give
improved learning algorithms for a number of related settings.
  We also prove that our PAC and agnostic learning algorithms are essentially
optimal via two lower bounds: (1) an information-theoretic lower bound of
$2^{\Omega(1/\epsilon^{2/3})}$ on the complexity of learning monotone
submodular functions in any reasonable model; (2) computational lower bound of
$n^{\Omega(1/\epsilon^{2/3})}$ based on a reduction to learning of sparse
parities with noise, widely-believed to be intractable. These are the first
lower bounds for learning of submodular functions over the uniform
distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0732</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0732</id><created>2013-03-30</created><authors><author><keyname>Foukalas</keyname><forenames>F.</forenames></author><author><keyname>Karetsos</keyname><forenames>G. T.</forenames></author></authors><title>On the Performance of Adaptive Modulation in Cognitive Radio Networks</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1210.6910</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the performance of cognitive radio networks (CRNs) when
incorporating adaptive modulation at the physical layer. Three types of CRNs
are considered, namely opportunistic spectrum access (OSA), spectrum sharing
(SS) and sensing-based SS. We obtain closed-form expressions for the average
spectral efficiency achieved at the secondary network and the optimal power
allocation for both continuous and discrete rate types of adaptive modulation
assuming perfect channel state information. The obtained numerical results show
the achievable performance gain in terms of average spectral efficiency and the
impact on power allocation when adaptive modulation is implemented at the
physical layer that is due to the effect of the cut-off level that is
determined from the received signal-to-noise ratio for each CRN type. The
performance assessment is taking place for different target bit error rate
values and fading regions, thereby providing useful performance insights for
various possible implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0733</identifier>
 <datestamp>2013-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0733</id><created>2013-04-02</created><updated>2013-06-10</updated><authors><author><keyname>Jir&#xe1;skov&#xe1;</keyname><forenames>Galina</forenames></author><author><keyname>Masopust</keyname><forenames>Tom&#xe1;&#x161;</forenames></author></authors><title>On the State Complexity of the Reverse of R- and J-trivial Regular
  Languages</title><categories>cs.FL</categories><comments>Full version of the paper accepted for DCFS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The tight upper bound on the state complexity of the reverse of R-trivial and
J-trivial regular languages of the state complexity n is 2^{n-1}. The witness
is ternary for R-trivial regular languages and (n-1)-ary for J-trivial regular
languages. In this paper, we prove that the bound can be met neither by a
binary R-trivial regular language nor by a J-trivial regular language over an
(n-2)-element alphabet. We provide a characterization of tight bounds for
R-trivial regular languages depending on the state complexity of the language
and the size of its alphabet. We show the tight bound for J-trivial regular
languages over an (n-2)-element alphabet and a few tight bounds for binary
J-trivial regular languages. The case of J-trivial regular languages over an
(n-k)-element alphabet, for 2 &lt;= k &lt;= n-3, is open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0740</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0740</id><created>2013-04-02</created><authors><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>He</keyname><forenames>Xiaofei</forenames></author></authors><title>O(logT) Projections for Stochastic Optimization of Smooth and Strongly
  Convex Functions</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional algorithms for stochastic optimization require projecting the
solution at each iteration into a given domain to ensure its feasibility. When
facing complex domains, such as positive semi-definite cones, the projection
operation can be expensive, leading to a high computational cost per iteration.
In this paper, we present a novel algorithm that aims to reduce the number of
projections for stochastic optimization. The proposed algorithm combines the
strength of several recent developments in stochastic optimization, including
mini-batch, extra-gradient, and epoch gradient descent, in order to effectively
explore the smoothness and strong convexity. We show, both in expectation and
with a high probability, that when the objective function is both smooth and
strongly convex, the proposed algorithm achieves the optimal $O(1/T)$ rate of
convergence with only $O(\log T)$ projections. Our empirical study verifies the
theoretical result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0748</identifier>
 <datestamp>2013-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0748</id><created>2013-04-02</created><updated>2013-09-10</updated><authors><author><keyname>Khan</keyname><forenames>Faisal Shah</forenames></author><author><keyname>Phoenix</keyname><forenames>Simon J. D.</forenames></author></authors><title>Mini-maximizing two qubit quantum computations</title><categories>quant-ph cs.GT</categories><comments>13 pages, 1 figure. A brief discussion, explicitly stating the
  insights gained into two qubit quantum computations using min-max, has been
  added to version 2</comments><doi>10.1007/s11128-013-0640-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two qubit quantum computations are viewed as two player, strictly competitive
games and a game-theoretic measure of optimality of these computations is
developed. To this end, the geometry of Hilbert space of quantum computations
is used to establish the equivalence of game-theoretic solution concepts of
Nash equilibrium and mini-max outcomes in games of this type, and quantum
mechanisms are designed for realizing these mini-max outcomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0751</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0751</id><created>2013-03-02</created><authors><author><keyname>Hall</keyname><forenames>Matthew</forenames></author></authors><title>A Cumulative Multi-Niching Genetic Algorithm for Multimodal Function
  Optimization</title><categories>cs.NE</categories><journal-ref>International Journal of Advanced Research in Artificial
  Intelligence 1(9) 6 - 13 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a cumulative multi-niching genetic algorithm (CMN GA),
designed to expedite optimization problems that have computationally-expensive
multimodal objective functions. By never discarding individuals from the
population, the CMN GA makes use of the information from every objective
function evaluation as it explores the design space. A fitness-related
population density control over the design space reduces unnecessary objective
function evaluations. The algorithm's novel arrangement of genetic operations
provides fast and robust convergence to multiple local optima. Benchmark tests
alongside three other multi-niching algorithms show that the CMN GA has a
greater convergence ability and provides an order-of-magnitude reduction in the
number of objective function evaluations required to achieve a given level of
convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0786</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0786</id><created>2013-04-02</created><updated>2013-10-04</updated><authors><author><keyname>Sreenivasan</keyname><forenames>Sameet</forenames></author></authors><title>Quantitative analysis of the evolution of novelty in cinema through
  crowdsourced keywords</title><categories>physics.soc-ph cs.CY</categories><comments>23 pages, 12 figures (including supplementary material)</comments><journal-ref>Scientific Reports 3, Article number: 2758 (2013)</journal-ref><doi>10.1038/srep02758</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generation of novelty is central to any creative endeavor. Novelty
generation and the relationship between novelty and individual hedonic value
have long been subjects of study in social psychology. However, few studies
have utilized large-scale datasets to quantitatively investigate these issues.
Here we consider the domain of American cinema and explore these questions
using a database of films spanning a 70 year period. We use crowdsourced
keywords from the Internet Movie Database as a window into the contents of
films, and prescribe novelty scores for each film based on occurrence
probabilities of individual keywords and keyword-pairs. These scores provide
revealing insights into the dynamics of novelty in cinema. We investigate how
novelty influences the revenue generated by a film, and find a relationship
that resembles the Wundt-Berlyne curve. We also study the statistics of keyword
occurrence and the aggregate distribution of keywords over a 100 year period.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0788</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0788</id><created>2013-04-02</created><authors><author><keyname>Helbing</keyname><forenames>Dirk</forenames></author></authors><title>Introduction: The FuturICT Knowledge Accelerator Towards a More
  Resilient and Sustainable Future</title><categories>physics.soc-ph cs.CY nlin.AO</categories><journal-ref>Eur. Phys. J. Special Topics vol. 214, pp. 5-9 (2012)</journal-ref><doi>10.1140/epjst/e2012-01685-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The FuturICT project is a response to the European Flagship Call in the Area
of Future and Emerging Technologies, which is planning to spend 1 billion EUR
on each of two flagship projects over a period of 10 years. FuturICT seeks to
create an open, global but decentralized, democratically controlled information
platform that will use online data and real-time measurements together with
novel theoretical models and experimental methods to achieve a paradigm shift
in our understanding of today's strongly interdependent and complex world and
make our techno-socio-economic systems more flexible, adaptive, resilient,
sustainable, and livable through a participatory approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0791</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0791</id><created>2013-04-02</created><authors><author><keyname>Lu</keyname><forenames>Yuan</forenames></author><author><keyname>Duel-Hallen</keyname><forenames>Alexandra</forenames></author></authors><title>Adaptation to the Primary User CSI in Cognitive Radio Sensing and Access</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Cognitive Radio (CR) networks, multiple secondary network users (SUs)
attempt to communicate over wide potential spectrum without causing significant
interference to the Primary Users (PUs). A spectrum sensing algorithm is a
critical component of any sensing strategy. Performance of conventional
spectrum detection methods is severely limited when the average SNR of the
fading channel between the PU transmitter and the SU sensor is low. Cooperative
sensing and advanced detection techniques only partially remedy this problem. A
key limitation of conventional approaches is that the sensing threshold is
determined from the miss detection rate averaged over the fading distribution.
In this paper, the threshold is adapted to the instantaneous PU-to-SU Channel
State Information (CSI) under the prescribed collision probability constraint,
and a novel sensing strategy design is proposed for overlay CR network where
the instantaneous false alarm probability is incorporated into the belief
update and the reward computation. It is demonstrated that the proposed sensing
approach improves SU confidence, randomizes sensing decisions, and
significantly improves SU network throughput while satisfying the collision
probability constraint to the PUs in the low average PU-to-SU SNR region.
Moreover, the proposed adaptive sensing strategy is robust to mismatched and
correlated fading CSI and improves significantly on conventional cooperative
sensing techniques. Finally, joint adaptation to PU channel gain and SU link
CSI is explored to further improve CR throughput and reduce SU collisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0793</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0793</id><created>2013-04-02</created><authors><author><keyname>Malekesmaeili</keyname><forenames>Mani</forenames></author><author><keyname>Ward</keyname><forenames>Rabab K.</forenames></author></authors><title>A local fingerprinting approach for audio copy detection</title><categories>cs.MM</categories><comments>10 pages, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study proposes an audio copy detection system that is robust to various
attacks. These include the severe pitch shift and tempo change attacks which
existing systems fail to detect. First, we propose a novel two dimensional
representation for audio signals called the time-chroma image. This image is
based on a modification of the concept of chroma in the music literature and is
shown to achieve better performance in song identification. Then, we propose a
novel fingerprinting algorithm that extracts local fingerprints from the
time-chroma image. The proposed local fingerprinting algorithm is invariant to
time/frequency scale changes in audio signals. It also outperforms existing
methods like SIFT by a great extent. Finally, we introduce a song
identification algorithm that uses the proposed fingerprints. The resulting
copy detection system is shown to significantly outperform existing methods.
Besides being able to detect whether a song (or a part of it) has been copied,
the proposed system can accurately estimate the amount of pitch shift and/or
tempo change that might have been applied to a song.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0806</identifier>
 <datestamp>2014-07-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0806</id><created>2013-04-02</created><authors><author><keyname>Karaaslan</keyname><forenames>Faruk</forenames></author><author><keyname>Cagman</keyname><forenames>Naim</forenames></author><author><keyname>Yilmaz</keyname><forenames>Saban</forenames></author></authors><title>IFP-Intuitionistic fuzzy soft set theory and its applications</title><categories>cs.AI</categories><comments>15 pages</comments><msc-class>62C86 (Decision theory and fuzziness)</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this work, we present definition of intuitionistic fuzzy parameterized
(IFP) intuitionistic fuzzy soft set and its operations. Then we define
IFP-aggregation operator to form IFP-intuitionistic fuzzy soft-decision-making
method which allows constructing more efficient decision processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0807</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0807</id><created>2013-04-02</created><authors><author><keyname>Lakbabi</keyname><forenames>Abdelmajid</forenames></author><author><keyname>Orhanou</keyname><forenames>Ghizlane</forenames></author><author><keyname>Hajji</keyname><forenames>Said El</forenames></author></authors><title>Network Access Control Technology - Proposition to contain new security
  challenges</title><categories>cs.CR</categories><comments>7 pages, 7 figures</comments><acm-class>D.4.6; C.2.0</acm-class><journal-ref>Int. J. Communications, Network and System Sciences, 2012, 5,
  505-512, Published Online August 2012 (http://www.scirp.org/journal/ijcns)</journal-ref><doi>10.4236/ijcns.2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional products working independently are no longer sufficient, since
threats are continually gaining in complexity, diversity and performance; In
order to proactively block such threats we need more integrated information
security solution. To achieve this objective, we will analyze a real-world
security platform, and focus on some key components Like, NAC, Firewall, and
IPS/IDS then study their interaction in the perspective to propose a new
security posture that coordinate and share security information between
different network security components, using a central policy server that will
be the NAC server or the PDP (the Policy Decision Point), playing an
orchestration role as a central point of control. Finally we will conclude with
potential research paths that will impact NAC technology evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0809</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0809</id><created>2013-04-02</created><updated>2013-06-17</updated><authors><author><keyname>Allais</keyname><forenames>Guillaume</forenames></author><author><keyname>Boutillier</keyname><forenames>Pierre</forenames></author><author><keyname>McBride</keyname><forenames>Conor</forenames></author></authors><title>New Equations for Neutral Terms: A Sound and Complete Decision
  Procedure, Formalized</title><categories>cs.PL</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The definitional equality of an intensional type theory is its test of type
compatibility. Today's systems rely on ordinary evaluation semantics to compare
expressions in types, frustrating users with type errors arising when
evaluation fails to identify two `obviously' equal terms. If only the machine
could decide a richer theory! We propose a way to decide theories which
supplement evaluation with `$\nu$-rules', rearranging the neutral parts of
normal forms, and report a successful initial experiment.
  We study a simple -calculus with primitive fold, map and append operations on
lists and develop in Agda a sound and complete decision procedure for an
equational theory enriched with monoid, functor and fusion laws.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0810</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0810</id><created>2013-04-02</created><authors><author><keyname>Angione</keyname><forenames>Claudio</forenames></author><author><keyname>Occhipinti</keyname><forenames>Annalisa</forenames></author><author><keyname>Stracquadanio</keyname><forenames>Giovanni</forenames></author><author><keyname>Nicosia</keyname><forenames>Giuseppe</forenames></author></authors><title>Bose-Einstein Condensation in Satisfiability Problems</title><categories>cs.DS cond-mat.stat-mech</categories><journal-ref>European Journal of Operational Research, 227, 44-54 (2013)</journal-ref><doi>10.1016/j.ejor.2012.11.039</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the complex behavior arising in satisfiability
problems. We present a new statistical physics-based characterization of the
satisfiability problem. Specifically, we design an algorithm that is able to
produce graphs starting from a k-SAT instance, in order to analyze them and
show whether a Bose-Einstein condensation occurs. We observe that, analogously
to complex networks, the networks of k-SAT instances follow Bose statistics and
can undergo Bose-Einstein condensation. In particular, k-SAT instances move
from a fit-get-rich network to a winner-takes-all network as the ratio of
clauses to variables decreases, and the phase transition of k-SAT approximates
the critical temperature for the Bose-Einstein condensation. Finally, we employ
the fitness-based classification to enhance SAT solvers (e.g., ChainSAT) and
obtain the consistently highest performing SAT solver for CNF formulas, and
therefore a new class of efficient hardware and software verification tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0823</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0823</id><created>2013-04-02</created><authors><author><keyname>Gong</keyname><forenames>Liyu</forenames></author><author><keyname>Chen</keyname><forenames>Meng</forenames></author><author><keyname>Hu</keyname><forenames>Chunlong</forenames></author></authors><title>Lie Algebrized Gaussians for Image Representation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an image representation method which is derived from analyzing
Gaussian probability density function (\emph{pdf}) space using Lie group
theory. In our proposed method, images are modeled by Gaussian mixture models
(GMMs) which are adapted from a globally trained GMM called universal
background model (UBM). Then we vectorize the GMMs based on two facts: (1)
components of image-specific GMMs are closely grouped together around their
corresponding component of the UBM due to the characteristic of the UBM
adaption procedure; (2) Gaussian \emph{pdf}s form a Lie group, which is a
differentiable manifold rather than a vector space. We map each Gaussian
component to the tangent vector space (named Lie algebra) of Lie group at the
manifold position of UBM. The final feature vector, named Lie algebrized
Gaussians (LAG) is then constructed by combining the Lie algebrized Gaussian
components with mixture weights. We apply LAG features to scene category
recognition problem and observe state-of-the-art performance on 15Scenes
benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0825</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0825</id><created>2013-04-02</created><authors><author><keyname>Kapur</keyname><forenames>Deepak</forenames></author><author><keyname>Zhan</keyname><forenames>Naijun</forenames></author><author><keyname>Zhao</keyname><forenames>Hengjun</forenames></author></authors><title>Synthesizing Switching Controllers for Hybrid Systems by Continuous
  Invariant Generation</title><categories>cs.SY cs.NA cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend a template-based approach for synthesizing switching controllers
for semi-algebraic hybrid systems, in which all expressions are polynomials.
This is achieved by combining a QE (quantifier elimination)-based method for
generating continuous invariants with a qualitative approach for predefining
templates. Our synthesis method is relatively complete with regard to a given
family of predefined templates. Using qualitative analysis, we discuss
heuristics to reduce the numbers of parameters appearing in the templates. To
avoid too much human interaction in choosing templates as well as the high
computational complexity caused by QE, we further investigate applications of
the SOS (sum-of-squares) relaxation approach and the template polyhedra
approach in continuous invariant generation, which are both well supported by
efficient numerical solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0828</identifier>
 <datestamp>2013-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0828</id><created>2013-04-02</created><updated>2013-04-26</updated><authors><author><keyname>Berthet</keyname><forenames>Quentin</forenames></author><author><keyname>Rigollet</keyname><forenames>Philippe</forenames></author></authors><title>Computational Lower Bounds for Sparse PCA</title><categories>math.ST cs.CC stat.ML stat.TH</categories><comments>Alternate title: &quot;Complexity Theoretic Lower Bounds for Sparse
  Principal Component Detection&quot;</comments><msc-class>62C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of sparse principal component detection, we bring evidence
towards the existence of a statistical price to pay for computational
efficiency. We measure the performance of a test by the smallest signal
strength that it can detect and we propose a computationally efficient method
based on semidefinite programming. We also prove that the statistical
performance of this test cannot be strictly improved by any computationally
efficient method. Our results can be viewed as complexity theoretic lower
bounds conditionally on the assumptions that some instances of the planted
clique problem cannot be solved in randomized polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0829</identifier>
 <datestamp>2014-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0829</id><created>2013-04-02</created><updated>2014-06-11</updated><authors><author><keyname>Kopczynski</keyname><forenames>Eryk</forenames></author><author><keyname>Tan</keyname><forenames>Tony</forenames></author></authors><title>Regular graphs and the spectra of two-variable logic with counting</title><categories>cs.LO cs.CC cs.DM math.CO</categories><comments>30 pages</comments><acm-class>F.4.1; F.1.3; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The {\em spectrum} of a first-order logic sentence is the set of natural
numbers that are cardinalities of its finite models. In this paper we show that
when restricted to using only two variables, but allowing counting quantifiers,
the spectra of first-order logic sentences are semilinear and hence, closed
under complement. At the heart of our proof are semilinear characterisations
for the existence of regular and biregular graphs, the class of graphs in which
there are a priori bounds on the degrees of the vertices.
  Our proof also provides a simple characterisation of models of two-variable
logic with counting -- that is, up to renaming and extending the relation
names, they are simply a collection of regular and biregular graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0835</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0835</id><created>2013-04-02</created><authors><author><keyname>Shi</keyname><forenames>Feng</forenames></author><author><keyname>Wu</keyname><forenames>Xuebin</forenames></author><author><keyname>Yan</keyname><forenames>Zhiyuan</forenames></author></authors><title>Improved Analytical Delay Models for RC-Coupled Interconnects</title><categories>cs.AR</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the process technologies scale into deep submicron region, crosstalk delay
is becoming increasingly severe, especially for global on-chip buses. To cope
with this problem, accurate delay models of coupled interconnects are needed.
In particular, delay models based on analytical approaches are desirable,
because they not only are largely transparent to technology, but also
explicitly establish the connections between delays of coupled interconnects
and transition patterns, thereby enabling crosstalk alleviating techniques such
as crosstalk avoidance codes (CACs). Unfortunately, existing analytical delay
models, such as the widely cited model in [1], have limited accuracy and do not
account for loading capacitance. In this paper, we propose analytical delay
models for coupled interconnects that address these disadvantages. By
accounting for more wires and eschewing the Elmore delay, our delay models
achieve better accuracy than the model in [1].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0839</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0839</id><created>2013-04-03</created><authors><author><keyname>Shamsi</keyname><forenames>Zahid Hussain</forenames></author><author><keyname>Kim</keyname><forenames>Dai-Gyoung</forenames></author></authors><title>Multiscale Hybrid Non-local Means Filtering Using Modified Similarity
  Measure</title><categories>cs.CV</categories><comments>7 pages, 3 figures, 2 tables</comments><msc-class>68U10, 68U05, 65D18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new multiscale implementation of non-local means filtering for image
denoising is proposed. The proposed algorithm also introduces a modification of
similarity measure for patch comparison. The standard Euclidean norm is
replaced by weighted Euclidean norm for patch based comparison. Assuming the
patch as an oriented surface, notion of normal vector patch is being associated
with each patch. The inner product of these normal vector patches is then used
in weighted Euclidean distance of photometric patches as the weight factor. The
algorithm involves two steps: The first step is multiscale implementation of an
accelerated non-local means filtering in the stationary wavelet domain to
obtain a refined version of the noisy patches for later comparison. This step
is inspired by a preselection phase of finding similar patches in various
non-local means approaches. The next step is to apply the modified non-local
means filtering to the noisy image using the reference patches obtained in the
first step. These refined patches contain less noise, and consequently the
computation of normal vectors and partial derivatives is more accurate.
Experimental results indicate equivalent or better performance of proposed
algorithm as compared to various state of the art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0840</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0840</id><created>2013-04-03</created><authors><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>A Fast Semidefinite Approach to Solving Binary Quadratic Problems</title><categories>cs.CV cs.LG</categories><comments>Appearing in Proc. IEEE Conf. Computer Vision and Pattern
  Recognition, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many computer vision problems can be formulated as binary quadratic programs
(BQPs). Two classic relaxation methods are widely used for solving BQPs,
namely, spectral methods and semidefinite programming (SDP), each with their
own advantages and disadvantages. Spectral relaxation is simple and easy to
implement, but its bound is loose. Semidefinite relaxation has a tighter bound,
but its computational complexity is high for large scale problems. We present a
new SDP formulation for BQPs, with two desirable properties. First, it has a
similar relaxation bound to conventional SDP formulations. Second, compared
with conventional SDP methods, the new SDP formulation leads to a significantly
more efficient and scalable dual optimization approach, which has the same
degree of complexity as spectral methods. Extensive experiments on various
applications including clustering, image segmentation, co-segmentation and
registration demonstrate the usefulness of our SDP formulation for solving
large-scale BQPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0844</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0844</id><created>2013-04-03</created><authors><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Kalinowski</keyname><forenames>Thomas</forenames></author><author><keyname>Narodytska</keyname><forenames>Nina</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Coalitional Manipulation for Schulze's Rule</title><categories>cs.AI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Schulze's rule is used in the elections of a large number of organizations
including Wikimedia and Debian. Part of the reason for its popularity is the
large number of axiomatic properties, like monotonicity and Condorcet
consistency, which it satisfies. We identify a potential shortcoming of
Schulze's rule: it is computationally vulnerable to manipulation. In
particular, we prove that computing an unweighted coalitional manipulation
(UCM) is polynomial for any number of manipulators. This result holds for both
the unique winner and the co-winner versions of UCM. This resolves an open
question stated by Parkes and Xia (2012). We also prove that computing a
weighted coalitional manipulation (WCM) is polynomial for a bounded number of
candidates. Finally, we discuss the relation between the unique winner UCM
problem and the co-winner UCM problem and argue that they have substantially
different necessary and sufficient conditions for the existence of a successful
manipulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0845</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0845</id><created>2013-04-03</created><authors><author><keyname>Spalek</keyname><forenames>Robert</forenames><affiliation>Google</affiliation></author></authors><title>Adversary Lower Bound for the Orthogonal Array Problem</title><categories>quant-ph cs.CC</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a quantum query lower bound \Omega(n^{(d+1)/(d+2)}) for the problem
of deciding whether an input string of size n contains a k-tuple which belongs
to a fixed orthogonal array on k factors of strength d&lt;=k-1 and index 1,
provided that the alphabet size is sufficiently large. Our lower bound is tight
when d=k-1.
  The orthogonal array problem includes the following problems as special
cases: k-sum problem with d=k-1, k-distinctness problem with d=1, k-pattern
problem with d=0, (d-1)-degree problem with 1&lt;=d&lt;=k-1, unordered search with
d=0 and k=1, and graph collision with d=0 and k=2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0848</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0848</id><created>2013-04-03</created><authors><author><keyname>Kim</keyname><forenames>Joonsuk</forenames></author></authors><title>Phase-Aligned Space-Time Coding for a Single Stream MIMO system</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE ICC 2013; Upload requested by Theodore Spathopoulos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a phase-aligned space-time coding scheme that expands the original
Alamouti codeword to three or four transmit antennas ($N_t = 3$ or 4) with
phase alignment. With $1 \sim 2$ bits feedback for the phase information, the
fundamental performance penalty of $10log_{10}(N_t)$ dB of orthogonal
space-time coding compared to the optimum beamforming is reduced by 1 dB (for
$N_t=3$) or 2 dB (for $N_t = 4$) on average. With the proposed scheme, the full
diversity order of $N_t$ is achievable, whereas the receiver architecture
remains the same as the legacy Alamouti decoding with codeword size of two,
since the spatial expansion is transparent to the receiver. Our results show
the proposed scheme outperforms open-loop space-time coding for three or four
transmit antennas by more than 3 dB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0857</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0857</id><created>2013-04-03</created><updated>2013-04-16</updated><authors><author><keyname>Boyer</keyname><forenames>R&#xe9;my</forenames></author><author><keyname>Korso</keyname><forenames>Mohammed Nabil El</forenames></author><author><keyname>Renaux</keyname><forenames>Alexandre</forenames></author><author><keyname>Marcos</keyname><forenames>Sylvie</forenames></author></authors><title>Coexistence of Near-Field and Far-Field Sources: the Angular Resolution
  Limit</title><categories>cs.IT math.IT</categories><doi>10.1088/1742-6596/464/1/012002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Passive source localization is a well known inverse problem in which we
convert the observed measurements into information about the direction of
arrivals. In this paper we focus on the optimal resolution of such problem.
More precisely, we propose in this contribution to derive and analyze the
Angular Resolution Limit (ARL) for the scenario of mixed Near-Field (NF) and
Far-Field (FF) Sources. This scenario is relevant to some realistic situations.
We base our analysis on the Smith's equation which involves the Cram\'er-Rao
Bound (CRB). This equation provides the theoretical ARL which is independent of
a specific estimator. Our methodology is the following: first, we derive a
closed-form expression of the CRB for the considered problem. Using these
expressions, we can rewrite the Smith's equation as a 4-th order polynomial by
assuming a small separation of the sources. Finally, we derive in closed-form
the analytic ARL under or not the assumption of low noise variance. The
obtained expression is compact and can provide useful qualitative informations
on the behavior of the ARL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0859</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0859</id><created>2013-04-03</created><authors><author><keyname>Zhang</keyname><forenames>Xin</forenames></author><author><keyname>Korso</keyname><forenames>Mohammed Nabil El</forenames></author><author><keyname>Pesavento</keyname><forenames>Marius</forenames></author></authors><title>Angular resolution limit for deterministic correlated sources</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is devoted to the analysis of the angular resolution limit (ARL),
an important performance measure in the directions-of-arrival estimation
theory. The main fruit of our endeavor takes the form of an explicit,
analytical expression of this resolution limit, w.r.t. the angular parameters
of interest between two closely spaced point sources in the far-field region.
As by-products, closed-form expressions of the Cram\'er-Rao bound have been
derived. Finally, with the aid of numerical tools, we confirm the validity of
our derivation and provide a detailed discussion on several enlightening
properties of the ARL revealed by our expression, with an emphasis on the
impact of the signal correlation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0863</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0863</id><created>2013-04-03</created><authors><author><keyname>Blaszczyszyn</keyname><forenames>Bartlomiej</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Karray</keyname><forenames>Mohamed Kadhem</forenames></author></authors><title>Quality of Service in Wireless Cellular Networks Subject to Log-Normal
  Shadowing</title><categories>cs.NI math.PR</categories><proxy>ccsd</proxy><journal-ref>IEEE Transactions on Communications 61, 2 (2012) 781 - 791</journal-ref><doi>10.1109/TCOMM.2012.120512.110673</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shadowing is believed to degrade the quality of service (QoS) in wireless
cellular networks. Assuming log-normal shadowing, and studying mobile's
path-loss with respect to the serving base station (BS) and the corresponding
interference factor (the ratio of the sum of the path-gains form interfering
BS's to the path-gain from the serving BS), which are two key ingredients of
the analysis and design of the cellular networks, we discovered a more subtle
reality. We observe, as commonly expected, that a strong variance of the
shadowing increases the mean path-loss with respect to the serving BS, which in
consequence, may compromise QoS. However, in some cases, an increase of the
variance of the shadowing can significantly reduce the mean interference factor
and, in consequence, improve some QoS metrics in interference limited systems,
provided the handover policy selects the BS with the smallest path loss as the
serving one. We exemplify this phenomenon, similar to stochastic resonance and
related to the &quot;single big jump principle&quot; of the heavy-tailed log-nornal
distribution, studying the blocking probability in regular, hexagonal networks
in a semi-analytic manner, using a spatial version of the Erlang's loss formula
combined with Kaufman-Roberts algorithm. More detailed probabilistic analysis
explains that increasing variance of the log-normal shadowing amplifies the
ratio between the strongest signal and all other signals thus reducing the
interference. The above observations might shed new light, in particular on the
design of indoor communication scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0864</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0864</id><created>2013-04-03</created><authors><author><keyname>Fouilh&#xe9;</keyname><forenames>Alexis</forenames><affiliation>VERIMAG - IMAG</affiliation></author><author><keyname>Monniaux</keyname><forenames>David</forenames><affiliation>VERIMAG - IMAG</affiliation></author><author><keyname>P&#xe9;rin</keyname><forenames>Micha&#xeb;l</forenames><affiliation>VERIMAG - IMAG</affiliation></author></authors><title>Efficient Generation of Correctness Certificates for the Abstract Domain
  of Polyhedra</title><categories>cs.PL cs.LO cs.MS</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polyhedra form an established abstract domain for inferring runtime
properties of programs using abstract interpretation. Computations on them need
to be certified for the whole static analysis results to be trusted. In this
work, we look at how far we can get down the road of a posteriori verification
to lower the overhead of certification of the abstract domain of polyhedra. We
demonstrate methods for making the cost of inclusion certificate generation
negligible. From a performance point of view, our single-representation,
constraints-based implementation compares with state-of-the-art
implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0869</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0869</id><created>2013-04-03</created><updated>2014-03-14</updated><authors><author><keyname>Wong</keyname><forenames>Yongkang</forenames></author><author><keyname>Chen</keyname><forenames>Shaokang</forenames></author><author><keyname>Mau</keyname><forenames>Sandra</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>Patch-based Probabilistic Image Quality Assessment for Face Selection
  and Improved Video-based Face Recognition</title><categories>cs.CV stat.AP</categories><acm-class>I.4.1; I.4.6; I.4.7; I.4.9; I.4.10; I.5.1; I.5.4; G.3</acm-class><journal-ref>IEEE Conference on Computer Vision and Pattern Recognition
  Workshops (CVPRW), pp. 74-81, 2011</journal-ref><doi>10.1109/CVPRW.2011.5981881</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In video based face recognition, face images are typically captured over
multiple frames in uncontrolled conditions, where head pose, illumination,
shadowing, motion blur and focus change over the sequence. Additionally,
inaccuracies in face localisation can also introduce scale and alignment
variations. Using all face images, including images of poor quality, can
actually degrade face recognition performance. While one solution it to use
only the &quot;best&quot; subset of images, current face selection techniques are
incapable of simultaneously handling all of the abovementioned issues. We
propose an efficient patch-based face image quality assessment algorithm which
quantifies the similarity of a face image to a probabilistic face model,
representing an &quot;ideal&quot; face. Image characteristics that affect recognition are
taken into account, including variations in geometric alignment (shift,
rotation and scale), sharpness, head pose and cast shadows. Experiments on
FERET and PIE datasets show that the proposed algorithm is able to identify
images which are simultaneously the most frontal, aligned, sharp and well
illuminated. Further experiments on a new video surveillance dataset (termed
ChokePoint) show that the proposed method provides better face subsets than
existing face selection techniques, leading to significant improvements in
recognition accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0872</identifier>
 <datestamp>2013-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0872</id><created>2013-04-03</created><authors><author><keyname>Doty</keyname><forenames>David</forenames></author></authors><title>Timing in chemical reaction networks</title><categories>cs.CC cs.DC cs.DS q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chemical reaction networks (CRNs) formally model chemistry in a well-mixed
solution. CRNs are widely used to describe information processing occurring in
natural cellular regulatory networks, and with upcoming advances in synthetic
biology, CRNs are a promising programming language for the design of artificial
molecular control circuitry. Due to a formal equivalence between CRNs and a
model of distributed computing known as population protocols, results transfer
readily between the two models.
  We show that if a CRN respects finite density (at most O(n) additional
molecules can be produced from n initial molecules), then starting from any
dense initial configuration (all molecular species initially present have
initial count Omega(n), where n is the initial molecular count and volume),
then every producible species is produced in constant time with high
probability.
  This implies that no CRN obeying the stated constraints can function as a
timer, able to produce a molecule, but doing so only after a time that is an
unbounded function of the input size. This has consequences regarding an open
question of Angluin, Aspnes, and Eisenstat concerning the ability of population
protocols to perform fast, reliable leader election and to simulate arbitrary
algorithms from a uniform initial state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0878</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0878</id><created>2013-04-03</created><updated>2013-04-10</updated><authors><author><keyname>Court&#xe8;s</keyname><forenames>Ludovic</forenames><affiliation>INRIA Bordeaux - Sud-Ouest</affiliation></author></authors><title>C Language Extensions for Hybrid CPU/GPU Programming with StarPU</title><categories>cs.MS cs.CE cs.DC</categories><proxy>ccsd</proxy><report-no>RR-8278</report-no><journal-ref>N&amp;deg; RR-8278 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern platforms used for high-performance computing (HPC) include machines
with both general-purpose CPUs, and &quot;accelerators&quot;, often in the form of
graphical processing units (GPUs). StarPU is a C library to exploit such
platforms. It provides users with ways to define &quot;tasks&quot; to be executed on CPUs
or GPUs, along with the dependencies among them, and by automatically
scheduling them over all the available processing units. In doing so, it also
relieves programmers from the need to know the underlying architecture details:
it adapts to the available CPUs and GPUs, and automatically transfers data
between main memory and GPUs as needed. While StarPU's approach is successful
at addressing run-time scheduling issues, being a C library makes for a poor
and error-prone programming interface. This paper presents an effort started in
2011 to promote some of the concepts exported by the library as C language
constructs, by means of an extension of the GCC compiler suite. Our main
contribution is the design and implementation of language extensions that map
to StarPU's task programming paradigm. We argue that the proposed extensions
make it easier to get started with StarPU,eliminate errors that can occur when
using the C library, and help diagnose possible mistakes. We conclude on future
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0886</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0886</id><created>2013-04-03</created><authors><author><keyname>Reddy</keyname><forenames>Vikas</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>Improved Anomaly Detection in Crowded Scenes via Cell-based Analysis of
  Foreground Speed, Size and Texture</title><categories>cs.CV</categories><acm-class>I.2.10; I.4.6; I.4.8; I.5.4</acm-class><journal-ref>IEEE Conference on Computer Vision and Pattern Recognition
  Workshops (CVPRW), pp. 55-61, 2011</journal-ref><doi>10.1109/CVPRW.2011.5981799</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A robust and efficient anomaly detection technique is proposed, capable of
dealing with crowded scenes where traditional tracking based approaches tend to
fail. Initial foreground segmentation of the input frames confines the analysis
to foreground objects and effectively ignores irrelevant background dynamics.
Input frames are split into non-overlapping cells, followed by extracting
features based on motion, size and texture from each cell. Each feature type is
independently analysed for the presence of an anomaly. Unlike most methods, a
refined estimate of object motion is achieved by computing the optical flow of
only the foreground pixels. The motion and size features are modelled by an
approximated version of kernel density estimation, which is computationally
efficient even for large training datasets. Texture features are modelled by an
adaptively grown codebook, with the number of entries in the codebook selected
in an online fashion. Experiments on the recently published UCSD Anomaly
Detection dataset show that the proposed method obtains considerably better
results than three recent approaches: MPPCA, social force, and mixture of
dynamic textures (MDT). The proposed method is also several orders of magnitude
faster than MDT, the next best performing method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0892</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0892</id><created>2013-04-03</created><authors><author><keyname>Zhang</keyname><forenames>Feng</forenames></author><author><keyname>Zhang</keyname><forenames>Wenyi</forenames></author></authors><title>Competition Between Wireless Service Providers: Pricing, Equilibrium and
  Efficiency</title><categories>cs.GT</categories><comments>accepted by WiOpt 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the communication network is in transition towards a commercial one
controlled by service providers (SP), the present paper considers a pricing
game in a communication market covered by several wireless access points
sharing the same spectrum and analyzes two business models: monopoly (APs
controlled by one SP) and oligopoly (APs controlled by different SPs). We use a
Stackelberg game to model the problem: SPs are the leader(s) and end users are
the followers. We prove, under certain conditions, the existence and uniqueness
of Nash equilibrium for both models and derive their expressions. In order to
compare the impact of different business models on social welfare and SPs'
profits, we define two metrics: PoCS (price of competition on social welfare)
and PoCP (price of competition on profits). For symmetric cross-AP
interferences, the tight lower bound of PoCS is 3/4, and that of PoCP is 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0897</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0897</id><created>2013-04-03</created><authors><author><keyname>Suda</keyname><forenames>Martin</forenames></author></authors><title>Duality in STRIPS planning</title><categories>cs.AI</categories><comments>6 pages (two columns), 4 tables</comments><acm-class>I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a duality mapping between STRIPS planning tasks. By exchanging
the initial and goal conditions, taking their respective complements, and
swapping for every action its precondition and delete list, one obtains for
every STRIPS task its dual version, which has a solution if and only if the
original does. This is proved by showing that the described transformation
essentially turns progression (forward search) into regression (backward
search) and vice versa.
  The duality sheds new light on STRIPS planning by allowing a transfer of
ideas from one search approach to the other. It can be used to construct new
algorithms from old ones, or (equivalently) to obtain new benchmarks from
existing ones. Experiments show that the dual versions of IPC benchmarks are in
general quite difficult for modern planners. This may be seen as a new
challenge. On the other hand, the cases where the dual versions are easier to
solve demonstrate that the duality can also be made useful in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0912</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0912</id><created>2013-04-03</created><authors><author><keyname>Kartzow</keyname><forenames>Alexander</forenames></author><author><keyname>Schlicht</keyname><forenames>Philipp</forenames></author></authors><title>Structures Without Scattered-Automatic Presentation</title><categories>cs.FL</categories><comments>10 pages + 20 pages Appendix; accepted for CiE 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bruyere and Carton lifted the notion of finite automata reading infinite
words to finite automata reading words with shape an arbitrary linear order L.
Automata on finite words can be used to represent infinite structures, the
so-called word-automatic structures. Analogously, for a linear order L there is
the class of L-automatic structures. In this paper we prove the following
limitations on the class of L-automatic structures for a fixed L of finite
condensation rank 1+\alpha. Firstly, no scattered linear order with finite
condensation rank above \omega^(\alpha+1) is L-\alpha-automatic. In particular,
every L-automatic ordinal is below \omega^\omega^(\alpha+1). Secondly, we
provide bounds on the (ordinal) height of well-founded order trees that are
L-automatic. If \alpha is finite or L is an ordinal, the height of such a tree
is bounded by \omega^{\alpha+1}. Finally, we separate the class of
tree-automatic structures from that of L-automatic structures for any ordinal
L: the countable atomless boolean algebra is known to be tree-automatic, but we
show that it is not L-automatic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0913</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0913</id><created>2013-04-03</created><authors><author><keyname>Salahi</keyname><forenames>Ahmad</forenames></author><author><keyname>Ansarinia</keyname><forenames>Morteza</forenames></author></authors><title>Predicting Network Attacks Using Ontology-Driven Inference</title><categories>cs.AI cs.CR cs.NI</categories><comments>9 pages</comments><journal-ref>International Journal of Information and Communication Technology
  (IJICT), Volume 4, Issue 1, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph knowledge models and ontologies are very powerful modeling and re
asoning tools. We propose an effective approach to model network attacks and
attack prediction which plays important roles in security management. The goals
of this study are: First we model network attacks, their prerequisites and
consequences using knowledge representation methods in order to provide
description logic reasoning and inference over attack domain concepts. And
secondly, we propose an ontology-based system which predicts potential attacks
using inference and observing information which provided by sensory inputs. We
generate our ontology and evaluate corresponding methods using CAPEC, CWE, and
CVE hierarchical datasets. Results from experiments show significant capability
improvements comparing to traditional hierarchical and relational models.
Proposed method also reduces false alarms and improves intrusion detection
effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0917</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0917</id><created>2013-04-03</created><updated>2013-06-14</updated><authors><author><keyname>Tabei</keyname><forenames>Yasuo</forenames></author><author><keyname>Takabatake</keyname><forenames>Yoshimasa</forenames></author><author><keyname>Sakamoto</keyname><forenames>Hiroshi</forenames></author></authors><title>A Succinct Grammar Compression</title><categories>cs.DS</categories><comments>The paper is accepted to 24th Annual Symposium on Combinatorial
  Pattern Matching (CPM2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We solve an open problem related to an optimal encoding of a straight line
program (SLP), a canonical form of grammar compression deriving a single string
deterministically. We show that an information-theoretic lower bound for
representing an SLP with n symbols requires at least 2n+logn!+o(n) bits. We
then present a succinct representation of an SLP; this representation is
asymptotically equivalent to the lower bound. The space is at most 2n log
{rho}(1 + o(1)) bits for rho leq 2sqrt{n}, while supporting random access to
any production rule of an SLP in O(log log n) time. In addition, we present a
novel dynamic data structure associating a digram with a unique symbol. Such a
data structure is called a naming function and has been implemented using a
hash table that has a space-time tradeoff. Thus, the memory space is mainly
occupied by the hash table during the development of production rules.
Alternatively, we build a dynamic data structure for the naming function by
leveraging the idea behind the wavelet tree. The space is strictly bounded by
2n log n(1 + o(1)) bits, while supporting O(log n) query and update time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0920</identifier>
 <datestamp>2013-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0920</id><created>2013-04-03</created><updated>2013-07-24</updated><authors><author><keyname>Geiger</keyname><forenames>Bernhard C.</forenames></author><author><keyname>Temmel</keyname><forenames>Christoph</forenames></author></authors><title>Information-Preserving Markov Aggregation</title><categories>cs.IT math.IT</categories><comments>7 pages, 3 figures, 2 tables</comments><journal-ref>Proc. IEEE Information Theory Workshop, 2013, pp. 258-262</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a sufficient condition for a non-injective function of a Markov
chain to be a second-order Markov chain with the same entropy rate as the
original chain. This permits an information-preserving state space reduction by
merging states or, equivalently, lossless compression of a Markov source on a
sample-by-sample basis. The cardinality of the reduced state space is bounded
from below by the node degrees of the transition graph associated with the
original Markov chain.
  We also present an algorithm listing all possible information-preserving
state space reductions, for a given transition graph. We illustrate our results
by applying the algorithm to a bi-gram letter model of an English text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0941</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0941</id><created>2013-04-03</created><updated>2015-04-04</updated><authors><author><keyname>Wang</keyname><forenames>Jian</forenames></author><author><keyname>Kwon</keyname><forenames>Suhyuk</forenames></author><author><keyname>Li</keyname><forenames>Ping</forenames></author><author><keyname>Shim</keyname><forenames>Byonghyo</forenames></author></authors><title>Recovery of Sparse Signals via Generalized Orthogonal Matching Pursuit:
  A New Analysis</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2012.2234512</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an extension of orthogonal matching pursuit (OMP) improving the recovery
performance of sparse signals, generalized OMP (gOMP) has recently been studied
in the literature. In this paper, we present a new analysis of the gOMP
algorithm using restricted isometry property (RIP). We show that if the
measurement matrix $\mathbf{\Phi} \in \mathcal{R}^{m \times n}$ satisfies the
RIP with $$\delta_{\max \left\{9, S + 1 \right\}K} \leq \frac{1}{8},$$ then
gOMP performs stable reconstruction of all $K$-sparse signals $\mathbf{x} \in
\mathcal{R}^n$ from the noisy measurements $\mathbf{y} = \mathbf{\Phi x} +
\mathbf{v}$ within $\max \left\{K, \left\lfloor \frac{8K}{S} \right\rfloor
\right\}$ iterations where $\mathbf{v}$ is the noise vector and $S$ is the
number of indices chosen in each iteration of the gOMP algorithm. For Gaussian
random measurements, our results indicate that the number of required
measurements is essentially $m = \mathcal{O}(K \log \frac{n}{K})$, which is a
significant improvement over the existing result $m = \mathcal{O}(K^2 \log
\frac{n}{K})$, especially for large $K$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0954</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0954</id><created>2013-04-03</created><updated>2014-01-10</updated><authors><author><keyname>Horvat</keyname><forenames>Marko</forenames></author><author><keyname>Grbin</keyname><forenames>Anton</forenames></author><author><keyname>Gledec</keyname><forenames>Gordan</forenames></author></authors><title>Labeling and Retrieval of Emotionally-Annotated Images using WordNet</title><categories>cs.IR cs.HC</categories><comments>16 pages, 4 figures</comments><journal-ref>International Journal of Knowledge-Based and Intelligent
  Engineering Systems, Vol. 17, No. 2, pp. 157-166, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Repositories of images with semantic and emotion content descriptions are
valuable tools in many areas such as Affective Computing and Human-Computer
Interaction, but they are also important in the development of multimodal
searchable online databases. Ever growing number of image documents available
on the Internet continuously motivates research of better annotation models and
more efficient retrieval methods which use mash-up of available data on
semantics, scenes, objects, events, context and emotion. Formal knowledge
representation of such high-level semantics requires rich, explicit, human but
also machine-processable information. To achieve these goals we present an
online ontology-based image annotation tool WNtags and demonstrate its
usefulness in knowledge representation and image retrieval using the
International Affective Picture System database. The WNtags uses WordNet as
image tagging glossary but considers Suggested Upper Merged Ontology as the
preferred upper labeling formalism. The retrieval is performed using node
distance metrics to establish semantic relatedness between a query and the
collaboratively weighted tags describing high-level image semantics, after
which the result is ranked according to the derived importance. We also
elaborate plans to improve the WNtags to create a collaborative Web-based
multimedia repository for research in human emotion and attention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0959</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0959</id><created>2013-04-03</created><authors><author><keyname>Grahne</keyname><forenames>Gosta</forenames></author><author><keyname>Onet</keyname><forenames>Adrian</forenames></author><author><keyname>Tartal</keyname><forenames>Nihat</forenames></author></authors><title>Conditional Tables in practice</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the ever increasing importance of the internet, interoperability of
heterogeneous data sources is as well of ever increasing importance.
Interoperability can be achieved e.g. through data integration and data
exchange. Common to both approaches is the need for the DBMS to be able to
store and query incomplete databases. In this report we present PossDB, a DBMS
capable of storing and querying incomplete databases. The system is wrapper
over PostgreSQL, and the query language is an extension of a subset of standard
SQL. Our experimental results show that our system scales well, actually better
than comparable systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0969</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0969</id><created>2013-04-03</created><authors><author><keyname>Mbikayi</keyname><forenames>Herve Kabamba</forenames></author></authors><title>Toward Evolution Strategies Application in Automatic Polyphonic Music
  Transcription using Electronic Synthesis</title><categories>cs.SD</categories><comments>6 pages</comments><journal-ref>International Journal of Advanced Computer Science and
  Applications(IJACSA), Volume 4 Issue 3, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present in this paper a new approach for polyphonic music transcription
using evolution strategies (ES). Automatic music transcription is a complex
process that still remains an open challenge. Using an audio signal to be
transcribed as target for our ES, information needed to generate a MIDI file
can be extracted from this latter one. Many techniques presented in the
literature at present exist and a few of them have applied evolutionary
algorithms to address this problem in the context of considering it as a search
space problem. However, ES have never been applied until now. The experiments
showed that by using these machines learning tools, some shortcomings presented
by other evolutionary algorithms based approaches for transcription can be
solved. They include the computation cost and the time for convergence. As
evolution strategies use self-adapting parameters, we show in this paper that
by correctly tuning the value of its strategy parameter that controls the
standard deviation, a fast convergence can be triggered toward the optima,
which from the results performs the transcription of the music with good
accuracy and in a short time. In the same context, the computation task is
tackled using parallelization techniques thus reducing the computation time and
the transcription time in the overall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0980</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0980</id><created>2013-04-03</created><authors><author><keyname>Ahmed</keyname><forenames>S.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Bouk</keyname><forenames>S. H.</forenames></author><author><keyname>Javaid</keyname><forenames>A.</forenames></author><author><keyname>Khan</keyname><forenames>M. A.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Quantum Cryptography Using Various Reversible Quantum Logic Gates in
  WSNs</title><categories>cs.CR cs.NI quant-ph</categories><comments>Journal of Basic and Applied Scientific Research (JBASR), 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As sensor nodes are deployed anywhere in a wireless sensor network, hence
their communication can be easily monitored. In these networks, message
protection and node identification are very issues. Hence, security of large
scale such networks requires efficient key distribution and management
mechanisms. Quantum cryptography and particularly quantum key distribution is
such a technique that allocates secure keys only for short distances. While not
completely secure, it offers huge advantages over traditional methods by the
use of entanglement swapping and quantum teleportation. Reversible logic gates
like CNOT, Toffoli, Fredkin etc. are of basic importance in Quantum Computing.
In our research, we adopted a EPR-pair allocation scheme in terms of these
quantum gates to overcome the susceptibility caused by malicious nodes. As the
qubits stored in a sensor node can be used only once and cannot be duplicated,
hence risk of information leakage reduced even if the node are compromised.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0984</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0984</id><created>2013-04-03</created><authors><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Ain</keyname><forenames>Q.</forenames></author><author><keyname>Khan</keyname><forenames>M. A.</forenames></author><author><keyname>Javaid</keyname><forenames>A.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author></authors><title>On Sink Mobility Trajectory in Clustering Routing Protocols in WSNs</title><categories>cs.NI</categories><journal-ref>Journal of Basic and Applied Scientific Research (JBASR), 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy efficient routing protocols are consistently cited as efficient
solutions for Wireless Sensor Networks (WSNs) routing. The area of WSNs is one
of the emerging and fast growing fields which brought low cost, low power and
multi-functional sensor nodes. In this paper, we examine some protocols related
to homogeneous and heterogeneous networks. To evaluate the efficiency of
different clustering schemes, we compare five clustering routing protocols; Low
Energy Adaptive Clustering Hierarchy (LEACH), Threshold Sensitive Energy
Efficient Sensor Network (TEEN), Distributed Energy Efficient Clustering (DEEC)
and two variants of TEEN which are Clustering and Multi-Hop Protocol in
Threshold Sensitive Energy Efficient Sensor Network (CAMPTEEN) and Hierarchical
Threshold Sensitive Energy Efficient Sensor Network (H-TEEN). The contribution
of this paper is to introduce sink mobility to increase the network life time
of hierarchal routing protocols. Two scenarios are discussed to compare the
performances of routing protocols; in first scenario static sink is implanted
and in later one mobile sink is used. We perform analytical simulations in
MATLAB by using different performance metrics such as, number of alive nodes,
number of dead nodes and throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0988</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0988</id><created>2013-04-03</created><updated>2015-02-13</updated><authors><author><keyname>Wild</keyname><forenames>Sebastian</forenames></author><author><keyname>Nebel</keyname><forenames>Markus E.</forenames></author><author><keyname>Neininger</keyname><forenames>Ralph</forenames></author></authors><title>Average Case and Distributional Analysis of Dual-Pivot Quicksort</title><categories>cs.DS math.PR</categories><comments>v3 is content-wise identical to TALG version</comments><acm-class>F.2.2; G.2.1; G.3; F.2.3; D.3.2</acm-class><journal-ref>ACM Transactions on Algorithms 11, 3, Article 22 (Jan 2015)</journal-ref><doi>10.1145/2629340</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2009, Oracle replaced the long-serving sorting algorithm in its Java 7
runtime library by a new dual-pivot Quicksort variant due to Vladimir
Yaroslavskiy. The decision was based on the strikingly good performance of
Yaroslavskiy's implementation in running time experiments. At that time, no
precise investigations of the algorithm were available to explain its superior
performance - on the contrary: Previous theoretical studies of other dual-pivot
Quicksort variants even discouraged the use of two pivots. Only in 2012, two of
the authors gave an average case analysis of a simplified version of
Yaroslavskiy's algorithm, proving that savings in the number of comparisons are
possible. However, Yaroslavskiy's algorithm needs more swaps, which renders the
analysis inconclusive.
  To force the issue, we herein extend our analysis to the fully detailed style
of Knuth: We determine the exact number of executed Java Bytecode instructions.
Surprisingly, Yaroslavskiy's algorithm needs sightly more Bytecode instructions
than a simple implementation of classic Quicksort - contradicting observed
running times. Like in Oracle's library implementation we incorporate the use
of Insertionsort on small subproblems and show that it indeed speeds up
Yaroslavskiy's Quicksort in terms of Bytecodes; but even with optimal
Insertionsort thresholds the new Quicksort variant needs slightly more Bytecode
instructions on average.
  Finally, we show that the (suitably normalized) costs of Yaroslavskiy's
algorithm converge to a random variable whose distribution is characterized by
a fixed-point equation. From that, we compute variances of costs and show that
for large n, costs are concentrated around their mean.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.0992</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.0992</id><created>2013-03-04</created><authors><author><keyname>Tushar</keyname><forenames>Wayes</forenames></author><author><keyname>Zhang</keyname><forenames>Jian A.</forenames></author><author><keyname>Smith</keyname><forenames>David B.</forenames></author><author><keyname>Thiebaux</keyname><forenames>Sylvie</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Prioritizing Consumers in Smart Grid: Energy Management Using Game
  Theory</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores an idea of demand-supply balance for smart grids in which
consumers are expected to play a significant role. The main objective is to
motivate the consumer, by maximizing their benefit both as a seller and a
buyer, to trade their surplus energy with the grid so as to balance the demand
at the peak hour. To that end, a Stackelberg game is proposed to capture the
interactions between the grid and consumers, and it is shown analytically that
optimal energy trading parameters that maximize customers utilities are
obtained at the solution of the game. A novel distributed algorithm is proposed
to reach the optimal solution of the game, and numerical examples are used to
assess the properties and effectiveness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1000</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1000</id><created>2013-04-03</created><authors><author><keyname>van der Aalst</keyname><forenames>Wil</forenames></author></authors><title>Passages in Graphs</title><categories>cs.DM math.CO</categories><comments>8 pages</comments><report-no>BPM Center Report BPM-12-19</report-no><msc-class>05C20</msc-class><acm-class>G.2.2; H.2.8; D.2.2</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Directed graphs can be partitioned in so-called passages. A passage P is a
set of edges such that any two edges sharing the same initial vertex or sharing
the same terminal vertex are both inside $P$ or are both outside of P. Passages
were first identified in the context of process mining where they are used to
successfully decompose process discovery and conformance checking problems. In
this article, we examine the properties of passages. We will show that passages
are closed under set operators such as union, intersection and difference.
Moreover, any passage is composed of so-called minimal passages. These
properties can be exploited when decomposing graph-based analysis and
computation problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1002</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1002</id><created>2013-04-03</created><authors><author><keyname>Pek</keyname><forenames>Ekaterina</forenames></author><author><keyname>L&#xe4;mmel</keyname><forenames>Ralf</forenames></author></authors><title>A Literature Survey on Empirical Evidence in Software Engineering</title><categories>cs.SE</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context: Software Engineering research makes use of collections of software
artifacts (corpora) to derive empirical evidence from. Goal: To improve quality
and reproducibility of research, we need to understand the characteristics of
used corpora. Method: For that, we perform a literature survey using grounded
theory. We analyze the latest proceedings of seven relevant conferences.
Results: While almost all papers use corpora of some kind with the common case
of collections of source code of open-source Java projects, there are no
frequently used projects or corpora across all the papers. For some conferences
we can detect recurrences. We discover several forms of requirements and
applied tunings for corpora which indicate more specific needs of research
efforts. Conclusion: Our survey feeds into a quantitative basis for discussing
the current state of empirical research in software engineering, thereby
enabling ultimately improvement of research quality specifically in terms of
use (and reuse) of empirical evidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1005</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1005</id><created>2013-04-03</created><authors><author><keyname>Vinodchandran</keyname><forenames>N. V.</forenames></author><author><keyname>Zimand</keyname><forenames>Marius</forenames></author></authors><title>On optimal language compression for sets in PSPACE/poly</title><categories>cs.CC</categories><comments>submitted to Theory of Computing Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that if DTIME[2^O(n)] is not included in DSPACE[2^o(n)], then, for
every set B in PSPACE/poly, all strings x in B of length n can be represented
by a string compressed(x) of length at most log(|B^{=n}|)+O(log n), such that a
polynomial-time algorithm, given compressed(x), can distinguish x from all the
other strings in B^{=n}. Modulo the O(log n) additive term, this achieves the
information-theoretic optimum for string compression. We also observe that
optimal compression is not possible for sets more complex than PSPACE/poly
because for any time-constructible superpolynomial function t, there is a set A
computable in space t(n) such that at least one string x of length n requires
compressed(x) to be of length 2 log(|A^=n|).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1007</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1007</id><created>2013-04-03</created><authors><author><keyname>G&#xf6;&#xf6;s</keyname><forenames>Mika</forenames></author><author><keyname>Hirvonen</keyname><forenames>Juho</forenames></author><author><keyname>Suomela</keyname><forenames>Jukka</forenames></author></authors><title>Linear-in-$\Delta$ Lower Bounds in the LOCAL Model</title><categories>cs.DC cs.CC cs.DS</categories><comments>1 + 21 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By prior work, there is a distributed algorithm that finds a maximal
fractional matching (maximal edge packing) in $O(\Delta)$ rounds, where
$\Delta$ is the maximum degree of the graph. We show that this is optimal:
there is no distributed algorithm that finds a maximal fractional matching in
$o(\Delta)$ rounds.
  Our work gives the first linear-in-$\Delta$ lower bound for a natural graph
problem in the standard model of distributed computing---prior lower bounds for
a wide range of graph problems have been at best logarithmic in $\Delta$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1014</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1014</id><created>2013-04-03</created><updated>2013-10-13</updated><authors><author><keyname>Allende</keyname><forenames>Hector</forenames></author><author><keyname>Frandi</keyname><forenames>Emanuele</forenames></author><author><keyname>Nanculef</keyname><forenames>Ricardo</forenames></author><author><keyname>Sartori</keyname><forenames>Claudio</forenames></author></authors><title>A Novel Frank-Wolfe Algorithm. Analysis and Applications to Large-Scale
  SVM Training</title><categories>cs.CV cs.AI cs.LG math.OC stat.ML</categories><comments>REVISED VERSION (October 2013) -- Title and abstract have been
  revised. Section 5 was added. Some proofs have been summarized (full-length
  proofs available in the previous version)</comments><journal-ref>Information Sciences 285, 66-99, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been a renewed interest in the machine learning community
for variants of a sparse greedy approximation procedure for concave
optimization known as {the Frank-Wolfe (FW) method}. In particular, this
procedure has been successfully applied to train large-scale instances of
non-linear Support Vector Machines (SVMs). Specializing FW to SVM training has
allowed to obtain efficient algorithms but also important theoretical results,
including convergence analysis of training algorithms and new characterizations
of model sparsity.
  In this paper, we present and analyze a novel variant of the FW method based
on a new way to perform away steps, a classic strategy used to accelerate the
convergence of the basic FW procedure. Our formulation and analysis is focused
on a general concave maximization problem on the simplex. However, the
specialization of our algorithm to quadratic forms is strongly related to some
classic methods in computational geometry, namely the Gilbert and MDM
algorithms.
  On the theoretical side, we demonstrate that the method matches the
guarantees in terms of convergence rate and number of iterations obtained by
using classic away steps. In particular, the method enjoys a linear rate of
convergence, a result that has been recently proved for MDM on quadratic forms.
  On the practical side, we provide experiments on several classification
datasets, and evaluate the results using statistical tests. Experiments show
that our method is faster than the FW method with classic away steps, and works
well even in the cases in which classic away steps slow down the algorithm.
Furthermore, these improvements are obtained without sacrificing the predictive
accuracy of the obtained SVM model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1018</identifier>
 <datestamp>2013-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1018</id><created>2013-04-03</created><updated>2013-06-12</updated><authors><author><keyname>Palaz</keyname><forenames>Dimitri</forenames></author><author><keyname>Collobert</keyname><forenames>Ronan</forenames></author><author><keyname>-Doss</keyname><forenames>Mathew Magimai.</forenames></author></authors><title>Estimating Phoneme Class Conditional Probabilities from Raw Speech
  Signal using Convolutional Neural Networks</title><categories>cs.LG cs.CL cs.NE</categories><comments>In Interspeech 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In hybrid hidden Markov model/artificial neural networks (HMM/ANN) automatic
speech recognition (ASR) system, the phoneme class conditional probabilities
are estimated by first extracting acoustic features from the speech signal
based on prior knowledge such as, speech perception or/and speech production
knowledge, and, then modeling the acoustic features with an ANN. Recent
advances in machine learning techniques, more specifically in the field of
image processing and text processing, have shown that such divide and conquer
strategy (i.e., separating feature extraction and modeling steps) may not be
necessary. Motivated from these studies, in the framework of convolutional
neural networks (CNNs), this paper investigates a novel approach, where the
input to the ANN is raw speech signal and the output is phoneme class
conditional probability estimates. On TIMIT phoneme recognition task, we study
different ANN architectures to show the benefit of CNNs and compare the
proposed approach against conventional approach where, spectral-based feature
MFCC is extracted and modeled by a multilayer perceptron. Our studies show that
the proposed approach can yield comparable or better phoneme recognition
performance when compared to the conventional approach. It indicates that CNNs
can learn features relevant for phoneme classification automatically from the
raw speech signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1020</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1020</id><created>2013-04-03</created><authors><author><keyname>Gotsis</keyname><forenames>Antonis G.</forenames></author><author><keyname>Alexiou</keyname><forenames>Angeliki</forenames></author></authors><title>Spatial Resources Optimization in Distributed MIMO Networks with Limited
  Data Sharing</title><categories>cs.IT math.IT</categories><comments>submitted to Globecom 2013 - Wireless Communications Symposium</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless access through a large distributed network of low-complexity
infrastructure nodes empowered with cooperation and coordination capabilities,
is an emerging radio architecture, candidate to deal with the mobile data
capacity crunch. In the 3GPP evolutionary path, this is known as the Cloud-RAN
paradigm for future radio. In such a complex network, distributed MIMO
resources optimization is of paramount importance, in order to achieve capacity
scaling. In this paper, we investigate efficient strategies towards optimizing
the pairing of access nodes with users as well as linear precoding designs for
providing fair QoS experience across the whole network, when data sharing is
limited due to complexity and overhead constraints. We propose a method for
obtaining the exact optimal spatial resources allocation solution which can be
applied in networks of limited scale, as well as an approximation algorithm
with bounded polynomial complexity which can be used in larger networks. The
particular algorithm outperforms existing user-oriented clustering techniques
and achieves quite high quality-of-service levels with reasonable complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1022</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1022</id><created>2013-04-03</created><authors><author><keyname>Sparavigna</keyname><forenames>Amelia Carolina</forenames></author></authors><title>A software for aging faces applied to ancient marble busts</title><categories>cs.CV</categories><comments>Image processing. Aging faces. Freely available software. Ancient
  marble busts. Augustus</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study and development of software able to show the effect of aging of
faces is one of the tasks of face recognition technologies. Some software
solutions are used for investigations, some others to show the effects of drugs
on healthy appearance, however some other applications can be proposed for the
analysis of visual arts. Here we use a freely available software, which is
providing interesting results, for the comparison of ancient marble busts. An
analysis of Augustus busts is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1039</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1039</id><created>2013-04-03</created><authors><author><keyname>Merritt</keyname><forenames>Sears</forenames></author><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author></authors><title>Environmental structure and competitive scoring advantages in team
  competitions</title><categories>physics.soc-ph cs.SI physics.data-an stat.AP</categories><comments>Main Text: 8 pages, 4 figures, 2 tables; Supplementary Information:
  12 pages, 13 figures, 9 tables</comments><journal-ref>Scientific Reports 3, 3067 (2013)</journal-ref><doi>10.1038/srep03067</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In most professional sports, the structure of the environment is kept neutral
so that scoring imbalances may be attributed to differences in team skill. It
thus remains unknown what impact structural heterogeneities can have on scoring
dynamics and producing competitive advantages. Applying a generative model of
scoring dynamics to roughly 10 million team competitions drawn from an online
game, we quantify the relationship between a competition's structure and its
scoring dynamics. Despite wide structural variations, we find the same
three-phase pattern in the tempo of events observed in many sports. Tempo and
balance are highly predictable from a competition's structural features alone
and teams exploit environmental heterogeneities for sustained competitive
advantage. The most balanced competitions are associated with specific
environmental heterogeneities, not from equally skilled teams. These results
shed new light on the principles of balanced competition, and illustrate the
potential of online game data for investigating social dynamics and
competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1045</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1045</id><created>2013-04-03</created><authors><author><keyname>Meneguette</keyname><forenames>Rodolfo I.</forenames></author><author><keyname>Bittencourt</keyname><forenames>Luiz F.</forenames></author><author><keyname>Madeira</keyname><forenames>Edmundo R. M.</forenames></author></authors><title>A Seamless Flow Mobility Management Architecture for Vehicular
  Communication Networks</title><categories>cs.NI</categories><comments>10 pages; 21 Figures; Journal of Communication and Networks -special
  issues - http://jcn.or.kr/</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Vehicular Ad Hoc Networks (VANETs) are self-organizing, self-healing networks
which provide wireless communication among vehicular and roadside devices.
Applications in such networks can take advantage of the use of simultaneous
connections, thereby maximizing the throughput and lowering latency. In order
to take advantage of all radio interfaces of the vehicle and to provide good
quality of service for vehicular applications, we de- veloped a seamless flow
mobility management architecture based on vehicular network application classes
with network-based mobility management. Our goal is to minimize the time of
flow connection exchange in order to comply with the minimum requirements of
vehicular application classes, as well as to maximize their throughput. NS3
simulations were performed to analyse the behaviour of our architecture by
comparing it with other three scenarios. As a result of this work, we observed
that the proposed architecture presented a low handover time, with lower packet
loss and lower delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1047</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1047</id><created>2013-04-03</created><authors><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Israr</keyname><forenames>I.</forenames></author><author><keyname>Khan</keyname><forenames>M. A.</forenames></author><author><keyname>Javaid</keyname><forenames>A.</forenames></author><author><keyname>Bouk</keyname><forenames>S. H.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Analyzing Medium Access Techniques in Wireless Body Area Networks</title><categories>cs.NI</categories><comments>Research Journal of Applied Sciences, Engineering and Technology,
  2013. arXiv admin note: substantial text overlap with arXiv:1208.2406</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents comparison of Access Techniques used in Medium Access
Control (MAC) protocol for Wireless Body Area Networks (WBANs). Comparison is
performed between Time Division Multiple Access (TDMA), Frequency Division
Multiple Access (FDMA), Carrier Sense Multiple Access with Collision Avoidance
(CSMA/CA), Pure ALOHA and Slotted ALOHA (S-ALOHA). Performance metrics used for
comparison are Throughput (T), Delay (D) and Offered Load (G). The main goal
for comparison is to show which technique gives highest Throughput and lowest
Delay with increase in Load. Energy efficiency is major issue in WBAN that is
why there is need to know which technique performs best for energy conservation
and also gives minimum delay. Simulations are performed for different scenarios
and results are compared for all techniques. We suggest TDMA as best technique
to be used in MAC protocol for WBANs due to its high throughput and minimum
delay with increase in load. MATLAB is the tool that is used for simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1059</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1059</id><created>2013-04-03</created><authors><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Yaqoob</keyname><forenames>M.</forenames></author><author><keyname>Khan</keyname><forenames>M. Y.</forenames></author><author><keyname>Khan</keyname><forenames>M. A.</forenames></author><author><keyname>Javaid</keyname><forenames>A.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author></authors><title>Analyzing Delay in Wireless Multi-hop Heterogeneous Body Area Networks</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1208.2409</comments><journal-ref>Research Journal of Applied Sciences, Engineering and Technology,
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With increase in ageing population, health care market keeps growing. There
is a need for monitoring of health issues. Wireless Body Area Network (WBAN)
consists of wireless sensors attached on or inside human body for monitoring
vital health related problems e.g, Electro Cardiogram (ECG), Electro
Encephalogram (EEG), ElectronyStagmography (ENG) etc. Due to life threatening
situations, timely sending of data is essential. For data to reach health care
center, there must be a proper way of sending data through reliable connection
and with minimum delay. In this paper transmission delay of different paths,
through which data is sent from sensor to health care center over heterogeneous
multi-hop wireless channel is analyzed. Data of medical related diseases is
sent through three different paths. In all three paths, data from sensors first
reaches ZigBee, which is the common link in all three paths. Wireless Local
Area Network (WLAN), Worldwide Interoperability for Microwave Access (WiMAX),
Universal Mobile Telecommunication System (UMTS) are connected with ZigBee.
Each network (WLAN, WiMAX, UMTS) is setup according to environmental
conditions, suitability of device and availability of structure for that
device. Data from these networks is sent to IP-Cloud, which is further
connected to health care center. Delay of data reaching each device is
calculated and represented graphically. Main aim of this paper is to calculate
delay of each link in each path over multi-hop wireless channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1063</identifier>
 <datestamp>2014-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1063</id><created>2013-04-03</created><updated>2014-04-29</updated><authors><author><keyname>Coja-Oghlan</keyname><forenames>Amin</forenames></author><author><keyname>Vilenchik</keyname><forenames>Dan</forenames></author></authors><title>Chasing the k-colorability threshold</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decade, physicists have developed deep but non-rigorous
techniques for studying phase transitions in discrete structures. Recently,
their ideas have been harnessed to obtain improved rigorous results on the
phase transitions in binary problems such as random $k$-SAT or $k$-NAESAT
(e.g., Coja-Oghlan and Panagiotou: STOC 2013). However, these rigorous
arguments, typically centered around the second moment method, do not extend
easily to problems where there are more than two possible values per variable.
The single most intensely studied example of such a problem is random graph
$k$-coloring. Here we develop a novel approach to the second moment method in
this problem. This new method, inspired by physics conjectures on the geometry
of the set of $k$-colorings, allows us to establish a substantially improved
lower bound on the $k$-colorability threshold. The new lower bound is within an
additive $2\ln 2+o_k(1)\approx 1.39$ of a simple first-moment upper bound and
within $2\ln 2-1+o_k(1)\approx 0.39$ of the physics conjecture. By comparison,
the best previous lower bound left a gap of about $2+\ln k$, unbounded in terms
of the number of colors [Achlioptas, Naor: STOC 2004].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1066</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1066</id><created>2013-04-03</created><authors><author><keyname>Zhou</keyname><forenames>Qi</forenames></author><author><keyname>Ma</keyname><forenames>Xiaoli</forenames></author></authors><title>An Improved LR-aided K-Best Algorithm for MIMO Detection</title><categories>cs.IT cs.DS math.IT math.OC</categories><comments>5 pages, 4 figures, 1 table, conference</comments><journal-ref>International Conference on Wireless Communications and Signal
  Processing (WCSP) 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, lattice reduction (LR) technique has caught great attention for
multi-input multi-output (MIMO) receiver because of its low complexity and high
performance. However, when the number of antennas is large, LR-aided linear
detectors and successive interference cancellation (SIC) detectors still
exhibit considerable performance gap to the optimal maximum likelihood detector
(MLD). To enhance the performance of the LR-aided detectors, the LR-aided
K-best algorithm was developed at the cost of the extra complexity on the order
$\mathcal{O}(N_t^2 K + N_t K^2)$, where $N_t$ is the number of transmit
antennas and $K$ is the number of candidates. In this paper, we develop an
LR-aided K-best algorithm with lower complexity by exploiting a priority queue.
With the aid of the priority queue, our analysis shows that the complexity of
the LR-aided K-best algorithm can be further reduced to $\mathcal{O}(N_t^2 K +
N_t K {\rm log}_2(K))$. The low complexity of the proposed LR-aided K-best
algorithm allows us to perform the algorithm for large MIMO systems (e.g.,
50x50 MIMO systems) with large candidate sizes. Simulations show that as the
number of antennas increases, the error performance approaches that of AWGN
channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1067</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1067</id><created>2013-04-03</created><authors><author><keyname>Bollen</keyname><forenames>Johan</forenames></author><author><keyname>Crandall</keyname><forenames>David</forenames></author><author><keyname>Junk</keyname><forenames>Damion</forenames></author><author><keyname>Ding</keyname><forenames>Ying</forenames></author><author><keyname>Boerner</keyname><forenames>Katy</forenames></author></authors><title>Collective allocation of science funding: from funding agencies to
  scientific agency</title><categories>physics.soc-ph cs.DL</categories><comments>main paper: 7 pages, excl. references + supplemental materials (9), 4
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Public agencies like the U.S. National Science Foundation (NSF) and the
National Institutes of Health (NIH) award tens of billions of dollars in annual
science funding. How can this money be distributed as efficiently as possible
to best promote scientific innovation and productivity? The present system
relies primarily on peer review of project proposals. In 2010 alone, NSF
convened more than 15,000 scientists to review 55,542 proposals. Although
considered the scientific gold standard, peer review requires significant
overhead costs, and may be subject to biases, inconsistencies, and oversights.
We investigate a class of funding models in which all participants receive an
equal portion of yearly funding, but are then required to anonymously donate a
fraction of their funding to peers. The funding thus flows from one participant
to the next, each acting as if he or she were a funding agency themselves. Here
we show through a simulation conducted over large-scale citation data (37M
articles, 770M citations) that such a distributed system for science may yield
funding patterns similar to existing NIH and NSF distributions, but may do so
at much lower overhead while exhibiting a range of other desirable features.
Self-correcting mechanisms in scientific peer evaluation can yield an efficient
and fair distribution of funding. The proposed model can be applied in many
situations in which top-down or bottom-up allocation of public resources is
either impractical or undesirable, e.g. public investments, distribution
chains, and shared resource management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1074</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1074</id><created>2013-03-20</created><authors><author><keyname>Vovk</keyname><forenames>Vladimir</forenames></author></authors><title>Kolmogorov's strong law of large numbers in game-theoretic probability:
  Reality's side</title><categories>cs.GT math.PR</categories><comments>3 pages</comments><msc-class>60F15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The game-theoretic version of Kolmogorov's strong law of large numbers says
that Skeptic has a strategy forcing the statement of the law in a game of
prediction involving Reality, Forecaster, and Skeptic. This note describes a
simple matching strategy for Reality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1075</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1075</id><created>2013-03-21</created><authors><author><keyname>Grabisch</keyname><forenames>Michel</forenames><affiliation>CES, EEP-PSE</affiliation></author></authors><title>The core of games on ordered structures and graphs</title><categories>cs.GT cs.DM math.CO</categories><proxy>ccsd</proxy><journal-ref>Annals of Operations Research (2013) 33-64</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cooperative games, the core is the most popular solution concept, and its
properties are well known. In the classical setting of cooperative games, it is
generally assumed that all coalitions can form, i.e., they are all feasible. In
many situations, this assumption is too strong and one has to deal with some
unfeasible coalitions. Defining a game on a subcollection of the power set of
the set of players has many implications on the mathematical structure of the
core, depending on the precise structure of the subcollection of feasible
coalitions. Many authors have contributed to this topic, and we give a unified
view of these different results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1081</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1081</id><created>2013-03-27</created><authors><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author></authors><title>Exploiting Functional Dependencies in Qualitative Probabilistic
  Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-2-9</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Functional dependencies restrict the potential interactions among variables
connected in a probabilistic network. This restriction can be exploited in
qualitative probabilistic reasoning by introducing deterministic variables and
modifying the inference rules to produce stronger conclusions in the presence
of functional relations. I describe how to accomplish these modifications in
qualitative probabilistic networks by exhibiting the update procedures for
graphical transformations involving probabilistic and deterministic variables
and combinations. A simple example demonstrates that the augmented scheme can
reduce qualitative ambiguity that would arise without the special treatment of
functional dependency. Analysis of qualitative synergy reveals that new
higher-order relations are required to reason effectively about synergistic
interactions among deterministic variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1082</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1082</id><created>2013-03-27</created><authors><author><keyname>Henrion</keyname><forenames>Max</forenames></author><author><keyname>Druzdzel</keyname><forenames>Marek J.</forenames></author></authors><title>Qualitative Propagation and Scenario-based Explanation of Probabilistic
  Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-10-20</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comprehensible explanations of probabilistic reasoning are a prerequisite for
wider acceptance of Bayesian methods in expert systems and decision support
systems. A study of human reasoning under uncertainty suggests two different
strategies for explaining probabilistic reasoning: The first, qualitative
belief propagation, traces the qualitative effect of evidence through a belief
network from one variable to the next. This propagation algorithm is an
alternative to the graph reduction algorithms of Wellman (1988) for inference
in qualitative probabilistic networks. It is based on a qualitative analysis of
intercausal reasoning, which is a generalization of Pearl's &quot;explaining away&quot;,
and an alternative to Wellman's definition of qualitative synergy. The other,
Scenario-based reasoning, involves the generation of alternative causal
&quot;stories&quot; accounting for the evidence. Comparing a few of the most probable
scenarios provides an approximate way to explain the results of probabilistic
reasoning. Both schemes employ causal as well as probabilistic knowledge.
Probabilities may be presented as phrases and/or numbers. Users can control the
style, abstraction and completeness of explanations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1083</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1083</id><created>2013-03-27</created><authors><author><keyname>Shultz</keyname><forenames>Thomas R.</forenames></author></authors><title>Managing Uncertainty in Rule Based Cognitive Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-21-26</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An experiment replicated and extended recent findings on psychologically
realistic ways of modeling propagation of uncertainty in rule based reasoning.
Within a single production rule, the antecedent evidence can be summarized by
taking the maximum of disjunctively connected antecedents and the minimum of
conjunctively connected antecedents. The maximum certainty factor attached to
each of the rule's conclusions can be sealed down by multiplication with this
summarized antecedent certainty. Heckerman's modified certainty factor
technique can be used to combine certainties for common conclusions across
production rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1084</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1084</id><created>2013-03-27</created><authors><author><keyname>Cheng</keyname><forenames>Yizong</forenames></author></authors><title>Context-Dependent Similarity</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-27-31</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Attribute weighting and differential weighting, two major mechanisms for
computing context-dependent similarity or dissimilarity measures are studied
and compared. A dissimilarity measure based on subset size in the context is
proposed and its metrization and application are given. It is also shown that
while all attribute weighting dissimilarity measures are metrics differential
weighting dissimilarity measures are usually non-metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1085</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1085</id><created>2013-03-27</created><updated>2015-05-16</updated><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>Similarity Networks for the Construction of Multiple-Faults Belief
  Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1990-PG-32-39</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A similarity network is a tool for constructing belief networks for the
diagnosis of a single fault. In this paper, we examine modifications to the
similarity-network representation that facilitate the construction of belief
networks for the diagnosis of multiple coexisting faults.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1086</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1086</id><created>2013-03-27</created><authors><author><keyname>Lin</keyname><forenames>Dekang</forenames></author><author><keyname>Goebel</keyname><forenames>Randy</forenames></author></authors><title>Integrating Probabilistic, Taxonomic and Causal Knowledge in Abductive
  Diagnosis</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-40-45</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an abductive diagnosis theory that integrates probabilistic,
causal and taxonomic knowledge. Probabilistic knowledge allows us to select the
most likely explanation; causal knowledge allows us to make reasonable
independence assumptions; taxonomic knowledge allows causation to be modeled at
different levels of detail, and allows observations be described in different
levels of precision. Unlike most other approaches where a causal explanation is
a hypothesis that one or more causative events occurred, we define an
explanation of a set of observations to be an occurrence of a chain of
causation events. These causation events constitute a scenario where all the
observations are true. We show that the probabilities of the scenarios can be
computed from the conditional probabilities of the causation events. Abductive
reasoning is inherently complex even if only modest expressive power is
allowed. However, our abduction algorithm is exponential only in the number of
observations to be explained, and is polynomial in the size of the knowledge
base. This contrasts with many other abduction procedures that are exponential
in the size of the knowledge base.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1087</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1087</id><created>2013-03-27</created><authors><author><keyname>Poole</keyname><forenames>David L.</forenames></author><author><keyname>Provan</keyname><forenames>Gregory M.</forenames></author></authors><title>What is an Optimal Diagnosis?</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-46-53</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within diagnostic reasoning there have been a number of proposed definitions
of a diagnosis, and thus of the most likely diagnosis, including most probable
posterior hypothesis, most probable interpretation, most probable covering
hypothesis, etc. Most of these approaches assume that the most likely diagnosis
must be computed, and that a definition of what should be computed can be made
a priori, independent of what the diagnosis is used for. We argue that the
diagnostic problem, as currently posed, is incomplete: it does not consider how
the diagnosis is to be used, or the utility associated with the treatment of
the abnormalities. In this paper we analyze several well-known definitions of
diagnosis, showing that the different definitions of the most likely diagnosis
have different qualitative meanings, even given the same input data. We argue
that the most appropriate definition of (optimal) diagnosis needs to take into
account the utility of outcomes and what the diagnosis is used for.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1088</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1088</id><created>2013-03-27</created><authors><author><keyname>Herskovits</keyname><forenames>Edward H.</forenames></author><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author></authors><title>Kutato: An Entropy-Driven System for Construction of Probabilistic
  Expert Systems from Databases</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-54-63</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kutato is a system that takes as input a database of cases and produces a
belief network that captures many of the dependence relations represented by
those data. This system incorporates a module for determining the entropy of a
belief network and a module for constructing belief networks based on entropy
calculations. Kutato constructs an initial belief network in which all
variables in the database are assumed to be marginally independent. The entropy
of this belief network is calculated, and that arc is added that minimizes the
entropy of the resulting belief network. Conditional probabilities for an arc
are obtained directly from the database. This process continues until an
entropy-based threshold is reached. We have tested the system by generating
databases from networks using the probabilistic logic-sampling method, and then
using those databases as input to Kutato. The system consistently reproduces
the original belief networks with high fidelity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1089</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1089</id><created>2013-03-27</created><authors><author><keyname>Breese</keyname><forenames>John S.</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author></authors><title>Ideal Reformulation of Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-64-72</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The intelligent reformulation or restructuring of a belief network can
greatly increase the efficiency of inference. However, time expended for
reformulation is not available for performing inference. Thus, under time
pressure, there is a tradeoff between the time dedicated to reformulating the
network and the time applied to the implementation of a solution. We
investigate this partition of resources into time applied to reformulation and
time used for inference. We shall describe first general principles for
computing the ideal partition of resources under uncertainty. These principles
have applicability to a wide variety of problems that can be divided into
interdependent phases of problem solving. After, we shall present results of
our empirical study of the problem of determining the ideal amount of time to
devote to searching for clusters in belief networks. In this work, we acquired
and made use of probability distributions that characterize (1) the performance
of alternative heuristic search methods for reformulating a network instance
into a set of cliques, and (2) the time for executing inference procedures on
various belief networks. Given a preference model describing the value of a
solution as a function of the delay required for its computation, the system
selects an ideal time to devote to reformulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1090</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1090</id><created>2013-03-27</created><authors><author><keyname>Einav</keyname><forenames>David</forenames></author><author><keyname>Fehling</keyname><forenames>Michael R.</forenames></author></authors><title>Computationally-Optimal Real-Resource Strategies</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-73-81</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on managing the cost of deliberation before action. In
many problems, the overall quality of the solution reflects costs incurred and
resources consumed in deliberation as well as the cost and benefit of
execution, when both the resource consumption in deliberation phase, and the
costs in deliberation and execution are uncertain and may be described by
probability distribution functions. A feasible (in terms of resource
consumption) strategy that minimizes the expected total cost is termed
computationally-optimal. For a situation with several independent,
uninterruptible methods to solve the problem, we develop a
pseudopolynomial-time algorithm to construct generate-and-test computationally
optimal strategy. We show this strategy-construction problem to be NP-complete,
and apply Bellman's Optimality Principle to solve it efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1091</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1091</id><created>2013-03-27</created><updated>2015-05-16</updated><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author></authors><title>Problem Formulation as the Reduction of a Decision Model</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1990-PG-82-89</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we extend the QMRDT probabilistic model for the domain of
internal medicine to include decisions about treatments. In addition, we
describe how we can use the comprehensive decision model to construct a simpler
decision model for a specific patient. In so doing, we transform the task of
problem formulation to that of narrowing of a larger problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1092</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1092</id><created>2013-03-27</created><authors><author><keyname>Goldman</keyname><forenames>Robert P.</forenames></author><author><keyname>Charniak</keyname><forenames>Eugene</forenames></author></authors><title>Dynamic Construction of Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-90-97</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a method for incrementally constructing belief networks. We have
developed a network-construction language similar to a forward-chaining
language using data dependencies, but with additional features for specifying
distributions. Using this language, we can define parameterized classes of
probabilistic models. These parameterized models make it possible to apply
probabilistic reasoning to problems for which it is impractical to have a
single large static model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1093</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1093</id><created>2013-03-27</created><authors><author><keyname>Shimony</keyname><forenames>Solomon Eyal</forenames></author><author><keyname>Charniak</keyname><forenames>Eugene</forenames></author></authors><title>A New Algorithm for Finding MAP Assignments to Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-98-105</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for finding maximum a-posterior) (MAP) assignments
of values to belief networks. The belief network is compiled into a network
consisting only of nodes with boolean (i.e. only 0 or 1) conditional
probabilities. The MAP assignment is then found using a best-first search on
the resulting network. We argue that, as one would anticipate, the algorithm is
exponential for the general case, but only linear in the size of the network
for poly trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1094</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1094</id><created>2013-03-27</created><authors><author><keyname>Bayse</keyname><forenames>K.</forenames></author><author><keyname>Lejter</keyname><forenames>M.</forenames></author><author><keyname>Kanazawa</keyname><forenames>Keiji</forenames></author></authors><title>Reducing Uncertainty in Navigation and Exploration</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-106-113</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A significant problem in designing mobile robot control systems involves
coping with the uncertainty that arises in moving about in an unknown or
partially unknown environment and relying on noisy or ambiguous sensor data to
acquire knowledge about that environment. We describe a control system that
chooses what activity to engage in next on the basis of expectations about how
the information re- turned as a result of a given activity will improve 2 its
knowledge about the spatial layout of its environment. Certain of the
higher-level components of the control system are specified in terms of
probabilistic decision models whose output is used to mediate the behavior of
lower-level control components responsible for movement and sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1095</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1095</id><created>2013-03-27</created><authors><author><keyname>Beinlich</keyname><forenames>Ingo</forenames></author><author><keyname>Herskovits</keyname><forenames>Edward H.</forenames></author></authors><title>Ergo: A Graphical Environment for Constructing Bayesian</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-114-121</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an environment that considerably simplifies the process of
generating Bayesian belief networks. The system has been implemented on readily
available, inexpensive hardware, and provides clarity and high performance. We
present an introduction to Bayesian belief networks, discuss algorithms for
inference with these networks, and delineate the classes of problems that can
be solved with this paradigm. We then describe the hardware and software that
constitute the system, and illustrate Ergo's use with several example
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1096</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1096</id><created>2013-03-27</created><authors><author><keyname>Breese</keyname><forenames>John S.</forenames></author><author><keyname>Fertig</keyname><forenames>Kenneth W.</forenames></author></authors><title>Decision Making with Interval Influence Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-122-129</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work (Fertig and Breese, 1989; Fertig and Breese, 1990) we
defined a mechanism for performing probabilistic reasoning in influence
diagrams using interval rather than point-valued probabilities. In this paper
we extend these procedures to incorporate decision nodes and interval-valued
value functions in the diagram. We derive the procedures for chance node
removal (calculating expected value) and decision node removal (optimization)
in influence diagrams where lower bounds on probabilities are stored at each
chance node and interval bounds are stored on the value function associated
with the diagram's value node. The output of the algorithm are a set of
admissible alternatives for each decision variable and a set of bounds on
expected value based on the imprecision in the input. The procedure can be
viewed as an approximation to a full e-dimensional sensitivity analysis where n
are the number of imprecise probability distributions in the input. We show the
transformations are optimal and sound. The performance of the algorithm on an
influence diagrams is investigated and compared to an exact algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1097</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1097</id><created>2013-03-27</created><authors><author><keyname>Chavez</keyname><forenames>R. Martin</forenames></author><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author></authors><title>A Randomized Approximation Algorithm of Logic Sampling</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-130-135</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, researchers in decision analysis and artificial intelligence
(AI) have used Bayesian belief networks to build models of expert opinion.
Using standard methods drawn from the theory of computational complexity,
workers in the field have shown that the problem of exact probabilistic
inference on belief networks almost certainly requires exponential computation
in the worst ease [3]. We have previously described a randomized approximation
scheme, called BN-RAS, for computation on belief networks [ 1, 2, 4]. We gave
precise analytic bounds on the convergence of BN-RAS and showed how to trade
running time for accuracy in the evaluation of posterior marginal
probabilities. We now extend our previous results and demonstrate the
generality of our framework by applying similar mathematical techniques to the
analysis of convergence for logic sampling [7], an alternative simulation
algorithm for probabilistic inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1098</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1098</id><created>2013-03-27</created><authors><author><keyname>Elfes</keyname><forenames>A.</forenames></author></authors><title>Occupancy Grids: A Stochastic Spatial Representation for Active Robot
  Perception</title><categories>cs.RO cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-136-146</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide an overview of a new framework for robot perception,
real-world modelling, and navigation that uses a stochastic tesselated
representation of spatial information called the Occupancy Grid. The Occupancy
Grid is a multi-dimensional random field model that maintains probabilistic
estimates of the occupancy state of each cell in a spatial lattice. Bayesian
estimation mechanisms employing stochastic sensor models allow incremental
updating of the Occupancy Grid using multi-view, multi-sensor data, composition
of multiple maps, decision-making, and incorporation of robot and sensor
position uncertainty. We present the underlying stochastic formulation of the
Occupancy Grid framework, and discuss its application to a variety of robotic
tusks. These include range-based mapping, multi-sensor integration,
path-planning and obstacle avoidance, handling of robot position uncertainty,
incorporation of pre-compiled maps, recovery of geometric representations, and
other related problems. The experimental results show that the Occupancy Grid
approach generates dense world models, is robust under sensor uncertainty and
errors, and allows explicit handling of uncertainty. It supports the
development of robust and agile sensor interpretation methods, incremental
discovery procedures, and composition of information from multiple sources.
Furthermore, the results illustrate that robotic tasks can be addressed through
operations performed di- rectly on the Occupancy Grid, and that these
operations have strong parallels to operations performed in the image
processing domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1099</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1099</id><created>2013-03-27</created><authors><author><keyname>Haddawy</keyname><forenames>Peter</forenames></author></authors><title>Time, Chance, and Action</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-147-154</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To operate intelligently in the world, an agent must reason about its
actions. The consequences of an action are a function of both the state of the
world and the action itself. Many aspects of the world are inherently
stochastic, so a representation for reasoning about actions must be able to
express chances of world states as well as indeterminacy in the effects of
actions and other events. This paper presents a propositional temporal
probability logic for representing and reasoning about actions. The logic can
represent the probability that facts hold and events occur at various times. It
can represent the probability that actions and other events affect the future.
It can represent concurrent actions and conditions that hold or change during
execution of an action. The model of probability relates probabilities over
time. The logical language integrates both modal and probabilistic constructs
and can thus represent and distinguish between possibility, probability, and
truth. Several examples illustrating the use of the logic are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1100</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1100</id><created>2013-03-27</created><authors><author><keyname>Horsch</keyname><forenames>Michael C.</forenames></author><author><keyname>Poole</keyname><forenames>David L.</forenames></author></authors><title>A Dynamic Approach to Probabilistic Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-155-161</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a framework for dynamically constructing Bayesian
networks. We introduce the notion of a background knowledge base of schemata,
which is a collection of parameterized conditional probability statements.
These schemata explicitly separate the general knowledge of properties an
individual may have from the specific knowledge of particular individuals that
may have these properties. Knowledge of individuals can be combined with this
background knowledge to create Bayesian networks, which can then be used in any
propagation scheme. We discuss the theory and assumptions necessary for the
implementation of dynamic Bayesian networks, and indicate where our approach
may be useful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1101</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1101</id><created>2013-03-27</created><authors><author><keyname>Jensen</keyname><forenames>Frank</forenames></author><author><keyname>Anderson</keyname><forenames>S. K.</forenames></author></authors><title>Approximations in Bayesian Belief Universe for Knowledge Based Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-162-169</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When expert systems based on causal probabilistic networks (CPNs) reach a
certain size and complexity, the &quot;combinatorial explosion monster&quot; tends to be
present. We propose an approximation scheme that identifies rarely occurring
cases and excludes these from being processed as ordinary cases in a CPN-based
expert system. Depending on the topology and the probability distributions of
the CPN, the numbers (representing probabilities of state combinations) in the
underlying numerical representation can become very small. Annihilating these
numbers and utilizing the resulting sparseness through data structuring
techniques often results in several orders of magnitude of improvement in the
consumption of computer resources. Bounds on the errors introduced into a
CPN-based expert system through approximations are established. Finally,
reports on empirical studies of applying the approximation scheme to a
real-world CPN are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1102</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1102</id><created>2013-03-27</created><authors><author><keyname>Lehner</keyname><forenames>Paul E.</forenames></author></authors><title>Robust Inference Policies</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-170-179</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A series of monte carlo studies were performed to assess the extent to which
different inference procedures robustly output reasonable belief values in the
context of increasing levels of judgmental imprecision. It was found that, when
compared to an equal-weights linear model, the Bayesian procedures are more
likely to deduce strong support for a hypothesis. But, the Bayesian procedures
are also more likely to strongly support the wrong hypothesis. Bayesian
techniques are more powerful, but are also more error prone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1103</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1103</id><created>2013-03-27</created><authors><author><keyname>Liu</keyname><forenames>L.</forenames></author><author><keyname>Ma</keyname><forenames>Y.</forenames></author><author><keyname>Wilkins</keyname><forenames>D.</forenames></author><author><keyname>Bian</keyname><forenames>Z.</forenames></author><author><keyname>Ying</keyname><forenames>X.</forenames></author></authors><title>Minimum Error Tree Decomposition</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-180-185</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a generalization of previous methods for constructing
tree-structured belief network with hidden variables. The major new feature of
the described method is the ability to produce a tree decomposition even when
there are errors in the correlation data among the input variables. This is an
important extension of existing methods since the correlational coefficients
usually cannot be measured with precision. The technique involves using a
greedy search algorithm that locally minimizes an error function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1104</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1104</id><created>2013-03-27</created><authors><author><keyname>Miller</keyname><forenames>J. W.</forenames></author><author><keyname>Goodman</keyname><forenames>R. M.</forenames></author></authors><title>A Polynomial Time Algorithm for Finding Bayesian Probabilities from
  Marginal Constraints</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-186-193</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method of calculating probability values from a system of marginal
constraints is presented. Previous systems for finding the probability of a
single attribute have either made an independence assumption concerning the
evidence or have required, in the worst case, time exponential in the number of
attributes of the system. In this paper a closed form solution to the
probability of an attribute given the evidence is found. The closed form
solution, however does not enforce the (non-linear) constraint that all terms
in the underlying distribution be positive. The equation requires O(r^3) steps
to evaluate, where r is the number of independent marginal constraints
describing the system at the time of evaluation. Furthermore, a marginal
constraint may be exchanged with a new constraint, and a new solution
calculated in O(r^2) steps. This method is appropriate for calculating
probabilities in a real time expert system
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1105</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1105</id><created>2013-03-27</created><authors><author><keyname>Neapolitan</keyname><forenames>Richard E.</forenames></author><author><keyname>Kenevan</keyname><forenames>James</forenames></author></authors><title>Computation of Variances in Causal Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-194-203</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The causal (belief) network is a well-known graphical structure for
representing independencies in a joint probability distribution. The exact
methods and the approximation methods, which perform probabilistic inference in
causal networks, often treat the conditional probabilities which are stored in
the network as certain values. However, if one takes either a subjectivistic or
a limiting frequency approach to probability, one can never be certain of
probability values. An algorithm for probabilistic inference should not only be
capable of reporting the inferred probabilities; it should also be capable of
reporting the uncertainty in these probabilities relative to the uncertainty in
the probabilities which are stored in the network. In section 2 of this paper a
method is given for determining the prior variances of the probabilities of all
the nodes. Section 3 contains an approximation method for determining the
variances in inferred probabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1106</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1106</id><created>2013-03-27</created><authors><author><keyname>Ng</keyname><forenames>Keung-Chi</forenames></author><author><keyname>Abramson</keyname><forenames>Bruce</forenames></author></authors><title>A Sensitivity Analysis of Pathfinder</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-204-211</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge elicitation is one of the major bottlenecks in expert system
design. Systems based on Bayes nets require two types of information--network
structure and parameters (or probabilities). Both must be elicited from the
domain expert. In general, parameters have greater opacity than structure, and
more time is spent in their refinement than in any other phase of elicitation.
Thus, it is important to determine the point of diminishing returns, beyond
which further refinements will promise little (if any) improvement. Sensitivity
analyses address precisely this issue--the sensitivity of a model to the
precision of its parameters. In this paper, we report the results of a
sensitivity analysis of Pathfinder, a Bayes net based system for diagnosing
pathologies of the lymph system. This analysis is intended to shed some light
on the relative importance of structure and parameters to system performance,
as well as the sensitivity of a system based on a Bayes net to noise in its
assessed parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1107</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1107</id><created>2013-03-27</created><authors><author><keyname>Srinivas</keyname><forenames>Sampath</forenames></author><author><keyname>Breese</keyname><forenames>John S.</forenames></author></authors><title>IDEAL: A Software Package for Analysis of Influence Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-212-219</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IDEAL (Influence Diagram Evaluation and Analysis in Lisp) is a software
environment for creation and evaluation of belief networks and influence
diagrams. IDEAL is primarily a research tool and provides an implementation of
many of the latest developments in belief network and influence diagram
evaluation in a unified framework. This paper describes IDEAL and some lessons
learned during its development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1108</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1108</id><created>2013-03-27</created><authors><author><keyname>Verma</keyname><forenames>Tom S.</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>On the Equivalence of Causal Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-220-227</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientists often use directed acyclic graphs (days) to model the qualitative
structure of causal theories, allowing the parameters to be estimated from
observational data. Two causal models are equivalent if there is no experiment
which could distinguish one from the other. A canonical representation for
causal models is presented which yields an efficient graphical criterion for
deciding equivalence, and provides a theoretical basis for extracting causal
structures from empirical data. This representation is then extended to the
more general case of an embedded causal model, that is, a dag in which only a
subset of the variables are observable. The canonical representation presented
here yields an efficient algorithm for determining when two embedded causal
models reflect the same dependency information. This algorithm leads to a model
theoretic definition of causation in terms of statistical dependencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1109</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1109</id><created>2013-03-27</created><authors><author><keyname>Wixson</keyname><forenames>Lambert E.</forenames></author></authors><title>Application of Confidence Intervals to the Autonomous Acquisition of
  High-level Spatial Knowledge</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-228-236</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objects in the world usually appear in context, participating in spatial
relationships and interactions that are predictable and expected. Knowledge of
these contexts can be used in the task of using a mobile camera to search for a
specified object in a room. We call this the object search task. This paper is
concerned with representing this knowledge in a manner facilitating its
application to object search while at the same time lending itself to
autonomous learning by a robot. The ability for the robot to learn such
knowledge without supervision is crucial due to the vast number of possible
relationships that can exist for any given set of objects. Moreover, since a
robot will not have an infinite amount of time to learn, it must be able to
determine an order in which to look for possible relationships so as to
maximize the rate at which new knowledge is gained. In effect, there must be a
&quot;focus of interest&quot; operator that allows the robot to choose which examples are
likely to convey the most new information and should be examined first. This
paper demonstrates how a representation based on statistical confidence
intervals allows the construction of a system that achieves the above goals. An
algorithm, based on the Highest Impact First heuristic, is presented as a means
for providing a &quot;focus of interest&quot; with which to control the learning process,
and examples are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1110</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1110</id><created>2013-03-27</created><authors><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author><author><keyname>Andersen</keyname><forenames>Stig K.</forenames></author><author><keyname>Poh</keyname><forenames>Kim-Leng</forenames></author></authors><title>Directed Reduction Algorithms and Decomposable Graphs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-237-244</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, there have been intense research efforts to develop
efficient methods for probabilistic inference in probabilistic influence
diagrams or belief networks. Many people have concluded that the best methods
are those based on undirected graph structures, and that those methods are
inherently superior to those based on node reduction operations on the
influence diagram. We show here that these two approaches are essentially the
same, since they are explicitly or implicity building and operating on the same
underlying graphical structures. In this paper we examine those graphical
structures and show how this insight can lead to an improved class of directed
reduction methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1111</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1111</id><created>2013-03-27</created><authors><author><keyname>Wen</keyname><forenames>Wilson X.</forenames></author></authors><title>Optimal Decomposition of Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-245-256</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, optimum decomposition of belief networks is discussed. Some
methods of decomposition are examined and a new method - the method of Minimum
Total Number of States (MTNS) - is proposed. The problem of optimum belief
network decomposition under our framework, as under all the other frameworks,
is shown to be NP-hard. According to the computational complexity analysis, an
algorithm of belief network decomposition is proposed in (Wee, 1990a) based on
simulated annealing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1112</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1112</id><created>2013-03-27</created><authors><author><keyname>Baker</keyname><forenames>Michelle</forenames></author><author><keyname>Boult</keyname><forenames>Terrance E.</forenames></author></authors><title>Pruning Bayesian Networks for Efficient Computation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-257-264</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the circumstances under which Bayesian networks can be
pruned in order to reduce computational complexity without altering the
computation for variables of interest. Given a problem instance which consists
of a query and evidence for a set of nodes in the network, it is possible to
delete portions of the network which do not participate in the computation for
the query. Savings in computational complexity can be large when the original
network is not singly connected. Results analogous to those described in this
paper have been derived before [Geiger, Verma, and Pearl 89, Shachter 88] but
the implications for reducing complexity of the computations in Bayesian
networks have not been stated explicitly. We show how a preprocessing step can
be used to prune a Bayesian network prior to using standard algorithms to solve
a given problem instance. We also show how our results can be used in a
parallel distributed implementation in order to achieve greater savings. We
define a computationally equivalent subgraph of a Bayesian network. The
algorithm developed in [Geiger, Verma, and Pearl 89] is modified to construct
the subgraphs described in this paper with O(e) complexity, where e is the
number of edges in the Bayesian network. Finally, we define a minimal
computationally equivalent subgraph and prove that the subgraphs described are
minimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1113</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1113</id><created>2013-03-27</created><authors><author><keyname>Stillman</keyname><forenames>Jonathan</forenames></author></authors><title>On Heuristics for Finding Loop Cutsets in Multiply-Connected Belief
  Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-265-272</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new heuristic algorithm for the problem of finding minimum
size loop cutsets in multiply connected belief networks. We compare this
algorithm to that proposed in [Suemmondt and Cooper, 1988]. We provide lower
bounds on the performance of these algorithms with respect to one another and
with respect to optimal. We demonstrate that no heuristic algorithm for this
problem cam be guaranteed to produce loop cutsets within a constant difference
from optimal. We discuss experimental results based on randomly generated
networks, and discuss future work and open questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1114</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1114</id><created>2013-03-27</created><authors><author><keyname>Suermondt</keyname><forenames>Jaap</forenames></author><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>A Combination of Cutset Conditioning with Clique-Tree Propagation in the
  Pathfinder System</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-273-280</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cutset conditioning and clique-tree propagation are two popular methods for
performing exact probabilistic inference in Bayesian belief networks. Cutset
conditioning is based on decomposition of a subset of network nodes, whereas
clique-tree propagation depends on aggregation of nodes. We describe a means to
combine cutset conditioning and clique- tree propagation in an approach called
aggregation after decomposition (AD). We discuss the application of the AD
method in the Pathfinder system, a medical expert system that offers assistance
with diagnosis in hematopathology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1115</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1115</id><created>2013-03-27</created><authors><author><keyname>Ruspini</keyname><forenames>Enrique H.</forenames></author></authors><title>Possibility as Similarity: the Semantics of Fuzzy Logic</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-281-289</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses fundamental issues on the nature of the concepts and
structures of fuzzy logic, focusing, in particular, on the conceptual and
functional differences that exist between probabilistic and possibilistic
approaches. A semantic model provides the basic framework to define
possibilistic structures and concepts by means of a function that quantifies
proximity, closeness, or resemblance between pairs of possible worlds. The
resulting model is a natural extension, based on multiple conceivability
relations, of the modal logic concepts of necessity and possibility. By
contrast, chance-oriented probabilistic concepts and structures rely on
measures of set extension that quantify the proportion of possible worlds where
a proposition is true. Resemblance between possible worlds is quantified by a
generalized similarity relation: a function that assigns a number between O and
1 to every pair of possible worlds. Using this similarity relation, which is a
form of numerical complement of a classic metric or distance, it is possible to
define and interpret the major constructs and methods of fuzzy logic:
conditional and unconditioned possibility and necessity distributions and the
generalized modus ponens of Zadeh.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1116</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1116</id><created>2013-03-27</created><authors><author><keyname>Dutta</keyname><forenames>Soumitra</forenames></author><author><keyname>Bonissone</keyname><forenames>Piero P.</forenames></author></authors><title>Integrating Case-Based and Rule-Based Reasoning: the Possibilistic
  Connection</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-290-300</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rule based reasoning (RBR) and case based reasoning (CBR) have emerged as two
important and complementary reasoning methodologies in artificial intelligence
(Al). For problem solving in complex, real world situations, it is useful to
integrate RBR and CBR. This paper presents an approach to achieve a compact and
seamless integration of RBR and CBR within the base architecture of rules. The
paper focuses on the possibilistic nature of the approximate reasoning
methodology common to both CBR and RBR. In CBR, the concept of similarity is
casted as the complement of the distance between cases. In RBR the transitivity
of similarity is the basis for the approximate deductions based on the
generalized modus ponens. It is shown that the integration of CBR and RBR is
possible without altering the inference engine of RBR. This integration is
illustrated in the financial domain of mergers and acquisitions. These ideas
have been implemented in a prototype system called MARS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1117</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1117</id><created>2013-03-27</created><authors><author><keyname>Yager</keyname><forenames>Ronald R.</forenames></author></authors><title>Credibility Discounting in the Theory of Approximate Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-301-306</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are concerned with the problem of introducing credibility type information
into reasoning systems. The concept of credibility allows us to discount
information provided by agents. An important characteristic of this kind of
procedure is that a complete lack of credibility rather than resulting in the
negation of the information provided results in the nullification of the
information provided. We suggest a representational scheme for credibility
qualification in the theory of approximate reasoning. We discuss the concept of
relative credibility. By this idea we mean to indicate situations in which the
credibility of a piece of evidence is determined by its compatibility with
higher priority evidence. This situation leads to structures very much in the
spirit of nonmonotonic reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1118</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1118</id><created>2013-03-27</created><authors><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>Updating with Belief Functions, Ordinal Conditioning Functions and
  Possibility Measures</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-307-316</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses how a measure of uncertainty representing a state of
knowledge can be updated when a new information, which may be pervaded with
uncertainty, becomes available. This problem is considered in various
framework, namely: Shafer's evidence theory, Zadeh's possibility theory,
Spohn's theory of epistemic states. In the two first cases, analogues of
Jeffrey's rule of conditioning are introduced and discussed. The relations
between Spohn's model and possibility theory are emphasized and Spohn's
updating rule is contrasted with the Jeffrey-like rule of conditioning in
possibility theory. Recent results by Shenoy on the combination of ordinal
conditional functions are reinterpreted in the language of possibility theory.
It is shown that Shenoy's combination rule has a well-known possibilistic
counterpart.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1119</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1119</id><created>2013-03-27</created><authors><author><keyname>Fagin</keyname><forenames>Ronald</forenames></author><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>A New Approach to Updating Beliefs</title><categories>cs.AI cs.LO</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-317-325</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a new notion of conditional belief, which plays the same role for
Dempster-Shafer belief functions as conditional probability does for
probability functions. Our definition is different from the standard definition
given by Dempster, and avoids many of the well-known problems of that
definition. Just as the conditional probability Pr (lB) is a probability
function which is the result of conditioning on B being true, so too our
conditional belief function Bel (lB) is a belief function which is the result
of conditioning on B being true. We define the conditional belief as the lower
envelope (that is, the inf) of a family of conditional probability functions,
and provide a closed form expression for it. An alternate way of understanding
our definition of conditional belief is provided by considering ideas from an
earlier paper [Fagin and Halpern, 1989], where we connect belief functions with
inner measures. In particular, we show here how to extend the definition of
conditional probability to non measurable sets, in order to get notions of
inner and outer conditional probabilities, which can be viewed as best
approximations to the true conditional probability, given our lack of
information. Our definition of conditional belief turns out to be an exact
analogue of our definition of inner conditional probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1120</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1120</id><created>2013-03-27</created><authors><author><keyname>Smets</keyname><forenames>Philippe</forenames></author></authors><title>The Transferable Belief Model and Other Interpretations of
  Dempster-Shafer's Model</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-326-333</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dempster-Shafer's model aims at quantifying degrees of belief But there are
so many interpretations of Dempster-Shafer's theory in the literature that it
seems useful to present the various contenders in order to clarify their
respective positions. We shall successively consider the classical probability
model, the upper and lower probabilities model, Dempster's model, the
transferable belief model, the evidentiary value model, the provability or
necessity model. None of these models has received the qualification of
Dempster-Shafer. In fact the transferable belief model is our interpretation
not of Dempster's work but of Shafer's work as presented in his book (Shafer
1976, Smets 1988). It is a ?purified' form of Dempster-Shafer's model in which
any connection with probability concept has been deleted. Any model for belief
has at least two components: one static that describes our state of belief, the
other dynamic that explains how to update our belief given new pieces of
information. We insist on the fact that both components must be considered in
order to study these models. Too many authors restrict themselves to the static
component and conclude that Dempster-Shafer theory is the same as some other
theory. But once the dynamic component is considered, these conclusions break
down. Any comparison based only on the static component is too restricted. The
dynamic component must also be considered as the originality of the models
based on belief functions lies in its dynamic component.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1121</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1121</id><created>2013-03-27</created><authors><author><keyname>Shenoy</keyname><forenames>Prakash P.</forenames></author><author><keyname>Shafer</keyname><forenames>Glenn</forenames></author></authors><title>Valuation-Based Systems for Discrete Optimization</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-334-343</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes valuation-based systems for representing and solving
discrete optimization problems. In valuation-based systems, we represent
information in an optimization problem using variables, sample spaces of
variables, a set of values, and functions that map sample spaces of sets of
variables to the set of values. The functions, called valuations, represent the
factors of an objective function. Solving the optimization problem involves
using two operations called combination and marginalization. Combination tells
us how to combine the factors of the joint objective function. Marginalization
is either maximization or minimization. Solving an optimization problem can be
simply described as finding the marginal of the joint objective function for
the empty set. We state some simple axioms that combination and marginalization
need to satisfy to enable us to solve an optimization problem using local
computation. For optimization problems, the solution method of valuation-based
systems reduces to non-serial dynamic programming. Thus our solution method for
VBS can be regarded as an abstract description of dynamic programming. And our
axioms can be viewed as conditions that permit the use of dynamic programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1122</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1122</id><created>2013-03-27</created><authors><author><keyname>Kennes</keyname><forenames>Robert</forenames></author><author><keyname>Smets</keyname><forenames>Philippe</forenames></author></authors><title>Computational Aspects of the Mobius Transform</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-344-351</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we associate with every (directed) graph G a transformation
called the Mobius transformation of the graph G. The Mobius transformation of
the graph (O) is of major significance for Dempster-Shafer theory of evidence.
However, because it is computationally very heavy, the Mobius transformation
together with Dempster's rule of combination is a major obstacle to the use of
Dempster-Shafer theory for handling uncertainty in expert systems. The major
contribution of this paper is the discovery of the 'fast Mobius
transformations' of (O). These 'fast Mobius transformations' are the fastest
algorithms for computing the Mobius transformation of (O). As an easy but
useful application, we provide, via the commonality function, an algorithm for
computing Dempster's rule of combination which is much faster than the usual
one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1123</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1123</id><created>2013-03-27</created><authors><author><keyname>Saffiotti</keyname><forenames>Alessandro</forenames></author></authors><title>Using Dempster-Shafer Theory in Knowledge Representation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-352-361</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we suggest marrying Dempster-Shafer (DS) theory with Knowledge
Representation (KR). Born out of this marriage is the definition of
&quot;Dempster-Shafer Belief Bases&quot;, abstract data types representing uncertain
knowledge that use DS theory for representing strength of belief about our
knowledge, and the linguistic structures of an arbitrary KR system for
representing the knowledge itself. A formal result guarantees that both the
properties of the given KR system and of DS theory are preserved. The general
model is exemplified by defining DS Belief Bases where First Order Logic and
(an extension of) KRYPTON are used as KR systems. The implementation problem is
also touched upon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1124</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1124</id><created>2013-03-27</created><authors><author><keyname>Berenji</keyname><forenames>Hamid R.</forenames></author><author><keyname>Chen</keyname><forenames>Yung-Yaw</forenames></author><author><keyname>Lee</keyname><forenames>Chuen-Chien</forenames></author><author><keyname>Jang</keyname><forenames>Jyh-Shing</forenames></author><author><keyname>Murugesan</keyname><forenames>S.</forenames></author></authors><title>A Hierarchical Approach to Designing Approximate Reasoning-Based
  Controllers for Dynamic Physical Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-362-369</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new technique for the design of approximate reasoning
based controllers for dynamic physical systems with interacting goals. In this
approach, goals are achieved based on a hierarchy defined by a control
knowledge base and remain highly interactive during the execution of the
control task. The approach has been implemented in a rule-based computer
program which is used in conjunction with a prototype hardware system to solve
the cart-pole balancing problem in real-time. It provides a complementary
approach to the conventional analytical control methodology, and is of
substantial use where a precise mathematical model of the process being
controlled is not available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1125</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1125</id><created>2013-03-27</created><authors><author><keyname>Chang</keyname><forenames>L. W.</forenames></author><author><keyname>Kashyap</keyname><forenames>Rangasami L.</forenames></author></authors><title>Evidence Combination and Reasoning and Its Application to Real-World
  Problem-Solving</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-370-377</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a new mathematical procedure is presented for combining
different pieces of evidence which are represented in the interval form to
reflect our knowledge about the truth of a hypothesis. Evidences may be
correlated to each other (dependent evidences) or conflicting in supports
(conflicting evidences). First, assuming independent evidences, we propose a
methodology to construct combination rules which obey a set of essential
properties. The method is based on a geometric model. We compare results
obtained from Dempster-Shafer's rule and the proposed combination rules with
both conflicting and non-conflicting data and show that the values generated by
proposed combining rules are in tune with our intuition in both cases.
Secondly, in the case that evidences are known to be dependent, we consider
extensions of the rules derived for handling conflicting evidence. The
performance of proposed rules are shown by different examples. The results show
that the proposed rules reasonably make decision under dependent evidences
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1126</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1126</id><created>2013-03-27</created><authors><author><keyname>da Silva</keyname><forenames>F. Correa</forenames></author><author><keyname>Bundy</keyname><forenames>Alan</forenames></author></authors><title>On Some Equivalence Relations between Incidence Calculus and
  Dempster-Shafer Theory of Evidence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-378-383</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incidence Calculus and Dempster-Shafer Theory of Evidence are both theories
to describe agents' degrees of belief in propositions, thus being appropriate
to represent uncertainty in reasoning systems. This paper presents a
straightforward equivalence proof between some special cases of these theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1127</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1127</id><created>2013-03-27</created><authors><author><keyname>McLeish</keyname><forenames>Mary</forenames></author><author><keyname>Yao</keyname><forenames>P.</forenames></author><author><keyname>Stirtzinger</keyname><forenames>T.</forenames></author></authors><title>Using Belief Functions for Uncertainty Management and Knowledge
  Acquisition: An Expert Application</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-384-391</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes recent work on an ongoing project in medical diagnosis
at the University of Guelph. A domain on which experts are not very good at
pinpointing a single disease outcome is explored. On-line medical data is
available over a relatively short period of time. Belief Functions
(Dempster-Shafer theory) are first extracted from data and then modified with
expert opinions. Several methods for doing this are compared and results show
that one formulation statistically outperforms the others, including a method
suggested by Shafer. Expert opinions and statistically derived information
about dependencies among symptoms are also compared. The benefits of using
uncertainty management techniques as methods for knowledge acquisition from
data are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1128</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1128</id><created>2013-03-27</created><authors><author><keyname>Fung</keyname><forenames>Robert</forenames></author><author><keyname>Crawford</keyname><forenames>S. L.</forenames></author><author><keyname>Appelbaum</keyname><forenames>Lee A.</forenames></author><author><keyname>Tong</keyname><forenames>Richard M.</forenames></author></authors><title>An Architecture for Probabilistic Concept-Based Information Retrieval</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-392-404</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While concept-based methods for information retrieval can provide improved
performance over more conventional techniques, they require large amounts of
effort to acquire the concepts and their qualitative and quantitative
relationships. This paper discusses an architecture for probabilistic
concept-based information retrieval which addresses the knowledge acquisition
problem. The architecture makes use of the probabilistic networks technology
for representing and reasoning about concepts and includes a knowledge
acquisition component which partially automates the construction of concept
knowledge bases from data. We describe two experiments that apply the
architecture to the task of retrieving documents about terrorism from a set of
documents from the Reuters news service. The experiments provide positive
evidence that the architecture design is feasible and that there are advantages
to concept-based methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1129</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1129</id><created>2013-03-27</created><authors><author><keyname>Hanson</keyname><forenames>A. J.</forenames></author></authors><title>Amplitude-Based Approach to Evidence Accumulation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-405-414</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We point out the need to use probability amplitudes rather than probabilities
to model evidence accumulation in decision processes involving real physical
sensors. Optical information processing systems are given as typical examples
of systems that naturally gather evidence in this manner. We derive a new,
amplitude-based generalization of the Hough transform technique used for object
recognition in machine vision. We argue that one should use complex Hough
accumulators and square their magnitudes to get a proper probabilistic
interpretation of the likelihood that an object is present. Finally, we suggest
that probability amplitudes may have natural applications in connectionist
models, as well as in formulating knowledge-based reasoning problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1130</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1130</id><created>2013-03-27</created><authors><author><keyname>Laskey</keyname><forenames>Kathryn Blackmond</forenames></author></authors><title>A Probabilistic Reasoning Environment</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-415-422</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A framework is presented for a computational theory of probabilistic
argument. The Probabilistic Reasoning Environment encodes knowledge at three
levels. At the deepest level are a set of schemata encoding the system's domain
knowledge. This knowledge is used to build a set of second-level arguments,
which are structured for efficient recapture of the knowledge used to construct
them. Finally, at the top level is a Bayesian network constructed from the
arguments. The system is designed to facilitate not just propagation of beliefs
and assimilation of evidence, but also the dynamic process of constructing a
belief network, evaluating its adequacy, and revising it when necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1131</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1131</id><created>2013-03-27</created><authors><author><keyname>Nguyen</keyname><forenames>Hung-Trung</forenames></author></authors><title>On Non-monotonic Conditional Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-423-427</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note is concerned with a formal analysis of the problem of non-monotonic
reasoning in intelligent systems, especially when the uncertainty is taken into
account in a quantitative way. A firm connection between logic and probability
is established by introducing conditioning notions by means of formal
structures that do not rely on quantitative measures. The associated
conditional logic, compatible with conditional probability evaluations, is
non-monotonic relative to additional evidence. Computational aspects of
conditional probability logic are mentioned. The importance of this development
lies on its role to provide a conceptual basis for various forms of evidence
combination and on its significance to unify multi-valued and non-monotonic
logics
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1132</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1132</id><created>2013-03-27</created><authors><author><keyname>Pittarelli</keyname><forenames>Michael</forenames></author></authors><title>Decisions with Limited Observations over a Finite Product Space: the
  Klir Effect</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-428-435</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probability estimation by maximum entropy reconstruction of an initial
relative frequency estimate from its projection onto a hypergraph model of the
approximate conditional independence relations exhibited by it is investigated.
The results of this study suggest that use of this estimation technique may
improve the quality of decisions that must be made on the basis of limited
observations over a decomposable finite product space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1133</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1133</id><created>2013-03-27</created><authors><author><keyname>Russell</keyname><forenames>Stuart</forenames></author></authors><title>Fine-Grained Decision-Theoretic Search Control</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-436-442</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision-theoretic control of search has previously used as its basic unit.
of computation the generation and evaluation of a complete set of successors.
Although this simplifies analysis, it results in some lost opportunities for
pruning and satisficing. This paper therefore extends the analysis of the value
of computation to cover individual successor evaluations. The analytic
techniques used may prove useful for control of reasoning in more general
settings. A formula is developed for the expected value of a node, k of whose n
successors have been evaluated. This formula is used to estimate the value of
expanding further successors, using a general formula for the value of a
computation in game-playing developed in earlier work. We exhibit an improved
version of the MGSS* algorithm, giving empirical results for the game of
Othello.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1134</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1134</id><created>2013-03-27</created><authors><author><keyname>Wilson</keyname><forenames>Nic</forenames></author></authors><title>Rules, Belief Functions and Default Logic</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-443-449</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a natural framework for rules, based on belief
functions, which includes a repre- sentation of numerical rules, default rules
and rules allowing and rules not allowing contraposition. In particular it
justifies the use of the Dempster-Shafer Theory for representing a particular
class of rules, Belief calculated being a lower probability given certain
independence assumptions on an underlying space. It shows how a belief function
framework can be generalised to other logics, including a general Monte-Carlo
algorithm for calculating belief, and how a version of Reiter's Default Logic
can be seen as a limiting case of a belief function formalism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1135</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1135</id><created>2013-03-27</created><authors><author><keyname>Wong</keyname><forenames>Michael S. K. M.</forenames></author><author><keyname>Lingras</keyname><forenames>P.</forenames></author></authors><title>Combination of Evidence Using the Principle of Minimum Information Gain</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-450-459</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most important aspects in any treatment of uncertain information
is the rule of combination for updating the degrees of uncertainty. The theory
of belief functions uses the Dempster rule to combine two belief functions
defined by independent bodies of evidence. However, with limited dependency
information about the accumulated belief the Dempster rule may lead to
unsatisfactory results. The present study suggests a method to determine the
accumulated belief based on the premise that the information gain from the
combination process should be minimum. This method provides a mechanism that is
equivalent to the Bayes rule when all the conditional probabilities are
available and to the Dempster rule when the normalization constant is equal to
one. The proposed principle of minimum information gain is shown to be
equivalent to the maximum entropy formalism, a special case of the principle of
minimum cross-entropy. The application of this principle results in a monotonic
increase in belief with accumulation of consistent evidence. The suggested
approach may provide a more reasonable criterion for identifying conflicts
among various bodies of evidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1136</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1136</id><created>2013-03-27</created><authors><author><keyname>Wu</keyname><forenames>Thomas D.</forenames></author></authors><title>Probabilistic Evaluation of Candidates and Symptom Clustering for
  Multidisorder Diagnosis</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-460-467</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper derives a formula for computing the conditional probability of a
set of candidates, where a candidate is a set of disorders that explain a given
set of positive findings. Such candidate sets are produced by a recent method
for multidisorder diagnosis called symptom clustering. A symptom clustering
represents a set of candidates compactly as a cartesian product of differential
diagnoses. By evaluating the probability of a candidate set, then, a large set
of candidates can be validated or pruned simultaneously. The probability of a
candidate set is then specialized to obtain the probability of a single
candidate. Unlike earlier results, the equation derived here allows the
specification of positive, negative, and unknown symptoms and does not make
assumptions about disorders not in the candidate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1137</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1137</id><created>2013-03-27</created><authors><author><keyname>Yen</keyname><forenames>John</forenames></author><author><keyname>Bonissone</keyname><forenames>Piero P.</forenames></author></authors><title>Extending Term Subsumption systems for Uncertainty Management</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-468-474</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major difficulty in developing and maintaining very large knowledge bases
originates from the variety of forms in which knowledge is made available to
the KB builder. The objective of this research is to bring together two
complementary knowledge representation schemes: term subsumption languages,
which represent and reason about defining characteristics of concepts, and
proximate reasoning models, which deal with uncertain knowledge and data in
expert systems. Previous works in this area have primarily focused on
probabilistic inheritance. In this paper, we address two other important issues
regarding the integration of term subsumption-based systems and approximate
reasoning models. First, we outline a general architecture that specifies the
interactions between the deductive reasoner of a term subsumption system and an
approximate reasoner. Second, we generalize the semantics of terminological
language so that terminological knowledge can be used to make plausible
inferences. The architecture, combined with the generalized semantics, forms
the foundation of a synergistic tight integration of term subsumption systems
and approximate reasoning models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1138</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1138</id><created>2013-03-27</created><authors><author><keyname>Chang</keyname><forenames>Kuo-Chu</forenames></author><author><keyname>Fung</keyname><forenames>Robert</forenames></author></authors><title>Refinement and Coarsening of Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-475-482</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In almost all situation assessment problems, it is useful to dynamically
contract and expand the states under consideration as assessment proceeds.
Contraction is most often used to combine similar events or low probability
events together in order to reduce computation. Expansion is most often used to
make distinctions of interest which have significant probability in order to
improve the quality of the assessment. Although other uncertainty calculi,
notably Dempster-Shafer [Shafer, 1976], have addressed these operations, there
has not yet been any approach of refining and coarsening state spaces for the
Bayesian Network technology. This paper presents two operations for refining
and coarsening the state space in Bayesian Networks. We also discuss their
practical implications for knowledge acquisition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1139</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1139</id><created>2013-03-27</created><authors><author><keyname>Paa&#xdf;</keyname><forenames>Gerhard</forenames></author></authors><title>Second Order Probabilities for Uncertain and Conflicting Evidence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-483-490</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the elicitation of probabilities from human experts is
considered as a measurement process, which may be disturbed by random
'measurement noise'. Using Bayesian concepts a second order probability
distribution is derived reflecting the uncertainty of the input probabilities.
The algorithm is based on an approximate sample representation of the basic
probabilities. This sample is continuously modified by a stochastic simulation
procedure, the Metropolis algorithm, such that the sequence of successive
samples corresponds to the desired posterior distribution. The procedure is
able to combine inconsistent probabilities according to their reliability and
is applicable to general inference networks with arbitrary structure.
Dempster-Shafer probability mass functions may be included using specific
measurement distributions. The properties of the approach are demonstrated by
numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1140</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1140</id><created>2013-03-27</created><authors><author><keyname>van der Gaag</keyname><forenames>Linda C.</forenames></author></authors><title>Computing Probability Intervals Under Independency Constraints</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-491-497</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many AI researchers argue that probability theory is only capable of dealing
with uncertainty in situations where a full specification of a joint
probability distribution is available, and conclude that it is not suitable for
application in knowledge-based systems. Probability intervals, however,
constitute a means for expressing incompleteness of information. We present a
method for computing such probability intervals for probabilities of interest
from a partial specification of a joint probability distribution. Our method
improves on earlier approaches by allowing for independency relationships
between statistical variables to be exploited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1141</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1141</id><created>2013-03-27</created><authors><author><keyname>Shwe</keyname><forenames>Michael</forenames></author><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author></authors><title>An Empirical Analysis of Likelihood-Weighting Simulation on a Large,
  Multiply-Connected Belief Network</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-498-508</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyzed the convergence properties of likelihood- weighting algorithms on
a two-level, multiply connected, belief-network representation of the QMR
knowledge base of internal medicine. Specifically, on two difficult diagnostic
cases, we examined the effects of Markov blanket scoring, importance sampling,
demonstrating that the Markov blanket scoring and self-importance sampling
significantly improve the convergence of the simulation on our model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1142</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1142</id><created>2013-03-27</created><authors><author><keyname>Sher</keyname><forenames>David</forenames></author></authors><title>Towards a Normative Theory of Scientific Evidence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-509-517</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A scientific reasoning system makes decisions using objective evidence in the
form of independent experimental trials, propositional axioms, and constraints
on the probabilities of events. As a first step towards this goal, we propose a
system that derives probability intervals from objective evidence in those
forms. Our reasoning system can manage uncertainty about data and rules in a
rule based expert system. We expect that our system will be particularly
applicable to diagnosis and analysis in domains with a wealth of experimental
evidence such as medicine. We discuss limitations of this solution and propose
future directions for this research. This work can be considered a
generalization of Nilsson's &quot;probabilistic logic&quot; [Nil86] to intervals and
experimental observations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1143</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1143</id><created>2013-03-27</created><authors><author><keyname>McLeish</keyname><forenames>Mary</forenames></author></authors><title>A Model for Non-Monotonic Reasoning Using Dempster's Rule</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-518-528</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considerable attention has been given to the problem of non-monotonic
reasoning in a belief function framework. Earlier work (M. Ginsberg) proposed
solutions introducing meta-rules which recognized conditional independencies in
a probabilistic sense. More recently an e-calculus formulation of default
reasoning (J. Pearl) shows that the application of Dempster's rule to a
non-monotonic situation produces erroneous results. This paper presents a new
belief function interpretation of the problem which combines the rules in a way
which is more compatible with probabilistic results and respects conditions of
independence necessary for the application of Dempster's combination rule. A
new general framework for combining conflicting evidence is also proposed in
which the normalization factor becomes modified. This produces more intuitively
acceptable results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1144</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1144</id><created>2013-03-27</created><authors><author><keyname>Smets</keyname><forenames>Philippe</forenames></author><author><keyname>Hsia</keyname><forenames>Yen-Teh</forenames></author></authors><title>Default Reasoning and the Transferable Belief Model</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-529-537</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inappropriate use of Dempster's rule of combination has led some authors to
reject the Dempster-Shafer model, arguing that it leads to supposedly
unacceptable conclusions when defaults are involved. A most classic example is
about the penguin Tweety. This paper will successively present: the origin of
the miss-management of the Tweety example; two types of default; the correct
solution for both types based on the transferable belief model (our
interpretation of the Dempster-Shafer model (Shafer 1976, Smets 1988)); Except
when explicitly stated, all belief functions used in this paper are simple
support functions, i.e. belief functions for which only one proposition (the
focus) of the frame of discernment receives a positive basic belief mass with
the remaining mass being given to the tautology. Each belief function will be
described by its focus and the weight of the focus (e.g. m(A)=.9). Computation
of the basic belief masses are always performed by vacuously extending each
belief function to the product space built from all variables involved,
combining them on that space by Dempster's rule of combination, and projecting
the result to the space corresponding to each individual variable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1145</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1145</id><created>2013-03-27</created><updated>2015-05-16</updated><authors><author><keyname>Geiger</keyname><forenames>Dan</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>Separable and transitive graphoids</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-1990-PG-538-545</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine three probabilistic formulations of the sentence a and b are
totally unrelated with respect to a given set of variables U. First, two
variables a and b are totally independent if they are independent given any
value of any subset of the variables in U. Second, two variables are totally
uncoupled if U can be partitioned into two marginally independent sets
containing a and b respectively. Third, two variables are totally disconnected
if the corresponding nodes are disconnected in every belief network
representation. We explore the relationship between these three formulations of
unrelatedness and explain their relevance to the process of acquiring
probabilistic knowledge from human experts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1146</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1146</id><created>2013-03-27</created><authors><author><keyname>Chamberlain</keyname><forenames>Bo</forenames></author><author><keyname>Jensen</keyname><forenames>Finn Verner</forenames></author><author><keyname>Jensen</keyname><forenames>Frank</forenames></author><author><keyname>Nordahl</keyname><forenames>Torsten</forenames></author></authors><title>Analysis in HUGIN of Data Conflict</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Sixth Conference on Uncertainty in
  Artificial Intelligence (UAI1990)</comments><proxy>auai</proxy><report-no>UAI-P-1990-PG-546-554</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After a brief introduction to causal probabilistic networks and the HUGIN
approach, the problem of conflicting data is discussed. A measure of conflict
is defined, and it is used in the medical diagnostic system MUNIN. Finally, it
is discussed how to distinguish between conflicting data and a rare case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1171</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1171</id><created>2013-04-03</created><updated>2013-04-20</updated><authors><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author><author><keyname>Macy</keyname><forenames>Michael</forenames></author><author><keyname>Redner</keyname><forenames>Sidney</forenames></author></authors><title>Editorial: Statistical Mechanics and Social Sciences</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>5 pages, 6 references. The contents of the special issue can be found
  at these links: http://link.springer.com/journal/10955/151/1/page/1 (Volume
  I) and http://link.springer.com/journal/10955/151/3/page/1 (Volume II). Minor
  modifications</comments><journal-ref>Journal of Statistical Physics 151 (1-2), 1-8 (2013)</journal-ref><doi>10.1007/s10955-013-0703-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This editorial opens the special issues that the Journal of Statistical
Physics has dedicated to the growing field of statistical physics modeling of
social dynamics. The issues include contributions from physicists and social
scientists, with the goal of fostering a better communication between these two
communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1185</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1185</id><created>2013-04-03</created><updated>2013-07-12</updated><authors><author><keyname>Esparza</keyname><forenames>Javier</forenames></author><author><keyname>Ganty</keyname><forenames>Pierre</forenames></author><author><keyname>Majumdar</keyname><forenames>Rupak</forenames></author></authors><title>Parameterized Verification of Asynchronous Shared-Memory Systems</title><categories>cs.LO cs.FL</categories><comments>26 pages, International Conference on Computer Aided Verification
  (CAV'13)</comments><doi>10.1007/978-3-642-39799-8_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the complexity of the safety verification problem for
parameterized systems consisting of a leader process and arbitrarily many
anonymous and identical contributors. Processes communicate through a shared,
bounded-value register. While each operation on the register is atomic, there
is no synchronization primitive to execute a sequence of operations atomically.
We analyze the complexity of the safety verification problem when processes are
modeled by finite-state machines, pushdown machines, and Turing machines. The
problem is coNP-complete when all processes are finite-state machines, and is
PSPACE-complete when they are pushdown machines. The complexity remains
coNP-complete when each Turing machine is allowed boundedly many interactions
with the register. Our proofs use combinatorial characterizations of
computations in the model, and in case of pushdown-systems, some
language-theoretic constructions of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1188</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1188</id><created>2013-04-03</created><updated>2013-04-11</updated><authors><author><keyname>Pagh</keyname><forenames>Rasmus</forenames></author><author><keyname>Segev</keyname><forenames>Gil</forenames></author><author><keyname>Wieder</keyname><forenames>Udi</forenames></author></authors><title>How to Approximate A Set Without Knowing Its Size In Advance</title><categories>cs.DS</categories><comments>Clarified a point in the lower bound proof</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamic approximate membership problem asks to represent a set S of size
n, whose elements are provided in an on-line fashion, supporting membership
queries without false negatives and with a false positive rate at most epsilon.
That is, the membership algorithm must be correct on each x in S, and may err
with probability at most epsilon on each x not in S.
  We study a well-motivated, yet insufficiently explored, variant of this
problem where the size n of the set is not known in advance. Existing optimal
approximate membership data structures require that the size is known in
advance, but in many practical scenarios this is not a realistic assumption.
Moreover, even if the eventual size n of the set is known in advance, it is
desirable to have the smallest possible space usage also when the current
number of inserted elements is smaller than n. Our contribution consists of the
following results:
  - We show a super-linear gap between the space complexity when the size is
known in advance and the space complexity when the size is not known in
advance.
  - We show that our space lower bound is tight, and can even be matched by a
highly efficient data structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1192</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1192</id><created>2013-04-03</created><authors><author><keyname>Qian</keyname><forenames>Qi</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Yi</keyname><forenames>Jinfeng</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author><author><keyname>Zhu</keyname><forenames>Shenghuo</forenames></author></authors><title>Efficient Distance Metric Learning by Adaptive Sampling and Mini-Batch
  Stochastic Gradient Descent (SGD)</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distance metric learning (DML) is an important task that has found
applications in many domains. The high computational cost of DML arises from
the large number of variables to be determined and the constraint that a
distance metric has to be a positive semi-definite (PSD) matrix. Although
stochastic gradient descent (SGD) has been successfully applied to improve the
efficiency of DML, it can still be computationally expensive because in order
to ensure that the solution is a PSD matrix, it has to, at every iteration,
project the updated distance metric onto the PSD cone, an expensive operation.
We address this challenge by developing two strategies within SGD, i.e.
mini-batch and adaptive sampling, to effectively reduce the number of updates
(i.e., projections onto the PSD cone) in SGD. We also develop hybrid approaches
that combine the strength of adaptive sampling with that of mini-batch online
learning techniques to further improve the computational efficiency of SGD for
DML. We prove the theoretical guarantees for both adaptive sampling and
mini-batch based approaches for DML. We also conduct an extensive empirical
study to verify the effectiveness of the proposed algorithms for DML.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1199</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1199</id><created>2013-04-03</created><updated>2013-06-08</updated><authors><author><keyname>van Leeuwen</keyname><forenames>David A.</forenames></author><author><keyname>Br&#xfc;mmer</keyname><forenames>Niko</forenames></author></authors><title>The distribution of calibrated likelihood-ratios in speaker recognition</title><categories>stat.AP cs.SD</categories><comments>Accepted to Interspeech 2013, fixed legend of fig 2</comments><journal-ref>PROC INTERSPEECH 2013, ISSN 2308-457X, pp 1619-1623</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies properties of the score distributions of calibrated
log-likelihood-ratios that are used in automatic speaker recognition. We derive
the essential condition for calibration that the log likelihood ratio of the
log-likelihood-ratio is the log-likelihood-ratio. We then investigate what the
consequence of this condition is to the probability density functions (PDFs) of
the log-likelihood-ratio score. We show that if the PDF of the non-target
distribution is Gaussian, then the PDF of the target distribution must be
Gaussian as well. The means and variances of these two PDFs are interrelated,
and determined completely by the discrimination performance of the recognizer
characterized by the equal error rate. These relations allow for a new way of
computing the offset and scaling parameters for linear calibration, and we
derive closed-form expressions for these and show that for modern i-vector
systems with PLDA scoring this leads to good calibration, comparable to
traditional logistic regression, over a wide range of system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1202</identifier>
 <datestamp>2014-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1202</id><created>2013-04-03</created><updated>2014-02-04</updated><authors><author><keyname>Guo</keyname><forenames>Alan</forenames></author></authors><title>High rate locally correctable codes via lifting</title><categories>cs.IT math.IT</categories><comments>Fixed some typos, replaced references, which were missing in previous
  arXiv version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general framework for constructing high rate error correcting
codes that are locally correctable (and hence locally decodable if linear) with
a sublinear number of queries, based on lifting codes with respect to functions
on the coordinates. Our approach generalizes the lifting of affine-invariant
codes of Guo, Kopparty, and Sudan and its generalization automorphic lifting,
suggested by Ben-Sasson et al, which lifts algebraic geometry codes with
respect to a group of automorphisms of the code. Our notion of lifting is a
natural alternative to the degree-lifting of Ben-Sasson et al and it carries
two advantages. First, it overcomes the rate barrier inherent in
degree-lifting. Second, it is extremely flexible, requiring no special
properties (e.g. linearity, invariance) of the base code, and requiring very
little structure on the set of functions on the coordinates of the code.
  As an application, we construct new explicit families of locally correctable
codes by lifting algebraic geometry codes. Like the multiplicity codes of
Kopparty, Saraf, Yekhanin and the affine-lifted codes of Guo, Kopparty, Sudan,
our codes of block-length $N$ can achieve $N^\epsilon$ query complexity and
$1-\alpha$ rate for any given $\epsilon, \alpha &gt; 0$ while correcting a
constant fraction of errors, in contrast to the Reed-Muller codes and the
degree-lifted AG codes of Ben-Sasson et al which face a rate barrier of
$\epsilon^{O(1/\epsilon)}$. However, like the degree-lifted AG codes, our codes
are over an alphabet significantly smaller than that obtained by Reed-Muller
codes, affine-lifted codes, and multiplicity codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1206</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1206</id><created>2013-04-03</created><updated>2013-11-03</updated><authors><author><keyname>Huang</keyname><forenames>Ming-Deh</forenames></author><author><keyname>Narayanan</keyname><forenames>Anand Kumar</forenames></author></authors><title>Finding Primitive Elements in Finite Fields of Small Characteristic</title><categories>cs.DM cs.CC math.CO</categories><comments>Modifications made to the polynomial selection and testing phases</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a deterministic algorithm for finding a generating element of the
multiplicative group of the finite field $\mathbb{F}_{p^n}$ where $p$ is a
prime. In time polynomial in $p$ and $n$, the algorithm either outputs an
element that is provably a generator or declares that it has failed in finding
one. The algorithm relies on a relation generation technique in Joux's
heuristically $L(1/4)$-method for discrete logarithm computation. Based on a
heuristic assumption, the algorithm does succeed in finding a generator. For
the special case when the order of $p$ in $(\mathbb{Z}/n\mathbb{Z})^\times$ is
small (that is $(\log_p(n))^{\mathcal{O}(1)}$), we present a modification with
greater guarantee of success while making weaker heuristic assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1207</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1207</id><created>2013-04-03</created><authors><author><keyname>Gluesing-Luerssen</keyname><forenames>Heide</forenames></author></authors><title>Fourier-Reflexive Partitions and MacWilliams Identities for Additive
  Codes</title><categories>cs.IT math.CO math.IT</categories><msc-class>94B05, 94B99, 16L60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A partition of a finite abelian group gives rise to a dual partition on the
character group via the Fourier transform. Properties of the dual partitions
are investigated and a convenient test is given for the case that the bidual
partition coincides the primal partition. Such partitions permit MacWilliams
identities for the partition enumerators of additive codes. It is shown that
dualization commutes with taking products and symmetrized products of
partitions on cartesian powers of the given group. After translating the
results to Frobenius rings, which are identified with their character module,
the approach is applied to partitions that arise from poset structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1209</identifier>
 <datestamp>2013-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1209</id><created>2013-04-03</created><authors><author><keyname>Fulcher</keyname><forenames>Ben D.</forenames></author><author><keyname>Little</keyname><forenames>Max A.</forenames></author><author><keyname>Jones</keyname><forenames>Nick S.</forenames></author></authors><title>Highly comparative time-series analysis: The empirical structure of time
  series and their methods</title><categories>physics.data-an cs.CV physics.bio-ph q-bio.QM stat.ML</categories><journal-ref>J. R. Soc. Interface vol. 10 no. 83 20130048 (2013)</journal-ref><doi>10.1098/rsif.2013.0048</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process of collecting and organizing sets of observations represents a
common theme throughout the history of science. However, despite the ubiquity
of scientists measuring, recording, and analyzing the dynamics of different
processes, an extensive organization of scientific time-series data and
analysis methods has never been performed. Addressing this, annotated
collections of over 35 000 real-world and model-generated time series and over
9000 time-series analysis algorithms are analyzed in this work. We introduce
reduced representations of both time series, in terms of their properties
measured by diverse scientific methods, and of time-series analysis methods, in
terms of their behaviour on empirical time series, and use them to organize
these interdisciplinary resources. This new approach to comparing across
diverse scientific data and methods allows us to organize time-series datasets
automatically according to their properties, retrieve alternatives to
particular analysis methods developed in other scientific disciplines, and
automate the selection of useful methods for time-series classification and
regression tasks. The broad scientific utility of these tools is demonstrated
on datasets of electroencephalograms, self-affine time series, heart beat
intervals, speech signals, and others, in each case contributing novel analysis
techniques to the existing literature. Highly comparative techniques that
compare across an interdisciplinary literature can thus be used to guide more
focused research in time-series analysis for applications across the scientific
disciplines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1217</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1217</id><created>2013-04-03</created><authors><author><keyname>Saglam</keyname><forenames>Mert</forenames></author><author><keyname>Tardos</keyname><forenames>Gabor</forenames></author></authors><title>On the communication complexity of sparse set disjointness and
  exists-equal problems</title><categories>cs.CC</categories><acm-class>F.1.1; F.1.2; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the two player randomized communication complexity of
the sparse set disjointness and the exists-equal problems and give matching
lower and upper bounds (up to constant factors) for any number of rounds for
both of these problems. In the sparse set disjointness problem, each player
receives a k-subset of [m] and the goal is to determine whether the sets
intersect. For this problem, we give a protocol that communicates a total of
O(k\log^{(r)}k) bits over r rounds and errs with very small probability. Here
we can take r=\log^{*}k to obtain a O(k) total communication \log^{*}k-round
protocol with exponentially small error probability, improving on the O(k)-bits
O(\log k)-round constant error probability protocol of Hastad and Wigderson
from 1997.
  In the exist-equal problem, the players receive vectors x,y\in [t]^n and the
goal is to determine whether there exists a coordinate i such that x_i=y_i.
Namely, the exists-equal problem is the OR of n equality problems. Observe that
exists-equal is an instance of sparse set disjointness with k=n, hence the
protocol above applies here as well, giving an O(n\log^{(r)}n) upper bound. Our
main technical contribution in this paper is a matching lower bound: we show
that when t=\Omega(n), any r-round randomized protocol for the exists-equal
problem with error probability at most 1/3 should have a message of size
\Omega(n\log^{(r)}n). Our lower bound holds even for super-constant r &lt;=
\log^*n, showing that any O(n) bits exists-equal protocol should have \log^*n -
O(1) rounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1220</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1220</id><created>2013-04-03</created><updated>2014-05-20</updated><authors><author><keyname>Gafni</keyname><forenames>Eli</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Petr</forenames></author><author><keyname>Manolescu</keyname><forenames>Ciprian</forenames></author></authors><title>A generalized asynchronous computability theorem</title><categories>cs.DC math.GN</categories><comments>16 pages, 5 figures</comments><msc-class>68Q85</msc-class><acm-class>C.2.4; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the models of distributed computation defined as subsets of the
runs of the iterated immediate snapshot model. Given a task $T$ and a model
$M$, we provide topological conditions for $T$ to be solvable in $M$. When
applied to the wait-free model, our conditions result in the celebrated
Asynchronous Computability Theorem (ACT) of Herlihy and Shavit. To demonstrate
the utility of our characterization, we consider a task that has been shown
earlier to admit only a very complex $t$-resilient solution. In contrast, our
generalized computability theorem confirms its $t$-resilient solvability in a
straightforward manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1221</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1221</id><created>2013-04-03</created><authors><author><keyname>Fritscher</keyname><forenames>Eliseu</forenames></author><author><keyname>Hoppen</keyname><forenames>Carlos</forenames></author><author><keyname>Trevisan</keyname><forenames>Vilmar</forenames></author></authors><title>Unicyclic Graphs with equal Laplacian Energy</title><categories>math.CO cs.DM math.SP</categories><comments>11 pages, 11 figures, slightly modified version of Theorem 1 when
  compared with original paper</comments><journal-ref>Linear and Multilinear Algebra 2013</journal-ref><doi>10.1080/03081087.2013.766395</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new operation on a class of graphs with the property that the
Laplacian eigenvalues of the input and output graphs are related. Based on this
operation, we obtain a family of order (square root of n) noncospectral
unicyclic graphs on n vertices with the same Laplacian energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1233</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1233</id><created>2013-04-03</created><authors><author><keyname>Sanin</keyname><forenames>Andres</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>Shadow Detection: A Survey and Comparative Evaluation of Recent Methods</title><categories>cs.CV cs.RO</categories><acm-class>I.4.6; I.4.8; I.5.4; I.2.10</acm-class><journal-ref>Pattern Recognition, Vol. 45, No. 4, pp. 1684-1695, 2012</journal-ref><doi>10.1016/j.patcog.2011.10.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a survey and a comparative evaluation of recent
techniques for moving cast shadow detection. We identify shadow removal as a
critical step for improving object detection and tracking. The survey covers
methods published during the last decade, and places them in a feature-based
taxonomy comprised of four categories: chromacity, physical, geometry and
textures. A selection of prominent methods across the categories is compared in
terms of quantitative performance measures (shadow detection and discrimination
rates, colour desaturation) as well as qualitative observations. Furthermore,
we propose the use of tracking performance as an unbiased approach for
determining the practical usefulness of shadow detection methods. The
evaluation indicates that all shadow detection approaches make different
contributions and all have individual strength and weaknesses. Out of the
selected methods, the geometry-based technique has strict assumptions and is
not generalisable to various environments, but it is a straightforward choice
when the objects of interest are easy to model and their shadows have different
orientation. The chromacity based method is the fastest to implement and run,
but it is sensitive to noise and less effective in low saturated scenes. The
physical method improves upon the accuracy of the chromacity method by adapting
to local shadow models, but fails when the spectral properties of the objects
are similar to that of the background. The small-region texture based method is
especially robust for pixels whose neighbourhood is textured, but may take
longer to implement and is the most computationally expensive. The large-region
texture based method produces the most accurate results, but has a significant
computational load due to its multiple processing steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1235</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1235</id><created>2013-04-03</created><authors><author><keyname>Thampi</keyname><forenames>Sabu M</forenames></author></authors><title>A Review on P2P Video Streaming</title><categories>cs.NI cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objective of this article is to provide an overview of P2P based
Video-on-Demand and live streaming services. The article starts with an
introduction to media streaming and its simplified architecture. Various
solutions offering video streaming in the context of widespread usage of
Internet are discussed. This is followed by a short introduction to P2P
networks and its applications. A broad discussion on various P2P streaming
schemes and P2P streaming applications are the main focus of this chapter.
Finally, the security issues and solutions for P2P video streaming are
discussed briefly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1238</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1238</id><created>2013-04-03</created><authors><author><keyname>Faug&#xe8;re</keyname><forenames>Jean-Charles</forenames></author><author><keyname>Mou</keyname><forenames>Chenqi</forenames></author></authors><title>Sparse FGLM algorithms</title><categories>cs.SC</categories><comments>40 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a zero-dimensional ideal I in K[x1,...,xn] of degree D, the
transformation of the ordering of its Groebner basis from DRL to LEX is a key
step in polynomial system solving and turns out to be the bottleneck of the
whole solving process. Thus it is of crucial importance to design efficient
algorithms to perform the change of ordering.
  The main contributions of this paper are several efficient methods for the
change of ordering which take advantage of the sparsity of multiplication
matrices in the classical FGLM algorithm. Combing all these methods, we propose
a deterministic top-level algorithm that automatically detects which method to
use depending on the input. As a by-product, we have a fast implementation that
is able to handle ideals of degree over 40000. Such an implementation
outperforms the Magma and Singular ones, as shown by our experiments.
  First for the shape position case, two methods are designed based on the
Wiedemann algorithm: the first is probabilistic and its complexity to complete
the change of ordering is O(D(N1+nlog(D))), where N1 is the number of nonzero
entries of a multiplication matrix; the other is deterministic and computes the
LEX Groebner basis of the radical of I via Chinese Remainder Theorem. Then for
the general case, the designed method is characterized by the
Berlekamp-Massey-Sakata algorithm from Coding Theory to handle the
multi-dimensional linearly recurring relations. Complexity analyses of all
proposed methods are also provided.
  Furthermore, for generic polynomial systems, we present an explicit formula
for the estimation of the sparsity of one main multiplication matrix, and prove
its construction is free. With the asymptotic analysis of such sparsity, we are
able to show for generic systems the complexity above becomes $O(\sqrt{6/n \pi}
D^{2+(n-1)/n}})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1239</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1239</id><created>2013-04-03</created><updated>2013-05-26</updated><authors><author><keyname>Selivanov</keyname><forenames>Victor</forenames></author></authors><title>Total Representations</title><categories>cs.LO math.LO</categories><comments>30 pages</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 2 (June 2,
  2013) lmcs:1191</journal-ref><doi>10.2168/LMCS-9(2:5)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Almost all representations considered in computable analysis are partial. We
provide arguments in favor of total representations (by elements of the Baire
space). Total representations make the well known analogy between numberings
and representations closer, unify some terminology, simplify some technical
details, suggest interesting open questions and new invariants of topological
spaces relevant to computable analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1245</identifier>
 <datestamp>2013-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1245</id><created>2013-04-04</created><updated>2013-04-08</updated><authors><author><keyname>Tsang</keyname><forenames>Hing Yin</forenames></author><author><keyname>Wong</keyname><forenames>Chung Hoi</forenames></author><author><keyname>Xie</keyname><forenames>Ning</forenames></author><author><keyname>Zhang</keyname><forenames>Shengyu</forenames></author></authors><title>Fourier sparsity, spectral norm, and the Log-rank conjecture</title><categories>cs.CC</categories><comments>v2: Corollary 31 of v1 removed because of a bug in the proof. (Other
  results not affected.)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study Boolean functions with sparse Fourier coefficients or small spectral
norm, and show their applications to the Log-rank Conjecture for XOR functions
f(x\oplus y) --- a fairly large class of functions including well studied ones
such as Equality and Hamming Distance. The rank of the communication matrix M_f
for such functions is exactly the Fourier sparsity of f. Let d be the F2-degree
of f and D^CC(f) stand for the deterministic communication complexity for
f(x\oplus y). We show that 1. D^CC(f) = O(2^{d^2/2} log^{d-2} ||\hat f||_1). In
particular, the Log-rank conjecture holds for XOR functions with constant
F2-degree. 2. D^CC(f) = O(d ||\hat f||_1) = O(\sqrt{rank(M_f)}\logrank(M_f)).
We obtain our results through a degree-reduction protocol based on a variant of
polynomial rank, and actually conjecture that its communication cost is already
\log^{O(1)}rank(M_f). The above bounds also hold for the parity decision tree
complexity of f, a measure that is no less than the communication complexity
(up to a factor of 2).
  Along the way we also show several structural results about Boolean functions
with small F2-degree or small spectral norm, which could be of independent
interest. For functions f with constant F2-degree: 1) f can be written as the
summation of quasi-polynomially many indicator functions of subspaces with
\pm-signs, improving the previous doubly exponential upper bound by Green and
Sanders; 2) being sparse in Fourier domain is polynomially equivalent to having
a small parity decision tree complexity; 3) f depends only on polylog||\hat
f||_1 linear functions of input variables. For functions f with small spectral
norm: 1) there is an affine subspace with co-dimension O(||\hat f||_1) on which
f is a constant; 2) there is a parity decision tree with depth O(||\hat f||_1
log ||\hat f||_0).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1247</identifier>
 <datestamp>2013-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1247</id><created>2013-04-04</created><updated>2013-11-13</updated><authors><author><keyname>Bei</keyname><forenames>Xiaohui</forenames></author><author><keyname>Chen</keyname><forenames>Ning</forenames></author><author><keyname>Zhang</keyname><forenames>Shengyu</forenames></author></authors><title>Solving Linear Programming with Constraints Unknown</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What is the value of input information in solving linear programming? The
celebrated ellipsoid algorithm tells us that the full information of input
constraints is not necessary; the algorithm works as long as there exists an
oracle that, on a proposed candidate solution, returns a violation in the
format of a separating hyperplane. Can linear programming still be efficiently
solved if the returned violation is in other formats?
  We study this question in a trial-and-error framework: there is an oracle
that, upon a proposed solution, returns the index of a violated constraint
(with the content of the constraint still hidden). When more than one
constraint is violated, two variants in the model are investigated. (1) The
oracle returns the index of a &quot;most violated&quot; constraint, measured by the
Euclidean distance of the proposed solution and the half-spaces defined by the
constraints. In this case, the LP can be efficiently solved. (2) The oracle
returns the index of an arbitrary (i.e., worst-case) violated constraint. In
this case, we give an algorithm with running time exponential in the number of
variables. We then show that the exponential dependence on n is unfortunately
necessary even for the query complexity. These results put together shed light
on the amount of information that one needs in order to solve a linear program
efficiently.
  The proofs of the results employ a variety of geometric techniques, including
McMullen's Upper Bound Theorem, the weighted spherical Voronoi diagram, and the
furthest Voronoi diagram. In addition, we give an alternative proof to a
conjecture of L\'aszl\'o Fejes T\'oth on bounding the number of disconnected
components formed by the union of m convex bodies in R^n. Our proof, inspired
by the Gauss-Bonnet Theorem in global differential geometry, is independent of
the known and reveals more clear insights into the problem and the bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1250</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1250</id><created>2013-04-04</created><authors><author><keyname>Shen</keyname><forenames>Fumin</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hill</keyname><forenames>Rhys</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Tang</keyname><forenames>Zhenmin</forenames></author></authors><title>Fast Approximate L_infty Minimization: Speeding Up Robust Regression</title><categories>cs.CV stat.CO</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Minimization of the $L_\infty$ norm, which can be viewed as approximately
solving the non-convex least median estimation problem, is a powerful method
for outlier removal and hence robust regression. However, current techniques
for solving the problem at the heart of $L_\infty$ norm minimization are slow,
and therefore cannot scale to large problems. A new method for the minimization
of the $L_\infty$ norm is presented here, which provides a speedup of multiple
orders of magnitude for data with high dimension. This method, termed Fast
$L_\infty$ Minimization, allows robust regression to be applied to a class of
problems which were previously inaccessible. It is shown how the $L_\infty$
norm minimization problem can be broken up into smaller sub-problems, which can
then be solved extremely efficiently. Experimental results demonstrate the
radical reduction in computation time, along with robustness against large
numbers of outliers in a few model-fitting problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1255</identifier>
 <datestamp>2013-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1255</id><created>2013-04-04</created><updated>2013-08-17</updated><authors><author><keyname>Nourdin</keyname><forenames>Ivan</forenames><affiliation>IECL</affiliation></author><author><keyname>Peccati</keyname><forenames>Giovanni</forenames><affiliation>FSTC</affiliation></author><author><keyname>Swan</keyname><forenames>Yvik</forenames><affiliation>FSTC</affiliation></author></authors><title>Entropy and the fourth moment phenomenon</title><categories>math.PR cs.IT math.IT</categories><comments>32 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new method for bounding the relative entropy of a random vector
in terms of its Stein factors. Our approach is based on a novel representation
for the score function of smoothly perturbed random variables, as well as on
the de Bruijn's identity of information theory. When applied to sequences of
functionals of a general Gaussian field, our results can be combined with the
Carbery-Wright inequality in order to yield multidimensional entropic rates of
convergence that coincide, up to a logarithmic factor, with those achievable in
smooth distances (such as the 1-Wasserstein distance). In particular, our
findings settle the open problem of proving a quantitative version of the
multidimensional fourth moment theorem for random vectors having chaotic
components, with explicit rates of convergence in total variation that are
independent of the order of the associated Wiener chaoses. The results proved
in the present paper are outside the scope of other existing techniques, such
as for instance the multidimensional Stein's method for normal approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1262</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1262</id><created>2013-04-04</created><authors><author><keyname>Wiliem</keyname><forenames>Arnold</forenames></author><author><keyname>Wong</keyname><forenames>Yongkang</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Hobson</keyname><forenames>Peter</forenames></author><author><keyname>Chen</keyname><forenames>Shaokang</forenames></author><author><keyname>Lovell</keyname><forenames>Brian C.</forenames></author></authors><title>Classification of Human Epithelial Type 2 Cell Indirect
  Immunofluoresence Images via Codebook Based Descriptors</title><categories>q-bio.CB cs.CV q-bio.QM</categories><acm-class>I.2.10; I.4.6; I.4.7; I.4.10; I.5.4; G.3</acm-class><journal-ref>IEEE Workshop on Applications of Computer Vision (WACV), pp.
  95-102, 2013</journal-ref><doi>10.1109/WACV.2013.6475005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Anti-Nuclear Antibody (ANA) clinical pathology test is commonly used to
identify the existence of various diseases. A hallmark method for identifying
the presence of ANAs is the Indirect Immunofluorescence method on Human
Epithelial (HEp-2) cells, due to its high sensitivity and the large range of
antigens that can be detected. However, the method suffers from numerous
shortcomings, such as being subjective as well as time and labour intensive.
Computer Aided Diagnostic (CAD) systems have been developed to address these
problems, which automatically classify a HEp-2 cell image into one of its known
patterns (eg., speckled, homogeneous). Most of the existing CAD systems use
handpicked features to represent a HEp-2 cell image, which may only work in
limited scenarios. In this paper, we propose a cell classification system
comprised of a dual-region codebook-based descriptor, combined with the Nearest
Convex Hull Classifier. We evaluate the performance of several variants of the
descriptor on two publicly available datasets: ICPR HEp-2 cell classification
contest dataset and the new SNPHEp-2 dataset. To our knowledge, this is the
first time codebook-based descriptors are applied and studied in this domain.
Experiments show that the proposed system has consistent high performance and
is more robust than two recent CAD systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1267</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1267</id><created>2013-04-04</created><authors><author><keyname>Radicchi</keyname><forenames>Filippo</forenames></author><author><keyname>Castellano</keyname><forenames>Claudio</forenames></author></authors><title>Analysis of bibliometric indicators for individual scholars in a large
  data set</title><categories>physics.soc-ph cs.DL</categories><comments>8 pages, 5 figures. The data set analyzed in this paper can be found
  at http://sitefilrad.homelinux.org:8080/?page_id=281</comments><journal-ref>Scientometrics 97, 627-637 (2013)</journal-ref><doi>10.1007/s11192-013-1027-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Citation numbers and other quantities derived from bibliographic databases
are becoming standard tools for the assessment of productivity and impact of
research activities. Though widely used, still their statistical properties
have not been well established so far. This is especially true in the case of
bibliometric indicators aimed at the evaluation of individual scholars, because
large-scale data sets are typically difficult to be retrieved. Here, we take
advantage of a recently introduced large bibliographic data set, Google Scholar
Citations, which collects the entire publication record of individual scholars.
We analyze the scientific profile of more than 30,000 researchers, and study
the relation between the h-index, the number of publications and the number of
citations of individual scientists. While the number of publications of a
scientist has a rather weak relation with his/her h-index, we find that the
h-index of a scientist is strongly correlated with the number of citations that
she/he has received so that the number of citations can be effectively be used
as a proxy of the h-index. Allowing for the h-index to depend on both the
number of citations and the number of publications, we find only a minor
improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1268</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1268</id><created>2013-04-04</created><authors><author><keyname>Di Fabio</keyname><forenames>Barbara</forenames></author><author><keyname>Frosini</keyname><forenames>Patrizio</forenames></author></authors><title>Filtrations induced by continuous functions</title><categories>math.GN cs.CG</categories><comments>13 pages, 4 figures</comments><msc-class>Primary 54E45, Secondary 65D18, 68U05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Persistent Homology and Topology, filtrations are usually given by
introducing an ordered collection of sets or a continuous function from a
topological space to $\R^n$. A natural question arises, whether these
approaches are equivalent or not. In this paper we study this problem and prove
that, while the answer to the previous question is negative in the general
case, the approach by continuous functions is not restrictive with respect to
the other, provided that some natural stability and completeness assumptions
are made. In particular, we show that every compact and stable 1-dimensional
filtration of a compact metric space is induced by a continuous function.
Moreover, we extend the previous result to the case of multi-dimensional
filtrations, requiring that our filtration is also complete. Three examples
show that we cannot drop the assumptions about stability and completeness.
Consequences of our results on the definition of a distance between filtrations
are finally discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1296</identifier>
 <datestamp>2013-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1296</id><created>2013-04-04</created><updated>2013-09-12</updated><authors><author><keyname>Frank</keyname><forenames>Morgan R.</forenames></author><author><keyname>Mitchell</keyname><forenames>Lewis</forenames></author><author><keyname>Dodds</keyname><forenames>Peter S.</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author></authors><title>Happiness and the Patterns of Life: A Study of Geolocated Tweets</title><categories>physics.soc-ph cs.SI</categories><comments>12 page main document, 12 page supplement, 21 figures</comments><journal-ref>Scientific Reports, Vol 3, No 2625, 2013</journal-ref><doi>10.1038/srep02625</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The patterns of life exhibited by large populations have been described and
modeled both as a basic science exercise and for a range of applied goals such
as reducing automotive congestion, improving disaster response, and even
predicting the location of individuals. However, these studies previously had
limited access to conversation content, rendering changes in expression as a
function of movement invisible. In addition, they typically use the
communication between a mobile phone and its nearest antenna tower to infer
position, limiting the spatial resolution of the data to the geographical
region serviced by each cellphone tower. We use a collection of 37 million
geolocated tweets to characterize the movement patterns of 180,000 individuals,
taking advantage of several orders of magnitude of increased spatial accuracy
relative to previous work. Employing the recently developed sentiment analysis
instrument known as the 'hedonometer', we characterize changes in word usage as
a function of movement, and find that expressed happiness increases
logarithmically with distance from an individual's average location.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1307</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1307</id><created>2013-04-04</created><authors><author><keyname>Plotnikov</keyname><forenames>Anatoly D.</forenames></author></authors><title>On the structure of the class NP</title><categories>cs.CC</categories><comments>7 pages, 7 references</comments><msc-class>68Q15, 03F20</msc-class><journal-ref>On the structure of the class NP. Computer Communication &amp;
  Collaboration (2013) 1: 19-23</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new class UF of problems is introduced, strictly included in the class NP,
which arises in the analysis of the time verifying the intermediate results of
computations. The implications of the introduction of this class are
considered. First of all, we prove that $P\not= NP$ and establish that it needs
to consider the problem &quot;P vs UF&quot; instead the problem &quot;P vs NP&quot;. Also, we
determine the set-theoretical of properties of a one-way functions that used in
cryptology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1309</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1309</id><created>2013-04-04</created><updated>2013-04-05</updated><authors><author><keyname>Salikhmetov</keyname><forenames>Anton</forenames></author></authors><title>Interaction Nets in Russian</title><categories>cs.LO cs.FL</categories><comments>22 pages, in Russian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Draft translation to Russian of Chapter 7, Interaction-Based Models of
Computation, from Models of Computation: An Introduction to Computability
Theory by Maribel Fernandez. &quot;In this chapter, we study interaction nets, a
model of computation that can be seen as a representative of a class of models
based on the notion of 'computation as interaction'. Interaction nets are a
graphical model of computation devised by Yves Lafont in 1990 as a
generalisation of the proof structures of linear logic. It can be seen as an
abstract formalism, used to define algorithms and analyse their cost, or as a
low-level language into which other programming languages can be compiled. This
is fruitful because interaction nets can be implemented with reasonable
efficiency.&quot;
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1318</identifier>
 <datestamp>2014-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1318</id><created>2013-04-04</created><updated>2014-04-30</updated><authors><author><keyname>Xie</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Chen</forenames></author><author><keyname>Zhang</keyname><forenames>Quan</forenames></author><author><keyname>Tang</keyname><forenames>Chaojing</forenames></author></authors><title>RFID Authentication Against an Unsecure Backend Server</title><categories>cs.CR</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper address a new problem in RFID authentication research for the
first time. That is, existing RFID authentication schemes generally assume that
the backend server is absolutely secure, however, this assumption is rarely
tenable in practical conditions. It disables existing RFID authentication
protocols from being safely applied to a reallife scenario in which the backend
server is actually vulnerable, compromised or even malicious itself. We propose
an RFID authentication scheme against an unsecure backend server. It is based
on hash chain, searching over encrypted data, and coprivacy, defending against
the privacy revealing to the backend server. The proposed scheme is scalable,
resistant to desynchronization attacks, and provides mutual authentication in
only three frontend communication steps. Moreover, it is the first scheme
meeting the special security and privacy requirement for a cloud-based RFID
authentication scenario in which the backend server is untrustworthy to readers
held by cloud clients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1329</identifier>
 <datestamp>2013-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1329</id><created>2013-04-04</created><updated>2013-08-06</updated><authors><author><keyname>Georgiou</keyname><forenames>Orestis</forenames></author><author><keyname>Dettmann</keyname><forenames>Carl P.</forenames></author><author><keyname>Coon</keyname><forenames>Justin</forenames></author></authors><title>k-connectivity for confined random networks</title><categories>cond-mat.dis-nn cs.DM math-ph math.MP math.PR</categories><comments>6 pages, 4 figures</comments><journal-ref>Europhys. Lett., 103, 28006, (2013)</journal-ref><doi>10.1209/0295-5075/103/28006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  k-connectivity is an important measure of network robustness and resilience
to random faults and disruptions. We undertake both local and global approaches
to k-connectivity and calculate closed form analytic formulas for the
probability that a confined random network remains fully connected after the
removal of k-1 nodes. Our analysis reveals that k-connectivity is governed by
microscopic details of the network domain such as sharp corners rather than the
macroscopic total volume. Hence, our results can aid in the design of reliable
networks, an important problem in e.g. wireless ad hoc and sensor networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1332</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1332</id><created>2013-04-04</created><authors><author><keyname>Voit</keyname><forenames>Karl</forenames></author></authors><title>What really happened on September 15th 2008? Getting The Most from Your
  Personal Information with Memacs</title><categories>cs.HC cs.IR</categories><comments>6 pages, 3 figures, 21 references</comments><msc-class>68N99</msc-class><acm-class>H.3.3; H.3.2; H.4.1; H.5.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Combining and summarizing meta-data from various kinds of data sources is one
possible solution to the data fragmentation we are suffering from. Multiple
projects have addressed this issue already. This paper presents a new approach
named Memacs. It automatically generates a detailed linked diary of our digital
artifacts scattered across local files of multiple formats as well as data
silos of the internet. Being elegantly simple and open, Memacs uses already
existing visualization features of GNU Emacs and Org-mode to provide a
promising platform for life-logging, Quantified Self movement, and people
looking for advanced Personal Information Management (PIM) in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1346</identifier>
 <datestamp>2013-04-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1346</id><created>2013-04-04</created><updated>2013-04-10</updated><authors><author><keyname>De Laet</keyname><forenames>Tinne</forenames></author><author><keyname>Schaekers</keyname><forenames>Wouter</forenames></author><author><keyname>de Greef</keyname><forenames>Jonas</forenames></author><author><keyname>Bruyninckx</keyname><forenames>Herman</forenames></author></authors><title>Domain Specific Language for Geometric Relations between Rigid Bodies
  targeted to robotic applications</title><categories>cs.RO</categories><comments>Presented at DSLRob 2012 (arXiv:cs/1302.5082)</comments><report-no>DSLRob/2012/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a DSL for geometric relations between rigid bodies such
as relative position, orientation, pose, linear velocity, angular velocity, and
twist. The DSL is the formal model of the recently proposed semantics for the
standardization of geometric relations between rigid bodies, referred to as
`geometric semantics'. This semantics explicitly states the
coordinate-invariant properties and operations, and, more importantly, all the
choices that are made in coordinate representations of these geometric
relations. This results in a set of concrete suggestions for standardizing
terminology and notation, allowing programmers to write fully unambiguous
software interfaces, including automatic checks for semantic correctness of all
geometric operations on rigid-body coordinate representations.
  The DSL is implemented in two different ways: an external DSL in Xcore and an
internal DSL in Prolog. Besides defining a grammar and operations, the DSL also
implements constraints. In the Xcore model, the Object Constraint Language
language is used, while in the Prolog model, the constraint are natively
modelled in Prolog.
  This paper discusses the implemented DSL and the tools developed on top of
this DSL. In particular an editor, checking the semantic constraints and
providing semantic meaningful errors during editing is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1347</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1347</id><created>2013-04-04</created><authors><author><keyname>O'Donnell</keyname><forenames>Ryan</forenames></author><author><keyname>Tan</keyname><forenames>Li-Yang</forenames></author></authors><title>A composition theorem for the Fourier Entropy-Influence conjecture</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Fourier Entropy-Influence (FEI) conjecture of Friedgut and Kalai [FK96]
seeks to relate two fundamental measures of Boolean function complexity: it
states that $H[f] \leq C Inf[f]$ holds for every Boolean function $f$, where
$H[f]$ denotes the spectral entropy of $f$, $Inf[f]$ is its total influence,
and $C &gt; 0$ is a universal constant. Despite significant interest in the
conjecture it has only been shown to hold for a few classes of Boolean
functions.
  Our main result is a composition theorem for the FEI conjecture. We show that
if $g_1,...,g_k$ are functions over disjoint sets of variables satisfying the
conjecture, and if the Fourier transform of $F$ taken with respect to the
product distribution with biases $E[g_1],...,E[g_k]$ satisfies the conjecture,
then their composition $F(g_1(x^1),...,g_k(x^k))$ satisfies the conjecture. As
an application we show that the FEI conjecture holds for read-once formulas
over arbitrary gates of bounded arity, extending a recent result [OWZ11] which
proved it for read-once decision trees. Our techniques also yield an explicit
function with the largest known ratio of $C \geq 6.278$ between $H[f]$ and
$Inf[f]$, improving on the previous lower bound of 4.615.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1351</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1351</id><created>2013-04-04</created><authors><author><keyname>Gatti</keyname><forenames>Nicola</forenames></author><author><keyname>Rocco</keyname><forenames>Marco</forenames></author><author><keyname>Sandholm</keyname><forenames>Tuomas</forenames></author></authors><title>On the complexity of strong Nash equilibrium: Hard-to-solve instances
  and smoothed complexity</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computational characterization of game-theoretic solution concepts is a
central topic in artificial intelligence, with the aim of developing
computationally efficient tools for finding optimal ways to behave in strategic
interactions. The central solution concept in game theory is Nash equilibrium
(NE). However, it fails to capture the possibility that agents can form
coalitions (even in the 2-agent case). Strong Nash equilibrium (SNE) refines NE
to this setting. It is known that finding an SNE is NP-complete when the number
of agents is constant. This hardness is solely due to the existence of
mixed-strategy SNEs, given that the problem of enumerating all pure-strategy
SNEs is trivially in P. Our central result is that, in order for a game to have
at least one non-pure-strategy SNE, the agents' payoffs restricted to the
agents' supports must, in the case of 2 agents, lie on the same line, and, in
the case of n agents, lie on an (n - 1)-dimensional hyperplane. Leveraging this
result, we provide two contributions. First, we develop worst-case instances
for support-enumeration algorithms. These instances have only one SNE and the
support size can be chosen to be of any size-in particular, arbitrarily large.
Second, we prove that, unlike NE, finding an SNE is in smoothed polynomial
time: generic game instances (i.e., all instances except knife-edge cases) have
only pure-strategy SNEs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1356</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1356</id><created>2013-04-04</created><authors><author><keyname>Mann</keyname><forenames>Martin</forenames></author><author><keyname>Ekker</keyname><forenames>Heinz</forenames></author><author><keyname>Flamm</keyname><forenames>Christoph</forenames></author></authors><title>The Graph Grammar Library - a generic framework for chemical graph
  rewrite systems</title><categories>cs.MS cs.CE q-bio.BM q-bio.MN</categories><comments>Extended version of an abstract published in proceedings of the
  International Conference on Model Transformation (ICMT) 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph rewrite systems are powerful tools to model and study complex problems
in various fields of research. Their successful application to chemical
reaction modelling on a molecular level was shown but no appropriate and simple
system is available at the moment.
  The presented Graph Grammar Library (GGL) implements a generic Double Push
Out approach for general graph rewrite systems. The framework focuses on a high
level of modularity as well as high performance, using state-of-the-art
algorithms and data structures, and comes with extensive documentation. The
large GGL chemistry module enables extensive and detailed studies of chemical
systems. It well meets the requirements and abilities envisioned by Yadav et
al. (2004) for such chemical rewrite systems. Here, molecules are represented
as undirected labeled graphs while chemical reactions are described by
according graph grammar rules. Beside the graph transformation, the GGL offers
advanced cheminformatics algorithms for instance to estimate energies
ofmolecules or aromaticity perception. These features are illustrated using a
set of reactions from polyketide chemistry a huge class of natural compounds of
medical relevance.
  The graph grammar based simulation of chemical reactions offered by the GGL
is a powerful tool for extensive cheminformatics studies on a molecular level.
The GGL already provides rewrite rules for all enzymes listed in the KEGG
LIGAND database is freely available at
http://www.tbi.univie.ac.at/software/GGL/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1359</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1359</id><created>2013-04-04</created><updated>2013-11-12</updated><authors><author><keyname>Guo</keyname><forenames>Zeyu</forenames></author><author><keyname>Sun</keyname><forenames>He</forenames></author></authors><title>Randomness-Efficient Rumor Spreading</title><categories>cs.DS cs.SI</categories><comments>This paper has been withdrawn by the author since a more general
  result is recently posted</comments><acm-class>F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the classical rumor spreading problem, which is used to spread
information in an unknown network with $n$ nodes. We present the first protocol
for any expander graph $G$ with $n$ nodes and minimum degree $\Theta(n)$ such
that, the protocol informs every node in $O(\log n)$ rounds with high
probability, and uses $O(\log n\log\log n)$ random bits in total. The runtime
of our protocol is tight, and the randomness requirement of $O(\log n\log\log
n)$ random bits almost matches the lower bound of $\Omega(\log n)$ random bits.
We further study rumor spreading protocols for more general graphs, and for
several graph topologies our protocols are as fast as the classical protocol
and use $\tilde{O}(\log n)$ random bits in total, in contrast to $O(n\log^2n)$
random bits used in the well-known rumor spreading push protocol. These results
together give us almost full understanding of the randomness requirement for
this basic epidemic process.
  Our protocols rely on a novel reduction between rumor spreading processes and
branching programs, and this reduction provides a general framework to
derandomize these complex and distributed epidemic processes. Interestingly,
one cannot simply apply PRGs for branching programs as rumor spreading process
is not characterized by small-space computation. Our protocols require the
composition of several pseudorandom objects, e.g. pseudorandom generators, and
pairwise independent generators. Besides designing rumor spreading protocols,
the techniques developed here may have applications in studying the randomness
complexity of distributed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1374</identifier>
 <datestamp>2013-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1374</id><created>2013-04-04</created><updated>2013-07-01</updated><authors><author><keyname>Bouche</keyname><forenames>Thierry</forenames><affiliation>IF, CCDNM</affiliation></author><author><keyname>R&#xe1;kosnik</keyname><forenames>Jiri</forenames></author></authors><title>Report on the EuDML external cooperation model</title><categories>cs.DL cs.CY</categories><comments>Small edits for publication. Cost section added</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main tasks of the European Digital Mathematics Library project was
to define a cooperation model with a variety of stakeholders that would allow
building a reliable and durable global reference library, aiming to be
eventually exhaustive. In this paper we present the EuDML external cooperation
model and the business plan as the basis for its sustainability and further
development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1386</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1386</id><created>2013-04-04</created><authors><author><keyname>Halanay</keyname><forenames>Andrei</forenames></author><author><keyname>Pandolfi</keyname><forenames>Luciano</forenames></author></authors><title>Lack of controllability of thermal systems with memory</title><categories>cs.SY math.OC</categories><msc-class>35Q93, 45K05, 93B03</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heat equations with memory of Gurtin-Pipkin type have controllability
properties which strongly resemble those of the wave equation. Instead, recent
counterexamples show that when the laplacian appears also out of the memory
term, the control properties do not parallel those of the (memoryless) heat
equation, in the sense that there are $L^2$-initial conditions which cannot be
controlled to zero. The proof of this fact (presented in previous papers)
consists in the construction of two quite special examples of systems with
memory which cannot be controlled to zero. Here we prove that lack of
controllability holds in general, for every systems with smooth memory kernel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1391</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1391</id><created>2013-04-04</created><authors><author><keyname>Nandan</keyname><forenames>Manu</forenames></author><author><keyname>Khargonekar</keyname><forenames>Pramod P.</forenames></author><author><keyname>Talathi</keyname><forenames>Sachin S.</forenames></author></authors><title>Fast SVM training using approximate extreme points</title><categories>cs.LG</categories><comments>The manuscript in revised form has been submitted to J. Machine
  Learning Research</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications of non-linear kernel Support Vector Machines (SVMs) to large
datasets is seriously hampered by its excessive training time. We propose a
modification, called the approximate extreme points support vector machine
(AESVM), that is aimed at overcoming this burden. Our approach relies on
conducting the SVM optimization over a carefully selected subset, called the
representative set, of the training dataset. We present analytical results that
indicate the similarity of AESVM and SVM solutions. A linear time algorithm
based on convex hulls and extreme points is used to compute the representative
set in kernel space. Extensive computational experiments on nine datasets
compared AESVM to LIBSVM \citep{LIBSVM}, CVM \citep{Tsang05}, BVM
\citep{Tsang07}, LASVM \citep{Bordes05}, $\text{SVM}^{\text{perf}}$
\citep{Joachims09}, and the random features method \citep{rahimi07}. Our AESVM
implementation was found to train much faster than the other methods, while its
classification accuracy was similar to that of LIBSVM in all cases. In
particular, for a seizure detection dataset, AESVM training was almost $10^3$
times faster than LIBSVM and LASVM and more than forty times faster than CVM
and BVM. Additionally, AESVM also gave competitively fast classification times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1402</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1402</id><created>2013-04-04</created><updated>2013-04-08</updated><authors><author><keyname>Grau</keyname><forenames>Bernardo Cuenca</forenames></author><author><keyname>Motik</keyname><forenames>Boris</forenames></author><author><keyname>Stoilos</keyname><forenames>Giorgos</forenames></author><author><keyname>Horrocks</keyname><forenames>Ian</forenames></author></authors><title>Computing Datalog Rewritings beyond Horn Ontologies</title><categories>cs.AI</categories><comments>14 pages. To appear at IJCAI 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rewriting-based approaches for answering queries over an OWL 2 DL ontology
have so far been developed mainly for Horn fragments of OWL 2 DL. In this
paper, we study the possibilities of answering queries over non-Horn ontologies
using datalog rewritings. We prove that this is impossible in general even for
very simple ontology languages, and even if PTIME = NP. Furthermore, we present
a resolution-based procedure for $\SHI$ ontologies that, in case it terminates,
produces a datalog rewriting of the ontology. Our procedure necessarily
terminates on DL-Lite_{bool}^H ontologies---an extension of OWL 2 QL with
transitive roles and Boolean connectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1405</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1405</id><created>2013-04-04</created><authors><author><keyname>Fan</keyname><forenames>Yun</forenames></author><author><keyname>Ling</keyname><forenames>San</forenames></author><author><keyname>Liu</keyname><forenames>Hongwei</forenames></author></authors><title>Homogeneous Weights of Matrix Product Codes over Finite Principal Ideal
  Rings</title><categories>cs.IT math.IT</categories><msc-class>96B05, 94B60, 15B33</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the homogeneous weights of matrix product codes over finite
principal ideal rings are studied and a lower bound for the minimum homogeneous
weights of such matrix product codes is obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1408</identifier>
 <datestamp>2014-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1408</id><created>2013-04-04</created><authors><author><keyname>Yan</keyname><forenames>Ming</forenames></author></authors><title>Restoration of Images Corrupted by Impulse Noise and Mixed Gaussian
  Impulse Noise using Blind Inpainting</title><categories>math.OC cs.CV math.NA</categories><comments>18 pages, 4 figures</comments><journal-ref>SIAM J. Imaging Sci., 6(2013), 1227-1245</journal-ref><doi>10.1137/12087178X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article studies the problem of image restoration of observed images
corrupted by impulse noise and mixed Gaussian impulse noise. Since the pixels
damaged by impulse noise contain no information about the true image, how to
find this set correctly is a very important problem. We propose two methods
based on blind inpainting and $\ell_0$ minimization that can simultaneously
find the damaged pixels and restore the image. By iteratively restoring the
image and updating the set of damaged pixels, these methods have better
performance than other methods, as shown in the experiments. In addition, we
provide convergence analysis for these methods, these algorithms will converge
to coordinatewise minimum points. In addition, they will converge to local
minimum points (or with probability one) with some modifications in the
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1411</identifier>
 <datestamp>2013-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1411</id><created>2013-04-04</created><updated>2013-07-19</updated><authors><author><keyname>Tran</keyname><forenames>Quoc Trung</forenames></author><author><keyname>Jimenez</keyname><forenames>Ivo</forenames></author><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Polyzotis</keyname><forenames>Neoklis</forenames></author><author><keyname>Ailamaki</keyname><forenames>Anastasia</forenames></author></authors><title>RITA: An Index-Tuning Advisor for Replicated Databases</title><categories>cs.DB</categories><comments>15 pages, 11 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Given a replicated database, a divergent design tunes the indexes in each
replica differently in order to specialize it for a specific subset of the
workload. This specialization brings significant performance gains compared to
the common practice of having the same indexes in all replicas, but requires
the development of new tuning tools for database administrators. In this paper
we introduce RITA (Replication-aware Index Tuning Advisor), a novel
divergent-tuning advisor that offers several essential features not found in
existing tools: it generates robust divergent designs that allow the system to
adapt gracefully to replica failures; it computes designs that spread the load
evenly among specialized replicas, both during normal operation and when
replicas fail; it monitors the workload online in order to detect changes that
require a recomputation of the divergent design; and, it offers suggestions to
elastically reconfigure the system (by adding/removing replicas or
adding/dropping indexes) to respond to workload changes. The key technical
innovation behind RITA is showing that the problem of selecting an optimal
design can be formulated as a Binary Integer Program (BIP). The BIP has a
relatively small number of variables, which makes it feasible to solve it
efficiently using any off-the-shelf linear-optimization software. Experimental
results demonstrate that RITA computes better divergent designs compared to
existing tools, offers more features, and has fast execution times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1419</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1419</id><created>2013-04-04</created><authors><author><keyname>Avanaki</keyname><forenames>Ali N.</forenames></author><author><keyname>Espig</keyname><forenames>Kathryn S.</forenames></author><author><keyname>Marchessoux</keyname><forenames>Cedric</forenames></author><author><keyname>Krupinski</keyname><forenames>Elizabeth A.</forenames></author><author><keyname>Bakic</keyname><forenames>Predrag R.</forenames></author><author><keyname>Kimpe</keyname><forenames>Tom R. L.</forenames></author><author><keyname>Maidment</keyname><forenames>Andrew D. A.</forenames></author></authors><title>Integration of spatio-temporal contrast sensitivity with a multi-slice
  channelized Hotelling observer</title><categories>cs.CV</categories><doi>10.1117/12.2001334</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Barten's model of spatio-temporal contrast sensitivity function of human
visual system is embedded in a multi-slice channelized Hotelling observer. This
is done by 3D filtering of the stack of images with the spatio-temporal
contrast sensitivity function and feeding the result (i.e., the perceived image
stack) to the multi-slice channelized Hotelling observer. The proposed
procedure of considering spatio-temporal contrast sensitivity function is
generic in the sense that it can be used with observers other than multi-slice
channelized Hotelling observer. Detection performance of the new observer in
digital breast tomosynthesis is measured in a variety of browsing speeds, at
two spatial sampling rates, using computer simulations. Our results show a peak
in detection performance in mid browsing speeds. We compare our results to
those of a human observer study reported earlier (I. Diaz et al. SPIE MI 2011).
The effects of display luminance, contrast and spatial sampling rate, with and
without considering foveal vision, are also studied. Reported simulations are
conducted with real digital breast tomosynthesis image stacks, as well as
stacks from an anthropomorphic software breast phantom (P. Bakic et al. Med
Phys. 2011). Lesion cases are simulated by inserting single
micro-calcifications or masses. Limitations of our methods and ways to improve
them are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1423</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1423</id><created>2013-04-04</created><updated>2014-11-26</updated><authors><author><keyname>Ara&#xfa;jo</keyname><forenames>Felipe F. B.</forenames></author><author><keyname>Costa</keyname><forenames>Alysson M.</forenames></author><author><keyname>Miralles</keyname><forenames>Crist&#xf3;bal</forenames></author></authors><title>Balancing parallel assembly lines with disabled workers</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an assembly line balancing problem that occurs in sheltered worker
centers for the disabled, where workers with very different characteristics are
present. We are interested in the situation in which parallel assembly lines
are allowed and name the resulting problem as parallel assembly line worker
assignment and balancing problem. We present a linear mixed-integer formulation
and a four-stage heuristic algorithm. Computational results with a large set of
instances recently proposed in the literature show the advantages of allowing
alternative line layouts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1424</identifier>
 <datestamp>2013-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1424</id><created>2013-04-04</created><updated>2013-08-20</updated><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author></authors><title>Improved approximation for 3-dimensional matching via bounded pathwidth
  local search</title><categories>cs.DS</categories><comments>To appear in proceedings of FOCS 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most natural optimization problems is the k-Set Packing problem,
where given a family of sets of size at most k one should select a maximum size
subfamily of pairwise disjoint sets. A special case of 3-Set Packing is the
well known 3-Dimensional Matching problem. Both problems belong to the Karp`s
list of 21 NP-complete problems. The best known polynomial time approximation
ratio for k-Set Packing is (k + eps)/2 and goes back to the work of Hurkens and
Schrijver [SIDMA`89], which gives (1.5 + eps)-approximation for 3-Dimensional
Matching. Those results are obtained by a simple local search algorithm, that
uses constant size swaps.
  The main result of the paper is a new approach to local search for k-Set
Packing where only a special type of swaps is considered, which we call swaps
of bounded pathwidth. We show that for a fixed value of k one can search the
space of r-size swaps of constant pathwidth in c^r poly(|F|) time. Moreover we
present an analysis proving that a local search maximum with respect to O(log
|F|)-size swaps of constant pathwidth yields a polynomial time (k + 1 +
eps)/3-approximation algorithm, improving the best known approximation ratio
for k-Set Packing. In particular we improve the approximation ratio for
3-Dimensional Matching from 3/2 + eps to 4/3 + eps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1428</identifier>
 <datestamp>2013-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1428</id><created>2013-04-04</created><updated>2013-05-24</updated><authors><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>Information and Computation</title><categories>cs.IT cs.HC cs.SI math.IT nlin.AO</categories><comments>9 pages, 3 figures. Draft of a chapter to be published in Michelucci,
  P. (Ed.) Handbook of Human Computation, Springer</comments><acm-class>H.1.2; H.1.1; F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this chapter, concepts related to information and computation are reviewed
in the context of human computation. A brief introduction to information theory
and different types of computation is given. Two examples of human computation
systems, online social networks and Wikipedia, are used to illustrate how these
can be described and compared in terms of information and computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1432</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1432</id><created>2013-04-04</created><authors><author><keyname>Ganesan</keyname><forenames>Abhinav</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Interference Alignment with Diversity for the $2 \times 2$ $X$ Network
  with four antennas</title><categories>cs.IT math.IT</categories><comments>16 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A transmission scheme based on the Alamouti code, which we call the
Li-Jafarkhani-Jafar (LJJ) scheme, was recently proposed for the $2 \times 2$
$X$ Network (i.e., two-transmitter (Tx) two-receiver (Rx) $X$ Network) with two
antennas at each node. This scheme was claimed to achieve a sum degrees of
freedom (DoF) of $\frac{8}{3}$ and also a diversity gain of two when fixed
finite constellations are employed at each Tx. Furthermore, each Tx required
the knowledge of only its own channel unlike the Jafar-Shamai scheme which
required global CSIT to achieve the maximum possible sum DoF of $\frac{8}{3}$.
In this paper, we extend the LJJ scheme to the $2 \times 2$ $X$ Network with
four antennas at each node. The proposed scheme also assumes only local channel
knowledge at each Tx. We prove that the proposed scheme achieves the maximum
possible sum DoF of $\frac{16}{3}$. In addition, we also prove that, using any
fixed finite constellation with appropriate rotation at each Tx, the proposed
scheme achieves a diversity gain of at least four.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1449</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1449</id><created>2013-04-04</created><updated>2015-08-20</updated><authors><author><keyname>Kamma</keyname><forenames>Lior</forenames></author><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author><author><keyname>Nguyen</keyname><forenames>Huy L.</forenames></author></authors><title>Cutting corners cheaply, or how to remove Steiner points</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our main result is that the Steiner Point Removal (SPR) problem can always be
solved with polylogarithmic distortion, which answers in the affirmative a
question posed by Chan, Xia, Konjevod, and Richa (2006). Specifically, we prove
that for every edge-weighted graph $G = (V,E,w)$ and a subset of terminals $T
\subseteq V$, there is a graph $G'=(T,E',w')$ that is isomorphic to a minor of
$G$, such that for every two terminals $u,v\in T$, the shortest-path distances
between them in $G$ and in $G'$ satisfy $d_{G,w}(u,v) \le d_{G',w'}(u,v) \le
O(\log^5|T|) \cdot d_{G,w}(u,v)$. Our existence proof actually gives a
randomized polynomial-time algorithm. Our proof features a new variant of
metric decomposition. It is well-known that every $n$-point metric space
$(X,d)$ admits a $\beta$-separating decomposition for $\beta=O(\log n)$, which
roughly means for every desired diameter bound $\Delta&gt;0$ there is a randomized
partitioning of $X$, which satisfies the following separation requirement: for
every $x,y \in X$, the probability they lie in different clusters of the
partition is at most $\beta\,d(x,y)/\Delta$. We introduce an additional
requirement, which is the following tail bound: for every shortest-path $P$ of
length $d(P) \leq \Delta/\beta$, the number of clusters of the partition that
meet the path $P$, denoted $Z_P$, satisfies $\Pr[Z_P &gt; t] \le 2e^{-\Omega(t)}$
for all $t&gt;0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1456</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1456</id><created>2013-04-04</created><authors><author><keyname>Gatti</keyname><forenames>Nicola</forenames></author><author><keyname>Panozzo</keyname><forenames>Fabio</forenames></author><author><keyname>Restelli</keyname><forenames>Marcello</forenames></author></authors><title>Efficient evolutionary dynamics with extensive-form games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary game theory combines game theory and dynamical systems and is
customarily adopted to describe evolutionary dynamics in multi-agent systems.
In particular, it has been proven to be a successful tool to describe
multi-agent learning dynamics. To the best of our knowledge, we provide in this
paper the first replicator dynamics applicable to the sequence form of an
extensive-form game, allowing an exponential reduction of time and space w.r.t.
the currently adopted replicator dynamics for normal form. Furthermore, our
replicator dynamics is realization equivalent to the standard replicator
dynamics for normal form. We prove our results for both discrete-time and
continuous-time cases. Finally, we extend standard tools to study the stability
of a strategy profile to our replicator dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1458</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1458</id><created>2013-04-04</created><authors><author><keyname>Braverman</keyname><forenames>Vladimir</forenames></author><author><keyname>Ostrovsky</keyname><forenames>Rafail</forenames></author><author><keyname>Vilenchik</keyname><forenames>Dan</forenames></author></authors><title>How Hard is Counting Triangles in the Streaming Model</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of (approximately) counting the number of triangles in a graph is
one of the basic problems in graph theory. In this paper we study the problem
in the streaming model. We study the amount of memory required by a randomized
algorithm to solve this problem. In case the algorithm is allowed one pass over
the stream, we present a best possible lower bound of $\Omega(m)$ for graphs
$G$ with $m$ edges on $n$ vertices. If a constant number of passes is allowed,
we show a lower bound of $\Omega(m/T)$, $T$ the number of triangles. We match,
in some sense, this lower bound with a 2-pass $O(m/T^{1/3})$-memory algorithm
that solves the problem of distinguishing graphs with no triangles from graphs
with at least $T$ triangles. We present a new graph parameter $\rho(G)$ -- the
triangle density, and conjecture that the space complexity of the triangles
problem is $\Omega(m/\rho(G))$. We match this by a second algorithm that solves
the distinguishing problem using $O(m/\rho(G))$-memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1459</identifier>
 <datestamp>2013-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1459</id><created>2013-04-04</created><authors><author><keyname>Shah</keyname><forenames>Tariq</forenames></author><author><keyname>Hussain</keyname><forenames>Sayed Azmat</forenames></author><author><keyname>de Andrade</keyname><forenames>Antonio Aparecido</forenames></author></authors><title>Bandwidth reduction in cognitive radio</title><categories>cs.IT math.IT</categories><comments>10 pages</comments><msc-class>11T71, 68P30, 94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to mushroom development of wireless devices cognitive radio is used to
resolve the bandwidth utilization and sacristy problem. The crafty usage of
bandwidth in cognitive radio based on error correcting codes is ensured to
accomodate un authorized user. This study proposes a transmission model by
which a finite sequence of binary cyclic codes constructed by a binary BCH code
of length $n=2^{s}-1$, in which all codes have same error correction capability
and code rate but sequentially increasing code lengths greater than $n$.
Initially all these codes are carrying data of their corresponding primary
users. A transmission pattern is planned in the sprit of interweave model deals
the transmission parameters; modulation scheme, bandwidth and code rate.
Whenever, any of the primary users having mod of transmission, the binary
cyclic code, is not using its allocated bandwidth, the user having its data
built by binary BCH code enter and exploit the free path as a secondary user.
Eventually whenever the primary user with $W$ bandwidth having binary BCH code
for its data transmission, change its status as a secondary user, it just
requires the bandwidth less than $W$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1467</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1467</id><created>2013-04-04</created><updated>2014-10-22</updated><authors><author><keyname>Zadeh</keyname><forenames>Reza Bosagh</forenames></author><author><keyname>Carlsson</keyname><forenames>Gunnar</forenames></author></authors><title>Dimension Independent Matrix Square using MapReduce</title><categories>cs.DS cs.DC math.SP</categories><comments>arXiv admin note: text overlap with arXiv:1206.2082</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compute the singular values of an $m \times n$ sparse matrix $A$ in a
distributed setting, without communication dependence on $m$, which is useful
for very large $m$. In particular, we give a simple nonadaptive sampling scheme
where the singular values of $A$ are estimated within relative error with
constant probability. Our proven bounds focus on the MapReduce framework, which
has become the de facto tool for handling such large matrices that cannot be
stored or even streamed through a single machine.
  On the way, we give a general method to compute $A^TA$. We preserve singular
values of $A^TA$ with $\epsilon$ relative error with shuffle size
$O(n^2/\epsilon^2)$ and reduce-key complexity $O(n/\epsilon^2)$. We further
show that if only specific entries of $A^TA$ are required and $A$ has
nonnegative entries, then we can reduce the shuffle size to $O(n \log(n) / s)$
and reduce-key complexity to $O(\log(n)/s)$, where $s$ is the minimum cosine
similarity for the entries being estimated. All of our bounds are independent
of $m$, the larger dimension. We provide open-source implementations in Spark
and Scalding, along with experiments in an industrial setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1491</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1491</id><created>2013-03-27</created><authors><author><keyname>Bacchus</keyname><forenames>Fahiem</forenames></author></authors><title>Lp : A Logic for Statistical Information</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-1-6</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This extended abstract presents a logic, called Lp, that is capable of
representing and reasoning with a wide variety of both qualitative and
quantitative statistical information. The advantage of this logical formalism
is that it offers a declarative representation of statistical knowledge;
knowledge represented in this manner can be used for a variety of reasoning
tasks. The logic differs from previous work in probability logics in that it
uses a probability distribution over the domain of discourse, whereas most
previous work (e.g., Nilsson [2], Scott et al. [3], Gaifinan [4], Fagin et al.
[5]) has investigated the attachment of probabilities to the sentences of the
logic (also, see Halpern [6] and Bacchus [7] for further discussion of the
differences). The logic Lp possesses some further important features. First, Lp
is a superset of first order logic, hence it can represent ordinary logical
assertions. This means that Lp provides a mechanism for integrating statistical
information and reasoning about uncertainty into systems based solely on logic.
Second, Lp possesses transparent semantics, based on sets and probabilities of
those sets. Hence, knowledge represented in Lp can be understood in terms of
the simple primative concepts of sets and probabilities. And finally, the there
is a sound proof theory that has wide coverage (the proof theory is complete
for certain classes of models). The proof theory captures a sufficient range of
valid inferences to subsume most previous probabilistic uncertainty reasoning
systems. For example, the linear constraints like those generated by Nilsson's
probabilistic entailment [2] can be generated by the proof theory, and the
Bayesian inference underlying belief nets [8] can be performed. In addition,
the proof theory integrates quantitative and qualitative reasoning as well as
statistical and logical reasoning. In the next section we briefly examine
previous work in probability logics, comparing it to Lp. Then we present some
of the varieties of statistical information that Lp is capable of expressing.
After this we present, briefly, the syntax, semantics, and proof theory of the
logic. We conclude with a few examples of knowledge representation and
reasoning in Lp, pointing out the advantages of the declarative representation
offered by Lp. We close with a brief discussion of probabilities as degrees of
belief, indicating how such probabilities can be generated from statistical
knowledge encoded in Lp. The reader who is interested in a more complete
treatment should consult Bacchus [7].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1492</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1492</id><created>2013-03-27</created><authors><author><keyname>Basye</keyname><forenames>Kenneth</forenames></author><author><keyname>Dean</keyname><forenames>Thomas L.</forenames></author></authors><title>Map Learning with Indistinguishable Locations</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-7-13</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nearly all spatial reasoning problems involve uncertainty of one sort or
another. Uncertainty arises due to the inaccuracies of sensors used in
measuring distances and angles. We refer to this as directional uncertainty.
Uncertainty also arises in combining spatial information when one location is
mistakenly identified with another. We refer to this as recognition
uncertainty. Most problems in constructing spatial representations (maps) for
the purpose of navigation involve both directional and recognition uncertainty.
In this paper, we show that a particular class of spatial reasoning problems
involving the construction of representations of large-scale space can be
solved efficiently even in the presence of directional and recognition
uncertainty. We pay particular attention to the problems that arise due to
recognition uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1493</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1493</id><created>2013-03-27</created><authors><author><keyname>Berzuini</keyname><forenames>Carlo</forenames></author><author><keyname>Bellazzi</keyname><forenames>Riccardo</forenames></author><author><keyname>Quaglini</keyname><forenames>Silvana</forenames></author></authors><title>Temporal Reasoning with Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-14-21</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore representations of temporal knowledge based upon the
formalism of Causal Probabilistic Networks (CPNs). Two different
?continuous-time? representations are proposed. In the first, the CPN includes
variables representing ?event-occurrence times?, possibly on different time
scales, and variables representing the ?state? of the system at these times. In
the second, the CPN describes the influences between random variables with
values in () representing dates, i.e. time-points associated with the
occurrence of relevant events. However, structuring a system of inter-related
dates as a network where all links commit to a single specific notion of cause
and effect is in general far from trivial and leads to severe difficulties. We
claim that we should recognize explicitly different kinds of relation between
dates, such as ?cause?, ?inhibition?, ?competition?, etc., and propose a method
whereby these relations are coherently embedded in a CPN using additional
auxiliary nodes corresponding to &quot;instrumental&quot; variables. Also discussed,
though not covered in detail, is the topic concerning how the quantitative
specifications to be inserted in a temporal CPN can be learned from specific
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1494</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1494</id><created>2013-03-27</created><authors><author><keyname>Bonissone</keyname><forenames>Piero P.</forenames></author></authors><title>Now that I Have a Good Theory of Uncertainty, What Else Do I Need?</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-22-33</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rather than discussing the isolated merits of a nominative theory of
uncertainty, this paper focuses on a class of problems, referred to as Dynamic
Classification Problem (DCP), which requires the integration of many theories,
including a prescriptive theory of uncertainty. We start by analyzing the
Dynamic Classification Problem and by defining its induced requirements on a
supporting (plausible) reasoning system. We provide a summary of the underlying
theory (based on the semantics of many-valed logics) and illustrate the
constraints imposed upon it to ensure the modularity and computational
performance required by the applications. We describe the technologies used for
knowledge engineering (such as object-based simulator to exercise requirements,
and development tools to build the Knowledge Base and functionally validate
it). We emphasize the difference between development environment and run-time
system, describe the rule cross-compiler, and the real-time inference engine
with meta-reasoning capabilities. Finally, we illustrate how our proposed
technology satisfies the pop's requirements and analyze some of the lessons
reamed from its applications to situation assessment problems for Pilot's
Associate and Submarine Commander Associate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1495</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1495</id><created>2013-03-27</created><authors><author><keyname>Bonissone</keyname><forenames>Piero P.</forenames></author><author><keyname>Cyrluk</keyname><forenames>David A.</forenames></author><author><keyname>Goodwin</keyname><forenames>James W.</forenames></author><author><keyname>Stillman</keyname><forenames>Jonathan</forenames></author></authors><title>Uncertainty and Incompleteness</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-34-45</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two major difficulties in using default logics are their intractability and
the problem of selecting among multiple extensions. We propose an approach to
these problems based on integrating nommonotonic reasoning with plausible
reasoning based on triangular norms. A previously proposed system for reasoning
with uncertainty (RUM) performs uncertain monotonic inferences on an acyclic
graph. We have extended RUM to allow nommonotonic inferences and cycles within
nonmonotonic rules. By restricting the size and complexity of the nommonotonic
cycles we can still perform efficient inferences. Uncertainty measures provide
a basis for deciding among multiple defaults. Different algorithms and
heuristics for finding the optimal defaults are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1496</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1496</id><created>2013-03-27</created><authors><author><keyname>Booker</keyname><forenames>Lashon B.</forenames></author><author><keyname>Hota</keyname><forenames>Naveen</forenames></author><author><keyname>Ramsey</keyname><forenames>Connie Loggia</forenames></author></authors><title>BaRT: A Bayesian Reasoning Tool for Knowledge Based Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-46-53</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the technology for building knowledge based systems has matured, important
lessons have been learned about the relationship between the architecture of a
system and the nature of the problems it is intended to solve. We are
implementing a knowledge engineering tool called BART that is designed with
these lessons in mind. BART is a Bayesian reasoning tool that makes belief
networks and other probabilistic techniques available to knowledge engineers
building classificatory problem solvers. BART has already been used to develop
a decision aid for classifying ship images, and it is currently being used to
manage uncertainty in systems concerned with analyzing intelligence reports.
This paper discusses how state-of-the-art probabilistic methods fit naturally
into a knowledge based approach to classificatory problem solving, and
describes the current capabilities of BART.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1497</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1497</id><created>2013-03-27</created><authors><author><keyname>Charniak</keyname><forenames>Eugene</forenames></author><author><keyname>Goldman</keyname><forenames>Robert P.</forenames></author></authors><title>Plan Recognition in Stories and in Life</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-54-59</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Plan recognition does not work the same way in stories and in &quot;real life&quot;
(people tend to jump to conclusions more in stories). We present a theory of
this, for the particular case of how objects in stories (or in life) influence
plan recognition decisions. We provide a Bayesian network formalization of a
simple first-order theory of plans, and show how a particular network parameter
seems to govern the difference between &quot;life-like&quot; and &quot;story-like&quot; response.
We then show why this parameter would be influenced (in the desired way) by a
model of speaker (or author) topic selection which assumes that facts in
stories are typically &quot;relevant&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1498</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1498</id><created>2013-03-27</created><authors><author><keyname>Chavez</keyname><forenames>R. Martin</forenames></author><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author></authors><title>An Empirical Evaluation of a Randomized Algorithm for Probabilistic
  Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-60-70</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, researchers in decision analysis and artificial intelligence
(Al) have used Bayesian belief networks to build models of expert opinion.
Using standard methods drawn from the theory of computational complexity,
workers in the field have shown that the problem of probabilistic inference in
belief networks is difficult and almost certainly intractable. K N ET, a
software environment for constructing knowledge-based systems within the
axiomatic framework of decision theory, contains a randomized approximation
scheme for probabilistic inference. The algorithm can, in many circumstances,
perform efficient approximate inference in large and richly interconnected
models of medical diagnosis. Unlike previously described stochastic algorithms
for probabilistic inference, the randomized approximation scheme computes a
priori bounds on running time by analyzing the structure and contents of the
belief network. In this article, we describe a randomized algorithm for
probabilistic inference and analyze its performance mathematically. Then, we
devote the major portion of the paper to a discussion of the algorithm's
empirical behavior. The results indicate that the generation of good trials
(that is, trials whose distribution closely matches the true distribution),
rather than the computation of numerous mediocre trials, dominates the
performance of stochastic simulation. Key words: probabilistic inference,
belief networks, stochastic simulation, computational complexity theory,
randomized algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1499</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1499</id><created>2013-03-27</created><authors><author><keyname>Cohen</keyname><forenames>Marvin S.</forenames></author></authors><title>Decision Making &quot;Biases&quot; and Support for Assumption-Based Higher-Order
  Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-71-80</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unaided human decision making appears to systematically violate consistency
constraints imposed by normative theories; these biases in turn appear to
justify the application of formal decision-analytic models. It is argued that
both claims are wrong. In particular, we will argue that the &quot;confirmation
bias&quot; is premised on an overly narrow view of how conflicting evidence is and
ought to be handled. Effective decision aiding should focus on supporting the
contral processes by means of which knowledge is extended into novel situations
and in which assumptions are adopted, utilized, and revised. The Non- Monotonic
Probabilist represents initial work toward such an aid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1500</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1500</id><created>2013-03-27</created><authors><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Lang</keyname><forenames>Jerome</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>Automated Reasoning Using Possibilistic Logic: Semantics, Belief
  Revision and Variable Certainty Weights</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-81-87</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper an approach to automated deduction under uncertainty,based on
possibilistic logic, is proposed ; for that purpose we deal with clauses
weighted by a degree which is a lower bound of a necessity or a possibility
measure, according to the nature of the uncertainty. Two resolution rules are
used for coping with the different situations, and the refutation method can be
generalized. Besides the lower bounds are allowed to be functions of variables
involved in the clause, which gives hypothetical reasoning capabilities. The
relation between our approach and the idea of minimizing abnormality is briefly
discussed. In case where only lower bounds of necessity measures are involved,
a semantics is proposed, in which the completeness of the extended resolution
principle is proved. Moreover deduction from a partially inconsistent knowledge
base can be managed in this approach and displays some form of
non-monotonicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1501</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1501</id><created>2013-03-27</created><authors><author><keyname>Elsaesser</keyname><forenames>Christopher</forenames></author><author><keyname>Henrion</keyname><forenames>Max</forenames></author></authors><title>How Much More Probable is &quot;Much More Probable&quot;? Verbal Expressions for
  Probability Updates</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-88-94</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian inference systems should be able to explain their reasoning to
users, translating from numerical to natural language. Previous empirical work
has investigated the correspondence between absolute probabilities and
linguistic phrases. This study extends that work to the correspondence between
changes in probabilities (updates) and relative probability phrases, such as
&quot;much more likely&quot; or &quot;a little less likely.&quot; Subjects selected such phrases to
best describe numerical probability updates. We examined three hypotheses about
the correspondence, and found the most descriptively accurate of these three to
be that each such phrase corresponds to a fixed difference in probability
(rather than fixed ratio of probabilities or of odds). The empirically derived
phrase selection function uses eight phrases and achieved a 72% accuracy in
correspondence with the subjects' actual usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1502</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1502</id><created>2013-03-27</created><authors><author><keyname>Farrency</keyname><forenames>Henri</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>Positive and Negative Explanations of Uncertain Reasoning in the
  Framework of Possibility Theory</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-95-101</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach for developing the explanation capabilities
of rule-based expert systems managing imprecise and uncertain knowledge. The
treatment of uncertainty takes place in the framework of possibility theory
where the available information concerning the value of a logical or numerical
variable is represented by a possibility distribution which restricts its more
or less possible values. We first discuss different kinds of queries asking for
explanations before focusing on the two following types : i) how, a particular
possibility distribution is obtained (emphasizing the main reasons only) ; ii)
why in a computed possibility distribution, a particular value has received a
possibility degree which is so high, so low or so contrary to the expectation.
The approach is based on the exploitation of equations in max-min algebra. This
formalism includes the limit case of certain and precise information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1503</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1503</id><created>2013-03-27</created><authors><author><keyname>Fertig</keyname><forenames>Kenneth W.</forenames></author><author><keyname>Breese</keyname><forenames>John S.</forenames></author></authors><title>Interval Influence Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-102-111</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a mechanism for performing probabilistic reasoning in influence
diagrams using interval rather than point valued probabilities. We derive the
procedures for node removal (corresponding to conditional expectation) and arc
reversal (corresponding to Bayesian conditioning) in influence diagrams where
lower bounds on probabilities are stored at each node. The resulting bounds for
the transformed diagram are shown to be optimal within the class of constraints
on probability distributions that can be expressed exclusively as lower bounds
on the component probabilities of the diagram. Sequences of these operations
can be performed to answer probabilistic queries with indeterminacies in the
input and for performing sensitivity analysis on an influence diagram. The
storage requirements and computational complexity of this approach are
comparable to those for point-valued probabilistic inference mechanisms, making
the approach attractive for performing sensitivity analysis and where
probability information is not available. Limited empirical data on an
implementation of the methodology are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1504</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1504</id><created>2013-03-27</created><authors><author><keyname>Fung</keyname><forenames>Robert</forenames></author><author><keyname>Chang</keyname><forenames>Kuo-Chu</forenames></author></authors><title>Weighing and Integrating Evidence for Stochastic Simulation in Bayesian
  Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-112-117</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic simulation approaches perform probabilistic inference in Bayesian
networks by estimating the probability of an event based on the frequency that
the event occurs in a set of simulation trials. This paper describes the
evidence weighting mechanism, for augmenting the logic sampling stochastic
simulation algorithm [Henrion, 1986]. Evidence weighting modifies the logic
sampling algorithm by weighting each simulation trial by the likelihood of a
network's evidence given the sampled state node values for that trial. We also
describe an enhancement to the basic algorithm which uses the evidential
integration technique [Chin and Cooper, 1987]. A comparison of the basic
evidence weighting mechanism with the Markov blanket algorithm [Pearl, 1987],
the logic sampling algorithm, and the evidence integration algorithm is
presented. The comparison is aided by analyzing the performance of the
algorithms in a simple example network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1505</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1505</id><created>2013-03-27</created><authors><author><keyname>Geiger</keyname><forenames>Dan</forenames></author><author><keyname>Verma</keyname><forenames>Tom S.</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>d-Separation: From Theorems to Algorithms</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-118-125</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An efficient algorithm is developed that identifies all independencies
implied by the topology of a Bayesian network. Its correctness and maximality
stems from the soundness and completeness of d-separation with respect to
probability theory. The algorithm runs in time O (l E l) where E is the number
of edges in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1506</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1506</id><created>2013-03-27</created><authors><author><keyname>Gil</keyname><forenames>Maria Angeles</forenames></author><author><keyname>Jain</keyname><forenames>Pramod</forenames></author></authors><title>The Effects of Perfect and Sample Information on Fuzzy Utilities in
  Decision-Making</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-126-133</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we first consider a Bayesian framework and model the &quot;utility
function&quot; in terms of fuzzy random variables. On the basis of this model, we
define the &quot;prior (fuzzy) expected utility&quot; associated with each action, and
the corresponding &quot;posterior (fuzzy) expected utility given sample information
from a random experiment&quot;. The aim of this paper is to analyze how sample
information can affect the expected utility. In this way, by using some fuzzy
preference relations, we conclude that sample information allows a decision
maker to increase the expected utility on the average. The upper bound on the
value of the expected utility is when the decision maker has perfect
information. Applications of this work to the field of artificial intelligence
are presented through two examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1507</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1507</id><created>2013-03-27</created><authors><author><keyname>Goldszmidt</keyname><forenames>Moises</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Deciding Consistency of Databases Containing Defeasible and Strict
  Information</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-134-141</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a norm of consistency for a mixed set of defeasible and strict
sentences, based on a probabilistic semantics. This norm establishes a clear
distinction between knowledge bases depicting exceptions and those containing
outright contradictions. We then define a notion of entailment based also on
probabilistic considerations and provide a characterization of the relation
between consistency and entailment. We derive necessary and sufficient
conditions for consistency, and provide a simple decision procedure for testing
consistency and deciding whether a sentence is entailed by a database. Finally,
it is shown that if al1 sentences are Horn clauses, consistency and entailment
can be tested in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1508</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1508</id><created>2013-03-27</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>The Relationship between Knowledge, Belief and Certainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-142-151</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the relation between knowledge and certainty, where a fact is
known if it is true at all worlds an agent considers possible and is certain if
it holds with probability 1. We identify certainty with probabilistic belief.
We show that if we assume one fixed probability assignment, then the logic
KD45, which has been identified as perhaps the most appropriate for belief,
provides a complete axiomatization for reasoning about certainty. Just as an
agent may believe a fact although phi is false, he may be certain that a fact
phi, is true although phi is false. However, it is easy to see that an agent
can have such false (probabilistic) beliefs only at a set of worlds of
probability 0. If we restrict attention to structures where all worlds have
positive probability, then S5 provides a complete axiomatization. If we
consider a more general setting, where there might be a different probability
assignment at each world, then by placing appropriate conditions on the support
of the probability function (the set of worlds which have non-zero
probability), we can capture many other well-known modal logics, such as T and
S4. Finally, we consider which axioms characterize structures satisfying
Miller's principle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1509</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1509</id><created>2013-03-27</created><authors><author><keyname>Hansson</keyname><forenames>Othar</forenames></author><author><keyname>Mayer</keyname><forenames>Andy</forenames></author></authors><title>Heuristic Search as Evidential Reasoning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-152-161</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  BPS, the Bayesian Problem Solver, applies probabilistic inference and
decision-theoretic control to flexible, resource-constrained problem-solving.
This paper focuses on the Bayesian inference mechanism in BPS, and contrasts it
with those of traditional heuristic search techniques. By performing sound
inference, BPS can outperform traditional techniques with significantly less
computational effort. Empirical tests on the Eight Puzzle show that after only
a few hundred node expansions, BPS makes better decisions than does the best
existing algorithm after several million node expansions
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1510</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1510</id><created>2013-03-27</created><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author><author><keyname>Breese</keyname><forenames>John S.</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author></authors><title>The Compilation of Decision Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-162-173</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce and analyze the problem of the compilation of decision models
from a decision-theoretic perspective. The techniques described allow us to
evaluate various configurations of compiled knowledge given the nature of
evidential relationships in a domain, the utilities associated with alternative
actions, the costs of run-time delays, and the costs of memory. We describe
procedures for selecting a subset of the total observations available to be
incorporated into a compiled situation-action mapping, in the context of a
binary decision with conditional independence of evidence. The methods allow us
to incrementally select the best pieces of evidence to add to the set of
compiled knowledge in an engineering setting. After presenting several
approaches to compilation, we exercise one of the methods to provide insight
into the relationship between the distribution over weights of evidence and the
preferred degree of compilation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1511</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1511</id><created>2013-03-27</created><authors><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>A Tractable Inference Algorithm for Diagnosing Multiple Diseases</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-174-181</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine a probabilistic model for the diagnosis of multiple diseases. In
the model, diseases and findings are represented as binary variables. Also,
diseases are marginally independent, features are conditionally independent
given disease instances, and diseases interact to produce findings via a noisy
OR-gate. An algorithm for computing the posterior probability of each disease,
given a set of observed findings, called quickscore, is presented. The time
complexity of the algorithm is O(nm-2m+), where n is the number of diseases, m+
is the number of positive findings and m- is the number of negative findings.
Although the time complexity of quickscore i5 exponential in the number of
positive findings, the algorithm is useful in practice because the number of
observed positive findings is usually far less than the number of diseases
under consideration. Performance results for quickscore applied to a
probabilistic version of Quick Medical Reference (QMR) are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1512</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1512</id><created>2013-03-27</created><authors><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author><author><keyname>Suermondt</keyname><forenames>Jaap</forenames></author><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author></authors><title>Bounded Conditioning: Flexible Inference for Decisions under Scarce
  Resources</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-182-193</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a graceful approach to probabilistic inference called bounded
conditioning. Bounded conditioning monotonically refines the bounds on
posterior probabilities in a belief network with computation, and converges on
final probabilities of interest with the allocation of a complete resource
fraction. The approach allows a reasoner to exchange arbitrary quantities of
computational resource for incremental gains in inference quality. As such,
bounded conditioning holds promise as a useful inference technique for
reasoning under the general conditions of uncertain and varying reasoning
resources. The algorithm solves a probabilistic bounding problem in complex
belief networks by breaking the problem into a set of mutually exclusive,
tractable subproblems and ordering their solution by the expected effect that
each subproblem will have on the final answer. We introduce the algorithm,
discuss its characterization, and present its performance on several belief
networks, including a complex model for reasoning about problems in
intensive-care medicine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1513</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1513</id><created>2013-03-27</created><authors><author><keyname>Kak</keyname><forenames>A. C.</forenames></author><author><keyname>Andress</keyname><forenames>K. M.</forenames></author><author><keyname>Lopez-Abadia</keyname><forenames>C.</forenames></author><author><keyname>Carroll</keyname><forenames>M. S.</forenames></author><author><keyname>Lewis</keyname><forenames>J. R.</forenames></author></authors><title>Hierarchical Evidence Accumulation in the Pseiki System and Experiments
  in Model-Driven Mobile Robot Navigation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-194-207</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we will review the process of evidence accumulation in the
PSEIKI system for expectation-driven interpretation of images of 3-D scenes.
Expectations are presented to PSEIKI as a geometrical hierarchy of
abstractions. PSEIKI's job is then to construct abstraction hierarchies in the
perceived image taking cues from the abstraction hierarchies in the
expectations. The Dempster-Shafer formalism is used for associating belief
values with the different possible labels for the constructed abstractions in
the perceived image. This system has been used successfully for autonomous
navigation of a mobile robot in indoor environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1514</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1514</id><created>2013-03-27</created><authors><author><keyname>Lehmann</keyname><forenames>Harold P.</forenames></author></authors><title>A Decision-Theoretic Model for Using Scientific Data</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-208-215</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many Artificial Intelligence systems depend on the agent's updating its
beliefs about the world on the basis of experience. Experiments constitute one
type of experience, so scientific methodology offers a natural environment for
examining the issues attendant to using this class of evidence. This paper
presents a framework which structures the process of using scientific data from
research reports for the purpose of making decisions, using decision analysis
as the basis for the structure and using medical research as the general
scientific domain. The structure extends the basic influence diagram for
updating belief in an object domain parameter of interest by expanding the
parameter into four parts: those of the patient, the population, the study
sample, and the effective study sample. The structure uses biases to perform
the transformation of one parameter into another, so that, for instance,
selection biases, in concert with the population parameter, yield the study
sample parameter. The influence diagram structure provides decision theoretic
justification for practices of good clinical research such as randomized
assignment and blindfolding of care providers. The model covers most research
designs used in medicine: case-control studies, cohort studies, and controlled
clinical trials, and provides an architecture to separate clearly between
statistical knowledge and domain knowledge. The proposed general model can be
the basis for clinical epidemiological advisory systems, when coupled with
heuristic pruning of irrelevant biases; of statistical workstations, when the
computational machinery for calculation of posterior distributions is added;
and of meta-analytic reviews, when multiple studies may impact on a single
population parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1515</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1515</id><created>2013-03-27</created><authors><author><keyname>Lehner</keyname><forenames>Paul E.</forenames></author><author><keyname>Mullin</keyname><forenames>Theresa M.</forenames></author><author><keyname>Cohen</keyname><forenames>Marvin S.</forenames></author></authors><title>When Should a Decision Maker Ignore the Advice of a Decision Aid?</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-216-223</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper argues that the principal difference between decision aids and
most other types of information systems is the greater reliance of decision
aids on fallible algorithms--algorithms that sometimes generate incorrect
advice. It is shown that interactive problem solving with a decision aid that
is based on a fallible algorithm can easily result in aided performance which
is poorer than unaided performance, even if the algorithm, by itself, performs
significantly better than the unaided decision maker. This suggests that unless
certain conditions are satisfied, using a decision aid as an aid is
counterproductive. Some conditions under which a decision aid is best used as
an aid are derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1516</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1516</id><created>2013-03-27</created><authors><author><keyname>Lehner</keyname><forenames>Paul E.</forenames></author></authors><title>Inference Policies</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-224-232</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is suggested that an AI inference system should reflect an inference
policy that is tailored to the domain of problems to which it is applied -- and
furthermore that an inference policy need not conform to any general theory of
rational inference or induction. We note, for instance, that Bayesian reasoning
about the probabilistic characteristics of an inference domain may result in
the specification of an nonBayesian procedure for reasoning within the
inference domain. In this paper, the idea of an inference policy is explored in
some detail. To support this exploration, the characteristics of some standard
and nonstandard inference policies are examined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1517</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1517</id><created>2013-03-27</created><authors><author><keyname>Levitt</keyname><forenames>Tod S.</forenames></author><author><keyname>Agosta</keyname><forenames>John Mark</forenames></author><author><keyname>Binford</keyname><forenames>Thomas O.</forenames></author></authors><title>Model-based Influence Diagrams for Machine Vision</title><categories>cs.CV cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-233-244</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show an approach to automated control of machine vision systems based on
incremental creation and evaluation of a particular family of influence
diagrams that represent hypotheses of imagery interpretation and possible
subsequent processing decisions. In our approach, model-based machine vision
techniques are integrated with hierarchical Bayesian inference to provide a
framework for representing and matching instances of objects and relationships
in imagery and for accruing probabilities to rank order conflicting scene
interpretations. We extend a result of Tatman and Shachter to show that the
sequence of processing decisions derived from evaluating the diagrams at each
stage is the same as the sequence that would have been derived by evaluating
the final influence diagram that contains all random variables created during
the run of the vision system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1518</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1518</id><created>2013-03-27</created><authors><author><keyname>Loui</keyname><forenames>Ronald P.</forenames></author></authors><title>Defeasible Decisions: What the Proposal is and isn't</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-245-252</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In two recent papers, I have proposed a description of decision analysis that
differs from the Bayesian picture painted by Savage, Jeffrey and other classic
authors. Response to this view has been either overly enthusiastic or unduly
pessimistic. In this paper I try to place the idea in its proper place, which
must be somewhere in between. Looking at decision analysis as defeasible
reasoning produces a framework in which planning and decision theory can be
integrated, but work on the details has barely begun. It also produces a
framework in which the meta-decision regress can be stopped in a reasonable
way, but it does not allow us to ignore meta-level decisions. The heuristics
for producing arguments that I have presented are only supposed to be
suggestive; but they are not open to the egregious errors about which some have
worried. And though the idea is familiar to those who have studied heuristic
search, it is somewhat richer because the control of dialectic is more
interesting than the deepening of search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1519</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1519</id><created>2013-03-27</created><authors><author><keyname>McLeish</keyname><forenames>Mary</forenames></author><author><keyname>Yao</keyname><forenames>P.</forenames></author><author><keyname>Cecile</keyname><forenames>M.</forenames></author><author><keyname>Stirtzinger</keyname><forenames>T.</forenames></author></authors><title>Experiments Using Belief Functions and Weights of Evidence incorporating
  Statistical Data and Expert Opinions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-253-264</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents some ideas and results of using uncertainty management
methods in the presence of data in preference to other statistical and machine
learning methods. A medical domain is used as a test-bed with data available
from a large hospital database system which collects symptom and outcome
information about patients. Data is often missing, of many variable types and
sample sizes for particular outcomes is not large. Uncertainty management
methods are useful for such domains and have the added advantage of allowing
for expert modification of belief values originally obtained from data.
Methodological considerations for using belief functions on statistical data
are dealt with in some detail. Expert opinions are Incorporated at various
levels of the project development and results are reported on an application to
liver disease diagnosis. Recent results contrasting the use of weights of
evidence and logistic regression on another medical domain are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1520</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1520</id><created>2013-03-27</created><authors><author><keyname>Moninger</keyname><forenames>W. R.</forenames></author><author><keyname>Flueck</keyname><forenames>J. A.</forenames></author><author><keyname>Lusk</keyname><forenames>C.</forenames></author><author><keyname>Roberts</keyname><forenames>W. F.</forenames></author></authors><title>Shootout-89: A Comparative Evaluation of Knowledge-based Systems that
  Forecast Severe Weather</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-265-271</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the summer of 1989, the Forecast Systems Laboratory of the National
Oceanic and Atmospheric Administration sponsored an evaluation of artificial
intelligence-based systems that forecast severe convective storms. The
evaluation experiment, called Shootout-89, took place in Boulder, and focussed
on storms over the northeastern Colorado foothills and plains (Moninger, et
al., 1990). Six systems participated in Shootout-89. These included traditional
expert systems, an analogy-based system, and a system developed using methods
from the cognitive science/judgment analysis tradition. Each day of the
exercise, the systems generated 2 to 9 hour forecasts of the probabilities of
occurrence of: non significant weather, significant weather, and severe
weather, in each of four regions in northeastern Colorado. A verification
coordinator working at the Denver Weather Service Forecast Office gathered
ground-truth data from a network of observers. Systems were evaluated on the
basis of several measures of forecast skill, and on other metrics such as
timeliness, ease of learning, and ease of use. Systems were generally easy to
operate, however the various systems required substantially different levels of
meteorological expertise on the part of their users--reflecting the various
operational environments for which the systems had been designed. Systems
varied in their statistical behavior, but on this difficult forecast problem,
the systems generally showed a skill approximately equal to that of persistence
forecasts and climatological (historical frequency) forecasts. The two systems
that appeared best able to discriminate significant from non significant
weather events were traditional expert systems. Both of these systems required
the operator to make relatively sophisticated meteorological judgments. We are
unable, based on only one summer's worth of data, to determine the extent to
which the greater skill of the two systems was due to the content of their
knowledge bases, or to the subjective judgments of the operator. A follow-on
experiment, Shootout-91, is currently being planned. Interested potential
participants are encouraged to contact the author at the address above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1521</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1521</id><created>2013-03-27</created><authors><author><keyname>Neufeld</keyname><forenames>Eric</forenames></author><author><keyname>Horton</keyname><forenames>J. D.</forenames></author></authors><title>Conditioning on Disjunctive Knowledge: Defaults and Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-272-278</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many writers have observed that default logics appear to contain the &quot;lottery
paradox&quot; of probability theory. This arises when a default &quot;proof by
contradiction&quot; lets us conclude that a typical X is not a Y where Y is an
unusual subclass of X. We show that there is a similar problem with default
&quot;proof by cases&quot; and construct a setting where we might draw a different
conclusion knowing a disjunction than we would knowing any particular disjunct.
Though Reiter's original formalism is capable of representing this distinction,
other approaches are not. To represent and reason about this case, default
logicians must specify how a &quot;typical&quot; individual is selected. The problem is
closely related to Simpson's paradox of probability theory. If we accept a
simple probabilistic account of defaults based on the notion that one
proposition may favour or increase belief in another, the &quot;multiple extension
problem&quot; for both conjunctive and disjunctive knowledge vanishes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1522</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1522</id><created>2013-03-27</created><authors><author><keyname>Pittarelli</keyname><forenames>Michael</forenames></author></authors><title>Maximum Uncertainty Procedures for Interval-Valued Probability
  Distributions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-279-286</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measures of uncertainty and divergence are introduced for interval-valued
probability distributions and are shown to have desirable mathematical
properties. A maximum uncertainty inference procedure for marginal interval
distributions is presented. A technique for reconstruction of interval
distributions from projections is developed based on this inference procedure
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1523</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1523</id><created>2013-03-27</created><authors><author><keyname>Provan</keyname><forenames>Gregory M.</forenames></author></authors><title>A Logical Interpretation of Dempster-Shafer Theory, with Application to
  Visual Recognition</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-287-294</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate Dempster Shafer Belief functions in terms of Propositional Logic
using the implicit notion of provability underlying Dempster Shafer Theory.
Given a set of propositional clauses, assigning weights to certain
propositional literals enables the Belief functions to be explicitly computed
using Network Reliability techniques. Also, the logical procedure corresponding
to updating Belief functions using Dempster's Rule of Combination is shown.
This analysis formalizes the implementation of Belief functions within an
Assumption-based Truth Maintenance System (ATMS). We describe the extension of
an ATMS-based visual recognition system, VICTORS, with this logical formulation
of Dempster Shafer theory. Without Dempster Shafer theory, VICTORS computes all
possible visual interpretations (i.e. all logical models) without determining
the best interpretation(s). Incorporating Dempster Shafer theory enables
optimal visual interpretations to be computed and a logical semantics to be
maintained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1524</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1524</id><created>2013-03-27</created><authors><author><keyname>Sember</keyname><forenames>Peter</forenames></author><author><keyname>Zukerman</keyname><forenames>Ingrid</forenames></author></authors><title>Strategies for Generating Micro Explanations for Bayesian Belief
  Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-295-302</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian Belief Networks have been largely overlooked by Expert Systems
practitioners on the grounds that they do not correspond to the human inference
mechanism. In this paper, we introduce an explanation mechanism designed to
generate intuitive yet probabilistically sound explanations of inferences drawn
by a Bayesian Belief Network. In particular, our mechanism accounts for the
results obtained due to changes in the causal and the evidential support of a
node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1525</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1525</id><created>2013-03-27</created><authors><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author></authors><title>Evidence Absorption and Propagation through Evidence Reversals</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-303-310</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The arc reversal/node reduction approach to probabilistic inference is
extended to include the case of instantiated evidence by an operation called
&quot;evidence reversal.&quot; This not only provides a technique for computing posterior
joint distributions on general belief networks, but also provides insight into
the methods of Pearl [1986b] and Lauritzen and Spiegelhalter [1988]. Although
it is well understood that the latter two algorithms are closely related, in
fact all three algorithms are identical whenever the belief network is a
forest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1526</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1526</id><created>2013-03-27</created><authors><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author><author><keyname>Peot</keyname><forenames>Mark Alan</forenames></author></authors><title>Simulation Approaches to General Probabilistic Inference on Belief
  Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-311-318</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of algorithms have been developed to solve probabilistic inference
problems on belief networks. These algorithms can be divided into two main
groups: exact techniques which exploit the conditional independence revealed
when the graph structure is relatively sparse, and probabilistic sampling
techniques which exploit the &quot;conductance&quot; of an embedded Markov chain when the
conditional probabilities have non-extreme values. In this paper, we
investigate a family of &quot;forward&quot; Monte Carlo sampling techniques similar to
Logic Sampling [Henrion, 1988] which appear to perform well even in some
multiply connected networks with extreme conditional probabilities, and thus
would be generally applicable. We consider several enhancements which reduce
the posterior variance using this approach and propose a framework and criteria
for choosing when to use those enhancements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1527</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1527</id><created>2013-03-27</created><authors><author><keyname>Smets</keyname><forenames>Philippe</forenames></author></authors><title>Decision under Uncertainty</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-319-326</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive axiomatically the probability function that should be used to make
decisions given any form of underlying uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1528</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1528</id><created>2013-03-27</created><authors><author><keyname>Smithson</keyname><forenames>Michael</forenames></author></authors><title>Freedom: A Measure of Second-order Uncertainty for Intervalic
  Probability Schemes</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-327-334</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses a new measure that is adaptable to certain intervalic
probability frameworks, possibility theory, and belief theory. As such, it has
the potential for wide use in knowledge engineering, expert systems, and
related problems in the human sciences. This measure (denoted here by F) has
been introduced in Smithson (1988) and is more formally discussed in Smithson
(1989a)o Here, I propose to outline the conceptual basis for F and compare its
properties with other measures of second-order uncertainty. I will argue that F
is an indicator of nonspecificity or alternatively, of freedom, as
distinguished from either ambiguity or vagueness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1529</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1529</id><created>2013-03-27</created><authors><author><keyname>Spiegelhalter</keyname><forenames>David J.</forenames></author><author><keyname>Franklin</keyname><forenames>Rodney C.</forenames></author><author><keyname>Bull</keyname><forenames>Kate</forenames></author></authors><title>Assessment, Criticism and Improvement of Imprecise Subjective
  Probabilities for a Medical Expert System</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-335-342</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three paediatric cardiologists assessed nearly 1000 imprecise subjective
conditional probabilities for a simple belief network representing congenital
heart disease, and the quality of the assessments has been measured using
prospective data on 200 babies. Quality has been assessed by a Brier scoring
rule, which decomposes into terms measuring lack of discrimination and
reliability. The results are displayed for each of 27 diseases and 24
questions, and generally the assessments are reliable although there was a
tendency for the probabilities to be too extreme. The imprecision allows the
judgements to be converted to implicit samples, and by combining with the
observed data the probabilities naturally adapt with experience. This appears
to be a practical procedure even for reasonably large expert systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1530</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1530</id><created>2013-03-27</created><authors><author><keyname>Srinivas</keyname><forenames>Sampath</forenames></author><author><keyname>Russell</keyname><forenames>Stuart</forenames></author><author><keyname>Agogino</keyname><forenames>Alice M.</forenames></author></authors><title>Automated Construction of Sparse Bayesian Networks from Unstructured
  Probabilistic Models and Domain Information</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-343-350</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm for automated construction of a sparse Bayesian network given an
unstructured probabilistic model and causal domain information from an expert
has been developed and implemented. The goal is to obtain a network that
explicitly reveals as much information regarding conditional independence as
possible. The network is built incrementally adding one node at a time. The
expert's information and a greedy heuristic that tries to keep the number of
arcs added at each step to a minimum are used to guide the search for the next
node to add. The probabilistic model is a predicate that can answer queries
about independencies in the domain. In practice the model can be implemented in
various ways. For example, the model could be a statistical independence test
operating on empirical data or a deductive prover operating on a set of
independence statements about the domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1531</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1531</id><created>2013-03-27</created><authors><author><keyname>Strat</keyname><forenames>Thomas M.</forenames></author></authors><title>Making Decisions with Belief Functions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-351-360</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A primary motivation for reasoning under uncertainty is to derive decisions
in the face of inconclusive evidence. However, Shafer's theory of belief
functions, which explicitly represents the underconstrained nature of many
reasoning problems, lacks a formal procedure for making decisions. Clearly,
when sufficient information is not available, no theory can prescribe actions
without making additional assumptions. Faced with this situation, some
assumption must be made if a clearly superior choice is to emerge. In this
paper we offer a probabilistic interpretation of a simple assumption that
disambiguates decision problems represented with belief functions. We prove
that it yields expected values identical to those obtained by a probabilistic
analysis that makes the same assumption. In addition, we show how the decision
analysis methodology frequently employed in probabilistic reasoning can be
extended for use with belief functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1532</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1532</id><created>2013-03-27</created><authors><author><keyname>Swain</keyname><forenames>Michael J.</forenames></author><author><keyname>Wixson</keyname><forenames>Lambert E.</forenames></author><author><keyname>Chou</keyname><forenames>Paul B.</forenames></author></authors><title>Efficient Parallel Estimation for Markov Random Fields</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-361-368</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new, deterministic, distributed MAP estimation algorithm for
Markov Random Fields called Local Highest Confidence First (Local HCF). The
algorithm has been applied to segmentation problems in computer vision and its
performance compared with stochastic algorithms. The experiments show that
Local HCF finds better estimates than stochastic algorithms with much less
computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1533</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1533</id><created>2013-03-27</created><authors><author><keyname>Vaughan</keyname><forenames>David S.</forenames></author><author><keyname>Perrin</keyname><forenames>Bruce M.</forenames></author><author><keyname>Yadrick</keyname><forenames>Robert M.</forenames></author></authors><title>Comparing Expert Systems Built Using Different Uncertain Inference
  Systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-369-376</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study compares the inherent intuitiveness or usability of the most
prominent methods for managing uncertainty in expert systems, including those
of EMYCIN, PROSPECTOR, Dempster-Shafer theory, fuzzy set theory, simplified
probability theory (assuming marginal independence), and linear regression
using probability estimates. Participants in the study gained experience in a
simple, hypothetical problem domain through a series of learning trials. They
were then randomly assigned to develop an expert system using one of the six
Uncertain Inference Systems (UISs) listed above. Performance of the resulting
systems was then compared. The results indicate that the systems based on the
PROSPECTOR and EMYCIN models were significantly less accurate for certain types
of problems compared to systems based on the other UISs. Possible reasons for
these differences are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1534</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1534</id><created>2013-03-27</created><authors><author><keyname>Wen</keyname><forenames>Wilson X.</forenames></author></authors><title>Directed Cycles in Belief Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-377-384</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most difficult task in probabilistic reasoning may be handling directed
cycles in belief networks. To the best knowledge of this author, there is no
serious discussion of this problem at all in the literature of probabilistic
reasoning so far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1535</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1535</id><created>2013-03-27</created><authors><author><keyname>Xiang</keyname><forenames>Yang</forenames></author><author><keyname>Beddoes</keyname><forenames>Michael P.</forenames></author><author><keyname>Poole</keyname><forenames>David L</forenames></author></authors><title>Can Uncertainty Management be Realized in a Finite Totally Ordered
  Probability Algebra?</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-385-393</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the feasibility of using finite totally ordered probability
models under Alelinnas's Theory of Probabilistic Logic [Aleliunas, 1988] is
investigated. The general form of the probability algebra of these models is
derived and the number of possible algebras with given size is deduced. Based
on this analysis, we discuss problems of denominator-indifference and
ambiguity-generation that arise in reasoning by cases and abductive reasoning.
An example is given that illustrates how these problems arise. The
investigation shows that a finite probability model may be of very limited
usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1536</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1536</id><created>2013-03-27</created><authors><author><keyname>Yager</keyname><forenames>Ronald R.</forenames></author></authors><title>Normalization and the Representation of Nonmonotonic Knowledge in the
  Theory of Evidence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Fifth Conference on Uncertainty in
  Artificial Intelligence (UAI1989)</comments><proxy>auai</proxy><report-no>UAI-P-1989-PG-394-403</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the Dempster-Shafer theory of evidence. We introduce a concept of
monotonicity which is related to the diminution of the range between belief and
plausibility. We show that the accumulation of knowledge in this framework
exhibits a nonmonotonic property. We show how the belief structure can be used
to represent typical or commonsense knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1548</identifier>
 <datestamp>2013-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1548</id><created>2013-04-04</created><updated>2013-05-14</updated><authors><author><keyname>Ugander</keyname><forenames>Johan</forenames></author><author><keyname>Backstrom</keyname><forenames>Lars</forenames></author><author><keyname>Kleinberg</keyname><forenames>Jon</forenames></author></authors><title>Subgraph Frequencies: Mapping the Empirical and Extremal Geography of
  Large Graph Collections</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages, 6 figures, 1 table</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A growing set of on-line applications are generating data that can be viewed
as very large collections of small, dense social graphs -- these range from
sets of social groups, events, or collaboration projects to the vast collection
of graph neighborhoods in large social networks. A natural question is how to
usefully define a domain-independent coordinate system for such a collection of
graphs, so that the set of possible structures can be compactly represented and
understood within a common space. In this work, we draw on the theory of graph
homomorphisms to formulate and analyze such a representation, based on
computing the frequencies of small induced subgraphs within each graph. We find
that the space of subgraph frequencies is governed both by its combinatorial
properties, based on extremal results that constrain all graphs, as well as by
its empirical properties, manifested in the way that real social graphs appear
to lie near a simple one-dimensional curve through this space.
  We develop flexible frameworks for studying each of these aspects. For
capturing empirical properties, we characterize a simple stochastic generative
model, a single-parameter extension of Erdos-Renyi random graphs, whose
stationary distribution over subgraphs closely tracks the concentration of the
real social graph families. For the extremal properties, we develop a tractable
linear program for bounding the feasible space of subgraph frequencies by
harnessing a toolkit of known extremal graph theory. Together, these two
complementary frameworks shed light on a fundamental question pertaining to
social graphs: what properties of social graphs are 'social' properties and
what properties are 'graph' properties?
  We conclude with a brief demonstration of how the coordinate system we
examine can also be used to perform classification tasks, distinguishing
between social graphs of different origins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1567</identifier>
 <datestamp>2013-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1567</id><created>2013-04-04</created><updated>2013-11-02</updated><authors><author><keyname>Bandari</keyname><forenames>Roja</forenames></author><author><keyname>Rahmandad</keyname><forenames>Hazhir</forenames></author><author><keyname>Roychowdhury</keyname><forenames>Vwani P.</forenames></author></authors><title>Blind Men and the Elephant: Detecting Evolving Groups In Social News</title><categories>cs.SI cs.IR physics.soc-ph</categories><comments>10 pages, icwsm2013</comments><msc-class>91C20, 91D30, 91D10, 68R10, 05C70</msc-class><acm-class>H.3.1; I.5.2; G.2.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an automated and unsupervised methodology for a novel
summarization of group behavior based on content preference. We show that graph
theoretical community evolution (based on similarity of user preference for
content) is effective in indexing these dynamics. Combined with text analysis
that targets automatically-identified representative content for each
community, our method produces a novel multi-layered representation of evolving
group behavior. We demonstrate this methodology in the context of political
discourse on a social news site with data that spans more than four years and
find coexisting political leanings over extended periods and a disruptive
external event that lead to a significant reorganization of existing patterns.
Finally, where there exists no ground truth, we propose a new evaluation
approach by using entropy measures as evidence of coherence along the evolution
path of these groups. This methodology is valuable to designers and managers of
online forums in need of granular analytics of user activity, as well as to
researchers in social and political sciences who wish to extend their inquiries
to large-scale data available on the web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1568</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1568</id><created>2013-04-04</created><authors><author><keyname>Florindo</keyname><forenames>Jo&#xe3;o Batista</forenames></author><author><keyname>Bruno</keyname><forenames>Odemir Martinez</forenames></author></authors><title>Multiscale Fractal Descriptors Applied to Texture Classification</title><categories>cs.CV</categories><comments>5 pages, 4 figures</comments><journal-ref>Journal of Physics: Conference Series, 410, 012022, 2013</journal-ref><doi>10.1088/1742-6596/410/1/012022</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes the combination of multiscale transform with fractal
descriptors employed in the classification of gray-level texture images. We
apply the space-scale transform (derivative + Gaussian filter) over the
Bouligand-Minkowski fractal descriptors, followed by a threshold over the
filter response, aiming at attenuating noise effects caused by the final part
of this response. The method is tested in the classification of a well-known
data set (Brodatz) and compared with other classical texture descriptor
techniques. The results demonstrate the advantage of the proposed approach,
achieving a higher success rate with a reduced amount of descriptors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1571</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1571</id><created>2013-04-04</created><authors><author><keyname>Jassim</keyname><forenames>Firas A.</forenames></author></authors><title>Hiding Image in Image by Five Modulus Method for Image Steganography</title><categories>cs.MM cs.CV</categories><comments>5 pages, 5 tables, 5 figures</comments><journal-ref>Journal of computing, volume 5, issue 2, 2013</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper is to create a practical steganographic implementation to hide
color image (stego) inside another color image (cover). The proposed technique
uses Five Modulus Method to convert the whole pixels within both the cover and
the stego images into multiples of five. Since each pixels inside the stego
image is divisible by five then the whole stego image could be divided by five
to get new range of pixels 0..51. Basically, the reminder of each number that
is not divisible by five is either 1,2,3 or 4 when divided by 5. Subsequently,
then a 4-by-4 window size has been implemented to accommodate the proposed
technique. For each 4-by-4 window inside the cover image, a number from 1 to 4
could be embedded secretly from the stego image. The previous discussion must
be applied separately for each of the R, G, and B arrays. Moreover, a stego-key
could be combined with the proposed algorithm to make it difficult for any
adversary to extract the secret image from the cover image. Based on the PSNR
value, the extracted stego image has high PSNR value. Hence this new
steganography algorithm is very efficient to hide color images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1572</identifier>
 <datestamp>2013-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1572</id><created>2013-04-04</created><updated>2013-06-03</updated><authors><author><keyname>Hu</keyname><forenames>Nan</forenames></author><author><keyname>Guibas</keyname><forenames>Leonidas</forenames></author></authors><title>Spectral Descriptors for Graph Matching</title><categories>cs.CV</categories><comments>the reference does not appear, missing .bbl file in submission, need
  replacement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the weighted graph matching problem. Recently,
approaches to this problem based on spectral methods have gained significant
attention. We propose two graph spectral descriptors based on the graph
Laplacian, namely a Laplacian family signature (LFS) on nodes, and a pairwise
heat kernel distance on edges. We show the stability of both our descriptors
under small perturbation of edges and nodes. In addition, we show that our
pairwise heat kernel distance is a noise-tolerant approximation of the
classical adjacency matrix-based second order compatibility function. These
nice properties suggest a descriptor-based matching scheme, for which we set up
an integer quadratic problem (IQP) and apply an approximate solver to find a
near optimal solution. We have tested our matching method on a set of randomly
generated graphs, the widely-used CMU house sequence and a set of real images.
These experiments show the superior performance of our selected node signatures
and edge descriptors for graph matching, as compared with other existing
signature-based matchings and adjacency matrix-based matchings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1574</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1574</id><created>2013-04-04</created><authors><author><keyname>Zhang</keyname><forenames>Chao</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author></authors><title>Generalization Bounds for Domain Adaptation</title><categories>cs.LG math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide a new framework to obtain the generalization bounds
of the learning process for domain adaptation, and then apply the derived
bounds to analyze the asymptotical convergence of the learning process. Without
loss of generality, we consider two kinds of representative domain adaptation:
one is with multiple sources and the other is combining source and target data.
  In particular, we use the integral probability metric to measure the
difference between two domains. For either kind of domain adaptation, we
develop a related Hoeffding-type deviation inequality and a symmetrization
inequality to achieve the corresponding generalization bound based on the
uniform entropy number. We also generalized the classical McDiarmid's
inequality to a more general setting where independent random variables can
take values from different domains. By using this inequality, we then obtain
generalization bounds based on the Rademacher complexity. Afterwards, we
analyze the asymptotic convergence and the rate of convergence of the learning
process for such kind of domain adaptation. Meanwhile, we discuss the factors
that affect the asymptotic behavior of the learning process and the numerical
experiments support our theoretical findings as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1575</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1575</id><created>2013-04-04</created><authors><author><keyname>Avramopoulos</keyname><forenames>Ioannis</forenames></author></authors><title>A general theory of equilibrium behavior</title><categories>cs.GT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Economists were content with the concept of the Nash equilibrium as game
theory's solution concept until Daskalakis, Goldberg, and Papadimitriou showed
that finding a Nash equilibrium is most likely a computationally hard problem,
a result that set off a deep scientific crisis. Motivated, in part, by their
result, in this paper, we propose a general theory of equilibrium behavior in
vector fields (and, therefore, also noncooperative games). Our line of
discourse is to show that these universal in nature mathematical objects are
endowed with significant structure, which we probe to unearth atypical,
previously unidentified, equilibrium behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1577</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1577</id><created>2013-04-04</created><authors><author><keyname>Chekuri</keyname><forenames>Chandra</forenames></author><author><keyname>Chuzhoy</keyname><forenames>Julia</forenames></author></authors><title>Large-Treewidth Graph Decompositions and Applications</title><categories>cs.DS cs.DM</categories><comments>An extended abstract of the paper is to appear in Proceedings of ACM
  STOC, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Treewidth is a graph parameter that plays a fundamental role in several
structural and algorithmic results. We study the problem of decomposing a given
graph $G$ into node-disjoint subgraphs, where each subgraph has sufficiently
large treewidth. We prove two theorems on the tradeoff between the number of
the desired subgraphs $h$, and the desired lower bound $r$ on the treewidth of
each subgraph. The theorems assert that, given a graph $G$ with treewidth $k$,
a decomposition with parameters $h,r$ is feasible whenever $hr^2 \le
k/\polylog(k)$, or $h^3r \le k/\polylog(k)$ holds. We then show a framework for
using these theorems to bypass the well-known Grid-Minor Theorem of Robertson
and Seymour in some applications. In particular, this leads to substantially
improved parameters in some Erdos-Posa-type results, and faster algorithms for
a class of fixed-parameter tractable problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1304.1584</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1304.1584</id><created>2013-04-04</created><authors><author><keyname>Chakraborty</keyname><forenames>Supratik</forenames></author><author><keyname>Meel</keyname><forenames>Kuldeep S.</forenames></author><author><keyname>Vardi</keyname><forenames>Moshe Y.</forenames></author></authors><title>A Scalable and Nearly Uniform Generator of SAT Witnesses</title><categories>cs.LO</categories><comments>Conference version will appear in CAV 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Functional verification constitutes one of the most challenging tasks in the
development of modern hardware systems, and simulation-based verification
techniques dominate the functional verification landscape. A dominant paradigm
in simulation-based verification is directed random testing, where a model of
the system is simulated with a set of random test stimuli that are uniformly or
near-uniformly distributed over the space of all stimuli satisfying a given set
of constraints. Uniform or near-uniform generation of solutions for large
constraint sets is therefore a problem of theoretical and practical interest.
For Boolean constraints, prior work offered heuristic approaches with no
guarantee of performance, and theoretical approaches with proven guarantees,
but poor performance in practice. We offer here a new approach with theoretical
performance guarantees and demonstrate its practical utility on large
constraint sets.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="43000" completeListSize="102538">1122234|44001</resumptionToken>
</ListRecords>
</OAI-PMH>
