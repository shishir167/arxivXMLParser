<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T00:43:41Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|13001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4075</identifier>
 <datestamp>2010-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4075</id><created>2010-04-23</created><updated>2010-07-08</updated><authors><author><keyname>Belfiore</keyname><forenames>Jean-Claude</forenames></author><author><keyname>Oggier</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames></author></authors><title>Secrecy Gain: a Wiretap Lattice Code Design</title><categories>cs.IT cs.CR math.IT</categories><comments>5 pages, 2 figures, proceedings of ISITA 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the notion of secrecy gain as a code design criterion for wiretap
lattice codes to be used over an additive white Gaussian noise channel. Our
analysis relies on the error probabilites of both the legitimate user and the
eavesdropper. We focus on geometrical properties of lattices, described by
their theta series, to characterize good wiretap codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4080</identifier>
 <datestamp>2010-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4080</id><created>2010-04-23</created><authors><author><keyname>Hariharan</keyname><forenames>Ramesh</forenames></author><author><keyname>Panigrahi</keyname><forenames>Debmalya</forenames></author></authors><title>A General Framework for Graph Sparsification</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a weighted graph $G$ and an error parameter $\epsilon &gt; 0$, the {\em
graph sparsification} problem requires sampling edges in $G$ and giving the
sampled edges appropriate weights to obtain a sparse graph $G_{\epsilon}$
(containing O(n\log n) edges in expectation) with the following property: the
weight of every cut in $G_{\epsilon}$ is within a factor of $(1\pm \epsilon)$
of the weight of the corresponding cut in $G$. We provide a generic framework
that sets out sufficient conditions for any particular sampling scheme to
result in good sparsifiers, and obtain a set of results by simple
instantiations of this framework. The results we obtain include the following:
(1) We improve the time complexity of graph sparsification from O(m\log^3 n) to
O(m + n\log^4 n) for graphs with polynomial edge weights. (2) We improve the
time complexity of graph sparsification from O(m\log^3 n) to O(m\log^2 n) for
graphs with arbitrary edge weights. (3) If the size of the sparsifier is
allowed to be O(n\log^2 n/\epsilon^2) instead of O(n\log n/\epsilon^2), we
improve the time complexity of sparsification to O(m) for graphs with
polynomial edge weights. (4) We show that sampling using standard
connectivities results in good sparsifiers, thus resolving an open question of
Benczur and Karger. As a corollary, we give a simple proof of (a slightly
weaker version of) a result due to Spielman and Srivastava showing that
sampling using effective resistances produces good sparsifiers. (5) We give a
simple proof showing that sampling using strong connectivities results in good
sparsifiers, a result obtained previously using a more involved proof by
Benczur and Karger. A key ingredient of our proofs is a generalization of
bounds on the number of small cuts in an undirected graph due to Karger; this
generalization might be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4089</identifier>
 <datestamp>2010-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4089</id><created>2010-04-23</created><authors><author><keyname>Tedesco</keyname><forenames>Gianni</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Real-Time Alert Correlation with Type Graphs</title><categories>cs.AI cs.CR</categories><comments>15 pages, 3 tables, (ICISS 2008)</comments><journal-ref>Proceedings of the 4th International Conference on Information
  Systems Security (ICISS 2008), Lecture Notes in Computer Science 5352,
  Hyderabad, India, 2008, p173-187</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The premise of automated alert correlation is to accept that false alerts
from a low level intrusion detection system are inevitable and use attack
models to explain the output in an understandable way. Several algorithms exist
for this purpose which use attack graphs to model the ways in which attacks can
be combined. These algorithms can be classified in to two broad categories
namely scenario-graph approaches, which create an attack model starting from a
vulnerability assessment and type-graph approaches which rely on an abstract
model of the relations between attack types. Some research in to improving the
efficiency of type-graph correlation has been carried out but this research has
ignored the hypothesizing of missing alerts. Our work is to present a novel
type-graph algorithm which unifies correlation and hypothesizing in to a single
operation. Our experimental results indicate that the approach is extremely
efficient in the face of intensive alerts and produces compact output graphs
comparable to other techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4095</identifier>
 <datestamp>2010-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4095</id><created>2010-04-23</created><authors><author><keyname>Feyereisl</keyname><forenames>Jan</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>STORM - A Novel Information Fusion and Cluster Interpretation Technique</title><categories>cs.AI cs.NE</categories><comments>11 pages, 2 figures, 10th International Conference on Intelligent
  Data Engineering and Automated Learning (IDEAL 09)</comments><journal-ref>Proceedings of the 10th International Conference on Intelligent
  Data Engineering and Automated Learning (IDEAL 09), Lecture Notes in Computer
  Science 5788, Burgos, Spain, 2009, p208-218</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of data without labels is commonly subject to scrutiny by
unsupervised machine learning techniques. Such techniques provide more
meaningful representations, useful for better understanding of a problem at
hand, than by looking only at the data itself. Although abundant expert
knowledge exists in many areas where unlabelled data is examined, such
knowledge is rarely incorporated into automatic analysis. Incorporation of
expert knowledge is frequently a matter of combining multiple data sources from
disparate hypothetical spaces. In cases where such spaces belong to different
data types, this task becomes even more challenging. In this paper we present a
novel immune-inspired method that enables the fusion of such disparate types of
data for a specific set of problems. We show that our method provides a better
visual understanding of one hypothetical space with the help of data from
another hypothetical space. We believe that our model has implications for the
field of exploratory data analysis and knowledge discovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4109</identifier>
 <datestamp>2010-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4109</id><created>2010-04-23</created><authors><author><keyname>Ruzankin</keyname><forenames>Pavel</forenames></author></authors><title>Operator-oriented programming: a new paradigm for implementing window
  interfaces and parallel algorithms</title><categories>cs.PL cs.DC</categories><acm-class>D.3.3; D.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new programming paradigm which can be useful, in particular, for
implementing window interfaces and parallel algorithms. This paradigm allows a
user to define operators which can contain nested operators. The new paradigm
is called operator-oriented. One of the goals of this paradigm is to escape the
complexity of objects definitions inherent in many object-oriented languages
and to move to transparent algorithms definitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4128</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4128</id><created>2010-04-23</created><updated>2010-04-26</updated><authors><author><keyname>Gluskin</keyname><forenames>Emanuel</forenames></author></authors><title>An approximate analytical (structural) superposition in terms of two, or
  more, &quot;alfa&quot;-circuits of the same topology: Pt.1 - description of the
  superposition</title><categories>cs.OH</categories><comments>This is my old (2005-6) Ms.. The &quot;f-connection&quot; is new and thus the
  work seems to be too detailed, but some central proofs were difficult for me,
  and having to be sure in good precision of the &quot;analytical superposition&quot;, I
  calculated different cases. See in http://www.ee.bgu.ac.il/~gluskin/ Article
  no 50 and the Conference Presentation of 2008. 25 pages, 7 figures, 1 table.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One-ports named &quot;f-circuits&quot;, composed of similar conductors described by a
monotonic polynomial, or quasi-polynomial (i.e. with positive but not
necessarily integer, powers) characteristic i = f(v) are studied, focusing on
the algebraic map f --&gt; F. Here F(.) is the input conductivity characteristic;
i.e., iin = F(vin) is the input current. The &quot;power-law&quot; &quot;alfa-circuit&quot;
introduced in [1], for which f(v) ~ v^&quot;alfa&quot;, is an important particular case.
By means of a generalization of a parallel connection, the f-circuits are
constructed from the alfa-circuits of the same topology, with different &quot;alfa&quot;,
so that the given topology is kept, and 'f' is an additive function of the
connection. We observe and consider an associated, generally approximated, but,
in all of the cases studied, always high-precision, specific superposition.
This superposition is in terms of f --&gt; F, and it means that F(.) of the
connection is close to the sum of the input currents of the independent
&quot;alfa&quot;-circuits, all connected in parallel to the same source. In other words,
F(.) is well approximated by a linear combination of the same degrees of the
independent variable as in f(.), i.e. the map of the characteristics f --&gt; F is
close to a linear one. This unexpected result is useful for understanding
nonlinear algebraic circuits, and is missed in the classical theory.
  The cases of f(v) = D1v + D2v^2 and f(v) = D1v + D3v^3, are analyzed in
examples. Special topologies when the superposition must be ideal, are also
considered. In the second part [2] of the work the &quot;circuit mechanism&quot; that is
responsible for the high precision of the superposition, in the most general
case, will be explained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4154</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4154</id><created>2010-04-23</created><updated>2011-06-26</updated><authors><author><keyname>Mitra</keyname><forenames>Swarup kumar</forenames></author><author><keyname>Chakraborty</keyname><forenames>Ayon</forenames></author><author><keyname>Mandal</keyname><forenames>Subhajit</forenames></author><author><keyname>Naskar</keyname><forenames>M. K.</forenames></author></authors><title>Simulation of Wireless Sensor Networks Using TinyOS- A case Study</title><categories>cs.NI</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn by the author due to a crucial sign error in
equation 1
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4170</identifier>
 <datestamp>2010-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4170</id><created>2010-04-23</created><authors><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author></authors><title>A New Metaheuristic Bat-Inspired Algorithm</title><categories>math.OC cs.NE physics.bio-ph physics.comp-ph</categories><comments>10 pages, 2 figures</comments><report-no>1004.4170</report-no><journal-ref>X.-S. Yang, A New Metaheuristic Bat-Inspired Algorithm, in: Nature
  Inspired Cooperative Strategies for Optimization (NICSO 2010) (Eds. J. R.
  Gonzalez et al.), SCI 284, 65-74 (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metaheuristic algorithms such as particle swarm optimization, firefly
algorithm and harmony search are now becoming powerful methods for solving many
tough optimization problems. In this paper, we propose a new metaheuristic
method, the Bat Algorithm, based on the echolocation behaviour of bats. We also
intend to combine the advantages of existing algorithms into the new bat
algorithm. After a detailed formulation and explanation of its implementation,
we will then compare the proposed algorithm with other existing algorithms,
including genetic algorithms and particle swarm optimization. Simulations show
that the proposed algorithm seems much superior to other algorithms, and
further studies are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4181</identifier>
 <datestamp>2010-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4181</id><created>2010-04-23</created><authors><author><keyname>Morrill</keyname><forenames>Glyn</forenames></author><author><keyname>Valent&#xed;n</keyname><forenames>Oriol</forenames></author></authors><title>Displacement Calculus</title><categories>cs.CL</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Lambek calculus provides a foundation for categorial grammar in the form
of a logic of concatenation. But natural language is characterized by
dependencies which may also be discontinuous. In this paper we introduce the
displacement calculus, a generalization of Lambek calculus, which preserves its
good proof-theoretic properties while embracing discontinuiity and subsuming
it. We illustrate linguistic applications and prove Cut-elimination, the
subformula property, and decidability
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4196</identifier>
 <datestamp>2010-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4196</id><created>2010-04-23</created><authors><author><keyname>Vakati</keyname><forenames>Sudheer</forenames></author><author><keyname>Fern&#xe1;ndez-Baca</keyname><forenames>David</forenames></author></authors><title>Graph Triangulations and the Compatibility of Unrooted Phylogenetic
  Trees</title><categories>cs.DM</categories><msc-class>68R10, 92B10</msc-class><acm-class>F.2.2; G.2.2; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the compatibility of a collection of unrooted phylogenetic
trees as a question of determining whether a graph derived from these trees ---
the display graph --- has a specific kind of triangulation, which we call
legal. Our result is a counterpart to the well known triangulation-based
characterization of the compatibility of undirected multi-state characters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4216</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4216</id><created>2010-04-23</created><authors><author><keyname>Sexton</keyname><forenames>Alan P.</forenames></author><author><keyname>Swinbank</keyname><forenames>Richard</forenames></author></authors><title>Symmetric M-tree</title><categories>cs.DB cs.DS</categories><comments>8 pages, 10 figures</comments><report-no>CSR-04-2</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The M-tree is a paged, dynamically balanced metric access method that
responds gracefully to the insertion of new objects. To date, no algorithm has
been published for the corresponding Delete operation. We believe this to be
non-trivial because of the design of the M-tree's Insert algorithm. We propose
a modification to Insert that overcomes this problem and give the corresponding
Delete algorithm. The performance of the tree is comparable to the M-tree and
offers additional benefits in terms of supported operations, which we briefly
discuss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4222</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4222</id><created>2010-04-23</created><updated>2011-02-24</updated><authors><author><keyname>Tang</keyname><forenames>Gongguo</forenames></author><author><keyname>Nehorai</keyname><forenames>Arye</forenames></author></authors><title>Performance Analysis of Sparse Recovery Based on Constrained Minimal
  Singular Values</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2011.2164913</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The stability of sparse signal reconstruction is investigated in this paper.
We design efficient algorithms to verify the sufficient condition for unique
$\ell_1$ sparse recovery. One of our algorithm produces comparable results with
the state-of-the-art technique and performs orders of magnitude faster. We show
that the $\ell_1$-constrained minimal singular value ($\ell_1$-CMSV) of the
measurement matrix determines, in a very concise manner, the recovery
performance of $\ell_1$-based algorithms such as the Basis Pursuit, the Dantzig
selector, and the LASSO estimator. Compared with performance analysis involving
the Restricted Isometry Constant, the arguments in this paper are much less
complicated and provide more intuition on the stability of sparse signal
recovery. We show also that, with high probability, the subgaussian ensemble
generates measurement matrices with $\ell_1$-CMSVs bounded away from zero, as
long as the number of measurements is relatively large. To compute the
$\ell_1$-CMSV and its lower bound, we design two algorithms based on the
interior point algorithm and the semi-definite relaxation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4223</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4223</id><created>2010-04-23</created><authors><author><keyname>Moitra</keyname><forenames>Ankur</forenames></author><author><keyname>Valiant</keyname><forenames>Gregory</forenames></author></authors><title>Settling the Polynomial Learnability of Mixtures of Gaussians</title><categories>cs.LG cs.DS</categories><comments>43 pages, 2 figures</comments><msc-class>68Q32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given data drawn from a mixture of multivariate Gaussians, a basic problem is
to accurately estimate the mixture parameters. We give an algorithm for this
problem that has a running time, and data requirement polynomial in the
dimension and the inverse of the desired accuracy, with provably minimal
assumptions on the Gaussians. As simple consequences of our learning algorithm,
we can perform near-optimal clustering of the sample points and density
estimation for mixtures of k Gaussians, efficiently. The building blocks of our
algorithm are based on the work Kalai et al. [STOC 2010] that gives an
efficient algorithm for learning mixtures of two Gaussians by considering a
series of projections down to one dimension, and applying the method of moments
to each univariate projection. A major technical hurdle in Kalai et al. is
showing that one can efficiently learn univariate mixtures of two Gaussians. In
contrast, because pathological scenarios can arise when considering univariate
projections of mixtures of more than two Gaussians, the bulk of the work in
this paper concerns how to leverage an algorithm for learning univariate
mixtures (of many Gaussians) to yield an efficient algorithm for learning in
high dimensions. Our algorithm employs hierarchical clustering and rescaling,
together with delicate methods for backtracking and recovering from failures
that can occur in our univariate algorithm. Finally, while the running time and
data requirements of our algorithm depend exponentially on the number of
Gaussians in the mixture, we prove that such a dependence is necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4239</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4239</id><created>2010-04-23</created><authors><author><keyname>Frieze</keyname><forenames>Alan</forenames></author><author><keyname>Sorkin</keyname><forenames>Gregory</forenames></author></authors><title>Average case performance of heuristics for multi-dimensional assignment
  problems</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider multi-dimensional assignment problems in a probabilistic setting.
Our main results are: (i) A new efficient algorithm for the 3-dimensional
planar problem, based on enumerating and selecting from a set of
&quot;alternating-path trees&quot;; (ii) A new efficient matching-based algorithm for the
3-dimensional axial problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4240</identifier>
 <datestamp>2015-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4240</id><created>2010-04-23</created><authors><author><keyname>Dasgupta</keyname><forenames>Anirban</forenames></author><author><keyname>Kumar</keyname><forenames>Ravi</forenames></author><author><keyname>Sarl&#xf3;s</keyname><forenames>Tam&#xe1;s</forenames></author></authors><title>A Sparse Johnson--Lindenstrauss Transform</title><categories>cs.DS</categories><comments>10 pages, conference version.</comments><msc-class>68Q25, 68Q87, 68W20</msc-class><acm-class>F.2.0; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dimension reduction is a key algorithmic tool with many applications
including nearest-neighbor search, compressed sensing and linear algebra in the
streaming model. In this work we obtain a {\em sparse} version of the
fundamental tool in dimension reduction --- the Johnson--Lindenstrauss
transform. Using hashing and local densification, we construct a sparse
projection matrix with just $\tilde{O}(\frac{1}{\epsilon})$ non-zero entries
per column. We also show a matching lower bound on the sparsity for a large
class of projection matrices. Our bounds are somewhat surprising, given the
known lower bounds of $\Omega(\frac{1}{\epsilon^2})$ both on the number of rows
of any projection matrix and on the sparsity of projection matrices generated
by natural constructions.
  Using this, we achieve an $\tilde{O}(\frac{1}{\epsilon})$ update time per
non-zero element for a $(1\pm\epsilon)$-approximate projection, thereby
substantially outperforming the $\tilde{O}(\frac{1}{\epsilon^2})$ update time
required by prior approaches. A variant of our method offers the same
guarantees for sparse vectors, yet its $\tilde{O}(d)$ worst case running time
matches the best approach of Ailon and Liberty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4241</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4241</id><created>2010-04-23</created><authors><author><keyname>Sodagari</keyname><forenames>Shabnam</forenames></author><author><keyname>Hesami</keyname><forenames>Peyman</forenames></author><author><keyname>Avanaki</keyname><forenames>Alireza Nasiri</forenames></author></authors><title>Error Concealment in Image Communication Using Edge Map Watermarking and
  Spatial Smoothing</title><categories>cs.MM</categories><comments>To appear in Proceeding of ICCET 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel error concealment algorithm to be used at the receiver
side of a lossy image transmission system. Our algorithm involves hiding the
edge map of the original image at the transmitter within itself using a robust
watermarking scheme. At the receiver, wherever a lost block is detected, the
extracted edge information is used as border constraint for the spatial
smoothing employing the intact neighboring blocks in order to conceal errors.
Simulation results show the superiority of our technique over existing methods
even in case of high packet loss ratios in the communication network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4275</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4275</id><created>2010-04-24</created><authors><author><keyname>Semenova</keyname><forenames>Irina</forenames></author></authors><title>Intelligent Technologies in Model Base Management System Design
  Automation</title><categories>cs.OH</categories><comments>4 pages, 2 figures</comments><acm-class>J.6; K.6.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article describes the prospects of model base management system design
automation for decision support systems and suggests the toolbox scheme for
design automation based on intelligent technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4277</identifier>
 <datestamp>2010-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4277</id><created>2010-04-24</created><authors><author><keyname>Huang</keyname><forenames>Xuan-Chao</forenames></author><author><keyname>Cheng</keyname><forenames>Jay</forenames></author></authors><title>Constructions of Optical Queues With a Limited Number of
  Recirculations--Part II: Optimal Constructions</title><categories>cs.IT math.IT math.NT</categories><comments>135 pages; 52 figures; this paper is submitted to IEEE Transactions
  on Information Theory for possible publication.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main problems in all-optical packet-switched networks is the lack
of optical buffers, and one feasible technology for the constructions of
optical buffers is to use optical crossbar Switches and fiber Delay Lines
(SDL). In this two-part paper, we consider SDL constructions of optical queues
with a limited number of recirculations through the optical switches and the
fiber delay lines. Such a problem arises from practical feasibility
considerations. In Part I, we have proposed a class of greedy constructions for
certain types of optical queues, including linear compressors, linear
decompressors, and 2-to-1 FIFO multiplexers, and have shown that every optimal
construction among our previous constructions of these types of optical queues
under the constraint of a limited number of recirculations must be a greedy
construction. In Part II, the present paper, we further show that there are at
most two optimal constructions and give a simple algorithm to obtain the
optimal construction(s). The main idea in Part II is to use \emph{pairwise
comparison} to remove a sequence $\dbf_1^M\in \Gcal_{M,k}$ such that
$B(\dbf_1^M;k)&lt;B({\dbf'}_1^M;k)$ for some ${\dbf'}_1^M\in \Gcal_{M,k}$. To our
surprise, the simple algorithm for obtaining the optimal construction(s) is
related to the well-known \emph{Euclid's algorithm} for finding the greatest
common divisor (gcd) of two integers. In particular, we show that if
$\gcd(M,k)=1$, then there is only one optimal construction; if $\gcd(M,k)=2$,
then there are two optimal constructions; and if $\gcd(M,k)\geq 3$, then there
are at most two optimal constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4286</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4286</id><created>2010-04-24</created><updated>2010-04-27</updated><authors><author><keyname>Br&#xe1;zdil</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Esparza</keyname><forenames>Javier</forenames></author><author><keyname>Kiefer</keyname><forenames>Stefan</forenames></author><author><keyname>Luttenberger</keyname><forenames>Michael</forenames></author></authors><title>Space-efficient scheduling of stochastically generated tasks</title><categories>cs.PF</categories><comments>technical report accompanying an ICALP'10 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of scheduling tasks for execution by a processor when
the tasks can stochastically generate new tasks. Tasks can be of different
types, and each type has a fixed, known probability of generating other tasks.
We present results on the random variable S^sigma modeling the maximal space
needed by the processor to store the currently active tasks when acting under
the scheduler sigma. We obtain tail bounds for the distribution of S^sigma for
both offline and online schedulers, and investigate the expected value of
S^sigma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4296</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4296</id><created>2010-04-24</created><authors><author><keyname>Cartledge</keyname><forenames>Charles L.</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>Analysis of Graphs for Digital Preservation Suitability</title><categories>cs.DL</categories><comments>Accepted for publication at HyperText 2010 ACM Conference on
  Hypertext and Hypermedia, 10 pages, 3 figures</comments><acm-class>H.4; I.6.8; E.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the use of autonomically created small-world graphs as a
framework for the long term storage of digital objects on the Web in a
potentially hostile environment. We attack the classic Erdos - Renyi random,
Barab'asi and Albert power law, Watts - Strogatz small world and our
Unsupervised Small-World (USW) graphs using different attacker strategies and
report their respective robustness. Using different attacker profiles, we
construct a game where the attacker is allowed to use a strategy of his choice
to remove a percentage of each graph's elements. The graph is then allowed to
repair some portion of its self. We report on the number of alternating attack
and repair turns until either the graph is disconnected, or the game exceeds
the number of permitted turns. Based on our analysis, an attack strategy that
focuses on removing the vertices with the highest betweenness value is most
advantageous to the attacker. Power law graphs can become disconnected with the
removal of a single edge; random graphs with the removal of as few as 1% of
their vertices, small-world graphs with the removal of 14% vertices, and USW
with the removal of 17% vertices. Watts - Strogatz small-world graphs are more
robust and resilient than random or power law graphs. USW graphs are more
robust and resilient than small world graphs. A graph of USW connected web
objects (WOs) filled with data could outlive the individuals and institutions
that created the data in an environment where WOs are lost due to random
failures or directed attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4297</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4297</id><created>2010-04-24</created><authors><author><keyname>Daneshrad</keyname><forenames>Pengkai Zhao Babak</forenames></author></authors><title>Bounds on the Maximum Number of Concurrent Links in MIMO Ad Hoc Networks
  with QoS Constraints</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple-Input Multiple-Output (MIMO) based Medium Access Control (MAC)
protocols have received a good deal of attention as researchers look to enhance
overall performance of Ad Hoc networks by leveraging multi antenna enabled
nodes. To date such MAC protocols have been evaluated through comparative
simulation based studies that report on the number of concurrent links the
protocol can support. However, a bound on the maximum number of concurrent
links (MNCL) that a MIMO based MAC protocol should strive to achieve has
hitherto been unavailable. In this paper we present a theoretical formulation
for calculating the bound on the MNCL in a Mobile Ad Hoc Network (MANET) where
the nodes have multiple antenna capability, while guaranteeing a minimum
Quality of Service (QoS). In an attempt to make our findings as practical and
realistic as possible, the study incorporates models for the following PHY
layer and channel dependent elements: (a) path loss and fast fading effects, in
order to accurately model adjacent link interference; (b) a Minimum Mean
Squared Error (MMSE) based detector in the receiver which provides a balance
between completely nulling of neighboring interference and hardware complexity.
In calculating the bound on the MNCL our work also delivers the optimal power
control solution for the network as well as the optimal link selection. The
results are readily applicable to MIMO systems using Receive Diversity, Space
Time Block Coding (STBC), and Transmit Beamforming and show that with a 4
element antenna system, as much as 3x improvement in the total number of
concurrent links can be achieved relative to a SISO based network. The results
also show diminishing improvement as the number of antennas is increased beyond
4, and the maximum allowable transmit power is increased beyond 10 dBm (for the
simulated parameters).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4299</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4299</id><created>2010-04-24</created><authors><author><keyname>Cadambe</keyname><forenames>Viveck R.</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author><author><keyname>Maleki</keyname><forenames>Hamed</forenames></author></authors><title>Distributed Data Storage with Minimum Storage Regenerating Codes - Exact
  and Functional Repair are Asymptotically Equally Efficient</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a set up where a file of size M is stored in n distributed
storage nodes, using an (n,k) minimum storage regenerating (MSR) code, i.e., a
maximum distance separable (MDS) code that also allows efficient exact-repair
of any failed node. The problem of interest in this paper is to minimize the
repair bandwidth B for exact regeneration of a single failed node, i.e., the
minimum data to be downloaded by a new node to replace the failed node by its
exact replica. Previous work has shown that a bandwidth of B=[M(n-1)]/[k(n-k)]
is necessary and sufficient for functional (not exact) regeneration. It has
also been shown that if k &lt; = max(n/2, 3), then there is no extra cost of exact
regeneration over functional regeneration. The practically relevant setting of
low-redundancy, i.e., k/n&gt;1/2 remains open for k&gt;3 and it has been shown that
there is an extra bandwidth cost for exact repair over functional repair in
this case. In this work, we adopt into the distributed storage context an
asymptotically optimal interference alignment scheme previously proposed by
Cadambe and Jafar for large wireless interference networks. With this scheme we
solve the problem of repair bandwidth minimization for (n,k) exact-MSR codes
for all (n,k) values including the previously open case of k &gt; \max(n/2,3). Our
main result is that, for any (n,k), and sufficiently large file sizes, there is
no extra cost of exact regeneration over functional regeneration in terms of
the repair bandwidth per bit of regenerated data. More precisely, we show that
in the limit as M approaches infinity, the ratio B/M = (n-1)/(k(n-k))$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4308</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4308</id><created>2010-04-24</created><authors><author><keyname>Taheri</keyname><forenames>Omid</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author></authors><title>Segmented compressed sampling for analog-to-information conversion:
  Method and performance analysis</title><categories>cs.IT math.IT stat.AP</categories><comments>32 pages, 5 figures, submitted to the IEEE Transactions on Signal
  Processing in April 2010</comments><journal-ref>O. Taheri and S.A. Vorobyov, &quot;Segmented compressed sampling for
  analog-to-information conversion: Method and performance analysis,&quot; IEEE
  Trans. Signal Processing, vol. 59, no. 2, pp. 554-572, Feb. 2011</journal-ref><doi>10.1109/TSP.2010.2091411</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new segmented compressed sampling method for analog-to-information
conversion (AIC) is proposed. An analog signal measured by a number of parallel
branches of mixers and integrators (BMIs), each characterized by a specific
random sampling waveform, is first segmented in time into $M$ segments. Then
the sub-samples collected on different segments and different BMIs are reused
so that a larger number of samples than the number of BMIs is collected. This
technique is shown to be equivalent to extending the measurement matrix, which
consists of the BMI sampling waveforms, by adding new rows without actually
increasing the number of BMIs. We prove that the extended measurement matrix
satisfies the restricted isometry property with overwhelming probability if the
original measurement matrix of BMI sampling waveforms satisfies it. We also
show that the signal recovery performance can be improved significantly if our
segmented AIC is used for sampling instead of the conventional AIC. Simulation
results verify the effectiveness of the proposed segmented compressed sampling
method and the validity of our theoretical studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4317</identifier>
 <datestamp>2015-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4317</id><created>2010-04-24</created><authors><author><keyname>Bateni</keyname><forenames>MohammadHossein</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>Immorlica</keyname><forenames>Nicole</forenames></author><author><keyname>Mahini</keyname><forenames>Hamid</forenames></author></authors><title>The cooperative game theory foundations of network bargaining games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study bargaining games between suppliers and manufacturers in a network
context. Agents wish to enter into contracts in order to generate surplus which
then must be divided among the participants. Potential contracts and their
surplus are represented by weighted edges in our bipartite network. Each agent
in the market is additionally limited by a capacity representing the number of
contracts which he or she may undertake. When all agents are limited to just
one contract each, prior research applied natural generalizations of the Nash
bargaining solution to the networked setting, defined the new solution concepts
of stable and balanced, and characterized the resulting bargaining outcomes. We
simplify and generalize these results to a setting in which participants in
only one side of the market are limited to one contract each. The heart of our
results uses a linear-programming formulation to establish a novel connection
between well-studied cooperative game theory concepts (such as core and
prekernel) and the solution concepts of stable and balanced defined for the
bargaining games. This immediately implies one can take advantage of the
results and algorithms in cooperative game theory to reproduce results such as
those of Azar et al. [1] and Kleinberg and Tardos [29] and also generalize them
to our setting. The cooperative-game-theoretic connection also inspires us to
refine our solution space using standard solution concepts from that literature
such as nucleolus and lexicographic kernel. The nucleolus is particularly
attractive as it is unique, always exists, and is supported by experimental
data in the network bargaining literature. Guided by algorithms from
cooperative game theory, we show how to compute the nucleolus by pruning and
iteratively solving a natural linear-programming formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4320</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4320</id><created>2010-04-24</created><updated>2010-12-27</updated><authors><author><keyname>Saeedi</keyname><forenames>Mehdi</forenames></author><author><keyname>Zamani</keyname><forenames>Morteza Saheb</forenames></author><author><keyname>Sedighi</keyname><forenames>Mehdi</forenames></author><author><keyname>Sasanian</keyname><forenames>Zahra</forenames></author></authors><title>Reversible Circuit Synthesis Using a Cycle-Based Approach</title><categories>quant-ph cs.ET</categories><comments>25 pages, 21 figures, 2 tables</comments><journal-ref>ACM Journal of Emerging Technologies in Computing Systems, Vol. 6,
  Issue 4, Article 13, December 2010</journal-ref><doi>10.1145/1877745.1877747</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible logic has applications in various research areas including signal
processing, cryptography and quantum computation. In this paper, direct
NCT-based synthesis of a given $k$-cycle in a cycle-based synthesis scenario is
examined. To this end, a set of seven building blocks is proposed that reveals
the potential of direct synthesis of a given permutation to reduce both quantum
cost and average runtime. To synthesize a given large cycle, we propose a
decomposition algorithm to extract the suggested building blocks from the input
specification. Then, a synthesis method is introduced which uses the building
blocks and the decomposition algorithm. Finally, a hybrid synthesis framework
is suggested which uses the proposed cycle-based synthesis method in
conjunction with one of the recent NCT-based synthesis approaches which is
based on Reed-Muller (RM) spectra. The time complexity and the effectiveness of
the proposed synthesis approach are analyzed in detail. Our analyses show that
the proposed hybrid framework leads to a better quantum cost in the worst-case
scenario compared to the previously presented methods. The proposed framework
always converges and typically synthesizes a given specification very fast
compared to the available synthesis algorithms. Besides, the quantum costs of
benchmark functions are improved about 20% on average (55% in the best case).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4326</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4326</id><created>2010-04-25</created><authors><author><keyname>Kumar</keyname><forenames>Udayan</forenames></author><author><keyname>Thakur</keyname><forenames>Gautam</forenames></author><author><keyname>Helmy</keyname><forenames>Ahmed</forenames></author></authors><title>PROTECT: Proximity-based Trust-advisor using Encounters for Mobile
  Societies</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many interactions between network users rely on trust, which is becoming
particularly important given the security breaches in the Internet today. These
problems are further exacerbated by the dynamics in wireless mobile networks.
In this paper we address the issue of trust advisory and establishment in
mobile networks, with application to ad hoc networks, including DTNs. We
utilize encounters in mobile societies in novel ways, noticing that mobility
provides opportunities to build proximity, location and similarity based trust.
Four new trust advisor filters are introduced - including encounter frequency,
duration, behavior vectors and behavior matrices - and evaluated over an
extensive set of real-world traces collected from a major university. Two sets
of statistical analyses are performed; the first examines the underlying
encounter relationships in mobile societies, and the second evaluates DTN
routing in mobile peer-to-peer networks using trust and selfishness models. We
find that for the analyzed trace, trust filters are stable in terms of growth
with time (3 filters have close to 90% overlap of users over a period of 9
weeks) and the results produced by different filters are noticeably different.
In our analysis for trust and selfishness model, our trust filters largely undo
the effect of selfishness on the unreachability in a network. Thus improving
the connectivity in a network with selfish nodes.
  We hope that our initial promising results open the door for further research
on proximity-based trust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4329</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4329</id><created>2010-04-25</created><authors><author><keyname>Shtok</keyname><forenames>Joseph</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author></authors><title>Analysis of Basis Pursuit Via Capacity Sets</title><categories>cs.NA cs.DS</categories><journal-ref>Journal of Fourier Analysis and Applications, Volume 14, Numbers
  5-6, December 2008, pp. 688-711</journal-ref><doi>10.1007/s00041-008-9036-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding the sparsest solution $\alpha$ for an under-determined linear system
of equations $D\alpha=s$ is of interest in many applications. This problem is
known to be NP-hard. Recent work studied conditions on the support size of
$\alpha$ that allow its recovery using L1-minimization, via the Basis Pursuit
algorithm. These conditions are often relying on a scalar property of $D$
called the mutual-coherence. In this work we introduce an alternative set of
features of an arbitrarily given $D$, called the &quot;capacity sets&quot;. We show how
those could be used to analyze the performance of the basis pursuit, leading to
improved bounds and predictions of performance. Both theoretical and numerical
methods are presented, all using the capacity values, and shown to lead to
improved assessments of the basis pursuit success in finding the sparest
solution of $D\alpha=s$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4334</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4334</id><created>2010-04-25</created><authors><author><keyname>Ahmadi</keyname><forenames>Hadi</forenames></author><author><keyname>Safavi-Naini</keyname><forenames>Reihaneh</forenames></author></authors><title>New Results on Secret Key Establishment over a Pair of Broadcast
  Channels</title><categories>cs.IT cs.CR math.IT</categories><comments>18 pages, 4 figures, submitted to the 2010 International Symposium on
  Information Theory and its Applications (ISITA2010).</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of Secret Key Establishment (SKE) over a pair of independent
Discrete Memoryless Broadcast Channels (DMBCs) has already been studied in
\cite{Ah10}, where we provided lower and upper bounds on the secret-key
capacity. In this paper, we study the above setup under each of the following
two cases: (1) the DMBCs have secrecy potential, and (2) the DMBCs are
stochastically degraded with independent channels. In the former case, we
propose a simple SKE protocol based on a novel technique, called Interactive
Channel Coding (ICC), and prove that it achieves the lower bound. In the latter
case, we give a simplified expression for the lower bound and prove a
single-letter capacity formula under the condition that one of the legitimate
parties sends only i.i.d. variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4342</identifier>
 <datestamp>2011-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4342</id><created>2010-04-25</created><updated>2010-07-22</updated><authors><author><keyname>Slota</keyname><forenames>Martin</forenames></author><author><keyname>Leite</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>Towards Closed World Reasoning in Dynamic Open Worlds (Extended Version)</title><categories>cs.AI</categories><comments>40 pages; an extended version of the article published in Theory and
  Practice of Logic Programming, 10 (4-6): 547 - 564, July. Copyright 2010
  Cambridge University Press</comments><journal-ref>Theory and Practice of Logic Programming, 10(4-6), 547-564, 2010</journal-ref><doi>10.1017/S147106841000027X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need for integration of ontologies with nonmonotonic rules has been
gaining importance in a number of areas, such as the Semantic Web. A number of
researchers addressed this problem by proposing a unified semantics for hybrid
knowledge bases composed of both an ontology (expressed in a fragment of
first-order logic) and nonmonotonic rules. These semantics have matured over
the years, but only provide solutions for the static case when knowledge does
not need to evolve. In this paper we take a first step towards addressing the
dynamics of hybrid knowledge bases. We focus on knowledge updates and,
considering the state of the art of belief update, ontology update and rule
update, we show that current solutions are only partial and difficult to
combine. Then we extend the existing work on ABox updates with rules, provide a
semantics for such evolving hybrid knowledge bases and study its basic
properties. To the best of our knowledge, this is the first time that an update
operator is proposed for hybrid knowledge bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4356</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4356</id><created>2010-04-25</created><authors><author><keyname>Thakur</keyname><forenames>Gautam S.</forenames></author><author><keyname>Sharma</keyname><forenames>Mukul</forenames></author><author><keyname>Helmy</keyname><forenames>Ahmed</forenames></author></authors><title>SHIELD: Social sensing and Help In Emergency using mobiLe Devices</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  School and College campuses face a perceived threat of violent crimes and
require a realistic plan against unpredictable emergencies and disasters.
Existing emergency systems (e.g., 911, campus-wide alerts) are quite useful,
but provide delayed response (often tens of minutes) and do not utilize
proximity or locality. There is a need to augment such systems with
proximity-based systems for more immediate response to attempt to prevent and
deter crime. In this paper we propose SHIELD, an on-campus emergency rescue and
alert management service. It is a fully distributed infrastructure-less
platform based on proximity-enabled trust and cooperation. It relies on
localized responses, sent using Bluetooth and/or WiFi on the fly to achieve
minimal response time and maximal availability thereby augmenting the
traditional notion of emergency services. Analysis of campus crime statistics
and WLAN traces surprisingly show a strong positive correlation (over 55%)
between on-campus crime statistics and spatio-temporal density distribution of
on-campus mobile users. This result provides a motivation to develop such
platform and points to the promise in reducing crime incidences. We also show
an implementation of a prototype application to be used in such scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4357</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4357</id><created>2010-04-25</created><authors><author><keyname>Born</keyname><forenames>Kenton</forenames></author></authors><title>Browser-Based Covert Data Exfiltration</title><categories>cs.CR</categories><comments>In Proceedings of the 9th Annual Security Conference, Las Vegas, NV,
  April 7-8, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current best practices heavily control user permissions on network systems.
This effectively mitigates many insider threats regarding the collection and
exfiltration of data. Many methods of covert communication involve crafting
custom packets, typically requiring both the necessary software and elevated
privileges on the system. By exploiting the functionality of a browser, covert
channels for data exfiltration may be created without additional software or
user privileges. This paper explores novel methods of using a browser's
JavaScript engine to exfiltrate documents over the Domain Name System (DNS)
protocol without sending less covert Hypertext Transfer Protocol (HTTP)
requests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4358</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4358</id><created>2010-04-25</created><authors><author><keyname>Born</keyname><forenames>Kenton</forenames></author><author><keyname>Gustafson</keyname><forenames>David</forenames></author></authors><title>Detecting DNS Tunnels Using Character Frequency Analysis</title><categories>cs.CR</categories><comments>In Proceedings of the 9th Annual Security Conference, Las Vegas, NV,
  April 7-8, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-bandwidth covert channels pose significant risks to sensitive and
proprietary information inside company networks. Domain Name System (DNS)
tunnels provide a means to covertly infiltrate and exfiltrate large amounts of
information passed network boundaries. This paper explores the possibility of
detecting DNS tunnels by analyzing the unigram, bigram, and trigram character
frequencies of domains in DNS queries and responses. It is empirically shown
how domains follow Zipf's law in a similar pattern to natural languages,
whereas tunneled traffic has more evenly distributed character frequencies.
This approach allows tunnels to be detected across multiple domains, whereas
previous methods typically concentrate on monitoring point to point systems.
Anomalies are quickly discovered when tunneled traffic is compared to the
character frequency fingerprint of legitimate domain traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4359</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4359</id><created>2010-04-25</created><authors><author><keyname>Born</keyname><forenames>Kenton</forenames></author><author><keyname>Gustafson</keyname><forenames>David</forenames></author></authors><title>NgViz: Detecting DNS Tunnels through N-Gram Visualization and
  Quantitative Analysis</title><categories>cs.CR</categories><comments>In Proceedings of the the 6th Annual Cyber Security and Information
  Intelligence Research Workshop, Oak Ridge, TN, April 21-23, 2010</comments><acm-class>C.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduced NgViz, a tool that examines DNS traffic and shows
anomalies in n-gram frequencies. This is accomplished by comparing input files
against a fingerprint of legitimate traffic. Both quantitative analysis and
visual aids are provided that allow the user to make determinations about the
legitimacy of the DNS traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4361</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4361</id><created>2010-04-25</created><authors><author><keyname>Bulitko</keyname><forenames>Valeriy</forenames></author></authors><title>Reduction of behavior of additive cellular automata on groups</title><categories>nlin.CG cs.NE</categories><comments>32 pages, 20 figures</comments><msc-class>37B15, 68Q80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A class of additive cellular automata (ACA) on a finite group is defined by
an index-group $\m g$ and a finite field $\m F_p$ for a prime modulus $p$
\cite{Bul_arch_1}. This paper deals mainly with ACA on infinite commutative
groups and direct products of them with some non commutative $p$-groups. It
appears that for all abelian groups, the rules and initial states with finite
supports define behaviors which being restricted to some infinite regular
series of time moments become significantly simplified. In particular, for free
abelian groups with $n$ generators states $V^{[t]}$ of ACA with a rule $R$ at
time moments $t=p^k,k&gt;k_0,$ can be viewed as $||R||$ copies of initial state
$V^{[0]}$ moving through an $n$-dimensional Euclidean space. That is the
behavior is similar to gliders from J.Conway's automaton {\sl Life}. For some
other special infinite series of time moments the automata states approximate
self-similar structures and the approximation becomes better with time. An
infinite class $\mathrm{DHC}(\mbf S,\theta)$ of non-commutative $p$-groups is
described which in particular includes quaternion and dihedral $p$-groups. It
is shown that the simplification of behaviors takes place as well for direct
products of non-commutative groups from the class $\mathrm{DHC}(\mbf S,\theta)$
with commutative groups. Finally, an automaton on a non-commutative group is
constructed such that its behavior at time moments $2^k,k\ge2,$ is similar to a
glider gun. It is concluded that ACA on non-commutative groups demonstrate more
diverse variety of behaviors comparing to ACA on commutative groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4371</identifier>
 <datestamp>2011-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4371</id><created>2010-04-25</created><updated>2011-10-06</updated><authors><author><keyname>Ding</keyname><forenames>Jian</forenames></author><author><keyname>Lee</keyname><forenames>James R.</forenames></author><author><keyname>Peres</keyname><forenames>Yuval</forenames></author></authors><title>Cover times, blanket times, and majorizing measures</title><categories>math.PR cs.DS math.MG</categories><comments>Revisions to Section 3; added and rearranged some material on the
  majorizing measures theory</comments><msc-class>60J10, 60G60, 60G15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We exhibit a strong connection between cover times of graphs, Gaussian
processes, and Talagrand's theory of majorizing measures. In particular, we
show that the cover time of any graph $G$ is equivalent, up to universal
constants, to the square of the expected maximum of the Gaussian free field on
$G$, scaled by the number of edges in $G$. This allows us to resolve a number
of open questions. We give a deterministic polynomial-time algorithm that
computes the cover time to within an O(1) factor for any graph, answering a
question of Aldous and Fill (1994). We also positively resolve the blanket time
conjectures of Winkler and Zuckerman (1996), showing that for any graph, the
blanket and cover times are within an O(1) factor. The best previous
approximation factor for both these problems was $O((\log \log n)^2)$ for
$n$-vertex graphs, due to Kahn, Kim, Lovasz, and Vu (2000).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4373</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4373</id><created>2010-04-25</created><authors><author><keyname>Shtok</keyname><forenames>Joseph</forenames></author><author><keyname>Zibulevsky</keyname><forenames>Michael</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author></authors><title>Spatially-Adaptive Reconstruction in Computed Tomography Based on
  Statistical Learning</title><categories>cs.CV</categories><comments>Submitted to IEEE Transactions on Image Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a direct reconstruction algorithm for Computed Tomography, based
on a local fusion of a few preliminary image estimates by means of a non-linear
fusion rule. One such rule is based on a signal denoising technique which is
spatially adaptive to the unknown local smoothness. Another, more powerful
fusion rule, is based on a neural network trained off-line with a high-quality
training set of images. Two types of linear reconstruction algorithms for the
preliminary images are employed for two different reconstruction tasks. For an
entire image reconstruction from full projection data, the proposed scheme uses
a sequence of Filtered Back-Projection algorithms with a gradually growing
cut-off frequency. To recover a Region Of Interest only from local projections,
statistically-trained linear reconstruction algorithms are employed. Numerical
experiments display the improvement in reconstruction quality when compared to
linear reconstruction algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4383</identifier>
 <datestamp>2010-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4383</id><created>2010-04-25</created><updated>2010-07-07</updated><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Patitz</keyname><forenames>Matthew J.</forenames></author><author><keyname>Schweller</keyname><forenames>Robert T.</forenames></author><author><keyname>Summers</keyname><forenames>Scott M.</forenames></author></authors><title>Self-Assembly of Arbitrary Shapes Using RNAse Enzymes: Meeting the
  Kolmogorov Bound with Small Scale Factor (extended abstract)</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a model of algorithmic self-assembly of geometric shapes out of
square Wang tiles studied in SODA 2010, in which there are two types of tiles
(e.g., constructed out of DNA and RNA material) and one operation that destroys
all tiles of a particular type (e.g., an RNAse enzyme destroys all RNA tiles).
We show that a single use of this destruction operation enables much more
efficient construction of arbitrary shapes. In particular, an arbitrary shape
can be constructed using an asymptotically optimal number of distinct tile
types (related to the shape's Kolmogorov complexity), after scaling the shape
by only a logarithmic factor. By contrast, without the destruction operation,
the best such result has a scale factor at least linear in the size of the
shape, and is connected only by a spanning tree of the scaled tiles. We also
characterize a large collection of shapes that can be constructed efficiently
without any scaling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4398</identifier>
 <datestamp>2011-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4398</id><created>2010-04-25</created><updated>2011-04-01</updated><authors><author><keyname>Kim</keyname><forenames>Jong Min</forenames></author><author><keyname>Lee</keyname><forenames>Ok Kyun</forenames></author><author><keyname>Ye</keyname><forenames>Jong Chul</forenames></author></authors><title>Compressive MUSIC: A Missing Link Between Compressive Sensing and Array
  Signal Processing</title><categories>cs.IT math.IT</categories><comments>33 pages, 7 figures, Parts of this work were presented on 14/04/2010
  at the SIAM Conference on Imaging Science, Chicago, USA, with the title
  &quot;Multiple measurement vector problem with subspace-based algorithm&quot;</comments><msc-class>94A20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multiple measurement vector (MMV) problem addresses the identification of
unknown input vectors that share common sparse support. Even though MMV
problems had been traditionally addressed within the context of sensor array
signal processing, the recent trend is to apply compressive sensing (CS) due to
its capability to estimate sparse support even with an insufficient number of
snapshots, in which case classical array signal processing fails. However, CS
guarantees the accurate recovery in a probabilistic manner, which often shows
inferior performance in the regime where the traditional array signal
processing approaches succeed. The apparent dichotomy between the {\em
probabilistic} CS and {\em deterministic} sensor array signal processing have
not been fully understood. The main contribution of the present article is a
unified approach that unveils a {missing link} between CS and array signal
processing. The new algorithm, which we call {\em compressive MUSIC},
identifies the parts of support using CS, after which the remaining supports
are estimated using a novel generalized MUSIC criterion. Using a large system
MMV model, we show that our compressive MUSIC requires a smaller number of
sensor elements for accurate support recovery than the existing CS methods and
can approach the optimal $l_0$-bound with finite number of snapshots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4420</identifier>
 <datestamp>2013-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4420</id><created>2010-04-26</created><authors><author><keyname>Angel</keyname><forenames>Eric</forenames></author><author><keyname>Bampis</keyname><forenames>Evripidis</forenames></author><author><keyname>Pollatos</keyname><forenames>Gerasimos G.</forenames></author><author><keyname>Zissimopoulos</keyname><forenames>Vassilis</forenames></author></authors><title>Optimal Data Placement on Networks With Constant Number of Clients</title><categories>cs.DS</categories><doi>10.1016/j.tcs.2013.03.025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce optimal algorithms for the problems of data placement (DP) and
page placement (PP) in networks with a constant number of clients each of which
has limited storage availability and issues requests for data objects. The
objective for both problems is to efficiently utilize each client's storage
(deciding where to place replicas of objects) so that the total incurred access
and installation cost over all clients is minimized. In the PP problem an extra
constraint on the maximum number of clients served by a single client must be
satisfied. Our algorithms solve both problems optimally when all objects have
uniform lengths. When objects lengths are non-uniform we also find the optimal
solution, albeit a small, asymptotically tight violation of each client's
storage size by $\epsilon$lmax where lmax is the maximum length of the objects
and $\epsilon$ some arbitrarily small positive constant. We make no assumption
on the underlying topology of the network (metric, ultrametric etc.), thus
obtaining the first non-trivial results for non-metric data placement problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4421</identifier>
 <datestamp>2010-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4421</id><created>2010-04-26</created><updated>2010-04-28</updated><authors><author><keyname>Cesa-Bianchi</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>Efficient Learning with Partially Observed Attributes</title><categories>cs.LG</categories><comments>This is a full version of the paper appearing in The 27th
  International Conference on Machine Learning (ICML 2010)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe and analyze efficient algorithms for learning a linear predictor
from examples when the learner can only view a few attributes of each training
example. This is the case, for instance, in medical research, where each
patient participating in the experiment is only willing to go through a small
number of tests. Our analysis bounds the number of additional examples
sufficient to compensate for the lack of full information on each training
example. We demonstrate the efficiency of our algorithms by showing that when
running on digit recognition data, they obtain a high prediction accuracy even
when the learner gets to see only four pixels of each image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4428</identifier>
 <datestamp>2010-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4428</id><created>2010-04-26</created><updated>2010-04-27</updated><authors><author><keyname>Gluskin</keyname><forenames>Emanuel</forenames></author></authors><title>An approximate analytical (structural) superposition in terms of two, or
  more, &quot;alfa&quot;-circuits of the same topology: Pt. 2 - the &quot;internal circuit
  mechanism&quot;</title><categories>cs.OH</categories><comments>This is the Second Part after the recently posted Part One. The
  circuit mechanism for the good precision of the &quot;analytical superposition&quot; is
  explained, and it is noted that the &quot;f-connection&quot; presents a very good field
  for application of Tellegen's theorem. 22 pages, 5 figures 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the second part, after [1], of the research devoted to analysis of
1-ports composed of similar conductors (&quot;f-circuits&quot;) described by the
characteristic i = f(v) of a polynomial type. This analysis is performed by
means of the power-law &quot;alfa&quot;-circuits&quot; introduced in [2], for which f(v) ~
v^&quot;alfa&quot;. The f-circuits are constructed from the &quot;alfa&quot;-circuits of the same
topology, with the proper &quot;alfa&quot;, so that the given topology is kept, and 'f'
is an additive function of the connection. Explaining the situation described
in detail in [1], we note and analyze a simple &quot;circuit mechanism&quot; that causes
the difference between the input current of the f-circuit and the sum of the
input currents of the f-circuits before the composition to be relatively small.
The case of two degrees, f(v) = Dmv^m + Dnv^n, m unequal n, is treated in the
main proofs. Some simulations are presented, and some boundaries for the error
of the superposition are found. The cases of f(.) being a polynomial of the
third or fourth degrees are finally briefly considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4431</identifier>
 <datestamp>2010-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4431</id><created>2010-04-26</created><updated>2010-06-30</updated><authors><author><keyname>Treibig</keyname><forenames>Jan</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>LIKWID: A lightweight performance-oriented tool suite for x86 multicore
  environments</title><categories>cs.DC cs.PF</categories><comments>10 pages, 11 figures. Some clarifications and corrections</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exploiting the performance of today's processors requires intimate knowledge
of the microarchitecture as well as an awareness of the ever-growing complexity
in thread and cache topology. LIKWID is a set of command-line utilities that
addresses four key problems: Probing the thread and cache topology of a
shared-memory node, enforcing thread-core affinity on a program, measuring
performance counter metrics, and toggling hardware prefetchers. An API for
using the performance counting features from user code is also included. We
clearly state the differences to the widely used PAPI interface. To demonstrate
the capabilities of the tool set we show the influence of thread pinning on
performance using the well-known OpenMP STREAM triad benchmark, and use the
affinity and hardware counter tools to study the performance of a stencil code
specifically optimized to utilize shared caches on multicore chips.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4432</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4432</id><created>2010-04-26</created><authors><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author></authors><title>Throughput-Delay-Reliability Tradeoff with ARQ in Wireless Ad Hoc
  Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delay-reliability (D-R), and throughput-delay-reliability (T-D-R) tradeoffs
in an ad hoc network are derived for single hop and multi-hop transmission with
automatic repeat request (ARQ) on each hop. The delay constraint is modeled by
assuming that each packet is allowed at most $D$ retransmissions end-to-end,
and the reliability is defined as the probability that the packet is
successfully decoded in at most $D$ retransmissions. The throughput of the ad
hoc network is characterized by the transmission capacity, which is defined to
be the maximum allowable density of transmitting nodes satisfying a per
transmitter receiver rate, and an outage probability constraint, multiplied
with the rate of transmission and the success probability. Given an end-to-end
retransmission constraint of $D$, the optimal allocation of the number of
retransmissions allowed at each hop is derived that maximizes a lower bound on
the transmission capacity. Optimizing over the number of hops, single hop
transmission is shown to be optimal for maximizing a lower bound on the
transmission capacity in the sparse network regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4437</identifier>
 <datestamp>2015-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4437</id><created>2010-04-26</created><updated>2010-08-17</updated><authors><author><keyname>Moon</keyname><forenames>Sungwook</forenames></author><author><keyname>Helmy</keyname><forenames>Ahmed</forenames></author></authors><title>Understanding periodicity and regularity of nodal encounters in mobile
  networks: A spectral analysis</title><categories>cs.NI</categories><comments>Shorter version has been accepted to IEEE GlobeCom 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Study on human mobility is gaining increasing attention from the research
community with its multiple applications to use in mobile networks,
particularly for the purpose of message delivery in the Delay Tolerant
Networks. To better understand the potential of mobile nodes as message relays,
our study investigates the encounter pattern of mobile devices. Specifically,
we examine the extensive network traces that reflect mobility of communication
devices. We analyze the periodicity in encounter pattern by using power
spectral analysis. Strong periodicity was observed among rarely encountering
mobile nodes while the periodicity was weaker among frequently encountering
nodes. Further, we present a method to search regularly encountering pairs and
discuss the findings. To our knowledge, we are the first to analyze the
periodicity of encounter pattern with large network traces, which is a critical
basis for designing an efficient delivery scheme using mobile nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4438</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4438</id><created>2010-04-26</created><authors><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author><author><keyname>Wu</keyname><forenames>Yunnan</forenames></author><author><keyname>Suh</keyname><forenames>Changho</forenames></author></authors><title>A Survey on Network Codes for Distributed Storage</title><categories>cs.IT cs.DC cs.NI math.IT</categories><comments>13 pages, 11 figures.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed storage systems often introduce redundancy to increase
reliability. When coding is used, the repair problem arises: if a node storing
encoded information fails, in order to maintain the same level of reliability
we need to create encoded information at a new node. This amounts to a partial
recovery of the code, whereas conventional erasure coding focuses on the
complete recovery of the information from a subset of encoded packets. The
consideration of the repair network traffic gives rise to new design
challenges. Recently, network coding techniques have been instrumental in
addressing these challenges, establishing that maintenance bandwidth can be
reduced by orders of magnitude compared to standard erasure codes. This paper
provides an overview of the research results on this topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4444</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4444</id><created>2010-04-26</created><authors><author><keyname>S.</keyname><forenames>Ramesh Babu H.</forenames></author><author><keyname>Gowrishankar</keyname></author><author><keyname>S</keyname><forenames>Satyanarayana P.</forenames></author></authors><title>An Intelligent Call Admission Control Decision Mechanism for Wireless
  Networks</title><categories>cs.NI</categories><comments>Journal of Computing online at
  https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Call admission control (CAC) is one of the Radio Resource Management
(RRM) techniques plays instrumental role in ensuring the desired Quality of
Service (QoS) to the users working on different applications which have
diversified nature of QoS requirements. This paper proposes a fuzzy neural
approach for call admission control in a multi class traffic based Next
Generation Wireless Networks (NGWN). The proposed Fuzzy Neural Call Admission
Control (FNCAC) scheme is an integrated CAC module that combines the linguistic
control capabilities of the fuzzy logic controller and the learning
capabilities of the neural networks .The model is based on Recurrent Radial
Basis Function Networks (RRBFN) which have better learning and adaptability
that can be used to develop the intelligent system to handle the incoming
traffic in the heterogeneous network environment. The proposed FNCAC can
achieve reduced call blocking probability keeping the resource utilisation at
an optimal level. In the proposed algorithm we have considered three classes of
traffic having different QoS requirements. We have considered the heterogeneous
network environment which can effectively handle this traffic. The traffic
classes taken for the study are Conversational traffic, Interactive traffic and
back ground traffic which are with varied QoS parameters. The paper also
presents the analytical model for the CAC .The paper compares the call blocking
probabilities for all the three types of traffic in both the models. The
simulation results indicate that compared to Fuzzy logic based CAC,
Conventional CAC, The simulation results are optimistic and indicates that the
proposed FNCAC algorithm performs better where the call blocking probability is
minimal when compared to other two methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4445</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4445</id><created>2010-04-26</created><authors><author><keyname>Abdulla</keyname><forenames>Sozan</forenames></author></authors><title>New Visual Cryptography Algorithm For Colored Image</title><categories>cs.CR</categories><comments>Journal of Computing online at
  https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual Cryptography is a special encryption technique to hide information in
images, which divide secret image into multiple layers. Each layer holds some
information. The receiver aligns the layers and the secret information is
revealed by human vision without any complex computation. The proposed
algorithm is for color image, that presents a system which takes four pictures
as an input and generates three images which correspond to three of the four
input pictures. The decoding requires only selecting some subset of these 3
images, making transparencies of them, and stacking them on top of each other,
so the forth picture is reconstructed by printing the three output images onto
transparencies and stacking them together. The reconstructed image achieved in
same size with original secret image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4447</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4447</id><created>2010-04-26</created><authors><author><keyname>Rizvi</keyname><forenames>S. W. A.</forenames></author><author><keyname>Khan</keyname><forenames>R. A.</forenames></author></authors><title>Maintainability Estimation Model for Object-Oriented Software in Design
  Phase (MEMOOD)</title><categories>cs.SE</categories><comments>Journal of Computing online at
  https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measuring software maintainability early in the development life cycle,
especially at the design phase, may help designers to incorporate required
enhancement and corrections for improving maintainability of the final
software. This paper developed a multivariate linear model 'Maintainability
Estimation Model for Object-Oriented software in Design phase' (MEMOOD), which
estimates the maintainability of class diagrams in terms of their
understandability and modifiability. While, in order to quantify class
diagram's understandability and modifiability the paper further developed two
more multivariate models. These two models use design level object-oriented
metrics, to quantify understandability and modifiability of class diagram. Such
early quantification of maintainability provides an opportunity to improve the
maintainability of class diagram and consequently the maintainability of final
software. All the three models have been validated through appropriate
statistical measures and contextual interpretation has been drawn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4448</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4448</id><created>2010-04-26</created><authors><author><keyname>Al-amri</keyname><forenames>Salem Saleh</forenames></author><author><keyname>Kalyankar</keyname><forenames>N. V.</forenames></author><author><keyname>D</keyname><forenames>Khamitkar S.</forenames></author></authors><title>Deblured Gaussian Blurred Images</title><categories>cs.CV</categories><comments>Journal of Computing online at
  https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper attempts to undertake the study of Restored Gaussian Blurred
Images. by using four types of techniques of deblurring image as Wiener filter,
Regularized filter, Lucy Richardson deconvlutin algorithm and Blind
deconvlution algorithm with an information of the Point Spread Function (PSF)
corrupted blurred image with Different values of Size and Alfa and then
corrupted by Gaussian noise. The same is applied to the remote sensing image
and they are compared with one another, So as to choose the base technique for
restored or deblurring image.This paper also attempts to undertake the study of
restored Gaussian blurred image with no any information about the Point Spread
Function (PSF) by using same four techniques after execute the guess of the
PSF, the number of iterations and the weight threshold of it. To choose the
base guesses for restored or deblurring image of this techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4449</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4449</id><created>2010-04-26</created><authors><author><keyname>Osman</keyname><forenames>Ahmed Hamza</forenames></author><author><keyname>Salim</keyname><forenames>Naomie</forenames></author><author><keyname>Binwahlan</keyname><forenames>Mohammed Salem</forenames></author></authors><title>Plagiarism Detection Using Graph-Based Representation</title><categories>cs.OH</categories><comments>Journal of Computing online at
  https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Plagiarism of material from the Internet is a widespread and growing problem.
Several methods used to detect the plagiarism and similarity between the source
document and suspected documents such as fingerprint based on character or
n-gram. In this paper, we discussed a new method to detect the plagiarism based
on graph representation; however, Preprocessing for each document is required
such as breaking down the document into its constituent sentences. Segmentation
of each sentence into separated terms and stop word removal. We build the graph
by grouping each sentence terms in one node, the resulted nodes are connected
to each other based on order of sentence within the document, all nodes in
graph are also connected to top level node &quot;Topic Signature&quot;. Topic signature
node is formed by extracting the concepts of each sentence terms and grouping
them in such node. The main advantage of the proposed method is the topic
signature which is main entry for the graph is used as quick guide to the
relevant nodes. which should be considered for the comparison between source
documents and suspected one. We believe the proposed method can achieve a good
performance in terms of effectiveness and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4450</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4450</id><created>2010-04-26</created><authors><author><keyname>Chen</keyname><forenames>Yee Ming</forenames></author></authors><title>Improving Supply Chain Coordination by Linking Dynamic Procurement
  Decision to Multi-Agent System</title><categories>cs.MA</categories><comments>Journal of Computing online at
  https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet has changed the way business is conducted in many ways. For
example, in the field of procurement, the possibility to directly interact with
a trading partner has given rise to new mechanisms in the supply chain
management. One such interactive dynamic procurement, which lets both buyer and
seller software agents bid by potential buyer agents instead of static
procurement by vendors. Dynamic procurement decision could provide the buying
and selling channel to buyer, to avoid occurring condition that seller could
not deliver on the contract promise. Using NYOP(Name Your Own Price) to be the
core of dynamic procurement negotiation algorithm sets up multi-agent dynamic
supply chain system, to present the DSINs(Dynamic Supply Chain Information
Networks) by JADE, and to present the dynamic supply chain logistic simulation
by eM-Plant. Finally, evaluating supply chain performance with supply chain
performance metrics (such as bullwhip, fill rate), to be the reference of
enterprise making deciding in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4454</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4454</id><created>2010-04-26</created><authors><author><keyname>Cherif</keyname><forenames>F.</forenames></author><author><keyname>Chighoub</keyname><forenames>R.</forenames></author></authors><title>Crowd simulation influenced by agent's socio-psychological state</title><categories>cs.MA</categories><comments>Journal of Computing online at
  https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim our work is to create virtual humans as intelligent entities, which
includes approximate the maximum as possible the virtual agent animation to the
natural human behavior. In order to accomplish this task, our agent must be
capable to interact with the environment, interacting with objects and other
agents. The virtual agent needs to act as real person, so he should be capable
to extract semantic information from the geometric model of the world where he
is inserted, based on his own perception, and he realizes his own decision. The
movement of the individuals is representing by the combination of two
approaches of movement which are, the social force model and the based-rule
model. These movements are influenced by a set of socio-psychological rules to
give a more realistic result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4457</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4457</id><created>2010-04-26</created><authors><author><keyname>Albidewi</keyname><forenames>Ibrahim A.</forenames></author><author><keyname>Ann</keyname><forenames>Yap Teck</forenames></author></authors><title>Combination of Subtractive Clustering and Radial Basis Function in
  Speaker Identification</title><categories>cs.MM</categories><comments>Journal of Computing online at
  https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speaker identification is the process of determining which registered speaker
provides a given utterance. Speaker identification required to make a claim on
the identity of speaker from the Ns trained speaker in its user database. In
this study, we propose the combination of clustering algorithm and the
classification technique - subtractive and Radial Basis Function (RBF). The
proposed technique is chosen because RBF is a simpler network structures and
faster learning algorithm. RBF finds the input to output map using the local
approximators which will combine the linear of the approximators and cause the
linear combiner have few weights. Besides that, RBF neural network model using
subtractive clustering algorithm for selecting the hidden node centers, which
can achieve faster training speed. In the meantime, the RBF network was trained
with a regularization term so as to minimize the variances of the nodes in the
hidden layer and perform more accu-rate prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4458</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4458</id><created>2010-04-26</created><authors><author><keyname>Hunagund</keyname><forenames>P. V.</forenames></author><author><keyname>Kalpana</keyname><forenames>A. B.</forenames></author></authors><title>Crosstalk Noise Modeling for RC and RLC interconnects in Deep Submicron
  VLSI Circuits</title><categories>cs.OH</categories><comments>Journal of Computing online at
  https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The crosstalk noise model for noise constrained interconnects optimization is
presented for RC interconnects. The proposed model has simple closed-form
expressions, which is capable of predicting the noise amplitude and the noise
pulse width of an RC interconnect as well as coupling locations (near-driver
and near-receiver) on victim net. This paper also presents a crosstalk noise
model for both identical and non identical coupled
resistance-inductance-capacitance (RLC) interconnects, which is developed based
on a decoupling technique exhibiting an average error of 6.8% as compared to
SPICE. The crosstalk noise model, together with a proposed concept of effective
mutual inductance, is applied to evaluate the effectiveness of the shielding
technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4459</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4459</id><created>2010-04-26</created><authors><author><keyname>Malviya</keyname><forenames>Anjali</forenames></author><author><keyname>Bhirud</keyname><forenames>S. G.</forenames></author></authors><title>Visual Infrared Video Fusion for Night Vision using Background
  Estimation</title><categories>cs.MM</categories><comments>https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010, 66-69</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video fusion is a process that combines visual data from different sensors to
obtain a single composite video preserving the information of the sources. The
availability of a system, enhancing human ability to perceive the observed
scenario, is crucial to improve the performance of a surveillance system. The
infrared (IR) camera captures thermal image of object in night-time
environment, when only limited visual information can be captured by RGB
camera. The fusion of data recorded by an IR sensor and a visible RGB camera
can produce information otherwise not obtainable by viewing the sensor outputs
separately. In this paper we consider the problem of fusing two video streams
acquired by an RGB camera and an IR sensor. The pedestrians, distinctly
captured by IR video, are separated and fused with the RGB video. The
algorithms implemented involve estimation of the background, followed by
detection of object from the IR Video, after necessary denoising. Finally a
suitable fusion algorithm is employed to combine the extracted pedestrians with
the visual output. The obtained results clearly demonstrate the effectiveness
of the proposed video fusion scheme, for night vision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4460</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4460</id><created>2010-04-26</created><authors><author><keyname>Ramachandran</keyname><forenames>Sumalatha</forenames></author><author><keyname>Joseph</keyname><forenames>Sharon</forenames></author><author><keyname>Paulraj</keyname><forenames>Sujaya</forenames></author><author><keyname>Ramaraj</keyname><forenames>Vetriselvi</forenames></author></authors><title>Handling Overload Conditions In High Performance Trustworthy Information
  Retrieval Systems</title><categories>cs.IR</categories><comments>https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010, 70-75</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web search engines retrieve a vast amount of information for a given search
query. But the user needs only trustworthy and high-quality information from
this vast retrieved data. The response time of the search engine must be a
minimum value in order to satisfy the user. An optimum level of response time
should be maintained even when the system is overloaded. This paper proposes an
optimal Load Shedding algorithm which is used to handle overload conditions in
real-time data stream applications and is adapted to the Information Retrieval
System of a web search engine. Experiment results show that the proposed
algorithm enables a web search engine to provide trustworthy search results to
the user within an optimum response time, even during overload conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4462</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4462</id><created>2010-04-26</created><authors><author><keyname>Saraswathi</keyname><forenames>S.</forenames></author><author><keyname>M</keyname><forenames>Asma Siddhiqaa.</forenames></author><author><keyname>K</keyname><forenames>Kalaimagal.</forenames></author><author><keyname>M</keyname><forenames>Kalaiyarasi.</forenames></author></authors><title>BiLingual Information Retrieval System for English and Tamil</title><categories>cs.IR</categories><comments>https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010, 85-89</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the design and implementation of BiLingual Information
Retrieval system on the domain, Festivals. A generic platform is built for
BiLingual Information retrieval which can be extended to any foreign or Indian
language working with the same efficiency. Search for the solution of the query
is not done in a specific predefined set of standard languages but is chosen
dynamically on processing the user's query. This paper deals with Indian
language Tamil apart from English. The task is to retrieve the solution for the
user given query in the same language as that of the query. In this process, a
Ontological tree is built for the domain in such a way that there are entries
in the above listed two languages in every node of the tree. A Part-Of-Speech
(POS) Tagger is used to determine the keywords from the given query. Based on
the context, the keywords are translated to appropriate languages using the
Ontological tree. A search is performed and documents are retrieved based on
the keywords. With the use of the Ontological tree, Information Extraction is
done. Finally, the solution for the query is translated back to the query
language (if necessary) and produced to the user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4463</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4463</id><created>2010-04-26</created><authors><author><keyname>Nazir</keyname><forenames>Mohd</forenames></author><author><keyname>Khan</keyname><forenames>Raees A.</forenames></author><author><keyname>Mustafa</keyname><forenames>Khurram</forenames></author></authors><title>A Metrics Based Model for Understandability Quantification</title><categories>cs.SE</categories><comments>https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010, 90-94</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software developers and maintainers need to read and understand source
programs and other software artifacts. The increase in size and complexity of
software drastically affects several quality attributes, especially
understandability and maintainability. False interpretation often leads to
ambiguities, misunderstanding and hence to faulty development results. Despite
the fact that software understandability is vital and one of the most
significant components of the software development process, it is poorly
managed. This is mainly due to the lack of its proper management and control.
The paper highlights the importance of understandability in general and as a
factor of software testability. Two major contributions are made in the paper.
A relation between testability factors and object oriented characteristics has
been established as a first contribution. In second contribution, a model has
been proposed for estimating understandability of object oriented software
using design metrics. In addition, the proposed model has been validated using
experimental try-out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4464</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4464</id><created>2010-04-26</created><authors><author><keyname>Saraswathi</keyname><forenames>S.</forenames></author><author><keyname>Sravan.</keyname><forenames>Narasimha</forenames><suffix>V</suffix></author><author><keyname>B.</keyname><forenames>Sai Vamsi Krishna.</forenames><suffix>V</suffix></author><author><keyname>S</keyname><forenames>Suresh Reddy.</forenames></author></authors><title>Audio enabled information extraction system for cricket and hockey
  domains</title><categories>cs.IR cs.MM cs.SD</categories><comments>Journal of Computing online at
  https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proposed system aims at the retrieval of the summarized information from
the documents collected from web based search engine as per the user query
related to cricket and hockey domain. The system is designed in a manner that
it takes the voice commands as keywords for search. The parts of speech in the
query are extracted using the natural language extractor for English. Based on
the keywords the search is categorized into 2 types: - 1.Concept wise -
information retrieved to the query is retrieved based on the keywords and the
concept words related to it. The retrieved information is summarized using the
probabilistic approach and weighted means algorithm.2.Keyword search - extracts
the result relevant to the query from the highly ranked document retrieved from
the search by the search engine. The relevant search results are retrieved and
then keywords are used for summarizing part. During summarization it follows
the weighted and probabilistic approaches in order to identify the data
comparable to the keywords extracted. The extracted information is then refined
repeatedly through the aggregation process to reduce redundancy. Finally the
resultant data is submitted to the user in the form of audio output.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4465</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4465</id><created>2010-04-26</created><authors><author><keyname>Anantdeep</keyname><forenames>Er.</forenames></author><author><keyname>kaur</keyname><forenames>Er. Sandeep</forenames></author><author><keyname>Kaur</keyname><forenames>Er. Balpreet</forenames></author></authors><title>Mobile Zigbee Sensor Networks</title><categories>cs.NI</categories><comments>https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010, 95-99</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  OPNET Modeler accelerates network R&amp;D and improves product quality through
high-fidelity modeling and scalable simulation. It provides a virtual
environment for designing protocols and devices, and for testing and
demonstrating designs in realistic scenarios prior to production. OPNET Modeler
supports 802.15.4 standard and has been used to make a model of PAN. Iterations
have been performed by changing the Power of the transmitter and the throughput
will has been analyzed to arrive at optimal values.An energy-efficient wireless
home network based on IEEE 802.15.4, a novel architecture has been proposed. In
this architecture, all nodes are classified into stationary nodes and mobile
nodes according to the functionality of each node. Mobile nodes are usually
battery-powered, and therefore need low-power operation. In order to improve
power consumption of mobile nodes, effective handover sequence based on MAC
broadcast and transmission power control based on LQ (link quality) are
employed. Experimental results demonstrate that by using the proposed
architecture, communication time and power consumption of mobile nodes can be
reduced by 1.2 seconds and 42.8%, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4466</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4466</id><created>2010-04-26</created><authors><author><keyname>Kaur</keyname><forenames>Er. Sandeep</forenames></author><author><keyname>Anantdeep</keyname><forenames>Er.</forenames></author><author><keyname>Aggarwal</keyname><forenames>Er. Deepak</forenames></author></authors><title>Effect of Crosstalk on Permutation in Optical Multistage Interconnection
  Networks</title><categories>cs.NI</categories><comments>Journal of Computing online at
  https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical MINs hold great promise and have advantages over their electronic
networks.they also hold their own challenges. More research has been done on
Electronic Multistage Interconnection Networks, (EMINs) but these days optical
communication is a good networking choice to meet the increasing demands of
high-performance computing communication applications for high bandwidth
applications. The electronic Multistage Interconnection Networks (EMINs) and
the Optical Multistage Interconnection Networks (OMINs) have many similarities,
but there are some fundamental differences between them such as the
optical-loss during switching and the crosstalk problem in the optical
switches. To reduce the negative effect of crosstalk, various approaches which
apply the concept of dilation in either the space or time domain have been
proposed. With the space domain approach, extra SEs are used to ensure that at
most one input and one output of every SE will be used at any given time. For
an Optical network without crosstalk, it is needed to divide the messages into
several groups, and then deliver the messages using one time slot (pass) for
each group, which is called the time division multiplexing. This Paper
discusses the permutation passability behavior of optical MINs. The bandwidth
of optical MINs with or without crosstalk has also been explained. The results
thus obtained shows that the performance of the networks improves by allowing
crosstalk to some extent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4467</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4467</id><created>2010-04-26</created><authors><author><keyname>Aggarwal</keyname><forenames>Er. Deepak</forenames></author><author><keyname>Kaur</keyname><forenames>Er. Sandeep</forenames></author><author><keyname>Anantdeep</keyname><forenames>Er.</forenames></author></authors><title>An Efficient Watermarking Algorithm to Improve Payload and Robustness
  without Affecting Image Perceptual Quality</title><categories>cs.CV</categories><comments>https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010, 105-109</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Capacity, Robustness, &amp; Perceptual quality of watermark data are very
important issues to be considered. A lot of research is going on to increase
these parameters for watermarking of the digital images, as there is always a
tradeoff among them. . In this paper an efficient watermarking algorithm to
improve payload and robustness without affecting perceptual quality of image
data based on DWT is discussed. The aim of the paper is to employ the nested
watermarks in wavelet domain which increases the capacity and ultimately the
robustness against attacks and selection of different scaling factor values for
LL &amp; HH bands and during embedding not to create the visible artifacts in the
original image and therefore the original and watermarked image is similar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4469</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4469</id><created>2010-04-26</created><authors><author><keyname>Hameed</keyname><forenames>Khawar</forenames></author><author><keyname>Ahsan</keyname><forenames>Kamran</forenames></author><author><keyname>Yang</keyname><forenames>Weijun</forenames></author></authors><title>Mobile Commerce and Applications: An Exploratory Study and Review</title><categories>cs.CY</categories><comments>Journal of Computing online at
  https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile commerce is enabling the development of additional revenue streams for
organizations through the delivery of chargeable mobile services. According to
the European Information Technology Observatory, the total amount of revenue
generated by mobile commerce was reported to be less than {\pounds}9 million in
the United Kingdom in 2001. By 2005 this had, at least, doubled and more recent
industry forecasts project significant global growth in this area. Mobile
commerce creates a range of business opportunities and new revenue streams for
businesses across industry sectors via the deployment of innovative services,
applications and associated information content. This paper presents a review
of mobile commerce business models and their importance for the creation of
mobile commerce solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4477</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4477</id><created>2010-04-26</created><authors><author><keyname>Kamakshi</keyname><forenames>P.</forenames></author><author><keyname>Babu</keyname><forenames>A. Vinaya</forenames></author></authors><title>Preserving Privacy and Sharing the Data in Distributed Environment using
  Cryptographic Technique on Perturbed data</title><categories>cs.CR</categories><comments>https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010, 115-119</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objective of data mining is to extract previously unknown patterns
from large collection of data. With the rapid growth in hardware, software and
networking technology there is outstanding growth in the amount data
collection. Organizations collect huge volumes of data from heterogeneous
databases which also contain sensitive and private information about and
individual .The data mining extracts novel patterns from such data which can be
used in various domains for decision making .The problem with data mining
output is that it also reveals some information, which are considered to be
private and personal. Easy access to such personal data poses a threat to
individual privacy. There has been growing concern about the chance of misusing
personal information behind the scene without the knowledge of actual data
owner. Privacy is becoming an increasingly important issue in many data mining
applications in distributed environment. Privacy preserving data mining
technique gives new direction to solve this problem. PPDM gives valid data
mining results without learning the underlying data values .The benefits of
data mining can be enjoyed, without compromising the privacy of concerned
individuals. The original data is modified or a process is used in such a way
that private data and private knowledge remain private even after the mining
process. In this paper we have proposed a framework that allows systemic
transformation of original data using randomized data perturbation technique
and the modified data is then submitted as result of client's query through
cryptographic approach. Using this approach we can achieve confidentiality at
client as well as data owner sites. This model gives valid data mining results
for analysis purpose but the actual or true data is not revealed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4478</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4478</id><created>2010-04-26</created><authors><author><keyname>Mehra</keyname><forenames>Anuj</forenames></author><author><keyname>Shukla</keyname><forenames>Anupam</forenames></author><author><keyname>Kumawat</keyname><forenames>Mahender</forenames></author><author><keyname>Ranjan</keyname><forenames>Rajiv</forenames></author><author><keyname>Tiwari</keyname><forenames>Ritu</forenames></author></authors><title>Intelligent System for Speaker Identification using Lip features with
  PCA and ICA</title><categories>cs.SD</categories><comments>https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010, 120-127</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biometric authentication techniques are more consistent and efficient than
conventional authentication techniques and can be used in monitoring,
transaction authentication, information retrieval, access control, forensics,
etc. In this paper, we have presented a detailed comparative analysis between
Principle Component Analysis (PCA) and Independent Component Analysis (ICA)
which are used for feature extraction on the basis of different Artificial
Neural Network (ANN) such as Back Propagation (BP), Radial Basis Function (RBF)
and Learning Vector Quantization (LVQ). In this paper, we have chosen &quot;TULIPS1
database, (Movellan, 1995)&quot; which is a small audiovisual database of 12
subjects saying the first 4 digits in English for the incorporation of above
methods. The six geometric lip features i.e. height of the outer corners of the
mouth, width of the outer corners of the mouth, height of the inner corners of
the mouth, width of the inner corners of the mouth, height of the upper lip,
and height of the lower lip which extracts the identity relevant information
are considered for the research work. After the comprehensive analysis and
evaluation a maximum of 91.07% accuracy in speaker recognition is achieved
using PCA and RBF and 87.36% accuracy is achieved using ICA and RBF. Speaker
identification has a wide scope of applications such as access control,
monitoring, transaction authentication, information retrieval, forensics, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4480</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4480</id><created>2010-04-26</created><authors><author><keyname>Ramakrishnan</keyname><forenames>S.</forenames></author><author><keyname>Venugopalan</keyname><forenames>S.</forenames></author><author><keyname>Jeyakumar</keyname><forenames>A. Ebenezer</forenames></author></authors><title>Prediction of Retained Capacity and EODV of Li-ion Batteries in LEO
  Spacecraft Batteries</title><categories>cs.OH</categories><comments>Journal of Computing online at
  https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In resent years ANN is widely reported for modeling in different areas of
science including electro chemistry. This includes modeling of different
technological batteries such as lead acid battery, Nickel cadmium batteries
etc. Lithium ion batteries are advance battery technology which satisfy most of
the space mission requirements. Low earth orbit (LEO)space craft batteries
undergo large number of charge discharge cycles (about 25000 cycles)compared to
other ground level or space applications. This study is indented to develop ANN
model for about 25000 cycles, cycled under various temperature, Depth Of
Discharge (DOD) settings with constant charge voltage limit to predict the
retained capacity and End of Discharge Voltage (EODV). To extract firm
conclusion and distinguish the capability of ANN method, the predicted values
are compared with experimental result by statistical method and Bland Altman
plot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4481</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4481</id><created>2010-04-26</created><authors><author><keyname>Yadav</keyname><forenames>Ravinder</forenames></author><author><keyname>Aggarwal</keyname><forenames>Rinkle Rani</forenames></author></authors><title>Survey and Comparison of Optical Switch Fabrication Techniques and
  Architectures</title><categories>cs.NI</categories><comments>https://sites.google.com/site/journalofcomputing/</comments><journal-ref>Journal of Computing, Volume 2, Issue 4, April 2010, 133-137</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main issue in the optical transmission is switching speed. The optical
packet switching faces many significant challenges in processing and buffering.
The generalized multilevel protocol switching seeks to eliminate the
asynchronous transfer mode and synchronous optical network layer, hence the
implementation of IP over WDM (wave length division multiplexing). Optical
burst switching attempts to minimize the need for processing and buffering by
aggregating flow of data packets in to burst. In this paper there is an
extensive overview on current technologies and techniques concerning optical
switching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4484</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4484</id><created>2010-04-26</created><authors><author><keyname>Patel</keyname><forenames>Viresh</forenames></author></authors><title>Determining Edge Expansion and Other Connectivity Measures of Graphs of
  Bounded Genus</title><categories>cs.DM math.CO</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that for an $n$-vertex graph $G$ of genus $g$, the
edge expansion of $G$ can be determined in time $n^{O(g^2)}$. We show that the
same is true for various other similar measures of edge connectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4485</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4485</id><created>2010-04-26</created><authors><author><keyname>Effenberger</keyname><forenames>Felix</forenames></author><author><keyname>Weiskopf</keyname><forenames>Daniel</forenames></author></authors><title>Finding and Classifying Critical Points of 2D Vector Fields: A
  Cell-Oriented Approach Using Group Theory</title><categories>cs.GR</categories><comments>37 pages, 18 figures</comments><msc-class>68-04, 68R05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach to finding critical points in cell-wise
barycentrically or bilinearly interpolated vector fields on surfaces. The
Poincar\e index of the critical points is determined by investigating the
qualitative behavior of 0-level sets of the interpolants of the vector field
components in parameter space using precomputed combinatorial results, thus
avoiding the computation of the Jacobian of the vector field at the critical
points in order to determine its index. The locations of the critical points
within a cell are determined analytically to achieve accurate results. This
approach leads to a correct treatment of cases with two first-order critical
points or one second-order critical point of bilinearly interpolated vector
fields within one cell, which would be missed by examining the linearized field
only. We show that for the considered interpolation schemes determining the
index of a critical point can be seen as a coloring problem of cell edges. A
complete classification of all possible colorings in terms of the types and
number of critical points yielded by each coloring is given using computational
group theory. We present an efficient algorithm that makes use of these
precomputed classifications in order to find and classify critical points in a
cell-by-cell fashion. Issues of numerical stability, construction of the
topological skeleton, topological simplification, and the statistics of the
different types of critical points are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4488</identifier>
 <datestamp>2015-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4488</id><created>2010-04-26</created><updated>2011-11-06</updated><authors><author><keyname>Huang</keyname><forenames>Dazu</forenames></author><author><keyname>Chen</keyname><forenames>Zhigang</forenames></author><author><keyname>Li</keyname><forenames>Xin</forenames></author><author><keyname>Guo</keyname><forenames>Ying</forenames></author></authors><title>Apologizing Comment on `Quantum Quasi-Cyclic Low-Density Parity-Check
  codes$\,$&quot;</title><categories>cs.IT math.IT</categories><comments>Some readers have falsely assumed that I am a co-author of the
  &quot;Apologizing Comment on `Quantum Quasi-Cyclic Low-Density Parity-Check code'&quot;</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In our recent paper entitled &quot;Quantum Quasi-Cyclic Low-Density Parity-Check
codes&quot; [ICIC 2009. LNCS 5754], it was claimed that some new quantum codes can
be constructed via the CSS encoding/decoding approach with various lengths and
rates. However, the further investigation shows that the proposed construction
may steal some ideas from the paper entitled &quot;Quantum Quasi-Cyclic LDPC codes&quot;
[quant-ph/0701020v2]. We feel that the apologizing point of the original
protocol is that some results are almost similar to that of construction
methods with algebraic combinatorics although we suggest the different approach
for improving them. Also, there is a weak point of the original coding approach
while considering the application of codes in imperfect channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4489</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4489</id><created>2010-04-26</created><authors><author><keyname>Hiemstra</keyname><forenames>Djoerd</forenames></author><author><keyname>Hauff</keyname><forenames>Claudia</forenames></author></authors><title>MIREX: MapReduce Information Retrieval Experiments</title><categories>cs.IR</categories><report-no>TR-CTIT-10-15</report-no><acm-class>H.3.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We propose to use MapReduce to quickly test new retrieval approaches on a
cluster of machines by sequentially scanning all documents. We present a small
case study in which we use a cluster of 15 low cost ma- chines to search a web
crawl of 0.5 billion pages showing that sequential scanning is a viable
approach to running large-scale information retrieval experiments with little
effort. The code is available to other researchers at:
http://mirex.sourceforge.net
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4490</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4490</id><created>2010-04-26</created><authors><author><keyname>Bustin</keyname><forenames>Ronit</forenames><affiliation>Shitz</affiliation></author><author><keyname>Payar&#xf3;</keyname><forenames>Miquel</forenames><affiliation>Shitz</affiliation></author><author><keyname>Palomar</keyname><forenames>Daniel P.</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>On MMSE Properties and I-MMSE Implications in Parallel MIMO Gaussian
  Channels</title><categories>cs.IT math.IT</categories><comments>5 pages, to appear in ISIT 2010, Austin, Texas</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends the &quot;single crossing point&quot; property of the scalar MMSE
function, derived by Guo, Shamai and Verd\'u (first presented in ISIT 2008), to
the parallel degraded MIMO scenario. It is shown that the matrix Q(t), which is
the difference between the MMSE assuming a Gaussian input and the MMSE assuming
an arbitrary input, has, at most, a single crossing point for each of its
eigenvalues. Together with the I-MMSE relationship, a fundamental connection
between Information Theory and Estimation Theory, this new property is employed
to derive results in Information Theory. As a simple application of this
property we provide an alternative converse proof for the broadcast channel
(BC) capacity region under covariance constraint in this specific setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4492</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4492</id><created>2010-04-26</created><updated>2010-11-01</updated><authors><author><keyname>Mochaourab</keyname><forenames>Rami</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard</forenames></author></authors><title>Optimal Beamforming in Interference Networks with Perfect Local Channel
  Information</title><categories>cs.IT math.IT</categories><comments>30 pages, 6 figures, submitted to IEEE Transactions on Signal
  Processing, revised October 2010</comments><doi>10.1109/TSP.2010.2094612</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider settings in which T multi-antenna transmitters and K
single-antenna receivers concurrently utilize the available communication
resources. Each transmitter sends useful information only to its intended
receivers and can degrade the performance of unintended systems. Here, we
assume the performance measures associated with each receiver are monotonic
with the received power gains. In general, the systems' joint operation is
desired to be Pareto optimal. However, designing Pareto optimal resource
allocation schemes is known to be difficult. In order to reduce the complexity
of achieving efficient operating points, we show that it is sufficient to
consider rank-1 transmit covariance matrices and propose a framework for
determining the efficient beamforming vectors. These beamforming vectors are
thereby also parameterized by T(K-1) real-valued parameters each between zero
and one. The framework is based on analyzing each transmitter's power
gain-region which is composed of all jointly achievable power gains at the
receivers. The efficient beamforming vectors are on a specific boundary section
of the power gain-region, and in certain scenarios it is shown that it is
necessary to perform additional power allocation on the beamforming vectors.
Two examples which include broadcast and multicast data as well as a cognitive
radio application scenario illustrate the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4520</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4520</id><created>2010-04-26</created><authors><author><keyname>Baldi</keyname><forenames>Marco</forenames></author><author><keyname>Bianchi</keyname><forenames>Marco</forenames></author><author><keyname>Chiaraluce</keyname><forenames>Franco</forenames></author></authors><title>Non-Systematic Codes for Physical Layer Security</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures; submitted to ITW 2010 Dublin</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a first study on the topic of achieving physical layer security
by exploiting non-systematic channel codes. The chance of implementing
transmission security at the physical layer is known since many years in
information theory, but it is now gaining an increasing interest due to its
many possible applications. It has been shown that channel coding techniques
can be effectively exploited for designing physical layer security schemes,
able to ensure that an unauthorized receiver, experiencing a channel different
from that of the the authorized receiver, is not able to gather any
information. Recently, it has been proposed to exploit puncturing techniques in
order to reduce the security gap between the authorized and unauthorized
channels. In this paper, we show that the same target can also be achieved by
using non-systematic codes, able to scramble information bits within the
transmitted codeword.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4529</identifier>
 <datestamp>2015-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4529</id><created>2010-04-26</created><authors><author><keyname>Davies</keyname><forenames>Mike E.</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Rank Awareness in Joint Sparse Recovery</title><categories>cs.IT math.IT</categories><comments>23 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we revisit the sparse multiple measurement vector (MMV) problem
where the aim is to recover a set of jointly sparse multichannel vectors from
incomplete measurements. This problem has received increasing interest as an
extension of the single channel sparse recovery problem which lies at the heart
of the emerging field of compressed sensing. However the sparse approximation
problem has origins which include links to the field of array signal processing
where we find the inspiration for a new family of MMV algorithms based on the
MUSIC algorithm. We highlight the role of the rank of the coefficient matrix X
in determining the difficulty of the recovery problem. We derive the necessary
and sufficient conditions for the uniqueness of the sparse MMV solution, which
indicates that the larger the rank of X the less sparse X needs to be to ensure
uniqueness. We also show that the larger the rank of X the less the
computational effort required to solve the MMV problem through a combinatorial
search. In the second part of the paper we consider practical suboptimal
algorithms for solving the sparse MMV problem. We examine the rank awareness of
popular algorithms such as SOMP and mixed norm minimization techniques and show
them to be rank blind in terms of worst case analysis. We then consider a
family of greedy algorithms that are rank aware. The simplest such algorithm is
a discrete version of MUSIC and is guaranteed to recover the sparse vectors in
the full rank MMV case under mild conditions. We extend this idea to develop a
rank aware pursuit algorithm that naturally reduces to Order Recursive Matching
Pursuit (ORMP) in the single measurement case and also provides guaranteed
recovery in the full rank multi-measurement case. Numerical simulations
demonstrate that the rank aware algorithms are significantly better than
existing algorithms in dealing with multiple measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4530</identifier>
 <datestamp>2015-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4530</id><created>2010-04-26</created><updated>2012-02-18</updated><authors><author><keyname>Iwamoto</keyname><forenames>Mitsugu</forenames></author><author><keyname>Koga</keyname><forenames>Hiroki</forenames></author><author><keyname>Yamamoto</keyname><forenames>Hirosuke</forenames></author></authors><title>Coding Theorems for a (2,2)-Threshold Scheme with Detectability of
  Impersonation Attacks</title><categories>cs.IT cs.CR math.IT</categories><comments>25 pages, 3 figures. Submitted to IEEE Trans. Inf. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discuss coding theorems on a $(2, 2)$--threshold scheme in
the presence of an opponent who impersonates one of the two shareholders in an
asymptotic setup. We consider a situation where $n$ secrets $S^n$ from a
memoryless source is blockwisely encoded to two shares and the two shares are
decoded to $S^n$ with permitting negligible decoding error. We introduce
correlation level of the two shares and characterize the minimum attainable
rates of the shares and a uniform random number for realizing a $(2,
2)$--threshold scheme that is secure against the impersonation attack by an
opponent. It is shown that, if the correlation level between the two shares
equals to an $\ell \ge 0$, the minimum attainable rates coincide with
$H(S)+\ell$, where $H(S)$ denotes the entropy of the source, and the maximum
attainable exponent of the success probability of the impersonation attack
equals to $\ell$. We also give a simple construction of an encoder and a
decoder using an ordinary $(2,2)$--threshold scheme where the two shares are
correlated and attains all the bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4541</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4541</id><created>2010-04-26</created><authors><author><keyname>Ruci&#x144;ski</keyname><forenames>Marek</forenames></author><author><keyname>Izzo</keyname><forenames>Dario</forenames></author><author><keyname>Biscani</keyname><forenames>Francesco</forenames></author></authors><title>On the Impact of the Migration Topology on the Island Model</title><categories>cs.DC math.OC</categories><comments>Accepted in Parallel Computing.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallel Global Optimization Algorithms (PGOA) provide an efficient way of
dealing with hard optimization problems. One method of parallelization of GOAs
that is frequently applied and commonly found in the contemporary literature is
the so-called Island Model (IM). In this paper we analyze the impact of the
migration topology on the performance of a PGOA which uses the Island Model. In
particular we consider parallel Differential Evolution and Simulated Annealing
with Adaptive Neighborhood and draw first conclusions that emerge from the
conducted experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4548</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4548</id><created>2010-04-26</created><authors><author><keyname>Biscani</keyname><forenames>Francesco</forenames></author></authors><title>Multiplication of sparse Laurent polynomials and Poisson series on
  modern hardware architectures</title><categories>cs.SC</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we present two algorithms for the multiplication of sparse
Laurent polynomials and Poisson series (the latter being algebraic structures
commonly arising in Celestial Mechanics from the application of perturbation
theories). Both algorithms first employ the Kronecker substitution technique to
reduce multivariate multiplication to univariate multiplication, and then use
the schoolbook method to perform the univariate multiplication. The first
algorithm, suitable for moderately-sparse multiplication, uses the exponents of
the monomials resulting from the univariate multiplication as trivial hash
values in a one dimensional lookup array of coefficients. The second algorithm,
suitable for highly-sparse multiplication, uses a cache-optimised hash table
which stores the coefficient-exponent pairs resulting from the multiplication
using the exponents as keys. Both algorithms have been implemented with
attention to modern computer hardware architectures. Particular care has been
devoted to the efficient exploitation of contemporary memory hierarchies
through cache-blocking techniques and cache-friendly term ordering. The first
algorithm has been parallelised for shared-memory multicore architectures,
whereas the second algorithm is in the process of being parallelised. We
present benchmarks comparing our algorithms to the routines of other computer
algebra systems, both in sequential and parallel mode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4554</identifier>
 <datestamp>2010-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4554</id><created>2010-04-26</created><updated>2010-06-23</updated><authors><author><keyname>Arbabi</keyname><forenames>Hadi</forenames></author><author><keyname>Weigle</keyname><forenames>Michele C.</forenames></author></authors><title>Highway Mobility and Vehicular Ad-Hoc Networks in NS-3</title><categories>cs.NI</categories><comments>Accepted to 2010 Winter Simulation Conference, new version addresses
  reviewers' comments, updated source code URLs, 12 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of vehicular ad-hoc networks (VANETs) requires efficient and
accurate simulation tools. As the mobility of vehicles and driver behavior can
be affected by network messages, these tools must include a vehicle mobility
model integrated with a quality network simulator. We present the first
implementation of a well-known vehicle mobility model to ns-3, the next
generation of the popular ns-2 networking simulator. Vehicle mobility and
network communication are integrated through events. User-created event
handlers can send network messages or alter vehicle mobility each time a
network message is received and each time vehicle mobility is updated by the
model. To aid in creating simulations, we have implemented a straight highway
model that manages vehicle mobility, while allowing for various user
customizations. We show that the results of our implementation of the mobility
model matches that of the model's author and provide an example of using our
implementation in ns-3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4559</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4559</id><created>2010-04-26</created><authors><author><keyname>Krishnamurthy</keyname><forenames>Supriya</forenames></author><author><keyname>Ardelius</keyname><forenames>John</forenames></author><author><keyname>Aurell</keyname><forenames>Erik</forenames></author><author><keyname>Dam</keyname><forenames>Mads</forenames></author><author><keyname>Stadler</keyname><forenames>Rolf</forenames></author><author><keyname>Wuhib</keyname><forenames>Fetahi</forenames></author></authors><title>The Accuracy of Tree-based Counting in Dynamic Networks</title><categories>cs.DC</categories><comments>15 pages, 3 figures</comments><report-no>KTH Technical Report TRITA-EE 2010:011</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tree-based protocols are ubiquitous in distributed systems. They are
flexible, they perform generally well, and, in static conditions, their
analysis is mostly simple. Under churn, however, node joins and failures can
have complex global effects on the tree overlays, making analysis surprisingly
subtle. To our knowledge, few prior analytic results for performance estimation
of tree based protocols under churn are currently known. We study a simple
Bellman-Ford-like protocol which performs network size estimation over a
tree-shaped overlay. A continuous time Markov model is constructed which allows
key protocol characteristics to be estimated, including the expected number of
nodes at a given (perceived) distance to the root and, for each such node, the
expected (perceived) size of the subnetwork rooted at that node. We validate
the model by simulation, using a range of network sizes, node degrees, and
churn-to-protocol rates, with convincing results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4560</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4560</id><created>2010-04-26</created><authors><author><keyname>Mertzios</keyname><forenames>George B.</forenames></author><author><keyname>Corneil</keyname><forenames>Derek G.</forenames></author></authors><title>A Simple Polynomial Algorithm for the Longest Path Problem on
  Cocomparability Graphs</title><categories>cs.DM</categories><comments>24 pages, 4 figures, 4 algorithms</comments><msc-class>05C85 (Primary), 05C38, 05C62, 68R10 (Secondary).</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph $G$, the longest path problem asks to compute a simple path of
$G$ with the largest number of vertices. This problem is the most natural
optimization version of the well known and well studied Hamiltonian path
problem, and thus it is NP-hard on general graphs. However, in contrast to the
Hamiltonian path problem, there are only few restricted graph families such as
trees and some small graph classes where polynomial algorithms for the longest
path problem have been found. Recently it has been shown that this problem can
be solved in polynomial time on interval graphs by applying dynamic programming
to a characterizing ordering of the vertices of the given graph
\cite{longest-int-algo}, thus answering an open question. In the present paper,
we provide the first polynomial algorithm for the longest path problem on a
much greater class, namely on cocomparability graphs. Our algorithm uses a
similar - but essentially simpler - dynamic programming approach, which is
applied to a Lexicographic Depth First Search (LDFS) characterizing ordering of
the vertices of a cocomparability graph. Therefore, our results provide
evidence that this general dynamic programming approach can be used in a more
general setting, leading to efficient algorithms for the longest path problem
on greater classes of graphs. LDFS has recently been introduced in
\cite{Corneil-LDFS08}. Since then, a similar phenomenon of extending an
existing interval graph algorithm to cocomparability graphs by using an LDFS
preprocessing step has also been observed for the minimum path cover problem
\cite{Corneil-MPC}. Therefore, more interestingly, our results also provide
evidence that cocomparability graphs present an interval graph structure when
they are considered using an LDFS ordering of their vertices, which may lead to
other new and more efficient combinatorial algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4580</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4580</id><created>2010-04-26</created><authors><author><keyname>Hamid</keyname><forenames>Nafiz Imtiaz Bin</forenames></author><author><keyname>Khandokar</keyname><forenames>Md. R. H.</forenames></author><author><keyname>Jamal</keyname><forenames>Taskin</forenames></author><author><keyname>Shoeb</keyname><forenames>Md. A.</forenames></author><author><keyname>Hossain</keyname><forenames>Md. Zakir</forenames></author></authors><title>In Quest of the Better Mobile Broadband Solution for South Asia Taking
  WiMAX and LTE into Consideration</title><categories>cs.CY</categories><comments>N. I. B. Hamid, Md. R. H. Khandokar, T. Jamal, Md. A. Shoeb and Md.
  Z. Hossain, &quot;In Quest of the Better Mobile Broadband Solution for South Asia
  Taking WiMAX and LTE into Consideration&quot;, Journal of Telecommunications,
  Volume 2, Issue 1, p86-94, April 2010</comments><journal-ref>Journal of Telecommunications, Volume 2, Issue 1, p86-94, April
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet generation is growing accustomed to having broadband access wherever
they go and not just at home or in the office, which turns mobile broadband
into a reality. This paper aims to look for a suitable mobile broadband
solution in the South Asian region through comparative analysis in various
perspectives. Both WiMAX and LTE are 4G technologies designed to move data
rather than voice having IP networks based on OFDM technology. Proving
competency in various significant aspects WiMAX and LTE already have made a
strong position in telecommunication industry. Again, because of certain
similarities in technology; they aren't like technological rivals as of GSM and
CDMA. But still they are treated as opponents and viewed as a major threat in
case of the flourishing of each other. Such view point is surely not conducive
for getting the best out of them. In this paper various aspects and
applications of WiMAX and LTE for deployment have been analyzed. South Asia
being the residence of an enormous number of people presents an exciting
opportunity for mobile operators, developers and internet service providers.
So, every consideration that has been made here also correlates successfully
with south Asia i.e. how mass people of this region may be benefited from it.
As a result, it might be regarded as a good source in case of making major BWA
deployment decisions in this region. Besides these, it also opens the path for
further research and thinking in this issue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4583</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4583</id><created>2010-04-26</created><authors><author><keyname>Adhicandra</keyname><forenames>Iwan</forenames></author></authors><title>Measuring Data and VoIP Traffic in WiMAX Networks</title><categories>cs.NI</categories><comments>Iwan Adhicandra, &quot;Measuring Data and VoIP Traffic in WiMAX Networks&quot;,
  Journal of Telecommunications, Volume 2, Issue 1, p1-6, April 2010</comments><journal-ref>Journal of Telecommunications, Volume 2, Issue 1, p1-6, April 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to its large coverage area, low cost of deployment and high speed data
rates, WiMAX is a promising technology for providing wireless last-mile
connectivity. Physical and MAC layer of this technology refer to the IEEE
802.16e standard, which defines 5 different data delivery service classes that
can be used in order to satisfy Quality of Service (QoS) requirements of
different applications, such as VoIP, videoconference, FTP, Web, etc. The main
aim of the paper is to examine a case of QoS deployment over a cellular WiMAX
network. In particular, the paper compares the performance obtained using two
different QoS configurations differing from the delivery service class used to
transport VoIP traffic, i.e. UGS or ertPS. Results indicate that for
delay-sensitive traffic that fluctuates beyond its nominal rate, having the
possibility to give back some of its reserved bandwidth, ertPS has the
advantage to permit the transmission of BE traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4586</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4586</id><created>2010-04-26</created><authors><author><keyname>Jedidi</keyname><forenames>A. Ahmed</forenames></author><author><keyname>Abid</keyname><forenames>B. Mohamed</forenames></author></authors><title>Optimal Crosstalk Detection and Localization Method for Optical Time
  Division Multiplexed Transmission Systems</title><categories>cs.NI</categories><comments>A. Ahmed Jedidi and B. Mohamed Abid, &quot;Optimal Crosstalk Detection and
  Localization Method for Optical Time Division Multiplexed Transmission
  Systems&quot;, Journal of Telecommunications, Volume 2, Issue 1, p7-11, April 2010</comments><journal-ref>Journal of Telecommunications, Volume 2, Issue 1, p7-11, April
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All-Optical Network (AON) is a network where the user-network interface is
optical and the data does undergo optical to electrical conversion within the
network. AONs are attractive because they promise very high rates, flexible
switching and broad application support. There are two technologies for AON:
Wavelength Division Multiplexed (WDM) and Optical Time Division Multiplexed
(OTDM). OTDM transmission systems are becoming increasingly important as one of
the key technologies satisfying the growing demand for large capacity optical
networks. Although OTDM has several advantages in terms of operation system,
such as natural accommodation of higher bit rate payloads, it introduces many
security vulnerabilities, which do not exist in traditional networks. One of
the serious problems with OTDM is the fact that optical crosstalk is additive,
and thus the aggregate effect of crosstalk over a whole all-optical network
(AON) may be more nefarious than a single point of crosstalk. This is because
crosstalk can spread rapidly through the network, causing additional awkward
failures and triggering multiple undesirable alarms. This results in the
continuous monitoring and identification of the impairments becoming
challenging in the event of transmission failures. In this paper we propose a
novel approach for detecting and localizing crosstalk in OTDM transmission
systems that can participate in some tasks for fault management in optical
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4588</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4588</id><created>2010-04-26</created><authors><author><keyname>Sulaiman</keyname><forenames>Norrozila</forenames></author><author><keyname>Yaakub</keyname><forenames>Che Yahaya</forenames></author></authors><title>Investigation on QoS of Campus-wide WiFi Networks</title><categories>cs.NI</categories><comments>Norrozila Sulaiman and Che Yahaya Yaakub, &quot;Investigation on QoS of
  Campus-wide WiFi Networks&quot;, Journal of Telecommunications, Volume 2, Issue 1,
  p12-16, April 2010</comments><journal-ref>Journal of Telecommunications, Volume 2, Issue 1, p12-16, April
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WiFi is widely implemented in campus wide including administrative, teaching
and student's accommodation. Wireless communications are associated with
interconnect devices which includes cellular networks, infrared, bluetooth and
WiFi enabled devices. It involves mobility and freedom of assessing information
anytime and anywhere. A study on WiFi networks in a campus environment is
presented in this paper. The aim of the research was to investigate the
connectivity problems to WiFi networks. The study includes WiFi performance
analysis as well as network auditing. Channel overlapping and saturation
condition were some of the problems encountered. Different types of software
were used for analyzing the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4590</identifier>
 <datestamp>2011-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4590</id><created>2010-04-26</created><updated>2011-02-21</updated><authors><author><keyname>Chance</keyname><forenames>Zachary</forenames></author><author><keyname>Love</keyname><forenames>David J.</forenames></author></authors><title>Concatenated Coding for the AWGN Channel with Noisy Feedback</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the authors. The paper has been
  moved to 0909.0105 to avoid redundancy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of open-loop coding can be easily extended to a closed-loop
concatenated code if the channel has access to feedback. This can be done by
introducing a feedback transmission scheme as an inner code. In this paper,
this process is investigated for the case when a linear feedback scheme is
implemented as an inner code and, in particular, over an additive white
Gaussian noise (AWGN) channel with noisy feedback. To begin, we look to derive
the optimal linear feedback scheme by optimizing over the received
signal-to-noise ratio. From this optimization, an asymptotically optimal linear
feedback scheme is produced and compared to other well-known schemes. Then, the
linear feedback scheme is implemented as an inner code to a concatenated code
over the AWGN channel with noisy feedback. This code shows improvements not
only in error exponent bounds, but also in bit-error-rate and frame-error-rate.
It is also shown that the if the concatenated code has total blocklength L and
the inner code has blocklength, N, the inner code blocklength should scale as N
= O(C/R), where C is the capacity of the channel and R is the rate of the outer
code. Simulations with low density parity check (LDPC) and turbo codes are
provided to display these advantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4594</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4594</id><created>2010-04-26</created><authors><author><keyname>Tavakoli</keyname><forenames>Saeed</forenames></author><author><keyname>Zeinadini</keyname><forenames>Mahdieh</forenames></author><author><keyname>Mohanna</keyname><forenames>Shahram</forenames></author></authors><title>Modelling and Design of a Microstrip Band-Pass Filter Using Space
  Mapping Techniques</title><categories>cs.OH</categories><comments>Saeed Tavakoli, Mahdieh Zeinadini and Shahram Mohanna, &quot;Modelling and
  Design of a Microstrip Band-Pass Filter Using Space Mapping Techniques&quot;,
  Journal of Telecommunications, Volume 2, Issue 1, p29-34, April 2010</comments><journal-ref>Journal of Telecommunications, Volume 2, Issue 1, p29-34, April
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determination of design parameters based on electromagnetic simulations of
microwave circuits is an iterative and often time-consuming procedure. Space
mapping is a powerful technique to optimize such complex models by efficiently
substituting accurate but expensive electromagnetic models, fine models, with
fast and approximate models, coarse models. In this paper, we apply two space
mapping, an explicit space mapping as well as an implicit and response residual
space mapping, techniques to a case study application, a microstrip band-pass
filter. First, we model the case study application and optimize its design
parameters, using explicit space mapping modelling approach. Then, we use
implicit and response residual space mapping approach to optimize the filter's
design parameters. Finally, the performance of each design methods is
evaluated. It is shown that the use of above-mentioned techniques leads to
achieving satisfactory design solutions with a minimum number of
computationally expensive fine model evaluations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4595</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4595</id><created>2010-04-26</created><authors><author><keyname>Bashir</keyname><forenames>G. M. M.</forenames></author><author><keyname>Hossain</keyname><forenames>M. J.</forenames></author><author><keyname>Karim</keyname><forenames>M. R.</forenames></author></authors><title>Clustering of Content Supporting Computer Mediated Courseware
  Development</title><categories>cs.OH</categories><comments>G. M. M. Bashir, M. J. Hossain and M. R. Karim, &quot;Clustering of
  Content Supporting Computer Mediated Courseware Development&quot;, Journal of
  Telecommunications, Volume 2, Issue 1, p30-35, April 2010</comments><journal-ref>Journal of Telecommunications, Volume 2, Issue 1, p30-35, April
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer Mediated Courseware (CMC) has been developed so far for individual
courses considering single or multiple text books. A group of courseware can be
developed by using multiple text books and in this case, it is a requirement to
cluster the contents of different books to form a generalized clustered
content. No work has been found to develop courseware applying generalized
clustered content. We have proposed a clustering of content supporting computer
mediated courseware development based on data mining techniques to construct a
hierarchical general structure of a group of courseware combining the
individual structure of a set of books. The clustering will help the courseware
developer to dynamically allocate contents to develop different courses using a
group of books. The authors have applied this methodology for different level
of courses on database. The methodology is generalized and can be applied to
any other courses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4598</identifier>
 <datestamp>2010-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4598</id><created>2010-04-26</created><updated>2010-10-27</updated><authors><author><keyname>Khan</keyname><forenames>M. Sadiq Ali</forenames></author></authors><title>Revealing Method for the Intrusion Detection System</title><categories>cs.NI</categories><comments>M. Sadiq Ali Khan, &quot;Revealing Method for the Intrusion Detection
  System&quot;, Journal of Telecommunications, Volume 2, Issue 1, p36-41, April 2010</comments><journal-ref>Journal of Telecommunications, Volume 2, Issue 1, p36-41, April
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of an Intrusion Detection is inadequate to detect errors and unusual
activity on a network or on the hosts belonging to a local network by
monitoring network activity. Algorithms for building detection models are
broadly classified into two categories, Misuse Detection and Anomaly Detection.
The proposed approach should be taken into account, as the security system
violations caused by both incompliance with the security policy and attacks on
the system resulting in the need to describe models. However, it is based on
unified mathematical formalism which is provided for subsequent merger of the
models. The above formalism in this paper presents a state machine describing
the behavior of a system subject. The set of intrusion description models is
used by the evaluation module and determines the likelihood of undesired
actions the system is capable of detecting. The number of attacks which are not
described by models determining the completeness of detection by the IDS linked
to the ability of detecting security violations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4600</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4600</id><created>2010-04-26</created><authors><author><keyname>Chaari</keyname><forenames>Lamia</forenames></author><author><keyname>Kamoun</keyname><forenames>Lotfi</forenames></author></authors><title>Wireless sensors networks MAC protocols analysis</title><categories>cs.NI</categories><comments>Lamia Chaari and Lotfi Kamoun, &quot;Wireless sensors networks MAC
  protocols analysis&quot;, Journal of Telecommunications, Volume 2, Issue 1,
  p42-48, April 2010</comments><journal-ref>Journal of Telecommunications, Volume 2, Issue 1, p42-48, April
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensors networks performance are strictly related to the medium
access mechanism. An effective one, require non-conventional paradigms for
protocol design due to several constraints. An adequate equilibrium between
communication improvement and data processing capabilities must be
accomplished. To achieve low power operation, several MAC protocols already
proposed for WSN. The aim of this paper is to survey and to analyze the most
energy efficient MAC protocol in order to categorize them and to compare their
performances. Furthermore we have implemented some of WSN MAC protocol under
OMNET++ with the purpose to evaluate their performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4601</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4601</id><created>2010-04-26</created><authors><author><keyname>Rudra</keyname><forenames>Atri</forenames></author><author><keyname>Uurtamo</keyname><forenames>Steve</forenames></author></authors><title>Data Stream Algorithms for Codeword Testing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by applications in storage systems and property testing, we study
data stream algorithms for local testing and tolerant testing of codes.
Ideally, we would like to know whether there exist asymptotically good codes
that can be local/tolerant tested with one-pass, poly-log space data stream
algorithms. We show that for the error detection problem (and hence, the local
testing problem), there exists a one-pass, log-space data stream algorithm for
a broad class of asymptotically good codes, including the Reed-Solomon (RS)
code and expander codes. In our technically more involved result, we give a
one-pass, $O(e\log^2{n})$-space algorithm for RS (and related) codes with
dimension $k$ and block length $n$ that can distinguish between the cases when
the Hamming distance between the received word and the code is at most $e$ and
at least $a\cdot e$ for some absolute constant $a&gt;1$. For RS codes with random
errors, we can obtain $e\le O(n/k)$. For folded RS codes, we obtain similar
results for worst-case errors as long as $e\le (n/k)^{1-\eps}$ for any constant
$\eps&gt;0$. These results follow by reducing the tolerant testing problem to the
error detection problem using results from group testing and the list
decodability of the code. We also show that using our techniques, the space
requirement and the upper bound of $e\le O(n/k)$ cannot be improved by more
than logarithmic factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4605</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4605</id><created>2010-04-26</created><authors><author><keyname>Amel</keyname><forenames>Abdelati Malek</forenames></author><author><keyname>Abdessalem</keyname><forenames>Ben Abdelali</forenames></author><author><keyname>Abdellatif</keyname><forenames>Mtibaa</forenames></author></authors><title>Video shot boundary detection using motion activity descriptor</title><categories>cs.OH</categories><comments>Abdelati Malek Amel, Ben Abdelali Abdessalem and Mtibaa Abdellatif,
  &quot;Video shot boundary detection using motion activity descriptor&quot;, Journal of
  Telecommunications, Volume 2, Issue 1, p54-59, April 2010</comments><journal-ref>Journal of Telecommunications, Volume 2, Issue 1, p54-59, April
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focus on the study of the motion activity descriptor for shot
boundary detection in video sequences. We interest in the validation of this
descriptor in the aim of its real time implementation with reasonable high
performances in shot boundary detection. The motion activity information is
extracted in uncompressed domain based on adaptive rood pattern search (ARPS)
algorithm. In this context, the motion activity descriptor was applied for
different video sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4610</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4610</id><created>2010-04-26</created><authors><author><keyname>Kaaniche</keyname><forenames>Heni</forenames></author><author><keyname>Kamoun</keyname><forenames>Farouk</forenames></author></authors><title>Mobility Prediction in Wireless Ad Hoc Networks using Neural Networks</title><categories>cs.NE</categories><comments>Heni Kaaniche and Farouk Kamoun, &quot;Mobility Prediction in Wireless Ad
  Hoc Networks using Neural Networks&quot;, Journal of Telecommunications, Volume 2,
  Issue 1, p95-101, April 2010</comments><journal-ref>Journal of Telecommunications, Volume 2, Issue 1, p95-101, April
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobility prediction allows estimating the stability of paths in a mobile
wireless Ad Hoc networks. Identifying stable paths helps to improve routing by
reducing the overhead and the number of connection interruptions. In this
paper, we introduce a neural network based method for mobility prediction in Ad
Hoc networks. This method consists of a multi-layer and recurrent neural
network using back propagation through time algorithm for training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4612</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4612</id><created>2010-04-26</created><authors><author><keyname>Reza</keyname><forenames>Md. Shamim</forenames></author><author><keyname>Hossain</keyname><forenames>Md. Maruf</forenames></author><author><keyname>Majumder</keyname><forenames>Satya Prasad</forenames></author></authors><title>Evaluation of Burst Loss Rate of an Optical Burst Switching (OBS)
  Network with Wavelength Conversion Capability</title><categories>cs.OH</categories><comments>Md. Shamim Reza, Md. Maruf Hossain and Satya Prasad Majumder,
  &quot;Evaluation of Burst Loss Rate of an Optical Burst Switching (OBS) Network
  with Wavelength Conversion Capability&quot;, Journal of Telecommunications, Volume
  2, Issue 1, p102-109, April 2010</comments><journal-ref>Journal of Telecommunications, Volume 2, Issue 1, p102-109, April
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new analytical model for calculating burst loss rate
(BLR) in a slotted optical burst switched network. The analytical result leads
to a framework which provides guidelines for optical burst switched networks.
Wavelength converter is used for burst contention resolution. The effect of
several design parameters such as burst arrival probability, wavelength
conversion capability, number of slots per burst and number of wavelengths is
incorporated on the above performance measure. We also extend the analytical
result of BLR for different types of service classes where each service class
has a reserved number of wavelengths in a network with fixed number of
wavelengths. We also introduce an algorithm to calculate the resultant number
of wavelength for each service classes depending on the various scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4614</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4614</id><created>2010-04-26</created><authors><author><keyname>Gond</keyname><forenames>Vitthal J.</forenames></author><author><keyname>Goel</keyname><forenames>Aditya</forenames></author></authors><title>Performance Evaluation of Wavelength Routed Optical Network with
  Wavelength Conversion</title><categories>cs.NI</categories><comments>Vitthal J. Gond and Aditya Goel, &quot;Performance Evaluation of
  Wavelength Routed Optical Network with Wavelength Conversion&quot;, Journal of
  Telecommunications, Volume 2, Issue 1, p110-114, April 2010</comments><journal-ref>Journal of Telecommunications, Volume 2, Issue 1, p110-114, April
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid development of telecommunication networks is driven by user demands
for new applications and advances in technologies. The explosive growth of the
internet traffic is due to its use for collecting the information,
communication, multimedia application, entertainment, etc. These applications
are imposing a tremendous demand for bandwidth capacity on telecommunication
network. The introduction of fiber optics had proved to meet the huge demand of
bandwidth. These requirement can be meet by all optical network which is
capable of transmitting enormous data at very high speed, around 50 Tera bits
per seconds (Tbps) A wavelength conversion technique is addressed in this paper
to reduced the blocking probability in wavelength routed networks. It is seen
that the blocking probability of traffic requests decreases as the wavelength
conversion factor increases. We explode the possibility for network with
different size with variation in wavelength per link. In this work the
evaluation of wavelength routed optical network with varying number of
wavelength converters, different traffic types are carried out and results are
shown that the blocking probability is minimum with 50% to 60% wavelength
convertible nodes. Wavelength convertible nodes more than 60% are not showing
much effect on reduction in blocking probability rather it results in increase
in overall cost of network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4616</identifier>
 <datestamp>2010-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4616</id><created>2010-04-26</created><authors><author><keyname>Chaari</keyname><forenames>Lamia</forenames></author><author><keyname>Ayadi</keyname><forenames>Rim</forenames></author><author><keyname>Kamoun</keyname><forenames>Lotfi</forenames></author></authors><title>Conception and FPGA implementation of IEEE 802.11s mesh network MAC
  layer transmitter</title><categories>cs.NI</categories><comments>Lamia Chaari, Rim Ayadi and Lotfi Kamoun, &quot;Conception and FPGA
  implementation of IEEE 802.11s mesh network MAC layer transmitter&quot;</comments><journal-ref>Journal of Telecommunications, Volume 2, Issue 1, p115-123, April
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes, a hardware implementation of Wireless Mesh Networks
(WMN) medium Access Controller (MAC) layer transmitter. In the literature a lot
of works are focused on WMN routing protocol as well as performance analysis
and software integration of WMN units, however few works deals with WMN
hardware implementation. In this field our contribution is to conceive and to
implements on FPGA a WMN MAC transmitter module. Our implementation, written in
hardware description language (HDL) is based on the IEEE 802.11 s standard. The
hardware implementation retains a good performance in speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4622</identifier>
 <datestamp>2010-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4622</id><created>2010-04-26</created><authors><author><keyname>Kawamura</keyname><forenames>Akitoshi</forenames></author></authors><title>Lipschitz Continuous Ordinary Differential Equations are
  Polynomial-Space Complete</title><categories>cs.CC cs.NA math.CA</categories><comments>22 pages, 9 figures; preliminary version presented at CCC 2009</comments><msc-class>03F60, 68Q17, 65Y20, 65L05, 03D15</msc-class><acm-class>F.2.1; F.2.2; G.1.7</acm-class><journal-ref>Computational Complexity 19(2):305-332, May 2010</journal-ref><doi>10.1007/s00037-010-0286-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In answer to Ko's question raised in 1983, we show that an initial value
problem given by a polynomial-time computable, Lipschitz continuous function
can have a polynomial-space complete solution. The key insight is simple: the
Lipschitz condition means that the feedback in the differential equation is
weak. We define a class of polynomial-space computation tableaux with equally
weak feedback, and show that they are still polynomial-space complete. The same
technique also settles Ko's two later questions on Volterra integral equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4641</identifier>
 <datestamp>2010-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4641</id><created>2010-04-26</created><updated>2010-07-19</updated><authors><author><keyname>Roche</keyname><forenames>Daniel S.</forenames></author></authors><title>Chunky and Equal-Spaced Polynomial Multiplication</title><categories>cs.SC cs.DS</categories><comments>23 Pages, pdflatex, accepted to Journal of Symbolic Computation (JSC)</comments><acm-class>I.1.2; G.4; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding the product of two polynomials is an essential and basic problem in
computer algebra. While most previous results have focused on the worst-case
complexity, we instead employ the technique of adaptive analysis to give an
improvement in many &quot;easy&quot; cases. We present two adaptive measures and methods
for polynomial multiplication, and also show how to effectively combine them to
gain both advantages. One useful feature of these algorithms is that they
essentially provide a gradient between existing &quot;sparse&quot; and &quot;dense&quot; methods.
We prove that these approaches provide significant improvements in many cases
but in the worst case are still comparable to the fastest existing algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4656</identifier>
 <datestamp>2011-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4656</id><created>2010-04-26</created><updated>2011-11-08</updated><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author><author><keyname>de Boer</keyname><forenames>Frank S.</forenames></author><author><keyname>Olderog</keyname><forenames>Ernst-Ruediger</forenames></author><author><keyname>de Gouw</keyname><forenames>Stijn</forenames></author></authors><title>Verification of Object-Oriented Programs: a Transformational Approach</title><categories>cs.LO cs.PL</categories><comments>49 pages. To appear in Journal of Computer and System Sciences. Stijn
  de Gouw is now a new author</comments><acm-class>F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that verification of object-oriented programs by means of the
assertional method can be achieved in a simple way by exploiting a
syntax-directed transformation from object-oriented programs to recursive
programs. This transformation suggests natural proofs rules and its correctness
helps us to establish soundness and relative completeness of the proposed proof
system. One of the difficulties is how to properly deal in the assertion
language with the instance variables and aliasing. The discussed programming
language supports arrays, instance variables, failures and recursive methods
with parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4663</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4663</id><created>2010-04-26</created><authors><author><keyname>Suh</keyname><forenames>Changho</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author></authors><title>On the Existence of Optimal Exact-Repair MDS Codes for Distributed
  Storage</title><categories>cs.IT math.IT</categories><comments>20 pages, 6 figures</comments><report-no>UCB/EECS-2010-46</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The high repair cost of (n,k) Maximum Distance Separable (MDS) erasure codes
has recently motivated a new class of codes, called Regenerating Codes, that
optimally trade off storage cost for repair bandwidth. In this paper, we
address bandwidth-optimal (n,k,d) Exact-Repair MDS codes, which allow for any
failed node to be repaired exactly with access to arbitrary d survivor nodes,
where k&lt;=d&lt;=n-1. We show the existence of Exact-Repair MDS codes that achieve
minimum repair bandwidth (matching the cutset lower bound) for arbitrary
admissible (n,k,d), i.e., k&lt;n and k&lt;=d&lt;=n-1. Our approach is based on
interference alignment techniques and uses vector linear codes which allow to
split symbols into arbitrarily small subsymbols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4668</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4668</id><created>2010-04-26</created><updated>2012-08-03</updated><authors><author><keyname>Jones</keyname><forenames>Nick S.</forenames></author><author><keyname>Moriarty</keyname><forenames>John</forenames></author></authors><title>Evolutionary Inference for Function-valued Traits: Gaussian Process
  Regression on Phylogenies</title><categories>q-bio.QM cs.LG physics.data-an stat.ML</categories><comments>7 pages, 1 figure</comments><journal-ref>Journal of the Royal Society Interface vol. 10 no. 78 20120616
  (2013)</journal-ref><doi>10.1098/rsif.2012.0616</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biological data objects often have both of the following features: (i) they
are functions rather than single numbers or vectors, and (ii) they are
correlated due to phylogenetic relationships. In this paper we give a flexible
statistical model for such data, by combining assumptions from phylogenetics
with Gaussian processes. We describe its use as a nonparametric Bayesian prior
distribution, both for prediction (placing posterior distributions on ancestral
functions) and model selection (comparing rates of evolution across a
phylogeny, or identifying the most likely phylogenies consistent with the
observed data). Our work is integrative, extending the popular phylogenetic
Brownian Motion and Ornstein-Uhlenbeck models to functional data and Bayesian
inference, and extending Gaussian Process regression to phylogenies. We provide
a brief illustration of the application of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4689</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4689</id><created>2010-04-26</created><authors><author><keyname>Malaney</keyname><forenames>Robert A</forenames></author></authors><title>Quantum Location Verification in Noisy Channels</title><categories>quant-ph cs.IT math.IT</categories><comments>6 Pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently it has been shown how the use of quantum entanglement can lead to
the creation of real-time communication channels whose viability can be made
location dependent. Such functionality leads to new security paradigms that are
not possible in classical communication networks. Key to these new security
paradigms are quantum protocols that can unconditionally determine that a
receiver is in fact at an a priori assigned location. A limiting factor of such
quantum protocols will be the decoherence of states held in quantum memory.
Here we investigate the performance of quantum location verification protocols
under decoherence effects. More specifically, we address the issue of how
decoherence impacts the verification using N = 2 qubits entangled as Bell
states, as compared to N &gt; 2 qubits entangled as GHZ states. We study the
original quantum location verification protocol, as well as a variant protocol,
introduced here, which utilizes teleportation. We find that the performance of
quantum location verification is in fact similar for Bell states and some N &gt; 2
GHZ states, even though quantum decoherence degrades larger-qubit entanglements
faster. Our results are important for the design and implementation of
location-dependent communications in emerging quantum networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4690</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4690</id><created>2010-04-26</created><authors><author><keyname>Zhu</keyname><forenames>Weiping</forenames></author></authors><title>Explicit Maximum Likelihood Loss Estimator in Multicast Tomography</title><categories>cs.NI</categories><comments>submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the tree topology, previous studies show the maximum likelihood estimate
(MLE) of a link/path takes a polynomial form with a degree that is one less
than the number of descendants connected to the link/path. Since then, the main
concern is focused on searching for methods to solve the high degree polynomial
without using iterative approximation. An explicit estimator based on the Law
of Large Numbers has been proposed to speed up the estimation. However, the
estimate obtained from the estimator is not a MLE. When $n&lt;\infty$, the
estimate may be noticeable different from the MLE. To overcome this, an
explicit MLE estimator is presented in this paper and a comparison between the
MLE estimator and the explicit estimator proposed previously is presented to
unveil the insight of the MLE estimator and point out the pitfall of the
previous one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4701</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4701</id><created>2010-04-26</created><updated>2010-10-15</updated><authors><author><keyname>Gafni</keyname><forenames>Eli</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Petr</forenames></author></authors><title>Relating L-Resilience and Wait-Freedom via Hitting Sets</title><categories>cs.DC</categories><doi>10.1007/978-3-642-17679-1_17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The condition of t-resilience stipulates that an n-process program is only
obliged to make progress when at least n-t processes are correct. Put another
way, the live sets, the collection of process sets such that progress is
required if all the processes in one of these sets are correct, are all sets
with at least n-t processes.
  We show that the ability of arbitrary collection of live sets L to solve
distributed tasks is tightly related to the minimum hitting set of L, a minimum
cardinality subset of processes that has a non-empty intersection with every
live set. Thus, finding the computing power of L is NP-complete.
  For the special case of colorless tasks that allow participating processes to
adopt input or output values of each other, we use a simple simulation to show
that a task can be solved L-resiliently if and only if it can be solved
(h-1)-resiliently, where h is the size of the minimum hitting set of L.
  For general tasks, we characterize L-resilient solvability of tasks with
respect to a limited notion of weak solvability: in every execution where all
processes in some set in L are correct, outputs must be produced for every
process in some (possibly different) participating set in L. Given a task T, we
construct another task T_L such that T is solvable weakly L-resiliently if and
only if T_L is solvable weakly wait-free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4704</identifier>
 <datestamp>2011-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4704</id><created>2010-04-27</created><updated>2010-11-29</updated><authors><author><keyname>Shalizi</keyname><forenames>Cosma Rohilla</forenames></author><author><keyname>Thomas</keyname><forenames>Andrew C.</forenames></author></authors><title>Homophily and Contagion Are Generically Confounded in Observational
  Social Network Studies</title><categories>stat.AP cs.SI physics.data-an physics.soc-ph</categories><comments>27 pages, 9 figures. V2: Revised in response to referees. V3: Ditto</comments><journal-ref>Sociological Methods and Research, vol. 40 (2011), pp. 211--239</journal-ref><doi>10.1177/0049124111404820</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider processes on social networks that can potentially involve three
factors: homophily, or the formation of social ties due to matching individual
traits; social contagion, also known as social influence; and the causal effect
of an individual's covariates on their behavior or other measurable responses.
We show that, generically, all of these are confounded with each other.
Distinguishing them from one another requires strong assumptions on the
parametrization of the social process or on the adequacy of the covariates used
(or both). In particular we demonstrate, with simple examples, that asymmetries
in regression coefficients cannot identify causal effects, and that very simple
models of imitation (a form of social contagion) can produce substantial
correlations between an individual's enduring traits and their choices, even
when there is no intrinsic affinity between them. We also suggest some possible
constructive responses to these results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4708</identifier>
 <datestamp>2015-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4708</id><created>2010-04-27</created><authors><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author></authors><title>Simulating Parallel Algorithms in the MapReduce Framework with
  Applications to Parallel Computational Geometry</title><categories>cs.DS cs.CG cs.DC</categories><comments>Version of paper appearing in MASSIVE 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe efficient MapReduce simulations of parallel
algorithms specified in the BSP and PRAM models. We also provide some
applications of these simulation results to problems in parallel computational
geometry for the MapReduce framework, which result in efficient MapReduce
algorithms for sorting, 1-dimensional all nearest-neighbors, 2-dimensional
convex hulls, 3-dimensional convex hulls, and fixed-dimensional linear
programming. For the case when reducers can have a buffer size of
$B=O(n^\epsilon)$, for a small constant $\epsilon&gt;0$, all of our MapReduce
algorithms for these applications run in a constant number of rounds and have a
linear-sized message complexity, with high probability, while guaranteeing with
high probability that all reducer lists are of size $O(B)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4709</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4709</id><created>2010-04-27</created><updated>2011-08-17</updated><authors><author><keyname>Bo</keyname><affiliation>Rambo</affiliation></author><author><keyname>Tan</keyname></author><author><keyname>Massoulie</keyname><forenames>Laurent</forenames></author></authors><title>Optimal Content Placement for Peer-to-Peer Video-on-Demand Systems</title><categories>cs.NI cs.DC cs.PF math.PR</categories><comments>18 pages (double column), 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of content placement in peer-to-peer
systems, with the objective of maximizing the utilization of peers' uplink
bandwidth resources. We consider system performance under a many-user
asymptotic. We distinguish two scenarios, namely &quot;Distributed Server Networks&quot;
(DSN) for which requests are exogenous to the system, and &quot;Pure P2P Networks&quot;
(PP2PN) for which requests emanate from the peers themselves. For both
scenarios, we consider a loss network model of performance, and determine
asymptotically optimal content placement strategies in the case of a limited
content catalogue. We then turn to an alternative &quot;large catalogue&quot; scaling
where the catalogue size scales with the peer population. Under this scaling,
we establish that storage space per peer must necessarily grow unboundedly if
bandwidth utilization is to be maximized. Relating the system performance to
properties of a specific random graph model, we then identify a content
placement strategy and a request acceptance policy which jointly maximize
bandwidth utilization, provided storage space per peer grows unboundedly,
although arbitrarily slowly, with system size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4710</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4710</id><created>2010-04-27</created><authors><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author><author><keyname>Zimmermann</keyname><forenames>Paul</forenames></author></authors><title>Modern Computer Arithmetic (version 0.5.1)</title><categories>cs.DS cs.NA math.NA math.NT</categories><comments>Preliminary version of a book to be published by Cambridge University
  Press. xvi+247 pages. Cite as &quot;Modern Computer Arithmetic, Version 0.5.1, 5
  March 2010&quot;. For further details, updates and errata see
  http://wwwmaths.anu.edu.au/~brent/pub/pub226.html or
  http://www.loria.fr/~zimmerma/mca/pub226.html</comments><msc-class>68-02 (Primary) 11Y16, 11Y60, 65G50, 65H05, 65Q99, 65Y04, 65Y20
  (Secondary)</msc-class><acm-class>B.2.4; F.2.1; G.1.0; G.1.2; G.1.5; G.2.1; G.4</acm-class><journal-ref>Richard P. Brent and Paul Zimmermann, Modern Computer Arithmetic,
  Cambridge Monographs on Computational and Applied Mathematics (No. 18),
  Cambridge University Press, November 2010, 236 pages</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a draft of a book about algorithms for performing arithmetic, and
their implementation on modern computers. We are concerned with software more
than hardware - we do not cover computer architecture or the design of computer
hardware. Instead we focus on algorithms for efficiently performing arithmetic
operations such as addition, multiplication and division, and their connections
to topics such as modular arithmetic, greatest common divisors, the Fast
Fourier Transform (FFT), and the computation of elementary and special
functions. The algorithms that we present are mainly intended for
arbitrary-precision arithmetic. They are not limited by the computer word size,
only by the memory and time available for the computation. We consider both
integer and real (floating-point) computations. The book is divided into four
main chapters, plus an appendix. Our aim is to present the latest developments
in a concise manner. At the same time, we provide a self-contained introduction
for the reader who is not an expert in the field, and exercises at the end of
each chapter. Chapter titles are: 1, Integer Arithmetic; 2, Modular Arithmetic
and the FFT; 3, Floating-Point Arithmetic; 4, Elementary and Special Function
Evaluation; 5 (Appendix), Implementations and Pointers. The book also contains
a bibliography of 236 entries, index, summary of notation, and summary of
complexities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4713</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4713</id><created>2010-04-27</created><authors><author><keyname>Shum</keyname><forenames>Kenneth W.</forenames></author><author><keyname>Wong</keyname><forenames>Wing Shing</forenames></author></authors><title>Construction of Short Protocol Sequences with Worst-Case Throughput
  Guarantee</title><categories>cs.IT cs.DM math.IT</categories><comments>Conference paper submitted to 2010 IEEE Int. Symp. on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protocol sequences are used in channel access for the multiple-access
collision channel without feedback. A new construction of protocol sequences
with a guarantee of worst-case system throughput is proposed. The construction
is based on Chinese remainder theorem. The Hamming crosscorrelation is proved
to be concentrated around the mean. The sequence period is much shorter than
existing protocol sequences with the same throughput performance. The new
construction reduces the complexity in implementation and also shortens the
waiting time until a packet can be sent successfully.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4718</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4718</id><created>2010-04-27</created><authors><author><keyname>Loh</keyname><forenames>Woong-Kee</forenames></author><author><keyname>Moon</keyname><forenames>Yang-Sae</forenames></author><author><keyname>Kang</keyname><forenames>Jun-Gyu</forenames></author></authors><title>A Data Cleansing Method for Clustering Large-scale Transaction Databases</title><categories>cs.DB</categories><comments>6 pages, 5 figures</comments><doi>10.1587/transinf.E93.D.3120</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we emphasize the need for data cleansing when clustering
large-scale transaction databases and propose a new data cleansing method that
improves clustering quality and performance. We evaluate our data cleansing
method through a series of experiments. As a result, the clustering quality and
performance were significantly improved by up to 165% and 330%, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4727</identifier>
 <datestamp>2011-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4727</id><created>2010-04-27</created><updated>2011-01-05</updated><authors><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author></authors><title>Direct Proofs of Order Independence</title><categories>cs.GT</categories><comments>9 pages</comments><journal-ref>Economics Bulletin, Vol. 31 no. 1 pp. 106-115 (2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a generic result concerning order independence of a dominance
relation on finite games. It allows us to draw conclusions about order
independence of various dominance relations in a direct and simple way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4729</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4729</id><created>2010-04-27</created><authors><author><keyname>Chakaravarthy</keyname><forenames>Venkatesan T.</forenames></author><author><keyname>Pandit</keyname><forenames>Vinayaka</forenames></author><author><keyname>Sabharwal</keyname><forenames>Yogish</forenames></author></authors><title>On the Complexity of the $k$-Anonymization Problem</title><categories>cs.CC cs.DB</categories><comments>9 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of anonymizing tables containing personal information
before releasing them for public use. One of the formulations considered in
this context is the $k$-anonymization problem: given a table, suppress a
minimum number of cells so that in the transformed table, each row is identical
to atleast $k-1$ other rows. The problem is known to be NP-hard and
MAXSNP-hard; but in the known reductions, the number of columns in the
constructed tables is arbitrarily large. However, in practical settings the
number of columns is much smaller. So, we study the complexity of the practical
setting in which the number of columns $m$ is small. We show that the problem
is NP-hard, even when the number of columns $m$ is a constant ($m=3$). We also
prove MAXSNP-hardness for this restricted version and derive that the problem
cannot be approximated within a factor of (6238/6237). Our reduction uses
alphabets $\Sigma$ of arbitrarily large size. A natural question is whether the
problem remains NP-hard when both $m$ and $|\Sigma|$ are small. We prove that
the $k$-anonymization problem is in $P$ when both $m$ and $|\Sigma|$ are
constants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4732</identifier>
 <datestamp>2010-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4732</id><created>2010-04-27</created><updated>2010-05-14</updated><authors><author><keyname>Ostrowski</keyname><forenames>Marcin</forenames></author></authors><title>Minimum energy required to copy one bit of information</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we calculate energy required to copy one bit of useful
information in the presence of thermal noise. For this purpose, we consider a
quantum system capable of storing one bit of classical information, which is
initially in a mixed state corresponding to temperature T. We calculate how
many of these systems must be used to store useful information and control bits
protecting the content against transmission errors. Finally, we analyze how
adding these extra bits changes the total energy consumed during the copying.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4734</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4734</id><created>2010-04-27</created><authors><author><keyname>Geiger</keyname><forenames>Martin Josef</forenames></author></authors><title>On the comparison of plans: Proposition of an instability measure for
  dynamic machine scheduling</title><categories>cs.AI</categories><journal-ref>Proceedings of the 25th Mini EURO Conference on Uncertainty and
  Robustness in Planning and Decision Making, April 15-17, 2010, Coimbra,
  Portugal. ISBN 978-989-95055-3-7.</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On the basis of an analysis of previous research, we present a generalized
approach for measuring the difference of plans with an exemplary application to
machine scheduling. Our work is motivated by the need for such measures, which
are used in dynamic scheduling and planning situations. In this context,
quantitative approaches are needed for the assessment of the robustness and
stability of schedules. Obviously, any `robustness' or `stability' of plans has
to be defined w. r. t. the particular situation and the requirements of the
human decision maker. Besides the proposition of an instability measure, we
therefore discuss possibilities of obtaining meaningful information from the
decision maker for the implementation of the introduced approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4753</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4753</id><created>2010-04-27</created><updated>2010-05-04</updated><authors><author><keyname>Cerri</keyname><forenames>Andrea</forenames></author><author><keyname>Frosini</keyname><forenames>Patrizio</forenames></author></authors><title>Invariance properties of the multidimensional matching distance in
  Persistent Topology and Homology</title><categories>math.AT cs.CG</categories><comments>14 pages, 2 figures</comments><msc-class>Primary 55N35, Secondary 68T10, 68U05, 55N05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Persistent Topology studies topological features of shapes by analyzing the
lower level sets of suitable functions, called filtering functions, and
encoding the arising information in a parameterized version of the Betti
numbers, i.e. the ranks of persistent homology groups. Initially introduced by
considering real-valued filtering functions, Persistent Topology has been
subsequently generalized to a multidimensional setting, i.e. to the case of
$\R^n$-valued filtering functions, leading to studying the ranks of
multidimensional homology groups. In particular, a multidimensional matching
distance has been defined, in order to compare these ranks. The definition of
the multidimensional matching distance is based on foliating the domain of the
ranks of multidimensional homology groups by a collection of half-planes, and
hence it formally depends on a subset of $\R^n\times\R^n$ inducing a
parameterization of these half-planes. It happens that it is possible to choose
this subset in an infinite number of different ways. In this paper we show that
the multidimensional matching distance is actually invariant with respect to
such a choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4758</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4758</id><created>2010-04-27</created><authors><author><keyname>Shinde</keyname><forenames>Sudarshan</forenames></author></authors><title>A Design of Paraunitary Polyphase Matrices of Rational Filter Banks
  Based on (P,Q) Shift-Invariant Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a method to design paraunitary polyphase matrices of
critically sampled rational filter banks. The method is based on (P,Q)
shift-invariant systems, and so any kind of rational splitting of the frequency
spectrum can be achieved using this method. Ideal (P,Q) shift-invariant system
with smallest P and Q that map of a band of input spectrum to the output
spectrum are obtained. A new set of filters is obtained that characterize a
(P,Q) shift-invariant system. Ideal frequency spectrum of these filters are
obtained using ideal $(P,Q)$ shift-invariant systems. Actual paraunitary
polyphase matrices are then obtained by minimizing the stopband energies of
these filters against the parameters of the paraunitary polyphase matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4759</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4759</id><created>2010-04-27</created><authors><author><keyname>Kj&#xe6;rgaard</keyname><forenames>Mikkel Baun</forenames></author></authors><title>Indoor Positioning with Radio Location Fingerprinting</title><categories>cs.NI</categories><comments>PhD Dissertation, Aarhus University</comments><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasingly important requirement for many novel applications is sensing
the positions of people, equipment, etc. GPS technology has proven itself as a
successfull technology for positioning in outdoor environments but indoor no
technology has yet gained a similar wide-scale adoption. A promising indoor
positioning technique is radio-based location fingerprinting, having the major
advantage of exploiting already existing radio infrastructures, like IEEE
802.11, which avoids extra deployment costs and effort. The research goal of
this thesis is to address the limitations of current indoor location
fingerprinting systems. In particular the aim is to advance location
fingerprinting techniques for the challenges of handling heterogeneous clients,
scalability to many clients, and interference between communication and
positioning. The wireless clients used for location fingerprinting are
heterogeneous even when only considering clients for the same technology.
Heterogeneity is a challenge for location fingerprinting because it severely
decreases the precision of location fingerprinting. To support many clients
location fingerprinting has to address how to scale estimate calculation,
measurement distribution, and distribution of position estimates. This is a
challenge because of the number of calculations involved and the frequency of
measurements and position updates. Positioning using location fingerprinting
requires the measurement of, for instance, signal strength for nearby base
stations. However, many wireless communication technologies block communication
while collecting such measurements. This interference is a challenge because it
is not desirable that positioning disables communication. An additional goal is
to improve the conceptual foundation of location fingerprinting. A better
foundation will aid researchers to better survey and design location
fingerprinting systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4769</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4769</id><created>2010-04-27</created><authors><author><keyname>Vabishchevich</keyname><forenames>Nikolay P.</forenames></author><author><keyname>Vabishchevich</keyname><forenames>Petr N.</forenames></author></authors><title>VAGO method for the solution of elliptic second-order boundary value
  problems</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mathematical physics problems are often formulated using differential
oprators of vector analysis - invariant operators of first order, namely,
divergence, gradient and rotor operators. In approximate solution of such
problems it is natural to employ similar operator formulations for grid
problems, too. The VAGO (Vector Analysis Grid Operators) method is based on
such a methodology. In this paper the vector analysis difference operators are
constructed using the Delaunay triangulation and the Voronoi diagrams. Further
the VAGO method is used to solve approximately boundary value problems for the
general elliptic equation of second order. In the convection-diffusion-reaction
equation the diffusion coefficient is a symmetric tensor of second order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4793</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4793</id><created>2010-04-27</created><authors><author><keyname>Fedorov</keyname><forenames>R. K.</forenames></author></authors><title>Logical methods of object recognition on satellite images using spatial
  constraints</title><categories>cs.CV</categories><comments>4 pages</comments><acm-class>I.4.8; I.5.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A logical approach to object recognition on image is proposed. The main idea
of the approach is to perform the object recognition as a logical inference on
a set of rules describing an object shape.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4796</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4796</id><created>2010-04-27</created><updated>2011-04-13</updated><authors><author><keyname>Thielemann</keyname><forenames>Henning</forenames></author></authors><title>Compiling Signal Processing Code embedded in Haskell via LLVM</title><categories>cs.PL cs.SD</categories><comments>8 pages, 1 figure, 3 listings, 1 table, accepted by Linux Audio
  Conference LAC2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss a programming language for real-time audio signal processing that
is embedded in the functional language Haskell and uses the Low-Level Virtual
Machine as back-end. With that framework we can code with the comfort and type
safety of Haskell while achieving maximum efficiency of fast inner loops and
full vectorisation. This way Haskell becomes a valuable alternative to special
purpose signal processing languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4801</identifier>
 <datestamp>2010-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4801</id><created>2010-04-27</created><authors><author><keyname>Besnard</keyname><forenames>Philippe</forenames><affiliation>INRIA - IRISA, IRIT</affiliation></author><author><keyname>Cordier</keyname><forenames>Marie-Odile</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Moinard</keyname><forenames>Yves</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>Ontology-based inference for causal explanation</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>Integrated Computer-Aided Engineering 15, 4 (2008) 351-367</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define an inference system to capture explanations based on causal
statements, using an ontology in the form of an IS-A hierarchy. We first
introduce a simple logical language which makes it possible to express that a
fact causes another fact and that a fact explains another fact. We present a
set of formal inference patterns from causal statements to explanation
statements. We introduce an elementary ontology which gives greater
expressiveness to the system while staying close to propositional reasoning. We
provide an inference system that captures the patterns discussed, firstly in a
purely propositional framework, then in a datalog (limited predicate)
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4802</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4802</id><created>2010-04-27</created><authors><author><keyname>Landsberg</keyname><forenames>J. M.</forenames></author><author><keyname>Manivel</keyname><forenames>Laurent</forenames></author><author><keyname>Ressayre</keyname><forenames>Nicolas</forenames></author></authors><title>Hypersurfaces with degenerate duals and the Geometric Complexity Theory
  Program</title><categories>math.AG cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We determine set-theoretic defining equations for the variety of
hypersurfaces of degree d in an N-dimensional complex vector space that have
dual variety of dimension at most k. We apply these equations to the
Mulmuley-Sohoni variety, the GL_{n^2} orbit closure of the determinant, showing
it is an irreducible component of the variety of hypersurfaces of degree $n$ in
C^{n^2} with dual of dimension at most 2n-2. We establish additional geometric
properties of the Mulmuley-Sohoni variety and prove a quadratic lower bound for
the determinental border-complexity of the permanent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4804</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4804</id><created>2010-04-27</created><authors><author><keyname>Levit</keyname><forenames>Vadim E.</forenames></author><author><keyname>Mandrescu</keyname><forenames>Eugen</forenames></author></authors><title>When G^2 is a Konig-Egervary graph?</title><categories>cs.DM math.CO</categories><comments>6 pages, 4 figures</comments><msc-class>Primary 05C78, 05C69, Secondary 05C12, 05C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The square of a graph G is the graph G^2 with the same vertex set as in G,
and an edge of G^2 is joining two distinct vertices, whenever the distance
between them in G is at most 2. G is a square-stable graph if it enjoys the
property alpha(G)=alpha(G^2), where alpha(G) is the size of a maximum stable
set in G. In this paper we show that G^2 is a Konig-Egervary graph if and only
if G is a square-stable Konig-Egervary graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4806</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4806</id><created>2010-04-27</created><updated>2011-03-25</updated><authors><author><keyname>Arnault</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Berger</keyname><forenames>Thierry</forenames></author><author><keyname>Minier</keyname><forenames>Marine</forenames></author><author><keyname>Pousse</keyname><forenames>Benjamin</forenames></author></authors><title>Revisiting LFSMs</title><categories>cs.CR</categories><comments>Submitted to IEEE-IT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear Finite State Machines (LFSMs) are particular primitives widely used in
information theory, coding theory and cryptography. Among those linear
automata, a particular case of study is Linear Feedback Shift Registers (LFSRs)
used in many cryptographic applications such as design of stream ciphers or
pseudo-random generation. LFSRs could be seen as particular LFSMs without
inputs.
  In this paper, we first recall the description of LFSMs using traditional
matrices representation. Then, we introduce a new matrices representation with
polynomial fractional coefficients. This new representation leads to sparse
representations and implementations. As direct applications, we focus our work
on the Windmill LFSRs case, used for example in the E0 stream cipher and on
other general applications that use this new representation.
  In a second part, a new design criterion called diffusion delay for LFSRs is
introduced and well compared with existing related notions. This criterion
represents the diffusion capacity of an LFSR. Thus, using the matrices
representation, we present a new algorithm to randomly pick LFSRs with good
properties (including the new one) and sparse descriptions dedicated to
hardware and software designs. We present some examples of LFSRs generated
using our algorithm to show the relevance of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4809</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4809</id><created>2010-04-27</created><authors><author><keyname>Lucas</keyname><forenames>Vincent</forenames></author><author><keyname>Pansiot</keyname><forenames>Jean-Jacques</forenames></author><author><keyname>Grad</keyname><forenames>Dominique</forenames></author><author><keyname>Hilt</keyname><forenames>Beno&#xee;t</forenames></author></authors><title>Efficient multicast data transfer with congestion control using dynamic
  source channels</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most efficient receiver-driven multicast congestion control protocols use
dynamic channels. This means that each group has a cyclic rate variation with a
continuously decreasing phase. Despite promising results in terms of fairness,
using efficiently these dynamic groups could be a challenging task for
application programmers. This paper presents a sequencer which maps out
application data to dynamic groups in an optimal way. Multiple applications
such as file transfer or video streaming, can use this sequencer, thanks to a
simple API usable with any buffer containing the most important data first. To
evaluate this solution, we designed a file transfer software using a FEC
encoding. Results show the sequencer optimal behavior and the file transfer
efficiency, as a single download generates only little more overhead than TCP .
Moreover, download time is almost independent of the number of receivers, and
is already faster than TCP with 2 competing downloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4815</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4815</id><created>2010-04-27</created><authors><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author><author><keyname>Pulikkoonattu</keyname><forenames>Rethnakaran</forenames></author></authors><title>Universal A Posteriori Metrics Game</title><categories>cs.IT math.IT math.PR</categories><comments>5 pages, 1 figure, submitted to ITW 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over binary input channels, uniform distribution is a universal prior, in the
sense that it allows to maximize the worst case mutual information over all
binary input channels, ensuring at least 94.2% of the capacity. In this paper,
we address a similar question, but with respect to a universal generalized
linear decoder. We look for the best collection of finitely many a posteriori
metrics, to maximize the worst case mismatched mutual information achieved by
decoding with these metrics (instead of an optimal decoder such as the Maximum
Likelihood (ML) tuned to the true channel). It is shown that for binary input
and output channels, two metrics suffice to actually achieve the same
performance as an optimal decoder. In particular, this implies that there exist
a decoder which is generalized linear and achieves at least 94.2% of the
compound capacity on any compound set, without the knowledge of the underlying
set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4821</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4821</id><created>2010-04-26</created><authors><author><keyname>Bhowmik</keyname><forenames>Wriddhi</forenames></author><author><keyname>Srivastava</keyname><forenames>Shweta</forenames></author></authors><title>Optimum Design of a 4x4 Planar Butler Matrix Array for WLAN Application</title><categories>cs.NI</categories><comments>Wriddhi Bhowmik and Shweta Srivastava, &quot;Optimum Design of a 4x4
  Planar Butler Matrix Array for WLAN Application&quot;, Journal of
  Telecommunications, Volume 2, Issue 1, p68-74, April 2010</comments><journal-ref>Journal of Telecommunications, Volume 2, Issue 1, p68-74, April
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, high-speed wireless communication is in vogue. In wireless
communication systems, multipath fading, delay and interference occurres by
reflection or diffraction. In a high-speed wireless communication, it becomes a
necessary to separate desired signal from delay or interference signal. Thus to
overcome these problems Smart antenna systems have been developed. Basically
there are two types of smart antenna systems, one is Switched beam system and
another Adaptive array system.This paper presents the optimum design of a 4x4
plannar Butler matrix array as a key component of a switched beam smart antenna
system, operating at 5.2 GHz for WLAN with a dielectric substrate, FR4 of er
=4.9 and h=1.6mm. Conception details, simulation results and measurements are
also given for the components (microstrip antenna, hybrid couplers,
cross-coupler, phase shifter) used to implement the matrix. In this
dissertation, mathematical calculations for all the components using MATLAB is
done and then every individual component is designed using the commercial
software SONNET. Then these entire components have been combined on a single
substrate and simulated using SONNET.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4824</identifier>
 <datestamp>2010-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4824</id><created>2010-04-27</created><authors><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Growth and structure of Slovenia's scientific collaboration network</title><categories>physics.soc-ph cond-mat.stat-mech cs.DB</categories><comments>10 pages, 3 figures; accepted for publication in Journal of
  Informetrics [related work available at http://arxiv.org/abs/1003.1018 and
  http://www.matjazperc.com/sicris/stats.html]</comments><journal-ref>Journal of Informetrics 4 (2010) 475-482</journal-ref><doi>10.1016/j.joi.2010.04.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the evolution of Slovenia's scientific collaboration network from
1960 till present with a yearly resolution. For each year the network was
constructed from publication records of Slovene scientists, whereby two were
connected if, up to the given year inclusive, they have coauthored at least one
paper together. Starting with no more than 30 scientists with an average of 1.5
collaborators in the year 1960, the network to date consists of 7380
individuals that, on average, have 10.7 collaborators. We show that, in spite
of the broad myriad of research fields covered, the networks form &quot;small
worlds&quot; and that indeed the average path between any pair of scientists scales
logarithmically with size after the largest component becomes large enough.
Moreover, we show that the network growth is governed by near-liner
preferential attachment, giving rise to a log-normal distribution of
collaborators per author, and that the average starting year is roughly
inversely proportional to the number of collaborators eventually acquired.
Understandably, not all that became active early have till now gathered many
collaborators. We also give results for the clustering coefficient and the
diameter of the network over time, and compare our conclusions with those
reported previously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4826</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4826</id><created>2010-04-27</created><authors><author><keyname>Hou</keyname><forenames>Xueying</forenames></author><author><keyname>Yang</keyname><forenames>Chenyang</forenames></author></authors><title>Impact of Channel Asymmetry on Base Station Cooperative Transmission
  with Limited Feedback</title><categories>cs.IT math.IT</categories><comments>submitted to the IEEE Global Telecommunication Conference (GlobeCom)
  2010.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Base station (BS) cooperative transmission, also known as coordinated
multi-point transmission (CoMP), is an effective way to avoid inter-cell
interference in universal frequency reuse cellular systems. To gain the
promised benefit, however, huge feedback overhead is in demand to gather the
channel information. In this paper, we analyze the impact of channel asymmetry,
which is inherent in CoMP systems, on downlink BS cooperative transmission with
limited feedback. We analyze the per-user rate loss of a multi-user CoMP system
led by quantization. Per-cell quantization of multicell channels is considered,
which quantizes the local channel and cross channel separately and is more
feasible in practice. From both the analytical and simulation results, we
provide a whole picture on various critical factors that lead to the
performance loss. Specifically, we show that the per user rate loss led by
limited feedback depends on the location of its paired users, except for
relying on its own signal to noise ratio and the quantization errors as in
single cell multi-user multiple antenna systems. This implies that the
quantization accuracy required for local and cross channel of each user depends
on the locations of its own as well as its paired users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4830</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4830</id><created>2010-04-26</created><authors><author><keyname>Reza</keyname><forenames>Md. Shamim</forenames></author><author><keyname>Hossain</keyname><forenames>Md. Maruf</forenames></author><author><keyname>Chowdhury</keyname><forenames>Adnan Ahmed</forenames></author><author><keyname>Reza</keyname><forenames>S. M. Shamim</forenames></author><author><keyname>Rahman</keyname><forenames>Md. Moshiur</forenames></author></authors><title>Performance Evaluation of SCM-WDM System Using Different Linecoding</title><categories>cs.OH cs.IT math.IT</categories><comments>Md. Shamim Reza, Md. Maruf Hossain, Adnan Ahmed Chowdhury, S. M.
  Shamim Reza and Md. Moshiur Rahman, &quot;Performance Evaluation of SCM-WDM System
  Using Different Linecoding&quot;, Journal of Telecommunications, Volume 2, Issue
  1, p60-67, April 2010</comments><journal-ref>Journal of Telecommunications, Volume 2, Issue 1, p60-67, April
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the theoretical performance analysis for a subcarrier
multiplexed (SCM) wavelength division multiplexing (WDM) optical transmission
system in presence of optical beat interference (OBI) which occurs during the
photo detection process. We have presented a comparison for improving the
performance of SCM-WDM system in presence of OBI. Non-return-to zero (NRZ),
Manchester and Miller code (MC) line coding are used for performance
investigation of SCM-WDM system. A suitable signal bandwidth is selected and
200 KHz is considered as channel bandwidth. Power spectrum of signal and cross
component for those line coding are analyzed. Comparison results are evaluated
in terms of signal to OBI ratio for the three linecoding schemes which is
called signal to interference ratio (SIR). It is found that there is a
significant increase in the SIR by employing Miller code compared to NRZ and
Manchester for the same data rate. For example, for a number of subcarriers of
10, the achievable SIR is about -24 dB for Miller coded system compared to -46
dB for NRZ coded system and -49 dB for Manchester coded system. The results are
found to be satisfactorily agreed with the expected results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4848</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4848</id><created>2010-04-27</created><authors><author><keyname>Ausloos</keyname><forenames>M.</forenames></author></authors><title>Punctuation effects in English and Esperanto texts</title><categories>cs.CL physics.data-an</categories><comments>13 pages, 7 figures (3x2+1), 60 references</comments><journal-ref>Physica A389 (2010) 2835-2840</journal-ref><doi>10.1016/j.physa.2010.02.038</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A statistical physics study of punctuation effects on sentence lengths is
presented for written texts: {\it Alice in wonderland} and {\it Through a
looking glass}. The translation of the first text into esperanto is also
considered as a test for the role of punctuation in defining a style, and for
contrasting natural and artificial, but written, languages. Several log-log
plots of the sentence length-rank relationship are presented for the major
punctuation marks. Different power laws are observed with characteristic
exponents. The exponent can take a value much less than unity ($ca.$ 0.50 or
0.30) depending on how a sentence is defined. The texts are also mapped into
time series based on the word frequencies. The quantitative differences between
the original and translated texts are very minutes, at the exponent level. It
is argued that sentences seem to be more reliable than word distributions in
discussing an author style.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4864</identifier>
 <datestamp>2010-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4864</id><created>2010-04-27</created><authors><author><keyname>Belkin</keyname><forenames>Mikhail</forenames></author><author><keyname>Sinha</keyname><forenames>Kaushik</forenames></author></authors><title>Polynomial Learning of Distribution Families</title><categories>cs.LG cs.DS</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question of polynomial learnability of probability distributions,
particularly Gaussian mixture distributions, has recently received significant
attention in theoretical computer science and machine learning. However,
despite major progress, the general question of polynomial learnability of
Gaussian mixture distributions still remained open. The current work resolves
the question of polynomial learnability for Gaussian mixtures in high dimension
with an arbitrary fixed number of components. The result on learning Gaussian
mixtures relies on an analysis of distributions belonging to what we call
&quot;polynomial families&quot; in low dimension. These families are characterized by
their moments being polynomial in parameters and include almost all common
probability distributions as well as their mixtures and products. Using tools
from real algebraic geometry, we show that parameters of any distribution
belonging to such a family can be learned in polynomial time and using a
polynomial number of sample points. The result on learning polynomial families
is quite general and is of independent interest. To estimate parameters of a
Gaussian mixture distribution in high dimensions, we provide a deterministic
algorithm for dimensionality reduction. This allows us to reduce learning a
high-dimensional mixture to a polynomial number of parameter estimations in low
dimension. Combining this reduction with the results on polynomial families
yields our result on learning arbitrary Gaussian mixtures in high dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4880</identifier>
 <datestamp>2010-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4880</id><created>2010-04-27</created><updated>2010-11-05</updated><authors><author><keyname>Qiu</keyname><forenames>Kun</forenames></author><author><keyname>Dogandzic</keyname><forenames>Aleksandar</forenames></author></authors><title>ECME Thresholding Methods for Sparse Signal Reconstruction</title><categories>cs.IT math.IT</categories><comments>39 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a probabilistic framework for interpreting and developing hard
thresholding sparse signal reconstruction methods and present several new
algorithms based on this framework. The measurements follow an underdetermined
linear model, where the regression-coefficient vector is the sum of an unknown
deterministic sparse signal component and a zero-mean white Gaussian component
with an unknown variance. We first derive an expectation-conditional
maximization either (ECME) iteration that guarantees convergence to a local
maximum of the likelihood function of the unknown parameters for a given signal
sparsity level. To analyze the reconstruction accuracy, we introduce the
minimum sparse subspace quotient (SSQ), a more flexible measure of the sampling
operator than the well-established restricted isometry property (RIP). We prove
that, if the minimum SSQ is sufficiently large, ECME achieves perfect or
near-optimal recovery of sparse or approximately sparse signals, respectively.
We also propose a double overrelaxation (DORE) thresholding scheme for
accelerating the ECME iteration. If the signal sparsity level is unknown, we
introduce an unconstrained sparsity selection (USS) criterion for its selection
and show that, under certain conditions, applying this criterion is equivalent
to finding the sparsest solution of the underlying underdetermined linear
system. Finally, we present our automatic double overrelaxation (ADORE)
thresholding method that utilizes the USS criterion to select the signal
sparsity level. We apply the proposed schemes to reconstruct sparse and
approximately sparse signals from tomographic projections and compressive
samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4882</identifier>
 <datestamp>2010-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4882</id><created>2010-04-27</created><authors><author><keyname>Silberstein</keyname><forenames>Natalia</forenames></author></authors><title>Properties of Codes in the Johnson Scheme</title><categories>cs.IT math.IT</categories><comments>This is an M.Sc.thesis submitted in February 2007 by Natalia
  Silberstein and supervised by Prof. Tuvi Etzion</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Codes which attain the sphere packing bound are called perfect codes. The
most important metrics in coding theory on which perfect codes are defined are
the Hamming metric and the Johnson metric. While for the Hamming metric all
perfect codes over finite fields are known, in the Johnson metric it was
conjectured by Delsarte in 1970's that there are no nontrivial perfect codes.
The general nonexistence proof still remains the open problem. In this work we
examine constant weight codes as well as doubly constant weight codes, and
reduce the range of parameters in which perfect codes may exist in both cases.
We start with the constant weight codes. We introduce an improvement of Roos'
bound for one-perfect codes, and present some new divisibility conditions,
which are based on the connection between perfect codes in Johnson graph J(n,w)
and block designs. Next, we consider binomial moments for perfect codes. We
show which parameters can be excluded for one-perfect codes. We examine
two-perfect codes in J(2w,w) and present necessary conditions for existence of
such codes. We prove that there are no two-perfect codes in J(2w,w) with length
less then 2.5*10^{15}. Next we examine perfect doubly constant weight codes. We
present a family of parameters for codes whose size of sphere divides the size
of whole space. We then prove a bound on length of such codes, similarly to
Roos' bound for perfect codes in Johnson graph. Finally we describe Steiner
systems and doubly Steiner systems, which are strongly connected with the
constant weight and doubly constant weight codes respectively. We provide an
anticode-based proof of a bound on length of Steiner system, prove that doubly
Steiner system is a diameter perfect code and present a bound on length of
doubly Steiner system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4901</identifier>
 <datestamp>2010-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4901</id><created>2010-04-27</created><authors><author><keyname>Cheung</keyname><forenames>Yam Ki</forenames></author><author><keyname>Daescu</keyname><forenames>Ovidiu</forenames></author></authors><title>Fr\'echet Distance Problems in Weighted Regions</title><categories>cs.CG</categories><comments>24 pages 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss two versions of the Fr\'echet distance problem in weighted planar
subdivisions. In the first one, the distance between two points is the weighted
length of the line segment joining the points. In the second one, the distance
between two points is the length of the shortest path between the points. In
both cases, we give algorithms for finding a (1+epsilon)-factor approximation
of the Fr\'echet distance between two polygonal curves. We also consider the
Fr\'echet distance between two polygonal curves among polyhedral obstacles in
R^3 and present a (1+epsilon)-factor approximation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4909</identifier>
 <datestamp>2010-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4909</id><created>2010-04-27</created><authors><author><keyname>Chanal</keyname><forenames>Val&#xe9;rie</forenames></author><author><keyname>Kimble</keyname><forenames>Chris</forenames></author></authors><title>Born to be Wild: Using Communities of Practice as a Tool for Knowledge
  Management</title><categories>cs.CY</categories><comments>Paper presented at the Ethicomp 2010: The 'Backwards, Forwards and
  Sideways' changes of ICT, Tarragona, Spain, April, 2010, pp. 71 - 80.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper looks at what happens when Communities of Practice are used as a
tool for Knowledge Management. The original concept of a Community of Practice
appears to have very little in common with the knowledge sharing communities
found in Knowledge Management, which are based on a revised view of
'cultivated' communities. We examine the risks and benefits of cultivating
Communities of Practice rather than leaving them 'in the wild'. The paper
presents the findings from two years of research in a small microelectronics
firm to provide some insights into the wild vs domesticated dichotomy and
discusses the implications of attempting to tame Communities of Practice in
this way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4914</identifier>
 <datestamp>2010-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4914</id><created>2010-04-27</created><updated>2010-05-13</updated><authors><author><keyname>Katta</keyname><forenames>Sandeep</forenames></author></authors><title>Recursive Information Hiding in Visual Cryptography</title><categories>cs.CR</categories><comments>13 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual Cryptography is a secret sharing scheme that uses the human visual
system to perform computations. This paper presents a recursive hiding scheme
for 3 out of 5 secret sharing. The idea used is to hide smaller secrets in the
shares of a larger secret without an expansion in the size of the latter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4915</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4915</id><created>2010-04-27</created><authors><author><keyname>Goel</keyname><forenames>Ashish</forenames></author><author><keyname>Kapralov</keyname><forenames>Michael</forenames></author><author><keyname>Khanna</keyname><forenames>Sanjeev</forenames></author></authors><title>Graph Sparsification via Refinement Sampling</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph G'(V,E') is an \eps-sparsification of G for some \eps&gt;0, if every
(weighted) cut in G' is within (1\pm \eps) of the corresponding cut in G. A
celebrated result of Benczur and Karger shows that for every undirected graph
G, an \eps-sparsification with O(n\log n/\e^2) edges can be constructed in
O(m\log^2n) time. Applications to modern massive data sets often constrain
algorithms to use computation models that restrict random access to the input.
The semi-streaming model, in which the algorithm is constrained to use \tilde
O(n) space, has been shown to be a good abstraction for analyzing graph
algorithms in applications to large data sets. Recently, a semi-streaming
algorithm for graph sparsification was presented by Anh and Guha; the total
running time of their implementation is \Omega(mn), too large for applications
where both space and time are important. In this paper, we introduce a new
technique for graph sparsification, namely refinement sampling, that gives an
\tilde{O}(m) time semi-streaming algorithm for graph sparsification.
  Specifically, we show that refinement sampling can be used to design a
one-pass streaming algorithm for sparsification that takes O(\log\log n) time
per edge, uses O(\log^2 n) space per node, and outputs an \eps-sparsifier with
O(n\log^3 n/\eps^2) edges. At a slightly increased space and time complexity,
we can reduce the sparsifier size to O(n \log n/\e^2) edges matching the
Benczur-Karger result, while improving upon the Benczur-Karger runtime for
m=\omega(n\log^3 n). Finally, we show that an \eps-sparsifier with O(n \log
n/\eps^2) edges can be constructed in two passes over the data and O(m) time
whenever m =\Omega(n^{1+\delta}) for some constant \delta&gt;0. As a by-product of
our approach, we also obtain an O(m\log\log n+n \log n) time streaming
algorithm to compute a sparse k-connectivity certificate of a graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4917</identifier>
 <datestamp>2010-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4917</id><created>2010-04-27</created><authors><author><keyname>Piantanida</keyname><forenames>Pablo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>On the Capacity of Compound State-Dependent Channels with States Known
  at the Transmitter</title><categories>cs.IT math.IT math.PR</categories><comments>TO APPEAR IN PROC. OF IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION
  THEORY (ISIT2010).</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the capacity of compound state-dependent channels
with non-causal state information available at only the transmitter. A new
lower bound on the capacity of this class of channels is derived. This bound is
shown to be tight for the special case of compound channels with stochastic
degraded components, yielding the full characterization of the capacity.
Specific results are derived for the compound Gaussian Dirty-Paper (GDP)
channel. This model consists of an additive white Gaussian noise (AWGN) channel
corrupted by an additive Gaussian interfering signal, known at the transmitter
only, where the input and the state signals are affected by fading coefficients
whose realizations are unknown at the transmitter. Our bounds are shown to be
tight for specific cases. Applications of these results arise in a variety of
wireless scenarios as multicast channels, cognitive radio and problems with
interference cancellation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4940</identifier>
 <datestamp>2010-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4940</id><created>2010-04-27</created><authors><author><keyname>Gualtieri</keyname><forenames>Devlin M.</forenames></author></authors><title>FauxCrypt - A Method of Text Obfuscation</title><categories>cs.CR</categories><comments>Five pages. Source code available at fauxcrypt.org</comments><acm-class>D.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Warnings have been raised about the steady diminution of privacy. More and
more personal information, such as that contained electronic mail, is moving to
cloud computing servers where it might be machine-searched and indexed.
FauxCrypt is an algorithm for modification of a plaintext document that leaves
it generally readable by a person but not readily searched or indexed by
machine. The algorithm employs a dictionary substitution of selected words, and
an obfuscating transposition of letters in other words. The obfuscation is
designed to leave the words understandable, although they are badly spelled.
FauxCrypt is free, open source software, with source code available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4942</identifier>
 <datestamp>2011-03-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4942</id><created>2010-04-27</created><updated>2011-03-23</updated><authors><author><keyname>Watanabe</keyname><forenames>Yusuke</forenames></author></authors><title>Discrete geometric analysis of message passing algorithm on graphs</title><categories>cs.DM</categories><comments>PhD thesis; March 24, 2010; 156 pages. Typos are corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We often encounter probability distributions given as unnormalized products
of non-negative functions. The factorization structures are represented by
hypergraphs called factor graphs. Such distributions appear in various fields,
including statistics, artificial intelligence, statistical physics, error
correcting codes, etc. Given such a distribution, computations of marginal
distributions and the normalization constant are often required. However, they
are computationally intractable because of their computational costs. One
successful approximation method is Loopy Belief Propagation (LBP) algorithm.
The focus of this thesis is an analysis of the LBP algorithm. If the factor
graph is a tree, i.e. having no cycle, the algorithm gives the exact
quantities. If the factor graph has cycles, however, the LBP algorithm does not
give exact results and possibly exhibits oscillatory and non-convergent
behaviors. The thematic question of this thesis is &quot;How the behaviors of the
LBP algorithm are affected by the discrete geometry of the factor graph?&quot; The
primary contribution of this thesis is the discovery of a formula that
establishes the relation between the LBP, the Bethe free energy and the graph
zeta function. This formula provides new techniques for analysis of the LBP
algorithm, connecting properties of the graph and of the LBP and the Bethe free
energy. We demonstrate applications of the techniques to several problems
including (non) convexity of the Bethe free energy, the uniqueness and
stability of the LBP fixed point. We also discuss the loop series initiated by
Chertkov and Chernyak. The loop series is a subgraph expansion of the
normalization constant, or partition function, and reflects the graph geometry.
We investigate theoretical natures of the series. Moreover, we show a partial
connection between the loop series and the graph zeta function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4944</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4944</id><created>2010-04-27</created><updated>2010-05-18</updated><authors><author><keyname>Rini</keyname><forenames>Stefano</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author><author><keyname>Devroye</keyname><forenames>Natasha</forenames></author></authors><title>Outer Bounds for the Interference Channel with a Cognitive Relay</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we first present an outer bound for a general interference
channel with a cognitive relay, i.e., a relay that has non-causal knowledge of
both independent messages transmitted in the interference channel. This outer
bound reduces to the capacity region of the deterministic broadcast channel and
of the deterministic cognitive interference channel through nulling of certain
channel inputs. It does not, however, reduce to that of certain deterministic
interference channels for which capacity is known. As such, we subsequently
tighten the bound for channels whose outputs satisfy an &quot;invertibility&quot;
condition. This second outer bound now reduces to the capacity of this special
class of deterministic interference channels. The second outer bound is further
tightened for the high SNR deterministic approximation of the Gaussian
interference channel with a cognitive relay by exploiting the special structure
of the interference. We provide an example that suggests that this third bound
is tight in at least some parameter regimes for the high SNR deterministic
approximation of the Gaussian channel. Another example shows that the third
bound is capacity in the special case where there are no direct links between
the non-cognitive transmitters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4949</identifier>
 <datestamp>2010-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4949</id><created>2010-04-28</created><authors><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author><author><keyname>Jafarpour</keyname><forenames>Sina</forenames></author></authors><title>Reed Muller Sensing Matrices and the LASSO</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct two families of deterministic sensing matrices where the columns
are obtained by exponentiating codewords in the quaternary Delsarte-Goethals
code $DG(m,r)$. This method of construction results in sensing matrices with
low coherence and spectral norm. The first family, which we call
Delsarte-Goethals frames, are $2^m$ - dimensional tight frames with redundancy
$2^{rm}$. The second family, which we call Delsarte-Goethals sieves, are
obtained by subsampling the column vectors in a Delsarte-Goethals frame.
Different rows of a Delsarte-Goethals sieve may not be orthogonal, and we
present an effective algorithm for identifying all pairs of non-orthogonal
rows. The pairs turn out to be duplicate measurements and eliminating them
leads to a tight frame. Experimental results suggest that all $DG(m,r)$ sieves
with $m\leq 15$ and $r\geq2$ are tight-frames; there are no duplicate rows. For
both families of sensing matrices, we measure accuracy of reconstruction
(statistical 0-1 loss) and complexity (average reconstruction time) as a
function of the sparsity level $k$. Our results show that DG frames and sieves
outperform random Gaussian matrices in terms of noiseless and noisy signal
recovery using the LASSO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4960</identifier>
 <datestamp>2010-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4960</id><created>2010-04-28</created><updated>2010-07-30</updated><authors><author><keyname>Koiran</keyname><forenames>Pascal</forenames><affiliation>LIP</affiliation></author></authors><title>Shallow Circuits with High-Powered Inputs</title><categories>cs.CC</categories><comments>A few typos corrected</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A polynomial identity testing algorithm must determine whether an input
polynomial (given for instance by an arithmetic circuit) is identically equal
to 0. In this paper, we show that a deterministic black-box identity testing
algorithm for (high-degree) univariate polynomials would imply a lower bound on
the arithmetic complexity of the permanent. The lower bounds that are known to
follow from derandomization of (low-degree) multivariate identity testing are
weaker. To obtain our lower bound it would be sufficient to derandomize
identity testing for polynomials of a very specific norm: sums of products of
sparse polynomials with sparse coefficients. This observation leads to new
versions of the Shub-Smale tau-conjecture on integer roots of univariate
polynomials. In particular, we show that a lower bound for the permanent would
follow if one could give a good enough bound on the number of real roots of
sums of products of sparse polynomials (Descartes' rule of signs gives such a
bound for sparse polynomials and products thereof). In this third version of
our paper we show that the same lower bound would follow even if one could only
prove a slightly superpolynomial upper bound on the number of real roots. This
is a consequence of a new result on reduction to depth 4 for arithmetic
circuits which we establish in a companion paper. We also show that an even
weaker bound on the number of real roots would suffice to obtain a lower bound
on the size of depth 4 circuits computing the permanent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4965</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4965</id><created>2010-04-28</created><authors><author><keyname>Zaslavskiy</keyname><forenames>Mikhail</forenames><affiliation>CBIO</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>INRIA Rocquencourt, LIENS</affiliation></author><author><keyname>Vert</keyname><forenames>Jean-Philippe</forenames><affiliation>CBIO</affiliation></author></authors><title>Many-to-Many Graph Matching: a Continuous Relaxation Approach</title><categories>stat.ML cs.CV</categories><comments>19</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs provide an efficient tool for object representation in various
computer vision applications. Once graph-based representations are constructed,
an important question is how to compare graphs. This problem is often
formulated as a graph matching problem where one seeks a mapping between
vertices of two graphs which optimally aligns their structure. In the classical
formulation of graph matching, only one-to-one correspondences between vertices
are considered. However, in many applications, graphs cannot be matched
perfectly and it is more interesting to consider many-to-many correspondences
where clusters of vertices in one graph are matched to clusters of vertices in
the other graph. In this paper, we formulate the many-to-many graph matching
problem as a discrete optimization problem and propose an approximate algorithm
based on a continuous relaxation of the combinatorial problem. We compare our
method with other existing methods on several benchmark computer vision
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4968</identifier>
 <datestamp>2010-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4968</id><created>2010-04-28</created><updated>2010-08-05</updated><authors><author><keyname>Chu</keyname><forenames>Hsuan-Yi</forenames></author></authors><title>On the Achievable Rate Regions for a Class of Cognitive Radio Channels:
  Interference Channel with Degraded Message Sets with Unidirectional
  Destination Cooperation</title><categories>cs.IT math.IT</categories><msc-class>94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the capacity gains due to unidirectional destination
cooperation in cognitive radio channels. We propose a novel channel,
interference channel with degraded message sets with unidirectional destination
cooperation (IC-DMS-UDC), to allow the receiver of cognitive radio (secondary
user) to participate in relaying the information for primary system (legitimate
user). Our main result is the development of an achievable rate region which
combines Gel'fand-Pinkser coding with partial-decode-and-forward strategy
employed in the relay channel. A numerical evaluation of the region in the
Gaussian case is also provided to demonstrate the improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.4998</identifier>
 <datestamp>2010-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.4998</id><created>2010-04-28</created><authors><author><keyname>Dom&#xed;nguez</keyname><forenames>C&#xe9;sar</forenames></author><author><keyname>Rubio</keyname><forenames>Julio</forenames></author></authors><title>Computing in Coq with Infinite Algebraic Data Structures</title><categories>cs.LO</categories><comments>To appear in Conferences on Intelligent Computer Mathematics 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational content encoded into constructive type theory proofs can be
used to make computing experiments over concrete data structures. In this
paper, we explore this possibility when working in Coq with chain complexes of
infinite type (that is to say, generated by infinite sets) as a part of the
formalization of a hierarchy of homological algebra structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5009</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5009</id><created>2010-04-28</created><authors><author><keyname>Galam</keyname><forenames>Serge</forenames></author></authors><title>Public debates driven by incomplete scientific data: the cases of
  evolution theory, global warming and H1N1 pandemic influenza</title><categories>physics.pop-ph cs.CY nlin.AO physics.soc-ph</categories><comments>31 pages, 7 figures</comments><doi>10.1016/j.physa.2010.04.039</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Public debates driven by incomplete scientific data where nobody can claim
absolute certainty, due to current state of scientific knowledge, are studied.
The cases of evolution theory, global warming and H1N1 pandemic influenza are
investigated. The first two are of controversial impact while the third is more
neutral and resolved. To adopt a cautious balanced attitude based on clear but
inconclusive data appears to be a lose-out strategy. In contrast overstating
arguments with wrong claims which cannot be scientifically refuted appear to be
necessary but not sufficient to eventually win a public debate. The underlying
key mechanism of these puzzling and unfortunate conclusions are identified
using the Galam sequential probabilistic model of opinion dynamics. It reveals
that the existence of inflexible agents and their respective proportions are
the instrumental parameters to determine the faith of incomplete scientific
data public debates. Acting on one's own inflexible proportion modifies the
topology of the flow diagram, which in turn can make irrelevant initial
supports. On the contrary focusing on open-minded agents may be useless given
some topologies. When the evidence is not as strong as claimed, the inflexibles
rather than the data are found to drive the opinion of the population. The
results shed a new but disturbing light on designing adequate strategies to win
a public debate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5010</identifier>
 <datestamp>2010-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5010</id><created>2010-04-28</created><updated>2010-10-15</updated><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Michal</forenames></author><author><keyname>Wojtaszczyk</keyname><forenames>Jakub Onufry</forenames></author></authors><title>The stubborn problem is stubborn no more (a polynomial algorithm for
  3-compatible colouring and the stubborn list partition problem)</title><categories>cs.DS</categories><comments>Full version of the paper accepted to SODA'11</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the driving problems in the CSP area is the Dichotomy Conjecture,
formulated in 1993 by Feder and Vardi [STOC'93], stating that for any fixed
relational structure G the Constraint Satisfaction Problem CSP(G) is either
NP--complete or polynomial time solvable. A large amount of research has gone
into checking various specific cases of this conjecture. One such variant which
attracted a lot of attention in the recent years is the LIST MATRIX PARTITION
problem. In 2004 Cameron et al. [SODA'04] classified almost all LIST MATRIX
PARTITION variants for matrices of size at most four. The only case which
resisted the classification became known as the STUBBORN PROBLEM. In this paper
we show a result which enables us to finish the classification - thus solving a
problem which resisted attacks for the last six years.
  Our approach is based on a combinatorial problem known to be at least as hard
as the STUBBORN PROBLEM - the 3-COMPATIBLE COLOURING problem. In this problem
we are given a complete graph with each edge assigned one of 3 possible colours
and we want to assign one of those 3 colours to each vertex in such a way that
no edge has the same colour as both of its endpoints. The tractability of the
3-COMPATIBLE COLOURING problem has been open for several years and the best
known algorithm prior to this paper is due to Feder et al. [SODA'05] - a
quasipolynomial algorithm with a n^O(log n / log log n) time complexity. In
this paper we present a polynomial-time algorithm for the 3-COMPATIBLE
COLOURING problem and consequently we prove a dichotomy for the k-COMPATIBLE
COLOURING problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5012</identifier>
 <datestamp>2010-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5012</id><created>2010-04-28</created><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author></authors><title>Bandwidth and Distortion Revisited</title><categories>cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we merge recent developments on exact algorithms for finding an
ordering of vertices of a given graph that minimizes bandwidth (the BANDWIDTH
problem) and for finding an embedding of a given graph into a line that
minimizes distortion (the DISTORTION problem). For both problems we develop
algorithms that work in O(9.363^n) time and polynomial space. For BANDWIDTH,
this improves O^*(10^n) algorithm by Feige and Kilian from 2000, for DISTORTION
this is the first polynomial space exact algorithm that works in O(c^n) time we
are aware of. As a byproduct, we enhance the O(5^{n+o(n)})-time and
O^*(2^n)-space algorithm for DISTORTION by Fomin et al. to an algorithm working
in O(4.383^n) time and space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5026</identifier>
 <datestamp>2010-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5026</id><created>2010-04-28</created><authors><author><keyname>Blanchard</keyname><forenames>Jeffrey D.</forenames></author><author><keyname>Cartis</keyname><forenames>Coralia</forenames></author><author><keyname>Tanner</keyname><forenames>Jared</forenames></author></authors><title>Compressed Sensing: How sharp is the Restricted Isometry Property</title><categories>cs.IT math.IT</categories><comments>21 pages, 7 figures, 54 references. To appear, SIAM Review.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed Sensing (CS) seeks to recover an unknown vector with $N$ entries
by making far fewer than $N$ measurements; it posits that the number of
compressed sensing measurements should be comparable to the information content
of the vector, not simply $N$. CS combines the important task of compression
directly with the measurement task. Since its introduction in 2004 there have
been hundreds of manuscripts on CS, a large fraction of which develop
algorithms to recover a signal from its compressed measurements. Because of the
paradoxical nature of CS -- exact reconstruction from seemingly undersampled
measurements -- it is crucial for acceptance of an algorithm that rigorous
analyses verify the degree of undersampling the algorithm permits. The
Restricted Isometry Property (RIP) has become the dominant tool used for the
analysis in such cases. We present here an asymmetric form of RIP which gives
tighter bounds than the usual symmetric one. We give the best known bounds on
the RIP constants for matrices from the Gaussian ensemble. Our derivations
illustrate the way in which the combinatorial nature of CS is controlled. Our
quantitative bounds on the RIP allow precise statements as to how aggressively
a signal can be undersampled, the essential question for practitioners. We also
document the extent to which RIP gives precise information about the true
performance limits of CS, by comparing with approaches from high-dimensional
geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5034</identifier>
 <datestamp>2010-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5034</id><created>2010-04-28</created><authors><author><keyname>Butelle</keyname><forenames>Franck</forenames></author><author><keyname>Hivert</keyname><forenames>Florent</forenames></author><author><keyname>Mayero</keyname><forenames>Micaela</forenames></author><author><keyname>Toumazet</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author></authors><title>Formal Proof of SCHUR Conjugate Function</title><categories>cs.LO cs.MS cs.SC cs.SE</categories><comments>To appear in CALCULEMUS 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main goal of our work is to formally prove the correctness of the key
commands of the SCHUR software, an interactive program for calculating with
characters of Lie groups and symmetric functions. The core of the computations
relies on enumeration and manipulation of combinatorial structures. As a first
&quot;proof of concept&quot;, we present a formal proof of the conjugate function,
written in C. This function computes the conjugate of an integer partition. To
formally prove this program, we use the Frama-C software. It allows us to
annotate C functions and to generate proof obligations, which are proved using
several automated theorem provers. In this paper, we also draw on methodology,
discussing on how to formally prove this kind of program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5049</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5049</id><created>2010-04-28</created><updated>2012-04-19</updated><authors><author><keyname>Nielsen</keyname><forenames>Frank</forenames></author><author><keyname>Boltz</keyname><forenames>Sylvain</forenames></author></authors><title>The Burbea-Rao and Bhattacharyya centroids</title><categories>cs.IT cs.CG math.IT</categories><comments>13 pages</comments><journal-ref>IEEE Transactions on Information Theory 57(8):5455-5466, 2011</journal-ref><doi>10.1109/TIT.2011.2159046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the centroid with respect to the class of information-theoretic
Burbea-Rao divergences that generalize the celebrated Jensen-Shannon divergence
by measuring the non-negative Jensen difference induced by a strictly convex
and differentiable function. Although those Burbea-Rao divergences are
symmetric by construction, they are not metric since they fail to satisfy the
triangle inequality. We first explain how a particular symmetrization of
Bregman divergences called Jensen-Bregman distances yields exactly those
Burbea-Rao divergences. We then proceed by defining skew Burbea-Rao
divergences, and show that skew Burbea-Rao divergences amount in limit cases to
compute Bregman divergences. We then prove that Burbea-Rao centroids are
unique, and can be arbitrarily finely approximated by a generic iterative
concave-convex optimization algorithm with guaranteed convergence property. In
the second part of the paper, we consider the Bhattacharyya distance that is
commonly used to measure overlapping degree of probability distributions. We
show that Bhattacharyya distances on members of the same statistical
exponential family amount to calculate a Burbea-Rao divergence in disguise.
Thus we get an efficient algorithm for computing the Bhattacharyya centroid of
a set of parametric distributions belonging to the same exponential families,
improving over former specialized methods found in the literature that were
limited to univariate or &quot;diagonal&quot; multivariate Gaussians. To illustrate the
performance of our Bhattacharyya/Burbea-Rao centroid algorithm, we present
experimental performance results for $k$-means and hierarchical clustering
methods of Gaussian mixture models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5051</identifier>
 <datestamp>2010-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5051</id><created>2010-04-28</created><authors><author><keyname>Hurley</keyname><forenames>Aaron C</forenames></author><author><keyname>Al-Radaideh</keyname><forenames>Ali</forenames></author><author><keyname>Li</keyname><forenames>Bai</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Coxon</keyname><forenames>Ron</forenames></author><author><keyname>Glover</keyname><forenames>Paul</forenames></author><author><keyname>Gowland</keyname><forenames>Penny A.</forenames></author></authors><title>Tailored RF pulse optimization for magnetization inversion at ultra high
  field</title><categories>cs.CE cs.NE physics.med-ph</categories><comments>8 pages, 9 figures, Magnetic Resonance Imaging for Medicine</comments><journal-ref>Magnetic Resonance Imaging for Medicine, 63(1), p51-58, 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The radiofrequency (RF) transmit field is severely inhomogeneous at ultrahigh
field due to both RF penetration and RF coil design issues. This particularly
impairs image quality for sequences that use inversion pulses such as
magnetization prepared rapid acquisition gradient echo and limits the use of
quantitative arterial spin labeling sequences such as flow-attenuated inversion
recovery. Here we have used a search algorithm to produce inversion pulses
tailored to take into account the heterogeneity of the RF transmit field at 7
T. This created a slice selective inversion pulse that worked well (good slice
profile and uniform inversion) over the range of RF amplitudes typically
obtained in the head at 7 T while still maintaining an experimentally
achievable pulse length and pulse amplitude in the brain at 7 T. The pulses
used were based on the frequency offset correction inversion technique, as well
as time dilation of functions, but the RF amplitude, frequency sweep, and
gradient functions were all generated using a genetic algorithm with an
evaluation function that took into account both the desired inversion profile
and the transmit field inhomogeneity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5070</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5070</id><created>2010-04-28</created><updated>2011-01-04</updated><authors><author><keyname>Gedalyahu</keyname><forenames>Kfir</forenames></author><author><keyname>Tur</keyname><forenames>Ronen</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Multichannel Sampling of Pulse Streams at the Rate of Innovation</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2011.2105481</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider minimal-rate sampling schemes for infinite streams of delayed and
weighted versions of a known pulse shape. The minimal sampling rate for these
parametric signals is referred to as the rate of innovation and is equal to the
number of degrees of freedom per unit time. Although sampling of infinite pulse
streams was treated in previous works, either the rate of innovation was not
achieved, or the pulse shape was limited to Diracs. In this paper we propose a
multichannel architecture for sampling pulse streams with arbitrary shape,
operating at the rate of innovation. Our approach is based on modulating the
input signal with a set of properly chosen waveforms, followed by a bank of
integrators. This architecture is motivated by recent work on sub-Nyquist
sampling of multiband signals. We show that the pulse stream can be recovered
from the proposed minimal-rate samples using standard tools taken from spectral
estimation in a stable way even at high rates of innovation. In addition, we
address practical implementation issues, such as reduction of hardware
complexity and immunity to failure in the sampling channels. The resulting
scheme is flexible and exhibits better noise robustness than previous
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5071</identifier>
 <datestamp>2010-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5071</id><created>2010-04-28</created><authors><author><keyname>Kohlhase</keyname><forenames>Andrea</forenames></author><author><keyname>Kohlhase</keyname><forenames>Michael</forenames></author><author><keyname>Lange</keyname><forenames>Christoph</forenames></author></authors><title>Dimensions of Formality: A Case Study for MKM in Software Engineering</title><categories>cs.DL cs.AI cs.SE</categories><comments>To appear in The 9th International Conference on Mathematical
  Knowledge Management: MKM 2010</comments><msc-class>68T35, 68T30</msc-class><acm-class>H.5.3; H.5.4; I.7.2; F.4.m; H.3.5; D.2.7; I.2.4; K.6.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the formalization of a collection of documents created for a
Software Engineering project from an MKM perspective. We analyze how document
and collection markup formats can cope with an open-ended, multi-dimensional
space of primary and secondary classifications and relationships. We show that
RDFa-based extensions of MKM formats, employing flexible &quot;metadata&quot;
relationships referencing specific vocabularies for distinct dimensions, are
well-suited to encode this and to put it into service. This formalized
knowledge can be used for enriching interactive document browsing, for enabling
multi-dimensional metadata queries over documents and collections, and for
exporting Linked Data to the Semantic Web and thus enabling further reuse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5080</identifier>
 <datestamp>2010-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5080</id><created>2010-04-28</created><authors><author><keyname>Datta</keyname><forenames>Samir</forenames></author><author><keyname>Kulkarni</keyname><forenames>Raghav</forenames></author><author><keyname>Tewari</keyname><forenames>Raghunath</forenames></author><author><keyname>Vinodchandran</keyname><forenames>N. V.</forenames></author></authors><title>Space Complexity of Perfect Matching in Bounded Genus Bipartite Graphs</title><categories>cs.CC</categories><comments>23 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the space complexity of certain perfect matching problems over
bipartite graphs embedded on surfaces of constant genus (orientable or
non-orientable). We show that the problems of deciding whether such graphs have
(1) a perfect matching or not and (2) a unique perfect matching or not, are in
the logspace complexity class \SPL. Since \SPL\ is contained in the logspace
counting classes $\oplus\L$ (in fact in \modk\ for all $k\geq 2$), \CeqL, and
\PL, our upper bound places the above-mentioned matching problems in these
counting classes as well. We also show that the search version, computing a
perfect matching, for this class of graphs is in $\FL^{\SPL}$. Our results
extend the same upper bounds for these problems over bipartite planar graphs
known earlier. As our main technical result, we design a logspace computable
and polynomially bounded weight function which isolates a minimum weight
perfect matching in bipartite graphs embedded on surfaces of constant genus. We
use results from algebraic topology for proving the correctness of the weight
function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5094</identifier>
 <datestamp>2011-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5094</id><created>2010-04-28</created><updated>2011-06-03</updated><authors><author><keyname>Jafarizadeh</keyname><forenames>Saber</forenames></author><author><keyname>Jamalipour</keyname><forenames>Abbas</forenames></author></authors><title>Fastest Distributed Consensus Problem on Branches of an Arbitrary
  Connected Sensor Network</title><categories>cs.IT cs.DC cs.DM math.IT</categories><comments>11 pages, 13 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the fastest distributed consensus averaging problem on
branches of an arbitrary connected sensor network. In the previous works full
knowledge about the sensor network's connectivity topology was required for
determining the optimal weights and convergence rate of distributed consensus
averaging algorithm over the network. Here in this work for the first time, the
optimal weights are determined analytically for the edges of certain types of
branches, independent of the rest of network. The solution procedure consists
of stratification of associated connectivity graph of the branches and
Semidefinite Programming (SDP), particularly solving the slackness conditions,
where the optimal weights are obtained by inductive comparing of the
characteristic polynomials initiated by slackness conditions. Several examples
and numerical results are provided to confirm the optimality of the obtained
weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5108</identifier>
 <datestamp>2012-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5108</id><created>2010-04-28</created><updated>2012-01-07</updated><authors><author><keyname>Zhang</keyname><forenames>Dan</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan B.</forenames></author></authors><title>Analyzing Random Network Coding with Differential Equations and
  Differential Inclusions</title><categories>cs.IT cs.NI math.DS math.IT</categories><comments>A more up-to-date version has been published on IEEE Trans. Inform.
  Theory</comments><acm-class>H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a framework based on differential equations (DE) and differential
inclusions (DI) for analyzing Random Network Coding (RNC), as well as a
nonlinear variant referred to as Random Coupon (RC), in a wireless network. The
DEDI framework serves as a powerful numerical and analytical tool to study RNC.
We demonstrate its versatility by proving theoretical results on multicast
information flows in a wireless network using RNC or RC. We also demonstrate
the accuracy and flexibility of the performance analysis enabled by this
framework via illustrative examples of networks with multiple multicast
sessions, user cooperation and arbitrary topologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5127</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5127</id><created>2010-04-28</created><authors><author><keyname>Farhi</keyname><forenames>Edward</forenames></author><author><keyname>Gosset</keyname><forenames>David</forenames></author><author><keyname>Hassidim</keyname><forenames>Avinatan</forenames></author><author><keyname>Lutomirski</keyname><forenames>Andrew</forenames></author><author><keyname>Shor</keyname><forenames>Peter</forenames></author></authors><title>Quantum money from knots</title><categories>quant-ph cs.CR</categories><comments>22 pages, 5 figures</comments><report-no>MIT CTP-4146</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum money is a cryptographic protocol in which a mint can produce a
quantum state, no one else can copy the state, and anyone (with a quantum
computer) can verify that the state came from the mint. We present a concrete
quantum money scheme based on superpositions of diagrams that encode oriented
links with the same Alexander polynomial. We expect our scheme to be secure
against computationally bounded adversaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5128</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5128</id><created>2010-04-28</created><authors><author><keyname>Sprouse</keyname><forenames>Brian P.</forenames></author><author><keyname>MacDonald</keyname><forenames>Christopher L.</forenames></author><author><keyname>Silva</keyname><forenames>Gabriel A.</forenames></author></authors><title>Computational efficiency of fractional diffusion using adaptive time
  step memory</title><categories>math-ph cs.NA math.MP math.PR</categories><comments>6 pages and 5 figures. Submitted to 4th IFAC Workshop on Fractional
  Differentiation and Its Applications.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerical solutions to fractional differential equations can be extremely
computationally intensive due to the effect of non-local derivatives in which
all previous time points contribute to the current iteration. In finite
difference methods this has been approximated using the 'short memory effect'
where it is assumed that previous events prior to some certain time point are
insignificant and thus not calculated. Here we present an adaptive time method
for smooth functions that is computationally efficient and results in smaller
errors during numerical simulations. Sampled points along the system's history
at progressively longer intervals are assumed to reflect the values of
neighboring time points. By including progressively fewer points as a function
of time, a temporally 'weighted' history is computed that includes
contributions from the entire past of the system, resulting in increased
accuracy, but with fewer points actually calculated, which ensures
computational efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5130</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5130</id><created>2010-04-25</created><authors><author><keyname>Al-Bataineh</keyname><forenames>Omar I.</forenames></author><author><keyname>van der Meyden</keyname><forenames>Ron</forenames></author></authors><title>Epistemic Model Checking for Knowledge-Based Program Implementation: an
  Application to Anonymous Broadcast</title><categories>cs.LO cs.DC</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Knowledge-based programs provide an abstract level of description of
protocols in which agent actions are related to their states of knowledge. The
paper describes how epistemic model checking technology may be applied to
discover and verify concrete implementations based on this abstract level of
description. The details of the implementations depend on the speci?c context
of use of the protocol. The knowledge-based approach enables the
implementations to be optimized relative to these conditions of use. The
approach is illustrated using extensions of the Dining Cryptographers protocol,
a security protocol for anonymous broadcast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5132</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5132</id><created>2010-04-28</created><authors><author><keyname>Vahid</keyname><forenames>Alireza</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author></authors><title>The Two-User Deterministic Interference Channel with Rate-Limited
  Feedback</title><categories>cs.IT math.IT</categories><comments>to appear in the proceedings of 2010 IEEE International Symposium on
  Information Theory (ISIT).</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the effect of rate-limited feedback on the sum-rate
capacity of the deterministic interference channel. We characterize the
sum-rate capacity of this channel in the symmetric case and show that having
feedback links can increase the sum-rate capacity by at most the rate of the
available feedback. Our proof includes a novel upper-bound on the sum-rate
capacity and a set of new achievability strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5157</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5157</id><created>2010-04-28</created><updated>2010-11-30</updated><authors><author><keyname>Pusane</keyname><forenames>Ali E.</forenames></author><author><keyname>Smarandache</keyname><forenames>Roxana</forenames></author><author><keyname>Vontobel</keyname><forenames>Pascal O.</forenames></author><author><keyname>Costello</keyname><forenames>Daniel J.</forenames><suffix>Jr</suffix></author></authors><title>Deriving Good LDPC Convolutional Codes from LDPC Block Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, April 2010;
  revised August 2010, revised November 2010 (essentially final version).
  (Besides many small changes, the first and second revised versions contain
  corrected entries in Tables I and II.)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-density parity-check (LDPC) convolutional codes are capable of achieving
excellent performance with low encoding and decoding complexity. In this paper
we discuss several graph-cover-based methods for deriving families of
time-invariant and time-varying LDPC convolutional codes from LDPC block codes
and show how earlier proposed LDPC convolutional code constructions can be
presented within this framework. Some of the constructed convolutional codes
significantly outperform the underlying LDPC block codes. We investigate some
possible reasons for this &quot;convolutional gain,&quot; and we also discuss the ---
mostly moderate --- decoder cost increase that is incurred by going from LDPC
block to LDPC convolutional codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5165</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5165</id><created>2010-04-28</created><authors><author><keyname>Libbrecht</keyname><forenames>Paul</forenames></author></authors><title>Notations Around the World: Census and Exploitation</title><categories>cs.DL</categories><comments>14 pages, To appear in The 9th International Conference on
  Mathematical Knowledge Management: MKM 2010</comments><acm-class>H.3.5; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mathematical notations around the world are diverse. Not as much as requiring
computing machines' makers to adapt to each culture, but as much as to
disorient a person landing on a web-page with a text in mathematics. In order
to understand better this diversity, we are building a census of notations: it
should allow any content creator or mathematician to grasp which mathematical
notation is used in which language and culture. The census is built
collaboratively, collected in pages with a given semantic and presenting
observations of the widespread notations being used in existing materials by a
graphical extract. We contend that our approach should dissipate the fallacies
found here and there about the notations in &quot;other cultures&quot; so that a better
understanding of the cultures can be realized. The exploitation of the census
in the math-bridge project is also presented: this project aims at taking
learners &quot;where they are in their math-knowledge&quot; and bring them to a level
ready to start engineering studies. The census serves as definitive reference
for the transformation elements that generate the rendering of formul{\ae} in
web-browsers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5168</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5168</id><created>2010-04-28</created><authors><author><keyname>Cormack</keyname><forenames>Gordon V.</forenames></author><author><keyname>Smucker</keyname><forenames>Mark D.</forenames></author><author><keyname>Clarke</keyname><forenames>Charles L. A.</forenames></author></authors><title>Efficient and Effective Spam Filtering and Re-ranking for Large Web
  Datasets</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The TREC 2009 web ad hoc and relevance feedback tasks used a new document
collection, the ClueWeb09 dataset, which was crawled from the general Web in
early 2009. This dataset contains 1 billion web pages, a substantial fraction
of which are spam --- pages designed to deceive search engines so as to deliver
an unwanted payload. We examine the effect of spam on the results of the TREC
2009 web ad hoc and relevance feedback tasks, which used the ClueWeb09 dataset.
We show that a simple content-based classifier with minimal training is
efficient enough to rank the &quot;spamminess&quot; of every page in the dataset using a
standard personal computer in 48 hours, and effective enough to yield
significant and substantive improvements in the fixed-cutoff precision (estP10)
as well as rank measures (estR-Precision, StatMAP, MAP) of nearly all submitted
runs. Moreover, using a set of &quot;honeypot&quot; queries the labeling of training data
may be reduced to an entirely automatic process. The results of classical
information retrieval methods are particularly enhanced by filtering --- from
among the worst to among the best.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5176</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5176</id><created>2010-04-28</created><authors><author><keyname>Milojevi&#x107;</keyname><forenames>Sta&#x161;a</forenames></author></authors><title>Modes of Collaboration in Modern Science - Beyond Power Laws and
  Preferential Attachment</title><categories>physics.soc-ph cs.DL</categories><comments>Accepted for publication in JASIST</comments><doi>10.1002/asi.21331</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of the study is to determine the underlying processes leading to the
observed collaborator distribution in modern scientific fields, with special
attention to non-power law behavior. Nanoscience is used as a case study of a
modern interdisciplinary field, and its coauthorship network for 2000-04 period
is constructed from NanoBank database. We find three collaboration modes that
correspond to three distinct ranges in the distribution of collaborators: (1)
for authors with fewer than 20 collaborators (the majority) preferential
attachment does not hold and they form a log-normal &quot;hook&quot; instead of a power
law, (2) authors with more than 20 collaborators benefit from preferential
attachment and form a power law tail, and (3) authors with between 250 and 800
collaborators are more frequent than expected because of the hyperauthorship
practices in certain subfields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5179</identifier>
 <datestamp>2012-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5179</id><created>2010-04-28</created><updated>2010-11-01</updated><authors><author><keyname>Houshmand</keyname><forenames>Monireh</forenames></author><author><keyname>Hosseini-Khayat</keyname><forenames>Saied</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Minimal memory requirements for pearl-necklace encoders of quantum
  convolutional codes</title><categories>quant-ph cs.DS</categories><comments>30 pages, 9 figures, Accepted for publication in the IEEE
  Transactions on Computers</comments><journal-ref>IEEE Transactions on Computers vol. 61, no. 3, pages 299-312
  (March 2012)</journal-ref><doi>10.1109/TC.2010.226</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the major goals in quantum information processing is to reduce the
overhead associated with the practical implementation of quantum protocols, and
often, routines for quantum error correction account for most of this overhead.
A particular technique for quantum error correction that may be useful for
protecting a stream of quantum information is quantum convolutional coding. The
encoder for a quantum convolutional code has a representation as a
convolutional encoder or as a &quot;pearl-necklace&quot; encoder. In the pearl-necklace
representation, it has not been particularly clear in the research literature
how much quantum memory such an encoder would require for implementation. Here,
we offer an algorithm that answers this question. The algorithm first
constructs a weighted, directed acyclic graph where each vertex of the graph
corresponds to a gate string in the pearl-necklace encoder, and each path
through the graph represents a path through non-commuting gates in the encoder.
We show that the weight of the longest path through the graph is equal to the
minimal amount of memory needed to implement the encoder. A dynamic programming
search through this graph determines the longest path. The running time for the
construction of the graph and search through it is quadratic in the number of
gate strings in the pearl-necklace encoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5181</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5181</id><created>2010-04-29</created><authors><author><keyname>Kim</keyname><forenames>Jaewon</forenames></author><author><keyname>Park</keyname><forenames>Jonghyun</forenames></author></authors><title>Analysis of Feedback Overhead for MIMO Beamforming over Time-Varying
  Channels</title><categories>cs.IT math.IT</categories><comments>22 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the required amount of feedback overhead for multiple-input
multiple-output (MIMO) beamforming over time-varying channels is presented in
terms of the entropy of the feedback messages. In the case that each transmit
antenna has its own power amplifier which has individual power limit, it has
been known that only phase steering information is necessary to form the
optimal transmit beamforming vector. Since temporal correlation exists for
wireless fading channels, one can utilize the previous reported feedback
messages as prior information to efficiently encode the current feedback
message. Thus, phase tracking information, difference between two phase
steering information in adjacent feedback slots, is sufficient as a feedback
message. We show that while the entropy of the phase steering information is a
constant, the entropy of the phase tracking information is a function of the
temporal correlation parameter. For the phase tracking information, upperbounds
on the entropy are presented in the Gaussian entropy and the von-Mises entropy
by using the theory on the maximum entropy distributions. Derived results can
quantify the amount of reduction in feedback overhead of the phase tracking
information over the phase steering information. For application perspective,
the signal-to-noise ratio (SNR) gain of phase tracking beamforming over phase
steering beamforming is evaluated by using Monte-Carlo simulation. Also we show
that the derived entropies can determine the appropriate duration of the
feedback reports with respect to the degree of the channel variation rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5186</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5186</id><created>2010-04-29</created><authors><author><keyname>Safro</keyname><forenames>Ilya</forenames></author><author><keyname>Temkin</keyname><forenames>Boris</forenames></author></authors><title>Multiscale approach for the network compression-friendly ordering</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We present a fast multiscale approach for the network minimum logarithmic
arrangement problem. This type of arrangement plays an important role in a
network compression and fast node/link access operations. The algorithm is of
linear complexity and exhibits good scalability which makes it practical and
attractive for using on large-scale instances. Its effectiveness is
demonstrated on a large set of real-life networks. These networks with
corresponding best-known minimization results are suggested as an open
benchmark for a research community to evaluate new methods for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5189</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5189</id><created>2010-04-29</created><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Rate-distortion function via minimum mean square error estimation</title><categories>cs.IT math.IT</categories><comments>11 pages, 1 figure, submitted for publication.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a simple general parametric representation of the rate-distortion
function of a memoryless source, where both the rate and the distortion are
given by integrals whose integrands include the minimum mean square error
(MMSE) of the distortion $\Delta=d(X,Y)$ based on the source symbol $X$, with
respect to a certain joint distribution of these two random variables. At first
glance, these relations may seem somewhat similar to the I-MMSE relations due
to Guo, Shamai and Verd\'u, but they are, in fact, quite different. The new
relations among rate, distortion, and MMSE are discussed from several aspects,
and more importantly, it is demonstrated that they can sometimes be rather
useful for obtaining non-trivial upper and lower bounds on the rate-distortion
function, as well as for determining the exact asymptotic behavior for very low
and for very large distortion. Analogous MMSE relations hold for channel
capacity as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5194</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5194</id><created>2010-04-29</created><authors><author><keyname>Ryabko</keyname><forenames>Daniil</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Clustering processes</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>in proceedings of ICML 2010. arXiv-admin note: for version 2 of this
  article please see: arXiv:1005.0826v1</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of clustering is considered, for the case when each data point is
a sample generated by a stationary ergodic process. We propose a very natural
asymptotic notion of consistency, and show that simple consistent algorithms
exist, under most general non-parametric assumptions. The notion of consistency
is as follows: two samples should be put into the same cluster if and only if
they were generated by the same distribution. With this notion of consistency,
clustering generalizes such classical statistical problems as homogeneity
testing and process classification. We show that, for the case of a known
number of clusters, consistency can be achieved under the only assumption that
the joint distribution of the data is stationary ergodic (no parametric or
Markovian assumptions, no assumptions of independence, neither between nor
within the samples). If the number of clusters is unknown, consistency can be
achieved under appropriate assumptions on the mixing rates of the processes.
(again, no parametric or independence assumptions). In both cases we give
examples of simple (at most quadratic in each argument) algorithms which are
consistent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5195</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5195</id><created>2010-04-29</created><authors><author><keyname>Silberstein</keyname><forenames>Natalia</forenames></author><author><keyname>Etzion</keyname><forenames>Tuvi</forenames></author></authors><title>On Perfect Codes in the Johnson Graph</title><categories>cs.IT math.IT</categories><comments>Submitted for ACCT 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the existence of nontrivial perfect codes in the
Johnson graph J(n,w). We present combinatorial and number theory techniques to
provide necessary conditions for existence of such codes and reduce the range
of parameters in which 1-perfect and 2-perfect codes may exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5214</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5214</id><created>2010-04-29</created><authors><author><keyname>Savin</keyname><forenames>Valentin</forenames></author></authors><title>Split-Extended LDPC codes for coded cooperation</title><categories>cs.IT math.IT</categories><comments>6 pages, ISITA10</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new code design that aims to distribute an LDPC code over a
relay channel. It is based on a split-and-extend approach, which allows the
relay to split the set of bits connected to some parity-check of the LDPC code
into two or several subsets. Subsequently, the sums of bits within each subset
are used in a repeat-accumulate manner in order to generate extra bits sent
from the relay toward the destination. We show that the proposed design yields
LDPC codes with enhanced correction capacity and can be advantageously applied
to existing codes, which allows for addressing cooperation issues for evolving
standards. Finally, we derive density evolution equations for the proposed
design, and we show that Split-Extended LDPC codes can approach very closely
the capacity of the Gaussian relay channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5215</identifier>
 <datestamp>2010-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5215</id><created>2010-04-29</created><authors><author><keyname>Figueredo</keyname><forenames>Grazziela P.</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Whitbrook</keyname><forenames>Amanda</forenames></author></authors><title>System Dynamics Modelling of the Processes Involving the Maintenance of
  the Naive T Cell Repertoire</title><categories>cs.AI q-bio.CB</categories><comments>6 pages, 2 figures, 1 table, 9th Annual Workshop on Computational
  Intelligence (UKCI 2009), Nottingham, UK</comments><journal-ref>Proceedings of the 9th Annual Workshop on Computational
  Intelligence (UKCI 2009), Nottingham, UK, p13-18,</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of immune system aging, i.e. immunosenescence, is a relatively new
research topic. It deals with understanding the processes of immunodegradation
that indicate signs of functionality loss possibly leading to death. Even
though it is not possible to prevent immunosenescence, there is great benefit
in comprehending its causes, which may help to reverse some of the damage done
and thus improve life expectancy. One of the main factors influencing the
process of immunosenescence is the number and phenotypical variety of naive T
cells in an individual. This work presents a review of immunosenescence,
proposes system dynamics modelling of the processes involving the maintenance
of the naive T cell repertoire and presents some preliminary results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5216</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5216</id><created>2010-04-29</created><updated>2010-04-30</updated><authors><author><keyname>Gorgoglione</keyname><forenames>Matteo</forenames></author><author><keyname>Savin</keyname><forenames>Valentin</forenames></author><author><keyname>Declercq</keyname><forenames>David</forenames></author></authors><title>Optimized puncturing distributions for irregular non-binary LDPC codes</title><categories>cs.IT math.IT</categories><comments>6 pages, ISITA10</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we design non-uniform bit-wise puncturing distributions for
irregular non-binary LDPC (NB-LDPC) codes. The puncturing distributions are
optimized by minimizing the decoding threshold of the punctured LDPC code, the
threshold being computed with a Monte-Carlo implementation of Density
Evolution. First, we show that Density Evolution computed with Monte-Carlo
simulations provides accurate (very close) and precise (small variance)
estimates of NB-LDPC code ensemble thresholds. Based on the proposed method, we
analyze several puncturing distributions for regular and semi-regular codes,
obtained either by clustering punctured bits, or spreading them over the
symbol-nodes of the Tanner graph. Finally, optimized puncturing distributions
for non-binary LDPC codes with small maximum degree are presented, which
exhibit a gap between 0.2 and 0.5 dB to the channel capacity, for punctured
rates varying from 0.5 to 0.9.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5217</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5217</id><created>2010-04-29</created><authors><author><keyname>Cunche</keyname><forenames>Mathieu</forenames></author><author><keyname>Savin</keyname><forenames>Valentin</forenames></author><author><keyname>Roca</keyname><forenames>Vincent</forenames></author></authors><title>Analysis of Quasi-Cyclic LDPC codes under ML decoding over the erasure
  channel</title><categories>cs.IT math.IT</categories><comments>6 pages, ISITA10</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that Quasi-Cyclic LDPC codes can efficiently
accommodate the hybrid iterative/ML decoding over the binary erasure channel.
We demonstrate that the quasi-cyclic structure of the parity-check matrix can
be advantageously used in order to significantly reduce the complexity of the
ML decoding. This is achieved by a simple row/column permutation that
transforms a QC matrix into a pseudo-band form. Based on this approach, we
propose a class of QC-LDPC codes with almost ideal error correction performance
under the ML decoding, while the required number of row/symbol operations
scales as $k\sqrt{k}$, where $k$ is the number of source symbols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5222</identifier>
 <datestamp>2010-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5222</id><created>2010-04-29</created><authors><author><keyname>Oates</keyname><forenames>Robert</forenames></author><author><keyname>Greensmith</keyname><forenames>Julie</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Garibaldi</keyname><forenames>Jonathan M.</forenames></author><author><keyname>Kendall</keyname><forenames>Graham</forenames></author></authors><title>The Application of a Dendritic Cell Algorithm to a Robotic Classifier</title><categories>cs.AI cs.NE cs.RO</categories><comments>12 pages, 4 figures, Proceedings of the 6th International Conference
  on Artificial Immune Systems (ICARIS2007)</comments><journal-ref>Proceedings of the 6th International Conference on Artificial
  Immune Systems (ICARIS2007), Lecture Notes in Computer Science 4628, Santos,
  Brazil, p204-215, 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dendritic cell algorithm is an immune-inspired technique for processing
time-dependant data. Here we propose it as a possible solution for a robotic
classification problem. The dendritic cell algorithm is implemented on a real
robot and an investigation is performed into the effects of varying the
migration threshold median for the cell population. The algorithm performs well
on a classification task with very little tuning. Ways of extending the
implementation to allow it to be used as a classifier within the field of
robotic security are suggested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5229</identifier>
 <datestamp>2011-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5229</id><created>2010-04-29</created><updated>2010-10-13</updated><authors><author><keyname>Filippi</keyname><forenames>Sarah</forenames><affiliation>LTCI</affiliation></author><author><keyname>Capp&#xe9;</keyname><forenames>Olivier</forenames><affiliation>LTCI</affiliation></author><author><keyname>Garivier</keyname><forenames>Aur&#xe9;lien</forenames><affiliation>LTCI</affiliation></author></authors><title>Optimism in Reinforcement Learning and Kullback-Leibler Divergence</title><categories>cs.LG math.ST stat.ML stat.TH</categories><comments>This work has been accepted and presented at ALLERTON 2010;
  Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton
  Conference on, Monticello (Illinois) : \'Etats-Unis (2010)</comments><proxy>ccsd</proxy><doi>10.1109/ALLERTON.2010.5706896</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider model-based reinforcement learning in finite Markov De- cision
Processes (MDPs), focussing on so-called optimistic strategies. In MDPs,
optimism can be implemented by carrying out extended value it- erations under a
constraint of consistency with the estimated model tran- sition probabilities.
The UCRL2 algorithm by Auer, Jaksch and Ortner (2009), which follows this
strategy, has recently been shown to guarantee near-optimal regret bounds. In
this paper, we strongly argue in favor of using the Kullback-Leibler (KL)
divergence for this purpose. By studying the linear maximization problem under
KL constraints, we provide an ef- ficient algorithm, termed KL-UCRL, for
solving KL-optimistic extended value iteration. Using recent deviation bounds
on the KL divergence, we prove that KL-UCRL provides the same guarantees as
UCRL2 in terms of regret. However, numerical experiments on classical
benchmarks show a significantly improved behavior, particularly when the MDP
has reduced connectivity. To support this observation, we provide elements of
com- parison between the two algorithms based on geometric considerations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5230</identifier>
 <datestamp>2011-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5230</id><created>2010-04-29</created><updated>2011-02-24</updated><authors><author><keyname>Foucaud</keyname><forenames>Florent</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Guerrini</keyname><forenames>Eleonora</forenames><affiliation>IF</affiliation></author><author><keyname>Kovse</keyname><forenames>Matjaz</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Naserasr</keyname><forenames>Reza</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Parreau</keyname><forenames>Aline</forenames><affiliation>IF</affiliation></author><author><keyname>Valicov</keyname><forenames>Petru</forenames><affiliation>LaBRI</affiliation></author></authors><title>Extremal graphs for the identifying code problem</title><categories>cs.DM math.CO</categories><proxy>ccsd</proxy><journal-ref>European Journal of Combinatorics 32, 4 (2011) 628-638</journal-ref><doi>10.1016/j.ejc.2011.01.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An identifying code of a graph G is a dominating set C such that every vertex
x of G is distinguished from all other vertices by the set of vertices in C
that are at distance at most 1 from x. The problem of finding an identifying
code of minimum possible size turned out to be a challenging problem. It was
proved by N. Bertrand that if a graph on n vertices with at least one edge
admits an identifying code, then a minimum identifying code has size at most
n-1. Some classes of graphs whose smallest identifying code is of size n-1 were
already known, and few conjectures were formulated to classify all these
graphs. In this paper, disproving these conjectures, we classify all finite
graphs for which all but one of the vertices are needed to form an identifying
code. We also classify all infinite graphs needing the whole set of vertices in
any identifying code. New upper bounds in terms of the number of vertices and
the maximum degree of a graph are also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5236</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5236</id><created>2010-04-29</created><authors><author><keyname>Jukna</keyname><forenames>S.</forenames></author><author><keyname>Schnitger</keyname><forenames>G.</forenames></author></authors><title>Circuits with arbitrary gates for random operators</title><categories>cs.CC</categories><comments>7 pages</comments><msc-class>68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider boolean circuits computing n-operators f:{0,1}^n --&gt; {0,1}^n. As
gates we allow arbitrary boolean functions; neither fanin nor fanout of gates
is restricted. An operator is linear if it computes n linear forms, that is,
computes a matrix-vector product y=Ax over GF(2). We prove the existence of
n-operators requiring about n^2 wires in any circuit, and linear n-operators
requiring about n^2/\log n wires in depth-2 circuits, if either all output
gates or all gates on the middle layer are linear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5256</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5256</id><created>2010-04-29</created><authors><author><keyname>Dubois</keyname><forenames>Swan</forenames><affiliation>LIP6, INRIA Rocquencourt</affiliation></author><author><keyname>Masuzawa</keyname><forenames>Toshimitsu</forenames><affiliation>LIP6</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LIP6</affiliation></author></authors><title>Construction auto-stabilisante d'arbre couvrant en d\'epit d'actions
  malicieuses</title><categories>cs.DC</categories><proxy>ccsd</proxy><journal-ref>12\`emes Rencontres Francophones sur les Aspects Algorithmiques de
  T\'el\'ecommunications (AlgoTel), Belle Dune : France (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A self-stabilizing protocol provides by definition a tolerance to transient
failures. Recently, a new class of self-stabilizing protocols appears. These
protocols provides also a tolerance to a given number of permanent failures. In
this article, we are interested in self-stabilizing protocols that deal with
Byzantines failures. We prove that, for some problems which not allow strict
stabilization (see [Nesterenko,Arora,2002]), there exist solutions that
tolerates Byzantine faults if we define a new criteria of tolerance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5257</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5257</id><created>2010-04-29</created><authors><author><keyname>Lescanne</keyname><forenames>Pierre</forenames><affiliation>LIP</affiliation></author><author><keyname>Matthieu</keyname><forenames>Perrinel</forenames><affiliation>LIP</affiliation></author></authors><title>On the Rationality of Escalation</title><categories>cs.GT cs.LO</categories><comments>23 p.</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Escalation is a typical feature of infinite games. Therefore tools conceived
for studying infinite mathematical structures, namely those deriving from
coinduction are essential. Here we use coinduction, or backward coinduction (to
show its connection with the same concept for finite games) to study carefully
and formally the infinite games especially those called dollar auctions, which
are considered as the paradigm of escalation. Unlike what is commonly admitted,
we show that, provided one assumes that the other agent will always stop,
bidding is rational, because it results in a subgame perfect equilibrium. We
show that this is not the only rational strategy profile (the only subgame
perfect equilibrium). Indeed if an agent stops and will stop at every step, we
claim that he is rational as well, if one admits that his opponent will never
stop, because this corresponds to a subgame perfect equilibrium. Amazingly, in
the infinite dollar auction game, the behavior in which both agents stop at
each step is not a Nash equilibrium, hence is not a subgame perfect
equilibrium, hence is not rational.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5262</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5262</id><created>2010-04-29</created><authors><author><keyname>Bondarenko</keyname><forenames>Anton</forenames></author></authors><title>On Application of the Local Search and the Genetic Algorithms Techniques
  to Some Combinatorial Optimization Problems</title><categories>cs.NE math.OC</categories><msc-class>90C27, 68P10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the approach to solving several combinatorial optimization
problems using the local search and the genetic algorithm techniques is
proposed. Initially this approach was developed in purpose to overcome some
difficulties inhibiting the application of above mentioned techniques to the
problems of the Questionnaire Theory. But when the algorithms were developed it
became clear that them could be successfully applied also to the Minimum Set
Cover, the 0-1-Knapsack and probably to other combinatorial optimization
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5274</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5274</id><created>2010-04-29</created><authors><author><keyname>Baudais</keyname><forenames>Jean-Yves</forenames></author><author><keyname>Muhammad</keyname><forenames>Fahad Syed</forenames></author><author><keyname>H&#xe9;lard</keyname><forenames>Jean-Fran\ccois</forenames></author></authors><title>Robustness maximization of parallel multichannel systems</title><categories>cs.IT math.IT</categories><comments>27 pages, 8 figures, submitted to IEEE Trans. Inform. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bit error rate (BER) minimization and SNR-gap maximization, two robustness
optimization problems, are solved, under average power and bit-rate
constraints, according to the waterfilling policy. Under peak-power constraint
the solutions differ and this paper gives bit-loading solutions of both
robustness optimization problems over independent parallel channels. The study
is based on analytical approach with generalized Lagrangian relaxation tool and
on greedy-type algorithm approach. Tight BER expressions are used for square
and rectangular quadrature amplitude modulations. Integer bit solution of
analytical continuous bit-rates is performed with a new generalized secant
method. The asymptotic convergence of both robustness optimizations is proved
for both analytical and algorithmic approaches. We also prove that, in
conventional margin maximization problem, the equivalence between SNR-gap
maximization and power minimization does not hold with peak-power limitation.
Based on a defined dissimilarity measure, bit-loading solutions are compared
over power line communication channel for multicarrier systems. Simulation
results confirm the asymptotic convergence of both allocation policies. In non
asymptotic regime the allocation policies can be interchanged depending on the
robustness measure and the operating point of the communication system. The low
computational effort of the suboptimal solution based on analytical approach
leads to a good trade-off between performance and complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5285</identifier>
 <datestamp>2011-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5285</id><created>2010-04-29</created><authors><author><keyname>Ch&#xe8;ze</keyname><forenames>Guillaume</forenames><affiliation>IMT</affiliation></author></authors><title>Nearly Optimal Algorithms for the Decomposition of Multivariate Rational
  Functions and the Extended L\&quot;uroth's Theorem</title><categories>cs.SC cs.DS</categories><proxy>ccsd</proxy><journal-ref>Journal of Complexity 26, 4 (2010) 344-363</journal-ref><doi>10.1016/j.jco.2010.05.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extended L\&quot;uroth's Theorem says that if the transcendence degree of
$\KK(\mathsf{f}_1,\dots,\mathsf{f}_m)/\KK$ is 1 then there exists $f \in
\KK(\underline{X})$ such that $\KK(\mathsf{f}_1,\dots,\mathsf{f}_m)$ is equal
to $\KK(f)$. In this paper we show how to compute $f$ with a probabilistic
algorithm. We also describe a probabilistic and a deterministic algorithm for
the decomposition of multivariate rational functions. The probabilistic
algorithms proposed in this paper are softly optimal when $n$ is fixed and $d$
tends to infinity. We also give an indecomposability test based on gcd
computations and Newton's polytope. In the last section, we show that we get a
polynomial time algorithm, with a minor modification in the exponential time
decomposition algorithm proposed by Gutierez-Rubio-Sevilla in 2001.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5305</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5305</id><created>2010-04-29</created><authors><author><keyname>Marim</keyname><forenames>Marcio</forenames><affiliation>TSI, AIQ</affiliation></author><author><keyname>Atlan</keyname><forenames>Michael</forenames><affiliation>TSI</affiliation></author><author><keyname>Angelini</keyname><forenames>Elsa</forenames><affiliation>TSI</affiliation></author><author><keyname>Olivo-Marin</keyname><forenames>Jean-Christophe</forenames><affiliation>AIQ</affiliation></author></authors><title>Compressed Sensing with off-axis frequency-shifting holography</title><categories>physics.optics cs.CV physics.med-ph</categories><comments>vol 35, pp 871-873</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work reveals an experimental microscopy acquisition scheme successfully
combining Compressed Sensing (CS) and digital holography in off-axis and
frequency-shifting conditions. CS is a recent data acquisition theory involving
signal reconstruction from randomly undersampled measurements, exploiting the
fact that most images present some compact structure and redundancy. We propose
a genuine CS-based imaging scheme for sparse gradient images, acquiring a
diffraction map of the optical field with holographic microscopy and recovering
the signal from as little as 7% of random measurements. We report experimental
results demonstrating how CS can lead to an elegant and effective way to
reconstruct images, opening the door for new microscopy applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5306</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5306</id><created>2010-04-29</created><authors><author><keyname>Ho&#xe0;ng</keyname><forenames>Chinh T.</forenames><affiliation>LGS</affiliation></author><author><keyname>Maffray</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>LGS</affiliation></author><author><keyname>Mechebbek</keyname><forenames>Meriem</forenames></author></authors><title>A characterization of b-perfect graphs</title><categories>cs.DM</categories><proxy>ccsd</proxy><journal-ref>Journal of Graph Theory 71 (2012) 95-122</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A b-coloring is a coloring of the vertices of a graph such that each color
class contains a vertex that has a neighbor in all other color classes, and the
b-chromatic number of a graph $G$ is the largest integer $k$ such that $G$
admits a b-coloring with $k$ colors. A graph is b-perfect if the b-chromatic
number is equal to the chromatic number for every induced subgraph of $G$. We
prove that a graph is b-perfect if and only if it does not contain as an
induced subgraph a member of a certain list of twenty-two graphs. This entails
the existence of a polynomial-time recognition algorithm and of a
polynomial-time algorithm for coloring exactly the vertices of every b-perfect
graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5326</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5326</id><created>2010-04-29</created><authors><author><keyname>Barber</keyname><forenames>Michael J.</forenames></author><author><keyname>Clark</keyname><forenames>John W.</forenames></author></authors><title>Designing neural networks that process mean values of random variables</title><categories>cond-mat.dis-nn cs.AI cs.LG</categories><comments>13 pages, elsarticle</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a class of neural networks derived from probabilistic models in
the form of Bayesian networks. By imposing additional assumptions about the
nature of the probabilistic models represented in the networks, we derive
neural networks with standard dynamics that require no training to determine
the synaptic weights, that perform accurate calculation of the mean values of
the random variables, that can pool multiple sources of evidence, and that deal
cleanly and consistently with inconsistent or contradictory evidence. The
presented neural networks capture many properties of Bayesian networks,
providing distributed versions of probabilistic models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5329</identifier>
 <datestamp>2011-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5329</id><created>2010-04-29</created><updated>2011-06-24</updated><authors><author><keyname>Elsaesser</keyname><forenames>Robert</forenames></author><author><keyname>Tscheuschner</keyname><forenames>Tobias</forenames></author></authors><title>Settling the complexity of local max-cut (almost) completely</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding a local optimum for Max-Cut with
FLIP-neighborhood, in which exactly one node changes the partition. Schaeffer
and Yannakakis (SICOMP, 1991) showed PLS-completeness of this problem on graphs
with unbounded degree. On the other side, Poljak (SICOMP, 1995) showed that in
cubic graphs every FLIP local search takes O(n^2) steps, where n is the number
of nodes. Due to the huge gap between degree three and unbounded degree,
Ackermann, Roeglin, and Voecking (JACM, 2008) asked for the smallest d for
which the local Max-Cut problem with FLIP-neighborhood on graphs with maximum
degree d is PLS-complete. In this paper, we prove that the computation of a
local optimum on graphs with maximum degree five is PLS-complete. Thus, we
solve the problem posed by Ackermann et al. almost completely by showing that d
is either four or five (unless PLS is in P). On the other side, we also prove
that on graphs with degree O(log n) every FLIP local search has probably
polynomial smoothed complexity. Roughly speaking, for any instance, in which
the edge weights are perturbated by a (Gaussian) random noise with variance
\sigma^2, every FLIP local search terminates in time polynomial in n and
\sigma^{-1}, with probability 1-n^{-\Omega(1)}. Putting both results together,
we may conclude that although local Max-Cut is likely to be hard on graphs with
bounded degree, it can be solved in polynomial time for slightly perturbated
instances with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5339</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5339</id><created>2010-04-29</created><updated>2011-07-21</updated><authors><author><keyname>Shchekotykhin</keyname><forenames>Kostyantyn</forenames></author><author><keyname>Friedrich</keyname><forenames>Gerhard</forenames></author><author><keyname>Fleiss</keyname><forenames>Philipp</forenames></author><author><keyname>Rodler</keyname><forenames>Patrick</forenames></author></authors><title>Query strategy for sequential ontology debugging</title><categories>cs.LO cs.AI</categories><comments>Preprint submitted to Web Semantics: Science, Services and Agents on
  the World Wide Web</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Debugging of ontologies is an important prerequisite for their wide-spread
application, especially in areas that rely upon everyday users to create and
maintain knowledge bases, as in the case of the Semantic Web. Recent approaches
use diagnosis methods to identify causes of inconsistent or incoherent
ontologies. However, in most debugging scenarios these methods return many
alternative diagnoses, thus placing the burden of fault localization on the
user. This paper demonstrates how the target diagnosis can be identified by
performing a sequence of observations, that is, by querying an oracle about
entailments of the target ontology. We exploit a-priori probabilities of
typical user errors to formulate information-theoretic concepts for query
selection. Our evaluation showed that the proposed method significantly reduces
the number of required queries compared to myopic strategies. We experimented
with different probability distributions of user errors and different qualities
of the a-priori probabilities. Our measurements showed the advantageousness of
information-theoretic approach to query selection even in cases where only a
rough estimate of the priors is available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5351</identifier>
 <datestamp>2010-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5351</id><created>2010-04-29</created><updated>2010-05-11</updated><authors><author><keyname>Saucan</keyname><forenames>Emil</forenames></author></authors><title>Isometric Embeddings in Imaging and Vision: Facts and Fiction</title><categories>cs.CV math.CV math.DG</categories><comments>23 pages, 1 figure Second version: Corrections made, subsection added</comments><msc-class>52B70, 57R40, 53C42, 30C65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the practicability of Nash's Embedding Theorem in vision and
imaging sciences. In particular, we investigate the relevance of a result of
Burago and Zalgaller regarding the existence of isometric embeddings of
polyhedral surfaces in $\mathbb{R}^3$ and we show that their proof does not
extended directly to higher dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5354</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5354</id><created>2010-04-29</created><authors><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author><author><keyname>Hogg</keyname><forenames>Tad</forenames></author></authors><title>Using a Model of Social Dynamics to Predict Popularity of News</title><categories>cs.CY</categories><journal-ref>In Proceedings of 19th International World Wide Web Conference
  (WWW10), 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Popularity of content in social media is unequally distributed, with some
items receiving a disproportionate share of attention from users. Predicting
which newly-submitted items will become popular is critically important for
both companies that host social media sites and their users. Accurate and
timely prediction would enable the companies to maximize revenue through
differential pricing for access to content or ad placement. Prediction would
also give consumers an important tool for filtering the ever-growing amount of
content. Predicting popularity of content in social media, however, is
challenging due to the complex interactions among content quality, how the
social media site chooses to highlight content, and influence among users.
While these factors make it difficult to predict popularity \emph{a priori}, we
show that stochastic models of user behavior on these sites allows predicting
popularity based on early user reactions to new content. By incorporating
aspects of the web site design, such models improve on predictions based on
simply extrapolating from the early votes. We validate this claim on the social
news portal Digg using a previously-developed model of social voting based on
the Digg user interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5367</identifier>
 <datestamp>2011-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5367</id><created>2010-04-29</created><updated>2011-07-13</updated><authors><author><keyname>Kasai</keyname><forenames>Kenta</forenames></author><author><keyname>Declercq</keyname><forenames>David</forenames></author><author><keyname>Poulliat</keyname><forenames>Charly</forenames></author><author><keyname>Sakaniwa</keyname><forenames>Kohichi</forenames></author></authors><title>Multiplicatively Repeated Non-Binary LDPC Codes</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Information Theory</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We propose non-binary LDPC codes concatenated with multiplicative repetition
codes. By multiplicatively repeating the (2,3)-regular non-binary LDPC mother
code of rate 1/3, we construct rate-compatible codes of lower rates 1/6, 1/9,
1/12,... Surprisingly, such simple low-rate non-binary LDPC codes outperform
the best low-rate binary LDPC codes so far. Moreover, we propose the decoding
algorithm for the proposed codes, which can be decoded with almost the same
computational complexity as that of the mother code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5370</identifier>
 <datestamp>2010-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5370</id><created>2010-04-29</created><authors><author><keyname>Zhang</keyname><forenames>Dell</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Cai</keyname><forenames>Deng</forenames></author><author><keyname>Lu</keyname><forenames>Jinsong</forenames></author></authors><title>Self-Taught Hashing for Fast Similarity Search</title><categories>cs.IR</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The ability of fast similarity search at large scale is of great importance
to many Information Retrieval (IR) applications. A promising way to accelerate
similarity search is semantic hashing which designs compact binary codes for a
large number of documents so that semantically similar documents are mapped to
similar codes (within a short Hamming distance). Although some recently
proposed techniques are able to generate high-quality codes for documents known
in advance, obtaining the codes for previously unseen documents remains to be a
very challenging problem. In this paper, we emphasise this issue and propose a
novel Self-Taught Hashing (STH) approach to semantic hashing: we first find the
optimal $l$-bit binary codes for all documents in the given corpus via
unsupervised learning, and then train $l$ classifiers via supervised learning
to predict the $l$-bit code for any query document unseen before. Our
experiments on three real-world text datasets show that the proposed approach
using binarised Laplacian Eigenmap (LapEig) and linear Support Vector Machine
(SVM) outperforms state-of-the-art techniques significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5382</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5382</id><created>2010-04-29</created><authors><author><keyname>Roy</keyname><forenames>Pritam</forenames></author></authors><title>Interface Building for Software by Modular Three-Valued Abstraction
  Refinement</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Verification of software systems is a very hard problem due to the large size
of program state-space. The traditional techniques (like model checking) do not
scale; since they include the whole state-space by inlining the library
function codes. Current research avoids these problem by creating a lightweight
representation of the library in form of an &quot;interface graph&quot; (call sequence
graph). In this paper we introduce a new algorithm to compute a safe,
permissive interface graph for C-type functions. In this modular analysis, each
function transition is summarized following three-valued abstraction semantics.
There are two kinds of abstraction used here. The global abstraction contains
predicates over global variables only; however the local abstraction inside
each function may also contain the local variables. The abstract summary needs
refinement to guarantee safety and permissiveness. We have implemented the
algorithms in TICC tool and compared this algorithm with some related interface
generation algorithms. We also discuss the application of interface as an
offline test-suite. We create an interface from the model program
(specification) and the interface will act as a test-suite for the new
implementation-under-test (IUT).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5409</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5409</id><created>2010-04-29</created><authors><author><keyname>Cao</keyname><forenames>Zhenwei</forenames></author><author><keyname>Elgart</keyname><forenames>Alexander</forenames></author></authors><title>Adiabatic quantum computation: Enthusiast and Sceptic's perspectives</title><categories>quant-ph cs.CC</categories><comments>4 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enthusiast's perspective: We analyze the effectiveness of AQC for a small
rank problem Hamiltonian $H_F$ with the arbitrary initial Hamiltonian $H_I$. We
prove that for the generic $H_I$ the running time cannot be smaller than
$O(\sqrt N)$, where $N$ is a dimension of the Hilbert space. We also construct
an explicit $H_I$ for which the running time is indeed $O(\sqrt N)$. Our
algorithm can be used to solve the unstructured search problem with the unknown
number of marked items. Sceptic's perspective: We show that for a robust
device, the running time for such $H_F$ cannot be much smaller than $O(N/\ln
N)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5421</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5421</id><created>2010-04-29</created><authors><author><keyname>Wang</keyname><forenames>I-Hsiang</forenames></author><author><keyname>Tse</keyname><forenames>David N. C.</forenames></author></authors><title>Interference Mitigation through Limited Transmitter Cooperation</title><categories>cs.IT math.IT</categories><comments>Submitted to Special Issue of the IEEE Transactions on Information
  Theory on Interference Networks.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference limits performance in wireless networks, and cooperation among
receivers or transmitters can help mitigate interference by forming distributed
MIMO systems. Earlier work shows how limited receiver cooperation helps
mitigate interference. The scenario with transmitter cooperation, however, is
more difficult to tackle. In this paper we study the two-user Gaussian
interference channel with conferencing transmitters to make progress towards
this direction. We characterize the capacity region to within 6.5 bits/s/Hz,
regardless of channel parameters. Based on the constant-to-optimality result,
we show that there is an interesting reciprocity between the scenario with
conferencing transmitters and the scenario with conferencing receivers, and
their capacity regions are within a constant gap to each other. Hence in the
interference-limited regime, the behavior of the benefit brought by transmitter
cooperation is the same as that by receiver cooperation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5424</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5424</id><created>2010-04-29</created><authors><author><keyname>Luqman</keyname><forenames>Muhammad Muzzamil</forenames></author><author><keyname>Brouard</keyname><forenames>Thierry</forenames></author><author><keyname>Ramel</keyname><forenames>Jean-Yves</forenames></author></authors><title>Graphic Symbol Recognition using Graph Based Signature and Bayesian
  Network Classifier</title><categories>cs.CV cs.GR</categories><comments>5 pages, 8 figures, Tenth International Conference on Document
  Analysis and Recognition (ICDAR), IEEE Computer Society, 2009, volume 10,
  1325-1329</comments><acm-class>I.4.0; I.5.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach for recognition of complex graphic symbols in
technical documents. Graphic symbol recognition is a well known challenge in
the field of document image analysis and is at heart of most graphic
recognition systems. Our method uses structural approach for symbol
representation and statistical classifier for symbol recognition. In our system
we represent symbols by their graph based signatures: a graphic symbol is
vectorized and is converted to an attributed relational graph, which is used
for computing a feature vector for the symbol. This signature corresponds to
geometry and topology of the symbol. We learn a Bayesian network to encode
joint probability distribution of symbol signatures and use it in a supervised
learning scenario for graphic symbol recognition. We have evaluated our method
on synthetically deformed and degraded images of pre-segmented 2D architectural
and electronic symbols from GREC databases and have obtained encouraging
recognition rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5427</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5427</id><created>2010-04-29</created><authors><author><keyname>Luqman</keyname><forenames>Muhammad Muzzamil</forenames></author><author><keyname>Delalandre</keyname><forenames>Mathieu</forenames></author><author><keyname>Brouard</keyname><forenames>Thierry</forenames></author><author><keyname>Ramel</keyname><forenames>Jean-Yves</forenames></author><author><keyname>Llad&#xf3;s</keyname><forenames>Josep</forenames></author></authors><title>Employing fuzzy intervals and loop-based methodology for designing
  structural signature: an application to symbol recognition</title><categories>cs.CV cs.GR</categories><comments>10 pages, Eighth IAPR International Workshop on Graphics RECognition
  (GREC), 2009, volume 8, 22-31</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation of our work is to present a new methodology for symbol
recognition. We support structural methods for representing visual associations
in graphic documents. The proposed method employs a structural approach for
symbol representation and a statistical classifier for recognition. We
vectorize a graphic symbol, encode its topological and geometrical information
by an ARG and compute a signature from this structural graph. To address the
sensitivity of structural representations to deformations and degradations, we
use data adapted fuzzy intervals while computing structural signature. The
joint probability distribution of signatures is encoded by a Bayesian network.
This network in fact serves as a mechanism for pruning irrelevant features and
choosing a subset of interesting features from structural signatures, for
underlying symbol set. Finally we deploy the Bayesian network in supervised
learning scenario for recognizing query symbols. We have evaluated the
robustness of our method against noise, on synthetically deformed and degraded
images of pre-segmented 2D architectural and electronic symbols from GREC
databases and have obtained encouraging recognition rates. A second set of
experimentation was carried out for evaluating the performance of our method
against context noise i.e. symbols cropped from complete documents. The results
support the use of our signature by a symbol spotting system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5429</identifier>
 <datestamp>2012-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5429</id><created>2010-04-29</created><authors><author><keyname>Butler</keyname><forenames>Brian K.</forenames></author><author><keyname>Siegel</keyname><forenames>Paul H.</forenames></author></authors><title>On Distance Properties of Quasi-Cyclic Protograph-Based LDPC Codes</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE International Symposium on Information Theory, to
  be held June 2010</comments><doi>10.1109/ISIT.2010.5513638</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has shown that properly designed protograph-based LDPC codes may
have minimum distance linearly increasing with block length. This notion rests
on ensemble arguments over all possible expansions of the base protograph. When
implementation complexity is considered, the expansion is typically chosen to
be quite orderly. For example, protograph expansion by cyclically shifting
connections creates a quasi-cyclic (QC) code. Other recent work has provided
upper bounds on the minimum distance of QC codes. In this paper, these bounds
are expanded upon to cover puncturing and tightened in several specific cases.
We then evaluate our upper bounds for the most prominent protograph code thus
far, one proposed for deep-space usage in the CCSDS experimental standard, the
code known as AR4JA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5433</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5433</id><created>2010-04-29</created><authors><author><keyname>Giesbrecht</keyname><forenames>Mark</forenames></author></authors><title>Some Results on the Functional Decomposition of Polynomials</title><categories>cs.SC</categories><comments>Masters Thesis, University of Toronto, 1988</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  If g and h are functions over some field, we can consider their composition f
= g(h). The inverse problem is decomposition: given f, determine the ex-
istence of such functions g and h. In this thesis we consider functional decom-
positions of univariate and multivariate polynomials, and rational functions
over a field F of characteristic p. In the polynomial case, &quot;wild&quot; behaviour
occurs in both the mathematical and computational theory of the problem if p
divides the degree of g. We consider the wild case in some depth, and deal with
those polynomials whose decompositions are in some sense the &quot;wildest&quot;: the
additive polynomials. We determine the maximum number of decompositions and
show some polynomial time algorithms for certain classes of polynomials with
wild decompositions. For the rational function case we present a definition of
the problem, a normalised version of the problem to which the general problem
reduces, and an exponential time solution to the normal problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5436</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5436</id><created>2010-04-29</created><authors><author><keyname>Ning</keyname><forenames>Kang</forenames></author></authors><title>Multiple oligo nucleotide arrays: Methods to reduce manufacture time and
  cost</title><categories>cs.DM q-bio.QM</categories><comments>11 pages, 7 figures. A simple method targets some researchers in the
  field.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The customized multiple arrays are becoming vastly used in microarray
experiments for varies purposes, mainly for its ability to handle a large
quantity of data and output high quality results. However, experimenters who
use customized multiple arrays still face many problems, such as the cost and
time to manufacture the masks, and the cost for production of the multiple
arrays by costly machines. Although there is some research on the multiple
arrays, there is little concern on the manufacture time and cost, which is
actually important to experimenters. In this paper, we have proposed methods to
reduce the time and cost for the manufacture of the customized multiple arrays.
We have first introduced a heuristic algorithm for the mask decomposition
problem for multiple arrays. Then a streamline method is proposed for the
integration of different steps of manufacture on a higher level. Experiments
show that our methods are very effective in reduction of the time and cost of
manufacture of multiple arrays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5437</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5437</id><created>2010-04-29</created><authors><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author></authors><title>Parallel algorithms in linear algebra</title><categories>cs.DS cs.NA math.NA</categories><comments>17 pages. An old Technical Report, submitted for archival purposes.
  For further details see http://wwwmaths.anu.edu.au/~brent/pub/pub128.html</comments><report-no>Technical Report TR-CS-91-06, Computer Sciences Laboratory,
  Australian National University, Canberra, August 1991, 17 pages</report-no><msc-class>65-01 (Primary) 65-02, 65F05, 65F15, 68-01, 68-02 (Secondary)</msc-class><acm-class>C.1.2; C.1.4; D.1.3; G.1.0; G.4</acm-class><journal-ref>Algorithms and Architectures: Proceedings of the Second NEC
  Research Symposium (edited by T. Ishiguro), SIAM, Philadelphia, 1993, 54-72</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report provides an introduction to algorithms for fundamental linear
algebra problems on various parallel computer architectures, with the emphasis
on distributed-memory MIMD machines. To illustrate the basic concepts and key
issues, we consider the problem of parallel solution of a nonsingular linear
system by Gaussian elimination with partial pivoting. This problem has come to
be regarded as a benchmark for the performance of parallel machines. We
consider its appropriateness as a benchmark, its communication requirements,
and schemes for data distribution to facilitate communication and load
balancing. In addition, we describe some parallel algorithms for orthogonal
(QR) factorization and the singular value decomposition (SVD).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5439</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5439</id><created>2010-04-29</created><authors><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author></authors><title>On the periods of generalized Fibonacci recurrences</title><categories>math.NT cs.DM</categories><comments>13 pages. An old Technical Report, submitted for archival purposes.
  For further details, see http://wwwmaths.anu.edu.au/~brent/pub/pub133.html</comments><report-no>Technical Report TR-CS-92-03, Computer Science Department,
  Australian National University, March 1992 (revised March 1993), 13 pages.</report-no><msc-class>11Y55 (Primary) 05A15, 11-04, 12-04, 12E05, 12E10, 65C10, 68R05
  (Secondary)</msc-class><acm-class>F.2.1; G.3</acm-class><journal-ref>Mathematics of Computation 63 (1994), 389-401.</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a simple condition for a linear recurrence (mod 2^w) of degree r to
have the maximal possible period 2^(w-1).(2^r-1). It follows that the period is
maximal in the cases of interest for pseudo-random number generation, i.e. for
3-term linear recurrences defined by trinomials which are primitive (mod 2) and
of degree r &gt; 2. We consider the enumeration of certain exceptional polynomials
which do not give maximal period, and list all such polynomials of degree less
than 15.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5442</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5442</id><created>2010-04-29</created><updated>2010-05-31</updated><authors><author><keyname>Chen</keyname><forenames>Feng</forenames></author><author><keyname>Xu</keyname><forenames>Aiguo</forenames></author><author><keyname>Zhang</keyname><forenames>Guangcai</forenames></author><author><keyname>Li</keyname><forenames>Yingjun</forenames></author><author><keyname>Succi</keyname><forenames>Sauro</forenames></author></authors><title>Multiple-Relaxation-Time Lattice Boltzmann Approach to Compressible
  Flows with Flexible Specific-Heat Ratio and Prandtl Number</title><categories>cond-mat.soft cs.CE nlin.CG physics.comp-ph physics.flu-dyn stat.CO</categories><comments>Accepted for publication in EPL</comments><journal-ref>EPL (Europhysics Letters) 90, 54003 (2010)</journal-ref><doi>10.1209/0295-5075/90/54003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new multiple-relaxation-time lattice Boltzmann scheme for compressible
flows with arbitrary specific heat ratio and Prandtl number is presented. In
the new scheme, which is based on a two-dimensional 16-discrete-velocity model,
the moment space and the corresponding transformation matrix are constructed
according to the seven-moment relations associated with the local equilibrium
distribution function. In the continuum limit, the model recovers the
compressible Navier-Stokes equations with flexible specific-heat ratio and
Prandtl number. Numerical experiments show that compressible flows with strong
shocks can be simulated by the present model up to Mach numbers $Ma \sim 5$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5466</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5466</id><created>2010-04-30</created><authors><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author></authors><title>On computing factors of cyclotomic polynomials</title><categories>math.NT cs.DS</categories><comments>21 pages. An old Technical Report, submitted for archival purposes.
  For further details, see http://wwwmaths.anu.edu.au/~brent/pub/pub135.html</comments><report-no>Technical Report TR-CS-92-13, Department of Computer Science,
  Australian National University, September 1992, 21 pages.</report-no><msc-class>11-04 (Primary) 05A15, 11T06, 11T22, 11T24, 11Y05, 11Y16, 12-04,
  12E10, 12Y05 (Secondary)</msc-class><acm-class>G.1.0; G.2.1</acm-class><journal-ref>Mathematics of Computation 61 (1993), 131-149.</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For odd square-free n &gt; 1 the n-th cyclotomic polynomial satisfies an
identity of Gauss. There are similar identity of Aurifeuille, Le Lasseur and
Lucas. These identities all involve certain polynomials with integer
coefficients. We show how these coefficients can be computed by simple
algorithms which require O(n^2) arithmetic operations and work over the
integers. We also give explicit formulae and generating functions for the
polynomials, and illustrate the application to integer factorization with some
numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5479</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5479</id><created>2010-04-30</created><authors><author><keyname>Zhang</keyname><forenames>Wenyi</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>On Minimax Robust Detection of Stationary Gaussian Signals in White
  Gaussian Noise</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>Submitted; extended abstract to appear in ISIT 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of detecting a wide-sense stationary Gaussian signal process
embedded in white Gaussian noise, where the power spectral density of the
signal process exhibits uncertainty, is investigated. The performance of
minimax robust detection is characterized by the exponential decay rate of the
miss probability under a Neyman-Pearson criterion with a fixed false alarm
probability, as the length of the observation interval grows without bound. A
dominance condition is identified for the uncertainty set of spectral density
functions, and it is established that, under the dominance condition, the
resulting minimax problem possesses a saddle point, which is achievable by the
likelihood ratio tests matched to a so-called dominated power spectral density
in the uncertainty set. No convexity condition on the uncertainty set is
required to establish this result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5495</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5495</id><created>2010-04-30</created><updated>2010-05-19</updated><authors><author><keyname>Mukherjee</keyname><forenames>Somnath</forenames></author><author><keyname>Ghosh</keyname><forenames>Pabitra Kumar</forenames></author></authors><title>The Role of Boolean Function in Fractal Formation and it s Application
  to CDMA Wireless Communication</title><categories>cs.NI</categories><comments>8 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new transformation is generated from a three variable
Boolean function 3, which is used to produce a self-similar fractal pattern of
dimension 1.58. This very fractal pattern is used to reconstruct the whole
structural position of resources in wireless CDMA network. This reconstruction
minimizes the number of resources in the network and so naturally network
consumption costs are getting reduced. Now -a -days resource controlling and
cost minimization are still a severe problem in wireless CDMA network. To
overcome this problem fractal pattern produced in our research provides a
complete solution of structural position of resources in this Wireless CDMA
Network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5500</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5500</id><created>2010-04-30</created><authors><author><keyname>Benzmueller</keyname><forenames>Christoph</forenames></author></authors><title>Simple Type Theory as Framework for Combining Logics</title><categories>cs.LO cs.AI</categories><comments>Contest paper at the World Congress and School on Universal Logic III
  (UNILOG'2010), Lisbon, Portugal, April 18-25, 2010.</comments><msc-class>Mathematical logic and foundations</msc-class><acm-class>F.4.1; I.2.3; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simple type theory is suited as framework for combining classical and
non-classical logics. This claim is based on the observation that various
prominent logics, including (quantified) multimodal logics and intuitionistic
logics, can be elegantly embedded in simple type theory. Furthermore, simple
type theory is sufficiently expressive to model combinations of embedded logics
and it has a well understood semantics. Off-the-shelf reasoning systems for
simple type theory exist that can be uniformly employed for reasoning within
and about combinations of logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5510</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5510</id><created>2010-04-30</created><authors><author><keyname>Bojanczyk</keyname><forenames>Adam W.</forenames></author><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author><author><keyname>de Hoog</keyname><forenames>Frank R.</forenames></author><author><keyname>Sweet</keyname><forenames>Douglas R.</forenames></author></authors><title>On the stability of the Bareiss and related Toeplitz factorization
  algorithms</title><categories>math.NA cs.NA</categories><comments>18 pages. An old Technical Report, submitted for archival purposes.
  For further details, see http://wwwmaths.anu.edu.au/~brent/pub/pub144.html</comments><report-no>Technical Report TR-CS-93-14, Computer Sciences Laboratory,
  Australian National University, November 1993, 18 pages.</report-no><msc-class>65F05 (Primary) 65G50 (Secondary)</msc-class><acm-class>G.1.3</acm-class><journal-ref>SIAM J. Matrix Analysis and Applications 16 (1995), 40-57.</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report contains a numerical stability analysis of factorization
algorithms for computing the Cholesky decomposition of symmetric positive
definite matrices of displacement rank 2. The algorithms in the class can be
expressed as sequences of elementary downdating steps. The stability of the
factorization algorithms follows directly from the numerical properties of
algorithms for realizing elementary downdating operations. It is shown that the
Bareiss algorithm for factorizing a symmetric positive definite Toeplitz matrix
is in the class and hence the Bareiss algorithm is stable. Some numerical
experiments that compare behavior of the Bareiss algorithm and the Levinson
algorithm are presented. These experiments indicate that in general (when the
reflection coefficients are not all positive) the Levinson algorithm is not
stable; certainly it can give much larger residuals than the Bareiss algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5512</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5512</id><created>2010-04-30</created><authors><author><keyname>Biasse</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>LIX, INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Michael</keyname><forenames>Jacobson John</forenames><affiliation>CPSC</affiliation></author><author><keyname>Alan</keyname><forenames>Silverster K.</forenames><affiliation>CPSC</affiliation></author></authors><title>Security Estimates for Quadratic Field Based Cryptosystems</title><categories>cs.CR</categories><proxy>ccsd</proxy><journal-ref>Lecture notes in computer science (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe implementations for solving the discrete logarithm problem in the
class group of an imaginary quadratic field and in the infrastructure of a real
quadratic field. The algorithms used incorporate improvements over
previously-used algorithms, and extensive numerical results are presented
demonstrating their efficiency. This data is used as the basis for
extrapolations, used to provide recommendations for parameter sizes providing
approximately the same level of security as block ciphers with $80,$ $112,$
$128,$ $192,$ and $256$-bit symmetric keys.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5517</identifier>
 <datestamp>2010-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5517</id><created>2010-04-30</created><updated>2010-06-12</updated><authors><author><keyname>Catusse</keyname><forenames>Nicolas</forenames></author><author><keyname>Chepoi</keyname><forenames>Victor</forenames></author><author><keyname>Nouioua</keyname><forenames>Karim</forenames></author><author><keyname>Vax&#xe8;s</keyname><forenames>Yann</forenames></author></authors><title>Minimum Manhattan network problem in normed planes with polygonal balls:
  a factor 2.5 approximation algorithm</title><categories>cs.CG</categories><comments>16 pages, 5 figures; corrected typos, reference added, figure added</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let B be a centrally symmetric convex polygon of R^2 and || p - q || be the
distance between two points p,q in R^2 in the normed plane whose unit ball is
B. For a set T of n points (terminals) in R^2, a B-Manhattan network on T is a
network N(T) = (V,E) with the property that its edges are parallel to the
directions of B and for every pair of terminals t_i and t_j, the network N(T)
contains a shortest B-path between them, i.e., a path of length || t_i - t_j
||. A minimum B-Manhattan network on T is a B-Manhattan network of minimum
possible length. The problem of finding minimum B-Manhattan networks has been
introduced by Gudmundsson, Levcopoulos, and Narasimhan (APPROX'99) in the case
when the unit ball B is a square (and hence the distance || p - q || is the l_1
or the l_infty-distance between p and q) and it has been shown recently by
Chin, Guo, and Sun (SoCG'09) to be strongly NP-complete. Several approximation
algorithms (with factors 8, 4 ,3 , and 2) for minimum Manhattan problem are
known. In this paper, we propose a factor 2.5 approximation algorithm for
minimum B-Manhattan network problem. The algorithm employs a simplified version
of the strip-staircase decomposition proposed in our paper (APPROX'05) and
subsequently used in other factor 2 approximation algorithms for minimum
Manhattan problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5529</identifier>
 <datestamp>2011-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5529</id><created>2010-04-30</created><updated>2011-05-04</updated><authors><author><keyname>Villard</keyname><forenames>Joffrey</forenames></author><author><keyname>Bianchi</keyname><forenames>Pascal</forenames></author></authors><title>High-Rate Vector Quantization for the Neyman-Pearson Detection of
  Correlated Processes</title><categories>cs.IT math.IT math.PR math.ST stat.TH</categories><comments>47 pages, 7 figures, 1 table. To appear in the IEEE Transactions on
  Information Theory</comments><doi>10.1109/TIT.2011.2158479</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the effect of quantization on the performance of the
Neyman-Pearson test. It is assumed that a sensing unit observes samples of a
correlated stationary ergodic multivariate process. Each sample is passed
through an N-point quantizer and transmitted to a decision device which
performs a binary hypothesis test. For any false alarm level, it is shown that
the miss probability of the Neyman-Pearson test converges to zero exponentially
as the number of samples tends to infinity, assuming that the observed process
satisfies certain mixing conditions. The main contribution of this paper is to
provide a compact closed-form expression of the error exponent in the high-rate
regime i.e., when the number N of quantization levels tends to infinity,
generalizing previous results of Gupta and Hero to the case of non-independent
observations. If d represents the dimension of one sample, it is proved that
the error exponent converges at rate N^{2/d} to the one obtained in the absence
of quantization. As an application, relevant high-rate quantization strategies
which lead to a large error exponent are determined. Numerical results indicate
that the proposed quantization rule can yield better performance than existing
ones in terms of detection error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5534</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5534</id><created>2010-04-30</created><authors><author><keyname>Wheeler</keyname><forenames>David A.</forenames></author></authors><title>Fully Countering Trusting Trust through Diverse Double-Compiling</title><categories>cs.CR cs.PL</categories><comments>PhD dissertation. Accepted by George Mason University, Fairfax,
  Virginia, USA's Volgenau School of Information Technology and Engineering in
  2009. 199 single-side printed pages.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An Air Force evaluation of Multics, and Ken Thompson's Turing award lecture
(&quot;Reflections on Trusting Trust&quot;), showed that compilers can be subverted to
insert malicious Trojan horses into critical software, including themselves. If
this &quot;trusting trust&quot; attack goes undetected, even complete analysis of a
system's source code will not find the malicious code that is running.
Previously-known countermeasures have been grossly inadequate. If this attack
cannot be countered, attackers can quietly subvert entire classes of computer
systems, gaining complete control over financial, infrastructure, military,
and/or business systems worldwide. This dissertation's thesis is that the
trusting trust attack can be detected and effectively countered using the
&quot;Diverse Double-Compiling&quot; (DDC) technique, as demonstrated by (1) a formal
proof that DDC can determine if source code and generated executable code
correspond, (2) a demonstration of DDC with four compilers (a small C compiler,
a small Lisp compiler, a small maliciously corrupted Lisp compiler, and a large
industrial-strength C compiler, GCC), and (3) a description of approaches for
applying DDC in various real-world scenarios. In the DDC technique, source code
is compiled twice: the source code of the compiler's parent is compiled using a
trusted compiler, and then the putative compiler source code is compiled using
the result of the first compilation. If the DDC result is bit-for-bit identical
with the original compiler-under-test's executable, and certain other
assumptions hold, then the compiler-under-test's executable corresponds with
its putative source code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5538</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5538</id><created>2010-04-30</created><authors><author><keyname>Orieux</keyname><forenames>Francois</forenames></author><author><keyname>Giovannelli</keyname><forenames>Jean-Francois</forenames></author><author><keyname>Rodet</keyname><forenames>Thomas</forenames></author></authors><title>Bayesian estimation of regularization and PSF parameters for Wiener-Hunt
  deconvolution</title><categories>stat.CO cs.CV physics.data-an stat.ME</categories><doi>10.1364/JOSAA.27.001593</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This paper tackles the problem of image deconvolution with joint estimation
of PSF parameters and hyperparameters. Within a Bayesian framework, the
solution is inferred via a global a posteriori law for unknown parameters and
object. The estimate is chosen as the posterior mean, numerically calculated by
means of a Monte-Carlo Markov chain algorithm. The estimates are efficiently
computed in the Fourier domain and the effectiveness of the method is shown on
simulated examples. Results show precise estimates for PSF parameters and
hyperparameters as well as precise image estimates including restoration of
high-frequencies and spatial details, within a global and coherent approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5540</identifier>
 <datestamp>2010-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5540</id><created>2010-04-30</created><authors><author><keyname>Suresh</keyname><forenames>Ananda T.</forenames></author><author><keyname>Subramanian</keyname><forenames>Arunkumar</forenames></author><author><keyname>Thangaraj</keyname><forenames>Andrew</forenames></author><author><keyname>Bloch</keyname><forenames>Matthieu</forenames></author><author><keyname>McLaughlin</keyname><forenames>Steven</forenames></author></authors><title>Strong Secrecy for Erasure Wiretap Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to the Information Theory Workship (ITW) 2010, Dublin</comments><doi>10.1109/CIG.2010.5592770</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that duals of certain low-density parity-check (LDPC) codes, when
used in a standard coset coding scheme, provide strong secrecy over the binary
erasure wiretap channel (BEWC). This result hinges on a stopping set analysis
of ensembles of LDPC codes with block length $n$ and girth $\geq 2k$, for some
$k \geq 2$. We show that if the minimum left degree of the ensemble is
$l_\mathrm{min}$, the expected probability of block error is
$\calO(\frac{1}{n^{\lceil l_\mathrm{min} k /2 \rceil - k}})$ when the erasure
probability $\epsilon &lt; \epsilon_\mathrm{ef}$, where $\epsilon_\mathrm{ef}$
depends on the degree distribution of the ensemble. As long as $l_\mathrm{min}
&gt; 2$ and $k &gt; 2$, the dual of this LDPC code provides strong secrecy over a
BEWC of erasure probability greater than $1 - \epsilon_\mathrm{ef}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5548</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5548</id><created>2010-04-30</created><authors><author><keyname>Wheeler</keyname><forenames>David A.</forenames></author></authors><title>Countering Trusting Trust through Diverse Double-Compiling</title><categories>cs.CR cs.PL</categories><comments>13 pages.</comments><journal-ref>Proc. of the 21st Annual Computer Security Applications Conference
  (ACSAC), December 5-9, 2005, Tucson, Arizona, pp. 28-40, Los Alamitos: IEEE
  Computer Society, ISBN 0-7695-2461-3, ISSN 1063-9527, IEEE Computer Society
  Order Number P2461.</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An Air Force evaluation of Multics, and Ken Thompson's famous Turing award
lecture &quot;Reflections on Trusting Trust,&quot; showed that compilers can be subverted
to insert malicious Trojan horses into critical software, including themselves.
If this attack goes undetected, even complete analysis of a system's source
code will not find the malicious code that is running, and methods for
detecting this particular attack are not widely known. This paper describes a
practical technique, termed diverse double-compiling (DDC), that detects this
attack and some compiler defects as well. Simply recompile the source code
twice: once with a second (trusted) compiler, and again using the result of the
first compilation. If the result is bit-for-bit identical with the untrusted
binary, then the source code accurately represents the binary. This technique
has been mentioned informally, but its issues and ramifications have not been
identified or discussed in a peer-reviewed work, nor has a public demonstration
been made. This paper describes the technique, justifies it, describes how to
overcome practical challenges, and demonstrates it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5549</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5549</id><created>2010-04-30</created><authors><author><keyname>Carette</keyname><forenames>Jacques</forenames></author><author><keyname>Sexton</keyname><forenames>Alan P.</forenames></author><author><keyname>Sorge</keyname><forenames>Volker</forenames></author><author><keyname>Watt</keyname><forenames>Stephen M.</forenames></author></authors><title>Symbolic Domain Decomposition</title><categories>cs.SC math.NA</categories><comments>Calculemus 2010 (17th Symposium on the Integration of Symbolic
  Computation and Mechanised Reasoning), part of Conferences on Intelligent
  Computer Mathematics 2010. 17 pages.</comments><msc-class>03E99, 05A18, 33F10</msc-class><acm-class>I.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decomposing the domain of a function into parts has many uses in mathematics.
A domain may naturally be a union of pieces, a function may be defined by
cases, or different boundary conditions may hold on different regions. For any
particular problem the domain can be given explicitly, but when dealing with a
family of problems given in terms of symbolic parameters, matters become more
difficult. This article shows how hybrid sets, that is multisets allowing
negative multiplicity, may be used to express symbolic domain decompositions in
an efficient, elegant and uniform way, simplifying both computation and
reasoning. We apply this theory to the arithmetic of piecewise functions and
symbolic matrices and show how certain operations may be reduced from
exponential to linear complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5551</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5551</id><created>2010-04-30</created><authors><author><keyname>Ahlswede</keyname><forenames>R.</forenames></author><author><keyname>Bjelakovic</keyname><forenames>I.</forenames></author><author><keyname>Boche</keyname><forenames>H.</forenames></author><author><keyname>Noetzel</keyname><forenames>J.</forenames></author></authors><title>Entanglement Transmission over Arbitrarily Varying Quantum Channels</title><categories>quant-ph cs.IT math.IT</categories><comments>6 pages, no figures. Accepted for presentation at the ISIT 2010
  Austin, Texas. This is the conference version of a paper in preparation.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a regularized formula for the common randomness assisted
entanglement transmission capacity of finite arbitrarily varying quantum
channels (AVQC's). For finite AVQC's with positive capacity for classical
message transmission we show, by derandomization through classical forward
communication, that the random capacity for entanglement transmission equals
the deterministic capacity for entanglement transmission. This is a quantum
version of the famous Ahlswede dichotomy. In the infinite case, we derive a
similar result for certain classes of AVQC's. At last, we give two possible
definitions of symmetrizability of an AVQC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5570</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5570</id><created>2010-04-30</created><authors><author><keyname>Kowshik</keyname><forenames>Hemant</forenames></author><author><keyname>Kumar</keyname><forenames>P. R.</forenames></author></authors><title>Optimal computation of symmetric Boolean functions in Tree networks</title><categories>cs.IT cs.NI math.IT</categories><comments>Accepted to 2010 IEEE International Symposium on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the scenario where nodes with sensor data are
connected in a tree network, and every node wants to compute a given symmetric
Boolean function of the sensor data. We first consider the problem of computing
a function of two nodes with integer measurements. We allow for block
computation to enhance data fusion efficiency, and determine the minimum
worst-case total number of bits to be exchanged to perform the desired
computation. We establish lower bounds using fooling sets, and provide a novel
scheme which attains the lower bounds, using information theoretic tools. For a
class of functions called sum-threshold functions, this scheme is shown to be
optimal. We then turn to tree networks and derive a lower bound for the number
of bits exchanged on each link by viewing it as a two node problem. We show
that the protocol of recursive innetwork aggregation achieves this lower bound
in the case of sumthreshold functions. Thus we have provided a communication
and in-network computation strategy that is optimal for each link. All the
results can be extended to the case of non-binary alphabets. In the case of
general graphs, we present a cut-set lower bound, and an achievable scheme
based on aggregation along trees. For complete graphs, the complexity of this
scheme is no more than twice that of the optimal scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5571</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5571</id><created>2010-04-30</created><authors><author><keyname>Kowshik</keyname><forenames>Hemant</forenames></author><author><keyname>Kumar</keyname><forenames>P. R.</forenames></author></authors><title>Optimal ordering of transmissions for computing Boolean threhold
  functions</title><categories>cs.IT cs.NI math.IT</categories><comments>Accepted to 2010 IEEE International Symposium on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address a sequential decision problem that arises in the computation of
symmetric Boolean functions of distributed data. We consider a collocated
network, where each node's transmissions can be heard by every other node. Each
node has a Boolean measurement and we wish to compute a given Boolean function
of these measurements. We suppose that the measurements are independent and
Bernoulli distributed. Thus, the problem of optimal computation becomes the
problem of optimally ordering node's transmissions so as to minimize the total
expected number of bits. We solve the ordering problem for the class of Boolean
threshold functions. The optimal ordering is dynamic, i.e., it could
potentially depend on the values of previously transmitted bits. Further, it
depends only on the ordering of the marginal probabilites, but not on their
exact values. This provides an elegant structure for the optimal strategy. For
the case where each node has a block of measurements, the problem is
significantly harder, and we conjecture the optimal strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5588</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5588</id><created>2010-04-30</created><updated>2010-10-13</updated><authors><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>On Achieving Local View Capacity Via Maximal Independent Graph
  Scheduling</title><categories>cs.IT math.IT</categories><comments>Submitted to Special Issue of the IEEE Transactions on Information
  Theory on Interference Networks, Apr 2010</comments><journal-ref>Special Issue of the IEEE Transactions on Information Theory on
  Interference Networks, vol.57, no.5, pp.2711,2729, May 2011</journal-ref><doi>10.1109/TIT.2011.2119630</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;If we know more, we can achieve more.&quot; This adage also applies to
communication networks, where more information about the network state
translates into higher sumrates. In this paper, we formalize this increase of
sum-rate with increased knowledge of the network state. The knowledge of
network state is measured in terms of the number of hops, h, of information
available to each transmitter and is labeled as h-local view. To understand how
much capacity is lost due to limited information, we propose to use the metric
of normalized sum-capacity, which is the h-local view sum-capacity divided by
global-view sum capacity. For the cases of one and two-local view, we
characterize the normalized sum-capacity for many classes of deterministic and
Gaussian interference networks. In many cases, a scheduling scheme called
maximal independent graph scheduling is shown to achieve normalized
sum-capacity. We also show that its generalization for 1-local view, labeled
coded set scheduling, achieves normalized sum-capacity in some cases where its
uncoded counterpart fails to do so.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5600</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5600</id><created>2010-04-30</created><authors><author><keyname>Machanavajjhala</keyname><forenames>Ashwin</forenames></author><author><keyname>Korolova</keyname><forenames>Aleksandra</forenames></author><author><keyname>Sarma</keyname><forenames>Atish Das</forenames></author></authors><title>On the (Im)possibility of Preserving Utility and Privacy in Personalized
  Social Recommendations</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the recent surge of social networks like Facebook, new forms of
recommendations have become possible -- personalized recommendations of ads,
content, and even new social and product connections based on one's social
interactions. In this paper, we study whether &quot;social recommendations&quot;, or
recommendations that utilize a user's social network, can be made without
disclosing sensitive links between users. More precisely, we quantify the loss
in utility when existing recommendation algorithms are modified to satisfy a
strong notion of privacy called differential privacy. We propose lower bounds
on the minimum loss in utility for any recommendation algorithm that is
differentially private. We also propose two recommendation algorithms that
satisfy differential privacy, analyze their performance in comparison to the
lower bound, both analytically and experimentally, and show that good private
social recommendations are feasible only for a few users in the social network
or for a lenient setting of privacy parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1004.5601</identifier>
 <datestamp>2010-05-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1004.5601</id><created>2010-04-30</created><authors><author><keyname>Barg</keyname><forenames>Alexander</forenames></author><author><keyname>Purkayastha</keyname><forenames>Punarbasu</forenames></author></authors><title>Near MDS poset codes and distributions</title><categories>cs.IT math.IT</categories><comments>13 pages, 1 figure</comments><msc-class>94B25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study $q$-ary codes with distance defined by a partial order of the
coordinates of the codewords. Maximum Distance Separable (MDS) codes in the
poset metric have been studied in a number of earlier works. We consider codes
that are close to MDS codes by the value of their minimum distance. For such
codes, we determine their weight distribution, and in the particular case of
the &quot;ordered metric&quot; characterize distributions of points in the unit cube
defined by the codes. We also give some constructions of codes in the ordered
Hamming space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0027</identifier>
 <datestamp>2011-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0027</id><created>2010-04-30</created><updated>2011-06-14</updated><authors><author><keyname>Harel</keyname><forenames>Maayan</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Learning from Multiple Outlooks</title><categories>cs.LG</categories><comments>with full proofs of theorems and all experiments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel problem formulation of learning a single task when the
data are provided in different feature spaces. Each such space is called an
outlook, and is assumed to contain both labeled and unlabeled data. The
objective is to take advantage of the data from all the outlooks to better
classify each of the outlooks. We devise an algorithm that computes optimal
affine mappings from different outlooks to a target outlook by matching moments
of the empirical distributions. We further derive a probabilistic
interpretation of the resulting algorithm and a sample complexity bound
indicating how many samples are needed to adequately find the mapping. We
report the results of extensive experiments on activity recognition tasks that
show the value of the proposed approach in boosting performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0043</identifier>
 <datestamp>2011-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0043</id><created>2010-04-30</created><updated>2011-03-31</updated><authors><author><keyname>Zeng</keyname><forenames>Bing</forenames></author><author><keyname>Tang</keyname><forenames>Xueming</forenames></author><author><keyname>Hsu</keyname><forenames>Chingfang</forenames></author></authors><title>A Framework For Fully-Simulatable $h$-Out-Of-$n$ Oblivious Transfer</title><categories>cs.CR</categories><comments>submitted to IEEE transaction on information theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for fully-simulatable $h$-out-of-$n$ oblivious
transfer ($OT^{n}_{h}$) with security against non-adaptive malicious
adversaries. The framework costs six communication rounds and costs at most
$40n$ public-key operations in computational overhead. Compared with the known
protocols for fully-simulatable oblivious transfer that works in the plain mode
(where there is no trusted common reference string available) and proven to be
secure under standard model (where there is no random oracle available), the
instantiation based on the decisional Diffie-Hellman assumption of the
framework is the most efficient one, no matter seen from communication rounds
or computational overhead.
  Our framework uses three abstract tools, i.e., perfectly binding commitment,
perfectly hiding commitment and our new smooth projective hash. This allows a
simple and intuitive understanding of its security.
  We instantiate the new smooth projective hash under the lattice assumption,
the decisional Diffie-Hellman assumption, the decisional $N$-th residuosity
assumption, the decisional quadratic residuosity assumption. This indeed shows
that the folklore that it is technically difficult to instantiate the
projective hash framework under the lattice assumption is not true. What's
more, by using this lattice-based hash and lattice-based commitment scheme, we
gain a concrete protocol for $OT^{n}_{h}$ which is secure against quantum
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0047</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0047</id><created>2010-05-01</created><authors><author><keyname>Agarwal</keyname><forenames>Arvind</forenames></author><author><keyname>Daume</keyname><forenames>Hal</forenames><suffix>III</suffix></author></authors><title>A Geometric View of Conjugate Priors</title><categories>cs.LG</categories><comments>16 pages, 4 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Bayesian machine learning, conjugate priors are popular, mostly due to
mathematical convenience. In this paper, we show that there are deeper reasons
for choosing a conjugate prior. Specifically, we formulate the conjugate prior
in the form of Bregman divergence and show that it is the inherent geometry of
conjugate priors that makes them appropriate and intuitive. This geometric
interpretation allows one to view the hyperparameters of conjugate priors as
the {\it effective} sample points, thus providing additional intuition. We use
this geometric understanding of conjugate priors to derive the hyperparameters
and expression of the prior used to couple the generative and discriminative
components of a hybrid model for semi-supervised learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0052</identifier>
 <datestamp>2010-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0052</id><created>2010-05-01</created><updated>2010-06-07</updated><authors><author><keyname>Kim</keyname><forenames>Byung-Hak</forenames></author><author><keyname>Pfister</keyname><forenames>Henry D.</forenames></author></authors><title>On the Joint Decoding of LDPC Codes and Finite-State Channels via Linear
  Programming</title><categories>cs.IT math.IT</categories><comments>To appear in Proc. 2010 IEEE Int. Symp. Information Theory, Ausin,
  TX, June 12-18, 2010 (a small error in the reference corrected)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the linear programming (LP) decoder for binary linear codes,
introduced by Feldman, et al. is extended to joint-decoding of binary-input
finite-state channels. In particular, we provide a rigorous definition of LP
joint-decoding pseudo-codewords (JD-PCWs) that enables evaluation of the
pairwise error probability between codewords and JD-PCWs. This leads naturally
to a provable upper bound on decoder failure probability. If the channel is a
finite-state intersymbol interference channel, then the LP joint decoder also
has the maximum-likelihood (ML) certificate property and all integer valued
solutions are codewords. In this case, the performance loss relative to ML
decoding can be explained completely by fractional valued JD-PCWs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0053</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0053</id><created>2010-05-01</created><authors><author><keyname>F&#xfa;ster-Sabater</keyname><forenames>A.</forenames></author><author><keyname>Caballero-Gil</keyname><forenames>P.</forenames></author></authors><title>Global Linear Complexity Analysis of Filter Keystream Generators</title><categories>cs.CR</categories><journal-ref>IEE Proceedings Computers and Digital Techniques, January 1997,
  Volume 144, Issue 1, p.33-38</journal-ref><doi>10.1049/ip-cdt:19970764</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An efficient algorithm for computing lower bounds on the global linear
complexity of nonlinearly filtered PN-sequences is presented. The technique
here developed is based exclusively on the realization of bit wise logic
operations, which makes it appropriate for both software simulation and
hardware implementation. The present algorithm can be applied to any arbitrary
nonlinear function with a unique term of maximum order. Thus, the extent of its
application for different types of filter generators is quite broad.
Furthermore, emphasis is on the large lower bounds obtained that confirm the
exponential growth of the global linear complexity for the class of nonlinearly
filtered sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0054</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0054</id><created>2010-05-01</created><authors><author><keyname>Caballero-Gil</keyname><forenames>P.</forenames></author><author><keyname>Hern&#xe1;ndez-Goya</keyname><forenames>C.</forenames></author></authors><title>Secret Sharing Based on a Hard-on-Average Problem</title><categories>cs.CR</categories><journal-ref>Linear Algebra and its Applications. Volume 414, Issues 2-3, 15
  April 2006, Pages 626-631</journal-ref><doi>10.1016/j.laa.2005.10.040</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main goal of this work is to propose the design of secret sharing schemes
based on hard-on-average problems. It includes the description of a new
multiparty protocol whose main application is key management in networks. Its
unconditionally perfect security relies on a discrete mathematics problem
classiffied as DistNP-Complete under the average-case analysis, the so-called
Distributional Matrix Representability Problem. Thanks to the use of the search
version of the mentioned decision problem, the security of the proposed scheme
is guaranteed. Although several secret sharing schemes connected with
combinatorial structures may be found in the bibliography, the main
contribution of this work is the proposal of a new secret sharing scheme based
on a hard-on-average problem, which allows to enlarge the set of tools for
designing more secure cryptographic applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0055</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0055</id><created>2010-05-01</created><authors><author><keyname>Caballero-Gil</keyname><forenames>Pino</forenames></author><author><keyname>F&#xfa;ster-Sabater</keyname><forenames>Amparo</forenames></author></authors><title>On the Design of Cryptographic Primitives</title><categories>cs.CR</categories><journal-ref>Acta Applicandae Mathematicae. Volume 93, Numbers 1-3, pp.
  279-297. Sept 2006. Springer.</journal-ref><doi>10.1007/s10440-006-9044-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objective of this work is twofold. On the one hand, it gives a brief
overview of the area of two-party cryptographic protocols. On the other hand,
it proposes new schemes and guidelines for improving the practice of robust
protocol design. In order to achieve such a double goal, a tour through the
descriptions of the two main cryptographic primitives is carried out. Within
this survey, some of the most representative algorithms based on the Theory of
Finite Fields are provided and new general schemes and specific algorithms
based on Graph Theory are proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0058</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0058</id><created>2010-05-01</created><authors><author><keyname>F&#xfa;ster-Sabater</keyname><forenames>A.</forenames></author><author><keyname>Caballero-Gil</keyname><forenames>P.</forenames></author></authors><title>Linear solutions for cryptographic nonlinear sequence generators</title><categories>cs.CR</categories><journal-ref>Physics Letters A Vol. 369, Is. 5-6, 1 Oct. 2007, pp. 432-437</journal-ref><doi>10.1016/j.physleta.2007.04.103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter shows that linear Cellular Automata based on rules 90/150
generate all the solutions of linear difference equations with binary constant
coefficients. Some of these solutions are pseudo-random noise sequences with
application in cryptography: the sequences generated by the class of shrinking
generators. Consequently, this contribution show that shrinking generators do
not provide enough guarantees to be used for encryption purposes. Furthermore,
the linearization is achieved through a simple algorithm about which a full
description is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0063</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0063</id><created>2010-05-01</created><updated>2010-07-28</updated><authors><author><keyname>Pathak</keyname><forenames>Manas A.</forenames></author><author><keyname>Raj</keyname><forenames>Bhiksha</forenames></author></authors><title>Large Margin Multiclass Gaussian Classification with Differential
  Privacy</title><categories>stat.ML cs.CR cs.LG</categories><comments>14 pages</comments><journal-ref>Proceedings of the ECML/PKDD Workshop on Privacy and Security
  issues in Data Mining and Machine Learning, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As increasing amounts of sensitive personal information is aggregated into
data repositories, it has become important to develop mechanisms for processing
the data without revealing information about individual data instances. The
differential privacy model provides a framework for the development and
theoretical analysis of such mechanisms. In this paper, we propose an algorithm
for learning a discriminatively trained multi-class Gaussian classifier that
satisfies differential privacy using a large margin loss function with a
perturbed regularization term. We present a theoretical upper bound on the
excess risk of the classifier introduced by the perturbation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0069</identifier>
 <datestamp>2010-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0069</id><created>2010-05-01</created><authors><author><keyname>Censor</keyname><forenames>Y.</forenames></author><author><keyname>Davidi</keyname><forenames>R.</forenames></author><author><keyname>Herman</keyname><forenames>G. T.</forenames></author></authors><title>Perturbation Resilience and Superiorization of Iterative Algorithms</title><categories>math.OC cs.CV physics.med-ph</categories><comments>Accepted for publication in Inverse Problems, 2010</comments><journal-ref>Inverse Problems, 26 (2010) 065008 (12pp)</journal-ref><doi>10.1088/0266-5611/26/6/065008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iterative algorithms aimed at solving some problems are discussed. For
certain problems, such as finding a common point in the intersection of a
finite number of convex sets, there often exist iterative algorithms that
impose very little demand on computer resources. For other problems, such as
finding that point in the intersection at which the value of a given function
is optimal, algorithms tend to need more computer memory and longer execution
time. A methodology is presented whose aim is to produce automatically for an
iterative algorithm of the first kind a &quot;superiorized version&quot; of it that
retains its computational efficiency but nevertheless goes a long way towards
solving an optimization problem. This is possible to do if the original
algorithm is &quot;perturbation resilient,&quot; which is shown to be the case for
various projection algorithms for solving the consistent convex feasibility
problem. The superiorized versions of such algorithms use perturbations that
drive the process in the direction of the optimizer of the given function.
After presenting these intuitive ideas in a precise mathematical form, they are
illustrated in image reconstruction from projections for two different
projection algorithms superiorized for the function whose value is the total
variation of the image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0072</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0072</id><created>2010-05-01</created><authors><author><keyname>El-Badry</keyname><forenames>Rania</forenames></author><author><keyname>Sultan</keyname><forenames>Ahmed</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author></authors><title>HyberLoc: Providing Physical Layer Location Privacy in Hybrid Sensor
  Networks</title><categories>cs.IT cs.CR math.IT</categories><comments>7 pages, 4 figures, ICC'10</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many hybrid wireless sensor networks' applications, sensor nodes are
deployed in hostile environments where trusted and un-trusted nodes co-exist.
In anchor-based hybrid networks, it becomes important to allow trusted nodes to
gain full access to the location information transmitted in beacon frames
while, at the same time, prevent un-trusted nodes from using this information.
The main challenge is that un-trusted nodes can measure the physical signal
transmitted from anchor nodes, even if these nodes encrypt their transmission.
Using the measured signal strength, un-trusted nodes can still tri-laterate the
location of anchor nodes. In this paper, we propose HyberLoc, an algorithm that
provides anchor physical layer location privacy in anchor-based hybrid sensor
networks. The idea is for anchor nodes to dynamically change their transmission
power following a certain probability distribution, degrading the localization
accuracy at un-trusted nodes while maintaining high localization accuracy at
trusted nodes. Given an average power constraint, our analysis shows that the
discretized exponential distribution is the distribution that maximizes
location uncertainty at the untrusted nodes. Detailed evaluation through
analysis, simulation, and implementation shows that HyberLoc gives trusted
nodes up to 3.5 times better localization accuracy as compared to untrusted
nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0075</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0075</id><created>2010-05-01</created><authors><author><keyname>Cui</keyname><forenames>Ying</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author></authors><title>Distributive Stochastic Learning for Delay-Optimal OFDMA Power and
  Subband Allocation</title><categories>cs.LG</categories><comments>To appear in Transactions on Signal Processing</comments><doi>10.1109/TSP.2010.2050062</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the distributive queue-aware power and subband
allocation design for a delay-optimal OFDMA uplink system with one base
station, $K$ users and $N_F$ independent subbands. Each mobile has an uplink
queue with heterogeneous packet arrivals and delay requirements. We model the
problem as an infinite horizon average reward Markov Decision Problem (MDP)
where the control actions are functions of the instantaneous Channel State
Information (CSI) as well as the joint Queue State Information (QSI). To
address the distributive requirement and the issue of exponential memory
requirement and computational complexity, we approximate the subband allocation
Q-factor by the sum of the per-user subband allocation Q-factor and derive a
distributive online stochastic learning algorithm to estimate the per-user
Q-factor and the Lagrange multipliers (LM) simultaneously and determine the
control actions using an auction mechanism. We show that under the proposed
auction mechanism, the distributive online learning converges almost surely
(with probability 1). For illustration, we apply the proposed distributive
stochastic learning framework to an application example with exponential packet
size distribution. We show that the delay-optimal power control has the {\em
multi-level water-filling} structure where the CSI determines the instantaneous
power allocation and the QSI determines the water-level. The proposed algorithm
has linear signaling overhead and computational complexity $\mathcal O(KN)$,
which is desirable from an implementation perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0080</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0080</id><created>2010-05-01</created><authors><author><keyname>Chen</keyname><forenames>Xiaoyu</forenames></author></authors><title>Electronic Geometry Textbook: A Geometric Textbook Knowledge Management
  System</title><categories>cs.AI cs.MS</categories><comments>To appear in The 9th International Conference on Mathematical
  Knowledge Management: MKM 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electronic Geometry Textbook is a knowledge management system that manages
geometric textbook knowledge to enable users to construct and share dynamic
geometry textbooks interactively and efficiently. Based on a knowledge base
organizing and storing the knowledge represented in specific languages, the
system implements interfaces for maintaining the data representing that
knowledge as well as relations among those data, for automatically generating
readable documents for viewing or printing, and for automatically discovering
the relations among knowledge data. An interface has been developed for users
to create geometry textbooks with automatic checking, in real time, of the
consistency of the structure of each resulting textbook. By integrating an
external geometric theorem prover and an external dynamic geometry software
package, the system offers the facilities for automatically proving theorems
and generating dynamic figures in the created textbooks. This paper provides a
comprehensive account of the current version of Electronic Geometry Textbook.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0082</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0082</id><created>2010-05-01</created><authors><author><keyname>Caballero-Gil</keyname><forenames>P.</forenames></author><author><keyname>Hern&#xe1;ndez-Goya</keyname><forenames>C.</forenames></author><author><keyname>Bruno-Casta&#xf1;eda</keyname><forenames>C.</forenames></author></authors><title>A Rational Approach to Cryptographic Protocols</title><categories>cs.CR</categories><journal-ref>Mathematical and Computer Modelling. Volume 46, Issues 1-2, July
  2007, Pages 80-87.</journal-ref><doi>10.1016/j.mcm.2006.12.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work initiates an analysis of several cryptographic protocols from a
rational point of view using a game-theoretical approach, which allows us to
represent not only the protocols but also possible misbehaviours of parties.
Concretely, several concepts of two-person games and of two-party cryptographic
protocols are here combined in order to model the latters as the formers. One
of the main advantages of analysing a cryptographic protocol in the game-theory
setting is the possibility of describing improved and stronger cryptographic
solutions because possible adversarial behaviours may be taken into account
directly. With those tools, protocols can be studied in a malicious model in
order to find equilibrium conditions that make possible to protect honest
parties against all possible strategies of adversaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0085</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0085</id><created>2010-05-01</created><authors><author><keyname>Jia</keyname><forenames>Xiaohong</forenames></author><author><keyname>Goldman</keyname><forenames>Ron</forenames></author></authors><title>Using Smith Normal Forms and mu-Bases to Compute all the Singularities
  of Rational Planar Curves</title><categories>cs.CG</categories><comments>journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the conjecture of Chen, Wang and Liu in [8] concerning how to
calculate the parameter values corresponding to all the singu- larities,
including the infinitely near singularities, of rational planar curves from the
Smith normal forms of certain Bezout resultant ma- trices derived from
mu-bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0086</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0086</id><created>2010-05-01</created><authors><author><keyname>Caballero-Gil</keyname><forenames>Pino</forenames></author><author><keyname>F&#xfa;ster-Sabater</keyname><forenames>Amparo</forenames></author><author><keyname>Delgado-Mohatar</keyname><forenames>Oscar</forenames></author></authors><title>Linear Cellular Automata as Discrete Models for Generating Cryptographic
  Sequences</title><categories>cs.CR</categories><journal-ref>Journal of Research and Practice in Information Technology, Vol.
  40, No. 4, November 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a new cellular automata-based linear model for
several nonlinear pseudorandom number generators with practical applications in
symmetric cryptography. Such a model generates all the solutions of linear
binary difference equations as well as many of these solutions are
pseudo-random keystream sequences. In this way, a linear structure based on
cellular automata may be used to generate not only difference equation
solutions but also cryptographic sequences. The proposed model is very simple
since it is based exclusively on successive concatenations of a basic linear
automaton.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0087</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0087</id><created>2010-05-01</created><authors><author><keyname>Caballero-Gil</keyname><forenames>Pino</forenames></author><author><keyname>F&#xfa;ster-Sabater</keyname><forenames>Amparo</forenames></author><author><keyname>Pazo-Robles</keyname><forenames>M. Eugenia</forenames></author></authors><title>New Attack Strategy for the Shrinking Generator</title><categories>cs.CR</categories><journal-ref>Journal of Research and Practice in Information Technology, Vol.
  41, No. 2, May 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work shows that the cryptanalysis of the shrinking generator requires
fewer intercepted bits than what indicated by the linear complexity. Indeed,
whereas the linear complexity of shrunken sequences is between $A \cdot
2^(S-2)$ and $A \cdot 2^(S-1)$, we claim that the initial states of both
component registers are easily computed with less than $A \cdot S$ shrunken
bits. Such a result is proven thanks to the definition of shrunken sequences as
interleaved sequences. Consequently, it is conjectured that this statement can
be extended to all interleaved sequences. Furthermore, this paper confirms that
certain bits of the interleaved sequences have a greater strategic importance
than others, which may be considered as a proof of weakness of interleaved
generators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0089</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0089</id><created>2010-05-01</created><authors><author><keyname>Kelsey</keyname><forenames>Tom</forenames></author><author><keyname>Kotthoff</keyname><forenames>Lars</forenames></author></authors><title>The Exact Closest String Problem as a Constraint Satisfaction Problem</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report (to our knowledge) the first evaluation of Constraint Satisfaction
as a computational framework for solving closest string problems. We show that
careful consideration of symbol occurrences can provide search heuristics that
provide several orders of magnitude speedup at and above the optimal distance.
We also report (to our knowledge) the first analysis and evaluation -- using
any technique -- of the computational difficulties involved in the
identification of all closest strings for a given input set. We describe
algorithms for web-scale distributed solution of closest string problems, both
purely based on AI backtrack search and also hybrid numeric-AI methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0092</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0092</id><created>2010-05-01</created><authors><author><keyname>Sagatov</keyname><forenames>E. S.</forenames></author><author><keyname>Sukhov</keyname><forenames>A. M.</forenames></author><author><keyname>Calyam</keyname><forenames>P.</forenames></author></authors><title>Influence of distortions of key frames on video transfer in wireless
  networks</title><categories>cs.NI cs.MM</categories><comments>6 pages, 4 figures, 2 Tables</comments><acm-class>H.4.3; C.2.1; C.2.5</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper it is shown that for substantial increase of video quality in
wireless network it is necessary to execute two obligatory points on
modernization of the communication scheme. The player on the received part
should throw back automatically duplicated RTP packets, server of streaming
video should duplicate the packets containing the information of key frames.
Coefficients of the mathematical model describing video quality in wireless
network have been found for WiFi and 3G standards and codecs MPEG-2 and MPEG-4
(DivX). The special experimental technique which has allowed collecting and
processing the data has been developed for calculation of values of factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0095</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0095</id><created>2010-05-01</created><authors><author><keyname>Caballero-Gil</keyname><forenames>P.</forenames></author><author><keyname>F&#xfa;ster-Sabater</keyname><forenames>A.</forenames></author></authors><title>A Simple Attack on Some Clock-Controlled Generators</title><categories>cs.CR</categories><journal-ref>Computers &amp; Mathematics with Applications. Vol. 58 , Is. 1 (July
  2009) pp. 179-188</journal-ref><doi>10.1016/j.camwa.2009.03.103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach to edit distance attacks on certain
clock-controlled generators, which applies basic concepts of Graph Theory to
simplify the search trees of the original attacks in such a way that only the
most promising branches are analyzed. In particular, the proposed improvement
is based on cut sets defined on some graphs so that certain shortest paths
provide the edit distances. The strongest aspects of the proposal are that the
obtained results from the attack are absolutely deterministic, and that many
inconsistent initial states of the target registers are recognized beforehand
and avoided during search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0104</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0104</id><created>2010-05-01</created><authors><author><keyname>Gupta</keyname><forenames>Rahul</forenames></author><author><keyname>Sarawagi</keyname><forenames>Sunita</forenames></author></authors><title>Joint Structured Models for Extraction from Overlapping Sources</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of jointly training structured models for extraction
from sources whose instances enjoy partial overlap. This has important
applications like user-driven ad-hoc information extraction on the web. Such
applications present new challenges in terms of the number of sources and their
arbitrary pattern of overlap not seen by earlier collective training schemes
applied on two sources. We present an agreement-based learning framework and
alternatives within it to trade-off tractability, robustness to noise, and
extent of agreement. We provide a principled scheme to discover low-noise
agreement sets in unlabeled data across the sources. Through extensive
experiments over 58 real datasets, we establish that our method of additively
rewarding agreement over maximal segments of text provides the best trade-offs,
and also scores over alternatives such as collective inference, staged
training, and multi-view learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0106</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0106</id><created>2010-05-01</created><authors><author><keyname>Caballero-Gil</keyname><forenames>P.</forenames></author><author><keyname>Hern&#xe1;ndez-Goya</keyname><forenames>C.</forenames></author></authors><title>Self-Organized Authentication in Mobile Ad-hoc Networks</title><categories>cs.CR</categories><journal-ref>Journal of Communications and Networks. 2009 11: 509-517</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a new distributed and self-organized authentication scheme
for Mobile Ad-hoc NETworks (MANETs). Apart from describing all its components,
special emphasis is placed on proving that the proposal fulfils most
requirements derived from the special characteristics of MANETs, including
limited physical protection of broadcast medium, frequent route changes caused
by mobility, and lack of structured hierarchy. Interesting conclusions are
obtained from an analysis of simulation experiments in different scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0108</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0108</id><created>2010-05-01</created><updated>2010-06-05</updated><authors><author><keyname>Caballero-Gil</keyname><forenames>P.</forenames></author><author><keyname>F&#xfa;ster-Sabater</keyname><forenames>A.</forenames></author><author><keyname>Pazo-Robles</keyname><forenames>M. E.</forenames></author></authors><title>Using Linear Difference Equations to Model Nonlinear Cryptographic
  Sequences</title><categories>cs.CR</categories><journal-ref>International Journal of Nonlinear Sciences &amp; Numerical Simulation
  11(3): 165-172, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new class of linear sequence generators based on cellular automata is here
introduced in order to model several nonlinear keystream generators with
practical applications in symmetric cryptography. The output sequences are
written as solutions of linear difference equations, and three basic properties
(period, linear complexity and number of different output sequences) are
analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0109</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0109</id><created>2010-05-01</created><updated>2010-05-10</updated><authors><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author><author><keyname>Gada</keyname><forenames>Dhaval</forenames></author><author><keyname>Gogri</keyname><forenames>Rajat</forenames></author><author><keyname>Rathod</keyname><forenames>Punit</forenames></author><author><keyname>Dedhia</keyname><forenames>Zalak</forenames></author><author><keyname>Mody</keyname><forenames>Nirali</forenames></author></authors><title>Security Scheme for Distributed DoS in Mobile Ad Hoc Networks</title><categories>cs.CR</categories><comments>12 Pg, 7 Fig, 2 Tables, Shorter version was presented in IWDC-2004,
  LNCS Vol.3326. ISBN: 3-540-24076-4,pp 541-542, 2004. NOTE:
  AODV_modifications.zip
  &lt;http://www.tifr.res.in/~sanyal/papers/AODV_modifications.zip&gt; contains all
  associated codes and its possible explanation.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Mobile Ad Hoc Networks (MANET), various types of Denial of Service Attacks
(DoS) are possible because of the inherent limitations of its routing
protocols. Considering the Ad Hoc On Demand Vector (AODV) routing protocol as
the base protocol it is possible to find a suitable solution to over-come the
attack of initiating / forwarding fake Route Requests (RREQs) that lead to
hogging of network resources and hence denial of service to genuine nodes. In
this paper, a proactive scheme is proposed that could prevent a specific kind
of DoS attack and identify the misbehaving node. Since the proposed scheme is
distributed in nature it has the capability to prevent Distributed DoS (DDoS)
as well. The performance of the proposed algorithm in a series of simulations
reveal that the proposed scheme provides a better solution than existing
approaches with no extra overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0117</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0117</id><created>2010-05-01</created><authors><author><keyname>Jalali</keyname><forenames>Shirin</forenames></author><author><keyname>Effros</keyname><forenames>Michelle</forenames></author></authors><title>On the Separation of Lossy Source-Network Coding and Channel Coding in
  Wireline Networks</title><categories>cs.IT math.IT</categories><comments>5 pages, to appear in the proceedings of 2010 IEEE International
  Symposium on Information Theory (ISIT)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proves the separation between source-network coding and channel
coding in networks of noisy, discrete, memoryless channels. We show that the
set of achievable distortion matrices in delivering a family of dependent
sources across such a network equals the set of achievable distortion matrices
for delivering the same sources across a distinct network which is built by
replacing each channel by a noiseless, point-to-point bit-pipe of the
corresponding capacity. Thus a code that applies source-network coding across
links that are made almost lossless through the application of independent
channel coding across each link asymptotically achieves the optimal performance
across the network as a whole.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0125</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0125</id><created>2010-05-02</created><authors><author><keyname>Di Castro</keyname><forenames>Dotan</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Adaptive Bases for Reinforcement Learning</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of reinforcement learning using function
approximation, where the approximating basis can change dynamically while
interacting with the environment. A motivation for such an approach is
maximizing the value function fitness to the problem faced. Three errors are
considered: approximation square error, Bellman residual, and projected Bellman
residual. Algorithms under the actor-critic framework are presented, and shown
to converge. The advantage of such an adaptive basis is demonstrated in
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0129</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0129</id><created>2010-05-02</created><authors><author><keyname>Ananichev</keyname><forenames>Dmitry S.</forenames></author><author><keyname>Gusev</keyname><forenames>Vladimir V.</forenames></author><author><keyname>Volkov</keyname><forenames>Mikhail V.</forenames></author></authors><title>Slowly synchronizing automata and digraphs</title><categories>cs.FL cs.DM</categories><comments>13 pages, 5 figures</comments><msc-class>68Q45, 68R10</msc-class><journal-ref>In: A. Kucera, P. Hlineny (eds.), Mathematical Foundations of
  Computer Science [Lect. Notes Comp. Sci., 6281], Springer-Verlag, 2010, 55-65</journal-ref><doi>10.1007/978-3-642-15155-2_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present several infinite series of synchronizing automata for which the
minimum length of reset words is close to the square of the number of states.
These automata are closely related to primitive digraphs with large exponent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0139</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0139</id><created>2010-05-02</created><authors><author><keyname>Kataria</keyname><forenames>Jayesh</forenames></author><author><keyname>Dhekne</keyname><forenames>P. S.</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>ACRR: Ad Hoc On-Demand Distance Vector Routing with Controlled Route
  Requests</title><categories>cs.CR</categories><comments>15 Pages, 6 Figures, 1 table, 3rd International Conference on
  Computers and Devices for Communication (CODEC-06) Institute of Radio Physics
  and Electronics, University of Calcutta, December 18-20, 2006.</comments><journal-ref>International Journal of Computers, Information Technology and
  Engineering (IJCITAE), Vol. 1, No. 1, pp 9-15, June 2007, Serial
  Publications.</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reactive routing protocols like Ad Hoc On-Demand Distance Vector Routing
(AODV) and Dynamic Source Routing (DSR)in Ad-Hoc Wireless Networks which are
used in Mobile and Ad Hoc Networks (MANETs) work by flooding the network with
control packets. There is generally a limit on the number of these packets that
can be generated or forwarded. But a malicious node can disregard this limit
and flood the network with fake control packets. These packets hog the limited
bandwidth and processing power of genuine nodes in the network while being
forwarded. Due to this, genuine route requests suffer and many routes either do
not get a chance to materialize or they end up being longer than otherwise. In
this paper we propose a non cryptographic solution to the above problem and
prove its efficiency by means of simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0146</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0146</id><created>2010-05-02</created><authors><author><keyname>Kovalchuk</keyname><forenames>Andriy</forenames></author><author><keyname>Levitsky</keyname><forenames>Vyacheslav</forenames></author><author><keyname>Samolyuk</keyname><forenames>Igor</forenames></author><author><keyname>Yanchuk</keyname><forenames>Valentyn</forenames></author></authors><title>The Formulator MathML Editor Project: User-Friendly Authoring of Content
  Markup Documents</title><categories>cs.DL cs.HC cs.MS</categories><comments>To appear in The 9th International Conference on Mathematical
  Knowledge Management: MKM 2010</comments><msc-class>68U15, 68U35, 97U70</msc-class><acm-class>G.4; H.5.2; I.7.2; K.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implementation of an editing process for Content MathML formulas in common
visual style is a real challenge for a software developer who does not really
want the user to have to understand the structure of Content MathML in order to
edit an expression, since it is expected that users are often not that
technically minded. In this paper, we demonstrate how this aim is achieved in
the context of the Formulator project and discuss features of this MathML
editor, which provides a user with a WYSIWYG editing style while authoring
MathML documents with Content or mixed markup. We also present the approach
taken to enhance availability of the MathML editor to end-users, demonstrating
an online version of the editor that runs inside a Web browser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0162</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0162</id><created>2010-05-02</created><updated>2010-05-04</updated><authors><author><keyname>Alhazmi</keyname><forenames>Ali</forenames></author><author><keyname>Al-Sharawi</keyname><forenames>Abdulrahman</forenames></author><author><keyname>Liu</keyname><forenames>Bing</forenames></author><author><keyname>Oliveira</keyname><forenames>Deyvisson</forenames></author><author><keyname>Sobh</keyname><forenames>Kanj</forenames></author><author><keyname>Mayantz</keyname><forenames>Max</forenames></author><author><keyname>de Bled</keyname><forenames>Robin</forenames></author><author><keyname>Zhang</keyname><forenames>Yu Ming</forenames></author></authors><title>Software Requirements Specification of the IUfA's UUIS -- a Team 4
  COMP5541-W10 Project Approach</title><categories>cs.SE</categories><comments>30 pages, 13 figures</comments><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document presents the business requirement of Unified University
Inventory System (UUIS) in Technology-independent manner. All attempts have
been made in using mostly business terminology and business language while
describing the requirements in this document. Very minimal and commonly
understood Technical terminology is used. Use case approach is used in modeling
the business requirements in this document.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0167</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0167</id><created>2010-05-02</created><updated>2011-05-20</updated><authors><author><keyname>Anand</keyname><forenames>M.</forenames></author><author><keyname>Kumar</keyname><forenames>P. R.</forenames></author></authors><title>A digital interface for Gaussian relay and interference networks:
  Lifting codes from the discrete superposition model</title><categories>cs.IT math.IT</categories><comments>Final version</comments><journal-ref>Special issue on interference Networks, IEEE Trans. Info. Theory,
  vol. 57, no. 5, pp. 2548 - 2564, May 2011</journal-ref><doi>10.1109/TIT.2011.2120070</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For every Gaussian network, there exists a corresponding deterministic
network called the discrete superposition network. We show that this discrete
superposition network provides a near-optimal digital interface for operating a
class consisting of many Gaussian networks in the sense that any code for the
discrete superposition network can be naturally lifted to a corresponding code
for the Gaussian network, while achieving a rate that is no more than a
constant number of bits lesser than the rate it achieves for the discrete
superposition network. This constant depends only on the number of nodes in the
network and not on the channel gains or SNR. Moreover the capacities of the two
networks are within a constant of each other, again independent of channel
gains and SNR. We show that the class of Gaussian networks for which this
interface property holds includes relay networks with a single
source-destination pair, interference networks, multicast networks, and the
counterparts of these networks with multiple transmit and receive antennas.
  The code for the Gaussian relay network can be obtained from any code for the
discrete superposition network simply by pruning it. This lifting scheme
establishes that the superposition model can indeed potentially serve as a
strong surrogate for designing codes for Gaussian relay networks.
  We present similar results for the K x K Gaussian interference network, MIMO
Gaussian interference networks, MIMO Gaussian relay networks, and multicast
networks, with the constant gap depending additionally on the number of
antennas in case of MIMO networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0169</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0169</id><created>2010-05-02</created><authors><author><keyname>Sobh</keyname><forenames>Kanj</forenames></author><author><keyname>Oliveira</keyname><forenames>Deyvisson</forenames></author><author><keyname>Liu</keyname><forenames>Bing</forenames></author><author><keyname>Mayantz</keyname><forenames>Max</forenames></author><author><keyname>Zhang</keyname><forenames>Yu Ming</forenames></author><author><keyname>Alhazmi</keyname><forenames>Ali</forenames></author><author><keyname>de Bled</keyname><forenames>Robin</forenames></author><author><keyname>Al-Sharawi</keyname><forenames>Abdulrahman</forenames></author></authors><title>Software Design Document, Testing, Deployment and Configuration
  Management, and User Manual of the UUIS -- a Team 4 COMP5541-W10 Project
  Approach</title><categories>cs.SE</categories><comments>90 pages, 94 figures</comments><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document provides a description of the technical design for Unified
University Inventory System - Web Portal. This document's primary purpose is to
describe the technical vision for how business requirements will be realized.
This document provides an architectural overview of the system to depict
different aspects of the system. This document also functions as a foundational
reference point for developers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0178</identifier>
 <datestamp>2010-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0178</id><created>2010-05-02</created><updated>2010-12-06</updated><authors><author><keyname>Wong</keyname><forenames>Pui King</forenames></author><author><keyname>Yin</keyname><forenames>Dongjie</forenames></author><author><keyname>Lee</keyname><forenames>Tony T.</forenames></author></authors><title>Analysis of Non-Persistent CSMA Protocols with Exponential Backoff
  Scheduling</title><categories>cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the performance of Non-persistent CSMA/CA protocols with
K-Exponential Backoff scheduling algorithms. A multi-queue single-server system
is proposed to model multiple access networks. The input buffer of each access
node is modeled as a Geo/G/1 queue, and the service time distribution of
head-of-line packets is derived from the Markov chain of underlying scheduling
algorithm. The main results include the complete analysis of the throughput and
delay distribution, from which we obtained stable regions with respect to the
throughput and bounded mean delay of the Geometric Retransmission and
Exponential Backoff schemes. We show that the throughput stable region of
Geometric Retransmission will vanish as the number of nodes n \rightarrow
\infty; thus, it is inherently unstable for large n. In contrast to Geometric
Retransmission, the throughput stable region of Exponential Backoff can be
obtained for an infinite population. We found that the bounded mean delay
region of Geometric Retransmission remains the same as its throughput stable
region. Besides, the variance of service time of Exponential Backoff can be
unbounded due to the capture effect; thus, its bounded delay region is only a
sub-set of its throughput stable region. Analytical results presented in this
paper are all verified by simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0188</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0188</id><created>2010-05-03</created><authors><author><keyname>Mehta</keyname><forenames>Nishant A.</forenames></author><author><keyname>Gray</keyname><forenames>Alexander G.</forenames></author></authors><title>Generative and Latent Mean Map Kernels</title><categories>cs.LG stat.ML</categories><comments>16 pages, 1 figure, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce two kernels that extend the mean map, which embeds probability
measures in Hilbert spaces. The generative mean map kernel (GMMK) is a smooth
similarity measure between probabilistic models. The latent mean map kernel
(LMMK) generalizes the non-iid formulation of Hilbert space embeddings of
empirical distributions in order to incorporate latent variable models. When
comparing certain classes of distributions, the GMMK exhibits beneficial
regularization and generalization properties not shown for previous generative
kernels. We present experiments comparing support vector machine performance
using the GMMK and LMMK between hidden Markov models to the performance of
other methods on discrete and continuous observation sequence data. The results
suggest that, in many cases, the GMMK has generalization error competitive with
or better than other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0198</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0198</id><created>2010-05-03</created><authors><author><keyname>Jerbi</keyname><forenames>Houssem</forenames><affiliation>IRIT</affiliation></author><author><keyname>Pujolle</keyname><forenames>Genevi&#xe8;ve</forenames><affiliation>IRIT</affiliation></author><author><keyname>Ravat</keyname><forenames>Franck</forenames><affiliation>IRIT</affiliation></author><author><keyname>Teste</keyname><forenames>Olivier</forenames><affiliation>IRIT</affiliation></author></authors><title>Personnalisation de Syst\`emes OLAP Annot\'es</title><categories>cs.DB</categories><proxy>ccsd</proxy><journal-ref>XXVIII\`eme Congr\`es Informatique des Organisations et Syst\`emes
  d'Information et de D\'ecision - INFORSID'10, Marseille : France (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with personalization of annotated OLAP systems. Data
constellation is extended to support annotations and user preferences.
Annotations reflect the decision-maker experience whereas user preferences
enable users to focus on the most interesting data. User preferences allow
annotated contextual recommendations helping the decision-maker during his/her
multidimensional navigations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0201</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0201</id><created>2010-05-03</created><authors><author><keyname>Ravat</keyname><forenames>Franck</forenames><affiliation>IRIT</affiliation></author><author><keyname>Teste</keyname><forenames>Olivier</forenames><affiliation>IRIT</affiliation></author><author><keyname>Zurfluh</keyname><forenames>Gilles</forenames><affiliation>IRIT</affiliation></author></authors><title>Personnalisation de bases de donn\'ees multidimensionnelles</title><categories>cs.DB</categories><proxy>ccsd</proxy><journal-ref>Congr\`es Informatique des Organisations et Syst\`emes
  d'Information et de D\'ecision - INFORSID'07, Perros-Guirec : France (2007)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with decision support systems resting on multidimensional
modelling of data. Moreover, we intend to offer a set of concepts and
mechanisms for personalized multidimensional database specifications. This
personalization consists in associating weights to different components of a
multidimensional schema. Personalization specifications are specified through
the use of a language based on the principle of Event Condition Action. This
personalisation determines multidimensional data display as well as their
analyses (with the use of drilling or rotating operations).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0202</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0202</id><created>2010-05-03</created><authors><author><keyname>Rosenblum</keyname><forenames>Kevin</forenames></author><author><keyname>Zelnik-Manor</keyname><forenames>Lihi</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Dictionary Optimization for Block-Sparse Representations</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has demonstrated that using a carefully designed dictionary
instead of a predefined one, can improve the sparsity in jointly representing a
class of signals. This has motivated the derivation of learning methods for
designing a dictionary which leads to the sparsest representation for a given
set of signals. In some applications, the signals of interest can have further
structure, so that they can be well approximated by a union of a small number
of subspaces (e.g., face recognition and motion segmentation). This implies the
existence of a dictionary which enables block-sparse representations of the
input signals once its atoms are properly sorted into blocks. In this paper, we
propose an algorithm for learning a block-sparsifying dictionary of a given set
of signals. We do not require prior knowledge on the association of signals
into groups (subspaces). Instead, we develop a method that automatically
detects the underlying block structure. This is achieved by iteratively
alternating between updating the block structure of the dictionary and updating
the dictionary atoms to better fit the data. Our experiments show that for
block-sparse data the proposed algorithm significantly improves the dictionary
recovery ability and lowers the representation error compared to dictionary
learning methods that do not employ block structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0212</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0212</id><created>2010-05-03</created><authors><author><keyname>Bret</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>IRIT</affiliation></author><author><keyname>Teste</keyname><forenames>Olivier</forenames><affiliation>IRIT</affiliation></author></authors><title>Construction graphique d'entrep\^ots et de magasins de donn\'ees</title><categories>cs.DB</categories><proxy>ccsd</proxy><journal-ref>Congr\`es INFormatique des ORganisations et Syst\`emes
  d'Information et de D\'ecision - INFORSID'99, La Garde : France (1999)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, decisional systems have became a significant research topic in
databases. Data warehouses and data marts are the main elements of such
systems. This paper presents our decisional support system. We present
graphical interfaces which help the administrator to build data warehouses and
data marts. We present a data warehouse building interface based on an
object-oriented conceptual model. This model allows the warehouse data
historisation at three levels: attribute, class and environment. Also, we
present a data mart building interface which allows warehouse data to be
reorganised through a multidimensional object-oriented model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0213</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0213</id><created>2010-05-03</created><authors><author><keyname>Ravat</keyname><forenames>Franck</forenames><affiliation>IRIT</affiliation></author><author><keyname>Teste</keyname><forenames>Olivier</forenames><affiliation>IRIT</affiliation></author><author><keyname>Zurfluh</keyname><forenames>Gilles</forenames><affiliation>IRIT</affiliation></author></authors><title>Alg\`ebre OLAP et langage graphique</title><categories>cs.DB</categories><proxy>ccsd</proxy><journal-ref>Congr\`es Informatique des Organisations et Syst\`emes
  d'Information et de D\'ecision - INFORSID'06, Hammamet : Tunisie (2006)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article deals with OLAP systems based on multidimensional model. The
conceptual model we provide, represents data through a constellation
(multi-facts) composed of several multi-hierarchy dimensions. In this model,
data are displayed through multidimensional tables. We define a query algebra
handling these tables. This user oriented algebra is composed of a closure core
of OLAP operators as soon as advanced operators dedicated to complex analysis.
Finally, we specify a graphical OLAP language based on this algebra. This
language facilitates analyses of decision makers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0214</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0214</id><created>2010-05-03</created><authors><author><keyname>Ravat</keyname><forenames>Franck</forenames><affiliation>IRIT</affiliation></author><author><keyname>Teste</keyname><forenames>Olivier</forenames><affiliation>IRIT</affiliation></author><author><keyname>Gilles</keyname><forenames>Zurfluh</forenames><affiliation>IRIT</affiliation></author></authors><title>Mod\'elisation et extraction de donn\'ees pour un entrep\^ot objet</title><categories>cs.DB</categories><proxy>ccsd</proxy><journal-ref>Bases de donn\'ees avanc\'ees (BDA 2000), Blois : France (2000)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes an object-oriented model for designing complex and
time-variant data warehouse data. The main contribution is the warehouse class
concept, which extends the class concept by temporal and archive filters as
well as a mapping function. Filters allow the keeping of relevant data changes
whereas the mapping function defines the warehouse class schema from a global
data source schema. The approach take into account static properties as well as
dynamic properties. The behaviour extraction is based on the use-matrix
concept.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0217</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0217</id><created>2010-05-03</created><authors><author><keyname>Hubert</keyname><forenames>Gilles</forenames><affiliation>IRIT</affiliation></author><author><keyname>Teste</keyname><forenames>Olivier</forenames><affiliation>IRIT</affiliation></author></authors><title>Analyse multigraduelle OLAP</title><categories>cs.DB</categories><proxy>ccsd</proxy><journal-ref>Revue des nouvelles technologies - RNTI, E-15 (2009) 253-258</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decisional systems are based on multidimensional databases improving OLAP
analyses. The paper describes a new OLAP operator named &quot;BLEND&quot; to perform
multigradual analyses. The operation transforms multidimensional structures
during querying in order to analyse measures according to various granularity
levels, which are reorganised into a single parameter. We study valid
combinations of the operation in the context of strict hierarchies. First
experimentations implement the operation in an R-OLAP framework showing the
slight cost of this operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0218</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0218</id><created>2010-05-03</created><authors><author><keyname>Ghozzi</keyname><forenames>Faiza</forenames><affiliation>IRIT</affiliation></author><author><keyname>Ravat</keyname><forenames>Franck</forenames><affiliation>IRIT</affiliation></author><author><keyname>Teste</keyname><forenames>Olivier</forenames><affiliation>IRIT</affiliation></author><author><keyname>Zurfluh</keyname><forenames>Gilles</forenames><affiliation>IRIT</affiliation></author></authors><title>Contraintes pour mod\`ele et langage multidimensionnels</title><categories>cs.DB</categories><proxy>ccsd</proxy><journal-ref>Bases de donn\'ees avanc\'ees 2003, Lyon : France (2003)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper defines a constraint-based model dedicated to multidimensional
databases. The model we define represents data through a constellation of facts
(subjects of analyse) associated to dimensions (axis of analyse), which are
possibly shared. Each dimension is organised according to several hierarchies
(views of analyse) integrating several levels of data granularity. In order to
insure data consistency, we introduce 5 semantic constraints (exclusion,
inclusion, partition, simultaneity, totality) which can be intra-dimension or
inter-dimensions; the intra-dimension constraints allow the expression of
constraints between hierarchies within a same dimension whereas the
inter-dimensions constraints focus on hierarchies of distinct dimensions. We
also study repercussions of these constraints on multidimensional manipulations
and we provide extensions of the multidimensional operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0219</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0219</id><created>2010-05-03</created><authors><author><keyname>Ravat</keyname><forenames>Franck</forenames><affiliation>IRIT</affiliation></author><author><keyname>Teste</keyname><forenames>Olivier</forenames><affiliation>IRIT</affiliation></author></authors><title>Mod\'elisation et manipulation de donn\'ees historis\'ees et archiv\'ees
  dans un entrep\^ot orient\'e objet</title><categories>cs.DB</categories><proxy>ccsd</proxy><journal-ref>Bases de donn\'ees avanc\'ees 2001, Agadir : Maroc (2001)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with temporal and archive object-oriented data warehouse
modelling and querying. In a first step, we define a data model describing
warehouses as central repositories of complex and temporal data extracted from
one information source. The model is based on the concepts of warehouse object
and environment. A warehouse object is composed of one current state, several
past states (modelling value changes) and several archive states (summarising
some value changes). An environment defines temporal parts in a warehouse
schema according to a relevant granularity (attribute, class or graph). In a
second step, we provide a query algebra dedicated to data warehouses. This
algebra, which is based on common object algebras, integrates temporal
operators and operators for querying object states. An other important
contribution concerns dedicated operators allowing users to transform warehouse
objects in temporal series as well as operators facilitating analytical
treatments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0220</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0220</id><created>2010-05-03</created><authors><author><keyname>Teste</keyname><forenames>Olivier</forenames><affiliation>IRIT</affiliation></author></authors><title>Elaboration d'entrep\^ots de donn\'ees complexes</title><categories>cs.DB</categories><proxy>ccsd</proxy><journal-ref>congr\`es INFormatique des ORganisations et Syst\`emes
  d'Information et de D\'ecision - INFORSID'00, Lyon : France (2000)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the data warehouse modelling used in decision support
systems. We provide an object-oriented data warehouse model allowing data
warehouse description as a central repository of relevant, complex and temporal
data. Our model integrates three concepts such as warehouse object, environment
and warehouse class. Each warehouse object is composed of one current state,
several past states (modelling its detailed evolutions) and several archive
states (modelling its evolutions within a summarised form). The environment
concept defines temporal parts in the data warehouse schema with significant
granularities (attribute, class, graph). Finally, we provide five functions
aiming at defining the data warehouse structures and two functions allowing the
warehouse class inheritance hierarchy organisation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0224</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0224</id><created>2010-05-03</created><authors><author><keyname>Teste</keyname><forenames>Olivier</forenames><affiliation>IRIT</affiliation></author></authors><title>Towards Conceptual Multidimensional Design in Decision Support Systems</title><categories>cs.DB</categories><proxy>ccsd</proxy><journal-ref>5th East-European Conference on Advances in Databases and
  Information Systems - ADBIS'01, Vilnius : Lithuania (2001)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multidimensional databases support efficiently on-line analytical processing
(OLAP). In this paper, we depict a model dedicated to multidimensional
databases. The approach we present designs decisional information through a
constellation of facts and dimensions. Each dimension is possibly shared
between several facts and it is organised according to multiple hierarchies. In
addition, we define a comprehensive query algebra regrouping the more popular
multidimensional operations in current commercial systems and research
approaches. We introduce new operators dedicated to a constellation. Finally,
we describe a prototype that allows managers to query constellations of facts,
dimensions and multiple hierarchies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0239</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0239</id><created>2010-05-03</created><authors><author><keyname>Campagna</keyname><forenames>Andrea</forenames></author><author><keyname>Pagh</keyname><forenames>Rasmus</forenames></author></authors><title>On Finding Frequent Patterns in Directed Acyclic Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a directed acyclic graph with labeled vertices, we consider the problem
of finding the most common label sequences (&quot;traces&quot;) among all paths in the
graph (of some maximum length m). Since the number of paths can be huge, we
propose novel algorithms whose time complexity depends only on the size of the
graph, and on the relative frequency epsilon of the most frequent traces. In
addition, we apply techniques from streaming algorithms to achieve space usage
that depends only on epsilon, and not on the number of distinct traces. The
abstract problem considered models a variety of tasks concerning finding
frequent patterns in event sequences. Our motivation comes from working with a
data set of 2 million RFID readings from baggage trolleys at Copenhagen
Airport. The question of finding frequent passenger movement patterns is mapped
to the above problem. We report on experimental findings for this data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0251</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0251</id><created>2010-05-03</created><updated>2010-12-08</updated><authors><author><keyname>Lee</keyname><forenames>Sang Hoon</forenames></author><author><keyname>Ha</keyname><forenames>Meesoon</forenames></author><author><keyname>Jeon</keyname><forenames>Chanil</forenames></author><author><keyname>Jeong</keyname><forenames>Hawoong</forenames></author></authors><title>Finite-size scaling in random $K$-satisfiability problems</title><categories>cond-mat.stat-mech cs.DS physics.comp-ph</categories><comments>5 pages, 3 figures (6 eps files), 1 table; published version</comments><journal-ref>PRE v82, 061109 (2010)</journal-ref><doi>10.1103/PhysRevE.82.061109</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a comprehensive view of various phase transitions in random
$K$-satisfiability problems solved by stochastic-local-search algorithms. In
particular, we focus on the finite-size scaling (FSS) exponent, which is
mathematically important and practically useful in analyzing finite systems.
Using the FSS theory of nonequilibrium absorbing phase transitions, we show
that the density of unsatisfied clauses clearly indicates the transition from
the solvable (absorbing) phase to the unsolvable (active) phase as varying the
noise parameter and the density of constraints. Based on the solution
clustering (percolation-type) argument, we conjecture two possible values of
the FSS exponent, which are confirmed reasonably well in numerical simulations
for $2\le K \le 3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0253</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0253</id><created>2010-05-03</created><updated>2011-03-03</updated><authors><author><keyname>Ben-Amram</keyname><forenames>Amir M.</forenames><affiliation>Tel-Aviv Academic College</affiliation></author></authors><title>Size-Change Termination, Monotonicity Constraints and Ranking Functions</title><categories>cs.LO</categories><comments>revised version of September 21</comments><proxy>LMCS</proxy><acm-class>D.2.4, F.3.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 6, Issue 3 (July 11,
  2010) lmcs:1001</journal-ref><doi>10.2168/LMCS-6(3:2)2010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Size-Change Termination (SCT) is a method of proving program termination
based on the impossibility of infinite descent. To this end we may use a
program abstraction in which transitions are described by monotonicity
constraints over (abstract) variables. When only constraints of the form x&gt;y'
and x&gt;=y' are allowed, we have size-change graphs. Both theory and practice are
now more evolved in this restricted framework then in the general framework of
monotonicity constraints. This paper shows that it is possible to extend and
adapt some theory from the domain of size-change graphs to the general case,
thus complementing previous work on monotonicity constraints. In particular, we
present precise decision procedures for termination; and we provide a procedure
to construct explicit global ranking functions from monotonicity constraints in
singly-exponential time, which is better than what has been published so far
even for size-change graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0265</identifier>
 <datestamp>2010-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0265</id><created>2010-05-03</created><updated>2010-08-09</updated><authors><author><keyname>Fung</keyname><forenames>Wai Shing</forenames></author><author><keyname>Harvey</keyname><forenames>Nicholas J. A.</forenames></author></authors><title>Graph Sparsification by Edge-Connectivity and Random Spanning Trees</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new approaches to constructing graph sparsifiers --- weighted
subgraphs for which every cut has the same value as the original graph, up to a
factor of $(1 \pm \epsilon)$. Our first approach independently samples each
edge $uv$ with probability inversely proportional to the edge-connectivity
between $u$ and $v$. The fact that this approach produces a sparsifier resolves
a question posed by Bencz\'ur and Karger (2002). Concurrent work of Hariharan
and Panigrahi also resolves this question. Our second approach constructs a
sparsifier by forming the union of several uniformly random spanning trees.
Both of our approaches produce sparsifiers with $O(n \log^2(n)/\epsilon^2)$
edges. Our proofs are based on extensions of Karger's contraction algorithm,
which may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0267</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0267</id><created>2010-05-03</created><authors><author><keyname>Sun</keyname><forenames>Qiyu</forenames></author></authors><title>Recovery of sparsest signals via $\ell^q$-minimization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, it is proved that every $s$-sparse vector ${\bf x}\in {\mathbb
R}^n$ can be exactly recovered from the measurement vector ${\bf z}={\bf A}
{\bf x}\in {\mathbb R}^m$ via some $\ell^q$-minimization with $0&lt; q\le 1$, as
soon as each $s$-sparse vector ${\bf x}\in {\mathbb R}^n$ is uniquely
determined by the measurement ${\bf z}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0268</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0268</id><created>2010-05-03</created><authors><author><keyname>Mirzal</keyname><forenames>Andri</forenames></author><author><keyname>Furukawa</keyname><forenames>Masashi</forenames></author></authors><title>Node-Context Network Clustering using PARAFAC Tensor Decomposition</title><categories>cs.IR</categories><comments>6 pages, 4 figures, International Conference on Information &amp;
  Communication Technology and Systems</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We describe a clustering method for labeled link network (semantic graph)
that can be used to group important nodes (highly connected nodes) with their
relevant link's labels by using PARAFAC tensor decomposition. In this kind of
network, the adjacency matrix can not be used to fully describe all information
about the network structure. We have to expand the matrix into 3-way adjacency
tensor, so that not only the information about to which nodes a node connects
to but by which link's labels is also included. And by applying PARAFAC
decomposition on this tensor, we get two lists, nodes and link's labels with
scores attached to each node and labels, for each decomposition group. So
clustering process to get the important nodes along with their relevant labels
can be done simply by sorting the lists in decreasing order. To test the
method, we construct labeled link network by using blog's dataset, where the
blogs are the nodes and labeled links are the shared words among them. The
similarity measures between the results and standard measures look promising,
especially for two most important tasks, finding the most relevant words to
blogs query and finding the most similar blogs to blogs query, about 0.87.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0291</identifier>
 <datestamp>2011-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0291</id><created>2010-05-03</created><updated>2011-02-01</updated><authors><author><keyname>Wiese</keyname><forenames>Moritz</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Bjelakovi&#x107;</keyname><forenames>Igor</forenames></author><author><keyname>Jungnickel</keyname><forenames>Volker</forenames></author></authors><title>The Compound Multiple Access Channel with Partially Cooperating Encoders</title><categories>cs.IT math.IT</categories><comments>accepted for publication in IEEE Transactions on Information Theory,
  Special Issue on Interference Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to provide a rigorous information-theoretic
analysis of subnetworks of interference networks. We prove two coding theorems
for the compound multiple-access channel with an arbitrary number of channel
states. The channel state information at the transmitters is such that each
transmitter has a finite partition of the set of states and knows which element
of the partition the actual state belongs to. The receiver may have arbitrary
channel state information. The first coding theorem is for the case that both
transmitters have a common message and that each has an additional common
message. The second coding theorem is for the case where rate-constrained, but
noiseless transmitter cooperation is possible. This cooperation may be used to
exchange information about channel state information as well as the messages to
be transmitted. The cooperation protocol used here generalizes Willems'
conferencing. We show how this models base station cooperation in modern
wireless cellular networks used for interference coordination and capacity
enhancement. In particular, the coding theorem for the cooperative case shows
how much cooperation is necessary in order to achieve maximal capacity in the
network considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0330</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0330</id><created>2010-05-03</created><authors><author><keyname>Sankaran</keyname><forenames>Abirami</forenames></author><author><keyname>Samsonyuk</keyname><forenames>Andriy</forenames></author><author><keyname>Attar</keyname><forenames>Maab</forenames></author><author><keyname>Parham</keyname><forenames>Mohammad</forenames></author><author><keyname>Zayikina</keyname><forenames>Olena</forenames></author><author><keyname>Rifai</keyname><forenames>Omar Jandali</forenames></author><author><keyname>Lepin</keyname><forenames>Pavel</forenames></author><author><keyname>Hassan</keyname><forenames>Rana</forenames></author></authors><title>Software Requirements Specification of the IUfA's UUIS -- a Team 1
  COMP5541-W10 Project Approach</title><categories>cs.SE</categories><comments>The 75 page-document has 16 figures, some of which show the link
  between users and their functions, and more than 30 use cases described.</comments><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unified University Inventory System (UUIS), is an inventory system created
for the Imaginary University of Arctica (IUfA) to facilitate its inventory
management, of all the faculties in one system. Team 1 elucidates the functions
of the system and the characteristics of the users who have access to these
functions. It shows the access restrictions to different functionalities of the
system provided to users, who are the staff and students of the University.
Team 1, also, emphasises on the necessary steps required to prevent the
security of the system and its data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0340</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0340</id><created>2010-05-03</created><authors><author><keyname>Tiwana</keyname><forenames>Moazzam Islam</forenames></author><author><keyname>Sayrac</keyname><forenames>Berna</forenames></author><author><keyname>Altman</keyname><forenames>Zwi</forenames></author></authors><title>Statistical Learning in Automated Troubleshooting: Application to LTE
  Interference Mitigation</title><categories>cs.LG</categories><comments>IEEE Transactions On Vehicular Technology 2010 IEEE transactions on
  vehicular technology</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a method for automated healing as part of off-line
automated troubleshooting. The method combines statistical learning with
constraint optimization. The automated healing aims at locally optimizing radio
resource management (RRM) or system parameters of cells with poor performance
in an iterative manner. The statistical learning processes the data using
Logistic Regression (LR) to extract closed form (functional) relations between
Key Performance Indicators (KPIs) and Radio Resource Management (RRM)
parameters. These functional relations are then processed by an optimization
engine which proposes new parameter values. The advantage of the proposed
formulation is the small number of iterations required by the automated healing
method to converge, making it suitable for off-line implementation. The
proposed method is applied to heal an Inter-Cell Interference Coordination
(ICIC) process in a 3G Long Term Evolution (LTE) network which is based on
soft-frequency reuse scheme. Numerical simulations illustrate the benefits of
the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0349</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0349</id><created>2010-05-03</created><authors><author><keyname>Asperti</keyname><forenames>Andrea</forenames></author><author><keyname>Tassi</keyname><forenames>Enrico</forenames></author></authors><title>Smart matching</title><categories>cs.LO</categories><comments>To appear in The 9th International Conference on Mathematical
  Knowledge Management: MKM 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most annoying aspects in the formalization of mathematics is the
need of transforming notions to match a given, existing result. This kind of
transformations, often based on a conspicuous background knowledge in the given
scientific domain (mostly expressed in the form of equalities or isomorphisms),
are usually implicit in the mathematical discourse, and it would be highly
desirable to obtain a similar behavior in interactive provers. The paper
describes the superposition-based implementation of this feature inside the
Matita interactive theorem prover, focusing in particular on the so called
smart application tactic, supporting smart matching between a goal and a given
result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0352</identifier>
 <datestamp>2010-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0352</id><created>2010-05-03</created><authors><author><keyname>Rothenberg</keyname><forenames>Christian Esteve</forenames></author><author><keyname>Macapuna</keyname><forenames>Carlos A. B.</forenames></author><author><keyname>Verdi</keyname><forenames>Fabio L.</forenames></author><author><keyname>Magalhaes</keyname><forenames>Mauricio F.</forenames></author></authors><title>The Deletable Bloom filter: A new member of the Bloom family</title><categories>cs.DS</categories><comments>3 pages, 4 figures. To appear in IEEE Comm Letters</comments><report-no>CL2010-0344</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the Deletable Bloom filter (DlBF) as a new spin on the popular
data structure based on compactly encoding the information of where collisions
happen when inserting elements. The DlBF design enables false-negative-free
deletions at a fraction of the cost in memory consumption, which turns to be
appealing for certain probabilistic filter applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0375</identifier>
 <datestamp>2010-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0375</id><created>2010-05-03</created><updated>2010-11-02</updated><authors><author><keyname>Akin</keyname><forenames>Sami</forenames></author><author><keyname>Gursoy</keyname><forenames>Mustafa Cenk</forenames></author></authors><title>Performance Analysis of Cognitive Radio Systems under QoS Constraints
  and Channel Uncertainty</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, performance of cognitive transmission over time-selective flat
fading channels is studied under quality of service (QoS) constraints and
channel uncertainty. Cognitive secondary users (SUs) are assumed to initially
perform channel sensing to detect the activities of the primary users, and then
attempt to estimate the channel fading coefficients through training. Energy
detection is employed for channel sensing, and different minimum
mean-square-error (MMSE) estimation methods are considered for channel
estimation. In both channel sensing and estimation, erroneous decisions can be
made, and hence, channel uncertainty is not completely eliminated. In this
setting, performance is studied and interactions between channel sensing and
estimation are investigated.
  Following the channel sensing and estimation tasks, SUs engage in data
transmission. Transmitter, being unaware of the channel fading coefficients, is
assumed to send the data at fixed power and rate levels that depend on the
channel sensing results. Under these assumptions, a state-transition model is
constructed by considering the reliability of the transmissions, channel
sensing decisions and their correctness, and the evolution of primary user
activity which is modeled as a two-state Markov process. In the data
transmission phase, an average power constraint on the secondary users is
considered to limit the interference to the primary users, and statistical
limitations on the buffer lengths are imposed to take into account the QoS
constraints of the secondary traffic. The maximum throughput under these
statistical QoS constraints is identified by finding the effective capacity of
the cognitive radio channel. Numerical results are provided for the power and
rate policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0390</identifier>
 <datestamp>2010-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0390</id><created>2010-05-03</created><updated>2010-06-01</updated><authors><author><keyname>Gauci</keyname><forenames>Adam</forenames></author><author><keyname>Adami</keyname><forenames>Kristian Zarb</forenames></author><author><keyname>Abela</keyname><forenames>John</forenames></author></authors><title>Machine Learning for Galaxy Morphology Classification</title><categories>astro-ph.GA cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, decision tree learning algorithms and fuzzy inferencing systems
are applied for galaxy morphology classification. In particular, the CART, the
C4.5, the Random Forest and fuzzy logic algorithms are studied and reliable
classifiers are developed to distinguish between spiral galaxies, elliptical
galaxies or star/unknown galactic objects. Morphology information for the
training and testing datasets is obtained from the Galaxy Zoo project while the
corresponding photometric and spectra parameters are downloaded from the SDSS
DR7 catalogue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0404</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0404</id><created>2010-05-03</created><authors><author><keyname>Mohajer</keyname><forenames>Soheil</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas N.</forenames></author><author><keyname>Fragouli</keyname><forenames>Christina</forenames></author><author><keyname>Tse</keyname><forenames>David N. C.</forenames></author></authors><title>Approximate Capacity of Gaussian Interference-Relay Networks with Weak
  Cross Links</title><categories>cs.IT math.IT</categories><comments>66 pages, 19 figures, submitted to IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study a Gaussian relay-interference network, in which relay
(helper) nodes are to facilitate competing information flows over a wireless
network. We focus on a two-stage relay-interference network where there are
weak cross-links, causing the networks to behave like a chain of Z Gaussian
channels. For these Gaussian ZZ and ZS networks, we establish an approximate
characterization of the rate region. The outer bounds to the capacity region
are established using genie-aided techniques that yield bounds sharper than the
traditional cut-set outer bounds. For the inner bound of the ZZ network, we
propose a new interference management scheme, termed interference
neutralization, which is implemented using structured lattice codes. This
technique allows for over-the-air interference removal, without the
transmitters having complete access the interfering signals. For both the ZZ
and ZS networks, we establish a new network decomposition technique that
(approximately) achieves the capacity region. We use insights gained from an
exact characterization of the corresponding linear deterministic version of the
problems, in order to establish the approximate characterization for Gaussian
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0416</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0416</id><created>2010-05-03</created><authors><author><keyname>Karaman</keyname><forenames>Sertac</forenames></author><author><keyname>Frazzoli</keyname><forenames>Emilio</forenames></author></authors><title>Incremental Sampling-based Algorithms for Optimal Motion Planning</title><categories>cs.RO</categories><comments>20 pages, 10 figures, this manuscript is submitted to the
  International Journal of Robotics Research, a short version is to appear at
  the 2010 Robotics: Science and Systems Conference.</comments><msc-class>68T40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the last decade, incremental sampling-based motion planning
algorithms, such as the Rapidly-exploring Random Trees (RRTs) have been shown
to work well in practice and to possess theoretical guarantees such as
probabilistic completeness. However, no theoretical bounds on the quality of
the solution obtained by these algorithms have been established so far. The
first contribution of this paper is a negative result: it is proven that, under
mild technical conditions, the cost of the best path in the RRT converges
almost surely to a non-optimal value. Second, a new algorithm is considered,
called the Rapidly-exploring Random Graph (RRG), and it is shown that the cost
of the best path in the RRG converges to the optimum almost surely. Third, a
tree version of RRG is introduced, called the RRT$^*$ algorithm, which
preserves the asymptotic optimality of RRG while maintaining a tree structure
like RRT. The analysis of the new algorithms hinges on novel connections
between sampling-based motion planning algorithms and the theory of random
geometric graphs. In terms of computational complexity, it is shown that the
number of simple operations required by both the RRG and RRT$^*$ algorithms is
asymptotically within a constant factor of that required by RRT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0418</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0418</id><created>2010-05-03</created><authors><author><keyname>Panigrahy</keyname><forenames>Rina</forenames></author><author><keyname>Talwar</keyname><forenames>Kunal</forenames></author><author><keyname>Wieder</keyname><forenames>Udi</forenames></author></authors><title>Lower Bounds on Near Neighbor Search via Metric Expansion</title><categories>cs.DS cs.CG</categories><comments>29 pages</comments><acm-class>F.2.2; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show how the complexity of performing nearest neighbor (NNS)
search on a metric space is related to the expansion of the metric space. Given
a metric space we look at the graph obtained by connecting every pair of points
within a certain distance $r$ . We then look at various notions of expansion in
this graph relating them to the cell probe complexity of NNS for randomized and
deterministic, exact and approximate algorithms. For example if the graph has
node expansion $\Phi$ then we show that any deterministic $t$-probe data
structure for $n$ points must use space $S$ where $(St/n)^t &gt; \Phi$. We show
similar results for randomized algorithms as well. These relationships can be
used to derive most of the known lower bounds in the well known metric spaces
such as $l_1$, $l_2$, $l_\infty$ by simply computing their expansion. In the
process, we strengthen and generalize our previous results (FOCS 2008).
Additionally, we unify the approach in that work and the communication
complexity based approach. Our work reduces the problem of proving cell probe
lower bounds of near neighbor search to computing the appropriate expansion
parameter. In our results, as in all previous results, the dependence on $t$ is
weak; that is, the bound drops exponentially in $t$. We show a much stronger
(tight) time-space tradeoff for the class of dynamic low contention data
structures. These are data structures that supports updates in the data set and
that do not look up any single cell too often.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0419</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0419</id><created>2010-05-03</created><authors><author><keyname>Ekrem</keyname><forenames>Ersen</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>Capacity-Equivocation Region of the Gaussian MIMO Wiretap Channel</title><categories>cs.IT cs.CR math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, April 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Gaussian multiple-input multiple-output (MIMO) wiretap channel,
which consists of a transmitter, a legitimate user, and an eavesdropper. In
this channel, the transmitter sends a common message to both the legitimate
user and the eavesdropper. In addition to this common message, the legitimate
user receives a private message, which is desired to be kept hidden as much as
possible from the eavesdropper. We obtain the entire capacity-equivocation
region of the Gaussian MIMO wiretap channel. This region contains all
achievable common message, private message, and private message's equivocation
(secrecy) rates. In particular, we show the sufficiency of jointly Gaussian
auxiliary random variables and channel input to evaluate the existing
single-letter description of the capacity-equivocation region due to
Csiszar-Korner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0426</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0426</id><created>2010-05-03</created><authors><author><keyname>Dikaliotis</keyname><forenames>Theodoros K.</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author><author><keyname>Ho</keyname><forenames>Tracey</forenames></author></authors><title>Security in Distributed Storage Systems by Communicating a Logarithmic
  Number of Bits</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of maintaining an encoded distributed storage
system when some nodes contain adversarial errors. Using the error-correction
capabilities that are built into the existing redundancy of the system, we
propose a simple linear hashing scheme to detect errors in the storage nodes.
Our main result is that for storing a data object of total size $\size$ using
an $(n,k)$ MDS code over a finite field $\F_q$, up to
$t_1=\lfloor(n-k)/2\rfloor$ errors can be detected, with probability of failure
smaller than $1/ \size$, by communicating only $O(n(n-k)\log \size)$ bits to a
trusted verifier. Our result constructs small projections of the data that
preserve the errors with high probability and builds on a pseudorandom
generator that fools linear functions. The transmission rate achieved by our
scheme is asymptotically equal to the min-cut capacity between the source and
any receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0437</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0437</id><created>2010-05-04</created><authors><author><keyname>Kloft</keyname><forenames>Marius</forenames></author><author><keyname>R&#xfc;ckert</keyname><forenames>Ulrich</forenames></author><author><keyname>Bartlett</keyname><forenames>Peter L.</forenames></author></authors><title>A Unifying View of Multiple Kernel Learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research on multiple kernel learning has lead to a number of
approaches for combining kernels in regularized risk minimization. The proposed
approaches include different formulations of objectives and varying
regularization strategies. In this paper we present a unifying general
optimization criterion for multiple kernel learning and show how existing
formulations are subsumed as special cases. We also derive the criterion's dual
representation, which is suitable for general smooth optimization algorithms.
Finally, we evaluate multiple kernel learning in this framework analytically
using a Rademacher complexity bound on the generalization error and empirically
in a set of experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0484</identifier>
 <datestamp>2014-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0484</id><created>2010-05-04</created><authors><author><keyname>Bucheli</keyname><forenames>Samuel</forenames></author><author><keyname>Kuznets</keyname><forenames>Roman</forenames></author><author><keyname>Studer</keyname><forenames>Thomas</forenames></author></authors><title>Explicit Evidence Systems with Common Knowledge</title><categories>cs.LO math.LO</categories><msc-class>03B42, 03B45, 03B70</msc-class><acm-class>F.4.1, I.2.4, I.2.11</acm-class><doi>10.3166/JANCL.21.35-60</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Justification logics are epistemic logics that explicitly include
justifications for the agents' knowledge. We develop a multi-agent
justification logic with evidence terms for individual agents as well as for
common knowledge. We define a Kripke-style semantics that is similar to
Fitting's semantics for the Logic of Proofs LP. We show the soundness,
completeness, and finite model property of our multi-agent justification logic
with respect to this Kripke-style semantics. We demonstrate that our logic is a
conservative extension of Yavorskaya's minimal bimodal explicit evidence logic,
which is a two-agent version of LP. We discuss the relationship of our logic to
the multi-agent modal logic S4 with common knowledge. Finally, we give a brief
analysis of the coordinated attack problem in the newly developed language of
our logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0488</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0488</id><created>2010-05-04</created><authors><author><keyname>Li</keyname><forenames>Shasha</forenames></author><author><keyname>Li</keyname><forenames>Xueliang</forenames></author></authors><title>Hardness results on generalized connectivity</title><categories>math.CO cs.DM</categories><comments>10 pages</comments><msc-class>05C40, 05C05, 68Q25, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a nontrivial connected graph of order $n$ and let $k$ be an
integer with $2\leq k\leq n$. For a set $S$ of $k$ vertices of $G$, let $\kappa
(S)$ denote the maximum number $\ell$ of edge-disjoint trees
$T_1,T_2,...,T_\ell$ in $G$ such that $V(T_i)\cap V(T_j)=S$ for every pair
$i,j$ of distinct integers with $1\leq i,j\leq \ell$. A collection
$\{T_1,T_2,...,T_\ell\}$ of trees in $G$ with this property is called an
internally disjoint set of trees connecting $S$. Chartrand et al. generalized
the concept of connectivity as follows: The $k$-$connectivity$, denoted by
$\kappa_k(G)$, of $G$ is defined by $\kappa_k(G)=$min$\{\kappa(S)\}$, where the
minimum is taken over all $k$-subsets $S$ of $V(G)$. Thus
$\kappa_2(G)=\kappa(G)$, where $\kappa(G)$ is the connectivity of $G$, for
which there are polynomial-time algorithms to solve it. This paper mainly focus
on the complexity of the generalized connectivity. At first, we obtain that for
two fixed positive integers $k_1$ and $k_2$, given a graph $G$ and a
$k_1$-subset $S$ of $V(G)$, the problem of deciding whether $G$ contains $k_2$
internally disjoint trees connecting $S$ can be solved by a polynomial-time
algorithm. Then, we show that when $k_1$ is a fixed integer of at least 4, but
$k_2$ is not a fixed integer, the problem turns out to be NP-complete. On the
other hand, when $k_2$ is a fixed integer of at least 2, but $k_1$ is not a
fixed integer, we show that the problem also becomes NP-complete. Finally we
give some open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0498</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0498</id><created>2010-05-04</created><authors><author><keyname>Tirza</keyname><forenames>Routtenberg</forenames></author><author><keyname>Tabrikian</keyname><forenames>Joseph</forenames></author></authors><title>Classes of lower bounds on outage error probability and MSE in Bayesian
  parameter estimation</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Trans. Inform. Theory (Journal paper)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, new classes of lower bounds on the outage error probability
and on the mean-square-error (MSE) in Bayesian parameter estimation are
proposed. The minima of the h-outage error probability and the MSE are obtained
by the generalized maximum a-posteriori probability and the minimum MSE (MMSE)
estimators, respectively. However, computation of these estimators and their
corresponding performance is usually not tractable and thus, lower bounds on
these terms can be very useful for performance analysis. The proposed class of
lower bounds on the outage error probability is derived using Holder's
inequality. This class is utilized to derive a new class of Bayesian MSE
bounds. It is shown that for unimodal symmetric conditional probability density
functions (pdf) the tightest probability of outage error lower bound in the
proposed class attains the minimum probability of outage error and the tightest
MSE bound coincides with the MMSE performance. In addition, it is shown that
the proposed MSE bounds are always tighter than the Ziv-Zakai lower bound
(ZZLB). The proposed bounds are compared with other existing performance lower
bounds via some examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0503</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0503</id><created>2010-05-04</created><authors><author><keyname>Bojanczyk</keyname><forenames>Adam W.</forenames></author><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author><author><keyname>de Hoog</keyname><forenames>Frank R.</forenames></author></authors><title>A weakly stable algorithm for general Toeplitz systems</title><categories>math.NA cs.NA</categories><comments>17 pages. An old Technical Report with postscript added. For further
  details, see http://wwwmaths.anu.edu.au/~brent/pub/pub143.html</comments><report-no>Technical Report TR-CS-93-15, Computer Sciences Laboratory,
  Australian National University, August 1993 (revised June 1994).</report-no><msc-class>65F05 (Primary) 15B05, 65G50 (Secondary)</msc-class><acm-class>F.2.1</acm-class><journal-ref>Stability analysis of a general Toeplitz system solver, Numerical
  Algorithms 10 (1995), 225-244.</journal-ref><doi>10.1007/BF02140770</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a fast algorithm for the QR factorization of a Toeplitz or
Hankel matrix A is weakly stable in the sense that R^T.R is close to A^T.A.
Thus, when the algorithm is used to solve the semi-normal equations R^T.Rx =
A^Tb, we obtain a weakly stable method for the solution of a nonsingular
Toeplitz or Hankel linear system Ax = b. The algorithm also applies to the
solution of the full-rank Toeplitz or Hankel least squares problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0505</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0505</id><created>2010-05-04</created><authors><author><keyname>Dartois</keyname><forenames>Luc</forenames></author><author><keyname>Kufleitner</keyname><forenames>Manfred</forenames></author><author><keyname>Lauser</keyname><forenames>Alexander</forenames></author></authors><title>Rankers over Infinite Words</title><categories>cs.FL cs.LO</categories><comments>To be presented at the 14th Int. Conference on Developments in
  Language Theory (DLT 2010).</comments><report-no>TR no. 2010/01, University of Stuttgart, Computer Science</report-no><msc-class>68Q45</msc-class><acm-class>F.4.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the four fragments FO2, the intersection of Sigma2 and FO2, the
intersection of Pi2 and FO2, and Delta2 of first-order logic FO[&lt;] over finite
and infinite words. For all four fragments, we give characterizations in terms
of rankers. In particular, we generalize the notion of a ranker to infinite
words in two possible ways. Both extensions are natural in the sense that over
finite words, they coincide with classical rankers and over infinite words,
they both have the full expressive power of FO2. Moreover, the first extension
of rankers admits a characterization of the intersection of Sigma2 and FO2
while the other leads to a characterization of the intersection of Pi2 and FO2.
Both versions of rankers yield characterizations of the fragment Delta2. As a
byproduct, we also obtain characterizations based on unambiguous temporal logic
and unambiguous interval temporal logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0512</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0512</id><created>2010-05-04</created><authors><author><keyname>Kasher</keyname><forenames>Roy</forenames></author><author><keyname>Kempe</keyname><forenames>Julia</forenames></author></authors><title>Two-Source Extractors Secure Against Quantum Adversaries</title><categories>quant-ph cs.CC</categories><comments>20 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate the study of multi-source extractors in the quantum world. In
this setting, our goal is to extract random bits from two independent weak
random sources, on which two quantum adversaries store a bounded amount of
information. Our main result is a two-source extractor secure against quantum
adversaries, with parameters closely matching the classical case and tight in
several instances. Moreover, the extractor is secure even if the adversaries
share entanglement. The construction is the Chor-Goldreich [CG88] two-source
inner product extractor and its multi-bit variant by Dodis et al. [DEOR04].
Previously, research in this area focused on the construction of seeded
extractors secure against quantum adversaries; the multi-source setting poses
new challenges, among which is the presence of entanglement that could
potentially break the independence of the sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0513</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0513</id><created>2010-05-04</created><authors><author><keyname>Cs&#xf3;ka</keyname><forenames>Endre</forenames></author></authors><title>Maximum flow is approximable by deterministic constant-time algorithm in
  sparse networks</title><categories>cs.DS</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show a deterministic constant-time parallel algorithm for finding an
almost maximum flow in multisource-multitarget networks with bounded degrees
and bounded edge capacities. As a consequence, we show that the value of the
maximum flow over the number of nodes is a testable parameter on these
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0518</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0518</id><created>2010-05-04</created><authors><author><keyname>Ben-Amram</keyname><forenames>Amir M.</forenames></author></authors><title>On Decidable Growth-Rate Properties of Imperative Programs</title><categories>cs.LO cs.PL</categories><proxy>EPTCS</proxy><acm-class>D.2.4; F.2.0; F.3.1</acm-class><journal-ref>EPTCS 23, 2010, pp. 1-14</journal-ref><doi>10.4204/EPTCS.23.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2008, Ben-Amram, Jones and Kristiansen showed that for a simple &quot;core&quot;
programming language - an imperative language with bounded loops, and
arithmetics limited to addition and multiplication - it was possible to decide
precisely whether a program had certain growth-rate properties, namely
polynomial (or linear) bounds on computed values, or on the running time.
  This work emphasized the role of the core language in mitigating the
notorious undecidability of program properties, so that one deals with
decidable problems.
  A natural and intriguing problem was whether more elements can be added to
the core language, improving its utility, while keeping the growth-rate
properties decidable. In particular, the method presented could not handle a
command that resets a variable to zero. This paper shows how to handle resets.
The analysis is given in a logical style (proof rules), and its complexity is
shown to be PSPACE-complete (in contrast, without resets, the problem was
PTIME). The analysis algorithm evolved from the previous solution in an
interesting way: focus was shifted from proving a bound to disproving it, and
the algorithm works top-down rather than bottom-up.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0519</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0519</id><created>2010-05-04</created><authors><author><keyname>Bonfante</keyname><forenames>Guillaume</forenames></author></authors><title>Observation of implicit complexity by non confluence</title><categories>cs.CC cs.PL</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 23, 2010, pp. 15-29</journal-ref><doi>10.4204/EPTCS.23.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to consider non confluence with respect to implicit complexity. We
come back to some well known classes of first-order functional program, for
which we have a characterization of their intentional properties, namely the
class of cons-free programs, the class of programs with an interpretation, and
the class of programs with a quasi-interpretation together with a termination
proof by the product path ordering. They all correspond to PTIME. We prove that
adding non confluence to the rules leads to respectively PTIME, NPTIME and
PSPACE. Our thesis is that the separation of the classes is actually a witness
of the intentional properties of the initial classes of programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0521</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0521</id><created>2010-05-04</created><authors><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames></author><author><keyname>Martini</keyname><forenames>Simone</forenames></author><author><keyname>Zorzi</keyname><forenames>Margherita</forenames></author></authors><title>General Ramified Recurrence is Sound for Polynomial Time</title><categories>cs.LO cs.CC</categories><proxy>EPTCS</proxy><acm-class>F.4.1; F.1.3</acm-class><journal-ref>EPTCS 23, 2010, pp. 47-62</journal-ref><doi>10.4204/EPTCS.23.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Leivant's ramified recurrence is one of the earliest examples of an implicit
characterization of the polytime functions as a subalgebra of the primitive
recursive functions. Leivant's result, however, is originally stated and proved
only for word algebras, i.e. free algebras whose constructors take at most one
argument. This paper presents an extension of these results to ramified
functions on any free algebras, provided the underlying terms are represented
as graphs rather than trees, so that sharing of identical subterms can be
exploited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0522</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0522</id><created>2010-05-04</created><authors><author><keyname>Roversi</keyname><forenames>Luca</forenames><affiliation>Universit&#xe0; di Torino</affiliation></author><author><keyname>Vercelli</keyname><forenames>Luca</forenames><affiliation>Universit&#xe0; di Torino</affiliation></author></authors><title>Safe Recursion on Notation into a Light Logic by Levels</title><categories>cs.LO cs.CC</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 23, 2010, pp. 63-77</journal-ref><doi>10.4204/EPTCS.23.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We embed Safe Recursion on Notation (SRN) into Light Affine Logic by Levels
(LALL), derived from the logic L4. LALL is an intuitionistic deductive system,
with a polynomial time cut elimination strategy.
  The embedding allows to represent every term t of SRN as a family of proof
nets |t|^l in LALL. Every proof net |t|^l in the family simulates t on
arguments whose bit length is bounded by the integer l. The embedding is based
on two crucial features. One is the recursive type in LALL that encodes Scott
binary numerals, i.e. Scott words, as proof nets. Scott words represent the
arguments of t in place of the more standard Church binary numerals. Also, the
embedding exploits the &quot;fuzzy&quot; borders of paragraph boxes that LALL inherits
from L4 to &quot;freely&quot; duplicate the arguments, especially the safe ones, of t.
Finally, the type of |t|^l depends on the number of composition and recursion
schemes used to define t, namely the structural complexity of t. Moreover, the
size of |t|^l is a polynomial in l, whose degree depends on the structural
complexity of t.
  So, this work makes closer both the predicative recursive theoretic
principles SRN relies on, and the proof theoretic one, called /stratification/,
at the base of Light Linear Logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0523</identifier>
 <datestamp>2010-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0523</id><created>2010-05-04</created><updated>2010-05-12</updated><authors><author><keyname>Chakraborty</keyname><forenames>Sourav</forenames></author><author><keyname>Fischer</keyname><forenames>Eldar</forenames></author><author><keyname>Matsliah</keyname><forenames>Arie</forenames></author><author><keyname>de Wolf</keyname><forenames>Ronald</forenames></author></authors><title>New Results on Quantum Property Testing</title><categories>quant-ph cs.CC</categories><comments>2nd version: updated some references, in particular to Aaronson's
  Fourier checking problem</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present several new examples of speed-ups obtainable by quantum algorithms
in the context of property testing. First, motivated by sampling algorithms, we
consider probability distributions given in the form of an oracle
$f:[n]\to[m]$. Here the probability $\PP_f(j)$ of an outcome $j\in[m]$ is the
fraction of its domain that $f$ maps to $j$. We give quantum algorithms for
testing whether two such distributions are identical or $\epsilon$-far in
$L_1$-norm. Recently, Bravyi, Hassidim, and Harrow \cite{BHH10} showed that if
$\PP_f$ and $\PP_g$ are both unknown (i.e., given by oracles $f$ and $g$), then
this testing can be done in roughly $\sqrt{m}$ quantum queries to the
functions. We consider the case where the second distribution is known, and
show that testing can be done with roughly $m^{1/3}$ quantum queries, which we
prove to be essentially optimal. In contrast, it is known that classical
testing algorithms need about $m^{2/3}$ queries in the unknown-unknown case and
about $\sqrt{m}$ queries in the known-unknown case. Based on this result, we
also reduce the query complexity of graph isomorphism testers with quantum
oracle access. While those examples provide polynomial quantum speed-ups, our
third example gives a much larger improvement (constant quantum queries vs
polynomial classical queries) for the problem of testing periodicity, based on
Shor's algorithm and a modification of a classical lower bound by Lachish and
Newman \cite{lachish&amp;newman:periodicity}. This provides an alternative to a
recent constant-vs-polynomial speed-up due to Aaronson \cite{aaronson:bqpph}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0524</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0524</id><created>2010-05-04</created><authors><author><keyname>Brunel</keyname><forenames>Alo&#xef;s</forenames><affiliation>ENS Lyon</affiliation></author><author><keyname>Terui</keyname><forenames>Kazushige</forenames><affiliation>RIMS, Kyoto University</affiliation></author></authors><title>Church =&gt; Scott = Ptime: an application of resource sensitive
  realizability</title><categories>cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 23, 2010, pp. 31-46</journal-ref><doi>10.4204/EPTCS.23.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a variant of linear logic with second order quantifiers and type
fixpoints, both restricted to purely linear formulas. The Church encodings of
binary words are typed by a standard non-linear type `Church,' while the Scott
encodings (purely linear representations of words) are by a linear type
`Scott.' We give a characterization of polynomial time functions, which is
derived from (Leivant and Marion 93): a function is computable in polynomial
time if and only if it can be represented by a term of type Church =&gt; Scott.
  To prove soundness, we employ a resource sensitive realizability technique
developed by Hofmann and Dal Lago.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0527</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0527</id><created>2010-05-03</created><updated>2010-05-05</updated><authors><author><keyname>Koroutchev</keyname><forenames>Kostadin</forenames></author><author><keyname>Korutcheva</keyname><forenames>Elka</forenames></author></authors><title>Detecting the Most Unusual Part of Two and Three-dimensional Digital
  Images</title><categories>physics.data-an cs.CV physics.med-ph</categories><comments>16 pages</comments><journal-ref>Pattern Recognition 42(8): 1684-1692 (2009)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to introduce an algorithm that can detect the
most unusual part of a digital image in probabilistic setting. The most unusual
part of a given shape is defined as a part of the image that has the maximal
distance to all non intersecting shapes with the same form. The method is
tested on two and three-dimensional images and has shown very good results
without any predefined model. A version of the method independent of the
contrast of the image is considered and is found to be useful for finding the
most unusual part (and the most similar part) of the image conditioned on given
image. The results can be used to scan large image databases, as for example
medical databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0530</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0530</id><created>2010-05-04</created><authors><author><keyname>Shah</keyname><forenames>Mohak</forenames></author><author><keyname>Marchand</keyname><forenames>Mario</forenames></author><author><keyname>Corbeil</keyname><forenames>Jacques</forenames></author></authors><title>Feature Selection with Conjunctions of Decision Stumps and Learning from
  Microarray Data</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the objectives of designing feature selection learning algorithms is
to obtain classifiers that depend on a small number of attributes and have
verifiable future performance guarantees. There are few, if any, approaches
that successfully address the two goals simultaneously. Performance guarantees
become crucial for tasks such as microarray data analysis due to very small
sample sizes resulting in limited empirical evaluation. To the best of our
knowledge, such algorithms that give theoretical bounds on the future
performance have not been proposed so far in the context of the classification
of gene expression data. In this work, we investigate the premise of learning a
conjunction (or disjunction) of decision stumps in Occam's Razor, Sample
Compression, and PAC-Bayes learning settings for identifying a small subset of
attributes that can be used to perform reliable classification tasks. We apply
the proposed approaches for gene identification from DNA microarray data and
compare our results to those of well known successful approaches proposed for
the task. We show that our algorithm not only finds hypotheses with much
smaller number of genes while giving competitive classification accuracy but
also have tight risk guarantees on future performance unlike other approaches.
The proposed approaches are general and extensible in terms of both designing
novel algorithms and application to other domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0545</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0545</id><created>2010-05-04</created><authors><author><keyname>Behboodi</keyname><forenames>Arash</forenames></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames></author></authors><title>Capacity of a Class of Broadcast Relay Channels</title><categories>cs.IT math.IT</categories><comments>5 pages, to appear in proc. IEEE ISIT, June 2010</comments><msc-class>94A15, 62B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the broadcast relay channel (BRC) which consists of a source sending
information over a two user broadcast
  channel in presence of two relay nodes that help the transmission to the
destinations. Clearly, this network with
  five nodes involves all the problems encountered in relay and broadcast
channels. New inner bounds on the capacity
  region of this class of channels are derived. These results can be seen as a
generalization and hence unification of
  previous work in this topic. Our bounds are based on the idea of
recombination of message bits and various effective
  coding strategies for relay and broadcast channels. Capacity result is
obtained for the semi-degraded BRC-CR, where
  one relay channel is degraded while the other one is reversely degraded. An
inner and upper bound is also presented
  for the degraded BRC with common relay (BRC-CR), where both the relay and
broadcast channel are degraded which is
  the capacity for the Gaussian case. Application of these results arise in the
context of opportunistic cooperation
  of cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0595</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0595</id><created>2010-05-04</created><authors><author><keyname>Sankaran</keyname><forenames>Abirami</forenames></author><author><keyname>Samsonyuk</keyname><forenames>Andriy</forenames></author><author><keyname>Attar</keyname><forenames>Maab</forenames></author><author><keyname>Parham</keyname><forenames>Mohammad</forenames></author><author><keyname>Zayikina</keyname><forenames>Olena</forenames></author><author><keyname>Rifai</keyname><forenames>Omar Jandali</forenames></author><author><keyname>Lepin</keyname><forenames>Pavel</forenames></author><author><keyname>Hassan</keyname><forenames>Rana</forenames></author></authors><title>Software Design Document, Testing, and Deployment and Configuration
  Management of the UUIS - a Team 1 COMP5541-W10 Project Approach</title><categories>cs.SE</categories><comments>The document has 67 figures and 24 tables in about 136 pages,
  including the test documents and configuration details as supplements</comments><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The document presents a detailed description of the designs for the
implementation of the Unified University Inventory System for the Imaginary
University of Arctica. The document, through numerous diagrams and UI samples,
gives the structure of the system and the functions of its modules. It also
gives test cases and reports that support the system's architecture and design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0600</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0600</id><created>2010-05-04</created><authors><author><keyname>Kauers</keyname><forenames>Manuel</forenames></author><author><keyname>Pillwein</keyname><forenames>Veronika</forenames></author></authors><title>When can we decide that a P-finite sequence is positive?</title><categories>cs.SC</categories><acm-class>I.1.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two algorithms which can be used for proving positivity of
sequences that are defined by a linear recurrence equation with polynomial
coefficients (P-finite sequences). Both algorithms have in common that while
they do succeed on a great many examples, there is no guarantee for them to
terminate, and they do in fact not terminate for every input. For some
restricted classes of P-finite recurrence equations of order up to three we
provide a priori criteria that assert the termination of the algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0602</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0602</id><created>2010-05-04</created><authors><author><keyname>Kauers</keyname><forenames>Manuel</forenames></author><author><keyname>Schneider</keyname><forenames>Carsten</forenames></author></authors><title>Partial Denominator Bounds for Partial Linear Difference Equations</title><categories>cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate which polynomials can possibly occur as factors in the
denominators of rational solutions of a given partial linear difference
equation (PLDE). Two kinds of polynomials are to be distinguished, we call them
/periodic/ and /aperiodic/. The main result is a generalization of a well-known
denominator bounding technique for univariate equations to PLDEs. This
generalization is able to find all the aperiodic factors of the denominators
for a given PLDE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0605</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0605</id><created>2010-04-26</created><authors><author><keyname>Gavrikov</keyname><forenames>Vladimir L.</forenames></author><author><keyname>Khlebopros</keyname><forenames>Rem G.</forenames></author></authors><title>An approach to visualize the course of solving of a research task in
  humans</title><categories>cs.AI</categories><comments>20 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A technique to study the dynamics of solving of a research task is suggested.
The research task was based on specially developed software Right- Wrong
Responder (RWR), with the participants having to reveal the response logic of
the program. The participants interacted with the program in the form of a
semi-binary dialogue, which implies the feedback responses of only two kinds -
&quot;right&quot; or &quot;wrong&quot;. The technique has been applied to a small pilot group of
volunteer participants. Some of them have successfully solved the task
(solvers) and some have not (non-solvers). In the beginning of the work, the
solvers did more wrong moves than non-solvers, and they did less wrong moves
closer to the finish of the work. A phase portrait of the work both in solvers
and non-solvers showed definite cycles that may correspond to sequences of
partially true hypotheses that may be formulated by the participants during the
solving of the task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0608</identifier>
 <datestamp>2010-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0608</id><created>2010-05-04</created><authors><author><keyname>Ammon</keyname><forenames>Kurt</forenames></author></authors><title>Informal Concepts in Machines</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper constructively proves the existence of an effective procedure
generating a computable (total) function that is not contained in any given
effectively enumerable set of such functions. The proof implies the existence
of machines that process informal concepts such as computable (total) functions
beyond the limits of any given Turing machine or formal system, that is, these
machines can, in a certain sense, &quot;compute&quot; function values beyond these
limits. We call these machines creative. We argue that any &quot;intelligent&quot;
machine should be capable of processing informal concepts such as computable
(total) functions, that is, it should be creative. Finally, we introduce
hypotheses on creative machines which were developed on the basis of
theoretical investigations and experiments with computer programs. The
hypotheses say that machine intelligence is the execution of a self-developing
procedure starting from any universal programming language and any input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0609</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0609</id><created>2010-05-04</created><authors><author><keyname>Daoudi</keyname><forenames>Ahmed</forenames></author><author><keyname>Zerkler</keyname><forenames>David</forenames></author><author><keyname>Hazan</keyname><forenames>Gay</forenames></author><author><keyname>Toutant</keyname><forenames>Isabelle</forenames></author><author><keyname>Diaz</keyname><forenames>Mariano</forenames></author><author><keyname>Toutant</keyname><forenames>Rene</forenames></author><author><keyname>Cook</keyname><forenames>Virginia</forenames></author><author><keyname>Nzoukou</keyname><forenames>William</forenames></author><author><keyname>Amaiche</keyname><forenames>Yassine</forenames></author></authors><title>Software Requirements Specification of the IUfA's UUIS -- a Team 3
  COMP5541-W10 Project Approach</title><categories>cs.SE</categories><comments>86 pages, 24 figures</comments><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this document is to specify the requirements of the University
Unified Inventory System, of the UIfA. The Team of Analysts used a Feedback
Waterfall approach to collect the requirements. UML diagrams, such as Use case
diagrams, Block Diagrams, Domain Models, and interface prototypes are some of
the tools employed to develop the present document.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0616</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0616</id><created>2010-05-04</created><updated>2012-10-18</updated><authors><author><keyname>Burnashev</keyname><forenames>Marat</forenames></author><author><keyname>Tchamkerten</keyname><forenames>Aslan</forenames></author></authors><title>Tracking a Random Walk First-Passage Time Through Noisy Observations</title><categories>math.ST cs.IT math.IT stat.TH</categories><comments>Reprint of the original article published in the Annals of Applied
  Probability</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a Gaussian random walk (or a Wiener process), possibly with drift,
observed through noise, we consider the problem of estimating its first-passage
time $\tau_\ell$ of a given level $\ell$ with a stopping time $\eta$ defined
over the noisy observation process.
  Main results are upper and lower bounds on the minimum mean absolute
deviation $\inf_\eta \ex|\eta-\tau_\ell|$ which become tight as
$\ell\to\infty$. Interestingly, in this regime the estimation error does not
get smaller if we allow $ \eta$ to be an arbitrary function of the entire
observation process, not necessarily a stopping time.
  In the particular case where there is no drift, we show that it is impossible
to track $\tau_\ell$: $\inf_\eta \ex|\eta-\tau_\ell|^p=\infty$ for any $\ell&gt;0$
and $p\geq1/2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0624</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0624</id><created>2010-05-04</created><authors><author><keyname>He</keyname><forenames>Xiang</forenames></author><author><keyname>Yener</keyname><forenames>Aylin</forenames></author></authors><title>The Gaussian Many-to-1 Interference Channel with Confidential Messages</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, April 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The many-to-one interference channel has received interest by virtue of
embodying the essence of an interference network while being more tractable
than the general K-user interference channel. In this paper, we introduce
information theoretic secrecy to this model and consider the many-to-one
interference channel with confidential messages, in which each receiver, in
particular, the one subject to interference, is also one from which the
interfering users' messages need to be kept secret from. We derive the
achievable secrecy sum rate for this channel using nested lattice codes, as
well as an upper bound on the secrecy sum rate for all possible channel gain
configurations. We identify several nontrivial cases where the gap between the
upper bound and the achieved secrecy sum rate is only a function of the number
of the users K, and is uniform over all possible channel gain configurations in
each case. In addition, we identify the secure degree of freedom for this
channel and show it to be equivalent to its degree of freedom, i.e., the
secrecy in high SNR comes for free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0644</identifier>
 <datestamp>2014-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0644</id><created>2010-05-04</created><updated>2014-05-09</updated><authors><author><keyname>Drucker</keyname><forenames>Andrew</forenames></author></authors><title>Improved Direct Product Theorems for Randomized Query Complexity</title><categories>cs.CC</categories><comments>Updated to essentially match journal version (which includes minor
  fixes, comparison with previous interactive DPTs, improved writing)</comments><journal-ref>Computational Complexity 21(2), pp 197-244, 2012 (SP Birkh\&quot;auser
  Verlag Basel)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The direct product problem is a fundamental question in complexity theory
which seeks to understand how the difficulty of computing a function on each of
k independent inputs scales with k. We prove the following direct product
theorem (DPT) for query complexity: if every T-query algorithm has success
probability at most 1 - eps in computing the Boolean function f on input
distribution Mu, then for alpha &lt;= 1, every (alpha eps Tk)-query algorithm has
success probability at most (2^{alpha eps}(1 - eps))^k in computing the k-fold
direct product f^k correctly on k independent inputs from Mu. In light of
examples due to Shaltiel, this statement gives an essentially optimal tradeoff
between the query bound and the error probability. As a corollary, we show that
for an absolute constant alpha &gt; 0, the worst-case success probability of any
(alpha R_2(f)k)-query randomized algorithm for f^k falls exponentially with k.
The best previous statement of this type, due to Klauck, Spalek, and de Wolf,
required a query bound of O(bs(f)k). The proof involves defining and analyzing
a collection of martingales associated with an algorithm attempting to solve
f^k. Our method is quite general and yields a new XOR lemma and threshold DPT
for the query model, as well as DPTs for the query complexity of learning
tasks, search problems, and tasks involving interaction with dyamic entities.
We also give a version of our DPT in which decision tree size is the resource
of interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0653</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0653</id><created>2010-05-04</created><authors><author><keyname>Baillot</keyname><forenames>Patrick</forenames><affiliation>ENS Lyon</affiliation></author></authors><title>Proceedings International Workshop on Developments in Implicit
  Computational complExity</title><categories>cs.LO cs.CC cs.PL</categories><proxy>EPTCS</proxy><acm-class>F.3.2; F.3.3; F.4.1</acm-class><journal-ref>EPTCS 23, 2010</journal-ref><doi>10.4204/EPTCS.23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the International Workshop on
Developments in Implicit Computational complExity (DICE 2010), which took place
on March 27-28 2010 in Paphos, Cyprus, as a satellite event of the Joint
European Conference on Theory and Practice of Software, ETAPS 2010.
  Implicit Computational Complexity aims at studying computational complexity
without referring to external measuring conditions or particular machine
models, but instead by considering restrictions on programming languages or
logical principles implying complexity properties. The aim of this workshop was
to bring together researchers working on implicit computational complexity,
from its logical and semantical aspects to those related to the static analysis
of programs, so as to foster their interaction and to give newcomers an
overview of the current trends in this area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0657</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0657</id><created>2010-05-04</created><updated>2010-05-06</updated><authors><author><keyname>Gallego</keyname><forenames>Alexander</forenames></author><author><keyname>Saxena</keyname><forenames>Nitesh</forenames></author><author><keyname>Voris</keyname><forenames>Jonathan</forenames></author></authors><title>Security Through Entertainment: Experiences Using a Memory Game for
  Secure Device Pairing</title><categories>cs.CR cs.HC</categories><comments>This paper is 11 pages with 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The secure &quot;pairing&quot; of wireless devices based on auxiliary or out-of-band
(OOB) communication, such as audio, visual, or tactile channels, is a
well-established research direction. However, prior work shows that this
approach to pairing can be prone to human errors of different forms that may
directly or indirectly translate into man-in-the-middle attacks. To address
this problem, we propose a general direction of the use of computer games for
pairing. Since games are a popular means of entertainment, our hypothesis is
that they may serve as an incentive to users and make the pairing process
enjoyable for them, thus improving the usability, as well as the security, of
the pairing process. We consider an emerging use case of pairing whereby two
different users are involved, each in possession of his or her own device
(e.g., Alice and Bob pairing their smartphones for social interactions). We
develop &quot;Alice Says,&quot; a pairing game based on a popular memory game called
Simon (Says), and discuss the underlying design challenges. We also present a
preliminary evaluation of Alice Says via a usability study and demonstrate its
feasibility in terms of usability and security. Our results indicate that
overall Alice Says was deemed as a fun and an enjoyable way to pair devices,
confirming our hypothesis. However, contrary to our intuition, the relatively
slower speed of Alice Says pairing was found to be a cause of concern and
prompts the need for the design of faster pairing games. We put forth several
ways in which this issue can be ameliorated. In addition, we also discuss
several other security problems which are lacking optimal solutions and suggest
ideas on how entertainment can be used to improve the current state of the art
solutions that have been developed to address them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0662</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0662</id><created>2010-05-04</created><authors><author><keyname>Golovin</keyname><forenames>Daniel</forenames></author></authors><title>The B-Skip-List: A Simpler Uniquely Represented Alternative to B-Trees</title><categories>cs.DS cs.DB</categories><comments>13 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work, the author introduced the B-treap, a uniquely represented
B-tree analogue, and proved strong performance guarantees for it. However, the
B-treap maintains complex invariants and is very complex to implement. In this
paper we introduce the B-skip-list, which has most of the guarantees of the
B-treap, but is vastly simpler and easier to implement. Like the B-treap, the
B-skip-list may be used to construct strongly history-independent index
structures and filesystems; such constructions reveal no information about the
historical sequence of operations that led to the current logical state. For
example, a uniquely represented filesystem would support the deletion of a file
in a way that, in a strong information-theoretic sense, provably removes all
evidence that the file ever existed. Like the B-tree, the B-skip-list has depth
O(log_B (n)) where B is the block transfer size of the external memory, uses
linear space with high probability, and supports efficient one-dimensional
range queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0665</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0665</id><created>2010-05-05</created><updated>2010-05-07</updated><authors><author><keyname>Ahmad</keyname><forenames>Omer Shahid</forenames><affiliation>Jun-Duo</affiliation></author><author><keyname>Alrashdi</keyname><forenames>Faisal</forenames><affiliation>Jun-Duo</affiliation></author><author><keyname>Jason</keyname><affiliation>Jun-Duo</affiliation></author><author><keyname>Chen</keyname></author><author><keyname>Ilham</keyname><forenames>Najah</forenames></author><author><keyname>Lu</keyname><forenames>Jianhai</forenames></author><author><keyname>Sun</keyname><forenames>Yiwei</forenames></author><author><keyname>Wang</keyname><forenames>Tong</forenames></author><author><keyname>Zhu</keyname><forenames>Yongxin</forenames></author></authors><title>Software Design Document, Testing, Deployment and Configuration
  Management of the UUIS--a Team 2 COMP5541-W10 Project Approach</title><categories>cs.SE</categories><comments>161 pages, 46 figures</comments><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Software Design Document of UUIS describes the prototype design details
of the system architecture, database layer, deployment and configuration
details as well as test cases produced while working the design and
implementation of the prototype. The requirements specification of UUIS are
detailed in arXiv:1005.0783.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0667</identifier>
 <datestamp>2010-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0667</id><created>2010-05-05</created><authors><author><keyname>Sweet</keyname><forenames>Douglas R.</forenames></author><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author></authors><title>Error analysis of a partial pivoting method for structured matrices</title><categories>math.NA cs.NA</categories><comments>18 pages. An old Technical Report, submitted for archival purposes.
  For further details see http://wwwmaths.anu.edu.au/~brent/pub/pub157.html</comments><report-no>Technical Report TR-CS-95-03, Computer Sciences Laboratory,
  Australian National University, June 1995.</report-no><msc-class>65F05 (Primary) 15B05, 65G50 (Secondary)</msc-class><acm-class>F.2.1</acm-class><journal-ref>Error analysis of a fast partial pivoting method for structured
  matrices, Proceedings SPIE, Volume 2363, Advanced Signal Processing
  Algorithms, 1995, 266-280.</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many matrices that arise in the solution of signal processing problems have a
special displacement structure. For example, adaptive filtering and
direction-of-arrival estimation yield matrices of Toeplitz type. A recent
method of Gohberg, Kailath and Olshevsky (GKO) allows fast Gaussian elimination
with partial pivoting for such structured matrices. In this paper, a rounding
error analysis is performed on the Cauchy and Toeplitz variants of the GKO
method. It is shown the error growth depends on the growth in certain auxiliary
vectors, the generators, which are computed by the GKO algorithms. It is also
shown that in certain circumstances, the growth in the generators can be large,
and so the error growth is much larger than would be encountered with normal
Gaussian elimination with partial pivoting. A modification of the algorithm to
perform a type of row-column pivoting is proposed; it may ameliorate this
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0670</identifier>
 <datestamp>2010-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0670</id><created>2010-05-05</created><authors><author><keyname>Hariharan</keyname><forenames>Ramesh</forenames></author><author><keyname>Panigrahi</keyname><forenames>Debmalya</forenames></author></authors><title>A Linear-time Algorithm for Sparsification of Unweighted Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph $G$ and an error parameter $\epsilon &gt; 0$, the {\em
graph sparsification} problem requires sampling edges in $G$ and giving the
sampled edges appropriate weights to obtain a sparse graph $G_{\epsilon}$ with
the following property: the weight of every cut in $G_{\epsilon}$ is within a
factor of $(1\pm \epsilon)$ of the weight of the corresponding cut in $G$. If
$G$ is unweighted, an $O(m\log n)$-time algorithm for constructing
$G_{\epsilon}$ with $O(n\log n/\epsilon^2)$ edges in expectation, and an
$O(m)$-time algorithm for constructing $G_{\epsilon}$ with $O(n\log^2
n/\epsilon^2)$ edges in expectation have recently been developed
(Hariharan-Panigrahi, 2010). In this paper, we improve these results by giving
an $O(m)$-time algorithm for constructing $G_{\epsilon}$ with $O(n\log
n/\epsilon^2)$ edges in expectation, for unweighted graphs. Our algorithm is
optimal in terms of its time complexity; further, no efficient algorithm is
known for constructing a sparser $G_{\epsilon}$. Our algorithm is Monte-Carlo,
i.e. it produces the correct output with high probability, as are all efficient
graph sparsification algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0671</identifier>
 <datestamp>2010-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0671</id><created>2010-05-05</created><authors><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author></authors><title>Stability of fast algorithms for structured linear systems</title><categories>math.NA cs.NA</categories><comments>13 pages. An old Technical Report (CSL, ANU, September 1997, 13
  pages), submitted for archival purposes. For further details see
  http://wwwmaths.anu.edu.au/~brent/pub/pub177.html</comments><report-no>TR-CS-97-18</report-no><msc-class>65F05 (Primary), 15B05, 65F30, 65G50 (secondary)</msc-class><acm-class>F.2.1</acm-class><journal-ref>&quot;Fast Reliable Algorithms for Matrices with Structure&quot; (edited by
  Ali Sayed and Thomas Kailath), SIAM, Philadelphia, 1999, 103-116</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey the numerical stability of some fast algorithms for solving systems
of linear equations and linear least squares problems with a low
displacement-rank structure. For example, the matrices involved may be Toeplitz
or Hankel. We consider algorithms which incorporate pivoting without destroying
the structure, and describe some recent results on the stability of these
algorithms. We also compare these results with the corresponding stability
results for the well known algorithms of Schur/Bareiss and Levinson, and for
algorithms based on the semi-normal equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0675</identifier>
 <datestamp>2010-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0675</id><created>2010-05-05</created><authors><author><keyname>Fekete</keyname><forenames>S&#xe1;ndor P.</forenames></author><author><keyname>Schmidt</keyname><forenames>Christiane</forenames></author><author><keyname>Wegener</keyname><forenames>Axel</forenames></author><author><keyname>Hellbr&#xfc;ck</keyname><forenames>Horst</forenames></author><author><keyname>Fischer</keyname><forenames>Stefan</forenames></author></authors><title>Empowered by Wireless Communication: Self-Organizing Traffic Collectives</title><categories>cs.NI cs.DS</categories><comments>28 pages, 9 figures; to appear in ACM Transactions on Autonomous and
  Adaptive Systems (TAAS)</comments><acm-class>C.2.4; E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, tremendous progress has been made in understanding the
dynamics of vehicle traffic flow and traffic congestion by interpreting traffic
as a multi-particle system. This helps to explain the onset and persistence of
many undesired phenomena, e.g., traffic jams. It also reflects the apparent
helplessness of drivers in traffic, who feel like passive particles that are
pushed around by exterior forces; one of the crucial aspects is the inability
to communicate and coordinate with other traffic participants. We present
distributed methods for solving these fundamental problems, employing modern
wireless, ad-hoc, multi-hop networks. The underlying idea is to use these
capabilities as the basis for self-organizing methods for coordinating data
collection and processing, recognizing traffic phenomena, and changing their
structure by coordinated behavior. The overall objective is a multi-level
approach that reaches from protocols for local wireless communication, data
dissemination, pattern recognition, over hierarchical structuring and
coordinated behavior, all the way to large-scale traffic regulation. In this
article we describe three types of results: (i) self-organizing and distributed
methods for maintaining and collecting data (using our concept of Hovering Data
Clouds); (ii) adaptive data dissemination for traffic information systems;
(iii) methods for self-recognition of traffic jams. We conclude by describing
higher-level aspects of our work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0677</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0677</id><created>2010-05-05</created><authors><author><keyname>Alishahi</keyname><forenames>K.</forenames></author><author><keyname>Dashmiz</keyname><forenames>S.</forenames></author><author><keyname>Pad</keyname><forenames>P.</forenames></author><author><keyname>Marvasti</keyname><forenames>F.</forenames></author><author><keyname>Shafinia</keyname><forenames>M. H.</forenames></author><author><keyname>Mansouri</keyname><forenames>M.</forenames></author></authors><title>The Enigma of CDMA Revisited</title><categories>cs.IT math.IT</categories><comments>71 pages, submitted to IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore the mystery of synchronous CDMA as applied to
wireless and optical communication systems under very general settings for the
user symbols and the signature matrix entries. The channel is modeled with
real/complex additive noise of arbitrary distribution. Two problems are
addressed. The first problem concerns whether overloaded error free codes exist
in the absence of additive noise under these general settings, and if so
whether there are any practical optimum decoding algorithms. The second one is
about the bounds for the sum channel capacity when user data and signature
codes employ any real or complex alphabets (finite or infinite). In response to
the first problem, we have developed practical Maximum Likelihood (ML) decoding
algorithms for overloaded CDMA systems for a large class of alphabets. In
response to the second problem, a general theorem has been developed in which
the sum capacity lower bounds with respect to the number of users and spreading
gain and Signal-to-Noise Ratio (SNR) can be derived as special cases for a
given CDMA system. To show the power and utility of the main theorem, a number
of sum capacity bounds for special cases are simulated. An important conclusion
of this paper is that the lower and upper bounds of the sum capacity for
small/medium size CDMA systems depend on both the input and the signature
symbols; this is contrary to the asymptotic results for large scale systems
reported in the literature (also confirmed in this paper) where the signature
symbols and statistics disappear in the asymptotic sum capacity. Moreover,
these questions are investigated for the case when not all users are active.
Furthermore, upper and asymptotic bounds are derived and numerically evaluated
and compared to other derivations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0693</identifier>
 <datestamp>2010-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0693</id><created>2010-05-05</created><authors><author><keyname>Park</keyname><forenames>Jaeok</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Adaptive MAC Protocols Using Memory for Networks with Critical Traffic</title><categories>cs.NI</categories><comments>24 pages, 7 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider wireless communication networks where network users are subject
to critical events such as emergencies and crises. If a critical event occurs
to a user, the user needs to send critical traffic as early as possible.
However, most existing medium access control (MAC) protocols are not adequate
to meet the urgent need for data transmission by users with critical traffic.
In this paper, we devise a class of distributed MAC protocols that achieve
coordination using the finite-length memory of users containing their own
observations and traffic types. We formulate a protocol design problem and find
optimal protocols that solve the problem. We show that the proposed protocols
enable a user with critical traffic to transmit its critical traffic without
interruption from other users after a short delay while allowing users to share
the channel efficiently when there is no critical traffic. Moreover, the
proposed protocols require short memory and can be implemented without explicit
message passing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0697</identifier>
 <datestamp>2010-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0697</id><created>2010-05-05</created><authors><author><keyname>Shahid</keyname><forenames>Mohammad Iqbal Bin</forenames></author><author><keyname>Kamruzzaman</keyname><forenames>Joarder</forenames></author></authors><title>Weighted Soft Decision for Cooperative Sensing in Cognitive Radio
  Networks</title><categories>cs.NI</categories><comments>6 pages, 8 figures, published in proceedings of IEEE International
  Conference on Networks 2008.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enhancing the current services or deploying new services operating in RF
spectrum requires more licensed spectrum which may not be provided by the
regulatory bodies because of spectrum scarcity. On the contrary, recent studies
suggest that many portions of the licensed spectrum remains unused or underused
for significant period of time raising the issue of spectrum access without
license in an opportunistic manner. Among all the spectrum accessing
techniques, sensing based methods are considered optimal for their simplicity
and cost effectiveness. In this paper, we introduce a new cooperative spectrum
sensing technique which considers the spatial variation of secondary
(unlicensed) users and each user's contribution is weighted by a factor that
depends on received power and path loss. Compared to existing techniques, the
proposed one increases the sensing ability and spectrum utilization, and offers
greater robustness to noise uncertainty. Moreover, this cooperative technique
uses very simple energy detector as its building block thereby reduces the cost
and operational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0704</identifier>
 <datestamp>2010-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0704</id><created>2010-05-05</created><authors><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames></author><author><keyname>Guyeux</keyname><forenames>Christophe</forenames></author></authors><title>A chaos-based approach for information hiding security</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new framework for data hiding security. Contrary to
the existing ones, the approach introduced here is not based on probability
theory. In this paper, a scheme is considered as secure if its behavior is
proven unpredictable. The objective of this study is to enrich the existing
notions of data hiding security with a new rigorous and practicable one. This
new definition of security is based on the notion of topological chaos. It
could be used to reinforce the confidence on a scheme previously proven as
secure by other approaches and it could also be used to study some classes of
attacks that currently cannot be studied by the existing security approaches.
After presenting the theoretical framework of the study, a concrete example is
detailed in order to show how our approach can be applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0705</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0705</id><created>2010-05-05</created><updated>2011-01-23</updated><authors><author><keyname>Guyeux</keyname><forenames>Christophe</forenames></author><author><keyname>Friot</keyname><forenames>Nicolas</forenames></author><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames></author></authors><title>Chaotic iterations versus Spread-spectrum: chaos and stego security</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new framework for information hiding security, called chaos-security, has
been proposed in a previous study. It is based on the evaluation of
unpredictability of the scheme, whereas existing notions of security, as
stego-security, are more linked to information leaks. It has been proven that
spread-spectrum techniques, a well-known stego-secure scheme, are chaos-secure
too. In this paper, the links between the two notions of security is deepened
and the usability of chaos-security is clarified, by presenting a novel data
hiding scheme that is twice stego and chaos-secure. This last scheme has better
scores than spread-spectrum when evaluating qualitative and quantitative
chaos-security properties. Incidentally, this result shows that the new
framework for security tends to improve the ability to compare data hiding
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0707</identifier>
 <datestamp>2010-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0707</id><created>2010-05-05</created><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>The Production of Probabilistic Entropy in Structure/Action Contingency
  Relations</title><categories>cs.AI physics.soc-ph</categories><journal-ref>Journal of Social and Evolutionary Systems 18 (1995) 339-356</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Luhmann (1984) defined society as a communication system which is
structurally coupled to, but not an aggregate of, human action systems. The
communication system is then considered as self-organizing (&quot;autopoietic&quot;), as
are human actors. Communication systems can be studied by using Shannon's
(1948) mathematical theory of communication. The update of a network by action
at one of the local nodes is then a well-known problem in artificial
intelligence (Pearl 1988). By combining these various theories, a general
algorithm for probabilistic structure/action contingency can be derived. The
consequences of this contingency for each system, its consequences for their
further histories, and the stabilization on each side by counterbalancing
mechanisms are discussed, in both mathematical and theoretical terms. An
empirical example is elaborated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0712</identifier>
 <datestamp>2013-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0712</id><created>2010-05-05</created><authors><author><keyname>Wilhelm</keyname><forenames>Matthias</forenames></author><author><keyname>Martinovic</keyname><forenames>Ivan</forenames></author><author><keyname>Schmitt</keyname><forenames>Jens B.</forenames></author></authors><title>Key Generation in Wireless Sensor Networks Based on Frequency-selective
  Channels - Design, Implementation, and Analysis</title><categories>cs.CR</categories><comments>Submitted to IEEE Transactions on Dependable and Secure Computing</comments><doi>10.1109/JSAC.2013.130911</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Key management in wireless sensor networks faces several new challenges. The
scale, resource limitations, and new threats such as node capture necessitate
the use of an on-line key generation by the nodes themselves. However, the cost
of such schemes is high since their secrecy is based on computational
complexity. Recently, several research contributions justified that the
wireless channel itself can be used to generate information-theoretic secure
keys. By exchanging sampling messages during movement, a bit string can be
derived that is only known to the involved entities. Yet, movement is not the
only possibility to generate randomness. The channel response is also strongly
dependent on the frequency of the transmitted signal. In our work, we introduce
a protocol for key generation based on the frequency-selectivity of channel
fading. The practical advantage of this approach is that we do not require node
movement. Thus, the frequent case of a sensor network with static motes is
supported. Furthermore, the error correction property of the protocol mitigates
the effects of measurement errors and other temporal effects, giving rise to an
agreement rate of over 97%. We show the applicability of our protocol by
implementing it on MICAz motes, and evaluate its robustness and secrecy through
experiments and analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0732</identifier>
 <datestamp>2010-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0732</id><created>2010-05-05</created><authors><author><keyname>Hadzi-Velkov</keyname><forenames>Zoran</forenames></author><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author></authors><title>Outage rates and outage durations of opportunistic relaying systems</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Communications Letters, vol. 14, no. 2, pp 148-150, February
  2010</journal-ref><doi>10.1109/LCOMM.2010.02.091683</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Opportunistic relaying is a simple yet efficient cooperation scheme that
achieves full diversity and preserves the spectral efficiency among the
spatially distributed stations. However, the stations' mobility causes temporal
correlation of the system's capacity outage events, which gives rise to its
important second-order outage statistical parameters, such as the average
outage rate (AOR) and the average outage duration (AOD). This letter presents
exact analytical expressions for the AOR and the AOD of an opportunistic
relaying system, which employs a mobile source and a mobile destination
(without a direct path), and an arbitrary number of (fixed-gain
amplify-and-forward or decode-and-forward) mobile relays in Rayleigh fading
environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0734</identifier>
 <datestamp>2010-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0734</id><created>2010-05-05</created><authors><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Hadzi-Velkov</keyname><forenames>Zoran</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George</forenames></author></authors><title>An efficient approximation to the correlated Nakagami-m sums and its
  application in equal gain diversity receivers</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Wireless Communications, vol. 9, no. 1, pp.
  302-310, January 2010</journal-ref><doi>10.1109/TWC.2010.01.090457</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  There are several cases in wireless communications theory where the
statistics of the sum of independent or correlated Nakagami-m random variables
(RVs) is necessary to be known. However, a closed-form solution to the
distribution of this sum does not exist when the number of constituent RVs
exceeds two, even for the special case of Rayleigh fading. In this paper, we
present an efficient closed-form approximation for the distribution of the sum
of arbitrary correlated Nakagami-m envelopes with identical and integer fading
parameters. The distribution becomes exact for maximal correlation, while the
tightness of the proposed approximation is validated statistically by using the
Chi-square and the Kolmogorov-Smirnov goodness-of-fit tests. As an application,
the approximation is used to study the performance of equal-gain combining
(EGC) systems operating over arbitrary correlated Nakagami-m fading channels,
by utilizing the available analytical results for the error-rate performance of
an equivalent maximal-ratio combining (MRC) system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0737</identifier>
 <datestamp>2010-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0737</id><created>2010-05-05</created><authors><author><keyname>Baudet</keyname><forenames>Mathieu</forenames></author><author><keyname>Cortier</keyname><forenames>V&#xe9;ronique</forenames></author><author><keyname>Delaune</keyname><forenames>St&#xe9;phanie</forenames></author></authors><title>YAPA: A generic tool for computing intruder knowledge</title><categories>cs.LO cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reasoning about the knowledge of an attacker is a necessary step in many
formal analyses of security protocols. In the framework of the applied pi
calculus, as in similar languages based on equational logics, knowledge is
typically expressed by two relations: deducibility and static equivalence.
Several decision procedures have been proposed for these relations under a
variety of equational theories. However, each theory has its particular
algorithm, and none has been implemented so far. We provide a generic procedure
for deducibility and static equivalence that takes as input any convergent
rewrite system. We show that our algorithm covers most of the existing decision
procedures for convergent theories. We also provide an efficient
implementation, and compare it briefly with the tools ProVerif and KiSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0747</identifier>
 <datestamp>2010-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0747</id><created>2010-05-05</created><authors><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames></author><author><keyname>Mateescu</keyname><forenames>Maria</forenames></author><author><keyname>Mikeev</keyname><forenames>Linar</forenames></author><author><keyname>Wolf</keyname><forenames>Verena</forenames></author></authors><title>Hybrid Numerical Solution of the Chemical Master Equation</title><categories>q-bio.QM cs.NA</categories><comments>10 pages</comments><msc-class>60J28</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a numerical approximation technique for the analysis of
continuous-time Markov chains that describe networks of biochemical reactions
and play an important role in the stochastic modeling of biological systems.
Our approach is based on the construction of a stochastic hybrid model in which
certain discrete random variables of the original Markov chain are approximated
by continuous deterministic variables. We compute the solution of the
stochastic hybrid model using a numerical algorithm that discretizes time and
in each step performs a mutual update of the transient probability distribution
of the discrete stochastic variables and the values of the continuous
deterministic variables. We implemented the algorithm and we demonstrate its
usefulness and efficiency on several case studies from systems biology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0749</identifier>
 <datestamp>2010-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0749</id><created>2010-04-27</created><authors><author><keyname>Heras</keyname><forenames>Jonathan</forenames></author><author><keyname>Pascual</keyname><forenames>Vico</forenames></author><author><keyname>Romero</keyname><forenames>Ana</forenames></author><author><keyname>Rubio</keyname><forenames>Julio</forenames></author></authors><title>Integrating multiple sources to answer questions in Algebraic Topology</title><categories>cs.SC cs.AI cs.HC</categories><comments>To appear in The 9th International Conference on Mathematical
  Knowledge Management: MKM 2010</comments><journal-ref>Lectures Notes in Artificial Intelligence, 6167: 331-335, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present in this paper an evolution of a tool from a user interface for a
concrete Computer Algebra system for Algebraic Topology (the Kenzo system), to
a front-end allowing the interoperability among di?erent sources for
computation and deduction. The architecture allows the system not only to
interface several systems, but also to make them cooperate in shared
calculations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0754</identifier>
 <datestamp>2010-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0754</id><created>2010-04-26</created><authors><author><keyname>Sinha</keyname><forenames>Nirmalendu Bikas</forenames></author><author><keyname>sonal</keyname><forenames>Manish</forenames></author><author><keyname>Snai</keyname><forenames>Makar Chand</forenames></author><author><keyname>Bera</keyname><forenames>R.</forenames></author><author><keyname>Mitra</keyname><forenames>M.</forenames></author></authors><title>Modelling and Implementation of ITWS: An ultimate solution to ITS</title><categories>cs.OH</categories><comments>Nirmalendu Bikas Sinha, Manish sonal, Makar Chand Snai, R. Bera and
  M.Mitra, &quot;Modelling and Implementation of ITWS: An ultimate solution to ITS&quot;,
  Journal of Telecommunications, Volume 2, Issue 1, p17-27, April 2010</comments><journal-ref>Journal of Telecommunications, Volume 2, Issue 1, p17-27, April
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Casualties due to traffic accidents are increasing day by day. Think of this
message being displayed on your computer screen while you were driving &quot;there's
a possibility of collision with a car in the next few minutes if you go on
driving with this speed and direction&quot;. Our research is intended towards
developing collision avoidance architecture for the latest Intelligent
Transport System. The exchange of safety messages among vehicles and with
infrastructure devices poses major challenges. Specially, safety messages have
to be adaptively distributed within a certain range of a basically unbounded
system. These messages are to be well coordinated and processed via different
algorithms. The purpose of the paper is to discuss the ITWS (intelligent
transportation warning system), we have discussed the Assisted Global
Positioning System(AGPS) system providing additional positioning information at
variable conditions. We have also discussed study the Data fusion and kalaman
filter in details. The performance of kalman filter and output are discussed.
Hardware realization of this model is achieved through software defined radio
(SDR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0762</identifier>
 <datestamp>2010-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0762</id><created>2010-05-05</created><authors><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author></authors><title>Note on Computing Ratings from Eigenvectors</title><categories>math.NA cs.NA</categories><comments>10 pages. Dedicated to Gene Golub 1932-2007.</comments><msc-class>65F15 (Primary) 65C40 65H10, 65H17 (Secondary)</msc-class><acm-class>G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing ratings using the results of games
played between a set of n players, and show how this problem can be reduced to
computing the positive eigenvectors corresponding to the dominant eigenvalues
of certain n by n matrices. There is a close connection with the stationary
probability distributions of certain Markov chains. In practice, if n is large,
then the matrices involved will be sparse, and the power method may be used to
solve the eigenvalue problems efficiently. We give an algorithm based on the
power method, and also derive the same algorithm by an independent method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0765</identifier>
 <datestamp>2010-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0765</id><created>2010-05-05</created><authors><author><keyname>Auger</keyname><forenames>David</forenames></author><author><keyname>Charon</keyname><forenames>Ir&#xe8;ne</forenames></author><author><keyname>Hudry</keyname><forenames>Olivier</forenames></author><author><keyname>Lobstein</keyname><forenames>Antoine</forenames></author></authors><title>Watching Systems in graphs: an extension of Identifying Codes</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of watching systems in graphs, which is a
generalization of that of identifying codes. We give some basic properties of
watching systems, an upper bound on the minimum size of a watching system, and
results on the graphs which achieve this bound; we also study the cases of the
paths and cycles, and give complexity results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0766</identifier>
 <datestamp>2011-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0766</id><created>2010-05-05</created><updated>2011-02-12</updated><authors><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Willsky</keyname><forenames>Alan S.</forenames></author></authors><title>Learning High-Dimensional Markov Forest Distributions: Analysis of Error
  Rates</title><categories>cs.IT math.IT stat.ML</categories><comments>Accepted to the Journal of Machine Learning Research (Feb 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of learning forest-structured discrete graphical models from
i.i.d. samples is considered. An algorithm based on pruning of the Chow-Liu
tree through adaptive thresholding is proposed. It is shown that this algorithm
is both structurally consistent and risk consistent and the error probability
of structure learning decays faster than any polynomial in the number of
samples under fixed model size. For the high-dimensional scenario where the
size of the model d and the number of edges k scale with the number of samples
n, sufficient conditions on (n,d,k) are given for the algorithm to satisfy
structural and risk consistencies. In addition, the extremal structures for
learning are identified; we prove that the independent (resp. tree) model is
the hardest (resp. easiest) to learn using the proposed algorithm in terms of
error rates for structure learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0771</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0771</id><created>2010-04-26</created><authors><author><keyname>Touil</keyname><forenames>Lamjed</forenames></author><author><keyname>Abdelali</keyname><forenames>Abdessalem Ben</forenames></author><author><keyname>Mtibaa</keyname><forenames>Abdellatif</forenames></author><author><keyname>Bourennane</keyname><forenames>Elbey</forenames></author></authors><title>Towards Hardware implementation of video applications in new
  telecommunications devices</title><categories>cs.MM</categories><comments>Lamjed Touil, Abdessalem Ben Abdelali, Abdellatif Mibaa and Elbey
  Bourennane, &quot;Towards Hardware implementation of video applications in new
  telecommunications devices&quot;, Journal of Telecommunications, Volume 2, Issue
  1, p75-85, April 2010</comments><journal-ref>Journal of Telecommunications, Volume 2, Issue 1, p75-85, April
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among the areas, most demanding in terms of calculation is the
telecommunication and video applications are now included in several
telecommunication devices such as set-top boxes, mobile phones. Embedded videos
applications in new generations of telecommunication devices need a processing
capacity that can not be achieved by the conventional processor, to work around
this problem the use of programmable technology has a lot of interest. First,
Field Programmable Gate Arrays (FPGAs) present many performance benefits for
real-time image processing applications. The FPGA structure is able to exploit
spatial and temporal parallelism. In this paper, we present a new method for
implementation of the Color Structure Descriptor (CSD) using the FPGA circuit.
In fact the (CSD) provides satisfactory image indexing and retrieval results
among all colorbased descriptors in MPEG-7. But the real time implementation of
this descriptor is still having problems. In this paper we propose a method for
adapting this descriptor for possible implementation under the constraints of
the video processing in real time. We have verified the real-time
implementation of the (CSD) with an image size of 120*80 pixels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0783</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0783</id><created>2010-05-05</created><updated>2010-05-07</updated><authors><author><keyname>Ahmad</keyname><forenames>Omer Shahid</forenames><affiliation>Jun-Duo</affiliation></author><author><keyname>Alrashdi</keyname><forenames>Faisal</forenames><affiliation>Jun-Duo</affiliation></author><author><keyname>Jason</keyname><affiliation>Jun-Duo</affiliation></author><author><keyname>Chen</keyname></author><author><keyname>Ilham</keyname><forenames>Najah</forenames></author><author><keyname>Lu</keyname><forenames>Jianhai</forenames></author><author><keyname>Sun</keyname><forenames>Yiwei</forenames></author><author><keyname>Wang</keyname><forenames>Tong</forenames></author><author><keyname>Zhu</keyname><forenames>Yongxin</forenames></author></authors><title>Software Requirements Specification of the IUfA's UUIS -- a Team 2
  COMP5541-W10 Project Approach</title><categories>cs.SE</categories><comments>52 pages. 51 tables, 4 figures</comments><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the 52-page document, we describe our approach to the Software
Requirements Specification of the IUfA's UUIS prototype. This includes the
overall system description, functional requirements, non-functional
requirements, use cases, the corresponding data dictionary for all entities
involved, mock user interface (UI) design, and the overall projected cost
estimate. The design specification of UUIS can be found in arXiv:1005.0665.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0794</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0794</id><created>2010-05-05</created><authors><author><keyname>Yan</keyname><forenames>Xiaoran</forenames></author><author><keyname>Zhu</keyname><forenames>Yaojia</forenames></author><author><keyname>Rouquier</keyname><forenames>Jean-Baptiste</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author></authors><title>Active Learning for Hidden Attributes in Networks</title><categories>stat.ML cond-mat.stat-mech cs.IT cs.LG math.IT physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many networks, vertices have hidden attributes, or types, that are
correlated with the networks topology. If the topology is known but these
attributes are not, and if learning the attributes is costly, we need a method
for choosing which vertex to query in order to learn as much as possible about
the attributes of the other vertices. We assume the network is generated by a
stochastic block model, but we make no assumptions about its assortativity or
disassortativity. We choose which vertex to query using two methods: 1)
maximizing the mutual information between its attributes and those of the
others (a well-known approach in active learning) and 2) maximizing the average
agreement between two independent samples of the conditional Gibbs
distribution. Experimental results show that both these methods do much better
than simple heuristics. They also consistently identify certain vertices as
important by querying them early on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0806</identifier>
 <datestamp>2010-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0806</id><created>2010-05-05</created><authors><author><keyname>Yoo</keyname><forenames>Andy B.</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Vaidya</keyname><forenames>Sheila</forenames></author><author><keyname>Poole</keyname><forenames>Stephen</forenames></author></authors><title>A New Benchmark For Evaluation Of Graph-Theoretic Algorithms</title><categories>cs.PF</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We propose a new graph-theoretic benchmark in this paper. The benchmark is
developed to address shortcomings of an existing widely-used graph benchmark.
We thoroughly studied a large number of traditional and contemporary graph
algorithms reported in the literature to have clear understanding of their
algorithmic and run-time characteristics. Based on this study, we designed a
suite of kernels, each of which represents a specific class of graph
algorithms. The kernels are designed to capture the typical run-time behavior
of target algorithms accurately, while limiting computational and spatial
overhead to ensure its computation finishes in reasonable time. We expect that
the developed benchmark will serve as a much needed tool for evaluating
different architectures and programming models to run graph algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0809</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0809</id><created>2010-05-05</created><authors><author><keyname>Ganguly</keyname><forenames>Sumit</forenames></author><author><keyname>Kar</keyname><forenames>Purushottam</forenames></author></authors><title>On Estimating the First Frequency Moment of Data Streams</title><categories>cs.DS</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating the first moment of a data stream defined as $F_1 = \sum_{i \in
\{1, 2, \ldots, n\}} \abs{f_i}$ to within $1 \pm \epsilon$-relative error with
high probability is a basic and influential problem in data stream processing.
A tight space bound of $O(\epsilon^{-2} \log (mM))$ is known from the work of
[Kane-Nelson-Woodruff-SODA10]. However, all known algorithms for this problem
require per-update stream processing time of $\Omega(\epsilon^{-2})$, with the
only exception being the algorithm of [Ganguly-Cormode-RANDOM07] that requires
per-update processing time of $O(\log^2(mM)(\log n))$ albeit with sub-optimal
space $O(\epsilon^{-3}\log^2(mM))$. In this paper, we present an algorithm for
estimating $F_1$ that achieves near-optimality in both space and update
processing time. The space requirement is $O(\epsilon^{-2}(\log n + (\log
\epsilon^{-1})\log(mM)))$ and the per-update processing time is $O( (\log
n)\log (\epsilon^{-1}))$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0813</identifier>
 <datestamp>2010-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0813</id><created>2010-04-13</created><authors><author><keyname>Weigel</keyname><forenames>R. S.</forenames></author><author><keyname>Lindholm</keyname><forenames>D. M.</forenames></author><author><keyname>Wilson</keyname><forenames>A.</forenames></author><author><keyname>Faden</keyname><forenames>J.</forenames></author></authors><title>TSDS: high-performance merge, subset, and filter software for time
  series-like data</title><categories>cs.DB</categories><comments>Submitted to Earth Science Informatics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time Series Data Server (TSDS) is a software package for implementing a
server that provides fast super-setting, sub-setting, filtering, and uniform
gridding of time series-like data. TSDS was developed to respond quickly to
requests for long time spans of data. Data may be served from a fast database,
typically created by aggregating granules (e.g., data files) from a remote data
source and storing them in a local cache that is optimized for serving time
series. The system was designed specifically for time series data, and is
optimized for requests where the longest dimension of the requested data
structure is time. Scalar, vector, and spectrogram time series types are
supported. The user can interact with the server by requesting a time series, a
date range, and an optional filter to apply to the data. Available filters
include strides, block average/minimum/maximum, exclude, and inequality.
Constraint expressions are supported, which allow such operations as a request
for data from one time series when a different time series satisfied a
specified relationship. TSDS builds upon DAP (Data Access Protocol), NcML
(netCDF Mark-up language) and related software libraries. In this work, we
describe the current design of this server, as well as planned features and
potential implementation strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0824</identifier>
 <datestamp>2011-11-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0824</id><created>2010-05-05</created><updated>2011-11-14</updated><authors><author><keyname>Boldo</keyname><forenames>Sylvie</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author><author><keyname>Cl&#xe9;ment</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Filli&#xe2;tre</keyname><forenames>Jean-Christophe</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author><author><keyname>Mayero</keyname><forenames>Micaela</forenames><affiliation>LIPN, Inria Grenoble Rh&#xf4;ne-Alpes / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme</affiliation></author><author><keyname>Melquiond</keyname><forenames>Guillaume</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author><author><keyname>Weis</keyname><forenames>Pierre</forenames><affiliation>INRIA Rocquencourt</affiliation></author></authors><title>Formal Proof of a Wave Equation Resolution Scheme: the Method Error</title><categories>cs.LO math.NA</categories><comments>replaces arXiv:1001.4898</comments><proxy>ccsd</proxy><report-no>arXiv:1005.0824</report-no><journal-ref>Interactive Theorem Proving 6172 (2010) 147-162</journal-ref><doi>10.1007/978-3-642-14052-5_12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Popular finite difference numerical schemes for the resolution of the
one-dimensional acoustic wave equation are well-known to be convergent. We
present a comprehensive formalization of the simplest one and formally prove
its convergence in Coq. The main difficulties lie in the proper definition of
asymptotic behaviors and the implicit way they are handled in the mathematical
pen-and-paper proofs. To our knowledge, this is the first time such kind of
mathematical proof is machine-checked.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0826</identifier>
 <datestamp>2013-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0826</id><created>2010-05-05</created><updated>2013-04-30</updated><authors><author><keyname>Ryabko</keyname><forenames>Daniil</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Clustering processes</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>in proceedings of ICML 2010. arXiv-admin Note: This is a newer
  version of the article arXiv:1004.5194v1, please see that article for any
  previous version</comments><proxy>ccsd</proxy><journal-ref>27th International Conference on Machine Learning (2010) 919-926</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of clustering is considered, for the case when each data point is
a sample generated by a stationary ergodic process. We propose a very natural
asymptotic notion of consistency, and show that simple consistent algorithms
exist, under most general non-parametric assumptions. The notion of consistency
is as follows: two samples should be put into the same cluster if and only if
they were generated by the same distribution. With this notion of consistency,
clustering generalizes such classical statistical problems as homogeneity
testing and process classification. We show that, for the case of a known
number of clusters, consistency can be achieved under the only assumption that
the joint distribution of the data is stationary ergodic (no parametric or
Markovian assumptions, no assumptions of independence, neither between nor
within the samples). If the number of clusters is unknown, consistency can be
achieved under appropriate assumptions on the mixing rates of the processes.
(again, no parametric or independence assumptions). In both cases we give
examples of simple (at most quadratic in each argument) algorithms which are
consistent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0830</identifier>
 <datestamp>2010-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0830</id><created>2010-05-05</created><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK</affiliation></author><author><keyname>Gautier</keyname><forenames>Thierry</forenames><affiliation>INRIA Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Roch</keyname><forenames>Jean-Louis</forenames><affiliation>INRIA Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author></authors><title>Generic design of Chinese remaindering schemes</title><categories>cs.SC cs.DC</categories><comments>International Symposium on Parallel Symbolic Computation, Grenoble :
  France (2010)</comments><proxy>ccsd</proxy><doi>10.1145/1837210.1837218</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a generic design for Chinese remainder algorithms. A Chinese
remainder computation consists in reconstructing an integer value from its
residues modulo non coprime integers. We also propose an efficient linear data
structure, a radix ladder, for the intermediate storage and computations. Our
design is structured into three main modules: a black box residue computation
in charge of computing each residue; a Chinese remaindering controller in
charge of launching the computation and of the termination decision; an integer
builder in charge of the reconstruction computation. We then show that this
design enables many different forms of Chinese remaindering (e.g.
deterministic, early terminated, distributed, etc.), easy comparisons between
these forms and e.g. user-transparent parallelism at different parallel grains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0835</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0835</id><created>2010-05-05</created><authors><author><keyname>Amadio</keyname><forenames>Roberto</forenames><affiliation>PPS</affiliation></author><author><keyname>Baillot</keyname><forenames>Patrick</forenames><affiliation>LIP</affiliation></author><author><keyname>Madet</keyname><forenames>Antoine</forenames><affiliation>PPS</affiliation></author></authors><title>An affine-intuitionistic system of types and effects: confluence and
  termination</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an affine-intuitionistic system of types and effects which can be
regarded as an extension of Barber-Plotkin Dual Intuitionistic Linear Logic to
multi-threaded programs with effects. In the system, dynamically generated
values such as references or channels are abstracted into a finite set of
regions. We introduce a discipline of region usage that entails the confluence
(and hence determinacy) of the typable programs. Further, we show that a
discipline of region stratification guarantees termination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0839</identifier>
 <datestamp>2010-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0839</id><created>2010-05-05</created><authors><author><keyname>Armbruster</keyname><forenames>Chris</forenames><affiliation>MPDL</affiliation></author><author><keyname>Romary</keyname><forenames>Laurent</forenames><affiliation>INRIA Saclay - Ile de France, IDSL</affiliation></author></authors><title>Comparing Repository Types - Challenges and barriers for subject-based
  repositories, research repositories, national repository systems and
  institutional repositories in serving scholarly communication</title><categories>cs.DL</categories><proxy>ccsd</proxy><journal-ref>International Journal of Digital Library Systems 1, 4 (2010) 61-73</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After two decades of repository development, some conclusions may be drawn as
to which type of repository and what kind of service best supports digital
scholarly communication, and thus the production of new knowledge. Four types
of publication repository may be distinguished, namely the subject-based
repository, research repository, national repository system and institutional
repository. Two important shifts in the role of repositories may be noted. With
regard to content, a well-defined and high quality corpus is essential. This
implies that repository services are likely to be most successful when
constructed with the user and reader uppermost in mind. With regard to service,
high value to specific scholarly communities is essential. This implies that
repositories are likely to be most useful to scholars when they offer dedicated
services supporting the production of new knowledge. Along these lines,
challenges and barriers to repository development may be identified in three
key dimensions: a) identification and deposit of content; b) access and use of
services; and c) preservation of content and sustainability of service. An
indicative comparison of challenges and barriers in some major world regions
such as Europe, North America and East Asia plus Australia is offered in
conclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0854</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0854</id><created>2010-05-05</created><authors><author><keyname>Amaiche</keyname><forenames>Yassine</forenames></author><author><keyname>Cook</keyname><forenames>Virginia</forenames></author><author><keyname>Daoudi</keyname><forenames>Ahmed</forenames></author><author><keyname>Diaz</keyname><forenames>Mariano</forenames></author><author><keyname>Hazan</keyname><forenames>Gay</forenames></author><author><keyname>Zerkler</keyname><forenames>David</forenames></author><author><keyname>Nzoukou</keyname><forenames>William</forenames></author><author><keyname>Toutant</keyname><forenames>Isabelle</forenames></author><author><keyname>Toutant</keyname><forenames>Ren&#xe9;</forenames></author></authors><title>Software Design Document, Testing, Deployment and Configuration
  Management of the IUfA's UUIS -- a Team 3 COMP5541-W10 Project Approach</title><categories>cs.SE</categories><comments>108 pages, 67 Figures, 26 Tables</comments><acm-class>D.2; K.6; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this document is to provide technical specifications concerned
to the Design of the University Unified Inventory System - Web Portal, of the
UIfA. The Team of Developers used a Feedback Waterfall approach to build up the
system, under an Object Oriented paradigm. The architectural model followed was
the Model-View-Controller, mixed with a Mapper layer between the database and
the Model. Some of the patterns utilized in the developing of the System were
the Observer Pattern, the Command Pattern, and the Mapper Pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0855</identifier>
 <datestamp>2010-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0855</id><created>2010-05-05</created><updated>2010-05-07</updated><authors><author><keyname>Shin</keyname><forenames>Won-Yong</forenames></author><author><keyname>Lucani</keyname><forenames>Daniel E.</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author><author><keyname>Stojanovic</keyname><forenames>Milica</forenames></author><author><keyname>Tarokh</keyname><forenames>Vahid</forenames></author></authors><title>On Capacity Scaling of Underwater Networks: An Information-Theoretic
  Perspective</title><categories>cs.IT math.IT</categories><comments>16 pages, 4 figures, Submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Capacity scaling laws are analyzed in an underwater acoustic network with $n$
regularly located nodes on a square. A narrow-band model is assumed where the
carrier frequency is allowed to scale as a function of $n$. In the network, we
characterize an attenuation parameter that depends on the frequency scaling as
well as the transmission distance. A cut-set upper bound on the throughput
scaling is then derived in extended networks. Our result indicates that the
upper bound is inversely proportional to the attenuation parameter, thus
resulting in a highly power-limited network. Interestingly, it is seen that
unlike the case of wireless radio networks, our upper bound is intrinsically
related to the attenuation parameter but not the spreading factor. Furthermore,
we describe an achievable scheme based on the simple nearest neighbor multi-hop
(MH) transmission. It is shown under extended networks that the MH scheme is
order-optimal as the attenuation parameter scales exponentially with $\sqrt{n}$
(or faster). Finally, these scaling results are extended to a random network
realization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0858</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0858</id><created>2010-05-05</created><authors><author><keyname>Zhang</keyname><forenames>Teng</forenames></author><author><keyname>Szlam</keyname><forenames>Arthur</forenames></author><author><keyname>Wang</keyname><forenames>Yi</forenames></author><author><keyname>Lerman</keyname><forenames>Gilad</forenames></author></authors><title>Randomized hybrid linear modeling by local best-fit flats</title><categories>cs.CV</categories><comments>To appear in the proceedings of CVPR 2010</comments><journal-ref>2010 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR) (13-18 June 2010), pp. 1927-1934</journal-ref><doi>10.1109/CVPR.2010.5539866</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hybrid linear modeling problem is to identify a set of d-dimensional
affine sets in a D-dimensional Euclidean space. It arises, for example, in
object tracking and structure from motion. The hybrid linear model can be
considered as the second simplest (behind linear) manifold model of data. In
this paper we will present a very simple geometric method for hybrid linear
modeling based on selecting a set of local best fit flats that minimize a
global l1 error measure. The size of the local neighborhoods is determined
automatically by the Jones' l2 beta numbers; it is proven under certain
geometric conditions that good local neighborhoods exist and are found by our
method. We also demonstrate how to use this algorithm for fast determination of
the number of affine subspaces. We give extensive experimental evidence
demonstrating the state of the art accuracy and speed of the algorithm on
synthetic and real hybrid linear data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0879</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0879</id><created>2010-05-05</created><authors><author><keyname>Ezerman</keyname><forenames>Martianus Frederic</forenames></author><author><keyname>Ling</keyname><forenames>San</forenames></author><author><keyname>Sole</keyname><forenames>Patrick</forenames></author><author><keyname>Yemen</keyname><forenames>Olfa</forenames></author></authors><title>From Skew-Cyclic Codes to Asymmetric Quantum Codes</title><categories>cs.IT math.IT</categories><comments>16 pages, 3 tables, submitted to Advances in Mathematics of
  Communications</comments><msc-class>58F15, 58F17 (Primary) 53C35 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an additive but not $\F_{4}$-linear map $S$ from $\F_{4}^{n}$ to
$\F_{4}^{2n}$ and exhibit some of its interesting structural properties. If $C$
is a linear $[n,k,d]_4$-code, then $S(C)$ is an additive
$(2n,2^{2k},2d)_4$-code. If $C$ is an additive cyclic code then $S(C)$ is an
additive quasi-cyclic code of index $2$. Moreover, if $C$ is a module
$\theta$-cyclic code, a recently introduced type of code which will be
explained below, then $S(C)$ is equivalent to an additive cyclic code if $n$ is
odd and to an additive quasi-cyclic code of index $2$ if $n$ is even. Given any
$(n,M,d)_4$-code $C$, the code $S(C)$ is self-orthogonal under the trace
Hermitian inner product. Since the mapping $S$ preserves nestedness, it can be
used as a tool in constructing additive asymmetric quantum codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0880</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0880</id><created>2010-05-05</created><authors><author><keyname>Su</keyname><forenames>Yi</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Structural Solutions For Additively Coupled Sum Constrained Games</title><categories>cs.GT</categories><comments>39 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and analyze a broad family of games played by resource-constrained
players, which are characterized by the following central features: 1) each
user has a multi-dimensional action space, subject to a single sum resource
constraint; 2) each user's utility in a particular dimension depends on an
additive coupling between the user's action in the same dimension and the
actions of the other users; and 3) each user's total utility is the sum of the
utilities obtained in each dimension. Familiar examples of such multi-user
environments in communication systems include power control over
frequency-selective Gaussian interference channels and flow control in Jackson
networks. In settings where users cannot exchange messages in real-time, we
study how users can adjust their actions based on their local observations. We
derive sufficient conditions under which a unique Nash equilibrium exists and
the best-response algorithm converges globally and linearly to the Nash
equilibrium. In settings where users can exchange messages in real-time, we
focus on user choices that optimize the overall utility. We provide the
convergence conditions of two distributed action update mechanisms, gradient
play and Jacobi update.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0895</identifier>
 <datestamp>2013-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0895</id><created>2010-05-06</created><updated>2012-03-05</updated><authors><author><keyname>Fiorini</keyname><forenames>Samuel</forenames></author><author><keyname>Joret</keyname><forenames>Gwena&#xeb;l</forenames></author><author><keyname>Theis</keyname><forenames>Dirk Oliver</forenames></author><author><keyname>Wood</keyname><forenames>David R.</forenames></author></authors><title>Small Minors in Dense Graphs</title><categories>math.CO cs.DM</categories><msc-class>05C83, 05C35</msc-class><journal-ref>European Journal of Combinatorics, 33/6:1226--1245, 2012</journal-ref><doi>10.1016/j.ejc.2012.02.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental result in structural graph theory states that every graph with
large average degree contains a large complete graph as a minor. We prove this
result with the extra property that the minor is small with respect to the
order of the whole graph. More precisely, we describe functions $f$ and $h$
such that every graph with $n$ vertices and average degree at least $f(t)$
contains a $K_t$-model with at most $h(t)\cdot\log n$ vertices. The logarithmic
dependence on $n$ is best possible (for fixed $t$). In general, we prove that
$f(t)\leq 2^{t-1}+\eps$. For $t\leq 4$, we determine the least value of $f(t)$;
in particular $f(3)=2+\eps$ and $f(4)=4+\eps$. For $t\leq4$, we establish
similar results for graphs embedded on surfaces, where the size of the
$K_t$-model is bounded (for fixed $t$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0896</identifier>
 <datestamp>2011-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0896</id><created>2010-05-06</created><authors><author><keyname>Tacnet</keyname><forenames>Jean-Marc</forenames><affiliation>UR ETGR</affiliation></author><author><keyname>Batton-Hubert</keyname><forenames>Mireille</forenames><affiliation>ENSM-SE</affiliation></author><author><keyname>Dezert</keyname><forenames>Jean</forenames><affiliation>ONERA</affiliation></author></authors><title>A two-step fusion process for multi-criteria decision applied to natural
  hazards in mountains</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>Workshop on the Theory of Belief Functions, April 1- 2, 2010
  Brest, France, Brest : France (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mountain river torrents and snow avalanches generate human and material
damages with dramatic consequences. Knowledge about natural phenomenona is
often lacking and expertise is required for decision and risk management
purposes using multi-disciplinary quantitative or qualitative approaches.
Expertise is considered as a decision process based on imperfect information
coming from more or less reliable and conflicting sources. A methodology mixing
the Analytic Hierarchy Process (AHP), a multi-criteria aid-decision method, and
information fusion using Belief Function Theory is described. Fuzzy Sets and
Possibilities theories allow to transform quantitative and qualitative criteria
into a common frame of discernment for decision in Dempster-Shafer Theory (DST
) and Dezert-Smarandache Theory (DSmT) contexts. Main issues consist in basic
belief assignments elicitation, conflict identification and management, fusion
rule choices, results validation but also in specific needs to make a
difference between importance and reliability and uncertainty in the fusion
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0897</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0897</id><created>2010-05-06</created><authors><author><keyname>Bouboulis</keyname><forenames>Pantelis</forenames></author><author><keyname>Theodoridis</keyname><forenames>Sergios</forenames></author></authors><title>The Complex Gaussian Kernel LMS algorithm</title><categories>cs.LG</categories><comments>10 pages, 3 figures Manuscript submitted to ICANN 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the real reproducing kernels are used in an increasing number of
machine learning problems, complex kernels have not, yet, been used, in spite
of their potential interest in applications such as communications. In this
work, we focus our attention on the complex gaussian kernel and its possible
application in the complex Kernel LMS algorithm. In order to derive the
gradients needed to develop the complex kernel LMS (CKLMS), we employ the
powerful tool of Wirtinger's Calculus, which has recently attracted much
attention in the signal processing community. Writinger's calculus simplifies
computations and offers an elegant tool for treating complex signals. To this
end, the notion of Writinger's calculus is extended to include complex RKHSs.
Experiments verify that the CKLMS offers significant performance improvements
over the traditional complex LMS or Widely Linear complex LMS (WL-LMS)
algorithms, when dealing with nonlinearities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0902</identifier>
 <datestamp>2010-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0902</id><created>2010-05-06</created><updated>2010-05-25</updated><authors><author><keyname>Bouboulis</keyname><forenames>Pantelis</forenames></author><author><keyname>Theodoridis</keyname><forenames>Sergios</forenames></author></authors><title>Extension of Wirtinger Calculus in RKH Spaces and the Complex Kernel LMS</title><categories>cs.LG</categories><comments>6 pages, 3 figures manuscript submitted to MLSP 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last decade, kernel methods for nonlinear processing have
successfully been used in the machine learning community. However, so far, the
emphasis has been on batch techniques. It is only recently, that online
adaptive techniques have been considered in the context of signal processing
tasks. To the best of our knowledge, no kernel-based strategy has been
developed, so far, that is able to deal with complex valued signals. In this
paper, we take advantage of a technique called complexification of real RKHSs
to attack this problem. In order to derive gradients and subgradients of
operators that need to be defined on the associated complex RKHSs, we employ
the powerful tool ofWirtinger's Calculus, which has recently attracted much
attention in the signal processing community. Writinger's calculus simplifies
computations and offers an elegant tool for treating complex signals. To this
end, in this paper, the notion of Writinger's calculus is extended, for the
first time, to include complex RKHSs and use it to derive the Complex Kernel
Least-Mean-Square (CKLMS) algorithm. Experiments verify that the CKLMS can be
used to derive nonlinear stable algorithms, which offer significant performance
improvements over the traditional complex LMS orWidely Linear complex LMS
(WL-LMS) algorithms, when dealing with nonlinearities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0905</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0905</id><created>2010-05-06</created><authors><author><keyname>Fang</keyname><forenames>Yechang</forenames></author><author><keyname>Yen</keyname><forenames>Kang</forenames></author><author><keyname>Pan</keyname><forenames>Deng</forenames></author><author><keyname>Sun</keyname><forenames>Zhuo</forenames></author></authors><title>Buffer Management Algorithm Design and Implementation Based on Network
  Processors</title><categories>cs.NI</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><report-no>Computer Science ISSN 19475500</report-no><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  To solve the parameter sensitive issue of the traditional RED (random early
detection) algorithm, an adaptive buffer management algorithm called PAFD
(packet adaptive fair dropping) is proposed. This algorithm supports DiffServ
(differentiated services) model of QoS (quality of service). In this algorithm,
both of fairness and throughput are considered. The smooth buffer occupancy
rate function is adopted to adjust the parameters. By implementing buffer
management and packet scheduling on Intel IXP2400, the viability of QoS
mechanisms on NPs (network processors) is verified. The simulation shows that
the PAFD smoothes the flow curve, and achieves better balance between fairness
and network throughput. It also demonstrates that this algorithm meets the
requirements of fast data packet processing, and the hardware resource
utilization of NPs is higher.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0907</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0907</id><created>2010-05-06</created><authors><author><keyname>Alginaih</keyname><forenames>Yasser M.</forenames></author><author><keyname>Siddiqi</keyname><forenames>Abdul Ahad</forenames></author></authors><title>Multistage Hybrid Arabic/Indian Numeral OCR System</title><categories>cs.CV</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The use of OCR in postal services is not yet universal and there are still
many countries that process mail sorting manually. Automated Arabic/Indian
numeral Optical Character Recognition (OCR) systems for Postal services are
being used in some countries, but still there are errors during the mail
sorting process, thus causing a reduction in efficiency. The need to
investigate fast and efficient recognition algorithms/systems is important so
as to correctly read the postal codes from mail addresses and to eliminate any
errors during the mail sorting stage. The objective of this study is to
recognize printed numerical postal codes from mail addresses. The proposed
system is a multistage hybrid system which consists of three different feature
extraction methods, i.e., binary, zoning, and fuzzy features, and three
different classifiers, i.e., Hamming Nets, Euclidean Distance, and Fuzzy Neural
Network Classifiers. The proposed system, systematically compares the
performance of each of these methods, and ensures that the numerals are
recognized correctly. Comprehensive results provide a very high recognition
rate, outperforming the other known developed methods in literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0909</identifier>
 <datestamp>2010-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0909</id><created>2010-05-06</created><updated>2010-05-06</updated><authors><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author></authors><title>George Forsythe's last paper</title><categories>cs.NA math.NA stat.CO</categories><comments>10 pages. Text of an invited talk presented at the Stanford 50
  Conference celebrating the 50th anniversary of George Forsythe's arrival at
  Stanford and the 75th birthday of Gene Golub. For further details see
  http://wwwmaths.anu.edu.au/~brent/pub/pub238.html</comments><msc-class>65-03 (Primary), 11K45, 65C10 (Secondary)</msc-class><acm-class>G.1.0; G.3; K.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe von Neumann's elegant idea for sampling from the exponential
distribution, Forsythe's generalization for sampling from a probability
distribution whose density has the form exp(-G(x)), where G(x) is easy to
compute (e.g. a polynomial), and my refinement of these ideas to give an
efficient algorithm for generating pseudo-random numbers with a normal
distribution. Later developments are also mentioned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0912</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0912</id><created>2010-05-06</created><authors><author><keyname>Kaplan</keyname><forenames>Haim</forenames></author><author><keyname>Rubin</keyname><forenames>Natan</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author></authors><title>A Kinetic Triangulation Scheme for Moving Points in The Plane</title><categories>cs.CG cs.DS</categories><comments>A preliminary version accepted to SoCG 2010</comments><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple randomized scheme for triangulating a set $P$ of $n$
points in the plane, and construct a kinetic data structure which maintains the
triangulation as the points of $P$ move continuously along piecewise algebraic
trajectories of constant description complexity. Our triangulation scheme
experiences an expected number of $O(n^2\beta_{s+2}(n)\log^2n)$ discrete
changes, and handles them in a manner that satisfies all the standard
requirements from a kinetic data structure: compactness, efficiency, locality
and responsiveness. Here $s$ is the maximum number of times where any specific
triple of points of $P$ can become collinear,
$\beta_{s+2}(q)=\lambda_{s+2}(q)/q$, and $\lambda_{s+2}(q)$ is the maximum
length of Davenport-Schinzel sequences of order $s+2$ on $n$ symbols. Thus,
compared to the previous solution of Agarwal et al.~\cite{AWY}, we achieve a
(slightly) improved bound on the number of discrete changes in the
triangulation. In addition, we believe that our scheme is simpler to implement
and analyze.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0917</identifier>
 <datestamp>2010-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0917</id><created>2010-05-06</created><authors><author><keyname>Rowinska-Schwarzweller</keyname><forenames>Agnieszka</forenames></author><author><keyname>Schwarzweller</keyname><forenames>Christoph</forenames></author></authors><title>On Building a Knowledge Base for Stability Theory</title><categories>cs.AI</categories><comments>To appear in The 9th International Conference on Mathematical
  Knowledge Management: MKM 2010</comments><journal-ref>Lecture Notes in Computer Science, 2010, Volume 6167, Intelligent
  Computer Mathematics, Pages 427-439</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A lot of mathematical knowledge has been formalized and stored in
repositories by now: different mathematical theorems and theories have been
taken into consideration and included in mathematical repositories.
Applications more distant from pure mathematics, however --- though based on
these theories --- often need more detailed knowledge about the underlying
theories. In this paper we present an example Mizar formalization from the area
of electrical engineering focusing on stability theory which is based on
complex analysis. We discuss what kind of special knowledge is necessary here
and which amount of this knowledge is included in existing repositories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0919</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0919</id><created>2010-05-06</created><authors><author><keyname>Farid</keyname><forenames>Dewan Md.</forenames></author><author><keyname>Rahman</keyname><forenames>Mohammad Zahidur</forenames></author></authors><title>Attribute Weighting with Adaptive NBTree for Reducing False Positives in
  Intrusion Detection</title><categories>cs.CR</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we introduce new learning algorithms for reducing false
positives in intrusion detection. It is based on decision tree-based attribute
weighting with adaptive na\&quot;ive Bayesian tree, which not only reduce the false
positives (FP) at acceptable level, but also scale up the detection rates (DR)
for different types of network intrusions. Due to the tremendous growth of
network-based services, intrusion detection has emerged as an important
technique for network security. Recently data mining algorithms are applied on
network-based traffic data and host-based program behaviors to detect
intrusions or misuse patterns, but there exist some issues in current intrusion
detection algorithms such as unbalanced detection rates, large numbers of false
positives, and redundant attributes that will lead to the complexity of
detection model and degradation of detection accuracy. The purpose of this
study is to identify important input attributes for building an intrusion
detection system (IDS) that is computationally efficient and effective.
Experimental results performed using the KDD99 benchmark network intrusion
detection dataset indicate that the proposed approach can significantly reduce
the number and percentage of false positives and scale up the balance detection
rates for different types of network intrusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0921</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0921</id><created>2010-05-06</created><authors><author><keyname>Frosini</keyname><forenames>Patrizio</forenames></author><author><keyname>Landi</keyname><forenames>Claudia</forenames></author></authors><title>No embedding of the automorphisms of a topological space into a compact
  metric space endows them with a composition that passes to the limit</title><categories>cs.CG math.MG</categories><comments>6 pages, no figures</comments><report-no>2768</report-no><msc-class>Primary 57S05, 57S10, Secondary 54C35, 68U05</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hausdorff distance, the Gromov-Hausdorff, the Fr\'echet and the natural
pseudo-distances are instances of dissimilarity measures widely used in shape
comparison. We show that they share the property of being defined as $\inf_\rho
F(\rho)$ where $F$ is a suitable functional and $\rho$ varies in a set of
correspondences containing the set of homeomorphisms. Our main result states
that the set of homeomorphisms cannot be enlarged to a metric space
$\mathcal{K}$, in such a way that the composition in $\mathcal{K}$ (extending
the composition of homeomorphisms) passes to the limit and, at the same time,
$\mathcal{K}$ is compact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0925</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0925</id><created>2010-05-06</created><authors><author><keyname>Bouyer</keyname><forenames>Asgarali</forenames></author><author><keyname>hoseyni</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Abdullah</keyname><forenames>Abdul Hanan</forenames></author></authors><title>Improving Overhead Computation and pre-processing Time for Grid
  Scheduling System</title><categories>cs.DC</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Computational Grid is enormous environments with heterogeneous resources and
stable infrastructures among other Internet-based computing systems. However,
the managing of resources in such systems has its special problems. Scheduler
systems need to get last information about participant nodes from information
centers for the purpose of firmly job scheduling. In this paper, we focus on
online updating resource information centers with processed and provided data
based on the assumed hierarchical model. A hybrid knowledge extraction method
has been used to classifying grid nodes based on prediction of jobs' features.
An affirmative point of this research is that scheduler systems don't waste
extra time for getting up-to-date information of grid nodes. The experimental
result shows the advantages of our approach compared to other conservative
methods, especially due to its ability to predict the behavior of nodes based
on comprehensive data tables on each node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0931</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0931</id><created>2010-05-06</created><authors><author><keyname>Abdurohman</keyname><forenames>Maman</forenames></author><author><keyname>Kuspriyanto</keyname></author><author><keyname>Sutikno</keyname><forenames>Sarwono</forenames></author><author><keyname>Sasongko</keyname><forenames>Arif</forenames></author></authors><title>The New Embedded System Design Methodology For Improving Design Process
  Performance</title><categories>cs.OH</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Time-to-market pressure and productivity gap force vendors and researchers to
improve embedded system design methodology. Current used design method,
Register Transfer Level (RTL), is no longer be adequate to comply with embedded
system design necessity. It needs a new methodology for facing the lack of RTL.
In this paper, a new methodology of hardware embedded system modeling process
is designed for improving design process performance using Transaction Level
Modeling (TLM). TLM is a higher abstraction design concept model above RTL
model. Parameters measured include design process time and accuracy of design.
For implementing RTL model used Avalon and Wishbone buses, both are System on
Chip bus. Performance improvement measured by comparing TLM and RTL model
process. The experiment results show performance improvements for Avalon RTL
using new design methodology are 1,03 for 3-tiers, 1,47 for 4-tiers and 1,69
for 5-tiers. Performance improvements for Wishbone RTL are 1,12 for 3-tiers,
1,17 for 4-tiers and 1,34 for 5-tiers. These results show the trend of design
process improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0940</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0940</id><created>2010-05-06</created><authors><author><keyname>Kaosar</keyname><forenames>Md. Golam</forenames></author><author><keyname>Yi</keyname><forenames>Xun</forenames></author></authors><title>Semi-Trusted Mixer Based Privacy Preserving Distributed Data Mining for
  Resource Constrained Devices</title><categories>cs.CR</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper a homomorphic privacy preserving association rule mining
algorithm is proposed which can be deployed in resource constrained devices
(RCD). Privacy preserved exchange of counts of itemsets among distributed
mining sites is a vital part in association rule mining process. Existing
cryptography based privacy preserving solutions consume lot of computation due
to complex mathematical equations involved. Therefore less computation involved
privacy solutions are extremely necessary to deploy mining applications in RCD.
In this algorithm, a semi-trusted mixer is used to unify the counts of itemsets
encrypted by all mining sites without revealing individual values. The proposed
algorithm is built on with a well known communication efficient association
rule mining algorithm named count distribution (CD). Security proofs along with
performance analysis and comparison show the well acceptability and
effectiveness of the proposed algorithm. Efficient and straightforward privacy
model and satisfactory performance of the protocol promote itself among one of
the initiatives in deploying data mining application in RCD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0944</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0944</id><created>2010-05-06</created><authors><author><keyname>Malathy</keyname><forenames>S.</forenames></author><author><keyname>Sadhasivam</keyname><forenames>G. Sudha</forenames></author><author><keyname>Murugan</keyname><forenames>K.</forenames></author><author><keyname>Lokesh</keyname><forenames>S.</forenames></author></authors><title>Adaptive Slot Allocation And Bandwidth Sharing For Prioritized Handoff
  Calls In Mobile Netwoks</title><categories>cs.NI</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Mobility management and bandwidth management are two major research issues in
a cellular mobile network. Mobility management consists of two basic
components: location management and handoff management. To Provide QoS to the
users Handoff is a key element in wireless cellular networks. It is often
initiated either by crossing a cell boundary or by deterioration in the quality
of signal in the current channel. In this paper, a new admission control policy
for cellular mobile network is being proposed. Two important QoS parameter in
cellular networks are Call Dropping Probability (CDP) and Handoff Dropping
Probability (HDP). CDP represents the probability that a call is dropped due to
a handoff failure. HDP represents the probability of a handoff failure due to
insufficient available resources in the target cell. Most of the algorithms try
to limit the HDP to some target maximum but not CDP. In this paper, we show
that when HDP is controlled, the CDP is also controlled to a minimum extent
while maintaining lower blocking rates for new calls in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0945</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0945</id><created>2010-05-06</created><authors><author><keyname>Soni</keyname><forenames>Mohit</forenames></author><author><keyname>Gupta</keyname><forenames>Sandesh</forenames></author><author><keyname>Rao</keyname><forenames>M. S.</forenames></author><author><keyname>Gupta</keyname><forenames>Phalguni</forenames></author></authors><title>An Efficient Vein Pattern-based Recognition System</title><categories>cs.CV</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents an efficient human recognition system based on vein
pattern from the palma dorsa. A new absorption based technique has been
proposed to collect good quality images with the help of a low cost camera and
light source. The system automatically detects the region of interest from the
image and does the necessary preprocessing to extract features. A Euclidean
Distance based matching technique has been used for making the decision. It has
been tested on a data set of 1750 image samples collected from 341 individuals.
The accuracy of the verification system is found to be 99.26% with false
rejection rate (FRR) of 0.03%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0950</identifier>
 <datestamp>2010-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0950</id><created>2010-05-06</created><authors><author><keyname>Grabowski</keyname><forenames>Adam</forenames></author><author><keyname>Schwarzweller</keyname><forenames>Christoph</forenames></author></authors><title>On Duplication in Mathematical Repositories</title><categories>cs.DL</categories><comments>To appear in The 9th International Conference on Mathematical
  Knowledge Management: MKM 2010</comments><journal-ref>Lecture Notes in Computer Science, 2010, Volume 6167, Intelligent
  Computer Mathematics, Pages 300-314</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building a repository of proof-checked mathematical knowledge is without any
doubt a lot of work, and besides the actual formalization process there also is
the task of maintaining the repository. Thus it seems obvious to keep a
repsoitory as small as possible, in particular each piece of mathematical
knowledge should be formalized only once. In this paper, however, we claim that
it might be reasonable or even necessary to duplicate knowledge in a
mathematical repository. We analyze different situations and reasons for doing
so and provide a number of examples supporting our thesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0952</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0952</id><created>2010-05-06</created><authors><author><keyname>Bhanu</keyname><forenames>S. Vijay</forenames></author><author><keyname>Chandrasekaran</keyname><forenames>RM.</forenames></author><author><keyname>Balakrishnan</keyname><forenames>V.</forenames></author></authors><title>Effective Bandwidth Utilization in IEEE802.11 for VOIP</title><categories>cs.NI</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Voice over Internet protocol (VoIP) is one of the most important applications
for the IEEE 802.11 wireless local area networks (WLANs). For network planners
who are deploying VoIP over WLANs, one of the important issues is the VoIP
capacity. VoIP bandwidth consumption over a WAN is one of the most important
factors to consider when building a VoIP infrastructure. Failure to account for
VoIP bandwidth requirements will severely limit the reliability of a VoIP
system and place a huge burden on the WAN infrastructure. Less bandwidth
utilization is the key reasons for reduced number of channel accesses in VOIP.
But in the QoS point of view the free bandwidth of atleast 1-5% will improve
the voice quality. This proposal utilizes the maximum bandwidth by leaving 1-5%
free bandwidth. A Bandwidth Data rate Moderation (BDM) algorithm has been
proposed which correlates the data rate specified in IEEE802.11b with the free
bandwidth. At each time BDM will calculate the bandwidth utilization before
sending the packet to improve performance and voice quality of VoIP. The
bandwidth calculation in BDM can be done by using Erlang and VOIP bandwidth
calculator. Finally, ns2 experimental study shows the relationship between
bandwidth utilization, free bandwidth and data rate. The paper concludes that
marginal VoIP call rate has been increased by BDM algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0957</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0957</id><created>2010-05-06</created><authors><author><keyname>Karpagachelvi</keyname><forenames>S.</forenames></author><author><keyname>Arthanari</keyname><forenames>M.</forenames></author><author><keyname>Sivakumar</keyname><forenames>M.</forenames></author></authors><title>ECG Feature Extraction Techniques - A Survey Approach</title><categories>cs.NE cs.AI physics.med-ph</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  ECG Feature Extraction plays a significant role in diagnosing most of the
cardiac diseases. One cardiac cycle in an ECG signal consists of the P-QRS-T
waves. This feature extraction scheme determines the amplitudes and intervals
in the ECG signal for subsequent analysis. The amplitudes and intervals value
of P-QRS-T segment determines the functioning of heart of every human.
Recently, numerous research and techniques have been developed for analyzing
the ECG signal. The proposed schemes were mostly based on Fuzzy Logic Methods,
Artificial Neural Networks (ANN), Genetic Algorithm (GA), Support Vector
Machines (SVM), and other Signal Analysis techniques. All these techniques and
algorithms have their advantages and limitations. This proposed paper discusses
various techniques and transformations proposed earlier in literature for
extracting feature from an ECG signal. In addition this paper also provides a
comparative study of various methods proposed by researchers in extracting the
feature from ECG signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0959</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0959</id><created>2010-05-06</created><authors><author><keyname>Aravinda</keyname><forenames>H. S.</forenames></author><author><keyname>Maheshappa</keyname><forenames>H. D.</forenames></author><author><keyname>Moodithaya</keyname><forenames>Ranjan</forenames></author></authors><title>Implementation of the Six Channel Redundancy to achieve fault tolerance
  in testing of satellites</title><categories>cs.OH</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper aims to implement the six channel redundancy to achieve fault
tolerance in testing of satellites with acoustic spectrum. We mainly focus here
on achieving fault tolerance. An immediate application is the microphone data
acquisition and to do analysis at the Acoustic Test Facility (ATF) centre,
National Aerospace Laboratories. It has an 1100 cubic meter reverberation
chamber in which a maximum sound pressure level of 157 dB is generated. The six
channel Redundancy software with fault tolerant operation is devised and
developed. The data are applied to program written in C language. The program
is run using the Code Composer Studio by accepting the inputs. This is tested
with the TMS 320C 6727 DSP, Pro Audio Development Kit (PADK).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0961</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0961</id><created>2010-05-06</created><authors><author><keyname>Umamaheswari</keyname><forenames>M.</forenames></author><author><keyname>Sivasubramanian</keyname><forenames>S.</forenames></author></authors><title>Performance Oriented Query Processing In GEO Based Location Search
  Engines</title><categories>cs.IR</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Geographic location search engines allow users to constrain and order search
results in an intuitive manner by focusing a query on a particular geographic
region. Geographic search technology, also called location search, has recently
received significant interest from major search engine companies. Academic
research in this area has focused primarily on techniques for extracting
geographic knowledge from the web. In this paper, we study the problem of
efficient query processing in scalable geographic search engines. Query
processing is a major bottleneck in standard web search engines, and the main
reason for the thousands of machines used by the major engines. Geographic
search engine query processing is different in that it requires a combination
of text and spatial data processing techniques. We propose several algorithms
for efficient query processing in geographic search engines, integrate them
into an existing web search query processor, and evaluate them on large sets of
real data and query traces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0963</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0963</id><created>2010-05-06</created><authors><author><keyname>Kumar</keyname><forenames>Manish</forenames></author><author><keyname>Srivastava</keyname><forenames>M. C.</forenames></author><author><keyname>Kumar</keyname><forenames>Umesh</forenames></author></authors><title>Tunable Multifunction Filter Using Current Conveyor</title><categories>cs.OH</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The paper presents a current tunable multifunction filter using current
conveyor. The proposed circuit can be realized as on chip tunable low pass,
high pass, band pass and elliptical notch filter. The circuit employs two
current conveyors, one OTA, four resistors and two grounded capacitors, ideal
for integration. It has only one output terminal and the number of input
terminals may be used. Further, there is no requirement for component matching
in the circuit. The resonance frequency ({\omega}0) and bandwidth ({\omega}0
/Q) enjoy orthogonal tuning. The cutoff frequency of the filter is tunable by
changing the bias current, which makes it on chip tunable filter. The circuit
is realized by using commercially available current conveyor AD844 and OTA
LM13700. A HSPICE simulation of circuit is also studied for the verification of
theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0965</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0965</id><created>2010-05-06</created><authors><author><keyname>Kaur</keyname><forenames>Bikrampal</forenames></author><author><keyname>Aggarwal</keyname><forenames>Himanshu</forenames></author></authors><title>Artificial Neural Network based Diagnostic Model For Causes of Success
  and Failures</title><categories>cs.NE</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper an attempt has been made to identify most important human
resource factors and propose a diagnostic model based on the back-propagation
and connectionist model approaches of artificial neural network (ANN). The
focus of the study is on the mobile -communication industry of India. The ANN
based approach is particularly important because conventional approaches (such
as algorithmic) to the problem solving have their inherent disadvantages. The
algorithmic approach is well-suited to the problems that are well-understood
and known solution(s). On the other hand the ANNs have learning by example and
processing capabilities similar to that of a human brain. ANN has been followed
due to its inherent advantage over conversion algorithmic like approaches and
having capabilities, training and human like intuitive decision making
capabilities. Therefore, this ANN based approach is likely to help researchers
and organizations to reach a better solution to the problem of managing the
human resource. The study is particularly important as many studies have been
carried in developed countries but there is a shortage of such studies in
developing nations like India. Here, a model has been derived using
connectionist-ANN approach and improved and verified via back-propagation
algorithm. This suggested ANN based model can be used for testing the success
and failure human factors in any of the communication Industry. Results have
been obtained on the basis of connectionist model, which has been further
refined by BPNN to an accuracy of 99.99%. Any company to predict failure due to
HR factors can directly deploy this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0967</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0967</id><created>2010-05-06</created><authors><author><keyname>Visumathi</keyname><forenames>J.</forenames></author><author><keyname>Shunmuganathan</keyname><forenames>K. L.</forenames></author></authors><title>Detecting Security threats in the Router using Computational
  Intelligence</title><categories>cs.CR</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  nformation security is an issue of global concern. As the Internet is
delivering great convenience and benefits to the modern society, the rapidly
increasing connectivity and accessibility to the Internet is also posing a
serious threat to security and privacy, to individuals, organizations, and
nations alike. Finding effective ways to detect, prevent, and respond to
intrusions and hacker attacks of networked computers and information systems.
This paper presents a knowledge discovery frame work to detect DoS attacks at
the boundary controllers (routers). The idea is to use machine learning
approach to discover network features that can depict the state of the network
connection. Using important network data (DoS relevant features), we have
developed kernel machine based and soft computing detection mechanisms that
achieve high detection accuracies. We also present our work of identifying DoS
pertinent features and evaluating the applicability of these features in
detecting novel DoS attacks. Architecture for detecting DoS attacks at the
router is presented. We demonstrate that highly efficient and accurate
signature based classifiers can be constructed by using important network
features and machine learning techniques to detect DoS attacks at the boundary
controllers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0972</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0972</id><created>2010-05-06</created><authors><author><keyname>Rodd</keyname><forenames>S. F.</forenames></author><author><keyname>Kulkarni</keyname><forenames>U. P.</forenames></author></authors><title>Adaptive Tuning Algorithm for Performance tuning of Database Management
  System</title><categories>cs.DB</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Performance tuning of Database Management Systems(DBMS) is both complex and
challenging as it involves identifying and altering several key performance
tuning parameters. The quality of tuning and the extent of performance
enhancement achieved greatly depends on the skill and experience of the
Database Administrator (DBA). As neural networks have the ability to adapt to
dynamically changing inputs and also their ability to learn makes them ideal
candidates for employing them for tuning purpose. In this paper, a novel tuning
algorithm based on neural network estimated tuning parameters is presented. The
key performance indicators are proactively monitored and fed as input to the
Neural Network and the trained network estimates the suitable size of the
buffer cache, shared pool and redo log buffer size. The tuner alters these
tuning parameters using the estimated values using a rate change computing
algorithm. The preliminary results show that the proposed method is effective
in improving the query response time for a variety of workload types. .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0976</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0976</id><created>2010-05-06</created><authors><author><keyname>Rakesh</keyname><forenames>Jha</forenames></author><author><keyname>A.</keyname><forenames>Wankhede Vishal</forenames></author><author><keyname>Dalal</keyname><forenames>Upena</forenames></author></authors><title>A Survey of Mobile WiMAX IEEE 802.16m Standard</title><categories>cs.NI</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  IEEE 802.16m amends the IEEE 802.16 Wireless MAN-OFDMA specification to
provide an advanced air interface for operation in licenced bands. It will meet
the cellular layer requirements of IMT-Advanced next generation mobile
networks. It will be designed to provide significantly improved performance
compared to other high rate broadband cellular network systems. For the next
generation mobile networks, it is important to consider increasing peak,
sustained data reates, corresponding spectral efficiencies, system capacity and
cell coverage as well as decreasing latency and providing QoS while carefully
considering overall system complexity. In this paper we provide an overview of
the state-of-the-art mobile WiMAX technology and its development. We focus our
discussion on Physical Layer, MAC Layer, Schedular,QoS provisioning and mobile
WiMAX specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0982</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0982</id><created>2010-05-06</created><authors><author><keyname>Elekes</keyname><forenames>Gy&#xf6;rgy</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author></authors><title>Incidences in Three Dimensions and Distinct Distances in the Plane</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first describe a reduction from the problem of lower-bounding the number
of distinct distances determined by a set $S$ of $s$ points in the plane to an
incidence problem between points and a certain class of helices (or parabolas)
in three dimensions. We offer conjectures involving the new setup, but are
still unable to fully resolve them.
  Instead, we adapt the recent new algebraic analysis technique of Guth and
Katz \cite{GK}, as further developed by Elekes et al. \cite{EKS}, to obtain
sharp bounds on the number of incidences between these helices or parabolas and
points in $\reals^3$. Applying these bounds, we obtain, among several other
results, the upper bound $O(s^3)$ on the number of rotations (rigid motions)
which map (at least) three points of $S$ to three other points of $S$. In fact,
we show that the number of such rotations which map at least $k\ge 3$ points of
$S$ to $k$ other points of $S$ is close to $O(s^3/k^{12/7})$.
  One of our unresolved conjectures is that this number is $O(s^3/k^2)$, for
$k\ge 2$. If true, it would imply the lower bound $\Omega(s/\log s)$ on the
number of distinct distances in the plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.0990</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.0990</id><created>2010-05-06</created><authors><author><keyname>Sevilla</keyname><forenames>David</forenames></author><author><keyname>Wachsmuth</keyname><forenames>Daniel</forenames></author></authors><title>Polynomial integration on regions defined by a triangle and a conic</title><categories>cs.SC math.OC</categories><comments>8 pages, accepted by ISSAC 2010</comments><acm-class>G.1.8; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an efficient solution to the following problem, of relevance in a
numerical optimization scheme: calculation of integrals of the type \[\iint_{T
\cap \{f\ge0\}} \phi_1\phi_2 \, dx\,dy\] for quadratic polynomials
$f,\phi_1,\phi_2$ on a plane triangle $T$. The naive approach would involve
consideration of the many possible shapes of $T\cap\{f\geq0\}$ (possibly after
a convenient transformation) and parameterizing its border, in order to
integrate the variables separately. Our solution involves partitioning the
triangle into smaller triangles on which integration is much simpler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1009</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1009</id><created>2010-05-06</created><authors><author><keyname>Jukna</keyname><forenames>S.</forenames></author><author><keyname>Schnitger</keyname><forenames>G.</forenames></author></authors><title>Min-Rank Conjecture for Log-Depth Circuits</title><categories>cs.CC</categories><comments>22 pages, to appear in: J. Comput.Syst.Sci.</comments><journal-ref>Journal of Computer and System Sciences 77:6 (2011), 1023-1038</journal-ref><doi>10.1016/j.jcss.2009.09.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A completion of an m-by-n matrix A with entries in {0,1,*} is obtained by
setting all *-entries to constants 0 or 1. A system of semi-linear equations
over GF(2) has the form Mx=f(x), where M is a completion of A and f:{0,1}^n --&gt;
{0,1}^m is an operator, the i-th coordinate of which can only depend on
variables corresponding to *-entries in the i-th row of A. We conjecture that
no such system can have more than 2^{n-c\cdot mr(A)} solutions, where c&gt;0 is an
absolute constant and mr(A) is the smallest rank over GF(2) of a completion of
A. The conjecture is related to an old problem of proving super-linear lower
bounds on the size of log-depth boolean circuits computing linear operators x
--&gt; Mx. The conjecture is also a generalization of a classical question about
how much larger can non-linear codes be than linear ones. We prove some special
cases of the conjecture and establish some structural properties of solution
sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1034</identifier>
 <datestamp>2010-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1034</id><created>2010-05-06</created><authors><author><keyname>von Issendorff</keyname><forenames>Hermann</forenames></author></authors><title>Programming Discrete Physical Systems</title><categories>cs.PL physics.class-ph q-bio.MN</categories><comments>Comments: 24 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every algorithm which can be executed on a computer can at least in principle
be realized in hardware, i.e. by a discrete physical system. The problem is
that up to now there is no programming language by which physical systems can
constructively be described. Such tool, however, is essential for the compact
description and automatic production of complex systems. This paper introduces
a programming language, called Akton-Algebra, which provides the foundation for
the complete description of discrete physical systems. The approach originates
from the finding that every discrete physical system reduces to a
spatiotemporal topological network of nodes, if the functional and metric
properties are deleted. A next finding is that there exists a homeomorphism
between the topological network and a sequence of symbols representing a
program by which the original nodal network can be reconstructed. Providing
Akton-Algebra with functionality turns it into a flow-controlled general data
processing language, which by introducing clock control and addressing can be
further transformed into a classical programming language. Providing
Akton-Algebra with metrics, i.e. the shape and size of the components, turns it
into a novel hardware system construction language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1053</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1053</id><created>2010-05-06</created><authors><author><keyname>Dickerson</keyname><forenames>Matthew T.</forenames></author><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author><author><keyname>Dickerson</keyname><forenames>Thomas D.</forenames></author></authors><title>Round-Trip Voronoi Diagrams and Doubling Density in Geographic Networks</title><categories>cs.DS</categories><comments>21 pages, 7 figures. To appear in ISVD 2010.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The round-trip distance function on a geographic network (such as a road
network, flight network, or utility distribution grid) defines the &quot;distance&quot;
from a single vertex to a pair of vertices as the minimum length tour visiting
all three vertices and ending at the starting vertex. Given a geographic
network and a subset of its vertices called &quot;sites&quot; (for example a road network
with a list of grocery stores), a two-site round-trip Voronoi diagram labels
each vertex in the network with the pair of sites that minimizes the round-trip
distance from that vertex. Alternatively, given a geographic network and two
sets of sites of different types (for example grocery stores and coffee shops),
a two-color round-trip Voronoi diagram labels each vertex with the pair of
sites of different types minimizing the round-trip distance. In this paper, we
prove several new properties of two-site and two-color round-trip Voronoi
diagrams in a geographic network, including a relationship between the
&quot;doubling density&quot; of sites and an upper bound on the number of non-empty
Voronoi regions. We show how those lemmas can be used in new algorithms
asymptotically more efficient than previous known algorithms when the networks
have reasonable distribution properties related to doubling density, and we
provide experimental data suggesting that road networks with standard
point-of-interest sites have these properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1059</identifier>
 <datestamp>2010-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1059</id><created>2010-05-06</created><updated>2010-09-12</updated><authors><author><keyname>Abhishek</keyname><forenames>Vineet</forenames></author><author><keyname>Hajek</keyname><forenames>Bruce</forenames></author></authors><title>Revenue Optimal Auction for Single-Minded Buyers</title><categories>cs.GT</categories><comments>19 pages, 2 figures. A short version of this will appear in the 49th
  IEEE Conference on Decision and Control (CDC), 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of characterizing revenue optimal auctions for
single-minded buyers. Each buyer is interested only in a specific bundle of
items and has a value for the same. Both his bundle and its value are his
private information. The bundles that buyers are interested in and their
corresponding values are assumed to be realized from known probability
distributions independent across the buyers. We identify revenue optimal
auctions with a simple structure, if the conditional distribution of any
buyer's valuation is nondecreasing, in the hazard rates ordering of probability
distributions, as a function of the bundle the buyer is interested in. The
revenue optimal auction is given by the solution of a maximum weight
independent set problem. We provide a novel graphical construction of the
weights and highlight important properties of the resulting auction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1062</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1062</id><created>2010-05-06</created><authors><author><keyname>Lentmaier</keyname><forenames>Michael</forenames></author><author><keyname>Mitchell</keyname><forenames>David G. M.</forenames></author><author><keyname>Fettweis</keyname><forenames>Gerhard P.</forenames></author><author><keyname>Costello,</keyname><forenames>Daniel J.</forenames><suffix>Jr.</suffix></author></authors><title>Asymptotically Regular LDPC Codes with Linear Distance Growth and
  Thresholds Close to Capacity</title><categories>cs.IT math.IT</categories><comments>Presented at the 2010 Information Theory and Applications Workshop,
  San Diego, CA.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Families of &quot;asymptotically regular&quot; LDPC block code ensembles can be formed
by terminating (J,K)-regular protograph-based LDPC convolutional codes. By
varying the termination length, we obtain a large selection of LDPC block code
ensembles with varying code rates and substantially better iterative decoding
thresholds than those of (J,K)-regular LDPC block code ensembles, despite the
fact that the terminated ensembles are almost regular. Also, by means of an
asymptotic weight enumerator analysis, we show that minimum distance grows
linearly with block length for all of the ensembles in these families, i.e.,
the ensembles are asymptotically good. We find that, as the termination length
increases, families of &quot;asymptotically regular&quot; codes with capacity approaching
iterative decoding thresholds and declining minimum distance growth rates are
obtained, allowing a code designer to trade-off between distance growth rate
and threshold. Further, we show that the thresholds and the distance growth
rates can be improved by carefully choosing the component protographs used in
the code construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1065</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1065</id><created>2010-05-06</created><authors><author><keyname>Mitchell</keyname><forenames>David G. M.</forenames></author><author><keyname>Lentmaier</keyname><forenames>Michael</forenames></author><author><keyname>Costello</keyname><forenames>Daniel J.</forenames><suffix>Jr.</suffix></author></authors><title>New Families of LDPC Block Codes Formed by Terminating Irregular
  Protograph-Based LDPC Convolutional Codes</title><categories>cs.IT math.IT</categories><comments>To be presented at the 2010 IEEE International Symposium on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a method of constructing new families of LDPC block
code ensembles formed by terminating irregular protograph-based LDPC
convolutional codes. Using the accumulate-repeat-by-4-jagged-accumulate (AR4JA)
protograph as an example, a density evolution analysis for the binary erasure
channel shows that this flexible design technique gives rise to a large
selection of LDPC block code ensembles with varying code rates and thresholds
close to capacity. Further, by means of an asymptotic weight enumerator
analysis, we show that all the ensembles in this family also have minimum
distance that grows linearly with block length, i.e., they are asymptotically
good.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1087</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1087</id><created>2010-05-06</created><authors><author><keyname>Gathen</keyname><forenames>Joachim von zur</forenames></author><author><keyname>Giesbrecht</keyname><forenames>Mark</forenames></author><author><keyname>Ziegler</keyname><forenames>Konstantin</forenames></author></authors><title>Composition collisions and projective polynomials</title><categories>math.AC cs.SC</categories><msc-class>68W30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The functional decomposition of polynomials has been a topic of great
interest and importance in pure and computer algebra and their applications.
The structure of compositions of (suitably normalized) polynomials f=g(h) over
finite fields is well understood in many cases, but quite poorly when the
degrees of both components are divisible by the characteristic p. This work
investigates the decomposition of polynomials whose degree is a power of p.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1117</identifier>
 <datestamp>2010-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1117</id><created>2010-05-06</created><updated>2010-07-07</updated><authors><author><keyname>Sinclair</keyname><forenames>Alistair</forenames></author><author><keyname>Stauffer</keyname><forenames>Alexandre</forenames></author></authors><title>Mobile Geometric Graphs, and Detection and Communication Problems in
  Mobile Wireless Networks</title><categories>math.PR cs.DM</categories><comments>This is a slightly updated version, with some proofs re-organized</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Static wireless networks are by now quite well understood mathematically
through the random geometric graph model. By contrast, there are relatively few
rigorous results on the practically important case of mobile networks, in which
the nodes move over time; moreover, these results often make unrealistic
assumptions about node mobility such as the ability to make very large jumps.
In this paper we consider a realistic model for mobile wireless networks which
we call mobile geometric graphs, and which is a natural extension of the random
geometric graph model. We study two fundamental questions in this model:
detection (the time until a given &quot;target&quot; point - which may be either fixed or
moving - is detected by the network), and percolation (the time until a given
node is able to communicate with the giant component of the network). For
detection, we show that the probability that the detection time exceeds t is
\exp(-\Theta(t/\log t)) in two dimensions, and \exp(-\Theta(t)) in three or
more dimensions, under reasonable assumptions about the motion of the target.
For percolation, we show that the probability that the percolation time exceeds
t is \exp(-\Omega(t^\frac{d}{d+2})) in all dimensions d\geq 2. We also give a
sample application of this result by showing that the time required to
broadcast a message through a mobile network with n nodes above the threshold
density for existence of a giant component is O(\log^{1+2/d} n) with high
probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1120</identifier>
 <datestamp>2010-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1120</id><created>2010-05-06</created><updated>2010-06-18</updated><authors><author><keyname>Ganguly</keyname><forenames>Sumit</forenames></author></authors><title>Estimating small moments of data stream in nearly optimal space-time</title><categories>cs.DS cs.LG</categories><comments>Withdrawn due to error in analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For each $p \in (0,2]$, we present a randomized algorithm that returns an
$\epsilon$-approximation of the $p$th frequency moment of a data stream $F_p =
\sum_{i = 1}^n \abs{f_i}^p$. The algorithm requires space $O(\epsilon^{-2} \log
(mM)(\log n))$ and processes each stream update using time $O((\log n) (\log
\epsilon^{-1}))$. It is nearly optimal in terms of space (lower bound
$O(\epsilon^{-2} \log (mM))$ as well as time and is the first algorithm with
these properties. The technique separates heavy hitters from the remaining
items in the stream using an appropriate threshold and estimates the
contribution of the heavy hitters and the light elements to $F_p$ separately. A
key component is the design of an unbiased estimator for $\abs{f_i}^p$ whose
data structure has low update time and low variance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1121</identifier>
 <datestamp>2010-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1121</id><created>2010-05-06</created><updated>2010-09-12</updated><authors><author><keyname>Abhishek</keyname><forenames>Vineet</forenames></author><author><keyname>Hajek</keyname><forenames>Bruce</forenames></author></authors><title>Efficiency Loss in Revenue Optimal Auctions</title><categories>cs.GT</categories><comments>25 pages and 1 figure. A short version of this will appear in the
  49th IEEE Conference on Decision and Control (CDC), 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study efficiency loss in Bayesian revenue optimal auctions. We quantify
this as the worst case ratio of loss in the realized social welfare to the
social welfare that can be realized by an efficient auction. Our focus is on
auctions with single-parameter buyers and where buyers' valuation sets are
finite. For binary valued single-parameter buyers with independent (not
necessarily identically distributed) private valuations, we show that the worst
case efficiency loss ratio (ELR) is no worse than it is with only one buyer;
moreover, it is at most 1/2. Moving beyond the case of binary valuations but
restricting to single item auctions, where buyers' private valuations are
independent and identically distributed, we obtain bounds on the worst case ELR
as a function of number of buyers, cardinality of buyers' valuation set, and
ratio of maximum to minimum possible values that buyers can have for the item.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1122</identifier>
 <datestamp>2010-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1122</id><created>2010-05-06</created><updated>2010-05-28</updated><authors><author><keyname>Ganguly</keyname><forenames>Sumit</forenames></author><author><keyname>Kar</keyname><forenames>Purushottam</forenames></author></authors><title>Estimating small frequency moments of data stream: a characteristic
  function approach</title><categories>cs.DS</categories><comments>Withdrawn due to an error in proof of Lemma 2.2 (acknowledgement:
  Jelani Nelson, David Woodruff)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A data stream is viewed as a sequence of $M$ updates of the form
$(\text{index},i,v)$ to an $n$-dimensional integer frequency vector $f$, where
the update changes $f_i$ to $f_i + v$, and $v$ is an integer and assumed to be
in $\{-m, ..., m\}$. The $p$th frequency moment $F_p$ is defined as
$\sum_{i=1}^n \abs{f_i}^p$. We consider the problem of estimating $F_p$ to
within a multiplicative approximation factor of $1\pm \epsilon$, for $p \in
[0,2]$. Several estimators have been proposed for this problem, including
Indyk's median estimator \cite{indy:focs00}, Li's geometric means estimator
\cite{pinglib:2006}, an \Hss-based estimator \cite{gc:random07}. The first two
estimators require space $\tilde{O}(\epsilon^{-2})$, where the $\tilde{O}$
notation hides polylogarithmic factors in $\epsilon^{-1}, m, n$ and $M$.
Recently, Kane, Nelson and Woodruff in \cite{knw:soda10} present a
space-optimal and novel estimator, called the log-cosine estimator. In this
paper, we present an elementary analysis of the log-cosine estimator in a
stand-alone setting. The analysis in \cite{knw:soda10} is more complicated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1141</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1141</id><created>2010-05-07</created><updated>2010-06-02</updated><authors><author><keyname>Bodirsky</keyname><forenames>Manuel</forenames></author><author><keyname>Jonsson</keyname><forenames>Peter</forenames></author><author><keyname>von Oertzen</keyname><forenames>Timo</forenames></author></authors><title>Horn versus full first-order: complexity dichotomies in algebraic
  constraint satisfaction</title><categories>cs.LO cs.CC math.LO</categories><comments>15 pages; in this version, some editing mistakes in the conclusion
  have been fixed</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We study techniques for deciding the computational complexity of
infinite-domain constraint satisfaction problems. For certain fundamental
algebraic structures Delta, we prove definability dichotomy theorems of the
following form: for every first-order expansion Gamma of Delta, either Gamma
has a quantifier-free Horn definition in Delta, or there is an element d of
Gamma such that all non-empty relations in Gamma contain a tuple of the form
(d,...,d), or all relations with a first-order definition in Delta have a
primitive positive definition in Gamma. The results imply that several families
of constraint satisfaction problems exhibit a complexity dichotomy: the
problems are in P or NP-hard, depending on the choice of the allowed relations.
As concrete examples, we investigate fundamental algebraic constraint
satisfaction problems. The first class consists of all first-order expansions
of (Q;+). The second class is the affine variant of the first class. In both
cases, we obtain full dichotomies by utilising our general methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1143</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1143</id><created>2010-05-07</created><updated>2010-07-30</updated><authors><author><keyname>Nest</keyname><forenames>M. Van den</forenames></author></authors><title>Quantum matchgate computations and linear threshold gates</title><categories>quant-ph cs.CC</categories><journal-ref>Proc. R. Soc. A 467, 821-840 (2011)</journal-ref><doi>10.1098/rspa.2010.0332</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of matchgates is of interest in various areas in physics and
computer science. Matchgates occur in e.g. the study of fermions and spin
chains, in the theory of holographic algorithms and in several recent works in
quantum computation. In this paper we completely characterize the class of
boolean functions computable by unitary two-qubit matchgate circuits with some
probability of success. We show that this class precisely coincides with that
of the linear threshold gates. The latter is a fundamental family which appears
in several fields, such as the study of neural networks. Using the above
characterization, we further show that the power of matchgate circuits is
surprisingly trivial in those cases where the computation is to succeed with
high probability. In particular, the only functions that are
matchgate-computable with success probability greater than 3/4 are functions
depending on only a single bit of the input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1155</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1155</id><created>2010-05-07</created><authors><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Yang</keyname><forenames>Chenyang</forenames></author></authors><title>Decentralized Estimation over Orthogonal Multiple-access Fading Channels
  in Wireless Sensor Networks - Optimal and Suboptimal Estimators</title><categories>cs.IT math.IT stat.ME</categories><doi>10.1186/1687-6180-2011-132</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal and suboptimal decentralized estimators in wireless sensor networks
(WSNs) over orthogonal multiple-access fading channels are studied in this
paper. Considering multiple-bit quantization before digital transmission, we
develop maximum likelihood estimators (MLEs) with both known and unknown
channel state information (CSI). When training symbols are available, we derive
a MLE that is a special case of the MLE with unknown CSI. It implicitly uses
the training symbols to estimate the channel coefficients and exploits the
estimated CSI in an optimal way. To reduce the computational complexity, we
propose suboptimal estimators. These estimators exploit both signal and data
level redundant information to improve the estimation performance. The proposed
MLEs reduce to traditional fusion based or diversity based estimators when
communications or observations are perfect. By introducing a general message
function, the proposed estimators can be applied when various analog or digital
transmission schemes are used. The simulations show that the estimators using
digital communications with multiple-bit quantization outperform the estimator
using analog-and-forwarding transmission in fading channels. When considering
the total bandwidth and energy constraints, the MLE using multiple-bit
quantization is superior to that using binary quantization at medium and high
observation signal-to-noise ratio levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1194</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1194</id><created>2010-05-07</created><updated>2010-05-10</updated><authors><author><keyname>Bringer</keyname><forenames>Julien</forenames></author><author><keyname>Chabanne</keyname><forenames>Herv&#xe9;</forenames></author></authors><title>Negative Databases for Biometric Data</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Negative databases - negative representations of a set of data - have been
introduced in 2004 to protect the data they contain. Today, no solution is
known to constitute biometric negative databases. This is surprising as
biometric applications are very demanding of such protection for privacy
reasons. The main difficulty comes from the fact that biometric captures of the
same trait give different results and comparisons of the stored reference with
the fresh captured biometric data has to take into account this variability. In
this paper, we give a first answer to this problem by exhibiting a way to
create and exploit biometric negative databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1195</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1195</id><created>2010-05-07</created><authors><author><keyname>Dubois</keyname><forenames>Swan</forenames><affiliation>LIP6, INRIA Rocquencourt</affiliation></author><author><keyname>Masuzawa</keyname><forenames>Toshimitsu</forenames><affiliation>Department of Information and Computer sciences Osaka University</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LIP6</affiliation></author></authors><title>The Impact of Topology on Byzantine Containment in Stabilization</title><categories>cs.DC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-stabilization is an versatile approach to fault-tolerance since it
permits a distributed system to recover from any transient fault that
arbitrarily corrupts the contents of all memories in the system. Byzantine
tolerance is an attractive feature of distributed system that permits to cope
with arbitrary malicious behaviors. We consider the well known problem of
constructing a maximum metric tree in this context. Combining these two
properties prove difficult: we demonstrate that it is impossible to contain the
impact of Byzantine nodes in a self-stabilizing context for maximum metric tree
construction (strict stabilization). We propose a weaker containment scheme
called topology-aware strict stabilization, and present a protocol for
computing maximum metric trees that is optimal for this scheme with respect to
impossibility result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1200</identifier>
 <datestamp>2011-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1200</id><created>2010-05-07</created><updated>2011-02-16</updated><authors><author><keyname>Lobo</keyname><forenames>Fernando G.</forenames></author><author><keyname>Zacarias</keyname><forenames>Marielba</forenames></author><author><keyname>Condado</keyname><forenames>Paulo A.</forenames></author><author><keyname>Rom&#xe3;o</keyname><forenames>Teresa</forenames></author><author><keyname>Godinho</keyname><forenames>Rui</forenames></author><author><keyname>Moreno</keyname><forenames>Manuel</forenames></author></authors><title>Evaluating Accessible Synchronous CMC Applications</title><categories>cs.HC</categories><comments>15 pages. Modifications with respect to v2: Added references [5] and
  [10], Renamed section 5 to 'Disability Test', changed names 'mystery1' and
  'mystery2' to 'advisor D' and 'advisor ND', elaborated on the discussion (sec
  7.3) and conclusions (sec 8)</comments><acm-class>H.4.3; H.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a more comprehensive evaluation methodology to measure
the usability and user experience qualities of accessible synchronous
computer-mediated communication applications. The methodology goes beyond
current practices by evaluating how the interaction between a user and a
product influences the user experience of those at the other endpoint of the
communication channel. A major contribution is given with the proposal of a
user test where one of the participants tries to guess whether the other
participant has a disability or not. The proposed test is inspired in the
Turing Test, and is a consequence of user requirements elicited from a group of
individuals with motor and speech disabilities. These ideas are tested and
validated with two examples of synchronous communication applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1206</identifier>
 <datestamp>2010-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1206</id><created>2010-05-07</created><authors><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author></authors><title>A Simple Approach to Error Reconciliation in Quantum Key Distribution</title><categories>cs.DS quant-ph</categories><comments>19 pages. Presented at the 53rd Annual Meeting of the Australian
  Mathematical Society, Adelaide, Oct 1, 2009. See also
  http://wwwmaths.anu.edu.au/~brent/pub/pub239.html</comments><msc-class>81P94 (Primary), 94A60 (Secondary)</msc-class><acm-class>E.3; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the error reconciliation phase in quantum key distribution (QKD)
and analyse a simple scheme in which blocks with bad parity (that is, blocks
containing an odd number of errors) are discarded. We predict the performance
of this scheme and show, using a simulation, that the prediction is accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1213</identifier>
 <datestamp>2011-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1213</id><created>2010-05-07</created><updated>2011-01-31</updated><authors><author><keyname>Cohen</keyname><forenames>Julien</forenames><affiliation>LINA</affiliation></author><author><keyname>Douence</keyname><forenames>R&#xe9;mi</forenames><affiliation>LINA, INRIA - EMN</affiliation></author></authors><title>Views, Program Transformations, and the Evolutivity Problem in a
  Functional Language</title><categories>cs.SE cs.PL</categories><comments>19 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on an experience to support multiple views of programs to solve the
tyranny of the dominant decomposition in a functional setting. We consider two
possible architectures in Haskell for the classical example of the expression
problem. We show how the Haskell Refactorer can be used to transform one view
into the other, and the other way back. That transformation is automated and we
discuss how the Haskell Refactorer has been adapted to be able to support this
automated transformation. Finally, we compare our implementation of views with
some of the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1252</identifier>
 <datestamp>2010-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1252</id><created>2010-05-07</created><authors><author><keyname>Litvinov</keyname><forenames>G. L.</forenames></author><author><keyname>Maslov</keyname><forenames>V. P.</forenames></author><author><keyname>Rodionov</keyname><forenames>A. Ya.</forenames></author><author><keyname>Sobolevski</keyname><forenames>A. N.</forenames></author></authors><title>Universal algorithms, mathematics of semirings and parallel computations</title><categories>math.NA cs.DS cs.MS cs.NE</categories><comments>36 pages, 1 figure. To appear in Springer Lecture Notes in
  Computational Science and Engineering.</comments><msc-class>15A80 (Primary), 20M99, 65F99, 65K10, 68N19, 65G30, 68W10, 68N30
  (Secondary), 68Q65, 49M99, 12K10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a survey paper on applications of mathematics of semirings to
numerical analysis and computing. Concepts of universal algorithm and generic
program are discussed. Relations between these concepts and mathematics of
semirings are examined. A very brief introduction to mathematics of semirings
(including idempotent and tropical mathematics) is presented. Concrete
applications to optimization problems, idempotent linear algebra and interval
analysis are indicated. It is known that some nonlinear problems (and
especially optimization problems) become linear over appropriate semirings with
idempotent addition (the so-called idempotent superposition principle). This
linearity over semirings is convenient for parallel computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1284</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1284</id><created>2010-05-07</created><updated>2012-02-28</updated><authors><author><keyname>Ozgur</keyname><forenames>Ayfer</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author></authors><title>Approximately achieving Gaussian relay network capacity with lattice
  codes</title><categories>cs.IT math.IT</categories><comments>ISIT 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, it has been shown that a quantize-map-and-forward scheme
approximately achieves (within a constant number of bits) the Gaussian relay
network capacity for arbitrary topologies. This was established using Gaussian
codebooks for transmission and random mappings at the relays. In this paper, we
show that the same approximation result can be established by using lattices
for transmission and quantization along with structured mappings at the relays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1292</identifier>
 <datestamp>2011-07-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1292</id><created>2010-05-07</created><authors><author><keyname>Frasca</keyname><forenames>Paolo</forenames></author><author><keyname>Fagnani</keyname><forenames>Fabio</forenames></author></authors><title>Broadcast gossip averaging algorithms: interference and asymptotical
  error in large networks</title><categories>math.OC cs.SY</categories><comments>22 pages, 6 figures. Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study two related iterative randomized algorithms for
distributed computation of averages. The first one is the recently proposed
Broadcast Gossip Algorithm, in which at each iteration one randomly selected
node broadcasts its own state to its neighbors. The second algorithm is a novel
de-synchronized version of the previous one, in which at each iteration every
node is allowed to broadcast, with a given probability: hence this algorithm is
affected by interference among messages. Both algorithms are proved to
converge, and their performance is evaluated in terms of rate of convergence
and asymptotical error: focusing on the behavior for large networks, we
highlight the role of topology and design parameters on the performance.
Namely, we show that on fully-connected graphs the rate is bounded away from
one, whereas the asymptotical error is bounded away from zero. On the contrary,
on a wide class of locally-connected graphs, the rate goes to one and the
asymptotical error goes to zero, as the size of the network grows larger.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1311</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1311</id><created>2010-05-07</created><updated>2014-12-13</updated><authors><author><keyname>Harper</keyname><forenames>Marc</forenames></author></authors><title>A Population-centric Approach to the Beauty Contest Game</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An population-centric analysis for a version of the p-beauty contest game is
given for the two-player, finite population, and infinite population cases.
Winning strategies are characterized in terms of iterative thinking relative to
the population. To win the game one needs to iterate more times than the
ambient population, but not too many more times depending on the population
size and the value of p.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1320</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1320</id><created>2010-05-08</created><authors><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author></authors><title>The myth of equidistribution for high-dimensional simulation</title><categories>cs.MS cs.DS cs.NA math.NA math.NT</categories><comments>8 pages. Based on material presented at a Workshop on High
  Dimensional Approximation held at the Australian National University,
  Canberra, 19 February 2007. For further details, see
  http://wwwmaths.anu.edu.au/~brent/pub/pub240.html</comments><msc-class>65C10 (Primary) 11K36, 11K38, 11K45 (Secondary)</msc-class><acm-class>G.3; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A pseudo-random number generator (RNG) might be used to generate w-bit random
samples in d dimensions if the number of state bits is at least dw. Some RNGs
perform better than others and the concept of equidistribution has been
introduced in the literature in order to rank different RNGs. We define what it
means for a RNG to be (d,w)-equidistributed, and then argue that
(d,w)-equidistribution is not necessarily a desirable property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1327</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1327</id><created>2010-05-08</created><authors><author><keyname>Legay</keyname><forenames>Axel</forenames></author><author><keyname>Delahaye</keyname><forenames>Benoit</forenames></author></authors><title>Statistical Model Checking : An Overview</title><categories>cs.LO</categories><comments>none</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantitative properties of stochastic systems are usually specified in logics
that allow one to compare the measure of executions satisfying certain temporal
properties with thresholds. The model checking problem for stochastic systems
with respect to such logics is typically solved by a numerical approach that
iteratively computes (or approximates) the exact measure of paths satisfying
relevant subformulas; the algorithms themselves depend on the class of systems
being analyzed as well as the logic used for specifying the properties. Another
approach to solve the model checking problem is to \emph{simulate} the system
for finitely many runs, and use \emph{hypothesis testing} to infer whether the
samples provide a \emph{statistical} evidence for the satisfaction or violation
of the specification. In this short paper, we survey the statistical approach,
and outline its main advantages in terms of efficiency, uniformity, and
simplicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1339</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1339</id><created>2010-05-08</created><authors><author><keyname>Liu</keyname><forenames>Xi</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Coordination and Bargaining over the Gaussian Interference Channel</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures, to appear in Proceedings of IEEE ISIT2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers coordination and bargaining between two selfish users
over a Gaussian interference channel using game theory. The usual information
theoretic approach assumes full cooperation among users for codebook and rate
selection. In the scenario investigated here, each selfish user is willing to
coordinate its actions only when an incentive exists and benefits of
cooperation are fairly allocated. To improve communication rates, the two users
are allowed to negotiate for the use of a simple Han-Kobayashi type scheme with
fixed power split and conditions for which users have incentives to cooperate
are identified. The Nash bargaining solution (NBS) is used as a tool to get
fair information rates. The operating point is obtained as a result of an
optimization problem and compared with a TDM-based one in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1340</identifier>
 <datestamp>2010-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1340</id><created>2010-05-08</created><updated>2010-05-16</updated><authors><author><keyname>Gwizdka</keyname><forenames>Jacek</forenames></author></authors><title>Distribution of Cognitive Load in Web Search</title><categories>cs.HC cs.IR</categories><comments>To appear in the Journal of the American Society for Information
  Science &amp; Technology (JASIST)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The search task and the system both affect the demand on cognitive resources
during information search. In some situations, the demands may become too high
for a person. This article has a three-fold goal. First, it presents and
critiques methods to measure cognitive load. Second, it explores the
distribution of load across search task stages. Finally, it seeks to improve
our understanding of factors affecting cognitive load levels in information
search. To this end, a controlled Web search experiment with forty-eight
participants was conducted. Interaction logs were used to segment search tasks
semi-automatically into task stages. Cognitive load was assessed using a new
variant of the dual-task method. Average cognitive load was found to vary by
search task stages. It was significantly higher during query formulation and
user description of a relevant document as compared to examining search results
and viewing individual documents. Semantic information shown next to the search
results lists in one of the studied interfaces was found to decrease mental
demands during query formulation and examination of the search results list.
These findings demonstrate that changes in dynamic cognitive load can be
detected within search tasks. Dynamic assessment of cognitive load is of core
interest to information science because it enriches our understanding of
cognitive demands imposed on people engaged in the search process by a task and
the interactive information retrieval system employed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1349</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1349</id><created>2010-05-08</created><authors><author><keyname>Al-Bashabsheh</keyname><forenames>Ali</forenames></author><author><keyname>Mao</keyname><forenames>Yongyi</forenames></author><author><keyname>Yongacoglu</keyname><forenames>Abbas</forenames></author></authors><title>On Holant Theorem and Its Proof</title><categories>cs.IT math.IT</categories><comments>25th Queen's Biennial Symposium on Communications.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Holographic algorithms are a recent breakthrough in computer science and has
found applications in information theory. This paper provides a proof to the
central component of holographic algorithms, namely, the Holant theorem.
Compared with previous works, the proof appears simpler and more direct. Along
the proof, we also develop a mathematical tool, which we call c-tensor. We
expect the notion of c-tensor may be applicable over a wide range of analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1364</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1364</id><created>2010-05-09</created><authors><author><keyname>Akin</keyname><forenames>Sami</forenames></author><author><keyname>Gursoy</keyname><forenames>Mustafa Cenk</forenames></author></authors><title>Cognitive Radio Transmission under QoS Constraints and Interference
  Limitations</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the performance of cognitive transmission under quality of
service (QoS)constraints and interference limitations is studied. Cognitive
secondary users are assumed to initially perform sensing over multiple
frequency bands (or equivalently channels) to detect the activities of primary
users. Subsequently, they perform transmission in a single channel at variable
power and rates depending on the channel sensing decisions and the fading
environment. A state transition model is constructed to model this cognitive
operation. Statistical limitations on the buffer lengths are imposed to take
into account the QoS constraints of the cognitive secondary users. Under such
QoS constraints and limitations on the interference caused to the primary
users, the maximum throughput is identified by finding the effective capacity
of the cognitive radio channel. Optimal power allocation strategies are
obtained and the optimal channel selection criterion is identified. The
intricate interplay between effective capacity, interference and QoS
constraints, channel sensing parameters and reliability, fading, and the number
of available frequency bands is investigated through numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1365</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1365</id><created>2010-05-09</created><authors><author><keyname>Jayaprakasam</keyname><forenames>ArunKumar</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author><author><keyname>Murthy</keyname><forenames>Chandra R.</forenames></author><author><keyname>Narayanan</keyname><forenames>Prashant</forenames></author></authors><title>Cooperative Sequential Spectrum Sensing Algorithms for OFDM</title><categories>cs.IT math.IT</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of spectrum sensing in cognitive radio
networks when the primary user employs Orthogonal Frequency Division
Multiplexing (OFDM). We develop cooperative sequential detection algorithms
based on energy detectors and the autocorrelation property of cyclic prefix
(CP) used in OFDM systems and compare their performances. We show that
sequential detection provides much better performance than the traditional
fixed sample size (snapshot) based detectors. We also study the effect of model
uncertainties such as timing and frequency offset, IQ-imbalance and uncertainty
in noise and transmit power on the performance of the detectors. We modify the
detectors to mitigate the effects of these impairments. The performance of the
proposed algorithms are studied via simulations. It is shown that energy
detector performs significantly better than the CP-based detector, except in
case of a snapshot detector with noise power uncertainty. Also, unlike for the
CP-based detector, most of the above mentioned impairments have no effect on
the energy detector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1369</identifier>
 <datestamp>2011-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1369</id><created>2010-05-09</created><updated>2011-12-24</updated><authors><author><keyname>Weinstein</keyname><forenames>Amit</forenames></author></authors><title>Simultaneous communication in noisy channels</title><categories>cs.IT math.IT</categories><journal-ref>Information Theory, IEEE Transactions on, Vol. 57, No. 10.
  (October 2011), pp. 6455-6462</journal-ref><doi>10.1109/TIT.2011.2165798</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sender wishes to broadcast a message of length $n$ over an alphabet to $r$
users, where each user $i$, $1 \leq i \leq r$ should be able to receive one of
$m_i$ possible messages. The broadcast channel has noise for each of the users
(possibly different noise for different users), who cannot distinguish between
some pairs of letters. The vector $(m_1, m_2,...s, m_r)_{(n)}$ is said to be
feasible if length $n$ encoding and decoding schemes exist enabling every user
to decode his message. A rate vector $(R_1, R_2,..., R_r)$ is feasible if there
exists a sequence of feasible vectors $(m_1, m_2,..., m_r)_{(n)}$ such that
$R_i = \lim_{n \mapsto \infty} \frac {\log_2 m_i} {n}, {for all} i$. We
determine the feasible rate vectors for several different scenarios and
investigate some of their properties. An interesting case discussed is when one
user can only distinguish between all the letters in a subset of the alphabet.
Tight restrictions on the feasible rate vectors for some specific noise types
for the other users are provided. The simplest non-trivial cases of two users
and alphabet of size three are fully characterized. To this end a more general
previously known result, to which we sketch an alternative proof, is used. This
problem generalizes the study of the Shannon capacity of a graph, by
considering more than a single user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1391</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1391</id><created>2010-05-09</created><authors><author><keyname>Dominguez-Montes</keyname><forenames>Juan</forenames></author></authors><title>Solution to the Counterfeit Coin Problem and its Generalization</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work deals with a classic problem: &quot;Given a set of coins among which
there is a counterfeit coin of a different weight, find this counterfeit coin
using ordinary balance scales, with the minimum number of weighings possible,
and indicate whether it weighs less or more than the rest&quot;. The method proposed
here not only calculates the minimum number of weighings necessary, but also
indicates how to perform these weighings, it is easily mechanizeable and valid
for any number of coins. Instructions are also given as to how to generalize
the procedure to include cases where there is more than one counterfeit coin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1392</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1392</id><created>2010-05-09</created><authors><author><keyname>Fox</keyname><forenames>Jacob</forenames></author><author><keyname>Gromov</keyname><forenames>Mikhail</forenames></author><author><keyname>Lafforgue</keyname><forenames>Vincent</forenames></author><author><keyname>Naor</keyname><forenames>Assaf</forenames></author><author><keyname>Pach</keyname><forenames>Janos</forenames></author></authors><title>Overlap properties of geometric expanders</title><categories>math.CO cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The {\em overlap number} of a finite $(d+1)$-uniform hypergraph $H$ is
defined as the largest constant $c(H)\in (0,1]$ such that no matter how we map
the vertices of $H$ into $\R^d$, there is a point covered by at least a
$c(H)$-fraction of the simplices induced by the images of its hyperedges.
In~\cite{Gro2}, motivated by the search for an analogue of the notion of graph
expansion for higher dimensional simplicial complexes, it was asked whether or
not there exists a sequence $\{H_n\}_{n=1}^\infty$ of arbitrarily large
$(d+1)$-uniform hypergraphs with bounded degree, for which $\inf_{n\ge 1}
c(H_n)&gt;0$. Using both random methods and explicit constructions, we answer this
question positively by constructing infinite families of $(d+1)$-uniform
hypergraphs with bounded degree such that their overlap numbers are bounded
from below by a positive constant $c=c(d)$. We also show that, for every $d$,
the best value of the constant $c=c(d)$ that can be achieved by such a
construction is asymptotically equal to the limit of the overlap numbers of the
complete $(d+1)$-uniform hypergraphs with $n$ vertices, as
$n\rightarrow\infty$. For the proof of the latter statement, we establish the
following geometric partitioning result of independent interest. For any $d$
and any $\epsilon&gt;0$, there exists $K=K(\epsilon,d)\ge d+1$ satisfying the
following condition. For any $k\ge K$, for any point $q \in \mathbb{R}^d$ and
for any finite Borel measure $\mu$ on $\mathbb{R}^d$ with respect to which
every hyperplane has measure $0$, there is a partition $\mathbb{R}^d=A_1 \cup
\ldots \cup A_{k}$ into $k$ measurable parts of equal measure such that all but
at most an $\epsilon$-fraction of the $(d+1)$-tuples
$A_{i_1},\ldots,A_{i_{d+1}}$ have the property that either all simplices with
one vertex in each $A_{i_j}$ contain $q$ or none of these simplices contain
$q$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1395</identifier>
 <datestamp>2011-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1395</id><created>2010-05-09</created><updated>2010-09-16</updated><authors><author><keyname>Ermann</keyname><forenames>L.</forenames></author><author><keyname>Chepelianskii</keyname><forenames>A. D.</forenames></author><author><keyname>Shepelyansky</keyname><forenames>D. L.</forenames></author></authors><title>Fractal Weyl law for Linux Kernel Architecture</title><categories>cs.CE cond-mat.dis-nn nlin.CD physics.data-an</categories><comments>RevTex 6 pages, 7 figs, linked to arXiv:1003.5455[cs.SE]. Research at
  http://www.quantware.ups-tlse.fr/, Improved version, changed format</comments><journal-ref>Eur. Phys. J. B 79, 115-120 (2011)</journal-ref><doi>10.1140/epjb/e2010-10774-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the properties of spectrum and eigenstates of the Google matrix of a
directed network formed by the procedure calls in the Linux Kernel. Our results
obtained for various versions of the Linux Kernel show that the spectrum is
characterized by the fractal Weyl law established recently for systems of
quantum chaotic scattering and the Perron-Frobenius operators of dynamical
maps. The fractal Weyl exponent is found to be $\nu \approx 0.63$ that
corresponds to the fractal dimension of the network $d \approx 1.2$. The
eigenmodes of the Google matrix of Linux Kernel are localized on certain
principal nodes. We argue that the fractal Weyl law should be generic for
directed networks with the fractal dimension $d&lt;2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1417</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1417</id><created>2010-05-09</created><authors><author><keyname>Elfoutayeni</keyname><forenames>Youssef</forenames><affiliation>LMDP</affiliation></author><author><keyname>Khaladi</keyname><forenames>Mohamed</forenames><affiliation>LMDP</affiliation></author></authors><title>Using vector divisions in solving linear complementarity problem</title><categories>cs.NA math.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The linear complementarity problem is to find vector $z$ in $\mathrm{IR}^{n}$
satisfying $z^{T}(Mz+q)=0$, $Mz+q\geqslant0,$ $z\geqslant0$, where $M$ as a
matrix and $q$ as a vector, are given data; this problem becomes in present the
subject of much important research because it arises in many areas and it
includes important fields, we cite for example the linear and nonlinear
programming, the convex quadratic programming and the variational inequalities
problems, ... It is known that the linear complementarity problem is completely
equivalent to solving nonlinear equation $F(x)=0$ with $F$ is a function from
$\mathrm{IR}^{n}$ into itself defined by $F(x)=(M+I)x+(M-I)|x|+q$. In this
paper we propose a globally convergent hybrid algorithm for solving this
equation; this method is based on an algorithm given by Shi \cite{Y. Shi}, he
uses vector divisions with the secant method; but for using this method we must
have a function continuous with partial derivatives on an open set of
$\mathrm{IR}^{n}$; so we built a sequence of functions $\tilde{F}(p,x)\in
C^{\infty}$ which converges uniformly to the function $F(x)$; and we show that
finding the zero of the function $F$ is completely equivalent to finding the
zero of the sequence of the functions $\tilde{F}(p,x)$. We close our paper with
some numerical simulation examples to illustrate our theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1418</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1418</id><created>2010-05-09</created><updated>2014-05-16</updated><authors><author><keyname>Shachar</keyname><forenames>Amir</forenames></author></authors><title>On a Relation Between the Integral Image Algorithm and Calculus</title><categories>cs.DM math.CA math.HO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Integral Image algorithm is often applied in tasks that require efficient
integration over images, such as object detection. In this paper we discuss
theoretical aspects of the algorithm's continuous version. We suggest to define
the coefficients at the formulation of the algorithm by applying a novel kind
of discrete derivative. Based on that operator we build a novel integration
method over curves in the plane, and apply it in a theorem that extends the
algorithm to general continuous domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1425</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1425</id><created>2010-05-09</created><authors><author><keyname>Shepherd</keyname><forenames>Daniel James</forenames></author></authors><title>Quantum Complexity: restrictions on algorithms and architectures</title><categories>cs.CC quant-ph</categories><comments>137 pages, 10 figs.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dissertation submitted to the University of Bristol in accordance with the
requirements of the degree of Doctor of Philosophy (PhD) in the Faculty of
Engineering, Department of Computer Science, July 2009.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1428</identifier>
 <datestamp>2010-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1428</id><created>2010-05-09</created><authors><author><keyname>Manimala</keyname><forenames>Jose Mathew</forenames></author></authors><title>RAmM Algorithm(Simplex)</title><categories>cs.CR</categories><comments>This is unpublished work and has no references because the work has
  been purely based on my own research and I have used no other papers for
  this.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evolution of encryption algorithms have led to the development of very
complicated and highly versatile algorithms that sacrifice efficiency for
better and harder to decrypt results. But by the application of a genetic
schema to the encryption of data, a new structure can be created. Genetic
methods and procedures are lethal in the way they handle and manipulate data.
The RAmM algorithm uses four genetic operations that have been developed
specifically for encryption of data. The operations are Replication,
Augmentation, Mutation and Multiplication. The proper application of these
methods according to the rules that have been found to be the best for getting
optimal and correct results produces a &quot;fingerprint&quot; that is unique to a pair
of &lt;data , key&gt;. This means that every single data entry can only be decrypted
by using the correct set of key. The application of the RAmM algorithm is in
the field of image encryption and restoration. The boundary and the pixel
values are separately encrypted to produce a very genuine sequence that is
never understood to be an image. The beauty of the procedure is that the entire
image can be reproduced without any color loss or loss of pixel quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1454</identifier>
 <datestamp>2010-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1454</id><created>2010-05-10</created><updated>2010-06-25</updated><authors><author><keyname>Kammerer</keyname><forenames>Jean-Gabriel</forenames></author><author><keyname>Lercier</keyname><forenames>Reynald</forenames></author><author><keyname>Renault</keyname><forenames>Gu&#xe9;na&#xeb;l</forenames></author></authors><title>Encoding points on hyperelliptic curves over finite fields in
  deterministic polynomial time</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present families of (hyper)elliptic curve which admit an efficient
deterministic encoding function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1466</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1466</id><created>2010-05-10</created><authors><author><keyname>Cohen</keyname><forenames>Julien</forenames><affiliation>LINA</affiliation></author><author><keyname>Giavitto</keyname><forenames>Jean-Louis</forenames><affiliation>IBISC</affiliation></author><author><keyname>Michel</keyname><forenames>Olivier</forenames><affiliation>LACL</affiliation></author></authors><title>Variable elimination for building interpreters</title><categories>cs.SE cs.PL</categories><comments>33 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we build an interpreter by reusing host language functions
instead of recoding mechanisms of function application that are already
available in the host language (the language which is used to build the
interpreter). In order to transform user-defined functions into host language
functions we use combinatory logic : lambda-abstractions are transformed into a
composition of combinators. We provide a mechanically checked proof that this
step is correct for the call-by-value strategy with imperative features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1471</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1471</id><created>2010-05-10</created><authors><author><keyname>Schnass</keyname><forenames>Karin</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>Classification via Incoherent Subspaces</title><categories>cs.CV</categories><comments>22 pages, 2 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a new classification framework that can extract
individual features per class. The scheme is based on a model of incoherent
subspaces, each one associated to one class, and a model on how the elements in
a class are represented in this subspace. After the theoretical analysis an
alternate projection algorithm to find such a collection is developed. The
classification performance and speed of the proposed method is tested on the AR
and YaleB databases and compared to that of Fisher's LDA and a recent approach
based on on $\ell_1$ minimisation. Finally connections of the presented scheme
to already existing work are discussed and possible ways of extensions are
pointed out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1475</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1475</id><created>2010-05-10</created><updated>2010-05-11</updated><authors><author><keyname>Loddo</keyname><forenames>Jean-Vincent</forenames></author><author><keyname>Saiu</keyname><forenames>Luca</forenames></author></authors><title>How to correctly prune tropical trees</title><categories>cs.AI cs.DM cs.GT cs.SC</categories><comments>To appear in &quot;Artificial Intelligence and Symbolic Computation,
  2010&quot;.</comments><msc-class>08-04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present tropical games, a generalization of combinatorial min-max games
based on tropical algebras. Our model breaks the traditional symmetry of
rational zero-sum games where players have exactly opposed goals (min vs. max),
is more widely applicable than min-max and also supports a form of pruning,
despite it being less effective than alpha-beta. Actually, min-max games may be
seen as particular cases where both the game and its dual are tropical: when
the dual of a tropical game is also tropical, the power of alpha-beta is
completely recovered. We formally develop the model and prove that the tropical
pruning strategy is correct, then conclude by showing how the problem of
approximated parsing can be modeled as a tropical game, profiting from pruning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1497</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1497</id><created>2010-05-10</created><authors><author><keyname>Chandra</keyname><forenames>Shekhar S.</forenames></author></authors><title>Fast Digital Convolutions using Bit-Shifts</title><categories>cs.NA cs.DM</categories><comments>4 pages, 2 figures, submitted to IEEE Signal Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An exact, one-to-one transform is presented that not only allows digital
circular convolutions, but is free from multiplications and quantisation errors
for transform lengths of arbitrary powers of two. The transform is analogous to
the Discrete Fourier Transform, with the canonical harmonics replaced by a set
of cyclic integers computed using only bit-shifts and additions modulo a prime
number. The prime number may be selected to occupy contemporary word sizes or
to be very large for cryptographic or data hiding applications. The transform
is an extension of the Rader Transforms via Carmichael's Theorem. These
properties allow for exact convolutions that are impervious to numerical
overflow and to utilise Fast Fourier Transform algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1516</identifier>
 <datestamp>2013-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1516</id><created>2010-05-10</created><authors><author><keyname>Leijnen</keyname><forenames>Stefan</forenames></author><author><keyname>Gabora</keyname><forenames>Liane</forenames></author></authors><title>An Agent-based Simulation of the Effectiveness of Creative Leadership</title><categories>cs.MA cs.NE cs.SI</categories><comments>6 pages, submitted to Annual Meeting of the Cognitive Science
  Society, August 11-14, 2010, Portland, Oregon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the effectiveness of creative versus uncreative
leadership using EVOC, an agent-based model of cultural evolution. Each
iteration, each agent in the artificial society invents a new action, or
imitates a neighbor's action. Only the leader's actions can be imitated by all
other agents, referred to as followers. Two measures of creativity were used:
(1) invention-to-imitation ratio, iLeader, which measures how often an agent
invents, and (2) rate of conceptual change, cLeader, which measures how
creative an invention is. High iLeader increased mean fitness of ideas, but
only when creativity of followers was low. High iLeader was associated with
greater diversity of ideas in the early stage of idea generation only. High
cLeader increased mean fitness of ideas in the early stage of idea generation;
in the later stage it decreased idea fitness. Reasons for these findings and
tentative implications for creative leadership in human society are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1518</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1518</id><created>2010-05-10</created><authors><author><keyname>Gabora</keyname><forenames>Liane</forenames></author></authors><title>Recognizability of Individual Creative Style Within and Across Domains:
  Preliminary Studies</title><categories>cs.AI</categories><comments>6 pages, submitted to Annual Meeting of the Cognitive Science
  Society. August 11-14, 2010, Portland, Oregon</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is hypothesized that creativity arises from the self-mending capacity of
an internal model of the world, or worldview. The uniquely honed worldview of a
creative individual results in a distinctive style that is recognizable within
and across domains. It is further hypothesized that creativity is domaingeneral
in the sense that there exist multiple avenues by which the distinctiveness of
one's worldview can be expressed. These hypotheses were tested using art
students and creative writing students. Art students guessed significantly
above chance both which painting was done by which of five famous artists, and
which artwork was done by which of their peers. Similarly, creative writing
students guessed significantly above chance both which passage was written by
which of five famous writers, and which passage was written by which of their
peers. These findings support the hypothesis that creative style is
recognizable. Moreover, creative writing students guessed significantly above
chance which of their peers produced particular works of art, supporting the
hypothesis that creative style is recognizable not just within but across
domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1524</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1524</id><created>2010-05-10</created><authors><author><keyname>Bezzateev</keyname><forenames>Sergey</forenames></author><author><keyname>Shekhunova</keyname><forenames>Natalia</forenames></author></authors><title>Cumulative-Separable Codes</title><categories>cs.IT math.IT</categories><comments>14 pages, 1 figure</comments><msc-class>94B05, 94B65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  q-ary cumulative-separable $\Gamma(L,G^{(j)})$-codes $L=\{ \alpha \in
GF(q^{m}):G(\alpha )\neq 0 \}$ and $G^{(j)}(x)=G(x)^{j}, 1 \leq i\leq q$ are
considered. The relation between different codes from this class is
demonstrated. Improved boundaries of the minimum distance and dimension are
obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1545</identifier>
 <datestamp>2011-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1545</id><created>2010-05-10</created><updated>2011-05-09</updated><authors><author><keyname>Li</keyname><forenames>Yu-Feng</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>Improving Semi-Supervised Support Vector Machines Through Unlabeled
  Instances Selection</title><categories>cs.LG</categories><comments>14 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semi-supervised support vector machines (S3VMs) are a kind of popular
approaches which try to improve learning performance by exploiting unlabeled
data. Though S3VMs have been found helpful in many situations, they may
degenerate performance and the resultant generalization ability may be even
worse than using the labeled data only. In this paper, we try to reduce the
chance of performance degeneration of S3VMs. Our basic idea is that, rather
than exploiting all unlabeled data, the unlabeled instances should be selected
such that only the ones which are very likely to be helpful are exploited,
while some highly risky unlabeled instances are avoided. We propose the
S3VM-\emph{us} method by using hierarchical clustering to select the unlabeled
instances. Experiments on a broad range of data sets over eighty-eight
different settings show that the chance of performance degeneration of
S3VM-\emph{us} is much smaller than that of existing S3VMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1560</identifier>
 <datestamp>2011-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1560</id><created>2010-05-10</created><updated>2010-11-11</updated><authors><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author><author><keyname>Khatri</keyname><forenames>Sunil</forenames></author><author><keyname>Horvath</keyname><forenames>Tamas</forenames></author></authors><title>Computation using Noise-based Logic: Efficient String Verification over
  a Slow Communication Channel</title><categories>cs.IT math.IT physics.gen-ph</categories><comments>Accepted for publication in European Journal of Physics B (November
  10, 2010)</comments><journal-ref>Eur. Phys. J. B 79, 85-90 (2011)</journal-ref><doi>10.1140/epjb/e2010-10399-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Utilizing the hyperspace of noise-based logic, we show two string
verification methods with low communication complexity. One of them is based on
continuum noise-based logic. The other one utilizes noise-based logic with
random telegraph signals where a mathematical analysis of the error probability
is also given. The last operation can also be interpreted as computing
universal hash functions with noise-based logic and using them for string
comparison. To find out with 10^-25 error probability that two strings with
arbitrary length are different (this value is similar to the error probability
of an idealistic gate in today's computer) Alice and Bob need to compare only
83 bits of the noise-based hyperspace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1564</identifier>
 <datestamp>2011-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1564</id><created>2010-05-10</created><updated>2011-03-22</updated><authors><author><keyname>Cooper</keyname><forenames>Colin</forenames></author><author><keyname>Frieze</keyname><forenames>Alan</forenames></author></authors><title>Component structure of the vacant set induced by a random walk on a
  random graph</title><categories>math.CO cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider random walks on several classes of graphs and explore the likely
structure of the vacant set, i.e. the set of unvisited vertices. Let \Gamma(t)
be the subgraph induced by the vacant set of the walk at step t. We show that
for random graphs G_{n,p} (above the connectivity threshold) and for random
regular graphs G_r, r \geq 3, the graph \Gamma(t) undergoes a phase transition
in the sense of the well-known Erdos-Renyi phase transition. Thus for t \leq
(1-\epsilon)t^*, there is a unique giant component, plus components of size
O(log n), and for t \geq (1+\epsilon)t^* all components are of size O(log n).
For G_{n,p} and G_r we give the value of t^*, and the size of \Gamma(t). For
G_r, we also give the degree sequence of \Gamma(t), the size of the giant
component (if any) of \Gamma(t) and the number of tree components of \Gamma(t)
of a given size k=O(log n). We also show that for random digraphs D_{n,p} above
the strong connectivity threshold, there is a similar directed phase
transition. Thus for t\leq (1-\epsilon)t^*, there is a unique strongly
connected giant component, plus strongly connected components of size O(log n),
and for t\geq (1+\epsilon)t^* all strongly connected components are of size
O(log n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1567</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1567</id><created>2010-05-10</created><updated>2010-06-30</updated><authors><author><keyname>Greco</keyname><forenames>Gianluigi</forenames></author><author><keyname>Scarcello</keyname><forenames>Francesco</forenames></author></authors><title>On The Power of Tree Projections: Structural Tractability of Enumerating
  CSP Solutions</title><categories>cs.AI cs.DB</categories><msc-class>68T20</msc-class><acm-class>F.2.2; G.2.1; I.2.8</acm-class><journal-ref>Constraints 18(1): 38-74 (2013)</journal-ref><doi>10.1007/s10601-012-9129-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of deciding whether CSP instances admit solutions has been deeply
studied in the literature, and several structural tractability results have
been derived so far. However, constraint satisfaction comes in practice as a
computation problem where the focus is either on finding one solution, or on
enumerating all solutions, possibly projected to some given set of output
variables. The paper investigates the structural tractability of the problem of
enumerating (possibly projected) solutions, where tractability means here
computable with polynomial delay (WPD), since in general exponentially many
solutions may be computed. A general framework based on the notion of tree
projection of hypergraphs is considered, which generalizes all known
decomposition methods. Tractability results have been obtained both for classes
of structures where output variables are part of their specification, and for
classes of structures where computability WPD must be ensured for any possible
set of output variables. These results are shown to be tight, by exhibiting
dichotomies for classes of structures having bounded arity and where the tree
decomposition method is considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1588</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1588</id><created>2010-05-10</created><updated>2013-11-17</updated><authors><author><keyname>Faugeras</keyname><forenames>Blaise</forenames><affiliation>JAD</affiliation></author><author><keyname>Abda</keyname><forenames>Amel Ben</forenames><affiliation>LAMSIN</affiliation></author><author><keyname>Blum</keyname><forenames>Jacques</forenames><affiliation>JAD</affiliation></author><author><keyname>Boulbe</keyname><forenames>Cedric</forenames><affiliation>JAD</affiliation></author></authors><title>Minimization of an energy error functional to solve a Cauchy problem
  arising in plasma physics: the reconstruction of the magnetic flux in the
  vacuum surrounding the plasma in a Tokamak</title><categories>math.NA cs.NA math.AP math.OC physics.plasm-ph</categories><proxy>ccsd</proxy><journal-ref>ARIMA 15 (2012) 37-60</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A numerical method for the computation of the magnetic flux in the vacuum
surrounding the plasma in a Tokamak is investigated. It is based on the
formulation of a Cauchy problem which is solved through the minimization of an
energy error functional. Several numerical experiments are conducted which show
the efficiency of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1594</identifier>
 <datestamp>2010-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1594</id><created>2010-05-10</created><authors><author><keyname>Kumar</keyname><forenames>K. Raj</forenames></author><author><keyname>Caire</keyname><forenames>Giuseppe</forenames></author></authors><title>Channel State Feedback over the MIMO-MAC</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory, April 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of designing low latency and low complexity schemes
for channel state feedback over the MIMO-MAC (multiple-input multiple-output
multiple access channel). We develop a framework for analyzing this problem in
terms of minimizing the MSE distortion, and come up with separated
source-channel schemes and joint source-channel schemes that perform better
than analog feedback. We also develop a strikingly simple code design based on
scalar quantization and uncoded QAM modulation that achieves the theoretical
asymptotic performance limit of the separated approach with very low complexity
and latency, in the case of single-antenna users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1625</identifier>
 <datestamp>2010-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1625</id><created>2010-05-10</created><authors><author><keyname>Dimitrov</keyname><forenames>Nikolay</forenames></author></authors><title>On Some Results Related to Napoleon's Configurations</title><categories>math.MG cs.IT math.GM math.IT</categories><comments>10 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to give a purely geometric proof of a theorem by
Branko Gr\&quot;unbaum concerning configuration of triangles coming from the
classical Napoleon's theorem in planar Euclidean geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1634</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1634</id><created>2010-05-10</created><updated>2010-09-13</updated><authors><author><keyname>Shah</keyname><forenames>Nihar B.</forenames></author><author><keyname>Rashmi</keyname><forenames>K. V.</forenames></author><author><keyname>Kumar</keyname><forenames>P. Vijay</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author></authors><title>Interference Alignment in Regenerating Codes for Distributed Storage:
  Necessity and Code Constructions</title><categories>cs.IT cs.DC cs.NI math.IT</categories><comments>38 pages, 12 figures, submitted to the IEEE Transactions on
  Information Theory;v3 - The title has been modified to better reflect the
  contributions of the submission. The paper is extensively revised with
  several carefully constructed figures and examples</comments><doi>10.1109/TIT.2011.2178588</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regenerating codes are a class of recently developed codes for distributed
storage that, like Reed-Solomon codes, permit data recovery from any arbitrary
k of n nodes. However regenerating codes possess in addition, the ability to
repair a failed node by connecting to any arbitrary d nodes and downloading an
amount of data that is typically far less than the size of the data file. This
amount of download is termed the repair bandwidth. Minimum storage regenerating
(MSR) codes are a subclass of regenerating codes that require the least amount
of network storage; every such code is a maximum distance separable (MDS) code.
Further, when a replacement node stores data identical to that in the failed
node, the repair is termed as exact.
  The four principal results of the paper are (a) the explicit construction of
a class of MDS codes for d = n-1 &gt;= 2k-1 termed the MISER code, that achieves
the cut-set bound on the repair bandwidth for the exact-repair of systematic
nodes, (b) proof of the necessity of interference alignment in exact-repair MSR
codes, (c) a proof showing the impossibility of constructing linear,
exact-repair MSR codes for d &lt; 2k-3 in the absence of symbol extension, and (d)
the construction, also explicit, of MSR codes for d = k+1. Interference
alignment (IA) is a theme that runs throughout the paper: the MISER code is
built on the principles of IA and IA is also a crucial component to the
non-existence proof for d &lt; 2k-3. To the best of our knowledge, the
constructions presented in this paper are the first, explicit constructions of
regenerating codes that achieve the cut-set bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1635</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1635</id><created>2010-05-10</created><updated>2010-05-26</updated><authors><author><keyname>Bagheri</keyname><forenames>Hossein</forenames></author><author><keyname>Motahari</keyname><forenames>Abolfazl S.</forenames></author><author><keyname>Khandani</keyname><forenames>Amir K.</forenames></author></authors><title>The Approximate Capacity Region of the Gaussian Z-Interference Channel
  with Conferencing Encoders</title><categories>cs.IT math.IT</categories><comments>25 pages, 6 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A two-user Gaussian Z-Interference Channel (GZIC) is considered, in which
encoders are connected through noiseless links with finite capacities. In this
setting, prior to each transmission block the encoders communicate with each
other over the cooperative links. The capacity region and the sum-capacity of
the channel are characterized within 1.71 bits per user and 2 bits in total,
respectively. It is also established that properly sharing the total limited
cooperation capacity between the cooperative links may enhance the achievable
region, even when compared to the case of unidirectional transmitter
cooperation with infinite cooperation capacity. To obtain the results,
genie-aided upper bounds on the sum-capacity and cut-set bounds on the
individual rates are compared with the achievable rate region. In the
interference-limited regime, the achievable scheme enjoys a simple type of
Han-Kobayashi signaling, together with the zero-forcing, and basic relaying
techniques. In the noise-limited regime, it is shown that treating interference
as noise achieves the capacity region up to a single bit per user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1659</identifier>
 <datestamp>2011-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1659</id><created>2010-05-10</created><authors><author><keyname>Karrer</keyname><forenames>Brian</forenames></author><author><keyname>Newman</keyname><forenames>M. E. J.</forenames></author></authors><title>Random graphs containing arbitrary distributions of subgraphs</title><categories>cond-mat.stat-mech cs.DM physics.soc-ph</categories><comments>12 pages, 6 figures, 1 table</comments><journal-ref>Phys. Rev. E 82, 066118 (2010)</journal-ref><doi>10.1103/PhysRevE.82.066118</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional random graph models of networks generate networks that are
locally tree-like, meaning that all local neighborhoods take the form of trees.
In this respect such models are highly unrealistic, most real networks having
strongly non-tree-like neighborhoods that contain short loops, cliques, or
other biconnected subgraphs. In this paper we propose and analyze a new class
of random graph models that incorporates general subgraphs, allowing for
non-tree-like neighborhoods while still remaining solvable for many fundamental
network properties. Among other things we give solutions for the size of the
giant component, the position of the phase transition at which the giant
component appears, and percolation properties for both site and bond
percolation on networks generated by the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1684</identifier>
 <datestamp>2011-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1684</id><created>2010-05-10</created><updated>2011-07-06</updated><authors><author><keyname>Scoville</keyname><forenames>John</forenames></author></authors><title>On Macroscopic Complexity and Perceptual Coding</title><categories>cs.IT cs.AI cs.MM cs.SD math.IT</categories><acm-class>I.4.2; I.5.0; I.5.1; I.2.6; I.2.10; H.5.5; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theoretical limits of 'lossy' data compression algorithms are considered.
The complexity of an object as seen by a macroscopic observer is the size of
the perceptual code which discards all information that can be lost without
altering the perception of the specified observer. The complexity of this
macroscopically observed state is the simplest description of any microstate
comprising that macrostate. Inference and pattern recognition based on
macrostate rather than microstate complexities will take advantage of the
complexity of the macroscopic observer to ignore irrelevant noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1694</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1694</id><created>2010-05-10</created><authors><author><keyname>Feldheim</keyname><forenames>Ohad N.</forenames></author><author><keyname>Hod</keyname><forenames>Rani</forenames></author></authors><title>3/2 Firefighters are not enough</title><categories>cs.DM math.CO</categories><comments>8 pages</comments><msc-class>05C57, 05C63, 90B10, 91A43, 91A50</msc-class><acm-class>G.2.2</acm-class><journal-ref>Discrete Applied Mathematics 161 (2013) 301-306</journal-ref><doi>10.1016/j.dam.2012.08.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The firefighter problem is a monotone dynamic process in graphs that can be
viewed as modeling the use of a limited supply of vaccinations to stop the
spread of an epidemic. In more detail, a fire spreads through a graph, from
burning vertices to their unprotected neighbors. In every round, a small amount
of unburnt vertices can be protected by firefighters. How many firefighters per
turn, on average, are needed to stop the fire from advancing? We prove tight
lower and upper bounds on the amount of firefighters needed to control a fire
in the Cartesian planar grid and in the strong planar grid, resolving two
conjectures of Ng and Raff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1695</identifier>
 <datestamp>2010-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1695</id><created>2010-05-10</created><authors><author><keyname>Gharaibeh</keyname><forenames>Abdullah</forenames></author><author><keyname>Al-Kiswany</keyname><forenames>Samer</forenames></author><author><keyname>Ripeanu</keyname><forenames>Matei</forenames></author></authors><title>CrystalGPU: Transparent and Efficient Utilization of GPU Power</title><categories>cs.OH</categories><comments>8 pages, 8 figures</comments><report-no>NetSysLab-TR-2010-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  General-purpose computing on graphics processing units (GPGPU) has recently
gained considerable attention in various domains such as bioinformatics,
databases and distributed computing. GPGPU is based on using the GPU as a
co-processor accelerator to offload computationally-intensive tasks from the
CPU. This study starts from the observation that a number of GPU features (such
as overlapping communication and computation, short lived buffer reuse, and
harnessing multi-GPU systems) can be abstracted and reused across different
GPGPU applications. This paper describes CrystalGPU, a modular framework that
transparently enables applications to exploit a number of GPU optimizations.
Our evaluation shows that CrystalGPU enables up to 16x speedup gains on
synthetic benchmarks, while introducing negligible latency overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1711</identifier>
 <datestamp>2010-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1711</id><created>2010-05-11</created><authors><author><keyname>Zeng</keyname><forenames>Meng</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>On Design of Distributed Beamforming for Two-Way Relay Networks</title><categories>cs.IT math.IT</categories><comments>12 pages, submitted to Trans. Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a two-way relay network, where two source nodes, S1 and S2,
exchange information through a cluster of relay nodes. The relay nodes receive
the sum signal from S1 and S2 in the first time slot. In the second time slot,
each relay node multiplies its received signal by a complex coefficient and
retransmits the signal to the two source nodes, which leads to a distributed
two-way beamforming system. By applying the principle of analog network coding,
each receiver at S1 and S2 cancels the ``self-interference'' in the received
signal from the relay cluster and decodes the message. This paper studies the
2-dimensional achievable rate region for such a two-way relay network with
distributed beamforming. With different assumptions of channel reciprocity
between the source-relay and relay-source channels, the achievable rate region
is characterized under two setups. First, with reciprocal channels, we
investigate the achievable rate regions when the relay cluster is subject to a
sum-power constraint or individual-power constraints. We show that the optimal
beamforming vectors obtained from solving the weighted sum inverse-SNR
minimization (WSISMin) problems are sufficient to characterize the
corresponding achievable rate region. Furthermore, we derive the closed form
solutions for those optimal beamforming vectors and consequently propose the
partially distributed algorithms to implement the optimal beamforming, where
each relay node only needs the local channel information and one global
parameter. Second, with the non-reciprocal channels, the achievable rate
regions are also characterized for both the sum-power constraint case and the
individual-power constraint case. Although no closed-form solutions are
available under this setup, we present efficient algorithms to compute the
optimal beamforming vectors, which are attained by solving SDP problems after
semi-definite relaxation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1715</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1715</id><created>2010-05-11</created><updated>2012-05-27</updated><authors><author><keyname>Jeon</keyname><forenames>Sang-Woon</forenames></author><author><keyname>Chung</keyname><forenames>Sae-Young</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>Degrees of Freedom Region of a Class of Multi-source Gaussian Relay
  Networks</title><categories>cs.IT math.IT</categories><comments>19 pages, 5 figures, published in IEEE Transactions on Information
  Theory</comments><journal-ref>EEE Transactions on Information Theory, Special Issue on
  Interference Networks, vol. 57, no. 5, pp. 3032-3044, May 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a layered $K$-user $M$-hop Gaussian relay network consisting of
$K_m$ nodes in the $m^{\operatorname{th}}$ layer, where $M\geq2$ and
$K=K_1=K_{M+1}$. We observe that the time-varying nature of wireless channels
or fading can be exploited to mitigate the inter-user interference. The
proposed amplify-and-forward relaying scheme exploits such channel variations
and works for a wide class of channel distributions including Rayleigh fading.
We show a general achievable degrees of freedom (DoF) region for this class of
Gaussian relay networks. Specifically, the set of all $(d_1,..., d_K)$ such
that $d_i\leq 1$ for all $i$ and $\sum_{i=1}^K d_i\leq K_{\Sigma}$ is
achievable, where $d_i$ is the DoF of the $i^{\operatorname{th}}$
source--destination pair and $K_{\Sigma}$ is the maximum integer such that
$K_{\Sigma}\leq \min_m\{K_m\}$ and $M/K_{\Sigma}$ is an integer. We show that
surprisingly the achievable DoF region coincides with the cut-set outer bound
if $M/\min_m\{K_m\}$ is an integer, thus interference-free communication is
possible in terms of DoF. We further characterize an achievable DoF region
assuming multi-antenna nodes and general message set, which again coincides
with the cut-set outer bound for a certain class of networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1716</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1716</id><created>2010-05-11</created><authors><author><keyname>Drescher</keyname><forenames>Christian</forenames></author><author><keyname>Gebser</keyname><forenames>Martin</forenames></author><author><keyname>Kaufmann</keyname><forenames>Benjamin</forenames></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author></authors><title>Heuristics in Conflict Resolution</title><categories>cs.AI cs.LO</categories><journal-ref>Proceedings of the Twelfth International Workshop on Nonmonotonic
  Reasoning (2008) 141-149</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern solvers for Boolean Satisfiability (SAT) and Answer Set Programming
(ASP) are based on sophisticated Boolean constraint solving techniques. In both
areas, conflict-driven learning and related techniques constitute key features
whose application is enabled by conflict analysis. Although various conflict
analysis schemes have been proposed, implemented, and studied both
theoretically and practically in the SAT area, the heuristic aspects involved
in conflict analysis have not yet received much attention. Assuming a fixed
conflict analysis scheme, we address the open question of how to identify
&quot;good'' reasons for conflicts, and we investigate several heuristics for
conflict analysis in ASP solving. To our knowledge, a systematic study like
ours has not yet been performed in the SAT area, thus, it might be beneficial
for both the field of ASP as well as the one of SAT solving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1734</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1734</id><created>2010-05-11</created><authors><author><keyname>Nonchev</keyname><forenames>Stanislav</forenames><affiliation>Tampere University of Technology, Finland</affiliation></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames><affiliation>Tampere University of Technology, Finland</affiliation></author></authors><title>Advanced Radio Resource Management for Multi Antenna Packet Radio
  Systems</title><categories>cs.NI</categories><comments>14 Pages, IJWMN</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks 2.2 (2010)
  1-14</journal-ref><doi>10.5121/ijwmn.2010.2201</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we propose fairness-oriented packet scheduling (PS) schemes
with power-efficient control mechanism for future packet radio systems. In
general, the radio resource management functionality plays an important role in
new OFDMA based networks. The control of the network resource division among
the users is performed by packet scheduling functionality based on maximizing
cell coverage and capacity satisfying, and certain quality of service
requirements. Moreover, multiantenna transmit-receive schemes provide
additional flexibility to packet scheduler functionality. In order to mitigate
inter-cell and co-channel interference problems in OFDMA cellular networks soft
frequency reuse with different power masks patterns is used. Stemming from the
earlier enhanced proportional fair scheduler studies for single-input
multiple-output (SIMO) and multiple-input multipleoutput (MIMO) systems, we
extend the development of efficient packet scheduling algorithms by adding
transmit power considerations in the overall priority metrics calculations and
scheduling decisions. Furthermore, we evaluate the proposed scheduling schemes
by simulating practical orthogonal frequency division multiple access (OFDMA)
based packet radio system in terms of throughput, coverage and fairness
distribution among users. As a concrete example, under reduced overall transmit
power constraint and unequal power distribution for different sub-bands, we
demonstrate that by using the proposed power-aware multi-user scheduling
schemes, significant coverage and fairness improvements in the order of 70% and
20%, respectively, can be obtained, at the expense of average throughput loss
of only 15%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1736</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1736</id><created>2010-05-11</created><authors><author><keyname>Babu</keyname><forenames>M. Rajesh</forenames><affiliation>PSG College of Technology, India</affiliation></author><author><keyname>Selvan</keyname><forenames>S.</forenames><affiliation>Francis Xavier Engineering College, India</affiliation></author></authors><title>A Lightweight and Attack Resistant Authenticated Routing Protocol for
  Mobile Adhoc Networks</title><categories>cs.CR</categories><comments>14 Pages, IJWMN</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks 2.2 (2010)
  16-29</journal-ref><doi>10.5121/ijwmn.2010.2202</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In mobile ad hoc networks, by attacking the corresponding routing protocol,
an attacker can easily disturb the operations of the network. For ad hoc
networks, till now many secured routing protocols have been proposed which
contains some disadvantages. Therefore security in ad hoc networks is a
controversial area till now. In this paper, we proposed a Lightweight and
Attack Resistant Authenticated Routing Protocol (LARARP) for mobile ad hoc
networks. For the route discovery attacks in MANET routing protocols, our
protocol gives an effective security. It supports the node to drop the invalid
packets earlier by detecting the malicious nodes quickly by verifying the
digital signatures of all the intermediate nodes. It punishes the misbehaving
nodes by decrementing a credit counter and rewards the well behaving nodes by
incrementing the credit counter. Thus it prevents uncompromised nodes from
attacking the routes with malicious or compromised nodes. It is also used to
prevent the denial-of-service (DoS) attacks. The efficiency and effectiveness
of LARARP are verified through the detailed simulation studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1737</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1737</id><created>2010-05-11</created><authors><author><keyname>Khedo</keyname><forenames>Kavi K.</forenames></author><author><keyname>Perseedoss</keyname><forenames>Rajiv</forenames></author><author><keyname>Mungur</keyname><forenames>Avinash</forenames></author><author><keyname>Mauritius</keyname><forenames>University of</forenames></author><author><keyname>Mauritius</keyname></author></authors><title>A Wireless Sensor Network Air Pollution Monitoring System</title><categories>cs.NI</categories><comments>15 Pages, IJWMN</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks 2.2 (2010)
  31-45</journal-ref><doi>10.5121/ijwmn.2010.2203</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Sensor networks are currently an active research area mainly due to the
potential of their applications. In this paper we investigate the use of
Wireless Sensor Networks (WSN) for air pollution monitoring in Mauritius. With
the fast growing industrial activities on the island, the problem of air
pollution is becoming a major concern for the health of the population. We
proposed an innovative system named Wireless Sensor Network Air Pollution
Monitoring System (WAPMS) to monitor air pollution in Mauritius through the use
of wireless sensors deployed in huge numbers around the island. The proposed
system makes use of an Air Quality Index (AQI) which is presently not available
in Mauritius. In order to improve the efficiency of WAPMS, we have designed and
implemented a new data aggregation algorithm named Recursive Converging
Quartiles (RCQ). The algorithm is used to merge data to eliminate duplicates,
filter out invalid readings and summarise them into a simpler form which
significantly reduce the amount of data to be transmitted to the sink and thus
saving energy. For better power management we used a hierarchical routing
protocol in WAPMS and caused the motes to sleep during idle time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1739</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1739</id><created>2010-05-11</created><authors><author><keyname>Sivagami</keyname><forenames>A.</forenames><affiliation>Anna University, India</affiliation></author><author><keyname>Pavai</keyname><forenames>K.</forenames><affiliation>Anna University, India</affiliation></author><author><keyname>Sridharan</keyname><forenames>D.</forenames><affiliation>Anna University, India</affiliation></author><author><keyname>Murty</keyname><forenames>S. A. V. Satya</forenames><affiliation>Indira Gandhi Centre for Atomic Research, India</affiliation></author></authors><title>Energy and Link Quality Based Routing for Data Gathering Tree in
  Wireless Sensor Networks Under TINYOS - 2.X</title><categories>cs.NI</categories><comments>14 Pages, IJWMN</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks 2.2 (2010)
  47-60</journal-ref><doi>10.5121/ijwmn.2010.2204</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Energy is one of the most important and scarce resources in Wireless Sensor
Networks (WSN). WSN nodes work with the embedded operating system called
TinyOS, which addresses the constrains of the WSN nodes such as limited
processing power, memory, energy, etc and it uses the collection Tree Protocol
(CTP) to collect the data from the sensor nodes. It uses either the four-bit
link estimation or Link Estimation Exchange Protocol (LEEP) to predict the bi
directional quality of the wireless link between the nodes and the next hop
candidate is based on the estimated link quality. The residual energy of the
node is an important key factor, which plays a vital role in the lifetime of
the network and hence this has to taken as one of the metric in the parent
selection. In this work, we consider the remaining energy of the node as one of
the metric to decide the parent in addition to the link quality metrics. The
proposed protocol was compared with CTP protocol in terms of number of packets
forwarded by each node and packet reception ratio (PRR) of the network. This
work was simulated in TOSSIM simulator and the same was tested in Crossbow IRIS
radio test bed. The results show that our algorithm performs better than CTP in
terms of load distribution and hence the increased lifetime
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1740</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1740</id><created>2010-05-11</created><authors><author><keyname>Panaousis</keyname><forenames>Emmanouil A.</forenames></author><author><keyname>Ramrekha</keyname><forenames>Tipu A.</forenames></author><author><keyname>Millar</keyname><forenames>Grant P.</forenames></author><author><keyname>Politis</keyname><forenames>Christos</forenames></author></authors><title>Adaptive and Secure Routing Protocol for Emergency Mobile Ad Hoc
  Networks</title><categories>cs.NI</categories><comments>17 Pages, IJWMN</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks 2.2 (2010)
  62-78</journal-ref><doi>10.5121/ijwmn.2010.2205</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The nature of Mobile Ad hoc NETworks (MANETs) makes them suitable to be
utilized in the context of an extreme emergency for all involved rescue teams.
We use the term emergency MANETs (eMANETs) in order to describe next generation
IP-based networks, which are deployed in emergency cases such as forest fires
and terrorist attacks. The main goal within the realm of eMANETs is to provide
emergency workers with intelligent devices such as smart phones and PDAs. This
technology allows communication &quot;islets&quot; to be established between the members
of the same or different emergency teams (policemen, firemen, paramedics). In
this article, we discuss an adaptive and secure routing protocol developed for
the purposes of eMANETs. We evaluate the performance of the protocol by
comparing it with other widely used routing protocols for MANETs. We finally
show that the overhead introduced due to security considerations is affordable
to support secure ad-hoc communications among lightweight devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1742</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1742</id><created>2010-05-11</created><authors><author><keyname>Manoharan</keyname><forenames>R.</forenames></author><author><keyname>Ilavarasan</keyname><forenames>E.</forenames></author></authors><title>Impact of Mobility on the Performance of Multicast Routing Protocols in
  MANET</title><categories>cs.NI</categories><comments>10 Pages, IJWMN</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks 2.2 (2010)
  110-119</journal-ref><doi>10.5121/ijwmn.2010.2208</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The advent of ubiquitous computing and the proliferation of portable
computing devices have raised the importance of mobile ad-hoc network. A major
challenge lies in adapting multicast communication into such environments where
mobility and link failures are inevitable. The purpose of this paper is to
study impact of mobility models in performance of multicast routing protocols
in MANET. In this work, three widely used mobility models such as Random Way
Point, Reference Point Group and Manhattan mobility models and three popular
multicast routing protocols such as On-Demand Multicast Routing Protocol,
Multicast Ad hoc On-demand Distance Vector Routing protocol and Adaptive Demand
driven Multicast Routing protocol have been chosen and implemented in NS2.
Several experiments have been carried out to study the relative strengths,
weakness and applicability of multicast protocols to these mobility models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1744</identifier>
 <datestamp>2010-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1744</id><created>2010-05-11</created><authors><author><keyname>Shepherd</keyname><forenames>Dan</forenames></author></authors><title>Binary Matroids and Quantum Probability Distributions</title><categories>cs.CC quant-ph</categories><comments>24 pages (inc appendix &amp; refs)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterise the probability distributions that arise from quantum
circuits all of whose gates commute, and show when these distributions can be
classically simulated efficiently. We consider also marginal distributions and
the computation of correlation coefficients, and draw connections between the
simulation of stabiliser circuits and the combinatorics of representable
matroids, as developed in the 1990s.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1747</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1747</id><created>2010-05-11</created><authors><author><keyname>Moiz</keyname><forenames>Salman Abdul</forenames><affiliation>Centre for Development of Advanced Computing, India</affiliation></author><author><keyname>Rajamani</keyname><forenames>Lakshmi</forenames><affiliation>Osmania University, India</affiliation></author></authors><title>A Real Time Optimistic Strategy to achieve Concurrency Control in Mobile
  Environments Using On-demand Multicasting</title><categories>cs.DC</categories><comments>14 Pages, IJWMN</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks 2.2 (2010)
  172-185</journal-ref><doi>10.5121/ijwmn.2010.2212</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In mobile database environments, multiple users may access similar data items
irrespective of their physical location leading to concurrent access anomalies.
As disconnections and mobility are the common characteristics in mobile
environment, performing concurrent access to a particular data item leads to
inconsistency. Most of the approaches use locking mechanisms to achieve
concurrency control. However this leads to increase in blocking and abort rate.
In this paper an optimistic concurrency control strategy using on-demand
multicasting is proposed for mobile database environments which guarantees
consistency and introduces application-specific conflict detection and
resolution strategies. The simulation results specify increase in system
throughput by reducing the transaction abort rates as compared to the other
optimistic strategies proposed in literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1749</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1749</id><created>2010-05-11</created><authors><author><keyname>Amer</keyname><forenames>Abdelsalam</forenames></author><author><keyname>Gebali</keyname><forenames>Fayez</forenames></author></authors><title>General Model for Infrastructure Multi-channel Wireless LANs</title><categories>cs.NI</categories><comments>11 Pages, IJCNC</comments><journal-ref>International Journal of Computer Networks &amp; Communications 2.3
  (2010) 1-11</journal-ref><doi>10.5121/ijcnc.2010.2301</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper we develop an integrated model for request mechanism and data
transmission in multi-channel wireless local area networks. We calculated the
performance parameters for single and multi-channel wireless networks when the
channel is noisy. The proposed model is general it can be applied to different
wireless networks such as IEEE802.11x, IEEE802.16, CDMA operated networks and
Hiperlan\2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1751</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1751</id><created>2010-05-11</created><authors><author><keyname>Iyengar</keyname><forenames>N. Ch. Sriman Narayana</forenames><affiliation>VIT University, India</affiliation></author><author><keyname>kumar</keyname><forenames>Syed Mohammad Ansar Sachin</forenames><affiliation>VIT University, India</affiliation></author><author><keyname>Nagar</keyname><forenames>Piyush</forenames><affiliation>VIT University, India</affiliation></author><author><keyname>Sharma</keyname><forenames>Siddharth</forenames><affiliation>VIT University, India</affiliation></author><author><keyname>Atrey</keyname><forenames>Akshay</forenames><affiliation>VIT University, India</affiliation></author></authors><title>An Efficient and Secure Routing Protocol for Mobile Ad-Hoc Networks</title><categories>cs.DC</categories><comments>9 Pages, IJCNC</comments><journal-ref>International Journal of Computer Networks &amp; Communications 2.3
  (2010) 28-36</journal-ref><doi>10.5121/ijcnc.2010.2303</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Efficiency and simplicity of random algorithms have made them a lucrative
alternative for solving complex problems in the domain of communication
networks. This paper presents a random algorithm for handling the routing
problem in Mobile Ad hoc Networks [MANETS].The performance of most existing
routing protocols for MANETS degrades in terms of packet delay and congestion
caused as the number of mobile nodes increases beyond a certain level or their
speed passes a certain level. As the network becomes more and more dynamic,
congestion in network increases due to control packets generated by the routing
protocols in the process of route discovery and route maintenance. Most of this
congestion is due to flooding mechanism used in protocols like AODV and DSDV
for the purpose of route discovery and route maintenance or for route discovery
as in the case of DSR protocol. This paper introduces the concept of random
routing algorithm that neither maintains a routing table nor floods the entire
network as done by various known protocols thereby reducing the load on network
in terms of number of control packets in a highly dynamic scenario. This paper
calculates the expected run time of the designed random algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1753</identifier>
 <datestamp>2010-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1753</id><created>2010-05-11</created><authors><author><keyname>Abid</keyname><forenames>Meriem</forenames><affiliation>Laboratoire d'Informatique de Paris, France</affiliation><affiliation>Ginkgo Networks, France</affiliation></author><author><keyname>Yahiya</keyname><forenames>Tara Ali</forenames><affiliation>Telecom SudParis, France</affiliation></author><author><keyname>Pujolle</keyname><forenames>Guy</forenames><affiliation>Laboratoire d'Informatique de Paris, France</affiliation></author></authors><title>On the Minimization of Handover Decision Instability in Wireless Local
  Area Networks</title><categories>cs.NI</categories><comments>13 Pages, IJWMN</comments><doi>10.5121/ijcnc.2010.2304</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper addresses handover decision instability which impacts negatively
on both user perception and network performances. To this aim, a new technique
called The HandOver Decision STAbility Technique (HODSTAT) is proposed for
horizontal handover in Wireless Local Area Networks (WLAN) based on IEEE
802.11standard. HODSTAT is based on a hysteresis margin analysis that, combined
with a utilitybased function, evaluates the need for the handover and
determines if the handover is needed or avoided. Indeed, if a Mobile Terminal
(MT) only transiently hands over to a better network, the gain from using this
new network may be diminished by the handover overhead and short usage
duration. The approach that we adopt throughout this article aims at reducing
the minimum handover occurrence that leads to the interruption of network
connectivity (this is due to the nature of handover in WLAN which is a break
before make which causes additional delay and packet loss). To this end, MT
rather performs a handover only if the connectivity of the current network is
threatened or if the performance of a neighboring network is really better
comparing the current one with a hysteresis margin. This hysteresis should make
a tradeoff between handover occurrence and the necessity to change the current
network of attachment. Our extensive simulation results show that our proposed
algorithm outperforms other decision stability approaches for handover decision
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1755</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1755</id><created>2010-05-11</created><authors><author><keyname>Mizanian</keyname><forenames>Kiarash</forenames></author><author><keyname>Vasef</keyname><forenames>Mehdi</forenames></author><author><keyname>Analoui</keyname><forenames>Morteza</forenames></author></authors><title>Bandwidth Modeling and Estimation in Peer to Peer Networks</title><categories>cs.NI</categories><comments>19 Pages, IJCNC</comments><journal-ref>International Journal of Computer Networks &amp; Communications 2.3
  (2010) 65-83</journal-ref><doi>10.5121/ijcnc.2010.2306</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recent studies have shown that the majority of today's internet traffic is
related to Peer to Peer (P2P) traffic. The study of bandwidth in P2P networks
is very important. Because it helps us in more efficient capacity planning and
QoS provisioning when we would like to design a large scale computer networks.
In this paper motivated by the behavior of peers (sources or seeds) that is
modeled by Ornstein Uhlenbeck (OU) process, we propose a model for bandwidth in
P2P networks. This model is represented with a stochastic integral. We also
model the bandwidth when we have multiple downloads or uploads. The
autocovariance structure of bandwidth in either case is studied and the
statistical parameters such as mean, variance and autocovariance are obtained.
We then study the queue length behavior of the bandwidth model. The methods for
generating synthetic bandwidth process and estimation of the bandwidth
parameters using maximum likehood estimation are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1757</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1757</id><created>2010-05-11</created><authors><author><keyname>Abbasi</keyname><forenames>Ubaid</forenames><affiliation>University of Bordeaux, France</affiliation></author><author><keyname>Ahmed</keyname><forenames>Toufik</forenames><affiliation>University of Bordeaux, France</affiliation></author></authors><title>Architecture for Cooperative Prefetching in P2P Video-on- Demand System</title><categories>cs.MM</categories><comments>13 Pages, IJCNC</comments><journal-ref>International Journal of Computer Networks &amp; Communications 2.3
  (2010) 126-138</journal-ref><doi>10.5121/ijcnc.2010.2310</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Most P2P VoD schemes focused on service architectures and overlays
optimization without considering segments rarity and the performance of
prefetching strategies. As a result, they cannot better support VCRoriented
service in heterogeneous environment having clients using free VCR controls.
Despite the remarkable popularity in VoD systems, there exist no prior work
that studies the performance gap between different prefetching strategies. In
this paper, we analyze and understand the performance of different prefetching
strategies. Our analytical characterization brings us not only a better
understanding of several fundamental tradeoffs in prefetching strategies, but
also important insights on the design of P2P VoD system. On the basis of this
analysis, we finally proposed a cooperative prefetching strategy called
&quot;cooching&quot;. In this strategy, the requested segments in VCR interactivities are
prefetched into session beforehand using the information collected through
gossips. We evaluate our strategy through extensive simulations. The results
indicate that the proposed strategy outperforms the existing prefetching
mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1758</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1758</id><created>2010-05-11</created><authors><author><keyname>Khalil</keyname><forenames>Ayman</forenames></author><author><keyname>Crussiere</keyname><forenames>Matthieu</forenames></author><author><keyname>Helard</keyname><forenames>Jean-Francois</forenames></author></authors><title>Cross-Layer Resource Allocation Scheme Under Heterogeneous Constraints
  for Next Generation High Rate WPAN</title><categories>cs.NI</categories><comments>17 Pages, IJCNC</comments><journal-ref>International Journal of Computer Networks &amp; Communications 2.3
  (2010) 152-168</journal-ref><doi>10.5121/ijcnc.2010.2312</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In the next generation wireless networks, the growing demand for new wireless
applications is accompanied with high expectations for better quality of
service (QoS) fulfillment especially for multimedia applications. Furthermore,
the coexistence of future unlicensed users with existing licensed users is
becoming a challenging task in the next generation communication systems to
overcome the underutilization of the spectrum. A QoS and interference aware
resource allocation is thus of special interest in order to respond to the
heterogeneous constraints of the next generation networks. In this work, we
address the issue of resource allocation under heterogeneous constraints for
unlicensed multiband ultra-wideband (UWB) systems in the context of Future Home
Networks, i.e. the wireless personal area network (WPAN). The problem is first
studied analytically using a heterogeneous constrained optimization problem
formulation. After studying the characteristics of the optimal solution, we
propose a low-complexity suboptimal algorithm based on a cross-layer approach
that combines information provided by the PHY and MAC layers. While the PHY
layer is responsible for providing the channel quality of the unlicensed UWB
users as well as their interference power that they cause on licensed users,
the MAC layer is responsible for classifying the unlicensed users using a
two-class based approach that guarantees for multimedia services a
high-priority level compared to other services. Combined in an efficient and
simple way, the PHY and MAC information present the key elements of the aimed
resource allocation. Simulation results demonstrate that the proposed scheme
provides a good tradeoff between the QoS satisfaction of the unlicensed
applications with hard QoS requirements and the limitation of the interference
affecting the licensed users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1759</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1759</id><created>2010-05-11</created><authors><author><keyname>Kanrar</keyname><forenames>Soumen</forenames></author><author><keyname>Siraj</keyname><forenames>Mohammad</forenames></author></authors><title>Class Based Admission Control by Complete Partitioning -Video on Demand
  Server</title><categories>cs.NI</categories><comments>12 Pages, IJCNC</comments><journal-ref>International Journal of Computer Networks &amp; Communications 2.3
  (2010) 101-112</journal-ref><doi>10.5121/ijcnc.2010.2308</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In the next generation network (NGN) environment specific consideration is on
bandwidth minimization, because this reduces the cost of network. In response
to the growing market demand for multimedia traffic transmission, NGN concept
has been produced. The next generation network provides multimedia services
over high speed networks, which supports DVD quality video on demand. Although
it has numerous advantages, more exploration of the large-scale deployment
video on demand is still needed. The focus of the research presented in this
paper is a class based admission control by the complete partitioning of the
video on demand server. In this paper we present analytically and by simulation
how the blockage probability of the server significantly affects the on demand
video request and the service. We also present how the blockage probability
affects the performance of the video on demand server.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1771</identifier>
 <datestamp>2010-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1771</id><created>2010-05-11</created><authors><author><keyname>Fuster-Sabater</keyname><forenames>Amparo</forenames></author><author><keyname>Caballero-Gil</keyname><forenames>Pino</forenames></author></authors><title>On the Use of Cellular Automata in Symmetric Cryptography</title><categories>cs.CR cs.DM</categories><comments>25 pages, 0 figures</comments><acm-class>H.2.3</acm-class><journal-ref>Acta Applicandae Mathematicae. Volume 93, Numbers 1-3, pp.
  215-236. Sept 2006. Springer.</journal-ref><doi>10.1007/s10440-006-9041-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, pseudorandom sequence generators based on finite fields have
been analyzed from the point of view of their cryptographic application. In
fact, a class of nonlinear sequence generators has been modelled in terms of
linear cellular automata. The algorithm that converts the given generator into
a linear model based on automata is very simple and is based on the
concatenation of a basic structure. Once the generator has been linearized, a
cryptanalytic attack that exploits the weaknesses of such a model has been
developed. Linear cellular structures easily model sequence generators with
application in stream cipher cryptography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1785</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1785</id><created>2010-05-11</created><updated>2011-06-18</updated><authors><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author></authors><title>Sidelobe Suppression for Robust Beamformer via The Mixed Norm Constraint</title><categories>cs.IT math.IT</categories><comments>10 pages, 3 figures; accepted by Wireless Personal Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applying a sparse constraint on the beam pattern has been suggested to
suppress the sidelobe of the minimum variance distortionless response (MVDR)
beamformer recently. To further improve the performance, we add a mixed norm
constraint on the beam pattern. It matches the beam pattern better and
encourages dense distribution in mainlobe and sparse distribution in sidelobe.
The obtained beamformer has a lower sidelobe level and deeper nulls for
interference avoidance than the standard sparse constraint based beamformer.
Simulation demonstrates that the SINR gain is considerable for its lower
sidelobe level and deeper nulling for interference, while the robustness
against the mismatch between the steering angle and the direction of arrival
(DOA) of the desired signal, caused by imperfect estimation of DOA, is
maintained too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1787</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1787</id><created>2010-05-11</created><authors><author><keyname>Mhala</keyname><forenames>Nitiket N.</forenames><affiliation>BDCOE, India</affiliation></author><author><keyname>Choudhari</keyname><forenames>N. K.</forenames><affiliation>Bhagwati Chadurvedi COE, India</affiliation></author></authors><title>An Envision of Low Cost Mobile Adhoc Network Test Bed in a Laboratory
  Environment Emulating an Actual MANET</title><categories>cs.NI</categories><comments>12 Pages, IJCNC</comments><journal-ref>International Journal of Computer Networks &amp; Communications 2.3
  (2010) 52-63</journal-ref><doi>10.5121/ijcnc.2010.2305</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Orchestrating a live field trial of wireless mobile networking involves
significant cost and logistical issues relating to mobile platforms, support
personnel, network and experiment automation and support equipment. The
significant cost and logistics required to execute such a field trial can also
be limiting in terms of achieving meaningful test results that exercise a
practical number of mobile nodes over a significant set of test conditions
within a given time. There is no argument that field trials are an important
component of dynamic network testing. A field test of prototype will show
whether simulations were on right track or not, but that's a big leap to take;
going from the simulator directly to the real thing. In conceiving our work, we
envisioned a mobile network emulation system that is low cost, flexible and
controllable. This paper describes our wireless MANET test bed under
development which emulates an actual MANET. Here, we focuses that, this test
bed allows the users to automatically generate arbitrary logically network
topologies in order to perform real time operations on adhoc network at a
relatively low cost in a laboratory environment without having to physically
move the nodes in the adhoc network. Thus, we try to &quot;compress&quot; wireless
network so that it fits on a single table.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1800</identifier>
 <datestamp>2010-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1800</id><created>2010-05-11</created><authors><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author><author><keyname>Chu</keyname><forenames>Xiaoli</forenames></author></authors><title>Power-Efficient Ultra-Wideband Waveform Design Considering Radio Channel
  Effects</title><categories>cs.IT math.IT</categories><comments>14 pages, 2 figures, 1 table</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a power-efficient mask-constrained ultra-wideband (UWB)
waveform design with radio channel effects taken into consideration. Based on a
finite impulse response (FIR) filter, we develop a convex optimization model
with respect to the autocorrelation of the filter coefficients to optimize the
transmitted signal power spectrum, subject to a regulatory emission mask. To
improve power efficiency, effects of transmitter radio frequency (RF)
components are included in the optimization of the transmitter-output waveform,
and radio propagation effects are considered for optimizing at the receiver.
Optimum coefficients of the FIR filter are obtained through spectral
factorization of their autocorrelations. Simulation results show that the
proposed method is able to maximize the transmitted UWB signal power under mask
constraints set by regulatory authorities, while mitigating the power loss
caused by channel attenuations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1801</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1801</id><created>2010-05-11</created><updated>2011-06-18</updated><authors><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author><author><keyname>Wen</keyname><forenames>Fei</forenames></author><author><keyname>Xu</keyname><forenames>Jia</forenames></author><author><keyname>Peng</keyname><forenames>Yingning</forenames></author></authors><title>Sparse Support Recovery with Phase-Only Measurements</title><categories>cs.IT math.IT math.NA</categories><comments>15 pages, 3 figures; accepted by International Journal of the
  Physical Sciences</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse support recovery (SSR) is an important part of the compressive sensing
(CS). Most of the current SSR methods are with the full information
measurements. But in practice the amplitude part of the measurements may be
seriously destroyed. The corrupted measurements mismatch the current SSR
algorithms, which leads to serious performance degeneration. This paper
considers the problem of SSR with only phase information. In the proposed
method, the minimization of the l1 norm of the estimated sparse signal enforces
sparse distribution, while a nonzero constraint of the uncorrupted random
measurements' amplitudes with respect to the reconstructed sparse signal is
introduced. Because it only requires the phase components of the measurements
in the constraint, it can avoid the performance deterioration by corrupted
amplitude components. Simulations demonstrate that the proposed phase-only SSR
is superior in the support reconstruction accuracy when the amplitude
components of the measurements are contaminated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1803</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1803</id><created>2010-05-11</created><updated>2011-06-18</updated><authors><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author></authors><title>Anti-Sampling-Distortion Compressive Wideband Spectrum Sensing for
  Cognitive Radio</title><categories>cs.IT math.IT</categories><comments>24 pages, 4 figures, 1 table; accepted by International Journal of
  Mobile Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Too high sampling rate is the bottleneck to wideband spectrum sensing for
cognitive radio in mobile communication. Compressed sensing (CS) is introduced
to transfer the sampling burden. The standard sparse signal recovery of CS does
not consider the distortion in the analogue-to-information converter (AIC). To
mitigate performance degeneration casued by the mismatch in least square
distortionless constraint which doesn't consider the AIC distortion, we define
the sparse signal with the sampling distortion as a bounded additive noise, and
An anti-sampling-distortion constraint (ASDC) is deduced. Then we combine the
\ell1 norm based sparse constraint with the ASDC to get a novel robust sparse
signal recovery operator with sampling distortion. Numerical simulations
demonstrate that the proposed method outperforms standard sparse wideband
spectrum sensing in accuracy, denoising ability, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1804</identifier>
 <datestamp>2010-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1804</id><created>2010-05-11</created><authors><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author></authors><title>Compressive Wideband Spectrum Sensing for Fixed Frequency Spectrum
  Allocation</title><categories>cs.IT math.IT</categories><comments>21 pages, 3 figures, 1 table</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Too high sampling rate is the bottleneck to wideband spectrum sensing for
cognitive radio (CR). As the survey shows that the sensed signal has a sparse
representation in frequency domain in the mass, compressed sensing (CS) can be
used to transfer the sampling burden to the digital signal processor. An analog
to information converter (AIC) can randomly sample the received signal with
sub-Nyquist rate to obtained the random measurements. Considering that the
static frequency spectrum allocation of primary radios means the bounds between
different primary radios is known in advance, here we incorporate information
of the spectrum boundaries between different primary user as a priori
information to obtain a mixed l2/l1 norm denoising operator (MNDO). In the
MNDO, the estimated power spectrum density (PSD) vector is divided into block
sections with bounds corresponding different allocated primary radios.
Different from previous standard l1-norm constraint on the whole PSD vector, a
sum of the l2 norm of each section of the PSD vector is minimized to encourage
the local grouping distribution while the sparse distribution in mass, while a
relaxed constraint is used to improve the denoising performance. Simulation
demonstrates that the proposed method outperforms standard sparse spectrum
estimation in accuracy, denoising ability, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1835</identifier>
 <datestamp>2010-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1835</id><created>2010-05-11</created><authors><author><keyname>Steinberg</keyname><forenames>Benjamin</forenames></author></authors><title>The Cerny conjecture for one-cluster automata with prime length cycle</title><categories>cs.FL math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the Cerny conjecture for one-cluster automata with prime length
cycle. Consequences are given for the hybrid Road-coloring-Cerny conjecture for
digraphs with a proper cycle of prime length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1853</identifier>
 <datestamp>2010-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1853</id><created>2010-05-10</created><authors><author><keyname>Mann</keyname><forenames>Martin</forenames><affiliation>Bioinformatics, University Freiburg, Germany</affiliation></author><author><keyname>Pal&#xf9;</keyname><forenames>Alessandro Dal</forenames><affiliation>Dip. di Matematica, Universit&#xe0; di Parma, Italy</affiliation></author></authors><title>Lattice model refinement of protein structures</title><categories>cs.CE physics.comp-ph q-bio.QM</categories><comments>In Proceedings of Workshop on Constraint Based Methods for
  Bioinformatics (WCB 2010); Jul 21, 2010; Edinburgh, UK (co-located with ICLP
  2010); 7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To find the best lattice model representation of a given full atom protein
structure is a hard computational problem. Several greedy methods have been
suggested where results are usually biased and leave room for improvement. In
this paper we formulate and implement a Constraint Programming method to refine
such lattice structure models. We show that the approach is able to provide
better quality solutions. The prototype is implemented in COLA and is based on
limited discrepancy search. Finally, some promising extensions based on local
search are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1856</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1856</id><created>2010-05-11</created><updated>2012-03-19</updated><authors><author><keyname>Toorani</keyname><forenames>M.</forenames></author><author><keyname>Beheshti</keyname><forenames>A. A.</forenames></author></authors><title>An Elliptic Curve-based Signcryption Scheme with Forward Secrecy</title><categories>cs.CR</categories><comments>13 Pages, 5 Figures, 2 Tables</comments><msc-class>94Axx</msc-class><acm-class>E.3; K.6.5; D.4.6; K.6.m</acm-class><journal-ref>Journal of Applied Sciences, Vol.9, No.6, pp.1025-1035, 2009</journal-ref><doi>10.3923/jas.2009.1025.1035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An elliptic curve-based signcryption scheme is introduced in this paper that
effectively combines the functionalities of digital signature and encryption,
and decreases the computational costs and communication overheads in comparison
with the traditional signature-then-encryption schemes. It simultaneously
provides the attributes of message confidentiality, authentication, integrity,
unforgeability, non-repudiation, public verifiability, and forward secrecy of
message confidentiality. Since it is based on elliptic curves and can use any
fast and secure symmetric algorithm for encrypting messages, it has great
advantages to be used for security establishments in store-and-forward
applications and when dealing with resource-constrained devices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1860</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1860</id><created>2010-05-11</created><updated>2010-05-20</updated><authors><author><keyname>Petrik</keyname><forenames>Marek</forenames></author><author><keyname>Taylor</keyname><forenames>Gavin</forenames></author><author><keyname>Parr</keyname><forenames>Ron</forenames></author><author><keyname>Zilberstein</keyname><forenames>Shlomo</forenames></author></authors><title>Feature Selection Using Regularization in Approximate Linear Programs
  for Markov Decision Processes</title><categories>cs.AI</categories><comments>Technical report corresponding to the ICML2010 submission of the same
  name</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate dynamic programming has been used successfully in a large variety
of domains, but it relies on a small set of provided approximation features to
calculate solutions reliably. Large and rich sets of features can cause
existing algorithms to overfit because of a limited number of samples. We
address this shortcoming using $L_1$ regularization in approximate linear
programming. Because the proposed method can automatically select the
appropriate richness of features, its performance does not degrade with an
increasing number of features. These results rely on new and stronger sampling
bounds for regularized approximate linear programs. We also propose a
computationally efficient homotopy method. The empirical evaluation of the
approach shows that the proposed method performs well on simple MDPs and
standard benchmark problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1871</identifier>
 <datestamp>2010-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1871</id><created>2010-05-11</created><authors><author><keyname>Hernando</keyname><forenames>Fernando</forenames></author><author><keyname>O'Sullivan</keyname><forenames>Michael E.</forenames></author><author><keyname>Popovici</keyname><forenames>Emanuel</forenames></author><author><keyname>Srivastava</keyname><forenames>Shraddha</forenames></author></authors><title>Subfield-Subcodes of Generalized Toric codes</title><categories>cs.IT math.IT</categories><comments>Submitted to 2010 IEEE International Symposium on Information Theory
  (ISIT 2010)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study subfield-subcodes of Generalized Toric (GT) codes over
$\mathbb{F}_{p^s}$. These are the multidimensional analogues of BCH codes,
which may be seen as subfield-subcodes of generalized Reed-Solomon codes. We
identify polynomial generators for subfield-subcodes of GT codes which allows
us to determine the dimensions and obtain bounds for the minimum distance. We
give several examples of binary and ternary subfield-subcodes of GT codes that
are the best known codes of a given dimension and length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1894</identifier>
 <datestamp>2010-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1894</id><created>2010-05-11</created><authors><author><keyname>Navasca</keyname><forenames>Carmeliza</forenames></author><author><keyname>Opperman</keyname><forenames>Michael</forenames></author><author><keyname>Penderghest</keyname><forenames>Timothy</forenames></author><author><keyname>Tamon</keyname><forenames>Christino</forenames></author></authors><title>Tensors as module homomorphisms over group rings</title><categories>math.NA cs.NA math.RA</categories><comments>11 pages, 4 figures.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Braman [B08] described a construction where third-order tensors are exactly
the set of linear transformations acting on the set of matrices with vectors as
scalars. This extends the familiar notion that matrices form the set of all
linear transformations over vectors with real-valued scalars. This result is
based upon a circulant-based tensor multiplication due to Kilmer et al.
[KMP08]. In this work, we generalize these observations further by viewing this
construction in its natural framework of group rings.The circulant-based
products arise as convolutions in these algebraic structures. Our
generalization allows for any abelian group to replace the cyclic group, any
commutative ring with identity to replace the field of real numbers, and an
arbitrary order tensor to replace third-order tensors, provided the underlying
ring is commutative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1899</identifier>
 <datestamp>2010-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1899</id><created>2010-05-11</created><authors><author><keyname>Stanley</keyname><forenames>Jo</forenames></author><author><keyname>Briscoe</keyname><forenames>Gerard</forenames></author></authors><title>The ABC of Digital Business Ecosystems</title><categories>cs.CY</categories><comments>24 pages, 5 figures</comments><journal-ref>The ABC of Digital Business Ecosystems. J Stanley and G Briscoe.
  Communications Law - Journal of Computer, Media and Telecommunications Law,
  15(1), 2010.</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The European Commission has the power to inspire, initiate and sponsor huge
transnational projects to an extent impossible for most other entities. These
projects can address universal themes and develop well-being models that are
valuable across a diversity of societies and economies. It is a universal fact
that SMEs in all countries provide a substantial proportion of total
employment, and conduct much of a nation's innovative activity. Yet these
smaller companies struggle in global markets on a far from level playing field,
where large companies have distinct advantages. To redress this imbalance the
Commission saw it as a priority to improve the trading capability of the Small
and Medium-sized Enterprises (SMEs), and perceived digital platforms as the
modern means to this end. They considered that the best operational model for a
vibrant Web2.0-based Internet services industry would be by analogy to
well-performing biological ecosystems. Open Source Software is adopted in the
DBE/OPAALS projects as the best support for sustainability of such complex
electronic webs, since it minimises interoperability problems, enables code
access for cheaper in-house modification or development of systems, and reduces
both capital and operating expenditure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1904</identifier>
 <datestamp>2010-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1904</id><created>2010-05-11</created><updated>2010-05-20</updated><authors><author><keyname>Pandey</keyname><forenames>Abhinav</forenames></author><author><keyname>Pandey</keyname><forenames>Akash</forenames></author><author><keyname>Tandon</keyname><forenames>Ankit</forenames></author><author><keyname>Maurya</keyname><forenames>Brajesh Kr</forenames></author><author><keyname>Kushwaha</keyname><forenames>Upendra</forenames></author><author><keyname>Mishra</keyname><forenames>Dr. Madhvendra</forenames></author><author><keyname>Tiwari</keyname><forenames>Vijayshree</forenames></author></authors><title>Cloud Computing: Exploring the scope</title><categories>cs.DC</categories><comments>9 pages, 7 figures, Paper accepted for the 2010 International
  Conference on Informatics, Cybernetics, and Computer Applications (ICICCA
  2010)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing refers to a paradigm shift to overall IT solutions while
raising the accessibility, scalability and effectiveness through its enabling
technologies. However, migrated cloud platforms and services cost benefits as
well as performances are neither clear nor summarized. Globalization and the
recessionary economic times have not only raised the bar of a better IT
delivery models but also have given access to technology enabled services via
internet. Cloud computing has vast potential in terms of lean Retail
methodologies that can minimize the operational cost by using the third party
based IT capabilities, as a service. It will not only increase the ROI but will
also help in lowering the total cost of ownership. In this paper we have tried
to compare the cloud computing cost benefits with the actual premise cost which
an organization incurs normally. However, in spite of the cost benefits, many
IT professional believe that the latest model i.e. &quot;cloud computing&quot; has risks
and security concerns. This report demonstrates how to answer the following
questions: (1) Idea behind cloud computing. (2) Monetary cost benefits of using
cloud with respect to traditional premise computing. (3) What are the various
security issues? We have tried to find out the cost benefit by comparing the
Microsoft Azure cloud cost with the prevalent premise cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1918</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1918</id><created>2010-05-11</created><updated>2010-06-04</updated><authors><author><keyname>Chernov</keyname><forenames>Alexey</forenames></author><author><keyname>Zhdanov</keyname><forenames>Fedor</forenames></author></authors><title>Prediction with Expert Advice under Discounted Loss</title><categories>cs.LG</categories><comments>26 pages; expanded (2 remarks -&gt; theorems), some misprints corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study prediction with expert advice in the setting where the losses are
accumulated with some discounting---the impact of old losses may gradually
vanish. We generalize the Aggregating Algorithm and the Aggregating Algorithm
for Regression to this case, propose a suitable new variant of exponential
weights algorithm, and prove respective loss bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1925</identifier>
 <datestamp>2010-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1925</id><created>2010-05-11</created><updated>2010-07-05</updated><authors><author><keyname>Jankowski</keyname><forenames>Bartosz</forenames></author><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Szczypiorski</keyname><forenames>Krzysztof</forenames></author></authors><title>Information Hiding Using Improper Frame Padding</title><categories>cs.CR</categories><comments>6 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hiding information in network traffic may lead to leakage of confidential
information. In this paper we introduce a new steganographic system: the
PadSteg (Padding Steganography). To authors' best knowledge it is the first
information hiding solution which represents interprotocol steganography i.e.
usage of relation between two or more protocols from the TCP/IP stack to enable
secret communication. PadSteg utilizes ARP and TCP protocols together with an
Etherleak vulnerability (improper Ethernet frame padding) to facilitate secret
communication for hidden groups in LANs (Local Area Networks). Basing on real
network traces we confirm that PadSteg is feasible in today's networks and we
estimate what steganographic bandwidth is achievable while limiting the chance
of disclosure. We also point at possible countermeasures against PadSteg.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1934</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1934</id><created>2010-05-11</created><authors><author><keyname>Wick</keyname><forenames>Michael</forenames></author><author><keyname>McCallum</keyname><forenames>Andrew</forenames></author><author><keyname>Miklau</keyname><forenames>Gerome</forenames></author></authors><title>Scalable Probabilistic Databases with Factor Graphs and MCMC</title><categories>cs.DB cs.AI</categories><comments>Submitted to VLDB 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic databases play a crucial role in the management and
understanding of uncertain data. However, incorporating probabilities into the
semantics of incomplete databases has posed many challenges, forcing systems to
sacrifice modeling power, scalability, or restrict the class of relational
algebra formula under which they are closed. We propose an alternative approach
where the underlying relational database always represents a single world, and
an external factor graph encodes a distribution over possible worlds; Markov
chain Monte Carlo (MCMC) inference is then used to recover this uncertainty to
a desired level of fidelity. Our approach allows the efficient evaluation of
arbitrary queries over probabilistic databases with arbitrary dependencies
expressed by graphical models with structure that changes during inference.
MCMC sampling provides efficiency by hypothesizing {\em modifications} to
possible worlds rather than generating entire worlds from scratch. Queries are
then run over the portions of the world that change, avoiding the onerous cost
of running full queries over each sampled world. A significant innovation of
this work is the connection between MCMC sampling and materialized view
maintenance techniques: we find empirically that using view maintenance
techniques is several orders of magnitude faster than naively querying each
sampled world. We also demonstrate our system's ability to answer relational
queries with aggregation, and demonstrate additional scalability through the
use of parallelization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1941</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1941</id><created>2010-05-11</created><authors><author><keyname>Ashok</keyname><forenames>B.</forenames></author><author><keyname>Patra</keyname><forenames>T. K.</forenames></author></authors><title>Locating phase transitions in computationally hard problems</title><categories>cond-mat.stat-mech cs.CC physics.comp-ph</categories><comments>To appear in Pramana: journal of physics (2010) (in press)</comments><doi>10.1007/s12043-010-0138-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss how phase-transitions may be detected in computationally hard
problems in the context of Anytime Algorithms. Treating the computational time,
value and utility functions involved in the search results in analogy with
quantities in statistical physics, we indicate how the onset of a
computationally hard regime can be detected and the transit to higher quality
solutions be quantified by an appropriate response function. The existence of a
dynamical critical exponent is shown, enabling one to predict the onset of
critical slowing down, rather than finding it after the event, in the specific
case of a Travelling Salesman Problem. This can be used as a means of improving
efficiency and speed in searches, and avoiding needless computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1951</identifier>
 <datestamp>2010-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1951</id><created>2010-05-11</created><authors><author><keyname>Pittel</keyname><forenames>Boris</forenames></author><author><keyname>Yeum</keyname><forenames>Ji-A</forenames></author></authors><title>How frequently is a system of 2-linear Boolean equations solvable?</title><categories>math.CO cs.DM</categories><msc-class>05C80, 60K35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a random system of equations $x_i+x_j=b_{(i,j)} (\text{mod }2)$,
$(x_u\in \{0,1\},\, b_{(u,v)}=b_{(v,u)}\in\{0,1\})$, with the pairs $(i,j)$
from $E$, a symmetric subset of $[n]\times [n]$. $E$ is chosen uniformly at
random among all such subsets of a given cardinality $m$; alternatively
$(i,j)\in E$ with a given probability $p$, independently of all other pairs.
Also, given $E$, $\pr\{b_{e}=0\}=\pr\{b_e=1\}$ for each $e\in E$, independently
of all other $b_{e^\prime}$. It is well known that, as $m$ passes through $n/2$
($p$ passes through $1/n$, resp.), the underlying random graph
$G(n,\#\text{edges}=m)$, ($G(n,\pr(\text{edge})=p)$, resp.) undergoes a rapid
transition, from essentially a forest of many small trees to a graph with one
large, multicyclic, component in a sea of small tree components. We should
expect then that the solvability probability decreases precipitously in the
vicinity of $m\sim n/2$ ($p\sim 1/n$), and indeed this probability is of order
$(1-2m/n)^{1/4}$, for $m&lt;n/2$ ($(1-pn)^{1/4}$, for $p&lt;1/n$, resp.). We show
that in a near-critical phase $m=(n/2)(1+\la n^{-1/3})$ ($p=(1+\la
n^{-1/3})/n$, resp.), $\la=o(n^{1/12})$, the system is solvable with
probability asymptotic to $c(\la)n^{-1/12}$, for some explicit function
$c(\la)&gt;0$. Mike Molloy noticed that the Boolean system with $b_e\equiv 1$ is
solvable iff the underlying graph is $2$-colorable, and asked whether this
connection might be used to determine an order of probability of
$2$-colorability in the near-critical case. We answer Mike's question
affirmatively and show that probability of $2$-colorability is $\lesssim
2^{-1/4}e^{1/8}c(\lambda)n^{-1/12}$, and asymptotic to
$2^{-1/4}e^{1/8}c(\la)n^{-1/12}$ at a critical phase $\la=O(1)$, and for
$\la\to -\infty$. (Submitted to Electronic Journal of Combinatorics on
September 7, 2009.)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1953</identifier>
 <datestamp>2010-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1953</id><created>2010-05-11</created><updated>2010-10-13</updated><authors><author><keyname>Dey</keyname><forenames>Sandipan</forenames></author><author><keyname>Al-Qaheri</keyname><forenames>Hameed</forenames></author><author><keyname>Sane</keyname><forenames>Suneeta</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>A Note On the Bounds for the Generalized Fibonacci-p-Sequence and its
  Application in Data-Hiding</title><categories>cs.CR</categories><comments>15 Pages, 2 Figures, 2 Tables</comments><journal-ref>International Journal of Computer Science and Applications, ISSN:
  0972-9038, Editor-in-Chief: Rajendra Akerkar, Vol. 7 No. 4, pp. 1 - 15,
  October, 2010, Published by: Technomathematics Research Foundation</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we suggest a lower and an upper bound for the Generalized
Fibonacci-p-Sequence, for different values of p. The Fibonacci-p-Sequence is a
generalization of the Classical Fibonacci Sequence. We ?first show that the
ratio of two consecutive terms in generalized Fibonacci sequence converges to a
p-degree polynomial and then use this result to prove the bounds for
generalized Fibonacci-p sequence, thereby generalizing the exponential bounds
for classical Fibonacci Sequence. Then we show how these results can be used to
prove efficiency for data hiding techniques using generalized Fibonacci
sequence. These steganographic techniques use generalized Fibonacci-p-Sequence
for increasing number available of bit-planes to hide data, so that more and
more data can be hidden into the higher bit-planes of any pixel without causing
much distortion of the cover image. This bound can be used as a theoretical
proof for efficiency of those techniques, for instance it explains why more and
more data can be hidden into the higher bit-planes of a pixel, without causing
considerable decrease in PSNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1967</identifier>
 <datestamp>2011-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1967</id><created>2010-05-11</created><authors><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author><author><keyname>Zimmermann</keyname><forenames>Paul</forenames></author></authors><title>The great trinomial hunt</title><categories>math.NT cs.DM</categories><comments>16 pages. For further details see
  http://wwwmaths.anu.edu.au/~brent/pub/pub235.html</comments><msc-class>11-04 (Primary), 11B83 (Secondary)</msc-class><acm-class>G.2.1; G.4</acm-class><journal-ref>Notices of the American Mathematical Society 58, 2 (2011), 233-239</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a search for primitive trinomials of high degree and its
interaction with the Great Internet Mersenne prime search (GIMPS). The search
is complete for trinomials whose degree is the exponent of a Mersenne prime,
for all 47 currently known Mersenne primes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.1992</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.1992</id><created>2010-05-12</created><authors><author><keyname>Ahammed</keyname><forenames>G. F. Ali</forenames><affiliation>Ghousia college of Engg.Ramanagaram, India</affiliation></author><author><keyname>Banu</keyname><forenames>Reshma</forenames><affiliation>Ghousia college of Engg.Ramanagaram, India</affiliation></author></authors><title>Analyzing the Performance of Active Queue Management Algorithms</title><categories>cs.PF</categories><comments>19 Pages, IJCNC 2010</comments><journal-ref>International Journal of Computer Networks &amp; Communications 2.2
  (2010) 1-19</journal-ref><doi>10.5121/ijcnc.2010.2201</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Congestion is an important issue which researchers focus on in the
Transmission Control Protocol (TCP) network environment. To keep the stability
of the whole network, congestion control algorithms have been extensively
studied. Queue management method employed by the routers is one of the
important issues in the congestion control study. Active queue management (AQM)
has been proposed as a router-based mechanism for early detection of congestion
inside the network. In this paper we analyzed several active queue management
algorithms with respect to their abilities of maintaining high resource
utilization, identifying and restricting disproportionate bandwidth usage, and
their deployment complexity. We compare the performance of FRED, BLUE, SFB, and
CHOKe based on simulation results, using RED and Drop Tail as the evaluation
baseline. The characteristics of different algorithms are also discussed and
compared. Simulation is done by using Network Simulator(NS2) and the graphs are
drawn using X- graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2001</identifier>
 <datestamp>2010-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2001</id><created>2010-05-12</created><updated>2010-05-31</updated><authors><author><keyname>Emiris</keyname><forenames>Ioannis Z.</forenames><affiliation>DI</affiliation></author><author><keyname>Galligo</keyname><forenames>Andr&#xe9;</forenames><affiliation>JAD</affiliation></author><author><keyname>Tsigaridas</keyname><forenames>Elias</forenames><affiliation>DI</affiliation></author></authors><title>Random polynomials and expected complexity of bisection methods for real
  solving</title><categories>cs.SC</categories><proxy>ccsd</proxy><journal-ref>International Symposium on Symbolic and Algebraic Computation
  (ISSAC), Munich : Germany (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our probabilistic analysis sheds light to the following questions: Why do
random polynomials seem to have few, and well separated real roots, on the
average? Why do exact algorithms for real root isolation may perform
comparatively well or even better than numerical ones? We exploit results by
Kac, and by Edelman and Kostlan in order to estimate the real root separation
of degree $d$ polynomials with i.i.d.\ coefficients that follow two zero-mean
normal distributions: for SO(2) polynomials, the $i$-th coefficient has
variance ${d \choose i}$, whereas for Weyl polynomials its variance is
${1/i!}$. By applying results from statistical physics, we obtain the expected
(bit) complexity of \func{sturm} solver, $\sOB(r d^2 \tau)$, where $r$ is the
number of real roots and $\tau$ the maximum coefficient bitsize. Our bounds are
two orders of magnitude tighter than the record worst case ones. We also derive
an output-sensitive bound in the worst case. The second part of the paper shows
that the expected number of real roots of a degree $d$ polynomial in the
Bernstein basis is $\sqrt{2d}\pm\OO(1)$, when the coefficients are i.i.d.\
variables with moderate standard deviation. Our paper concludes with
experimental results which corroborate our analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2004</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2004</id><created>2010-05-12</created><authors><author><keyname>Mahini</keyname><forenames>Hamidreza</forenames><affiliation>Iran University of Science and Technology, Iran</affiliation></author><author><keyname>Berangi</keyname><forenames>Reza</forenames><affiliation>Iran University of Science and Technology, Iran</affiliation></author><author><keyname>Mahini</keyname><forenames>Alireza</forenames><affiliation>Islamic Azad University, Iran</affiliation></author></authors><title>MLET: A Power Efficient Approach for TCAM Based, IP Lookup Engines in
  Internet Routers</title><categories>cs.NI</categories><comments>14 Pages, IJCNC 2010</comments><journal-ref>International Journal of Computer Networks &amp; Communications 2.3
  (2010) 13-26</journal-ref><doi>10.5121/ijcnc.2010.2302</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Routers are one of the important entities in computer networks specially the
Internet. Forwarding IP packets is a valuable and vital function in Internet
routers. Routers extract destination IP address from packets and lookup those
addresses in their own routing table. This task is called IP lookup. Internet
address lookup is a challenging problem due to the increasing routing table
sizes. Ternary Content-Addressable Memories (TCAMs)are becoming very popular
for designing high-throughput address lookup?engines on routers: they are fast,
cost-effective and simple to manage. Despite the TCAMs speed, their high power
consumption is their major drawback. In this paper, Multilevel Enabling
Technique (MLET), a power efficient TCAM based hardware architecture has been
proposed. This scheme is employed after an Espresso-II minimization algorithm
to achieve lower power consumption. The performance evaluation of the proposed
approach shows that it can save considerable amount of routing table's power
consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2012</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2012</id><created>2010-05-12</created><updated>2011-04-10</updated><authors><author><keyname>Duchi</keyname><forenames>John</forenames></author><author><keyname>Agarwal</keyname><forenames>Alekh</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin</forenames></author></authors><title>Dual Averaging for Distributed Optimization: Convergence Analysis and
  Network Scaling</title><categories>math.OC cs.SY stat.ML</categories><comments>40 pages, 4 figures</comments><journal-ref>IEEE Transactions on Automatic Control 57(3), pp. 592 - 606. March
  2012</journal-ref><doi>10.1109/TAC.2011.2161027</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of decentralized optimization over a network is to optimize a global
objective formed by a sum of local (possibly nonsmooth) convex functions using
only local computation and communication. It arises in various application
domains, including distributed tracking and localization, multi-agent
co-ordination, estimation in sensor networks, and large-scale optimization in
machine learning. We develop and analyze distributed algorithms based on dual
averaging of subgradients, and we provide sharp bounds on their convergence
rates as a function of the network size and topology. Our method of analysis
allows for a clear separation between the convergence of the optimization
algorithm itself and the effects of communication constraints arising from the
network structure. In particular, we show that the number of iterations
required by our algorithm scales inversely in the spectral gap of the network.
The sharpness of this prediction is confirmed both by theoretical lower bounds
and simulations for various networks. Our approach includes both the cases of
deterministic optimization and communication, as well as problems with
stochastic optimization and/or communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2027</identifier>
 <datestamp>2010-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2027</id><created>2010-05-12</created><authors><author><keyname>Roy</keyname><forenames>Sarbani</forenames></author><author><keyname>Halder</keyname><forenames>Saikat</forenames></author><author><keyname>Mukherjee</keyname><forenames>Nandini</forenames></author></authors><title>A Multi-agent Framework for Performance Tuning in Distributed
  Environment</title><categories>cs.DC</categories><comments>International Conference On High Performance Computing HiPC 2005,
  Posters Web Proceedings, Goa, India, December 18-21, 2005</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the overall design of a multi-agent framework for tuning
the performance of an application executing in a distributed environment. The
multi-agent framework provides services like resource brokering, analyzing
performance monitoring data, local tuning and also rescheduling in case of any
performance problem on a specific resource provider. The paper also briefly
describes the implementation of some part of the framework. In particular, job
migration on the basis of performance monitoring data is particularly
highlighted in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2037</identifier>
 <datestamp>2010-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2037</id><created>2010-05-12</created><authors><author><keyname>De Sarkar</keyname><forenames>Ajanta</forenames></author><author><keyname>Roy</keyname><forenames>Sarbani</forenames></author><author><keyname>Biswas</keyname><forenames>Sudipto</forenames></author><author><keyname>Mukherjee</keyname><forenames>Nandini</forenames></author></authors><title>An Integrated Framework for Performance Analysis and Tuning in Grid
  Environment</title><categories>cs.DC</categories><comments>International Conference On High Performance Computing HiPC 2006,
  Posters Web Proceedings, 2006, Bangalore, India, December 18-21, 2006</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a heterogeneous, dynamic environment, like Grid, post-mortem analysis is
of no use and data needs to be collected and analysed in real time. Novel
techniques are also required for dynamically tuning the application's
performance and resource brokering in order to maintain the desired QoS. The
objective of this paper is to propose an integrated framework for performance
analysis and tuning of the application, and rescheduling the application, if
necessary, to some other resources in order to adapt to the changing resource
usage scenario in a dynamic environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2050</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2050</id><created>2010-05-12</created><authors><author><keyname>Zayani</keyname><forenames>Hafedh</forenames><affiliation>National Engineering School of Tunis</affiliation></author><author><keyname>Barkaoui</keyname><forenames>Kamel</forenames><affiliation>CNAM - CEDRIC, France</affiliation></author><author><keyname>Ayed</keyname><forenames>Rahma Ben</forenames><affiliation>National Engineering School of Tunis</affiliation></author></authors><title>Probabilistic verification and evaluation of Backoff procedure of the
  WSN ECo-MAC protocol</title><categories>cs.NI</categories><comments>15 Pages, IJWMN 2010</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks 2.2 (2010)
  156-170</journal-ref><doi>10.5121/ijwmn.2010.2211</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Communication protocols and techniques are often evaluated using simulation
techniques. However, the use of formal modeling and analysis techniques for
verification and evaluation in particular for Wireless Sensor Networks (WSN)
becomes a necessity. In this paper we present a formal analysis of the backoff
procedure integrated in the medium access control protocol named ECo-MAC
designed for WSN. We describe this backoff procedure in terms of discrete time
Markov chains (DTMCs) and evaluated using the well known probabilistic model
checker PRISM. After checking the different invariants of the proposed model,
we study the effect of contention window length (in number of time contention
unit) on the acceptable number of simultaneous senders in a neighborhood of a
given receiver. The obtained quantitative results confirm those provided by the
simulation using OPNET tool and justify the validity of the adopted value for
the time contention unit TCU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2061</identifier>
 <datestamp>2011-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2061</id><created>2010-05-12</created><updated>2011-10-31</updated><authors><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Hadzi-Velkov</keyname><forenames>Zoran</forenames></author><author><keyname>Karagiannidis</keyname><forenames>George K.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Cooperative Diversity with Mobile Nodes: Capacity Outage Rate and
  Duration</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Information Theory (2011)</comments><doi>10.1109/TIT.2011.2165794</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The outage probability is an important performance measure for cooperative
diversity schemes. However, in mobile environments, the outage probability does
not completely describe the behavior of cooperative diversity schemes since the
mobility of the involved nodes introduces variations in the channel gains. As a
result, the capacity outage events are correlated in time and second-order
statistical parameters of the achievable information-theoretic capacity such as
the average capacity outage rate (AOR) and the average capacity outage duration
(AOD) are required to obtain a more complete description of the properties of
cooperative diversity protocols. In this paper, assuming slow Rayleigh fading,
we derive exact expressions for the AOR and the AOD of three well-known
cooperative diversity protocols: variable-gain amplify-and-forward,
decode-and-forward, and selection decode-and-forward relaying. Furthermore, we
develop asymptotically tight high signal-to-noise ratio (SNR) approximations,
which offer important insights into the influence of various system and channel
parameters on the AOR and AOD. In particular, we show that on a
double-logarithmic scale, similar to the outage probability, the AOR
asymptotically decays with the SNR with a slope that depends on the diversity
gain of the cooperative protocol, whereas the AOD asymptotically decays with a
slope of -1/2 independent of the diversity gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2072</identifier>
 <datestamp>2010-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2072</id><created>2010-05-12</created><authors><author><keyname>Mokhov</keyname><forenames>Serguei A.</forenames></author><author><keyname>Mia</keyname><forenames>Mashrur</forenames></author><author><keyname>Solodov</keyname><forenames>Petr</forenames></author><author><keyname>Zhao</keyname><forenames>Kai</forenames></author><author><keyname>Halimi</keyname><forenames>Jihed</forenames></author></authors><title>A UI Design Case Study and a Prototype of a Travel Search Engine</title><categories>cs.SE cs.HC</categories><comments>65 pages; a 2002-2003 project report</comments><acm-class>H.2; D.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We review a case study of a UI design project for a complete travel search
engine system prototype for regular and corporate users. We discuss various
usage scenarios, guidelines, and so for, and put them into a web-based
prototype with screenshots and the like. We combined into our prototype the
best features found at the time (2002) on most travel-like sites and added more
to them as a part of our research. We conducted feasibility studies, review
common design guidelines and Nelson's heuristics while constructing this work.
The prototype is itself open-source, but has no backend functionality, as the
focus is the user-centered design of such a system. While the prototype is
mostly static, some dynamic activity is present through the use of PHP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2079</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2079</id><created>2010-05-12</created><authors><author><keyname>&#xc9;sik</keyname><forenames>Zolt&#xe1;n</forenames></author><author><keyname>Maletti</keyname><forenames>Andreas</forenames></author></authors><title>Simulations of Weighted Tree Automata</title><categories>cs.FL</categories><comments>17 pages, 2 figures</comments><msc-class>68Q45, 68Q70</msc-class><doi>10.1007/978-3-642-18098-9_34</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulations of weighted tree automata (wta) are considered. It is shown how
such simulations can be decomposed into simpler functional and dual functional
simulations also called forward and backward simulations. In addition, it is
shown in several cases (fields, commutative rings, Noetherian semirings,
semiring of natural numbers) that all equivalent wta M and N can be joined by a
finite chain of simulations. More precisely, in all mentioned cases there
exists a single wta that simulates both M and N. Those results immediately
yield decidability of equivalence provided that the semiring is finitely (and
effectively) presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2086</identifier>
 <datestamp>2010-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2086</id><created>2010-05-12</created><authors><author><keyname>Vabishchevich</keyname><forenames>Petr N.</forenames></author></authors><title>On a new class of additive (splitting) operator-difference schemes</title><categories>cs.NA math.NA</categories><msc-class>65N06, 65M06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applied time-dependent problems are characterized by an additive
representation of the problem operator. Additive schemes are constructed using
such a splitting and associated with the transition to a new time level on the
basis of the solution of more simple problems for the individual operators in
the additive decomposition. We consider a new class of additive schemes for
problems with additive representation of the operator at the time derivative.
In this paper we construct and study the vector operator-difference schemes,
which are characterized by a transition from one initial the evolution equation
to a system of such equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2122</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2122</id><created>2010-05-12</created><updated>2010-09-28</updated><authors><author><keyname>Guo</keyname><forenames>Jin-Li</forenames></author></authors><title>S-curve networks and an approximate method for estimating degree
  distributions of complex networks</title><categories>physics.comp-ph cs.NI physics.soc-ph</categories><comments>CHINESE PHYS (In press)</comments><doi>10.1088/1674-1056/19/12/120503</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In the study of complex networks almost all theoretical models have the
property of infinite growth, but the size of actual networks is finite.
According to statistics from the China Internet IPv4 (Internet Protocol version
4) addresses, this paper proposes a forecasting model by using S curve
(Logistic curve). The growing trend of IPv4 addresses in China is forecasted.
There are some reference value for optimizing the distribution of IPv4 address
resource and the development of IPv6. Based on the laws of IPv4 growth, that
is, the bulk growth and the finitely growing limit, it proposes a finite
network model with a bulk growth. The model is said to be an S-curve network.
Analysis demonstrates that the analytic method based on uniform distributions
(i.e., Barab\'asi-Albert method) is not suitable for the network. It develops
an approximate method to predict the growth dynamics of the individual nodes,
and use this to calculate analytically the degree distribution and the scaling
exponents. The analytical result agrees with the simulation well, obeying an
approximately power-law form. This method can overcome a shortcoming of
Barab\'asi-Albert method commonly used in current network research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2135</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2135</id><created>2010-05-12</created><updated>2011-04-13</updated><authors><author><keyname>Wu</keyname><forenames>Haoyang</forenames></author></authors><title>Two-agent Nash implementation: A new result</title><categories>cs.GT physics.soc-ph</categories><comments>15 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  [J. Moore and R. Repullo, \emph{Econometrica} \textbf{58} (1990) 1083-1099]
and [B. Dutta and A. Sen, \emph{Rev. Econom. Stud.} \textbf{58} (1991) 121-128]
are two important papers on two-agent Nash implementation. Recently, [H. Wu,
Quantum mechanism helps agents combat &quot;bad&quot; social choice rules.
\emph{International Journal of Quantum Information}, 2010 (accepted).
abs/1002.4294 ] broke through traditional results on Nash implementation with
three or more agents. In this paper, we will investigate two-agent Nash
implementation by virtue of Wu's quantum mechanism. The main result is: A
two-agent social choice rule that satisfies Condition $\mu2$ will no longer be
Nash implementable if an additional Condition $\lambda'$ is satisfied.
Moreover, according to a classical two-agent algorithm, this result holds not
only in the quantum world, but also in the macro world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2146</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2146</id><created>2010-05-12</created><authors><author><keyname>Saha</keyname><forenames>Ankan</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>On the Finite Time Convergence of Cyclic Coordinate Descent Methods</title><categories>cs.LG cs.NA</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyclic coordinate descent is a classic optimization method that has witnessed
a resurgence of interest in machine learning. Reasons for this include its
simplicity, speed and stability, as well as its competitive performance on
$\ell_1$ regularized smooth optimization problems. Surprisingly, very little is
known about its finite time convergence behavior on these problems. Most
existing results either just prove convergence or provide asymptotic rates. We
fill this gap in the literature by proving $O(1/k)$ convergence rates (where
$k$ is the iteration counter) for two variants of cyclic coordinate descent
under an isotonicity assumption. Our analysis proceeds by comparing the
objective values attained by the two variants with each other, as well as with
the gradient descent algorithm. We show that the iterates generated by the
cyclic coordinate descent methods remain better than those of gradient descent
uniformly over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2179</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2179</id><created>2010-05-12</created><authors><author><keyname>Li</keyname><forenames>Zhongmou</forenames></author><author><keyname>Xiong</keyname><forenames>Hui</forenames></author><author><keyname>Liu</keyname><forenames>Yanchi</forenames></author></authors><title>Detecting Blackholes and Volcanoes in Directed Networks</title><categories>cs.LG</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we formulate a novel problem for finding blackhole and volcano
patterns in a large directed graph. Specifically, a blackhole pattern is a
group which is made of a set of nodes in a way such that there are only inlinks
to this group from the rest nodes in the graph. In contrast, a volcano pattern
is a group which only has outlinks to the rest nodes in the graph. Both
patterns can be observed in real world. For instance, in a trading network, a
blackhole pattern may represent a group of traders who are manipulating the
market. In the paper, we first prove that the blackhole mining problem is a
dual problem of finding volcanoes. Therefore, we focus on finding the blackhole
patterns. Along this line, we design two pruning schemes to guide the blackhole
finding process. In the first pruning scheme, we strategically prune the search
space based on a set of pattern-size-independent pruning rules and develop an
iBlackhole algorithm. The second pruning scheme follows a divide-and-conquer
strategy to further exploit the pruning results from the first pruning scheme.
Indeed, a target directed graphs can be divided into several disconnected
subgraphs by the first pruning scheme, and thus the blackhole finding can be
conducted in each disconnected subgraph rather than in a large graph. Based on
these two pruning schemes, we also develop an iBlackhole-DC algorithm. Finally,
experimental results on real-world data show that the iBlackhole-DC algorithm
can be several orders of magnitude faster than the iBlackhole algorithm, which
has a huge computational advantage over a brute-force method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2197</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2197</id><created>2010-05-12</created><authors><author><keyname>Acar</keyname><forenames>Evrim</forenames></author><author><keyname>Kolda</keyname><forenames>Tamara G.</forenames></author><author><keyname>Dunlavy</keyname><forenames>Daniel M.</forenames></author><author><keyname>Morup</keyname><forenames>Morten</forenames></author></authors><title>Scalable Tensor Factorizations for Incomplete Data</title><categories>math.NA cs.NA physics.data-an</categories><acm-class>G.1.3; G.1.6</acm-class><journal-ref>Chemometrics and Intelligent Laboratory Systems 106(1):41-56, Mar.
  2011</journal-ref><doi>10.1016/j.chemolab.2010.08.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of incomplete data - i.e., data with missing or unknown values -
in multi-way arrays is ubiquitous in biomedical signal processing, network
traffic analysis, bibliometrics, social network analysis, chemometrics,
computer vision, communication networks, etc. We consider the problem of how to
factorize data sets with missing values with the goal of capturing the
underlying latent structure of the data and possibly reconstructing missing
values (i.e., tensor completion). We focus on one of the most well-known tensor
factorizations that captures multi-linear structure, CANDECOMP/PARAFAC (CP). In
the presence of missing data, CP can be formulated as a weighted least squares
problem that models only the known entries. We develop an algorithm called
CP-WOPT (CP Weighted OPTimization) that uses a first-order optimization
approach to solve the weighted least squares problem. Based on extensive
numerical experiments, our algorithm is shown to successfully factorize tensors
with noise and up to 99% missing data. A unique aspect of our approach is that
it scales to sparse large-scale data, e.g., 1000 x 1000 x 1000 with five
million known entries (0.5% dense). We further demonstrate the usefulness of
CP-WOPT on two real-world applications: a novel EEG (electroencephalogram)
application where missing data is frequently encountered due to disconnections
of electrodes and the problem of modeling computer network traffic where data
may be absent due to the expense of the data collection process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2211</identifier>
 <datestamp>2014-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2211</id><created>2010-05-12</created><authors><author><keyname>Lin</keyname><forenames>Min Chih</forenames></author><author><keyname>Soulignac</keyname><forenames>Francisco J.</forenames></author><author><keyname>Szwarcfiter</keyname><forenames>Jayme L.</forenames></author></authors><title>Arboricity, h-Index, and Dynamic Algorithms</title><categories>cs.DS</categories><comments>19 pages, no figures</comments><doi>10.1016/j.tcs.2011.12.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a modification of a technique by Chiba and Nishizeki
[Chiba and Nishizeki: Arboricity and Subgraph Listing Algorithms, SIAM J.
Comput. 14(1), pp. 210--223 (1985)]. Based on it, we design a data structure
suitable for dynamic graph algorithms. We employ the data structure to
formulate new algorithms for several problems, including counting subgraphs of
four vertices, recognition of diamond-free graphs, cop-win graphs and strongly
chordal graphs, among others. We improve the time complexity for graphs with
low arboricity or h-index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2218</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2218</id><created>2010-05-12</created><updated>2012-09-10</updated><authors><author><keyname>Dumitrescu</keyname><forenames>Adrian</forenames></author><author><keyname>Jiang</keyname><forenames>Minghui</forenames></author><author><keyname>Pach</keyname><forenames>J&#xe1;nos</forenames></author></authors><title>Opaque sets</title><categories>cs.CG cs.DM</categories><comments>18 pages, 7 figures. This version replaces the previous version;
  Lemma 1 and its proof have been revised and simplified</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding &quot;small&quot; sets that meet every straight-line which
intersects a given convex region was initiated by Mazurkiewicz in 1916. We call
such a set an {\em opaque set} or a {\em barrier} for that region. We consider
the problem of computing the shortest barrier for a given convex polygon with
$n$ vertices. No exact algorithm is currently known even for the simplest
instances such as a square or an equilateral triangle. For general barriers, we
present an approximation algorithm with ratio $1/2 + \frac{2
+\sqrt{2}}{\pi}=1.5867...$. For connected barriers we achieve the approximation
ratio 1.5716, while for single-arc barriers we achieve the approximation ratio
$\frac{\pi+5}{\pi+2} = 1.5834...$. All three algorithms run in O(n) time. We
also show that if the barrier is restricted to the (interior and the boundary
of the) input polygon, then the problem admits a fully polynomial-time
approximation scheme for the connected case and a quadratic-time exact
algorithm for the single-arc case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2223</identifier>
 <datestamp>2010-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2223</id><created>2010-05-12</created><authors><author><keyname>Moya-Anegon</keyname><forenames>Felix</forenames></author><author><keyname>Herrero-Solana</keyname><forenames>Victor</forenames></author></authors><title>Worldwide topology of the scientific subject profile: a macro approach
  on the country level</title><categories>cs.DL physics.soc-ph</categories><comments>21 pages, 2 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Models for the production of knowledge and systems of innovation and science
are key elements for characterizing a country in view of its scientific
thematic profile. With regard to scientific output and publication in journals
of international visibility, the countries of the world may be classified into
three main groups according to their thematic bias. This paper aims to classify
the countries of the world in several broad groups, described in terms of
behavioural models that attempt to sum up the characteristics of their systems
of knowledge and innovation. We perceive three clusters in our analysis: 1) the
biomedical cluster, 2) the basic science &amp; engineering cluster, and 3) the
agricultural cluster. The countries are conceptually associated with the
clusters via Principal Component Analysis (PCA), and a Multidimensional Scaling
(MDS) map with all the countries is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2228</identifier>
 <datestamp>2010-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2228</id><created>2010-05-12</created><updated>2010-06-16</updated><authors><author><keyname>McLeish</keyname><forenames>Don</forenames></author></authors><title>A general method for debiasing a Monte Carlo estimator</title><categories>q-fin.CP cs.NA stat.CO</categories><comments>11 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a process, stochastic or deterministic, obtained by using a
numerical integration scheme, or from Monte-Carlo methods involving an
approximation to an integral, or a Newton-Raphson iteration to approximate the
root of an equation. We will assume that we can sample from the distribution of
the process from time 0 to finite time n. We propose a scheme for unbiased
estimation of the limiting value of the process, together with estimates of
standard error and apply this to examples including numerical integrals,
root-finding and option pricing in a Heston Stochastic Volatility model. This
results in unbiased estimators in place of biased ones i nmany potential
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2243</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2243</id><created>2010-05-12</created><authors><author><keyname>Xu</keyname><forenames>Huan</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Robustness and Generalization</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive generalization bounds for learning algorithms based on their
robustness: the property that if a testing sample is &quot;similar&quot; to a training
sample, then the testing error is close to the training error. This provides a
novel approach, different from the complexity or stability arguments, to study
generalization of learning algorithms. We further show that a weak notion of
robustness is both sufficient and necessary for generalizability, which implies
that robustness is a fundamental property for learning algorithms to work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2249</identifier>
 <datestamp>2011-06-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2249</id><created>2010-05-12</created><updated>2011-06-03</updated><authors><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Sparse Recovery with Orthogonal Matching Pursuit under RIP</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new analysis for the orthogonal matching pursuit (OMP)
algorithm. It is shown that if the restricted isometry property (RIP) is
satisfied at sparsity level $O(\bar{k})$, then OMP can recover a
$\bar{k}$-sparse signal in 2-norm. For compressed sensing applications, this
result implies that in order to uniformly recover a $\bar{k}$-sparse signal in
$\Real^d$, only $O(\bar{k} \ln d)$ random projections are needed. This analysis
improves earlier results on OMP that depend on stronger conditions such as
mutual incoherence that can only be satisfied with $\Omega(\bar{k}^2 \ln d)$
random projections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2251</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2251</id><created>2010-05-12</created><authors><author><keyname>Sahin</keyname><forenames>Onur</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Interference Channel with a Half-Duplex Out-of-Band Relay</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures, to appear in Proceedings of IEEE ISIT 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Gaussian interference channel (IC) aided by a half-duplex relay is
considered, in which the relay receives and transmits in an orthogonal band
with respect to the IC. The system thus consists of two parallel channels, the
IC and the channel over which the relay is active, which is referred to as
Out-of-Band Relay Channel (OBRC). The OBRC is operated by separating a multiple
access phase from the sources to the relay and a broadcast phase from the relay
to the destinations. Conditions under which the optimal operation, in terms of
the sum-capacity, entails either signal relaying and/or interference forwarding
by the relay are identified. These conditions also assess the optimality of
either separable or non-separable transmission over the IC and OBRC.
Specifically, the optimality of signal relaying and separable coding is
established for scenarios where the relay-to-destination channels set the
performance bottleneck with respect to the source-to-relay channels on the
OBRC. Optimality of interference forwarding and non-separable operation is also
established in special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2254</identifier>
 <datestamp>2011-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2254</id><created>2010-05-12</created><updated>2011-09-16</updated><authors><author><keyname>Scoville</keyname><forenames>John</forenames></author></authors><title>On Universal Complexity Measures</title><categories>cs.IT cs.CC math.IT</categories><comments>The paper has been withdrawn due to the poor predictive performance
  of circuit complexity vs. universal data compression. Given this fact, as
  well as the high computational complexity, the method seems to have little
  practical utility</comments><msc-class>94A17 (Primary), 03D32 (Secondary), 03B10, 03G05, 94A15, 94C10</msc-class><acm-class>F.1.1; F.4.1; H.1.1; G.2.1; G.2.2; I.2.4; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We relate the computational complexity of finite strings to universal
representations of their underlying symmetries. First, Boolean functions are
classified using the universal covering topologies of the circuits which
enumerate them. A binary string is classified as a fixed point of its
automorphism group; the irreducible representation of this group is the
string's universal covering group. Such a measure may be used to test the
quasi-randomness of binary sequences with regard to first-order set membership.
Next, strings over general alphabets are considered. The complexity of a
general string is given by a universal representation which recursively factors
the codeword number associated with a string. This is the complexity of the
representation recursively decoding a Godel number having the value of the
string; the result is a tree of prime numbers which forms a universal
representation of the string's group symmetries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2263</identifier>
 <datestamp>2011-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2263</id><created>2010-05-13</created><updated>2011-05-30</updated><authors><author><keyname>Dimitrakakis</keyname><forenames>Christos</forenames></author></authors><title>Context models on sequences of covers</title><categories>stat.ML cs.LG</categories><comments>14 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a class of models that, via a simple construction, enables exact,
incremental, non-parametric, polynomial-time, Bayesian inference of conditional
measures. The approach relies upon creating a sequence of covers on the
conditioning variable and maintaining a different model for each set within a
cover. Inference remains tractable by specifying the probabilistic model in
terms of a random walk within the sequence of covers. We demonstrate the
approach on problems of conditional density estimation, which, to our knowledge
is the first closed-form, non-parametric Bayesian approach to this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2267</identifier>
 <datestamp>2010-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2267</id><created>2010-05-13</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Peng</keyname><forenames>Wei</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author><author><keyname>Adachi</keyname><forenames>Fumiyuki</forenames></author></authors><title>A Fast Compressive Channel Estimation with Modified Smoothed L0
  Algorithm</title><categories>cs.IT math.IT</categories><comments>4 pages, 4 figures, ICCACS2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Broadband wireless channel is a time dispersive and becomes strongly
frequency selective. In most cases, the channel is composed of a few dominant
coefficients and a large part of coefficients is approximately zero or zero. To
exploit the sparsity of multi-path channel (MPC), there are various methods
have been proposed. They are, namely, greedy algorithms, iterative algorithms,
and convex program. The former two algorithms are easy to be implemented but
not stable; on the other hand, the last method is stable but difficult to be
implemented as practical channel estimation problems because of computational
complexity. In this paper, we proposed a novel channel estimation strategy by
using modified smoothed (MSL0) algorithm which combines stable and low
complexity. Computer simulations confirm the effectiveness of the introduced
algorithm comparisons with the existing methods. We also give
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2269</identifier>
 <datestamp>2010-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2269</id><created>2010-05-13</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Huang</keyname><forenames>An-min</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author></authors><title>Sparse Multipath Channel Estimation using DS Algorithm in Wideband
  Communication Systems</title><categories>cs.IT cs.NI math.IT</categories><comments>4 pages, 4 figures, Wicom2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wideband wireless channel is a time dispersive channel and becomes strongly
frequency-selective. However, in most cases, the channel is composed of a few
dominant taps and a large part of taps is approximately zero or zero. They are
often called sparse multi-path channels (MPC). Conventional linear MPC methods,
such as the least squares (LS), do not exploit the sparsity of MPC. In general,
accurate sparse MPC estimator can be obtained by solving a LASSO problem even
in the presence of noise. In this paper, a novel CS-based sparse MPC method by
using Dantzig selector (DS) [1] is introduced. This method exploits a channel's
sparsity to reduce the number of training sequence and, hence, increase
spectral efficiency when compared to existed methods with computer simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2270</identifier>
 <datestamp>2010-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2270</id><created>2010-05-13</created><authors><author><keyname>Gui</keyname><forenames>Guan</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author><author><keyname>Peng</keyname><forenames>Wei</forenames></author><author><keyname>Adachi</keyname><forenames>Fumiyuki</forenames></author></authors><title>Sparse Multipath Channel Estimation Using Compressive Sampling Matching
  Pursuit Algorithm</title><categories>cs.IT math.IT</categories><comments>5 pages, 7figures, IEEE APWCS 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wideband wireless channel is a time dispersive channel and becomes strongly
frequency-selective. However, in most cases, the channel is composed of a few
dominant taps and a large part of taps is approximately zero or zero. To
exploit the sparsity of multi-path channel (MPC), two methods have been
proposed. They are, namely, greedy algorithm and convex program. Greedy
algorithm is easy to be implemented but not stable; on the other hand, the
convex program method is stable but difficult to be implemented as practical
channel estimation problems. In this paper, we introduce a novel channel
estimation strategy using compressive sampling matching pursuit (CoSaMP)
algorithm which was proposed in [1]. This algorithm will combine the greedy
algorithm with the convex program method. The effectiveness of the proposed
algorithm will be confirmed through comparisons with the existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2273</identifier>
 <datestamp>2010-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2273</id><created>2010-05-13</created><authors><author><keyname>F&#xfa;ster-Sabater</keyname><forenames>Amparo</forenames></author><author><keyname>Garc&#xed;a-Villalba</keyname><forenames>L. J.</forenames></author></authors><title>Likelihood that a pseudorandom sequence generator has optimal properties</title><categories>cs.CR cs.DM</categories><comments>3 pages, 0 figures</comments><report-no>Electronics Letters Online No: 19980499, INSPEC Accession Number:
  5896277</report-no><msc-class>94A60, 11T71, 14G50</msc-class><acm-class>E.3; F.1.1</acm-class><journal-ref>Electronics Letters. Vol. 34, No. 7, pp. 646-647. April 1998.</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The authors prove that the probability of choosing a nonlinear filter of
m-sequences with optimal properties, that is, maximum period and maximum linear
complexity, tends assymptotically to 1 as the linear feedback shift register
length increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2277</identifier>
 <datestamp>2010-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2277</id><created>2010-05-13</created><authors><author><keyname>F&#xfa;ster-Sabater</keyname><forenames>Amparo</forenames></author><author><keyname>Garc&#xed;a-Mochales</keyname><forenames>Pedro</forenames></author></authors><title>A Simple Computational Model for Acceptance/Rejection of Binary Sequence
  Generators</title><categories>cs.CR cs.DM</categories><comments>16 pages, 0 figures</comments><msc-class>94A60, 11T71, 14G50</msc-class><acm-class>E.3; F.1.1</acm-class><journal-ref>Applied Mathematical Modelling. Volume 31, Issue 8, pp. 1548-1558.
  August 2007.</journal-ref><doi>10.1016/j.apm.2006.05.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple binary model to compute the degree of balancedness in the output
sequence of LFSR-combinational generators has been developed. The computational
method is based exclusively on the handling of binary strings by means of logic
operations. The proposed model can serve as a deterministic alternative to
existing probabilistic methods for checking balancedness in binary sequence
generators. The procedure here described can be devised as a first selective
criterium for acceptance/rejection of this type of generators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2280</identifier>
 <datestamp>2010-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2280</id><created>2010-05-13</created><authors><author><keyname>F&#xfa;ster-Sabater</keyname><forenames>Amparo</forenames></author><author><keyname>de la Gu&#xed;a-Mart&#xed;nez</keyname><forenames>Dolores</forenames></author></authors><title>Modelling Nonlinear Sequence Generators in terms of Linear Cellular
  Automata</title><categories>cs.CR cs.DM</categories><comments>15 pages, 0 figures</comments><msc-class>94A60, 11T71, 14G50</msc-class><acm-class>E.3; F.1.1</acm-class><journal-ref>Applied Mathematical Modelling. Volume 31, Issue 2, pp. 226-235.
  February 2007.</journal-ref><doi>10.1016/j.apm.2005.08.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a wide family of LFSR-based sequence generators, the so-called
Clock-Controlled Shrinking Generators (CCSGs), has been analyzed and identified
with a subset of linear Cellular Automata (CA). In fact, a pair of linear
models describing the behavior of the CCSGs can be derived. The algorithm that
converts a given CCSG into a CA-based linear model is very simple and can be
applied to CCSGs in a range of practical interest. The linearity of these
cellular models can be advantageously used in two different ways: (a) for the
analysis and/or cryptanalysis of the CCSGs and (b) for the reconstruction of
the output sequence obtained from this kind of generators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2281</identifier>
 <datestamp>2010-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2281</id><created>2010-05-13</created><authors><author><keyname>Cohen</keyname><forenames>Greg</forenames></author></authors><title>A new algebraic technique for polynomial-time computing the number
  modulo 2 of Hamiltonian decompositions and similar partitions of a graph's
  edge set</title><categories>cs.DM</categories><comments>The present article introduces a new algebraic technique which
  generalizes the notion of counting modulo 2 via applying fields of
  Characteristic 2 and determinants</comments><msc-class>-----</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Graph Theory a number of results were devoted to studying the
computational complexity of the number modulo 2 of a graph's edge set
decompositions of various kinds, first of all including its Hamiltonian
decompositions, as well as the number modulo 2 of, say, Hamiltonian
cycles/paths etc. While the problems of finding a Hamiltonian decomposition and
Hamiltonian cycle are NP-complete, counting these objects modulo 2 in
polynomial time is yet possible for certain types of regular undirected graphs.
Some of the most known examples are the theorems about the existence of an even
number of Hamiltonian decompositions in a 4-regular graph and an even number of
such decompositions where two given edges e and g belong to different cycles
(Thomason, 1978), as well as an even number of Hamiltonian cycles passing
through any given edge in a regular odd-degreed graph (Smith's theorem). The
present article introduces a new algebraic technique which generalizes the
notion of counting modulo 2 via applying fields of Characteristic 2 and
determinants and, for instance, allows to receive a polynomial-time formula for
the number modulo 2 of a 4-regular bipartite graph's Hamiltonian decompositions
such that a given edge and a given path of length 2 belong to different
Hamiltonian cycles - hence refining/extending (in a computational sense)
Thomason's result for bipartite graphs. This technique also provides a
polynomial-time calculation of the number modulo 2 of a graph's edge set
decompositions into simple cycles each containing at least one element of a
given set of its edges what is a similar kind of extension of Thomason's
theorem as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2296</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2296</id><created>2010-05-13</created><updated>2010-05-20</updated><authors><author><keyname>Cesa-Bianchi</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>Online Learning of Noisy Data with Kernels</title><categories>cs.LG</categories><comments>This is a full version of the paper appearing in the 23rd
  International Conference on Learning Theory (COLT 2010)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study online learning when individual instances are corrupted by
adversarially chosen random noise. We assume the noise distribution is unknown,
and may change over time with no restriction other than having zero mean and
bounded variance. Our technique relies on a family of unbiased estimators for
non-linear functions, which may be of independent interest. We show that a
variant of online gradient descent can learn functions in any dot-product
(e.g., polynomial) or Gaussian kernel space with any analytic convex loss
function. Our variant uses randomized estimates that need to query a random
number of noisy copies of each instance, where with high probability this
number is upper bounded by a constant. Allowing such multiple queries cannot be
avoided: Indeed, we show that online learning is in general impossible when
only one noisy copy of each instance can be accessed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2299</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2299</id><created>2010-05-13</created><authors><author><keyname>Calcavecchia</keyname><forenames>Nicolo' Maria</forenames></author><author><keyname>Di Nitto</keyname><forenames>Elisabetta</forenames></author></authors><title>Incorporating prediction models in the SelfLet framework: a plugin
  approach</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A complex pervasive system is typically composed of many cooperating
\emph{nodes}, running on machines with different capabilities, and pervasively
distributed across the environment. These systems pose several new challenges
such as the need for the nodes to manage autonomously and dynamically in order
to adapt to changes detected in the environment. To address the above issue, a
number of autonomic frameworks has been proposed. These usually offer either
predefined self-management policies or programmatic mechanisms for creating new
policies at design time. From a more theoretical perspective, some works
propose the adoption of prediction models as a way to anticipate the evolution
of the system and to make timely decisions. In this context, our aim is to
experiment with the integration of prediction models within a specific
autonomic framework in order to assess the feasibility of such integration in a
setting where the characteristics of dynamicity, decentralization, and
cooperation among nodes are important. We extend an existing infrastructure
called \emph{SelfLets} in order to make it ready to host various prediction
models that can be dynamically plugged and unplugged in the various component
nodes, thus enabling a wide range of predictions to be performed. Also, we show
in a simple example how the system works when adopting a specific prediction
model from the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2303</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2303</id><created>2010-05-13</created><authors><author><keyname>Jones</keyname><forenames>Jeff</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Towards Physarum Binary Adders</title><categories>nlin.PS cs.AI physics.bio-ph q-bio.CB</categories><comments>Biosystems (2010), in press. Please download final version of the
  paper from the Publishers's site</comments><journal-ref>Biosystems Volume 101, Issue 1, July 2010, Pages 51-58</journal-ref><doi>10.1016/j.biosystems.2010.04.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Plasmodium of \emph{Physarum polycephalum} is a single cell visible by
unaided eye. The plasmodium's foraging behaviour is interpreted in terms of
computation. Input data is a configuration of nutrients, result of computation
is a network of plasmodium's cytoplasmic tubes spanning sources of nutrients.
Tsuda et al (2004) experimentally demonstrated that basic logical gates can be
implemented in foraging behaviour of the plasmodium. We simplify the original
designs of the gates and show --- in computer models --- that the plasmodium is
capable for computation of two-input two-output gate $&lt;x, y&gt; \to &lt;xy, x+y&gt;$ and
three-input two-output $&lt;x, y, z&gt; \to &lt; \bar{x}yz, x+y+z&gt;$. We assemble the
gates in a binary one-bit adder and demonstrate validity of the design using
computer simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2305</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2305</id><created>2010-05-13</created><updated>2010-09-02</updated><authors><author><keyname>Kolmogorov</keyname><forenames>Vladimir</forenames></author></authors><title>Generalized roof duality and bisubmodular functions</title><categories>cs.DM</categories><comments>14 pages. Shorter version to appear in NIPS 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a convex relaxation $\hat f$ of a pseudo-boolean function $f$. We
say that the relaxation is {\em totally half-integral} if $\hat f(x)$ is a
polyhedral function with half-integral extreme points $x$, and this property is
preserved after adding an arbitrary combination of constraints of the form
$x_i=x_j$, $x_i=1-x_j$, and $x_i=\gamma$ where $\gamma\in\{0, 1, 1/2}$ is a
constant. A well-known example is the {\em roof duality} relaxation for
quadratic pseudo-boolean functions $f$. We argue that total half-integrality is
a natural requirement for generalizations of roof duality to arbitrary
pseudo-boolean functions. Our contributions are as follows. First, we provide a
complete characterization of totally half-integral relaxations $\hat f$ by
establishing a one-to-one correspondence with {\em bisubmodular functions}.
Second, we give a new characterization of bisubmodular functions. Finally, we
show some relationships between general totally half-integral relaxations and
relaxations based on the roof duality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2308</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2308</id><created>2010-05-13</created><authors><author><keyname>Henneken</keyname><forenames>Edwin A.</forenames></author><author><keyname>Kurtz</keyname><forenames>Michael J.</forenames></author><author><keyname>Accomazzi</keyname><forenames>Alberto</forenames></author><author><keyname>Grant</keyname><forenames>Carolyn</forenames></author><author><keyname>Thompson</keyname><forenames>Donna</forenames></author><author><keyname>Bohlen</keyname><forenames>Elizabeth</forenames></author><author><keyname>Di Milia</keyname><forenames>Giovanni</forenames></author><author><keyname>Luker</keyname><forenames>Jay</forenames></author><author><keyname>Murray</keyname><forenames>Stephen S.</forenames></author></authors><title>Finding Your Literature Match -- A Recommender System</title><categories>cs.DL cs.IR</categories><comments>Contribution to the proceedings of the colloquium Future Professional
  Communication in Astronomy II, 13-14 April 2010, Cambridge, Massachusetts. 11
  pages, 4 figures.</comments><doi>10.1007/978-1-4419-8369-5_14</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The universe of potentially interesting, searchable literature is expanding
continuously. Besides the normal expansion, there is an additional influx of
literature because of interdisciplinary boundaries becoming more and more
diffuse. Hence, the need for accurate, efficient and intelligent search tools
is bigger than ever. Even with a sophisticated search engine, looking for
information can still result in overwhelming results. An overload of
information has the intrinsic danger of scaring visitors away, and any
organization, for-profit or not-for-profit, in the business of providing
scholarly information wants to capture and keep the attention of its target
audience. Publishers and search engine engineers alike will benefit from a
service that is able to provide visitors with recommendations that closely meet
their interests. Providing visitors with special deals, new options and
highlights may be interesting to a certain degree, but what makes more sense
(especially from a commercial point of view) than to let visitors do most of
the work by the mere action of making choices? Hiring psychics is not an
option, so a technological solution is needed to recommend items that a visitor
is likely to be looking for. In this presentation we will introduce such a
solution and argue that it is practically feasible to incorporate this approach
into a useful addition to any information retrieval system with enough usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2314</identifier>
 <datestamp>2010-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2314</id><created>2010-05-13</created><authors><author><keyname>Brent</keyname><forenames>Richard P.</forenames></author></authors><title>Some comments on C. S. Wallace's random number generators</title><categories>cs.MS stat.CO</categories><comments>13 pages. For further information, see
  http://wwwmaths.anu.edu.au/~brent/pub/pub213.html</comments><msc-class>11K45 (Primary) 65-03, 65C10 (Secondary)</msc-class><acm-class>G.3; G.4; K.2</acm-class><journal-ref>The Computer Journal 51, 5 (Sept. 2008), 579-584.</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We outline some of Chris Wallace's contributions to pseudo-random number
generation. In particular, we consider his idea for generating normally
distributed variates without relying on a source of uniform random numbers, and
compare it with more conventional methods for generating normal random numbers.
Implementations of Wallace's idea can be very fast (approximately as fast as
good uniform generators). We discuss the statistical quality of the output, and
mention how certain pitfalls can be avoided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2321</identifier>
 <datestamp>2010-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2321</id><created>2010-05-13</created><authors><author><keyname>Mitran</keyname><forenames>Patrick</forenames></author></authors><title>Typical Sequences for Polish Alphabets</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory. 29 pages.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of typical sequences plays a key role in the theory of
information. Central to the idea of typicality is that a sequence $x_1, x_2,
..., x_n$ that is $P_X$-typical should, loosely speaking, have an empirical
distribution that is in some sense close to the distribution $P_X$. The two
most common notions of typicality are that of strong (letter) typicality and
weak (entropy) typicality. While weak typicality allows one to apply many
arguments that can be made with strongly typical arguments, some arguments for
strong typicality cannot be generalized to weak typicality. In this paper, we
consider an alternate definition of typicality, namely one based on the weak*
topology and that is applicable to Polish alphabets (which includes
$\reals^n$). This notion is a generalization of strong typicality in the sense
that it degenerates to strong typicality in the finite alphabet case, and can
also be applied to mixed and continuous distributions. Furthermore, it is
strong enough to prove a Markov lemma, and thus can be used to directly prove a
more general class of results than weak typicality. As an example of this
technique, we directly prove achievability for Gel'fand-Pinsker channels with
input constraints for a large class of alphabets and channels without first
proving a finite alphabet result and then resorting to delicate quantization
arguments. While this large class does not include Gaussian distributions with
power constraints, it is shown to be straightforward to recover this case by
considering a sequence of truncated Gaussian distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2329</identifier>
 <datestamp>2010-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2329</id><created>2010-05-13</created><authors><author><keyname>Bloom</keyname><forenames>Stephen L.</forenames></author><author><keyname>Zhang</keyname><forenames>YiDi</forenames></author></authors><title>A Note on Ordinal DFAs</title><categories>cs.FL</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the following theorem. Suppose that $M$ is a trim DFA on the Boolean
alphabet $0,1$. The language $\L(M)$ is well-ordered by the lexicographic order
$\slex$ iff whenever the non sink states $q,q.0$ are in the same strong
component, then $q.1$ is a sink. It is easy to see that this property is
sufficient. In order to show the necessity, we analyze the behavior of a
$\slex$-descending sequence of words. This property is used to obtain a
polynomial time algorithm to determine, given a DFA $M$, whether $\L(M)$ is
well-ordered by the lexicographic order. Last, we apply an argument in
\cite{BE,BEa} to give a proof that the least nonregular ordinal is
$\omega^\omega $.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2340</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2340</id><created>2010-05-13</created><updated>2010-07-20</updated><authors><author><keyname>Brotherston</keyname><forenames>James</forenames><affiliation>Imperial College London</affiliation></author><author><keyname>Calcagno</keyname><forenames>Cristiano</forenames><affiliation>Imperial College London</affiliation></author></authors><title>Classical BI: Its Semantics and Proof Theory</title><categories>cs.LO</categories><comments>42 pages, 8 figures</comments><proxy>LMCS</proxy><acm-class>F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 6, Issue 3 (July 20,
  2010) lmcs:1014</journal-ref><doi>10.2168/LMCS-6(3:3)2010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Classical BI (CBI), a new addition to the family of bunched logics
which originates in O'Hearn and Pym's logic of bunched implications BI. CBI
differs from existing bunched logics in that its multiplicative connectives
behave classically rather than intuitionistically (including in particular a
multiplicative version of classical negation). At the semantic level,
CBI-formulas have the normal bunched logic reading as declarative statements
about resources, but its resource models necessarily feature more structure
than those for other bunched logics; principally, they satisfy the requirement
that every resource has a unique dual. At the proof-theoretic level, a very
natural formalism for CBI is provided by a display calculus \`a la Belnap,
which can be seen as a generalisation of the bunched sequent calculus for BI.
In this paper we formulate the aforementioned model theory and proof theory for
CBI, and prove some fundamental results about the logic, most notably
completeness of the proof theory with respect to the semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2364</identifier>
 <datestamp>2010-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2364</id><created>2010-05-13</created><updated>2010-05-14</updated><authors><author><keyname>Nannen</keyname><forenames>Volker</forenames></author></authors><title>A Short Introduction to Model Selection, Kolmogorov Complexity and
  Minimum Description Length (MDL)</title><categories>cs.LG cs.CC</categories><comments>20 pages, Chapter 1 of The Paradox of Overfitting, Master's thesis,
  Rijksuniversiteit Groningen, 2003</comments><msc-class>F.2.3</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of overfitting in model selection is explained and demonstrated
with an example. After providing some background information on information
theory and Kolmogorov complexity, we provide a short explanation of Minimum
Description Length and error minimization. We conclude with a discussion of the
typical features of overfitting in model selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2393</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2393</id><created>2010-05-13</created><authors><author><keyname>Li</keyname><forenames>L. Erran</forenames></author><author><keyname>Nowlan</keyname><forenames>M. F.</forenames></author><author><keyname>Yang</keyname><forenames>Y. R.</forenames></author></authors><title>Mosaic: Policy Homomorphic Network Extension</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advent of large-scale cloud computing infrastructure, network
extension and migration has emerged as a major challenge in the management of
modern enterprise networks. Many enterprises are considering extending or
relocating their network components, in whole or in part, to remote, private
and public data centers, in order to attain scalability, failure resilience,
and cost savings for their network applications. In this paper, we conduct a
first rigorous study on the extension and migration of an enterprise network
while preserving its performance and security requirements, such as layer
2/layer 3 reachability, and middle-box traversal through load balancer,
intrusion detection and ACLs. We formulate this increasingly important problem,
present preliminary designs, and conduct experiments to validate the
feasibility of our designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2395</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2395</id><created>2010-05-13</created><updated>2011-08-10</updated><authors><author><keyname>Krivine</keyname><forenames>Jean-Louis</forenames></author></authors><title>Realizability algebras: a program to well order R</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 3 (August 9,
  2011) lmcs:1070</journal-ref><doi>10.2168/LMCS-7(3:2)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of classical realizability is a framework in which we can develop
the proof-program correspondence. Using this framework, we show how to
transform into programs the proofs in classical analysis with dependent choice
and the existence of a well ordering of the real line. The principal tools are:
The notion of realizability algebra, which is a three-sorted variant of the
well known combinatory algebra of Curry. An adaptation of the method of forcing
used in set theory to prove consistency results. Here, it is used in another
way, to obtain programs associated with a well ordering of R and the existence
of a non trivial ultrafilter on N.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2400</identifier>
 <datestamp>2010-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2400</id><created>2010-05-13</created><updated>2010-05-14</updated><authors><author><keyname>Nannen</keyname><forenames>Volker</forenames></author></authors><title>A Short Introduction to Kolmogorov Complexity</title><categories>cs.CC</categories><comments>7 pages; from The Paradox of Overfitting, Master's thesis,
  Rijksuniversiteit Groningen, 2003</comments><msc-class>F.2.3</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a short introduction to Kolmogorov Complexity. The interested reader
is referred to the text books by Cover &amp; Thomas as well as Li &amp; V\'itanyi,
which cover the fields of information theory and Kolmogorov complexity in depth
and with all the necessary rigor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2405</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2405</id><created>2010-05-13</created><updated>2010-06-24</updated><authors><author><keyname>Candogan</keyname><forenames>Ozan</forenames></author><author><keyname>Menache</keyname><forenames>Ishai</forenames></author><author><keyname>Ozdaglar</keyname><forenames>Asuman</forenames></author><author><keyname>Parrilo</keyname><forenames>Pablo A.</forenames></author></authors><title>Flows and Decompositions of Games: Harmonic and Potential Games</title><categories>cs.GT math.OC</categories><journal-ref>Mathematics of Operations Research, Vol. 36, No. 3, pp. 474-503,
  2011</journal-ref><doi>10.1287/moor.1110.0500</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a novel flow representation for finite games in
strategic form. This representation allows us to develop a canonical direct sum
decomposition of an arbitrary game into three components, which we refer to as
the potential, harmonic and nonstrategic components. We analyze natural classes
of games that are induced by this decomposition, and in particular, focus on
games with no harmonic component and games with no potential component. We show
that the first class corresponds to the well-known potential games. We refer to
the second class of games as harmonic games, and study the structural and
equilibrium properties of this new class of games. Intuitively, the potential
component of a game captures interactions that can equivalently be represented
as a common interest game, while the harmonic part represents the conflicts
between the interests of the players. We make this intuition precise, by
studying the properties of these two classes, and show that indeed they have
quite distinct and remarkable characteristics. For instance, while finite
potential games always have pure Nash equilibria, harmonic games generically
never do. Moreover, we show that the nonstrategic component does not affect the
equilibria of a game, but plays a fundamental role in their efficiency
properties, thus decoupling the location of equilibria and their payoff-related
properties. Exploiting the properties of the decomposition framework, we obtain
explicit expressions for the projections of games onto the subspaces of
potential and harmonic games. This enables an extension of the properties of
potential and harmonic games to &quot;nearby&quot; games. We exemplify this point by
showing that the set of approximate equilibria of an arbitrary game can be
characterized through the equilibria of its projection onto the set of
potential games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2443</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2443</id><created>2010-05-13</created><authors><author><keyname>Kurniawan</keyname><forenames>E.</forenames></author><author><keyname>Sun</keyname><forenames>S.</forenames></author><author><keyname>Yen</keyname><forenames>K.</forenames></author><author><keyname>Chong</keyname><forenames>K. F. E.</forenames></author></authors><title>Network Coded Transmission of Fountain Codes over Cooperative Relay
  Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a transmission strategy of fountain codes over cooperative
relay networks is proposed. When more than one relay nodes are available, we
apply network coding to fountain-coded packets. By doing this, partial
information is made available to the destination node about the upcoming
message block. It is therefore able to reduce the required number of
transmissions over erasure channels, hence increasing the effective throughput.
Its application to wireless channels with Rayleigh fading and AWGN noise is
also analysed, whereby the role of analogue network coding and optimal weight
selection is demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2452</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2452</id><created>2010-05-14</created><updated>2014-04-24</updated><authors><author><keyname>LaMar</keyname><forenames>M. Drew</forenames></author></authors><title>Split digraphs</title><categories>cs.DM math.CO</categories><comments>14 pages, 2 figures; Accepted author manuscript (AAM) version</comments><msc-class>05C20, 05C69, 05C75</msc-class><journal-ref>M. Drew LaMar. Split Digraphs. Discrete Mathematics,
  312(7):1314-1325, 2012</journal-ref><doi>10.1016/j.disc.2011.12.023</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the class of split graphs to the directed case and show that
these split digraphs can be identified from their degree sequences. The first
degree sequence characterization is an extension of the concept of splittance
to directed graphs, while the second characterization says a digraph is split
if and only if its degree sequence satisfies one of the Fulkerson inequalities
(which determine when an integer-pair sequence is digraphic) with equality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2465</identifier>
 <datestamp>2010-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2465</id><created>2010-05-14</created><updated>2010-07-05</updated><authors><author><keyname>Madgazin</keyname><forenames>Vadim R.</forenames></author></authors><title>Dichotic harmony for the musical practice</title><categories>cs.SD</categories><comments>14 pages, in Russian, changed content</comments><acm-class>H.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dichotic method of hearing sound adapts in the region of musical harmony.
The algorithm of the separation of the being dissonant voices into several
separate groups is proposed. For an increase in the pleasantness of chords the
different groups of voices are heard out through the different channels of
headphones. Is created two demonstration program for PC. Keywords: music,
harmony, chord, dichotic listening, dissonance, consonance, headphones,
pleasantness, midi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2469</identifier>
 <datestamp>2010-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2469</id><created>2010-05-14</created><updated>2010-08-04</updated><authors><author><keyname>Imran</keyname><forenames>Nomica</forenames><affiliation>Monash University, Australia</affiliation></author><author><keyname>Khan</keyname><forenames>Salman</forenames><affiliation>Kyung Hee University, Korea</affiliation></author><author><keyname>Rao</keyname><forenames>Imran</forenames><affiliation>The University of Melbourne, Australia</affiliation></author></authors><title>A Trustworthy and well-organized data disseminating scheme for ad-hoc
  wsns</title><categories>cs.NI</categories><comments>12 Pages, IJCNC 2010</comments><journal-ref>International Journal of Computer Networks &amp; Communications 2.3
  (2010) 170-181</journal-ref><doi>10.5121/ijcnc.2010.2313</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Wireless Sensor Networks (WSNs) generate massive amount of live data and
events sensed through dispersedly deployed tiny sensors. This generated data
needed to be disseminate to the sink with slight consumption of network
resources. One of the ways to efficiently transmit this bulk data is gossiping.
An important consideration in gossip-based dissemination protocols is to keep
routing table up to date. Considering the inherent resource constrained nature
of adhoc wireless sensor networks, we propose a gossip based protocol that
consumes little resources. Our proposed scheme aims to keep the routing table
size R as low as possible yet it ensures that the diameter is small too. We
learned the performance of our proposed protocol through simulations. Results
show that our proposed protocol attains major improvement in network
reachability and connectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2499</identifier>
 <datestamp>2010-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2499</id><created>2010-05-14</created><authors><author><keyname>Sanyal</keyname><forenames>S.</forenames></author><author><keyname>Iyengar</keyname><forenames>S.</forenames></author><author><keyname>Roy</keyname><forenames>A. A.</forenames></author><author><keyname>Karnik</keyname><forenames>N. N.</forenames></author><author><keyname>Mengale</keyname><forenames>N. M.</forenames></author><author><keyname>Menon</keyname><forenames>S. B.</forenames></author><author><keyname>Feng</keyname><forenames>Wu Geng</forenames></author></authors><title>Defuzzification Method for a Faster and More Accurate Control</title><categories>cs.OH</categories><comments>3 Pages, 4 Figures, TENCON-1993, Beijing, 1993, Region 10
  International Conference on 'Computers, Communications, Control and Power
  Engineering', Vol. 4, pp. 316-318.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today manufacturers are using fuzzy logic in everything from cameras to
industrial process control. Fuzzy logic controllers are easier to design and so
are cheaper to produce. Fuzzy logic captures the impreciseness inherent in most
input data. Electromechanical controllers respond better to imprecise input if
their behavior was modeled on spontaneous human reasoning. In a conventional
PID controller, what is modeled is the system or process being controlled,
whereas in the Fuzzy logic controller, the focus is the human operator
behavior. In the first case, the system is modeled analytically by a set of
differential equations and their solutions tells the PID controllers how to
adjust the system's control parameters for each type of behavior required 3. In
the Fuzzy controller these adjustments are handled by a Fuzzy rule based expert
system. A logical model of the thinking process a person might go through in
the course of manipulating the system. This shift in focus from process to
person involved changes the entire approach to automatic control problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2514</identifier>
 <datestamp>2010-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2514</id><created>2010-05-14</created><authors><author><keyname>Cassaigne</keyname><forenames>Julien</forenames></author><author><keyname>Richomme</keyname><forenames>Gw&#xe9;na&#xeb;l</forenames></author><author><keyname>Saari</keyname><forenames>Kalle</forenames></author><author><keyname>Zamboni</keyname><forenames>Luca Q.</forenames></author></authors><title>Avoiding Abelian powers in binary words with bounded Abelian complexity</title><categories>math.CO cs.DM</categories><comments>16 pages, submitted</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of Abelian complexity of infinite words was recently used by the
three last authors to investigate various Abelian properties of words. In
particular, using van der Waerden's theorem, they proved that if a word avoids
Abelian $k$-powers for some integer $k$, then its Abelian complexity is
unbounded. This suggests the following question: How frequently do Abelian
$k$-powers occur in a word having bounded Abelian complexity? In particular,
does every uniformly recurrent word having bounded Abelian complexity begin in
an Abelian $k$-power? While this is true for various classes of uniformly
recurrent words, including for example the class of all Sturmian words, in this
paper we show the existence of uniformly recurrent binary words, having bounded
Abelian complexity, which admit an infinite number of suffixes which do not
begin in an Abelian square. We also show that the shift orbit closure of any
infinite binary overlap-free word contains a word which avoids Abelian cubes in
the beginning. We also consider the effect of morphisms on Abelian complexity
and show that the morphic image of a word having bounded Abelian complexity has
bounded Abelian complexity. Finally, we give an open problem on avoidability of
Abelian squares in infinite binary words and show that it is equivalent to a
well-known open problem of Pirillo-Varricchio and Halbeisen-Hungerb\&quot;uhler.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2533</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2533</id><created>2010-05-14</created><updated>2010-09-28</updated><authors><author><keyname>Yeung</keyname><forenames>C. H.</forenames></author><author><keyname>Cimini</keyname><forenames>G.</forenames></author><author><keyname>Jin</keyname><forenames>C. -H.</forenames></author></authors><title>Dynamics underlying Box-office: Movie Competition on Recommender Systems</title><categories>physics.soc-ph cs.IR</categories><comments>8 pages, 6 figures</comments><journal-ref>Phys. Rev. E Vol. 83, 016105 (2011)</journal-ref><doi>10.1103/PhysRevE.83.016105</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a simple model to study movie competition in the recommender
systems. Movies of heterogeneous quality compete against each other through
viewers' reviews and generate interesting dynamics of box-office. By assuming
mean-field interactions between the competing movies, we show that run-away
effect of popularity spreading is triggered by defeating the average review
score, leading to hits in box-office. The average review score thus
characterizes the critical movie quality necessary for transition from
box-office bombs to blockbusters. The major factors affecting the critical
review score are examined. By iterating the mean-field dynamical equations, we
obtain qualitative agreements with simulations and real systems in the
dynamical forms of box-office, revealing the significant role of competition in
understanding box-office dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2534</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2534</id><created>2010-05-14</created><authors><author><keyname>Mart&#xed;nez-Mateo</keyname><forenames>Jes&#xfa;s</forenames></author><author><keyname>Munoz-Hernandez</keyname><forenames>Susana</forenames></author><author><keyname>P&#xe9;rez-Rey</keyname><forenames>David</forenames></author></authors><title>A Discussion of Thin Client Technology for Computer Labs</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer literacy is not negotiable for any professional in an increasingly
computerised environment. Educational institutions should be equipped to
provide this new basic training for modern life. Accordingly, computer labs are
an essential medium for education in almost any field. Computer labs are one of
the most popular IT infrastructures for technical training in primary and
secondary schools, universities and other educational institutions all over the
world. Unfortunately, a computer lab is expensive, in terms of both initial
purchase and annual maintenance costs, and especially when we want to run the
latest software. Hence, research efforts addressing computer lab efficiency,
performance or cost reduction would have a worldwide repercussion. In response
to this concern, this paper presents a survey on thin client technology for
computer labs in educational environments. Besides setting out the advantages
and drawbacks of this technology, we aim to refute false prejudices against
thin clients, identifying a set of educational scenarios where thin clients are
a better choice and others requiring traditional solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2544</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2544</id><created>2010-05-14</created><authors><author><keyname>Liang</keyname><forenames>Quanquan</forenames></author><author><keyname>Liu</keyname><forenames>Mingyan</forenames></author></authors><title>Channel Estimation for Opportunistic Spectrum Access: Uniform and Random
  Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The knowledge of channel statistics can be very helpful in making sound
opportunistic spectrum access decisions. It is therefore desirable to be able
to efficiently and accurately estimate channel statistics. In this paper we
study the problem of optimally placing sensing times over a time window so as
to get the best estimate on the parameters of an on-off renewal channel. We are
particularly interested in a sparse sensing regime with a small number of
samples relative to the time window size. Using Fisher information as a
measure, we analytically derive the best and worst sensing sequences under a
sparsity condition. We also present a way to derive the best/worst sequences
without this condition using a dynamic programming approach. In both cases the
worst turns out to be the uniform sensing sequence, where sensing times are
evenly spaced within the window. With these results we argue that without a
priori knowledge, a robust sensing strategy should be a randomized strategy. We
then compare different random schemes using a family of distributions generated
by the circular $\beta$ ensemble, and propose an adaptive sensing scheme to
effectively track time-varying channel parameters. We further discuss the
applicability of compressive sensing for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2567</identifier>
 <datestamp>2010-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2567</id><created>2010-05-14</created><authors><author><keyname>Cornejo</keyname><forenames>Alejandro</forenames></author><author><keyname>Kuhn</keyname><forenames>Fabian</forenames></author></authors><title>Deploying Wireless Networks with Beeps</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the \emph{discrete beeping} communication model, which assumes
nodes have minimal knowledge about their environment and severely limited
communication capabilities. Specifically, nodes have no information regarding
the local or global structure of the network, don't have access to synchronized
clocks and are woken up by an adversary. Moreover, instead on communicating
through messages they rely solely on carrier sensing to exchange information.
We study the problem of \emph{interval coloring}, a variant of vertex coloring
specially suited for the studied beeping model. Given a set of resources, the
goal of interval coloring is to assign every node a large contiguous fraction
of the resources, such that neighboring nodes share no resources. To highlight
the importance of the discreteness of the model, we contrast it against a
continuous variant described in [17]. We present an O(1$ time algorithm that
terminates with probability 1 and assigns an interval of size
$\Omega(T/\Delta)$ that repeats every $T$ time units to every node of the
network. This improves an $O(\log n)$ time algorithm with the same guarantees
presented in \cite{infocom09}, and accentuates the unrealistic assumptions of
the continuous model. Under the more realistic discrete model, we present a Las
Vegas algorithm that solves $\Omega(T/\Delta)$-interval coloring in $O(\log n)$
time with high probability and describe how to adapt the algorithm for dynamic
networks where nodes may join or leave. For constant degree graphs we prove a
lower bound of $\Omega(\log n)$ on the time required to solve interval coloring
for this model against randomized algorithms. This lower bound implies that our
algorithm is asymptotically optimal for constant degree graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2581</identifier>
 <datestamp>2011-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2581</id><created>2010-05-14</created><updated>2011-05-16</updated><authors><author><keyname>Karimi</keyname><forenames>Kamran</forenames></author><author><keyname>Dickson</keyname><forenames>Neil G.</forenames></author><author><keyname>Hamze</keyname><forenames>Firas</forenames></author></authors><title>A Performance Comparison of CUDA and OpenCL</title><categories>cs.PF cs.DC physics.comp-ph</categories><comments>12 pages, 6 Tables, 5 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CUDA and OpenCL are two different frameworks for GPU programming. OpenCL is
an open standard that can be used to program CPUs, GPUs, and other devices from
different vendors, while CUDA is specific to NVIDIA GPUs. Although OpenCL
promises a portable language for GPU programming, its generality may entail a
performance penalty. In this paper, we use complex, near-identical kernels from
a Quantum Monte Carlo application to compare the performance of CUDA and
OpenCL. We show that when using NVIDIA compiler tools, converting a CUDA kernel
to an OpenCL kernel involves minimal modifications. Making such a kernel
compile with ATI's build tools involves more modifications. Our performance
tests measure and compare data transfer times to and from the GPU, kernel
execution times, and end-to-end application execution times for both CUDA and
OpenCL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2603</identifier>
 <datestamp>2010-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2603</id><created>2010-05-14</created><updated>2010-07-12</updated><authors><author><keyname>Mirzal</keyname><forenames>Andri</forenames></author><author><keyname>Furukawa</keyname><forenames>Masashi</forenames></author></authors><title>Eigenvectors for clustering: Unipartite, bipartite, and directed graph
  cases</title><categories>cs.LG math.SP</categories><comments>9 pages, no figure, to appear in ICEIE 2010</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents a concise tutorial on spectral clustering for broad
spectrum graphs which include unipartite (undirected) graph, bipartite graph,
and directed graph. We show how to transform bipartite graph and directed graph
into corresponding unipartite graph, therefore allowing a unified treatment to
all cases. In bipartite graph, we show that the relaxed solution to the $K$-way
co-clustering can be found by computing the left and right eigenvectors of the
data matrix. This gives a theoretical basis for $K$-way spectral co-clustering
algorithms proposed in the literatures. We also show that solving row and
column co-clustering is equivalent to solving row and column clustering
separately, thus giving a theoretical support for the claim: ``column
clustering implies row clustering and vice versa''. And in the last part, we
generalize the Ky Fan theorem---which is the central theorem for explaining
spectral clustering---to rectangular complex matrix motivated by the results
from bipartite graph analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2613</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2613</id><created>2010-05-14</created><updated>2010-12-04</updated><authors><author><keyname>Candes</keyname><forenames>Emmanuel J.</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Needell</keyname><forenames>Deanna</forenames></author><author><keyname>Randall</keyname><forenames>Paige</forenames></author></authors><title>Compressed Sensing with Coherent and Redundant Dictionaries</title><categories>math.NA cs.IT math.IT</categories><msc-class>94A12, 41A45, 42A10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents novel results concerning the recovery of signals from
undersampled data in the common situation where such signals are not sparse in
an orthonormal basis or incoherent dictionary, but in a truly redundant
dictionary. This work thus bridges a gap in the literature and shows not only
that compressed sensing is viable in this context, but also that accurate
recovery is possible via an L1-analysis optimization problem. We introduce a
condition on the measurement/sensing matrix, which is a natural generalization
of the now well-known restricted isometry property, and which guarantees
accurate recovery of signals that are nearly sparse in (possibly) highly
overcomplete and coherent dictionaries. This condition imposes no incoherence
restriction on the dictionary and our results may be the first of this kind. We
discuss practical examples and the implications of our results on those
applications, and complement our study by demonstrating the potential of
L1-analysis for such problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2616</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2616</id><created>2010-05-14</created><authors><author><keyname>Batu</keyname><forenames>Tugkan</forenames></author><author><keyname>Berenbrink</keyname><forenames>Petra</forenames></author><author><keyname>Cooper</keyname><forenames>Colin</forenames></author></authors><title>Chains-into-Bins Processes</title><categories>cs.DS</categories><acm-class>F.2.2; G.3</acm-class><doi>10.1007/978-3-642-19222-7_32</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of {\em balls-into-bins processes} or {\em occupancy problems} has
a long history. These processes can be used to translate realistic problems
into mathematical ones in a natural way. In general, the goal of a
balls-into-bins process is to allocate a set of independent objects (tasks,
jobs, balls) to a set of resources (servers, bins, urns) and, thereby, to
minimize the maximum load. In this paper, we analyze the maximum load for the
{\em chains-into-bins} problem, which is defined as follows. There are $n$
bins, and $m$ objects to be allocated. Each object consists of balls connected
into a chain of length $\ell$, so that there are $m \ell$ balls in total. We
assume the chains cannot be broken, and that the balls in one chain have to be
allocated to $\ell$ consecutive bins. We allow each chain $d$ independent and
uniformly random bin choices for its starting position. The chain is allocated
using the rule that the maximum load of any bin receiving a ball of that chain
is minimized. We show that, for $d \ge 2$ and $m\cdot\ell=O(n)$, the maximum
load is $((\ln \ln m)/\ln d) +O(1)$ with probability $1-\tilde O(1/m^{d-1})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2632</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2632</id><created>2010-05-14</created><authors><author><keyname>Cai</keyname><forenames>Jin-Yi</forenames></author><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Lipton</keyname><forenames>Richard</forenames></author><author><keyname>Lu</keyname><forenames>Pinyan</forenames></author></authors><title>On Tractable Exponential Sums</title><categories>cs.CC</categories><doi>10.1007/978-3-642-14553-7_16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of evaluating certain exponential sums. These sums
take the form $\sum_{x_1,...,x_n \in Z_N} e^{f(x_1,...,x_n) {2 \pi i / N}} $,
where each x_i is summed over a ring Z_N, and f(x_1,...,x_n) is a multivariate
polynomial with integer coefficients. We show that the sum can be evaluated in
polynomial time in n and log N when f is a quadratic polynomial. This is true
even when the factorization of N is unknown. Previously, this was known for a
prime modulus N. On the other hand, for very specific families of polynomials
of degree \ge 3, we show the problem is #P-hard, even for any fixed prime or
prime power modulus. This leads to a complexity dichotomy theorem - a complete
classification of each problem to be either computable in polynomial time or
#P-hard - for a class of exponential sums. These sums arise in the
classifications of graph homomorphisms and some other counting CSP type
problems, and these results lead to complexity dichotomy theorems. For the
polynomial-time algorithm, Gauss sums form the basic building blocks. For the
hardness results, we prove group-theoretic necessary conditions for
tractability. These tests imply that the problem is #P-hard for even very
restricted families of simple cubic polynomials over fixed modulus N.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2633</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2633</id><created>2010-05-14</created><updated>2011-04-22</updated><authors><author><keyname>Wei</keyname><forenames>Ermin</forenames></author><author><keyname>Ozdaglar</keyname><forenames>Asuman</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>A Distributed Newton Method for Network Utility Maximization</title><categories>math.OC cs.SY</categories><comments>27 pages, 4 figures, LIDS report, submitted to CDC 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing work uses dual decomposition and subgradient methods to solve
Network Utility Maximization (NUM) problems in a distributed manner, which
suffer from slow rate of convergence properties. This work develops an
alternative distributed Newton-type fast converging algorithm for solving
network utility maximization problems with self-concordant utility functions.
By using novel matrix splitting techniques, both primal and dual updates for
the Newton step can be computed using iterative schemes in a decentralized
manner with limited information exchange. Similarly, the stepsize can be
obtained via an iterative consensus-based averaging scheme. We show that even
when the Newton direction and the stepsize in our method are computed within
some error (due to finite truncation of the iterative schemes), the resulting
objective function value still converges superlinearly to an explicitly
characterized error neighborhood. Simulation results demonstrate significant
convergence rate improvement of our algorithm relative to the existing
subgradient methods based on dual decomposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2636</identifier>
 <datestamp>2010-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2636</id><created>2010-05-14</created><authors><author><keyname>da Cunha</keyname><forenames>Aubrey</forenames></author></authors><title>Turing Machines on Graphs and Inescapable Groups</title><categories>math.LO cs.FL</categories><comments>18 pages, one table</comments><msc-class>68Q05, 03D25, 20E</msc-class><acm-class>F.1.1; F.4.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a generalization of standard Turing machines based on allowing
unusual tapes. We present a set of reasonable constraints on tape geometry and
classify all tapes conforming to these constraints. Surprisingly, this
generalization does not lead to yet another equivalent formulation of the
notion of computable function. Rather, it gives an alternative definition of
the recursively enumerable Turing degrees that does not rely on oracles. The
definitions give rise to a number of questions about computable paths inside
Cayley graphs of finitely generated groups, and several of these questions are
answered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2638</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2638</id><created>2010-05-14</created><authors><author><keyname>Murtagh</keyname><forenames>Fionn</forenames></author><author><keyname>Contreras</keyname><forenames>Pedro</forenames></author></authors><title>Hierarchical Clustering for Finding Symmetries and Other Patterns in
  Massive, High Dimensional Datasets</title><categories>stat.ML cs.CV cs.LG</categories><comments>41 pages, 13 figures, 6 tables. 81 references</comments><msc-class>62H30, 68P01</msc-class><acm-class>G.3; H.2.8; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data analysis and data mining are concerned with unsupervised pattern finding
and structure determination in data sets. &quot;Structure&quot; can be understood as
symmetry and a range of symmetries are expressed by hierarchy. Such symmetries
directly point to invariants, that pinpoint intrinsic properties of the data
and of the background empirical domain of interest. We review many aspects of
hierarchy here, including ultrametric topology, generalized ultrametric,
linkages with lattices and other discrete algebraic structures and with p-adic
number representations. By focusing on symmetries in data we have a powerful
means of structuring and analyzing massive, high dimensional data stores. We
illustrate the powerfulness of hierarchical clustering in case studies in
chemistry and finance, and we provide pointers to other published case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2642</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2642</id><created>2010-05-14</created><authors><author><keyname>Cook</keyname><forenames>Stephen</forenames></author><author><keyname>McKenzie</keyname><forenames>Pierre</forenames></author><author><keyname>Wehr</keyname><forenames>Dustin</forenames></author><author><keyname>Braverman</keyname><forenames>Mark</forenames></author><author><keyname>Santhanam</keyname><forenames>Rahul</forenames></author></authors><title>Pebbles and Branching Programs for Tree Evaluation</title><categories>cs.CC</categories><comments>Journal version of mostly-previously-published work. 47 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the Tree Evaluation Problem, show that it is in logDCFL (and
hence in P), and study its branching program complexity in the hope of
eventually proving a superlogarithmic space lower bound. The input to the
problem is a rooted, balanced d-ary tree of height h, whose internal nodes are
labeled with d-ary functions on [k] = {1,...,k}, and whose leaves are labeled
with elements of [k]. Each node obtains a value in [k] equal to its d-ary
function applied to the values of its d children. The output is the value of
the root. We show that the standard black pebbling algorithm applied to the
binary tree of height h yields a deterministic k-way branching program with
Theta(k^h) states solving this problem, and we prove that this upper bound is
tight for h=2 and h=3. We introduce a simple semantic restriction called
&quot;thrifty&quot; on k-way branching programs solving tree evaluation problems and show
that the same state bound of Theta(k^h) is tight (up to a constant factor) for
all h &gt;= 2 for deterministic thrifty programs. We introduce fractional pebbling
for trees and show that this yields nondeterministic thrifty programs with
Theta(k^{h/2+1}) states solving the Boolean problem &quot;determine whether the root
has value 1&quot;. We prove that this bound is tight for h=2,3,4, and tight for
unrestricted nondeterministic k-way branching programs for h=2,3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2643</identifier>
 <datestamp>2010-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2643</id><created>2010-05-14</created><authors><author><keyname>Deelman</keyname><forenames>Ewa</forenames></author><author><keyname>Berriman</keyname><forenames>Bruce</forenames></author><author><keyname>Chervenak</keyname><forenames>Ann</forenames></author><author><keyname>Corcho</keyname><forenames>Oscar</forenames></author><author><keyname>Groth</keyname><forenames>Paul</forenames></author><author><keyname>Moreau</keyname><forenames>Luc</forenames></author></authors><title>Metadata and provenance management</title><categories>astro-ph.IM cs.DL</categories><journal-ref>Scientific Data Management: Challenges, Existing Technology, and
  Deployment (Arie Shoshani and Doron Rotem, Editors) CRC Press 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientists today collect, analyze, and generate TeraBytes and PetaBytes of
data. These data are often shared and further processed and analyzed among
collaborators. In order to facilitate sharing and data interpretations, data
need to carry with it metadata about how the data was collected or generated,
and provenance information about how the data was processed. This chapter
describes metadata and provenance in the context of the data lifecycle. It also
gives an overview of the approaches to metadata and provenance management,
followed by examples of how applications use metadata and provenance in their
scientific processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2646</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2646</id><created>2010-05-14</created><authors><author><keyname>Feng</keyname><forenames>Chen</forenames></author><author><keyname>Silva</keyname><forenames>Danilo</forenames></author><author><keyname>Kschischang</keyname><forenames>Frank R.</forenames></author></authors><title>An Algebraic Approach to Physical-Layer Network Coding</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, accepted to IEEE Int. Symp. Information Theory,
  2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of designing new physical-layer network coding (PNC) schemes via
lattice partitions is considered. Building on a recent work by Nazer and
Gastpar, who demonstrated its asymptotic gain using information-theoretic
tools, we take an algebraic approach to show its potential in non-asymptotic
settings. We first relate Nazer-Gastpar's approach to the fundamental theorem
of finitely generated modules over a principle ideal domain. Based on this
connection, we generalize their code construction and simplify their encoding
and decoding methods. This not only provides a transparent understanding of
their approach, but more importantly, it opens up the opportunity to design
efficient and practical PNC schemes. Finally, we apply our framework for PNC to
a Gaussian relay network and demonstrate its advantage over conventional PNC
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2654</identifier>
 <datestamp>2013-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2654</id><created>2010-05-15</created><updated>2010-06-08</updated><authors><author><keyname>Salehi</keyname><forenames>Saeed</forenames></author></authors><title>Herbrand Consistency of Some Arithmetical Theories</title><categories>math.LO cs.LO</categories><comments>MANUSCRIPT (Submitted) - 20 pages -
  http://saeedsalehi.ir/pdf/hcon2.pdf</comments><msc-class>Primary 03F40, 03F30, Secondary 03F05, 03H15</msc-class><journal-ref>Journal of Symbolic Logic 77:3 (2012) 807-827</journal-ref><doi>10.2178/jsl/1344862163</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  G\&quot;odel's second incompleteness theorem is proved for Herbrand consistency of
some arithmetical theories with bounded induction, by using a technique of
logarithmic shrinking the witnesses of bounded formulas, due to Z. Adamowicz
[Herbrand consistency and bounded arithmetic, \textit{Fundamenta Mathematicae}
171 (2002) 279--292]. In that paper, it was shown that one cannot always shrink
the witness of a bounded formula logarithmically, but in the presence of
Herbrand consistency, for theories ${\rm I\Delta_0+\Omega_m}$ with $m\geqslant
2$, any witness for any bounded formula can be shortened logarithmically. This
immediately implies the unprovability of Herbrand consistency of a theory
$T\supseteq {\rm I\Delta_0+\Omega_2}$ in $T$ itself.
  In this paper, the above results are generalized for ${\rm
I\Delta_0+\Omega_1}$. Also after tailoring the definition of Herbrand
consistency for ${\rm I\Delta_0}$ we prove the corresponding theorems for ${\rm
I\Delta_0}$. Thus the Herbrand version of G\&quot;odel's second incompleteness
theorem follows for the theories ${\rm I\Delta_0+\Omega_1}$ and ${\rm
I\Delta_0}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2662</identifier>
 <datestamp>2010-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2662</id><created>2010-05-15</created><authors><author><keyname>Jafarizadeh</keyname><forenames>Saber</forenames></author></authors><title>Fastest Distributed Consensus Averaging Problem on Perfect and Complete
  n-ary Tree networks</title><categories>cs.IT cs.DC cs.NI math.IT</categories><comments>19 pages, 3 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving fastest distributed consensus averaging problem (i.e., finding
weights on the edges to minimize the second-largest eigenvalue modulus of the
weight matrix) over networks with different topologies is one of the primary
areas of research in the field of sensor networks and one of the well known
networks in this issue is tree network. Here in this work we present analytical
solution for the problem of fastest distributed consensus averaging algorithm
by means of stratification and semidefinite programming, for two particular
types of tree networks, namely perfect and complete n-ary tree networks. Our
method in this paper is based on convexity of fastest distributed consensus
averaging problem, and inductive comparing of the characteristic polynomials
initiated by slackness conditions in order to find the optimal weights. Also
the optimal weights for the edges of certain types of branches such as perfect
and complete n-ary tree branches are determined independently of rest of the
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2672</identifier>
 <datestamp>2010-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2672</id><created>2010-05-15</created><authors><author><keyname>Tankink</keyname><forenames>Carst</forenames></author><author><keyname>Geuvers</keyname><forenames>Herman</forenames></author><author><keyname>McKinna</keyname><forenames>James</forenames></author><author><keyname>Wiedijk</keyname><forenames>Freek</forenames></author></authors><title>Proviola: A Tool for Proof Re-animation</title><categories>cs.LO cs.DL cs.HC cs.MM</categories><comments>Accepted for the 9th International Conference on Mathematical
  Knowledge Management (MKM 2010), 15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To improve on existing models of interaction with a proof assistant (PA), in
particular for storage and replay of proofs, we in- troduce three related
concepts, those of: a proof movie, consisting of frames which record both user
input and the corresponding PA response; a camera, which films a user's
interactive session with a PA as a movie; and a proviola, which replays a movie
frame-by-frame to a third party. In this paper we describe the movie data
structure and we discuss a proto- type implementation of the camera and
proviola based on the ProofWeb system. ProofWeb uncouples the interaction with
a PA via a web- interface (the client) from the actual PA that resides on the
server. Our camera films a movie by &quot;listening&quot; to the ProofWeb communication.
The first reason for developing movies is to uncouple the reviewing of a formal
proof from the PA used to develop it: the movie concept enables users to
discuss small code fragments without the need to install the PA or to load a
whole library into it. Other advantages include the possibility to develop a
separate com- mentary track to discuss or explain the PA interaction. We assert
that a combined camera+proviola provides a generic layer between a client
(user) and a server (PA). Finally we claim that movies are the right type of
data to be stored in an encyclopedia of formalized mathematics, based on our
experience in filming the Coq standard library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2678</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2678</id><created>2010-05-15</created><updated>2011-10-24</updated><authors><author><keyname>Bulatov</keyname><forenames>Andrei</forenames></author><author><keyname>Dyer</keyname><forenames>Martin</forenames></author><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>Jalsenius</keyname><forenames>Markus</forenames></author><author><keyname>Jerrum</keyname><forenames>Mark</forenames></author><author><keyname>Richerby</keyname><forenames>David</forenames></author></authors><title>The complexity of weighted and unweighted #CSP</title><categories>cs.CC</categories><comments>11 pages</comments><acm-class>F.2.2; F.4.1; G.2.1</acm-class><journal-ref>JCSS 2012</journal-ref><doi>10.1016/j.jcss.2011.12.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give some reductions among problems in (nonnegative) weighted #CSP which
restrict the class of functions that needs to be considered in computational
complexity studies. Our reductions can be applied to both exact and approximate
computation. In particular, we show that a recent dichotomy for unweighted #CSP
can be extended to rational-weighted #CSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2704</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2704</id><created>2010-05-15</created><updated>2010-10-10</updated><authors><author><keyname>Ratkiewicz</keyname><forenames>Jacob</forenames></author><author><keyname>Menczer</keyname><forenames>Filippo</forenames></author><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author><author><keyname>Flammini</keyname><forenames>Alessandro</forenames></author><author><keyname>Vespignani</keyname><forenames>Alessandro</forenames></author></authors><title>Characterizing and modeling the dynamics of online popularity</title><categories>physics.soc-ph cs.CY cs.SI</categories><comments>5 pages, 4 figures. Modeling part detailed. Final version published
  in Physical Review Letters</comments><journal-ref>Physical Review Letters 105, 158701 (2010)</journal-ref><doi>10.1103/PhysRevLett.105.158701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online popularity has enormous impact on opinions, culture, policy, and
profits. We provide a quantitative, large scale, temporal analysis of the
dynamics of online content popularity in two massive model systems, the
Wikipedia and an entire country's Web space. We find that the dynamics of
popularity are characterized by bursts, displaying characteristic features of
critical systems such as fat-tailed distributions of magnitude and inter-event
time. We propose a minimal model combining the classic preferential popularity
increase mechanism with the occurrence of random popularity shifts due to
exogenous factors. The model recovers the critical features observed in the
empirical analysis of the systems analyzed here, highlighting the key factors
needed in the description of popularity dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2710</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2710</id><created>2010-05-15</created><updated>2011-10-18</updated><authors><author><keyname>Lee</keyname><forenames>Si-Hyeon</forenames></author><author><keyname>Chung</keyname><forenames>Sae-Young</forenames></author></authors><title>Capacity of a Class of Multicast Tree Networks</title><categories>cs.IT math.IT</categories><comments>27 pages, 2 figures, 1 table. Revised version in response to referee
  comments (in revision for IEEE Transactions on Information Theory)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we characterize the capacity of a new class of single-source
multicast discrete memoryless relay networks having a tree topology in which
the root node is the source and each parent node in the graph has at most one
noisy child node and any number of noiseless child nodes. This class of
multicast tree networks includes the class of diamond networks studied by Kang
and Ulukus as a special case, where they showed that the capacity can be
strictly lower than the cut-set bound. For achievablity, a novel coding scheme
is constructed where each noisy relay employs a combination of
decode-and-forward (DF) and compress-and-forward (CF) and each noiseless relay
performs a random binning such that codebook constructions and relay operations
are independent for each node and do not depend on the network topology. For
converse, a new technique of iteratively manipulating inequalities exploiting
the tree topology is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2714</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2714</id><created>2010-05-15</created><updated>2012-02-27</updated><authors><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author><author><keyname>Whalen</keyname><forenames>Sean</forenames></author></authors><title>Structural Drift: The Population Dynamics of Sequential Learning</title><categories>q-bio.PE cs.LG</categories><comments>15 pages, 9 figures;
  http://csc.ucdavis.edu/~cmg/compmech/pubs/sdrift.htm</comments><report-no>Santa Fe Institute Working Paper 10-05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a theory of sequential causal inference in which learners in a
chain estimate a structural model from their upstream teacher and then pass
samples from the model to their downstream student. It extends the population
dynamics of genetic drift, recasting Kimura's selectively neutral theory as a
special case of a generalized drift process using structured populations with
memory. We examine the diffusion and fixation properties of several drift
processes and propose applications to learning, inference, and evolution. We
also demonstrate how the organization of drift process space controls fidelity,
facilitates innovations, and leads to information loss in sequential learning
with and without memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2715</identifier>
 <datestamp>2010-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2715</id><created>2010-05-15</created><authors><author><keyname>Tzimiropoulos</keyname><forenames>Georgios</forenames></author><author><keyname>Zafeiriou</keyname><forenames>Stefanos</forenames></author></authors><title>On the Subspace of Image Gradient Orientations</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of Principal Component Analysis (PCA) of image
gradient orientations. As image data is typically noisy, but noise is
substantially different from Gaussian, traditional PCA of pixel intensities
very often fails to estimate reliably the low-dimensional subspace of a given
data population. We show that replacing intensities with gradient orientations
and the $\ell_2$ norm with a cosine-based distance measure offers, to some
extend, a remedy to this problem. Our scheme requires the eigen-decomposition
of a covariance matrix and is as computationally efficient as standard $\ell_2$
PCA. We demonstrate some of its favorable properties on robust subspace
estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2718</identifier>
 <datestamp>2010-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2718</id><created>2010-05-15</created><authors><author><keyname>Juve</keyname><forenames>Gideon</forenames></author><author><keyname>Deelman</keyname><forenames>Ewa</forenames></author><author><keyname>Vahi</keyname><forenames>Karan</forenames></author><author><keyname>Mehta</keyname><forenames>Gaurang</forenames></author><author><keyname>Berriman</keyname><forenames>Bruce</forenames></author><author><keyname>Berman</keyname><forenames>Benjamin P.</forenames></author><author><keyname>Maechling</keyname><forenames>Phil</forenames></author></authors><title>Scientific Workflow Applications on Amazon EC2</title><categories>astro-ph.IM cs.DC</categories><journal-ref>5th IEEE International Conference on e-Science. 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of commercial cloud computing providers has generated
significant interest in the scientific computing community. Much recent
research has attempted to determine the benefits and drawbacks of cloud
computing for scientific applications. Although clouds have many attractive
features, such as virtualization, on-demand provisioning, and &quot;pay as you go&quot;
usage-based pricing, it is not clear whether they are able to deliver the
performance required for scientific applications at a reasonable price. In this
paper we examine the performance and cost of clouds from the perspective of
scientific workflow applications. We use three characteristic workflows to
compare the performance of a commercial cloud with that of a typical HPC
system, and we analyze the various costs associated with running those
workflows in the cloud. We find that the performance of clouds is not
unreasonable given the hardware resources provided, and that performance
comparable to HPC systems can be achieved given similar resources. We also find
that the cost of running workflows on a commercial cloud can be reduced by
storing data in the cloud rather than transferring it from outside.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2724</identifier>
 <datestamp>2010-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2724</id><created>2010-05-16</created><updated>2010-10-27</updated><authors><author><keyname>Magen</keyname><forenames>Avner</forenames></author><author><keyname>Zouzias</keyname><forenames>Anastasios</forenames></author></authors><title>Low Rank Matrix-Valued Chernoff Bounds and Approximate Matrix
  Multiplication</title><categories>cs.DS cs.DM math.PR</categories><comments>15 pages, To appear in 22nd ACM-SIAM Symposium on Discrete Algorithms
  (SODA 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop algorithms for approximating matrix multiplication
with respect to the spectral norm. Let A\in{\RR^{n\times m}} and B\in\RR^{n
\times p} be two matrices and \eps&gt;0. We approximate the product A^\top B using
two down-sampled sketches, \tilde{A}\in\RR^{t\times m} and
\tilde{B}\in\RR^{t\times p}, where t\ll n such that \norm{\tilde{A}^\top
\tilde{B} - A^\top B} \leq \eps \norm{A}\norm{B} with high probability. We use
two different sampling procedures for constructing \tilde{A} and \tilde{B}; one
of them is done by i.i.d. non-uniform sampling rows from A and B and the other
is done by taking random linear combinations of their rows. We prove bounds
that depend only on the intrinsic dimensionality of A and B, that is their rank
and their stable rank; namely the squared ratio between their Frobenius and
operator norm. For achieving bounds that depend on rank we employ standard
tools from high-dimensional geometry such as concentration of measure arguments
combined with elaborate \eps-net constructions. For bounds that depend on the
smaller parameter of stable rank this technology itself seems weak. However, we
show that in combination with a simple truncation argument is amenable to
provide such bounds. To handle similar bounds for row sampling, we develop a
novel matrix-valued Chernoff bound inequality which we call low rank
matrix-valued Chernoff bound. Thanks to this inequality, we are able to give
bounds that depend only on the stable rank of the input matrices...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2731</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2731</id><created>2010-05-16</created><authors><author><keyname>Hou</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Lin</forenames></author><author><keyname>Yang</keyname><forenames>Lei</forenames></author><author><keyname>Zheng</keyname><forenames>Heather</forenames></author><author><keyname>Shan</keyname><forenames>Xiuming</forenames></author></authors><title>Cross-Band Interference Considered Harmful in OFDM Based Distributed
  Spectrum Sharing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past few years we have witnessed the paradigm shift from static
spectrum allocation to dynamic spectrum access/sharing. Orthogonal
Frequency-Division Multiple Access (OFDMA) is a promising mechanism to
implement the agile spectrum access. However, in wireless distributed networks
where tight synchronization is infeasible, OFDMA faces the problem of
cross-band interference. Subcarriers used by different users are no longer
orthogonal, and transmissions operating on non-overlapping subcarriers can
interfere with each other. In this paper, we explore the cause of cross-band
interference and analytically quantify its strength and impact on packet
transmissions. Our analysis captures three key practical artifacts: inter-link
frequency offset, temporal sampling mismatch and power heterogeneity. To our
best knowledge, this work is the first to systematically analyze the cause and
impact of cross-band interference. Using insights from our analysis, we then
build and compared three mitigating methods to combat cross-band interference.
Analytical and simulation results show that placing frequency guardband at link
boundaries is the most effective solution in distributed spectrum sharing,
while the other two frequency-domain methods are sensitive to either temporal
sampling mismatch or inter-link frequency offset. We find that the proper
guardband size depends heavily on power heterogeneity. Consequently, protocol
designs for dynamic spectrum access should carefully take into account the
cross-band interference when configuring spectrum usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2759</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2759</id><created>2010-05-16</created><updated>2010-08-15</updated><authors><author><keyname>Hof</keyname><forenames>Eran</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author></authors><title>Secrecy-Achieving Polar-Coding for Binary-Input Memoryless Symmetric
  Wire-Tap Channels</title><categories>cs.IT cs.CR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A polar coding scheme is introduced in this paper for the wire-tap channel.
It is shown that the provided scheme achieves the entire rate-equivocation
region for the case of symmetric and degraded wire-tap channel, where the weak
notion of secrecy is assumed. For the particular case of binary erasure
wire-tap channel, an alternative proof is given. The case of general
non-degraded wire-tap channels is also considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2770</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2770</id><created>2010-05-16</created><updated>2012-08-19</updated><authors><author><keyname>Hof</keyname><forenames>Eran</forenames></author><author><keyname>Sason</keyname><forenames>Igal</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author><author><keyname>Tian</keyname><forenames>Chao</forenames></author></authors><title>Capacity-Achieving Polar Codes for Arbitrarily-Permuted Parallel
  Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Channel coding over arbitrarily-permuted parallel channels was first studied
by Willems et al. (2008). This paper introduces capacity-achieving polar coding
schemes for arbitrarily-permuted parallel channels where the component channels
are memoryless, binary-input and output-symmetric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2791</identifier>
 <datestamp>2010-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2791</id><created>2010-05-16</created><authors><author><keyname>Vondrak</keyname><forenames>Jan</forenames></author></authors><title>A note on concentration of submodular functions</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey a few concentration inequalities for submodular and fractionally
subadditive functions of independent random variables, implied by the entropy
method for self-bounding functions. The power of these concentration bounds is
that they are dimension-free, in particular implying standard deviation
O(\sqrt{\E[f]}) rather than O(\sqrt{n}) which can be obtained for any
1-Lipschitz function of n variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2815</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2815</id><created>2010-05-17</created><authors><author><keyname>Nicolau</keyname><forenames>Miguel</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author><author><keyname>Schoenauer</keyname><forenames>Marc</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author><author><keyname>Banzhaf</keyname><forenames>W.</forenames></author></authors><title>Evolving Genes to Balance a Pole</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>EUropean Conference on Genetic Programming, Istanbul : Turkey
  (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss how to use a Genetic Regulatory Network as an evolutionary
representation to solve a typical GP reinforcement problem, the pole balancing.
The network is a modified version of an Artificial Regulatory Network proposed
a few years ago, and the task could be solved only by finding a proper way of
connecting inputs and outputs to the network. We show that the representation
is able to generalize well over the problem domain, and discuss the performance
of different models of this kind.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2816</identifier>
 <datestamp>2010-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2816</id><created>2010-05-17</created><authors><author><keyname>Sopena</keyname><forenames>Eric</forenames><affiliation>LaBRI</affiliation></author></authors><title>Upper oriented chromatic number of undirected graphs and oriented
  colorings of product graphs</title><categories>cs.DM</categories><comments>14 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The oriented chromatic number of an oriented graph $\vec G$ is the minimum
order of an oriented graph $\vev H$ such that $\vec G$ admits a homomorphism to
$\vev H$. The oriented chromatic number of an undirected graph $G$ is then the
greatest oriented chromatic number of its orientations. In this paper, we
introduce the new notion of the upper oriented chromatic number of an
undirected graph $G$, defined as the minimum order of an oriented graph $\vev
U$ such that every orientation $\vec G$ of $G$ admits a homomorphism to $\vec
U$. We give some properties of this parameter, derive some general upper bounds
on the ordinary and upper oriented chromatic numbers of Cartesian, strong,
direct and lexicographic products of graphs, and consider the particular case
of products of paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2819</identifier>
 <datestamp>2010-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2819</id><created>2010-05-17</created><authors><author><keyname>Didier</keyname><forenames>Frederic</forenames></author><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames></author><author><keyname>Mateescu</keyname><forenames>Maria</forenames></author><author><keyname>Wolf</keyname><forenames>Verena</forenames></author></authors><title>SABRE: A Tool for Stochastic Analysis of Biochemical Reaction Networks</title><categories>cs.CE cs.MS q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The importance of stochasticity within biological systems has been shown
repeatedly during the last years and has raised the need for efficient
stochastic tools. We present SABRE, a tool for stochastic analysis of
biochemical reaction networks. SABRE implements fast adaptive uniformization
(FAU), a direct numerical approximation algorithm for computing transient
solutions of biochemical reaction networks. Biochemical reactions networks
represent biological systems studied at a molecular level and these reactions
can be modeled as transitions of a Markov chain. SABRE accepts as input the
formalism of guarded commands, which it interprets either as continuous-time or
as discrete-time Markov chains. Besides operating in a stochastic mode, SABRE
may also perform a deterministic analysis by directly computing a mean-field
approximation of the system under study. We illustrate the different
functionalities of SABRE by means of biological case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2821</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2821</id><created>2010-05-17</created><authors><author><keyname>Acharya</keyname><forenames>Tamaghna</forenames><affiliation>BESU, India</affiliation></author><author><keyname>Chattopadhyay</keyname><forenames>Samiran</forenames><affiliation>Jadavpur University, India</affiliation></author><author><keyname>Roy</keyname><forenames>Rajarshi</forenames><affiliation>Indian Institute of Technology - Kharagpur, India</affiliation></author></authors><title>Constructing Path Efficient and Energy Aware Virtual Multicast Backbones
  in Static Ad Hoc Wireless Networks</title><categories>cs.NI</categories><comments>17 Pages, IJWMN 2010</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks 2.2 (2010)
  138-154</journal-ref><doi>10.5121/ijwmn.2010.2210</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  For stationary wireless ad hoc networks, one of the key challenging issues in
routing and multicasting is to conserve as much energy as possible without
compromising path efficiency measured as end-to-end delay. In this paper, we
address the problem of path efficient and energy aware multicasting in static
wireless ad hoc networks. We propose a novel distributed scalable algorithm for
finding a virtual multicast backbone (VMB). Based on this VMB, we have further
developed a multicasting scheme that jointly improves path efficiency and
energy conservation. By exploiting inherent broadcast advantage of wireless
communication and employing a more realistic energy consumption model for
wireless communication which not only depends on radio propagation losses but
also on energy losses in transceiver circuitry, our simulation results show
that the proposed VMB-based multicasting scheme outperforms existing prominent
tree based energy conserving, path efficient multicasting schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2822</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2822</id><created>2010-05-17</created><updated>2010-08-07</updated><authors><author><keyname>Shardt</keyname><forenames>Orest</forenames></author><author><keyname>Bowman</keyname><forenames>John C.</forenames></author></authors><title>Surface Parametrization of Nonsimply Connected Planar B\'ezier Regions</title><categories>cs.CG</categories><comments>29 pages, 12 figures; interactive 3D figures require Adobe Reader 9.0</comments><msc-class>65D17, 68U05, 68U15</msc-class><acm-class>I.3.5; J.2; J.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A technique is described for constructing three-dimensional vector graphics
representations of planar regions bounded by cubic B\'ezier curves, such as
smooth glyphs. It relies on a novel algorithm for compactly partitioning planar
B\'ezier regions into nondegenerate Coons patches. New optimizations are also
described for B\'ezier inside-outside tests and the computation of global
bounds of directionally monotonic functions over a B\'ezier surface (such as
its bounding box or optimal field-of-view angle). These algorithms underlie the
three-dimensional illustration and typography features of the TeX-aware vector
graphics language Asymptote.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2839</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2839</id><created>2010-05-17</created><authors><author><keyname>Elsenhans</keyname><forenames>Andreas-Stephan</forenames></author><author><keyname>Kohnert</keyname><forenames>Axel</forenames></author><author><keyname>Wassermann</keyname><forenames>Alfred</forenames></author></authors><title>Construction of Codes for Network Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on ideas of K\&quot;otter and Kschischang we use constant dimension
subspaces as codewords in a network. We show a connection to the theory of
q-analogues of a combinatorial designs, which has been studied in Braun, Kerber
and Laue as a purely combinatorial object. For the construction of network
codes we successfully modified methods (construction with prescribed
automorphisms) originally developed for the q-analogues of a combinatorial
designs. We then give a special case of that method which allows the
construction of network codes with a very large ambient space and we also show
how to decode such codes with a very small number of operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2848</identifier>
 <datestamp>2010-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2848</id><created>2010-05-17</created><authors><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Yeo</keyname><forenames>Anders</forenames></author></authors><title>Note on Maximal Bisection above Tight Lower Bound</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a graph $G=(V,E)$, a bisection $(X,Y)$ is a partition of $V$ into sets $X$
and $Y$ such that $|X|\le |Y|\le |X|+1$. The size of $(X,Y)$ is the number of
edges between $X$ and $Y$. In the Max Bisection problem we are given a graph
$G=(V,E)$ and are required to find a bisection of maximum size. It is not hard
to see that $\lceil |E|/2 \rceil$ is a tight lower bound on the maximum size of
a bisection of $G$. We study parameterized complexity of the following
parameterized problem called Max Bisection above Tight Lower Bound
(Max-Bisec-ATLB): decide whether a graph $G=(V,E)$ has a bisection of size at
least $\lceil |E|/2 \rceil+k,$ where $k$ is the parameter. We show that this
parameterized problem has a kernel with $O(k^2)$ vertices and $O(k^3)$ edges,
i.e., every instance of Max-Bisec-ATLB is equivalent to an instance of
Max-Bisec-ATLB on a graph with at most $O(k^2)$ vertices and $O(k^3)$ edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2880</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2880</id><created>2010-05-17</created><authors><author><keyname>Routtenberg</keyname><forenames>Tirza</forenames></author><author><keyname>Tabrikian</keyname><forenames>Joseph</forenames></author></authors><title>General Classes of Lower Bounds on the Probability of Error in Multiple
  Hypothesis Testing</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, two new classes of lower bounds on the probability of error
for $m$-ary hypothesis testing are proposed. Computation of the minimum
probability of error which is attained by the maximum a-posteriori probability
(MAP) criterion is usually not tractable. The new classes are derived using
Holder's inequality and reverse Holder's inequality. The bounds in these
classes provide good prediction of the minimum probability of error in multiple
hypothesis testing. The new classes generalize and extend existing bounds and
their relation to some existing upper bounds is presented. It is shown that the
tightest bounds in these classes asymptotically coincide with the optimum
probability of error provided by the MAP criterion for binary or multiple
hypothesis testing problem. These bounds are compared with other existing lower
bounds in several typical detection and classification problems in terms of
tightness and computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2882</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2882</id><created>2010-05-17</created><authors><author><keyname>Lazaar</keyname><forenames>Nadjib</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Gotlieb</keyname><forenames>Arnaud</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Yahia</keyname><forenames>Lebbah</forenames></author></authors><title>On Testing Constraint Programs</title><categories>cs.SE</categories><proxy>ccsd</proxy><report-no>RR-7291</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of several constraint-based modeling languages such as OPL, ZINC,
or COMET, appeals for better software engineering practices, particularly in
the testing phase. This paper introduces a testing framework enabling automated
test case generation for constraint programming. We propose a general framework
of constraint program development which supposes that a first declarative and
simple constraint model is available from the problem specifications analysis.
Then, this model is refined using classical techniques such as constraint
reformulation, surrogate and global constraint addition, or symmetry-breaking
to form an improved constraint model that must be thoroughly tested before
being used to address real-sized problems. We think that most of the faults are
introduced in this refinement step and propose a process which takes the first
declarative model as an oracle for detecting non-conformities. We derive
practical test purposes from this process to generate automatically test data
that exhibit non-conformities. We implemented this approach in a new tool
called CPTEST that was used to automatically detect non-conformities on two
classical benchmark programs, namely the Golomb rulers and the car-sequencing
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2894</identifier>
 <datestamp>2010-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2894</id><created>2010-05-17</created><updated>2010-08-17</updated><authors><author><keyname>Kuhn</keyname><forenames>Fabian</forenames></author><author><keyname>Lenzen</keyname><forenames>Christoph</forenames></author><author><keyname>Locher</keyname><forenames>Thomas</forenames></author><author><keyname>Oshman</keyname><forenames>Rotem</forenames></author></authors><title>Optimal Gradient Clock Synchronization in Dynamic Networks</title><categories>cs.DC cs.DS</categories><comments>37 pages; conference version: 29th Annual ACM Symposium on Principles
  of Distributed Computing (PODC 2010)</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of clock synchronization in highly dynamic networks,
where communication links can appear or disappear at any time. The nodes in the
network are equipped with hardware clocks, but the rate of the hardware clocks
can vary arbitrarily within specific bounds, and the estimates that nodes can
obtain about the clock values of other nodes are inherently inaccurate. Our
goal in this setting is to output a logical clock at each node, such that the
logical clocks of any two nodes are not too far apart, and nodes that remain
close to each other in the network for a long time are better synchronized than
distant nodes. This property is called gradient clock synchronization.
  Gradient clock synchronization has been widely studied in the static setting,
where the network topology does not change. We show that the asymptotically
optimal bounds obtained for the static case also apply to our highly dynamic
setting: if two nodes remain at distance d from each other for sufficiently
long, it is possible to upper bound the difference between their clock values
by O(d*log(D/d)), where D is the diameter of the network. This is known to be
optimal for static networks, and since a static network is a special case of a
dynamic network, it is optimal for dynamic networks as well. Furthermore, we
show that our algorithm has optimal stabilization time: when a path of length d
appears between two nodes, the time required until the clock skew between the
two nodes is reduced to O(d*log(D/d)) is O(D), which we prove is optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2897</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2897</id><created>2010-05-17</created><updated>2014-12-09</updated><authors><author><keyname>Assaf</keyname><forenames>Ali</forenames><affiliation>&#xc9;cole Polytechnique &amp; INRIA</affiliation></author><author><keyname>D&#xed;az-Caro</keyname><forenames>Alejandro</forenames><affiliation>Universidad Nacional de Quilmes, Buenos Aires, Argentina</affiliation></author><author><keyname>Perdrix</keyname><forenames>Simon</forenames><affiliation>CNRS &amp; LORIA</affiliation></author><author><keyname>Tasson</keyname><forenames>Christine</forenames><affiliation>PPS, Universit&#xe9; Paris-Diderot</affiliation></author><author><keyname>Valiron</keyname><forenames>Beno&#xee; t</forenames><affiliation>PPS, Universit&#xe9; Paris-Diderot</affiliation></author></authors><title>Call-by-value, call-by-name and the vectorial behaviour of the algebraic
  \lambda-calculus</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 4 (December
  9, 2014) lmcs:927</journal-ref><doi>10.2168/LMCS-10(4:8)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the relationship between the algebraic lambda-calculus, a fragment
of the differential lambda-calculus and the linear-algebraic lambda-calculus, a
candidate lambda-calculus for quantum computation. Both calculi are algebraic:
each one is equipped with an additive and a scalar-multiplicative structure,
and their set of terms is closed under linear combinations. However, the two
languages were built using different approaches: the former is a call-by-name
language whereas the latter is call-by-value; the former considers algebraic
equalities whereas the latter approaches them through rewrite rules. In this
paper, we analyse how these different approaches relate to one another. To this
end, we propose four canonical languages based on each of the possible choices:
call-by-name versus call-by-value, algebraic equality versus algebraic
rewriting. We show that the various languages simulate one another. Due to
subtle interaction between beta-reduction and algebraic rewriting, to make the
languages consistent some additional hypotheses such as confluence or
normalisation might be required. We carefully devise the required properties
for each proof, making them general enough to be valid for any sub-language
satisfying the corresponding properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2898</identifier>
 <datestamp>2010-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2898</id><created>2010-05-17</created><authors><author><keyname>Hadzi-Velkov</keyname><forenames>Zoran</forenames></author><author><keyname>Spasenovski</keyname><forenames>Boris</forenames></author></authors><title>Saturation Throughput - Delay Analysis of IEEE 802.11 DCF in Fading
  Channel</title><categories>cs.NI cs.IT cs.PF math.IT</categories><comments>Proceedings of the IEEE International Conference on Communications
  2003 (ICC 2003), 6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analytically analyzed the impact of an error-prone channel
over all performance measures in a trafficsaturated IEEE 802.11 WLAN. We
calculated station's transmission probability by using the modified Markov
chain model of the backoff window size that considers the frame-error rates and
maximal allowable number of retransmission attempts. The frame error rate has a
significant impact over theoretical throughput, mean frame delay, and discard
probability. The peak throughput of a WLAN is insensitive of the maximal number
of retransmissions. Discard probabilities are insensitive to the station access
method, Basic or RTS/CTS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2907</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2907</id><created>2010-05-17</created><authors><author><keyname>Berardi</keyname><forenames>Stefano</forenames></author><author><keyname>de'Liguoro</keyname><forenames>Ugo</forenames></author></authors><title>Interactive Realizers and Monads</title><categories>cs.LO</categories><acm-class>D.1.1; D.1.2; F.1.2; F.4.1; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a realizability interpretation of a system for quantifier free
arithmetic which is equivalent to the fragment of classical arithmetic without
&quot;nested&quot; quantifiers, called here EM1-arithmetic. We interpret classical proofs
as interactive learning strategies, namely as processes going through several
stages of knowledge and learning by interacting with the &quot;environment&quot; and with
each other. We give a categorical presentation of the interpretation through
the construction of two suitable monads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2914</identifier>
 <datestamp>2010-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2914</id><created>2010-05-17</created><authors><author><keyname>S</keyname><forenames>Iyengar.</forenames></author><author><keyname>Apte</keyname><forenames>N</forenames></author><author><keyname>Roy</keyname><forenames>A. A.</forenames></author><author><keyname>Sanyal</keyname><forenames>S.</forenames></author><author><keyname>Singhi</keyname><forenames>N. M.</forenames></author><author><keyname>Feng</keyname><forenames>Wu Geng</forenames></author></authors><title>A Multiprocessor Communication Architecture For High Speed Networks</title><categories>cs.NI</categories><comments>4 Pages, 7 Figures, TENCON'93, Beijing, 1993 Region 10 International
  Conference on 'Computers, Communications, Control and Power Engineering',
  Vol.1, pp. 262-265</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the years, communication speed of networks has increased from a few Kbps
to several Mbps, as also the bandwidth demand, Communication Protocols, however
have not improved to that extent. With the advent of Wavelength Division
Multiplexing (WDM), it is now possible to &quot;tune&quot; protocols to current and
future demands. The purpose of this paper is to evolve a High Speed Network
architecture, which will cater to the needs of bandwidth-consuming
applications, such as voice, video and high definition image transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2949</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2949</id><created>2010-05-17</created><authors><author><keyname>Chebira</keyname><forenames>Amina</forenames></author><author><keyname>Fickus</keyname><forenames>Matthew</forenames></author><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author></authors><title>Filter Bank Fusion Frames</title><categories>cs.IT math.IT math.RT</categories><comments>keywords: filter banks, frames, tight, fusion, erasures, polyphase</comments><doi>10.1109/TSP.2010.2097255</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we characterize and construct novel oversampled filter banks
implementing fusion frames. A fusion frame is a sequence of orthogonal
projection operators whose sum can be inverted in a numerically stable way.
When properly designed, fusion frames can provide redundant encodings of
signals which are optimally robust against certain types of noise and erasures.
However, up to this point, few implementable constructions of such frames were
known; we show how to construct them using oversampled filter banks. In this
work, we first provide polyphase domain characterizations of filter bank fusion
frames. We then use these characterizations to construct filter bank fusion
frame versions of discrete wavelet and Gabor transforms, emphasizing those
specific finite impulse response filters whose frequency responses are
well-behaved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.2967</identifier>
 <datestamp>2010-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.2967</id><created>2010-05-17</created><updated>2010-12-26</updated><authors><author><keyname>Tang</keyname><forenames>Choon Yik</forenames></author><author><keyname>Lu</keyname><forenames>Jie</forenames></author></authors><title>Controlled Hopwise Averaging: Bandwidth/Energy-Efficient Asynchronous
  Distributed Averaging for Wireless Networks</title><categories>math.OC cs.DC cs.SY</categories><comments>33 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of averaging numbers across a wireless
network from an important, but largely neglected, viewpoint: bandwidth/energy
efficiency. We show that existing distributed averaging schemes have several
drawbacks and are inefficient, producing networked dynamical systems that
evolve with wasteful communications. Motivated by this, we develop Controlled
Hopwise Averaging (CHA), a distributed asynchronous algorithm that attempts to
&quot;make the most&quot; out of each iteration by fully exploiting the broadcast nature
of wireless medium and enabling control of when to initiate an iteration. We
show that CHA admits a common quadratic Lyapunov function for analysis, derive
bounds on its exponential convergence rate, and show that they outperform the
convergence rate of Pairwise Averaging for some common graphs. We also
introduce a new way to apply Lyapunov stability theory, using the Lyapunov
function to perform greedy, decentralized, feedback iteration control. Finally,
through extensive simulation on random geometric graphs, we show that CHA is
substantially more efficient than several existing schemes, requiring far fewer
transmissions to complete an averaging task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3004</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3004</id><created>2010-05-17</created><updated>2014-09-21</updated><authors><author><keyname>Altendorfer</keyname><forenames>Richard</forenames></author></authors><title>Observable dynamics and coordinate systems for automotive target
  tracking</title><categories>cs.RO</categories><comments>6 pages, 3 figures</comments><journal-ref>Proceedings of the IEEE Intelligent Vehicles Symposium (2009)
  741-746</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate several coordinate systems and dynamical vector fields for
target tracking to be used in driver assistance systems. We show how to express
the discrete dynamics of maneuvering target vehicles in arbitrary coordinates
starting from the target's and the own (ego) vehicle's assumed dynamical model
in global coordinates. We clarify the notion of &quot;ego compensation&quot; and show how
non-inertial effects are to be included when using a body-fixed coordinate
system for target tracking. We finally compare the tracking error of different
combinations of target tracking coordinates and dynamical vector fields for
simulated data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3010</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3010</id><created>2010-05-17</created><updated>2010-09-21</updated><authors><author><keyname>Wan</keyname><forenames>Changlin</forenames></author></authors><title>A Proof for P =? NP Problem</title><categories>cs.CC</categories><comments>10 pages, 1 figure</comments><acm-class>F.1.3; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The \textbf{P} =? \textbf{NP} problem is an important problem in contemporary
mathematics and theoretical computer science. Many proofs have been proposed to
this problem. This paper proposes a theoretic proof for \textbf{P} =?
\textbf{NP} problem. The central idea of this proof is a recursive definition
for Turing machine (shortly TM) that accepts the encoding strings of valid TMs.
By the definition, an infinite sequence of TM is constructed, and it is proven
that the sequence includes all valid TMs. Based on these TMs, the class
\textbf{D} that includes all decidable languages is defined. By proving
\textbf{P}=\textbf{D}, the result \textbf{P}=\textbf{NP} is proven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3011</identifier>
 <datestamp>2010-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3011</id><created>2010-05-17</created><authors><author><keyname>Knysh</keyname><forenames>S.</forenames></author><author><keyname>Smelyanskiy</keyname><forenames>V.</forenames></author></authors><title>On the relevance of avoided crossings away from quantum critical point
  to the complexity of quantum adiabatic algorithm</title><categories>quant-ph cond-mat.mes-hall cs.CC</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two recent preprints [B. Altshuler, H. Krovi, and J. Roland, &quot;Quantum
adiabatic optimization fails for random instances of NP-complete problems&quot;,
arXiv:0908.2782 and &quot;Anderson localization casts clouds over adiabatic quantum
optimization&quot;, arXiv:0912.0746] argue that random 4th order perturbative
corrections to the energies of local minima of random instances of NP-complete
problem lead to avoided crossings that cause the failure of quantum adiabatic
algorithm (due to exponentially small gap) close to the end, for very small
transverse field that scales as an inverse power of instance size N. The
theoretical portion of this work does not to take into account the exponential
degeneracy of the ground and excited states at zero field. A corrected analysis
shows that unlike those in the middle of the spectrum, avoided crossings at the
edge would require high [O(1)] transverse fields, at which point the
perturbation theory may become divergent due to quantum phase transition. This
effect manifests itself only in large instances [exp(0.02 N) &gt;&gt; 1], which might
be the reason it had not been observed in the authors' numerical work. While we
dispute the proposed mechanism of failure of quantum adiabatic algorithm, we
cannot draw any conclusions on its ultimate complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3014</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3014</id><created>2010-05-17</created><updated>2011-12-16</updated><authors><author><keyname>Ackerman</keyname><forenames>Nathanael L.</forenames></author><author><keyname>Freer</keyname><forenames>Cameron E.</forenames></author><author><keyname>Roy</keyname><forenames>Daniel M.</forenames></author></authors><title>On the computability of conditional probability</title><categories>math.LO cs.LO math.PR math.ST stat.ML stat.TH</categories><comments>36 pages, 3 figures. Substantially expanded, with minor corrections.
  Full version; extended abstract appeared in Proceedings of LICS 2011, pp.
  107-116</comments><msc-class>03D78, 62F15, 68T37, 60B05, 03F60, 65G50, 60A05, 60A10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As inductive inference and machine learning methods in computer science see
continued success, researchers are aiming to describe even more complex
probabilistic models and inference algorithms. What are the limits of
mechanizing probabilistic inference? We investigate the computability of
conditional probability, a fundamental notion in probability theory and a
cornerstone of Bayesian statistics, and show that there are computable joint
distributions with noncomputable conditional distributions, ruling out the
prospect of general inference algorithms, even inefficient ones. Specifically,
we construct a pair of computable random variables in the unit interval such
that the conditional distribution of the first variable given the second
encodes the halting problem. Nevertheless, probabilistic inference is possible
in many common modeling settings, and we prove several results giving broadly
applicable conditions under which conditional distributions are computable. In
particular, conditional distributions become computable when measurements are
corrupted by independent computable noise with a sufficiently smooth density.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3045</identifier>
 <datestamp>2010-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3045</id><created>2010-05-17</created><updated>2010-09-13</updated><authors><author><keyname>Stein</keyname><forenames>Noah D.</forenames></author><author><keyname>Parrilo</keyname><forenames>Pablo A.</forenames></author><author><keyname>Ozdaglar</keyname><forenames>Asuman</forenames></author></authors><title>A partial proof of Nash's Theorem via exchangeable equilibria</title><categories>cs.GT math.OC</categories><comments>Announcement of an error found in the original proof submitted 17 May
  2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document consists of two parts: the second part was submitted earlier as
a new proof of Nash's theorem, and the first part is a note explaining a
problem found in that proof. We are indebted to Sergiu Hart and Eran Shmaya for
their careful study which led to their simultaneous discovery of this error. So
far the error has not been fixed, but many of the results and techniques of the
paper remain valid, so we will continue to make it available online.
  Abstract for the original paper:
  We give a novel proof of the existence of Nash equilibria in all finite games
without using fixed point theorems or path following arguments. Our approach
relies on a new notion intermediate between Nash and correlated equilibria
called exchangeable equilibria, which are correlated equilibria with certain
symmetry and factorization properties. We prove these exist by a duality
argument, using Hart and Schmeidler's proof of correlated equilibrium existence
as a first step.
  In an appropriate limit exchangeable equilibria converge to the convex hull
of Nash equilibria, proving that these exist as well. Exchangeable equilibria
are defined in terms of symmetries of the game, so this method automatically
proves the stronger statement that a symmetric game has a symmetric Nash
equilibrium. The case without symmetries follows by a symmetrization argument.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3063</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3063</id><created>2010-05-17</created><updated>2012-09-22</updated><authors><author><keyname>Amancio</keyname><forenames>D. R.</forenames></author><author><keyname>Nunes</keyname><forenames>M. G. V.</forenames></author><author><keyname>Oliveira</keyname><forenames>O. N.</forenames><suffix>Jr.</suffix></author><author><keyname>Costa</keyname><forenames>L. da F.</forenames></author></authors><title>Good practices for a literature survey are not followed by authors while
  preparing scientific manuscripts</title><categories>physics.soc-ph cs.DL</categories><journal-ref>Scientometrics, v. 90, p. 2, (2012)</journal-ref><doi>10.1007/s11192-012-0630-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of citations received by authors in scientific journals has become
a major parameter to assess individual researchers and the journals themselves
through the impact factor. A fair assessment therefore requires that the
criteria for selecting references in a given manuscript should be unbiased with
respect to the authors or the journals cited. In this paper, we advocate that
authors should follow two mandatory principles to select papers (later
reflected in the list of references) while studying the literature for a given
research: i) consider similarity of content with the topics investigated, lest
very related work should be reproduced or ignored; ii) perform a systematic
search over the network of citations including seminal or very related papers.
We use formalisms of complex networks for two datasets of papers from the arXiv
repository to show that neither of these two criteria is fulfilled in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3073</identifier>
 <datestamp>2010-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3073</id><created>2010-05-17</created><authors><author><keyname>Alam</keyname><forenames>S. M. Nazrul</forenames></author><author><keyname>Haas</keyname><forenames>Zygmunt</forenames></author></authors><title>Hierarchical and Nonhierarchical Three-Dimensional Underwater Wireless
  Sensor Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In some underwater sensor networks, sensor nodes may be deployed at various
depths of an ocean making those networks three-dimensional (3D). While most
terrestrial sensor networks can usually be modeled as two dimensional (2D)
networks, these underwater sensor networks must be modeled as 3D networks. This
leads to new research challenges in the area of network architecture and
topology. In this paper, we present two different network architectures for 3D
underwater sensor networks. The first one is a hierarchical architecture that
uses a relatively small number of robust backbone nodes to create the network
where a large number of inexpensive sensors communicate with their nearest
backbone nodes, and packets from a backbone node to the sink is routed through
other backbone nodes. This hierarchical approach allows creating a network of
smaller number of expensive backbone nodes while keeping the mobile sensors
simple and inexpensive. Along with network topology, we also study energy
efficiency and frequency reuse issues for such 3D networks. The second approach
is a nonhierarchical architecture which assumes that all nodes are identical
and randomly deployed. It partitions the whole 3D network space into identical
cells and keeps one node active in each cell such that sensing coverage and
connectivity are maintained while limiting the energy consumed. We also study
closeness to optimality of our proposed scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3093</identifier>
 <datestamp>2011-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3093</id><created>2010-05-17</created><updated>2011-02-17</updated><authors><author><keyname>Xu</keyname><forenames>Zhiqiang</forenames></author></authors><title>A remark about orthogonal matching pursuit algorithm</title><categories>cs.IT math.IT math.NA</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we investigate the theoretical properties of Orthogonal
Matching Pursuit (OMP), a class of decoder to recover sparse signal in
compressed sensing. In particular, we show that the OMP decoder can give
$(p,q)$ instance optimality for a large class of encoders with $1\leq p\leq q
\leq 2$ and $(p,q)\neq (2,2)$. We also show that, if the encoding matrix is
drawn from an appropriate distribution, then the OMP decoder is $(2,2)$
instance optimal in probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3097</identifier>
 <datestamp>2010-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3097</id><created>2010-05-18</created><authors><author><keyname>Drineas</keyname><forenames>Petros</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author></authors><title>Effective Resistances, Statistical Leverage, and Applications to Linear
  Equation Solving</title><categories>cs.NA</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work in theoretical computer science and scientific computing has
focused on nearly-linear-time algorithms for solving systems of linear
equations. While introducing several novel theoretical perspectives, this work
has yet to lead to practical algorithms. In an effort to bridge this gap, we
describe in this paper two related results. Our first and main result is a
simple algorithm to approximate the solution to a set of linear equations
defined by a Laplacian (for a graph $G$ with $n$ nodes and $m \le n^2$ edges)
constraint matrix. The algorithm is a non-recursive algorithm; even though it
runs in $O(n^2 \cdot \polylog(n))$ time rather than $O(m \cdot polylog(n))$
time (given an oracle for the so-called statistical leverage scores), it is
extremely simple; and it can be used to compute an approximate solution with a
direct solver. In light of this result, our second result is a straightforward
connection between the concept of graph resistance (which has proven useful in
recent algorithms for linear equation solvers) and the concept of statistical
leverage (which has proven useful in numerically-implementable randomized
algorithms for large matrix problems and which has a natural data-analytic
interpretation).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3124</identifier>
 <datestamp>2012-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3124</id><created>2010-05-18</created><updated>2010-05-25</updated><authors><author><keyname>Liu</keyname><forenames>Chuang</forenames></author><author><keyname>Zhou</keyname><forenames>Wei-Xing</forenames></author></authors><title>An improved HeatS+ProbS hybrid recommendation algorithm based on
  heterogeneous initial resource configurations</title><categories>physics.soc-ph cs.IR</categories><comments>6 pages 3 figures</comments><journal-ref>Physica A 391 (22), 5704-5711 (2012)</journal-ref><doi>10.1016/j.physa.2012.06.034</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network-based recommendation algorithms for user-object link predictions have
achieved significant developments in recent years. For bipartite graphs, the
reallocation of resource in such algorithms is analogous to heat spreading
(HeatS) or probability spreading (ProbS) processes. The best algorithm to date
is a hybrid of the HeatS and ProbS techniques with homogenous initial resource
configurations, which fulfills simultaneously high accuracy and large
diversity. We investigate the effect of heterogeneity in initial configurations
on the HeatS+ProbS hybrid algorithm and find that both recommendation accuracy
and diversity can be further improved in this new setting. Numerical
experiments show that the improvement is robust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3143</identifier>
 <datestamp>2010-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3143</id><created>2010-05-18</created><authors><author><keyname>Caballero-Gil</keyname><forenames>P.</forenames></author><author><keyname>Molina-Gil</keyname><forenames>J.</forenames></author><author><keyname>Hern&#xe1;ndez-Goya</keyname><forenames>C.</forenames></author><author><keyname>Caballero-Gil</keyname><forenames>C.</forenames></author></authors><title>Stimulating Cooperation in Self-Organized Vehicular Networks</title><categories>cs.CR</categories><journal-ref>Proceedings of APCC IEEE Asia Pacific Conference on
  Communications. Vol. 82 , (October 2009) pp. 346-349</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Vehicular Ad-hoc NETwork (VANET) is a special form of Mobile Ad-hoc Network
designed to provide communications among nearby vehicles and between vehicles
and nearby fixed roadside equipment. Its main goal is to improve safety and
comfort for passengers, but it can also be used for commercial applications. In
this latter case, it will be necessary to motivate drivers to cooperate and
contribute to packet forwarding in Vehicle-to-Vehicle and Vehicle-to-Roadside
communications. This paper examines the problem, analyzes the drawbacks of
known schemes and proposes a new secure incentive scheme to stimulate
cooperation in VANETs, taking into account factors such as time and distance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3148</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3148</id><created>2010-05-18</created><authors><author><keyname>Argyraki</keyname><forenames>Katerina</forenames></author><author><keyname>Maniatis</keyname><forenames>Petros</forenames></author><author><keyname>Singla</keyname><forenames>Ankit</forenames></author></authors><title>Verifiable Network-Performance Measurements</title><categories>cs.NI</categories><comments>14 pages</comments><journal-ref>Proceedings of the ACM Conference on Emerging Networking
  Experiments and Technologies (CoNEXT), November 2010</journal-ref><doi>10.1145/1921168.1921170</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the current Internet, there is no clean way for affected parties to react
to poor forwarding performance: when a domain violates its Service Level
Agreement (SLA) with a contractual partner, the partner must resort to ad-hoc
probing-based monitoring to determine the existence and extent of the
violation. Instead, we propose a new, systematic approach to the problem of
forwarding-performance verification. Our mechanism relies on voluntary
reporting, allowing each domain to disclose its loss and delay performance to
its neighbors; it does not disclose any information regarding the participating
domains' topology or routing policies beyond what is already publicly
available. Most importantly, it enables verifiable performance measurements,
i.e., domains cannot abuse it to significantly exaggerate their performance.
Finally, our mechanism is tunable, allowing each participating domain to
determine how many resources to devote to it independently (i.e., without any
inter-domain coordination), exposing a controllable trade-off between
performance-verification quality and resource consumption. Our mechanism comes
at the cost of deploying modest functionality at the participating domains'
border routers; we show that it requires reasonable processing and memory
resources within modern network capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3153</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3153</id><created>2010-05-18</created><updated>2010-05-21</updated><authors><author><keyname>Tanaka</keyname><forenames>Satoru</forenames></author><author><keyname>Ogura</keyname><forenames>Naoki</forenames></author><author><keyname>Nakamula</keyname><forenames>Ken</forenames></author><author><keyname>Matsui</keyname><forenames>Tetsushi</forenames></author><author><keyname>Uchiyama</keyname><forenames>Shigenori</forenames></author></authors><title>NZMATH 1.0</title><categories>math.NT cs.MS</categories><comments>This paper has been withdrawn by the author due to rush into the
  conclusion that both release of this paper and NZMATH 1.0 are the same.
  NZMATH 1.0 is expected to be available soon</comments><msc-class>11Y40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is an announcement of the first official release ver.1.0 of a Python
system NZMATH for number theory. We overview all functions in NZMATH 1.0, show
main properties after former report on NZMATH 0.5.0, and describe new features
for stable development. The most important point of the release is that we can
now treat number fields. The second big change is that new type of polynomial
programs are provided. Elliptic curve primality proving and its related
programs are also available, where we partly use a library outside NZMATH as an
advantage of writing the system only by Python. On method of development, a new
feature is that NZMATH is registered on SourceForge as an open source project
to keep continuous development of the project. This is a unique attempt among
existing systems for number theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3158</identifier>
 <datestamp>2010-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3158</id><created>2010-05-18</created><authors><author><keyname>Tavakoli</keyname><forenames>Ruhollah</forenames></author></authors><title>Improvement Cache Efficiency of Explicit Finite Element Procedure and
  its Application to Parallel Casting Solidification Simulation</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple method for improving cache efficiency of serial and parallel
explicit finite procedure with application to casting solidification simulation
over three-dimensional complex geometries is presented. The method is based on
division of the global data to smaller blocks and treating each block
independently from others at each time step. A novel parallel finite element
algorithm for non-overlapped element-base decomposed domain is presented for
implementation of serial and parallel version of the presented method. Effect
of mesh reordering on the efficiency is also investigated. A simple algorithm
is presented for high quality decomposition of decoupled global mesh. Our
result shows 10-20 \% performance improvement by mesh reordering and 1.2-2.2
speedup with application of the presented cache efficient algorithm (for serial
and parallel versions). Also the presented parallel solver (without
cache-efficient feature) shows nearly linear speedup on the traditional
Ethernet networked Linux cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3163</identifier>
 <datestamp>2010-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3163</id><created>2010-05-18</created><authors><author><keyname>Neu</keyname><forenames>Andreas</forenames><affiliation>RWTH Aachen University, Computer Graphics &amp; Multimedia</affiliation></author></authors><title>Virtual Texturing</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis a rendering system and an accompanying tool chain for Virtual
Texturing is presented. Our tools allow to automatically retexture existing
geometry in order to apply unique texturing on each face. Furthermore we
investigate several techniques that try to minimize visual artifacts in the
case that only a small amount of pages can be streamed per frame. We analyze
the influence of different heuristics that are responsible for the page
selection. Alongside these results we present a measurement method to allow the
comparison of our heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3181</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3181</id><created>2010-05-18</created><authors><author><keyname>Marliere</keyname><forenames>Sylvain</forenames><affiliation>LEPES</affiliation></author><author><keyname>Urma</keyname><forenames>Daniela</forenames><affiliation>ICA</affiliation></author><author><keyname>Florens</keyname><forenames>Jean-Loup</forenames><affiliation>ACROE</affiliation></author><author><keyname>Marchi</keyname><forenames>Florence</forenames><affiliation>LEPES</affiliation></author></authors><title>Multi-sensorial interaction with a nano-scale phenomenon : the force
  curve</title><categories>cs.GR cs.HC physics.comp-ph</categories><proxy>ccsd</proxy><journal-ref>EuroHaptics 2004, Munich : Germany (2004)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using Atomic Force Microscopes (AFM) to manipulate nano-objects is an actual
challenge for surface scientists. Basic haptic interfacesbetween the AFM and
experimentalists have already been implemented. Themulti-sensory renderings
(seeing, hearing and feeling) studied from acognitive point of view increase
the efficiency of the actual interfaces. Toallow the experimentalist to feel
and touch the nano-world, we add mixedrealities between an AFM and a force
feedback device, enriching thus thedirect connection by a modeling engine. We
present in this paper the firstresults from a real-time remote-control handling
of an AFM by our ForceFeedback Gestural Device through the example of the
approach-retract curve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3182</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3182</id><created>2010-05-18</created><updated>2010-06-07</updated><authors><author><keyname>Castagn&#xe9;</keyname><forenames>Nicolas</forenames><affiliation>ACROE</affiliation></author><author><keyname>Cadoz</keyname><forenames>Claude</forenames><affiliation>ACROE, ICA</affiliation></author><author><keyname>Florens</keyname><forenames>Jean-Loup</forenames><affiliation>ACROE</affiliation></author><author><keyname>Luciani</keyname><forenames>Annie</forenames><affiliation>ACROE, ICA</affiliation></author></authors><title>Haptics in computer music : a paradigm shift</title><categories>cs.HC cs.SD</categories><comments>Document accompagn\'e du poster</comments><proxy>ccsd</proxy><journal-ref>EuroHaptics 2004, Munich : Germany (2004)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With an historical point of view combined with a bibliographic overview, the
article discusses the idea that haptic force feedback transducers correspond
with a paradigm shift in our real-time tools for creating music. So doing, il
shows that computer music may be regarded as a major field of research and
application for haptics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3184</identifier>
 <datestamp>2010-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3184</id><created>2010-05-18</created><authors><author><keyname>Yakovlev</keyname><forenames>Viktor</forenames></author><author><keyname>Korzhik</keyname><forenames>Valery</forenames></author><author><keyname>Morales-Luna</keyname><forenames>Guillermo</forenames></author><author><keyname>Bakaev</keyname><forenames>Mihail</forenames></author></authors><title>Key Distribution Protocols Based on Extractors Under the Condition of
  Noisy Channels in the Presence of an Active Adversary</title><categories>cs.IT math.IT</categories><msc-class>68U10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider in this paper the information-theoretic secure key distribution
problem over main and wire-tap noise channels with a public discussion in
presence of an active adversary. In contrast to the solution proposed by
ourselves for a similar problem using hashing for privacy amplification, in the
current paper we use a technique of extractors.
  We propose modified key distribution protocols for which we prove explicit
estimates of key rates without the use of estimates with uncertain coefficients
in notations $O,\Omega,\Theta$.
  This leads in the new conclusion that the use of extractors is superior to
the use of hash functions only with the very large key lengths $\ell$ (of order
$\ell&gt;10^5$ bits).
  We suggest hybrid key distribution protocols consisting from two
consecutively executed stages. At the fist stage it is generated a short
authentication key based on hash function, whereas at the second stage it is
generated the final key with the use of extractors. We show that in fact the
use of extraction procedure is effective only at the second stage. We get also
some constructive estimates of the key rates for such protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3185</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3185</id><created>2010-05-18</created><authors><author><keyname>Florens</keyname><forenames>Jean-Loup</forenames><affiliation>ACROE</affiliation></author><author><keyname>Voda</keyname><forenames>Alina</forenames><affiliation>LAG</affiliation></author><author><keyname>Urma</keyname><forenames>Daniela</forenames><affiliation>ICA</affiliation></author></authors><title>Dynamical issues in interactive representation of physical objects</title><categories>cs.GR cs.HC cs.RO</categories><proxy>ccsd</proxy><journal-ref>EuroHaptics 2006, Paris : France (2006)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The quality of a simulator equipped with a haptic interface is given by the
dynamical properties of its components: haptic interface, simulator and control
system. Some application areas of such kind of simulator like musical
synthesis, animation or more general, instrumental art have specific
requirements as for the &quot;haptic rendering&quot; of small movements that go beyond
the usual haptic interfaces allow. Object properties variability and different
situations of object combination represent important aspects of such type of
application which makes that the user can be interested as much in the
restitution of certain global properties of an entire object domain as in the
restitution of properties that are specific to an isolate object. In the
traditional approaches, the usual criteria are founded on the paradigm of
transparency and are related to the impedance error introduced by the technical
aspects of the system. As a general aim, rather than to minimize these effects,
we look to characterize them by physical metaphors conferring to haptic medium
the role of a tool. This positioning leads to firstly analyze the natural human
object interaction as a simplified evolutive system and then considers its
synthesis in the case of the interactive physical simulation. By means of a
frequential method, this approach is presented for some elementary
configurations of the simulator
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3190</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3190</id><created>2010-05-18</created><authors><author><keyname>Luciani</keyname><forenames>Annie</forenames><affiliation>ACROE</affiliation></author></authors><title>From granular avalanches to fluid turbulences through oozing pastes. A
  mesoscopic physically-based particle model</title><categories>cs.GR physics.comp-ph</categories><proxy>ccsd</proxy><journal-ref>Graphicon 2000, Moscou : Russian Federation (2000)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe how we can precisely produce complex and various
dynamic morphological features such as structured and chaotic features which
occur in sand pilings (piles, avalanches, internal collapses, arches) , in
flowing fluids (laminar flowing, Kelvin-Helmholtz and Von Karmann eddies), and
in cohesive pastes (twist-and-turn oozing and packing) using only a single
unified model, called &quot;mesoscopic model&quot;. This model is a physically-based
particle model whose behavior depends on only four simple, but easy to
understand, physically-based parameters : elasticity, viscosity and their local
areas of influence. It is fast to compute and easy to understand by
non-physicist users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3198</identifier>
 <datestamp>2010-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3198</id><created>2010-05-18</created><authors><author><keyname>Caballero-Gil</keyname><forenames>P.</forenames></author><author><keyname>Caballero-Gil</keyname><forenames>C.</forenames></author><author><keyname>Molina-Gil</keyname><forenames>J.</forenames></author><author><keyname>Hern&#xe1;ndez-Goya</keyname><forenames>C.</forenames></author></authors><title>Flexible Authentication in Vehicular Ad hoc Networks</title><categories>cs.CR</categories><journal-ref>Proceedings of APCC IEEE Asia Pacific Conference on
  Communications. Vol. 208 , pp. 876-879, 2009.</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Vehicular Ad-Hoc Network (VANET) is a form of Mobile ad-hoc network, to
provide communications among nearby vehicles and between vehicles and nearby
fixed roadside equipment. The key operation in VANETs is the broadcast of
messages. Consequently, the vehicles need to make sure that the information has
been sent by an authentic node in the network. VANETs present unique challenges
such as high node mobility, real-time constraints, scalability, gradual
deployment and privacy. No existent technique addresses all these requirements.
In particular, both inter-vehicle and vehicle-to-roadside wireless
communications present different characteristics that should be taken into
account when defining node authentication services. That is exactly what is
done in this paper, where the features of inter-vehicle and vehicle-to-roadside
communications are analyzed to propose differentiated services for node
authentication, according to privacy and efficiency needs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3199</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3199</id><created>2010-05-18</created><updated>2011-04-25</updated><authors><author><keyname>Konur</keyname><forenames>Savas</forenames></author></authors><title>A Survey on Temporal Logics</title><categories>cs.LO</categories><report-no>ULCS-08-021</report-no><journal-ref>Frontiers of Computer Science 7(3): 370-403 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper surveys main and recent studies on temporal logics in a broad
sense by presenting various logic systems, dealing with various time
structures, and discussing important features, such as decidability (or
undecidability) results, expressiveness and proof systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3200</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3200</id><created>2010-05-18</created><updated>2011-04-25</updated><authors><author><keyname>Konur</keyname><forenames>Savas</forenames></author></authors><title>Real-time and Probabilistic Temporal Logics: An Overview</title><categories>cs.LO</categories><report-no>ULCS-08-020</report-no><journal-ref>Frontiers of Computer Science 7(3): 370-403 (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last two decades, there has been an extensive study on logical
formalisms for specifying and verifying real-time systems. Temporal logics have
been an important research subject within this direction. Although numerous
logics have been introduced for the formal specification of real-time and
complex systems, an up to date comprehensive analysis of these logics does not
exist in the literature. In this paper we analyse real-time and probabilistic
temporal logics which have been widely used in this field. We extrapolate the
notions of decidability, axiomatizability, expressiveness, model checking, etc.
for each logic analysed. We also provide a comparison of features of the
temporal logics discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3224</identifier>
 <datestamp>2010-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3224</id><created>2010-05-18</created><authors><author><keyname>F&#xfa;ster-Sabater</keyname><forenames>Amparo</forenames></author></authors><title>Cellular Automata in Stream Ciphers</title><categories>cs.CR cs.DM</categories><comments>26 pages, 1 figure</comments><msc-class>40B05, 40C05, 68R01</msc-class><acm-class>E.3; F.1.1</acm-class><journal-ref>Contemporary Mathematics, Volume 477, pp. 1-20, 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide family of nonlinear sequence generators, the so-called
clock-controlled shrinking generators, has been analyzed and identified with a
subset of linear cellular automata. The algorithm that converts the given
generator into a linear model based on automata is very simple and can be
applied in a range of practical interest. Due to the linearity of these
automata as well as the characteristics of this class of generators, a
cryptanalytic approach can be proposed. Linear cellular structures easily model
keystream generators with application in stream cipher cryptography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3238</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3238</id><created>2010-05-18</created><updated>2010-05-18</updated><authors><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Lau</keyname><forenames>Vincent Kin Nang</forenames></author></authors><title>Power Control and Performance Analysis of Outage-Limited Cellular
  Network with MUD-SIC and Macro-Diversity</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we analyze the uplink goodput (bits/sec/Hz successfully
decoded) and per-user packet outage in a cellular network using multi-user
detection with successive interference cancellation (MUD-SIC). We consider
non-ergodic fading channels where microscopic fading channel information is not
available at the transmitters. As a result, packet outage occurs whenever the
data rate of packet transmissions exceeds the instantaneous mutual information
even if powerful channel coding is applied for protection. We are interested to
study the role of macro-diversity (MDiv) between multiple base stations on the
MUD-SIC performance where the effect of potential error-propagation during the
SIC processing is taken into account. While the jointly optimal power and
decoding order in the MUD-SIC are NP hard problem, we derive a simple on/off
power control and asymptotically optimal decoding order with respect to the
transmit power. Based on the information theoretical framework, we derive the
closed-form expressions on the total system goodput as well as the per-user
packet outage probability. We show that the system goodput does not scale with
SNR due to mutual interference in the SIC process and macro-diversity (MDiv)
could alleviate the problem and benefit to the system goodput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3257</identifier>
 <datestamp>2010-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3257</id><created>2010-05-18</created><authors><author><keyname>Andres</keyname><forenames>Daniel</forenames></author><author><keyname>Brickenstein</keyname><forenames>Michael</forenames></author><author><keyname>Levandovskyy</keyname><forenames>Viktor</forenames></author><author><keyname>Mart&#xed;n-Morales</keyname><forenames>Jorge</forenames></author><author><keyname>Sch&#xf6;nemann</keyname><forenames>Hans</forenames></author></authors><title>Constructive $D$-module Theory with \textsc{Singular}</title><categories>math.AG cs.SC</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We overview numerous algorithms in computational $D$-module theory together
with the theoretical background as well as the implementation in the computer
algebra system \textsc{Singular}. We discuss new approaches to the computation
of Bernstein operators, of logarithmic annihilator of a polynomial, of
annihilators of rational functions as well as complex powers of polynomials. We
analyze algorithms for local Bernstein-Sato polynomials and also algorithms,
recovering any kind of Bernstein-Sato polynomial from partial knowledge of its
roots. We address a novel way to compute the Bernstein-Sato polynomial for an
affine variety algorithmically. All the carefully selected nontrivial examples,
which we present, have been computed with our implementation. We address such
applications as the computation of a zeta-function for certain integrals and
revealing the algebraic dependence between pairwise commuting elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3290</identifier>
 <datestamp>2011-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3290</id><created>2010-05-18</created><updated>2011-02-25</updated><authors><author><keyname>Zhuk</keyname><forenames>Sergiy</forenames></author></authors><title>Minimax state estimation for linear continuous differential-algebraic
  equations</title><categories>math.OC cs.SY</categories><comments>9 pages, 1 figure</comments><msc-class>34K32, 49N10, 49N15, 49N30, 49N45x</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a minimax state estimation approach for linear
Differential-Algebraic Equations (DAE) with uncertain parameters. The approach
addresses continuous-time DAE with non-stationary rectangular matrices and
uncertain bounded deterministic input. An observation's noise is supposed to be
random with zero mean and unknown bounded correlation function. Main results
are a Generalized Kalman Duality (GKD) principle and sub-optimal minimax state
estimation algorithm. GKD is derived by means of Young-Fenhel duality theorem.
GKD proves that the minimax estimate coincides with a solution to a Dual
Control Problem (DCP) with DAE constraints. The latter is ill-posed and,
therefore, the DCP is solved by means of Tikhonov regularization approach
resulting a sub-optimal state estimation algorithm in the form of filter. We
illustrate the approach by an synthetic example and we discuss connections with
impulse-observability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3324</identifier>
 <datestamp>2011-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3324</id><created>2010-05-18</created><updated>2011-02-02</updated><authors><author><keyname>Pritchard</keyname><forenames>David</forenames></author></authors><title>An LP with Integrality Gap 1+epsilon for Multidimensional Knapsack</title><categories>cs.DM</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this note we study packing or covering integer programs with at most k
constraints, which are also known as k-dimensional knapsack problems. For any
integer k &gt; 0 and real epsilon &gt; 0, we observe there is a polynomial-sized LP
for the k-dimensional knapsack problem with integrality gap at most 1+epsilon.
The variables may be unbounded or have arbitrary upper bounds. In the packing
case, we can also remove the dependence of the LP on the cost-function,
yielding a polyhedral approximation of the integer hull. This generalizes a
recent result of Bienstock on the classical knapsack problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3338</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3338</id><created>2010-05-18</created><authors><author><keyname>Suh</keyname><forenames>Changho</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>Feedback Capacity of the Gaussian Interference Channel to within 2 Bits</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory (Special
  Issue: Interference Networks)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize the capacity region to within 2 bits/s/Hz and the symmetric
capacity to within 1 bit/s/Hz for the two-user Gaussian interference channel
(IC) with feedback. We develop achievable schemes and derive a new outer bound
to arrive at this conclusion. One consequence of the result is that feedback
provides multiplicative gain, i.e., the gain becomes arbitrarily large for
certain channel parameters. It is a surprising result because feedback has been
so far known to provide no gain in memoryless point-to-point channels and only
bounded additive gain in multiple access channels. The gain comes from using
feedback to maximize resource utilization, thereby enabling more efficient
resource sharing between the interfering users. The result makes use of a
deterministic model to provide insights into the Gaussian channel. This
deterministic model is a special case of El Gamal-Costa deterministic model and
as a side-generalization, we establish the exact feedback capacity region of
this general class of deterministic ICs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3350</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3350</id><created>2010-05-18</created><updated>2011-06-18</updated><authors><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>Xu</keyname><forenames>Jia</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author><author><keyname>Peng</keyname><forenames>Yingning</forenames></author></authors><title>Minimum Variance Multi-Frequency Distortionless Restriction for Digital
  Wideband Beamformer</title><categories>cs.IT math.IT</categories><comments>3 pages, 1 figures; accepted by RADAR2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a digital amplitude-phase weighting array based a minimum
variance multi-frequency distortionless restriction (MVMFDR) to aviod the
frequency band signal distortion in digital beamformer and too short time delay
line (TDL) requirement in analoge wideband TDL array.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3358</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3358</id><created>2010-05-19</created><authors><author><keyname>Berriman</keyname><forenames>G. Bruce</forenames></author><author><keyname>Deelman</keyname><forenames>Ewa</forenames></author></authors><title>The Role of Provenance Management in Accelerating the Rate of
  Astronomical Research</title><categories>astro-ph.IM cs.IR</categories><comments>8 pages, 1 figure; Proceedings of Science, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of vast quantities of data through electronic archives has
transformed astronomical research. It has also enabled the creation of new
products, models and simulations, often from distributed input data and models,
that are themselves made electronically available. These products will only
provide maximal long-term value to astronomers when accompanied by records of
their provenance; that is, records of the data and processes used in the
creation of such products. We use the creation of image mosaics with the
Montage grid-enabled mosaic engine to emphasize the necessity of provenance
management and to understand the science requirements that higher-level
products impose on provenance management technologies. We describe experiments
with one technology, the &quot;Provenance Aware Service Oriented Architecture&quot;
(PASOA), that stores provenance information at each step in the computation of
a mosaic. The results inform the technical specifications of provenance
management systems, including the need for extensible systems built on common
standards. Finally, we describe examples of provenance management technology
emerging from the fields of geophysics and oceanography that have applicability
to astronomy applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3367</identifier>
 <datestamp>2011-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3367</id><created>2010-05-19</created><updated>2011-02-10</updated><authors><author><keyname>Dubois</keyname><forenames>Swan</forenames><affiliation>LIP6, INRIA Rocquencourt</affiliation></author><author><keyname>Masuzawa</keyname><forenames>Toshimitsu</forenames><affiliation>Department of Information and Computer sciences Osaka University</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LIP6</affiliation></author></authors><title>Bounding the Impact of Unbounded Attacks in Stabilization</title><categories>cs.DC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-stabilization is a versatile approach to fault-tolerance since it
permits a distributed system to recover from any transient fault that
arbitrarily corrupts the contents of all memories in the system. Byzantine
tolerance is an attractive feature of distributed systems that permits to cope
with arbitrary malicious behaviors. Combining these two properties proved
difficult: it is impossible to contain the spatial impact of Byzantine nodes in
a self-stabilizing context for global tasks such as tree orientation and tree
construction. We present and illustrate a new concept of Byzantine containment
in stabilization. Our property, called Strong Stabilization enables to contain
the impact of Byzantine nodes if they actually perform too many Byzantine
actions. We derive impossibility results for strong stabilization and present
strongly stabilizing protocols for tree orientation and tree construction that
are optimal with respect to the number of Byzantine nodes that can be tolerated
in a self-stabilizing context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3390</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3390</id><created>2010-05-19</created><authors><author><keyname>Cerf</keyname><forenames>Rapha&#xeb;l</forenames></author></authors><title>Critical control of a genetic algorithm</title><categories>cs.NE cond-mat.stat-mech</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on speculations coming from statistical mechanics and the conjectured
existence of critical states, I propose a simple heuristic in order to control
the mutation probability and the population size of a genetic algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3391</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3391</id><created>2010-05-19</created><authors><author><keyname>Caballero-Gil</keyname><forenames>P.</forenames></author><author><keyname>Caballero-Gil</keyname><forenames>C.</forenames></author><author><keyname>Molina-Gil</keyname><forenames>J.</forenames></author><author><keyname>Hern&#xe1;ndez-Goya</keyname><forenames>C.</forenames></author></authors><title>Self-Organized Authentication Architecture for Mobile Ad-hoc Networks</title><categories>cs.CR</categories><journal-ref>Proceedings of the 6th International Symposium on Modeling and
  Optimization in Mobile, Ad Hoc, and Wireless Networks, WiOpt , (April 2008)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a new architecture, called Global Authentication Scheme
for Mobile Ad-hoc Networks (GASMAN), for fully distributed and self-organized
authentication. In this paper apart from describing all the GASMAN components,
special emphasis is placed on proving that it fulfils every requirement of a
secure distributed authentication scheme, including limited physical protection
of broadcast medium, frequent route changes caused by mobility, lack of
structured hierarchy, etc. Furthermore, an extensive analysis through
simulation experiments in different scenarios is described and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3439</identifier>
 <datestamp>2012-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3439</id><created>2010-05-19</created><updated>2011-08-23</updated><authors><author><keyname>Ghaffari</keyname><forenames>Hamed O.</forenames></author><author><keyname>Sharifzadeh</keyname><forenames>M.</forenames></author><author><keyname>Evgin</keyname><forenames>E.</forenames></author></authors><title>Small World Property of a Rock Joint(Complexity of Frictional
  Interfaces: A Complex Network Perspective)</title><categories>physics.geo-ph cond-mat.dis-nn cs.CE nlin.AO</categories><journal-ref>Geomechanics and Geoengineering: An International Journal,2012</journal-ref><doi>10.1080/17486025.2012.727036</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The shear strength and stick-slip behavior of a rough rock joint are analyzed
using the complex network approach. We develop a network approach on
correlation patterns of void spaces of an evolvable rough fracture (crack type
II). Correlation among networks properties with the hydro -mechanical
attributes (obtained from experimental tests) of fracture before and after slip
is the direct result of the revealed non-contacts networks. Joint distribution
of locally and globally filtered correlation gives a close relation to the
contact zones attachment-detachment sequences through the evolution of shear
strength of the rock joint. Especially spread of node's degree rate to spread
of clustering coefficient rate yielded possible stick and slip sequences during
the displacements. Our method can be developed to investigate the complexity of
stick-slip behavior of faults as well as energy /stress localization on
crumpled shells/sheets in which ridge networks are controlling the energy
distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3450</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3450</id><created>2010-05-19</created><authors><author><keyname>Aviram</keyname><forenames>Amittai</forenames><affiliation>Yale University</affiliation></author><author><keyname>Weng</keyname><forenames>Shu-Chun</forenames><affiliation>Yale University</affiliation></author><author><keyname>Hu</keyname><forenames>Sen</forenames><affiliation>Yale University</affiliation></author><author><keyname>Ford</keyname><forenames>Bryan</forenames><affiliation>Yale University</affiliation></author></authors><title>Efficient System-Enforced Deterministic Parallelism</title><categories>cs.OS cs.DC</categories><comments>14 pages, 12 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deterministic execution offers many benefits for debugging, fault tolerance,
and security. Running parallel programs deterministically is usually difficult
and costly, however - especially if we desire system-enforced determinism,
ensuring precise repeatability of arbitrarily buggy or malicious software.
Determinator is a novel operating system that enforces determinism on both
multithreaded and multi-process computations. Determinator's kernel provides
only single-threaded, &quot;shared-nothing&quot; address spaces interacting via
deterministic synchronization. An untrusted user-level runtime uses distributed
computing techniques to emulate familiar abstractions such as Unix processes,
file systems, and shared memory multithreading. The system runs parallel
applications deterministically both on multicore PCs and across nodes in a
cluster. Coarse-grained parallel benchmarks perform and scale comparably to -
sometimes better than - conventional systems, though determinism is costly for
fine-grained parallel applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3473</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3473</id><created>2010-05-19</created><authors><author><keyname>Alka</keyname></author></authors><title>Efficient Algorithms and Data Structures for Massive Data Sets</title><categories>cs.DS</categories><comments>PhD Thesis (144 pages)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For many algorithmic problems, traditional algorithms that optimise on the
number of instructions executed prove expensive on I/Os. Novel and very
different design techniques, when applied to these problems, can produce
algorithms that are I/O efficient. This thesis adds to the growing chorus of
such results. The computational models we use are the external memory model and
the W-Stream model.
  On the external memory model, we obtain the following results. (1) An I/O
efficient algorithm for computing minimum spanning trees of graphs that
improves on the performance of the best known algorithm. (2) The first external
memory version of soft heap, an approximate meldable priority queue. (3) Hard
heap, the first meldable external memory priority queue that matches the
amortised I/O performance of the known external memory priority queues, while
allowing a meld operation at the same amortised cost. (4) I/O efficient exact,
approximate and randomised algorithms for the minimum cut problem, which has
not been explored before on the external memory model. (5) Some lower and upper
bounds on I/Os for interval graphs.
  On the W-Stream model, we obtain the following results. (1) Algorithms for
various tree problems and list ranking that match the performance of the best
known algorithms and are easier to implement than them. (2) Pass efficient
algorithms for sorting, and the maximal independent set problems, that improve
on the best known algorithms. (3) Pass efficient algorithms for the graphs
problems of finding vertex-colouring, approximate single source shortest paths,
maximal matching, and approximate weighted vertex cover. (4) Lower bounds on
passes for list ranking and maximal matching.
  We propose two variants of the W-Stream model, and design algorithms for the
maximal independent set, vertex-colouring, and planar graph single source
shortest paths problems on those models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3477</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3477</id><created>2010-05-19</created><authors><author><keyname>Chebolu</keyname><forenames>Prasad</forenames></author><author><keyname>Cryan</keyname><forenames>Mary</forenames></author><author><keyname>Martin</keyname><forenames>Russell</forenames></author></authors><title>Exact counting of Euler Tours for generalized series-parallel graphs</title><categories>cs.DS cs.DM math.CO</categories><comments>Article - 17 pages + Abstract - 1 page, 2 figures</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a simple polynomial-time algorithm to exactly count the number of
Euler Tours (ETs) of any Eulerian generalized series-parallel graph, and show
how to adapt this algorithm to exactly sample a random ET of the given
generalized series-parallel graph. Note that the class of generalized
seriesparallel graphs includes all outerplanar graphs. We can perform the
counting in time $O(m\Delta^3)$, where $\Delta$ is the maximum degree of the
graph with $m$ edges. We use $O(m\Delta^2 \log \Delta)$ bits to store
intermediate values during our computations. To date, these are the first known
polynomial-time algorithms to count or sample ETs of any class of graphs; there
are no other known polynomial-time algorithms to even approximately count or
sample ETs of any other class of graphs. The problem of counting ETs is known
to be $#P$-complete for general graphs (Brightwell and Winkler, 2005 [3]) and
also for planar graphs (Creed, 2009 [4]).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3486</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3486</id><created>2010-05-19</created><authors><author><keyname>Zumbragel</keyname><forenames>Jens</forenames></author><author><keyname>Flanagan</keyname><forenames>Mark F.</forenames></author><author><keyname>Skachek</keyname><forenames>Vitaly</forenames></author></authors><title>Exploration of AWGNC and BSC Pseudocodeword Redundancy</title><categories>cs.IT math.IT</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The AWGNC, BSC, and max-fractional pseudocodeword redundancy of a code is
defined as the smallest number of rows in a parity-check matrix such that the
corresponding minimum pseudoweight is equal to the minimum Hamming distance of
the code. This paper provides new results on the AWGNC, BSC, and max-fractional
pseudocodeword redundancies of codes. The pseudocodeword redundancies for all
codes of small length (at most 9) are computed. Also, comprehensive results are
provided on the cases of cyclic codes of length at most 250 for which the
eigenvalue bound of Vontobel and Koetter is sharp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3502</identifier>
 <datestamp>2010-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3502</id><created>2010-05-19</created><authors><author><keyname>Kotthoff</keyname><forenames>Lars</forenames></author><author><keyname>Gent</keyname><forenames>Ian</forenames></author><author><keyname>Miguel</keyname><forenames>Ian</forenames></author></authors><title>Using machine learning to make constraint solver implementation
  decisions</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programs to solve so-called constraint problems are complex pieces of
software which require many design decisions to be made more or less
arbitrarily by the implementer. These decisions affect the performance of the
finished solver significantly. Once a design decision has been made, it cannot
easily be reversed, although a different decision may be more appropriate for a
particular problem.
  We investigate using machine learning to make these decisions automatically
depending on the problem to solve with the alldifferent constraint as an
example. Our system is capable of making non-trivial, multi-level decisions
that improve over always making a default choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3529</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3529</id><created>2010-05-19</created><updated>2010-08-06</updated><authors><author><keyname>Hunt</keyname><forenames>D.</forenames></author><author><keyname>Korniss</keyname><forenames>G.</forenames></author><author><keyname>Szymanski</keyname><forenames>B. K.</forenames></author></authors><title>Network Synchronization in a Noisy Environment with Time Delays:
  Fundamental Limits and Trade-Offs</title><categories>cond-mat.stat-mech cond-mat.dis-nn cs.MA</categories><comments>3 figures</comments><journal-ref>Phys. Rev. Lett. 105, 068701 (2010)</journal-ref><doi>10.1103/PhysRevLett.105.068701</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the effects of nonzero time delays in stochastic synchronization
problems with linear couplings in an arbitrary network. Using the known exact
threshold value from the theory of differential equations with delays, we
provide the synchronizability threshold for an arbitrary network. Further, by
constructing the scaling theory of the underlying fluctuations, we establish
the absolute limit of synchronization efficiency in a noisy environment with
uniform time delays, i.e., the minimum attainable value of the width of the
synchronization landscape. Our results have also strong implications for
optimization and trade-offs in network synchronization with delays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3561</identifier>
 <datestamp>2013-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3561</id><created>2010-05-19</created><authors><author><keyname>Farsani</keyname><forenames>Reza K.</forenames></author></authors><title>Two-Way Writing on Dirty Paper</title><categories>cs.IT math.IT</categories><comments>3 pages, 1 figure, submitted to IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the Two-Way Channel (TWC) with Cannel State Information (CSI)
is investigated. First, an achievable rate region is derived for the discrete
memoryless channel. Then by extending the result to the Gaussian TWC with
additive interference noise, it is shown that the capacity region of the later
channel is the same as the capacity when there is no interference, i.e. a
two-way version of Costa's writing on dirty paper problem is established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3566</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3566</id><created>2010-05-19</created><authors><author><keyname>Kanade</keyname><forenames>Varun</forenames></author><author><keyname>Valiant</keyname><forenames>Leslie G.</forenames></author><author><keyname>Vaughan</keyname><forenames>Jennifer Wortman</forenames></author></authors><title>Evolution with Drifting Targets</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the question of the stability of evolutionary algorithms to
gradual changes, or drift, in the target concept. We define an algorithm to be
resistant to drift if, for some inverse polynomial drift rate in the target
function, it converges to accuracy 1 -- \epsilon , with polynomial resources,
and then stays within that accuracy indefinitely, except with probability
\epsilon , at any one time. We show that every evolution algorithm, in the
sense of Valiant (2007; 2009), can be converted using the Correlational Query
technique of Feldman (2008), into such a drift resistant algorithm. For certain
evolutionary algorithms, such as for Boolean conjunctions, we give bounds on
the rates of drift that they can resist. We develop some new evolution
algorithms that are resistant to significant drift. In particular, we give an
algorithm for evolving linear separators over the spherically symmetric
distribution that is resistant to a drift rate of O(\epsilon /n), and another
algorithm over the more general product normal distributions that resists a
smaller drift rate.
  The above translation result can be also interpreted as one on the robustness
of the notion of evolvability itself under changes of definition. As a second
result in that direction we show that every evolution algorithm can be
converted to a quasi-monotonic one that can evolve from any starting point
without the performance ever dipping significantly below that of the starting
point. This permits the somewhat unnatural feature of arbitrary performance
degradations to be removed from several known robustness translations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3579</identifier>
 <datestamp>2010-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3579</id><created>2010-05-19</created><authors><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Kim</keyname><forenames>Seyoung</forenames></author><author><keyname>Lin</keyname><forenames>Qihang</forenames></author><author><keyname>Carbonell</keyname><forenames>Jaime G.</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Graph-Structured Multi-task Regression and an Efficient Optimization
  Method for General Fused Lasso</title><categories>stat.ML cs.LG math.OC</categories><comments>21 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning a structured multi-task regression, where
the output consists of multiple responses that are related by a graph and the
correlated response variables are dependent on the common inputs in a sparse
but synergistic manner. Previous methods such as l1/l2-regularized multi-task
regression assume that all of the output variables are equally related to the
inputs, although in many real-world problems, outputs are related in a complex
manner. In this paper, we propose graph-guided fused lasso (GFlasso) for
structured multi-task regression that exploits the graph structure over the
output variables. We introduce a novel penalty function based on fusion penalty
to encourage highly correlated outputs to share a common set of relevant
inputs. In addition, we propose a simple yet efficient proximal-gradient method
for optimizing GFlasso that can also be applied to any optimization problems
with a convex smooth loss and the general class of fusion penalty defined on
arbitrary graph structures. By exploiting the structure of the non-smooth
''fusion penalty'', our method achieves a faster convergence rate than the
standard first-order method, sub-gradient method, and is significantly more
scalable than the widely adopted second-order cone-programming and
quadratic-programming formulations. In addition, we provide an analysis of the
consistency property of the GFlasso model. Experimental results not only
demonstrate the superiority of GFlasso over the standard lasso but also show
the efficiency and scalability of our proximal-gradient method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3616</identifier>
 <datestamp>2012-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3616</id><created>2010-05-20</created><updated>2012-01-17</updated><authors><author><keyname>Smorodinsky</keyname><forenames>Shakhar</forenames></author></authors><title>Conflict-Free Coloring and its Applications</title><categories>math.CO cs.CG cs.DM cs.DS</categories><comments>35 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $H=(V,E)$ be a hypergraph. A {\em conflict-free} coloring of $H$ is an
assignment of colors to $V$ such that in each hyperedge $e \in E$ there is at
least one uniquely-colored vertex. This notion is an extension of the classical
graph coloring. Such colorings arise in the context of frequency assignment to
cellular antennae, in battery consumption aspects of sensor networks, in RFID
protocols and several other fields, and has been the focus of many recent
research papers. In this paper, we survey this notion and its combinatorial and
algorithmic aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3620</identifier>
 <datestamp>2010-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3620</id><created>2010-05-20</created><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Threshold effects in parameter estimation as phase transitions in
  statistical mechanics</title><categories>cs.IT cond-mat.dis-nn cond-mat.stat-mech math.IT</categories><comments>24 pages, 3 figures, submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Threshold effects in the estimation of parameters of non-linearly modulated,
continuous-time, wide-band waveforms, are examined from a statistical physics
perspective. These threshold effects are shown to be analogous to phase
transitions of certain disordered physical systems in thermal equilibrium. The
main message, in this work, is in demonstrating that this physical point of
view may be insightful for understanding the interactions between two or more
parameters to be estimated, from the aspects of the threshold effect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3632</identifier>
 <datestamp>2010-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3632</id><created>2010-05-20</created><authors><author><keyname>F&#xfa;ster-Sabater</keyname><forenames>Amparo</forenames></author></authors><title>A Joint Criterion for Reachability and Observability of Nonuniformly
  Sampled Discrete Systems</title><categories>math.DS cs.DM</categories><comments>8 pages, 1 figure</comments><msc-class>93C55, 93B05, 93B07</msc-class><acm-class>F.1.1</acm-class><journal-ref>IEEE Transactions on Automatic Control. Volume 36, No. 11, pp.
  1281-1284. Nov. 1991</journal-ref><doi>10.1109/9.100938</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A joint characterization of reachability (controllability) and observability
(constructibility) for linear SISO nonuniformly sampled discrete systems is
presented. The work generalizes to the nonuniform sampling the criterion known
for the uniform sampling. Emphasis is on the nonuniform sampling sequence,
which is believed to be an additional element for analysis and handling of
discrete systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3658</identifier>
 <datestamp>2011-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3658</id><created>2010-05-20</created><authors><author><keyname>Serrour</keyname><forenames>Belkacem</forenames></author><author><keyname>Arenas</keyname><forenames>Alex</forenames></author><author><keyname>Gomez</keyname><forenames>Sergio</forenames></author></authors><title>Detecting communities of triangles in complex networks using spectral
  optimization</title><categories>physics.data-an cs.OH physics.soc-ph</categories><comments>Computer Communications (in press)</comments><journal-ref>Computer Communications 34 (2011) 629-634</journal-ref><doi>10.1016/j.comcom.2010.05.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of the sub-structure of complex networks is of major importance to
relate topology and functionality. Many efforts have been devoted to the
analysis of the modular structure of networks using the quality function known
as modularity. However, generally speaking, the relation between topological
modules and functional groups is still unknown, and depends on the semantic of
the links. Sometimes, we know in advance that many connections are transitive
and, as a consequence, triangles have a specific meaning. Here we propose the
study of the modular structure of networks considering triangles as the
building blocks of modules. The method generalizes the standard modularity and
uses spectral optimization to find its maximum. We compare the partitions
obtained with those resulting from the optimization of the standard modularity
in several real networks. The results show that the information reported by the
analysis of modules of triangles complements the information of the classical
modularity analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3681</identifier>
 <datestamp>2010-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3681</id><created>2010-05-20</created><updated>2010-08-01</updated><authors><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author></authors><title>Learning Kernel-Based Halfspaces with the Zero-One Loss</title><categories>cs.LG</categories><comments>This is a full version of the paper appearing in the 23rd
  International Conference on Learning Theory (COLT 2010). Compared to the
  previous arXiv version, this version contains some small corrections in the
  proof of Lemma 3 and in appendix A</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe and analyze a new algorithm for agnostically learning
kernel-based halfspaces with respect to the \emph{zero-one} loss function.
Unlike most previous formulations which rely on surrogate convex loss functions
(e.g. hinge-loss in SVM and log-loss in logistic regression), we provide finite
time/sample guarantees with respect to the more natural zero-one loss function.
The proposed algorithm can learn kernel-based halfspaces in worst-case time
$\poly(\exp(L\log(L/\epsilon)))$, for $\emph{any}$ distribution, where $L$ is a
Lipschitz constant (which can be thought of as the reciprocal of the margin),
and the learned classifier is worse than the optimal halfspace by at most
$\epsilon$. We also prove a hardness result, showing that under a certain
cryptographic assumption, no algorithm can learn kernel-based halfspaces in
time polynomial in $L$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3729</identifier>
 <datestamp>2010-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3729</id><created>2010-05-20</created><authors><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Compressive Sensing over the Grassmann Manifold: a Unified Geometric
  Framework</title><categories>cs.IT cs.DM math.IT</categories><comments>58 pages,7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $\ell_1$ minimization is often used for finding the sparse solutions of an
under-determined linear system. In this paper we focus on finding sharp
performance bounds on recovering approximately sparse signals using $\ell_1$
minimization, possibly under noisy measurements. While the restricted isometry
property is powerful for the analysis of recovering approximately sparse
signals with noisy measurements, the known bounds on the achievable sparsity
(The &quot;sparsity&quot; in this paper means the size of the set of nonzero or
significant elements in a signal vector.) level can be quite loose. The
neighborly polytope analysis which yields sharp bounds for ideally sparse
signals cannot be readily generalized to approximately sparse signals. Starting
from a necessary and sufficient condition, the &quot;balancedness&quot; property of
linear subspaces, for achieving a certain signal recovery accuracy, we give a
unified \emph{null space Grassmann angle}-based geometric framework for
analyzing the performance of $\ell_1$ minimization. By investigating the
&quot;balancedness&quot; property, this unified framework characterizes sharp
quantitative tradeoffs between the considered sparsity and the recovery
accuracy of the $\ell_{1}$ optimization. As a consequence, this generalizes the
neighborly polytope result for ideally sparse signals. Besides the robustness
in the &quot;strong&quot; sense for \emph{all} sparse signals, we also discuss the
notions of &quot;weak&quot; and &quot;sectional&quot; robustness. Our results concern fundamental
properties of linear subspaces and so may be of independent mathematical
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3730</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3730</id><created>2010-05-20</created><authors><author><keyname>Marquezino</keyname><forenames>F. L.</forenames></author><author><keyname>Portugal</keyname><forenames>R.</forenames></author><author><keyname>Sasse</keyname><forenames>F. D.</forenames></author></authors><title>Obtaining the Quantum Fourier Transform from the Classical FFT with QR
  Decomposition</title><categories>quant-ph cs.CC cs.DS</categories><comments>12 pages, 1 figure (generated within LaTeX). To appear in Journal of
  Computational and Applied Mathematics</comments><msc-class>81-08 (Primary), 65T50, 68W40 (Secondary)</msc-class><journal-ref>Journal of Computational and Applied Mathematics, v. 235, p.
  74-81, 2010</journal-ref><doi>10.1016/j.cam.2010.05.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the detailed process of converting the classical Fourier Transform
algorithm into the quantum one by using QR decomposition. This provides an
example of a technique for building quantum algorithms using classical ones.
The Quantum Fourier Transform is one of the most important quantum subroutines
known at present, used in most algorithms that have exponential speed up
compared to the classical ones. We briefly review Fast Fourier Transform and
then make explicit all the steps that led to the quantum formulation of the
algorithm, generalizing Coppersmith's work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3731</identifier>
 <datestamp>2010-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3731</id><created>2010-05-20</created><authors><author><keyname>Levy</keyname><forenames>Jordi</forenames></author><author><keyname>Villaret</keyname><forenames>Mateu</forenames></author></authors><title>Nominal Unification from a Higher-Order Perspective</title><categories>cs.LO cs.SC</categories><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nominal Logic is a version of first-order logic with equality, name-binding,
renaming via name-swapping and freshness of names. Contrarily to higher-order
logic, bindable names, called atoms, and instantiable variables are considered
as distinct entities. Moreover, atoms are capturable by instantiations,
breaking a fundamental principle of lambda-calculus. Despite these differences,
nominal unification can be seen from a higher-order perspective. From this
view, we show that nominal unification can be reduced to a particular fragment
of higher-order unification problems: Higher-Order Pattern Unification. This
reduction proves that nominal unification can be decided in quadratic
deterministic time, using the linear algorithm for Higher-Order Pattern
Unification. We also prove that the translation preserves most generality of
unifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3773</identifier>
 <datestamp>2010-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3773</id><created>2010-05-20</created><authors><author><keyname>Wang</keyname><forenames>Guozhang</forenames></author><author><keyname>Salles</keyname><forenames>Marcos Vaz</forenames></author><author><keyname>Sowell</keyname><forenames>Benjamin</forenames></author><author><keyname>Wang</keyname><forenames>Xun</forenames></author><author><keyname>Cao</keyname><forenames>Tuan</forenames></author><author><keyname>Demers</keyname><forenames>Alan</forenames></author><author><keyname>Gehrke</keyname><forenames>Johannes</forenames></author><author><keyname>White</keyname><forenames>Walker</forenames></author></authors><title>Behavioral Simulations in MapReduce</title><categories>cs.DB cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many scientific domains, researchers are turning to large-scale behavioral
simulations to better understand important real-world phenomena. While there
has been a great deal of work on simulation tools from the high-performance
computing community, behavioral simulations remain challenging to program and
automatically scale in parallel environments. In this paper we present BRACE
(Big Red Agent-based Computation Engine), which extends the MapReduce framework
to process these simulations efficiently across a cluster. We can leverage
spatial locality to treat behavioral simulations as iterated spatial joins and
greatly reduce the communication between nodes. In our experiments we achieve
nearly linear scale-up on several realistic simulations.
  Though processing behavioral simulations in parallel as iterated spatial
joins can be very efficient, it can be much simpler for the domain scientists
to program the behavior of a single agent. Furthermore, many simulations
include a considerable amount of complex computation and message passing
between agents, which makes it important to optimize the performance of a
single node and the communication across nodes. To address both of these
challenges, BRACE includes a high-level language called BRASIL (the Big Red
Agent SImulation Language). BRASIL has object oriented features for programming
simulations, but can be compiled to a data-flow representation for automatic
parallelization and optimization. We show that by using various optimization
techniques, we can achieve both scalability and single-node performance similar
to that of a hand-coded simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3818</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3818</id><created>2010-05-20</created><updated>2012-06-25</updated><authors><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author><author><keyname>Hsieh</keyname><forenames>Min-Hsiu</forenames></author></authors><title>Public and private resource trade-offs for a quantum channel</title><categories>quant-ph cs.IT math.IT</categories><comments>26 pages, 5 figures; v2 has minor corrections; v3 has correction
  regarding the optimization of the region. arXiv admin note: substantial text
  overlap with arXiv:1004.0458</comments><journal-ref>Quantum Information Processing, vol. 11, no. 6, pp. 1465-1501
  (2012)</journal-ref><doi>10.1007/s11128-011-0317-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collins and Popescu realized a powerful analogy between several resources in
classical and quantum information theory. The Collins-Popescu analogy states
that public classical communication, private classical communication, and
secret key interact with one another somewhat similarly to the way that
classical communication, quantum communication, and entanglement interact. This
paper discusses the information-theoretic treatment of this analogy for the
case of noisy quantum channels. We determine a capacity region for a quantum
channel interacting with the noiseless resources of public classical
communication, private classical communication, and secret key. We then compare
this region with the classical-quantum-entanglement region from our prior
efforts and explicitly observe the information-theoretic consequences of the
strong correlations in entanglement and the lack of a super-dense coding
protocol in the public-private-secret-key setting. The region simplifies for
several realistic, physically-motivated channels such as entanglement-breaking
channels, Hadamard channels, and quantum erasure channels, and we are able to
compute and plot the region for several examples of these channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3835</identifier>
 <datestamp>2011-08-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3835</id><created>2010-05-20</created><updated>2011-08-01</updated><authors><author><keyname>Li</keyname><forenames>Fei</forenames></author></authors><title>A Better Memoryless Online Algorithm for FIFO Buffering Packets with Two
  Values</title><categories>cs.DS</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider scheduling packets with values in a capacity-bounded buffer in an
online setting. In this model, there is a buffer with limited capacity $B$. At
any time, the buffer cannot accommodate more than $B$ packets. Packets arrive
over time. Each packet is associated with a non-negative value. Packets leave
the buffer only because they are either sent or dropped. Those packets that
have left the buffer will not be reconsidered for delivery any more. In each
time step, at most one packet in the buffer can be sent. The order in which the
packets are sent should comply with the order of their arrival time. The
objective is to maximize the total value of the packets sent in an online
manner. In this paper, we study a variant of this FIFO buffering model in which
a packet's value is either 1 or $\alpha &gt; 1$. We present a deterministic
memoryless 1.304-competitive algorithm. This algorithm has the same competitive
ratio as the one presented in (Lotker and Patt-Shamir. PODC 2002, Computer
Networks 2003). However, our algorithm is simpler and does not employ any
marking bits. The idea used in our algorithm is novel and different from all
previous approaches applied for the general model and its variants. We do not
proactively preempt one packet when a new packet arrives. Instead, we may
preempt more than one 1-value packet when the buffer contains sufficiently many
$\alpha$-value packets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3873</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3873</id><created>2010-05-20</created><authors><author><keyname>Yang</keyname><forenames>Ruiming</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>Yang</keyname><forenames>Wanlin</forenames></author></authors><title>Improved OMP Approach to Sparse Multi-path Channel Estimation via
  Adaptive Inter-atom Interference Mitigation</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures, accepted by VTC2010-SPRING</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Since most components of sparse multi-path channel (SMPC) are zero, impulse
response of SMPC can be recovered from a short training sequence. Though the
ordinary orthogonal matching pursuit (OMP) algorithm provides a very fast
implementation of SMPC estimation, it suffers from inter-atom interference
(IAI), especially in the case of SMPC with a large delay spread and short
training sequence. In this paper, an adaptive IAI mitigation method is proposed
to improve the performance of SMPC estimation based on a general OMP algorithm.
Unlike the ordinary OMP algorithm, a sensing dictionary is designed adaptively
and posterior information is utilized efficiently to prevent false atoms from
being selected due to serious IAI. Numeral experiments illustrate that the
proposed general OMP algorithm based on adaptive IAI mitigation outperform both
the ordinary OMP algorithm and the general OMP algorithm based on non-adaptive
IAI mitigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3889</identifier>
 <datestamp>2011-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3889</id><created>2010-05-21</created><updated>2011-05-16</updated><authors><author><keyname>Ikeda</keyname><forenames>Shiro</forenames></author><author><keyname>Hayashi</keyname><forenames>Kazunori</forenames></author><author><keyname>Tanaka</keyname><forenames>Toshiyuki</forenames></author></authors><title>Capacity and Modulations with Peak Power Constraint</title><categories>cs.IT math.IT</categories><comments>9 pages with 12 figures. Preparing for submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A practical communication channel often suffers from constraints on input
other than the average power, such as the peak power constraint. In order to
compare achievable rates with different constellations as well as the channel
capacity under such constraints, it is crucial to take these constraints into
consideration properly. In this paper, we propose a direct approach to compare
the achievable rates of practical input constellations and the capacity under
such constraints. As an example, we study the discrete-time complex-valued
additive white Gaussian noise (AWGN) channel and compare the capacity under the
peak power constraint with the achievable rates of phase shift keying (PSK) and
quadrature amplitude modulation (QAM) input constellations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3902</identifier>
 <datestamp>2010-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3902</id><created>2010-05-21</created><authors><author><keyname>Hathout</keyname><forenames>Nabil</forenames><affiliation>CLLE</affiliation></author></authors><title>Morphonette: a morphological network of French</title><categories>cs.CL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes in details the first version of Morphonette, a new
French morphological resource and a new radically lexeme-based method of
morphological analysis. This research is grounded in a paradigmatic conception
of derivational morphology where the morphological structure is a structure of
the entire lexicon and not one of the individual words it contains. The
discovery of this structure relies on a measure of morphological similarity
between words, on formal analogy and on the properties of two morphological
paradigms:
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3918</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3918</id><created>2010-05-21</created><authors><author><keyname>Depardon</keyname><forenames>Benjamin</forenames></author><author><keyname>Caron</keyname><forenames>Eddy</forenames></author><author><keyname>Desprez</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Blaizot</keyname><forenames>J&#xe9;r&#xe9;my</forenames></author><author><keyname>Courtois</keyname><forenames>H&#xe9;l&#xe8;ne M.</forenames></author></authors><title>Cosmological Simulations on a Grid of Computers</title><categories>astro-ph.IM astro-ph.CO cs.DC</categories><comments>Accepted and Published in AIP Conference Proceedings 1241, 2010,
  pages 816-825</comments><journal-ref>AIP Conference Proceedings , 2010, 1241, pages 816-825</journal-ref><doi>10.1063/1.3462722</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work presented in this paper aims at restricting the input parameter
values of the semi-analytical model used in GALICS and MOMAF, so as to derive
which parameters influence the most the results, e.g., star formation, feedback
and halo recycling efficiencies, etc. Our approach is to proceed empirically:
we run lots of simulations and derive the correct ranges of values. The
computation time needed is so large, that we need to run on a grid of
computers. Hence, we model GALICS and MOMAF execution time and output files
size, and run the simulation using a grid middleware: DIET. All the complexity
of accessing resources, scheduling simulations and managing data is harnessed
by DIET and hidden behind a web portal accessible to the users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3968</identifier>
 <datestamp>2010-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3968</id><created>2010-05-21</created><updated>2010-05-31</updated><authors><author><keyname>Santos</keyname><forenames>G. O.</forenames></author><author><keyname>Assis</keyname><forenames>F. M.</forenames></author><author><keyname>Lima</keyname><forenames>A. F.</forenames></author></authors><title>A Scheme of Concatenated Quantum Code to Protect against both
  Computational Error and an Erasure</title><categories>cs.IT math.IT quant-ph</categories><comments>21 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a description of encoding/decoding for a concatenated quantum code
that enables both protection against quantum computational errors and the
occurrence of one quantum erasure. For this, it is presented how encoding and
decoding for quantum graph codes are done, which will provide the protection
against the occurrence of computational errors (external code). As internal
code is used encoding and decoding via scheme of GHZ states for protection
against the occurrence of one quantum erasure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3986</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3986</id><created>2010-05-21</created><authors><author><keyname>McKinley</keyname><forenames>Richard</forenames></author></authors><title>Proof nets for Herbrand's Theorem</title><categories>math.LO cs.LO</categories><comments>40 pages</comments><msc-class>03F05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the connection between two central results in the proof
theory of classical logic: Gentzen's cut-elimination for the sequent calculus
and Herbrands &quot;fundamental theorem&quot;. Starting from Miller's
expansion-tree-proofs, a highly structured way presentation of Herbrand's
theorem, we define a calculus of weakening-free proof nets for (prenex)
first-order classical logic, and give a weakly-normalizing cut-elimination
procedure. It is not possible to formulate the usual counterexamples to
confluence of cut-elimination in this calculus, but it is nonetheless
nonconfluent, lending credence to the view that classical logic is inherently
nonconfluent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.3992</identifier>
 <datestamp>2010-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.3992</id><created>2010-05-19</created><updated>2010-06-26</updated><authors><author><keyname>Malesevic</keyname><forenames>Branko J.</forenames></author><author><keyname>Jovovic</keyname><forenames>Ivana V.</forenames></author><author><keyname>Campara</keyname><forenames>Milan Z.</forenames></author></authors><title>Groebner bases in Java with applications in computer graphics</title><categories>cs.MS cs.GR math.MG math.RA</categories><comments>International convention on Descriptive Geometry and Engineering
  Graphics moNGeometrija 2010, http://www.mongeometrija.org/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a Java implementation of the algorithm that computes
Buchbereger's and reduced Groebner's basis step by step. The Java application
enables graphical representation of the intersection of two surfaces in
3-dimensional space and determines conditions of existence and planarity of the
intersection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4005</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4005</id><created>2010-05-21</created><authors><author><keyname>Bahich</keyname><forenames>Mustapha</forenames></author><author><keyname>Afifi</keyname><forenames>Mohamed</forenames></author><author><keyname>Barj</keyname><forenames>Elmostafa</forenames></author></authors><title>Optical phase extraction algorithm based on the continuous wavelet and
  the Hilbert transforms</title><categories>cs.CE</categories><comments>www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an algorithm for optical phase evaluation based on
the wavelet transform technique. The main advantage of this method is that it
requires only one fringe pattern. This algorithm is based on the use of a
second {\pi}/2 phase shifted fringe pattern where it is calculated via the
Hilbert transform. To test its validity, the algorithm was used to demodulate a
simulated fringe pattern giving the phase distribution with a good accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4006</identifier>
 <datestamp>2013-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4006</id><created>2010-05-21</created><updated>2010-06-19</updated><authors><author><keyname>Dunlavy</keyname><forenames>Daniel M.</forenames></author><author><keyname>Kolda</keyname><forenames>Tamara G.</forenames></author><author><keyname>Acar</keyname><forenames>Evrim</forenames></author></authors><title>Temporal Link Prediction using Matrix and Tensor Factorizations</title><categories>math.NA cs.NA physics.data-an stat.ML</categories><journal-ref>ACM Transactions on Knowledge Discovery from Data 5(2):10 (27
  pages), February 2011</journal-ref><doi>10.1145/1921632.1921636</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The data in many disciplines such as social networks, web analysis, etc. is
link-based, and the link structure can be exploited for many different data
mining tasks. In this paper, we consider the problem of temporal link
prediction: Given link data for times 1 through T, can we predict the links at
time T+1? If our data has underlying periodic structure, can we predict out
even further in time, i.e., links at time T+2, T+3, etc.? In this paper, we
consider bipartite graphs that evolve over time and consider matrix- and
tensor-based methods for predicting future links. We present a weight-based
method for collapsing multi-year data into a single matrix. We show how the
well-known Katz method for link prediction can be extended to bipartite graphs
and, moreover, approximated in a scalable way using a truncated singular value
decomposition. Using a CANDECOMP/PARAFAC tensor decomposition of the data, we
illustrate the usefulness of exploiting the natural three-dimensional structure
of temporal link data. Through several numerical experiments, we demonstrate
that both matrix- and tensor-based techniques are effective for temporal link
prediction despite the inherent difficulty of the problem. Additionally, we
show that tensor-based techniques are particularly effective for temporal data
with varying periodic patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4008</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4008</id><created>2010-05-21</created><authors><author><keyname>De Giusti</keyname><forenames>Marisa R.</forenames></author><author><keyname>Villarreal</keyname><forenames>Gonzalo L.</forenames></author><author><keyname>Vosou</keyname><forenames>Agust&#xed;n</forenames></author><author><keyname>Mart&#xed;nez</keyname><forenames>Juan P.</forenames></author></authors><title>An Ontology-based Context Aware System for Selective Dissemination of
  Information in a Digital Library</title><categories>cs.DL</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Users of Institutional Repositories and Digital Libraries are known by their
needs for very specific information about one or more subjects. To characterize
users profiles and offer them new documents and resources is one of the main
challenges of today's libraries. In this paper, a Selective Dissemination of
Information service is described, which proposes an Ontology-based Context
Aware system for identifying user's context (research subjects, work team,
areas of interest). This system enables librarians to broaden users profiles
beyond the information that users have introduced by hand (such as institution,
age and language). The system requires a context retrieval layer to capture
user information and behavior, and an inference engine to support context
inference from many information sources (selected documents and users'
queries).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4009</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4009</id><created>2010-05-21</created><authors><author><keyname>JebaJothi</keyname><forenames>E. Jenefa</forenames></author><author><keyname>Kavitha</keyname><forenames>V.</forenames></author><author><keyname>Kavitha</keyname><forenames>T.</forenames></author></authors><title>Contention Based Routing in Mobile Ad Hoc Networks with Multiple Copies</title><categories>cs.NI</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Routing the packets efficiently in mobile ad hoc network does not have end to
end paths. Multiple copies are forwarded from the source to the destination. To
deal with such networks, researches introduced flooding based routing schemes
which leads to high probability of delivery. But the flooding based routing
schemes suffered with contention and large delays. Here the proposed protocol
&quot;Spray Select Focus&quot;, sprays a few message copies into the network, neighbors
receives a copy and by that relay nodes we are choosing the shortest route and
then route that copy towards the destination. Previous works assumption is that
there is no contention and dead ends. But we argue that contention and dead
ends must be considered for finding efficiency in routing. So we are including
a network which has contention and dead ends and we applied the proposed
protocol. We can say that this protocol works well for the contention based
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4010</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4010</id><created>2010-05-21</created><updated>2010-06-04</updated><authors><author><keyname>Kamboj</keyname><forenames>Pariza</forenames></author><author><keyname>Sharma</keyname><forenames>A. K.</forenames></author></authors><title>Scalable Energy Efficient Location Aware Multicast Protocol for MANET
  (SEELAMP)</title><categories>cs.NI</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multicast plays an important role in implementing the group communications in
bandwidth scarce multihop mobile ad hoc networks. However, due to the dynamic
topology of MANETs it is very difficult to build optimal multicast trees and
maintaining group membership, making even more challenging to implement
scalable and robust multicast in Mobile Ad hoc Networks (MANET). A scalable and
energy efficient location aware multicast algorithm, called SEELAMP, for mobile
ad hoc networks is presented in the paper that is based on creation of shared
tree using the physical location of the nodes for the multicast sessions. It
constructs a shared bi-directional multicast tree for its routing operations
rather than a mesh, which helps in achieving more efficient multicast delivery.
The algorithm uses the concept of small overlapped zones around each node for
proactive topology maintenance with in the zone. Protocol depends on the
location information obtained using a distributed location service, which
effectively reduces the overheads for route searching and shared multicast tree
maintenance. In this paper a new technique of local connectivity management is
being proposed that attempts to improve the performance and reliability. It
employs a preventive route reconfiguration to avoid the latency in case of link
breakages and to prevent the network from splitting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4012</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4012</id><created>2010-05-21</created><authors><author><keyname>Aboud</keyname><forenames>Sattar J.</forenames></author></authors><title>Criticism of Knapsack Encryption Scheme</title><categories>cs.CR</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze a knapsack schemes. The one is suggested by Su,
which is relied on a new method entitled permutation combination method. We
demonstrate that this permutation method is useless to the security of the
scheme. Since the special super increasing construction, we can break this
scheme employ the algorithm provided by Shamir scheme. Finally, we provide an
enhanced version of Su scheme to avoid these attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4013</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4013</id><created>2010-05-21</created><authors><author><keyname>Halawani</keyname><forenames>Sami</forenames></author><author><keyname>Khan</keyname><forenames>Abdul Waheed</forenames></author></authors><title>Sensors Lifetime Enhancement Techniques in Wireless Sensor Networks - A
  Survey</title><categories>cs.NI</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks are basically used for gathering information needed
by smart environments but they are particularly useful in unattended situations
where terrain, climate and other environmental constraints may hinder in the
deployment of wired/conventional networks. Unlike traditional networks, these
sensor networks do not have a continuous power supply at their disposal. Rather
the individual sensors are battery operated and the lifetime of the individual
sensors and thus the overall network depend heavily on duty cycle of these
sensors. Analysis on WSNs shows that communication module is the main part
which consumes most of the sensor energy and that is why energy conservation is
the major optimization goal. Since routing protocols and MAC protocols directly
access the communication module therefore the design of protocols in these two
domains should take into account the energy conservation goal. In this paper,
we discuss different state-of-the-art protocols both in MAC and routing domains
that have been proposed for WSNs to achieve the overall goal of prolonging the
network lifetime. The routing protocols in WSNs are generally categorized into
three groups - data centric, hierarchical and location-based but we focus on
only the first two categories because location-based routing protocols
generally require a prior knowledge about sensors location which most of the
times is not available due to random deployment of the sensors. We then discuss
how schedule-based and contention-based MAC protocols can contribute to achieve
optimal utilization of the limited energy resource by avoiding or reducing the
chances of collisions and thus the need for retransmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4014</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4014</id><created>2010-05-21</created><authors><author><keyname>Siang</keyname><forenames>Gilbert Phuah Leong</forenames></author><author><keyname>Ismail</keyname><forenames>Nor Azman</forenames></author><author><keyname>Yong</keyname><forenames>Pang Yee</forenames></author></authors><title>A Study on Potential of Integrating Multimodal Interaction into Musical
  Conducting Education</title><categories>cs.MM</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development of computer technology, computer music has begun
to appear in the laboratory. Many potential utility of computer music is
gradually increasing. The purpose of this paper is attempted to analyze the
possibility of integrating multimodal interaction such as vision-based hand
gesture and speech interaction into musical conducting education. To achieve
this purpose, this paper is focus on discuss some related research and the
traditional musical conducting education. To do so, six musical conductors had
been interviewed to share their musical conducting learning/ teaching
experience. These interviews had been analyzed in this paper to show the
syllabus and the focus of musical conducting education for beginners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4015</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4015</id><created>2010-05-21</created><authors><author><keyname>Omara</keyname><forenames>A. N.</forenames></author><author><keyname>El-Kader</keyname><forenames>Sherine M. Abd</forenames></author><author><keyname>Eissa</keyname><forenames>Hussein S.</forenames></author><author><keyname>El-Ramly</keyname><forenames>S.</forenames></author></authors><title>A Prioritized Access Point Algorithm for 802.11b Networks in a Lossy
  Environment</title><categories>cs.NI</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, WLAN technology has been gaining popularity around the world
with its sub standard 802.11b receiving major deployments in many indoor and
outdoor environments. In this article we investigate the performance of IEEE
802.11b infrastructure networks in the lossless and lossy environments by means
of a simulation study. Also, this study shows how the FIFO discipline of the
802.11b MAC affects on the global performance when at least one channel is
under the influence of the bursty errors. Furthermore, this paper proposes a
channel aware backoff algorithm for the Access Point (AP) to prioritize its
transmissions and to accelerate the transmissions in the poor radio channels to
enhance the performance of the real time applications. The final results of
this simulation study showed that the proposed algorithm is able to enhance the
throughput and the delay in lossy environment by an average of 49% and 83%
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4016</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4016</id><created>2010-05-21</created><authors><author><keyname>I</keyname><forenames>Bostan</forenames></author></authors><title>Democracy, essential element of the electronic government</title><categories>cs.CY</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper emphasizes a determinant aim of identifying different approaches,
as comparing to the education and democracy ways specific to e-government
system. Introducing the information technology should offer the possibility by
which reform processes of the government should become more efficient,
transparent and much more public for the citizens; in this way, their ability
of participating directly to government activities should prove the carrying
out of a democratic and free frame. One of the essential issues of such
phenomenon is that of proving that adopting the information and communication
technology programs to government process or electronic government depends upon
a series of external factors, such as the level of state's development, the
cultural level, the frame of developing the structures of central and local
public authority, criteria that differentiate the applicability of such system,
to various countries. This difference is especially seen as comparing to the
East states of European Union. Information systems can be applied in order to
allow the citizens to monitor and coordinate the providing of local services;
such exchanges have created trust and the feeling of influence, encouraging the
participation to political life. Carrying into effect the new informational
technologies, aiming to issuing, informing and to participation of citizens to
political life, will model the concept of democracy within a new frame.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4017</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4017</id><created>2010-05-21</created><authors><author><keyname>Dinakaran</keyname><forenames>M.</forenames></author><author><keyname>Balasubramanie</keyname><forenames>P.</forenames></author></authors><title>A Route Optimization technique for registered and unregistered CN's in
  NEMO</title><categories>cs.NI</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the demand of, requesting the Internet without any disturbance by the
mobile users of any network is increasing the IETF started working on Network
Mobility (NEMO). Maintaining the session of all the nodes in mobile network
with its home network and external nodes can be provided by the basic Network
Mobility support protocol. It provides mobility at IP level to complete
networks, allowing a Mobile Network to change its point of attachment to the
Internet, while maintaining the ongoing sessions of the nodes of the network.
The Mobile Router (MR) manages the mobility even though the nodes don't know
the status of mobility. This article discusses few basic concepts and
limitations of NEMO protocol and proposes two ways to optimize the NEMO routing
technique for registered and unregistered Correspondent Nodes (CN) of the
Mobile Network Node (MNN).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4018</identifier>
 <datestamp>2010-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4018</id><created>2010-05-21</created><updated>2010-09-20</updated><authors><author><keyname>Viezens</keyname><forenames>Fred</forenames></author></authors><title>A grid environment consisting of heterogeneous compute resources for
  high performance computation</title><categories>cs.NI</categories><comments>Author requested a withdrawal</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4020</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4020</id><created>2010-05-21</created><authors><author><keyname>Al-amri</keyname><forenames>Salem Saleh</forenames></author><author><keyname>Kalyankar</keyname><forenames>N. V.</forenames></author><author><keyname>D.</keyname><forenames>Khamitkar S.</forenames></author></authors><title>Image Segmentation by Using Threshold Techniques</title><categories>cs.CV</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper attempts to undertake the study of segmentation image techniques
by using five threshold methods as Mean method, P-tile method, Histogram
Dependent Technique (HDT), Edge Maximization Technique (EMT) and visual
Technique and they are compared with one another so as to choose the best
technique for threshold segmentation techniques image. These techniques applied
on three satellite images to choose base guesses for threshold segmentation
image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4021</identifier>
 <datestamp>2010-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4021</id><created>2010-05-21</created><updated>2010-07-25</updated><authors><author><keyname>Reddy</keyname><forenames>P. V. G. D. Prasad</forenames></author><author><keyname>Sudha</keyname><forenames>K. R.</forenames></author><author><keyname>Sree</keyname><forenames>P. Rama</forenames></author><author><keyname>Ramesh</keyname><forenames>S. N. S. V. S. C.</forenames></author></authors><title>Software Effort Estimation using Radial Basis and Generalized Regression
  Neural Networks</title><categories>cs.SE</categories><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software development effort estimation is one of the most major activities in
software project management. A number of models have been proposed to construct
a relationship between software size and effort; however we still have problems
for effort estimation. This is because project data, available in the initial
stages of project is often incomplete, inconsistent, uncertain and unclear. The
need for accurate effort estimation in software industry is still a challenge.
Artificial Neural Network models are more suitable in such situations. The
present paper is concerned with developing software effort estimation models
based on artificial neural networks. The models are designed to improve the
performance of the network that suits to the COCOMO Model. Artificial Neural
Network models are created using Radial Basis and Generalized Regression. A
case study based on the COCOMO81 database compares the proposed neural network
models with the Intermediate COCOMO. The results were analyzed using five
different criterions MMRE, MARE, VARE, Mean BRE and Prediction. It is observed
that the Radial Basis Neural Network provided better results
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4022</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4022</id><created>2010-05-21</created><authors><author><keyname>Pradhan</keyname><forenames>Manas Ranjan</forenames></author><author><keyname>Rajan</keyname><forenames>E. G.</forenames></author></authors><title>Molecular Programming Pseudo-code Representation to Molecular
  Electronics</title><categories>cs.PL</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research paper is proposing the idea of pseudo code representation to
molecular programming used in designing molecular electronics devices. Already
the schematic representation of logical gates like AND, OR, NOT etc.from
molecular diodes or resonant tunneling diode are available. This paper is
setting a generic pseudo code model so that various logic gates can be
formulated. These molecular diodes have designed from organic molecules or
Bio-molecules. Our focus is on to give a scenario of molecular computation
through molecular programming. We have restricted our study to molecular
rectifying diode and logic device as AND gate from organic molecules only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4023</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4023</id><created>2010-05-21</created><authors><author><keyname>Bathla</keyname><forenames>Himani</forenames></author><author><keyname>Lakhani</keyname><forenames>Kanika</forenames></author></authors><title>A Novel Method for Intrusion Detection System to Enhance Security in Ad
  hoc Network</title><categories>cs.CR</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of an ad hoc network is a new paradigm that allows mobile hosts
(nodes) to communicate without relying on a predefined infrastructure to keep
the network connected. Most nodes are assumed to be mobile and communication is
assumed to be wireless. The mobility of nodes in an ad-hoc network means that
both the population and the topology of the network are highly dynamic. It is
very difficult to design a once-for-all intrusion detection system. A secure
protocol should atleast include mechanisms against known attack types. In
addition, it should provide a scheme to easily add new security features in the
future. The paper includes the detailed description of Proposed Intrusion
Detection System based on Local Reputation Scheme. The proposed System also
includes concept of Redemption and Fading these are mechanism that allow nodes
previously considered malicious to become a part of the network again. The
simulation of the proposed system is to be done using NS-2 simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4025</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4025</id><created>2010-05-21</created><authors><author><keyname>Biswas</keyname><forenames>Siddharths Sankar</forenames></author></authors><title>A Soft Computing Model for Physicians' Decision Process</title><categories>cs.AI</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the author presents a kind of Soft Computing Technique, mainly
an application of fuzzy set theory of Prof. Zadeh [16], on a problem of Medical
Experts Systems. The choosen problem is on design of a physician's decision
model which can take crisp as well as fuzzy data as input, unlike the
traditional models. The author presents a mathematical model based on fuzzy set
theory for physician aided evaluation of a complete representation of
information emanating from the initial interview including patient past
history, present symptoms, and signs observed upon physical examination and
results of clinical and diagnostic tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4026</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4026</id><created>2010-05-21</created><authors><author><keyname>Hmood</keyname><forenames>Ali K.</forenames></author><author><keyname>Zaidan</keyname><forenames>M. A.</forenames></author><author><keyname>Alanazi</keyname><forenames>Hamdan. O.</forenames></author><author><keyname>Alnaqeib</keyname><forenames>Rami</forenames></author><author><keyname>Al-Nabhani</keyname><forenames>Yahya</forenames></author></authors><title>Dissertations Repository System Using Context Module</title><categories>cs.DL</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Without a doubt, the electronic learning makes education quite flexible.
Nowadays, all organizations and institutions are trying to avoid Monotony and
the delay and inertia. As well the universities should be improving their
systems continually to achieve success. Whereas, the students need to access
the dissertations in the library. In this paper we will present Dissertations
Repository System Using Context Module to allow the students to benefit the
dissertations which is in the library flexibly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4028</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4028</id><created>2010-05-21</created><authors><author><keyname>Alnaqeib</keyname><forenames>Rami</forenames></author><author><keyname>Alanazi</keyname><forenames>Hamdan. O.</forenames></author><author><keyname>Jalab</keyname><forenames>Hamid. A.</forenames></author><author><keyname>Zaidan</keyname><forenames>M. A.</forenames></author><author><keyname>Hmood</keyname><forenames>Ali K.</forenames></author></authors><title>Internet Banking System Prototype</title><categories>cs.HC</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet Banking System refers to systems that enable bank customers to
access accounts and general information on bank products and services through a
personal computer or other intelligent device. Internet banking products and
services can include detailed account information for corporate customers as
well as account summery and transfer money. Ultimately, the products and
services obtained through Internet Banking may mirror products and services
offered through other bank delivery channels. In this paper, Internet Banking
System Prototype has been proposed in order to illustrate the services which is
provided by the Bank online services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4029</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4029</id><created>2010-05-21</created><authors><author><keyname>Alanazi</keyname><forenames>Hamdan. O.</forenames></author><author><keyname>Alnaqeib</keyname><forenames>Rami</forenames></author><author><keyname>Hmood</keyname><forenames>Ali K.</forenames></author><author><keyname>Zaidan</keyname><forenames>M. A.</forenames></author><author><keyname>Al-Nabhani</keyname><forenames>Yahya</forenames></author></authors><title>On the Module of Internet Banking System</title><categories>cs.OH</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Because of the speed, flexibility, and efficiency that it offers, the
Internet has become the means for conducting growing numbers of transactions
between suppliers and large international corporations. In this way, the
Internet has opened new markets to the world and has accelerated the diffusion
of knowledge. The meaning of Internet markets or online business has been
widely used in these days. The success of the business depends on its
flexibility, availability and security. Since that the web-based systems should
have a special way to design the system and implement it. Nowadays, the
Internet Banking System widely used and the banks looking to provide the best
quality system with highly available, fast response, secure and safe to use.
The Unified Modelling Language (UML) is the uniquely language which is used to
analyse and design any system. In this paper, the UML diagrams has been
proposed to illustrate the design phase for any banking system. The authors,
presented two types of architecture which is used for the Internet Banking
System.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4030</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4030</id><created>2010-05-21</created><authors><author><keyname>Sharma</keyname><forenames>Monika</forenames></author><author><keyname>Mehra</keyname><forenames>Ashwani</forenames></author><author><keyname>Jola</keyname><forenames>Haresh</forenames></author><author><keyname>Kumar</keyname><forenames>Anand</forenames></author><author><keyname>Misra</keyname><forenames>Madhvendra</forenames></author><author><keyname>Tiwari</keyname><forenames>Vijayshri</forenames></author></authors><title>Scope of cloud computing for SMEs in India</title><categories>cs.CY</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is a set of services that provide infrastructure resources
using internet media and data storage on a third party server. SMEs are said to
be the lifeblood of any vibrant economy. They are known to be the silent
drivers of a nation's economy. SMEs of India are one of the most aggressive
adopters of ERP Packages. Most of the Indian SMEs have adopted the traditional
ERP Systems and have incurred a heavy cost while implementing these systems.
This paper presents the cost savings and reduction in the level of difficulty
in adopting a cloud computing Service (CCS) enabled ERP system. For the study,
IT people from 30 North Indian SMEs were interviewed. In the cloud computing
environment the SMEs will not have to own the infrastructure so they can
abstain from any capital expenditure and instead they can utilize the resources
as a service and pay as per their usage. We consider the results of the paper
to be supportive to our proposed research concept.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4031</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4031</id><created>2010-05-21</created><authors><author><keyname>Soni</keyname><forenames>Surender</forenames></author><author><keyname>Chand</keyname><forenames>Narottam</forenames></author></authors><title>Energy Efficient Multi-Level Clustering To Prolong The Lifetime of
  Wireless Sensor Networks</title><categories>cs.NI</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering in wireless sensor networks (WSNs) is an important technique to
ease topology management and routing. Clustering provides an effective method
for prolonging lifetime of a WSN. This paper proposes energy efficient
multi-level clustering schemes for wireless sensor networks. Wireless sensor
nodes are extremely energy constrained with a limited transmission range. Due
to large area of deployment, the network needs to have a multi-level clustering
protocol that will enable far-off nodes to communicate with the base station.
Simulation is used to analyze the proposed protocols and compare their
performance with existing protocol EEMC. Simulation results demonstrate that
our proposed protocols are effective in prolonging the network lifetime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4032</identifier>
 <datestamp>2010-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4032</id><created>2010-05-21</created><authors><author><keyname>Arora</keyname><forenames>Sandhya</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Debotosh</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author><author><keyname>Kundu</keyname><forenames>Mahantapas</forenames></author></authors><title>Combining Multiple Feature Extraction Techniques for Handwritten
  Devnagari Character Recognition</title><categories>cs.CV cs.AI</categories><comments>6 pages, 8-10 December 2008</comments><journal-ref>ICIIS 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an OCR for Handwritten Devnagari Characters. Basic
symbols are recognized by neural classifier. We have used four feature
extraction techniques namely, intersection, shadow feature, chain code
histogram and straight line fitting features. Shadow features are computed
globally for character image while intersection features, chain code histogram
features and line fitting features are computed by dividing the character image
into different segments. Weighted majority voting technique is used for
combining the classification decision obtained from four Multi Layer
Perceptron(MLP) based classifier. On experimentation with a dataset of 4900
samples the overall recognition rate observed is 92.80% as we considered top
five choices results. This method is compared with other recent methods for
Handwritten Devnagari Character Recognition and it has been observed that this
approach has better success rate than other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4033</identifier>
 <datestamp>2010-05-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4033</id><created>2010-05-21</created><authors><author><keyname>Andoni</keyname><forenames>Alexandr</forenames></author><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author><author><keyname>Onak</keyname><forenames>Krzysztof</forenames></author></authors><title>Polylogarithmic Approximation for Edit Distance and the Asymmetric Query
  Complexity</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a near-linear time algorithm that approximates the edit distance
between two strings within a polylogarithmic factor; specifically, for strings
of length n and every fixed epsilon&gt;0, it can compute a (log n)^O(1/epsilon)
approximation in n^(1+epsilon) time. This is an exponential improvement over
the previously known factor, 2^(O (sqrt(log n))), with a comparable running
time (Ostrovsky and Rabani J.ACM 2007; Andoni and Onak STOC 2009). Previously,
no efficient polylogarithmic approximation algorithm was known for any
computational task involving edit distance (e.g., nearest neighbor search or
sketching).
  This result arises naturally in the study of a new asymmetric query model. In
this model, the input consists of two strings x and y, and an algorithm can
access y in an unrestricted manner, while being charged for querying every
symbol of x. Indeed, we obtain our main result by designing an algorithm that
makes a small number of queries in this model. We then provide a
nearly-matching lower bound on the number of queries.
  Our lower bound is the first to expose hardness of edit distance stemming
from the input strings being &quot;repetitive&quot;, which means that many of their
substrings are approximately identical. Consequently, our lower bound provides
the first rigorous separation between edit distance and Ulam distance, which is
edit distance on non-repetitive strings, such as permutations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4034</identifier>
 <datestamp>2010-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4034</id><created>2010-05-21</created><authors><author><keyname>Halder</keyname><forenames>Santanu</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Debotosh</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author><author><keyname>Kundu</keyname><forenames>Mahantapas</forenames></author></authors><title>Face Synthesis (FASY) System for Generation of a Face Image from Human
  Description</title><categories>cs.CV</categories><journal-ref>ICIIS 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at generating a new face based on the human like description
using a new concept. The FASY (FAce SYnthesis) System is a Face Database
Retrieval and new Face generation System that is under development. One of its
main features is the generation of the requested face when it is not found in
the existing database, which allows a continuous growing of the database also.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4035</identifier>
 <datestamp>2010-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4035</id><created>2010-05-21</created><authors><author><keyname>Bhowmik</keyname><forenames>Mrinal Kanti</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Debotosh</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author><author><keyname>Kundu</keyname><forenames>Mahantapas</forenames></author></authors><title>Classification of Polar-Thermal Eigenfaces using Multilayer Perceptron
  for Human Face Recognition</title><categories>cs.CV</categories><journal-ref>ICIIS 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach to handle the challenges of face
recognition. In this work thermal face images are considered, which minimizes
the affect of illumination changes and occlusion due to moustache, beards,
adornments etc. The proposed approach registers the training and testing
thermal face images in polar coordinate, which is capable to handle
complicacies introduced by scaling and rotation. Polar images are projected
into eigenspace and finally classified using a multi-layer perceptron. In the
experiments we have used Object Tracking and Classification Beyond Visible
Spectrum (OTCBVS) database benchmark thermal face images. Experimental results
show that the proposed approach significantly improves the verification and
identification performance and the success rate is 97.05%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4044</identifier>
 <datestamp>2010-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4044</id><created>2010-05-21</created><authors><author><keyname>Bhattacharjee</keyname><forenames>Debotosh</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author><author><keyname>Kundu</keyname><forenames>M.</forenames></author></authors><title>Reduction of Feature Vectors Using Rough Set Theory for Human Face
  Recognition</title><categories>cs.CV</categories><journal-ref>ICCS 2005</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a procedure to reduce the size of the input feature
vector. A complex pattern recognition problem like face recognition involves
huge dimension of input feature vector. To reduce that dimension here we have
used eigenspace projection (also called as Principal Component Analysis), which
is basically transformation of space. To reduce further we have applied feature
selection method to select indispensable features, which will remain in the
final feature vectors. Features those are not selected are removed from the
final feature vector considering them as redundant or superfluous. For
selection of features we have used the concept of reduct and core from rough
set theory. This method has shown very good performance. It is worth to mention
that in some cases the recognition rate increases with the decrease in the
feature vector dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4103</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4103</id><created>2010-05-22</created><authors><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Wang</keyname><forenames>Peng</forenames></author><author><keyname>Li</keyname><forenames>Hanxi</forenames></author></authors><title>LACBoost and FisherBoost: Optimally Building Cascade Classifiers</title><categories>cs.CV</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object detection is one of the key tasks in computer vision. The cascade
framework of Viola and Jones has become the de facto standard. A classifier in
each node of the cascade is required to achieve extremely high detection rates,
instead of low overall classification error. Although there are a few reported
methods addressing this requirement in the context of object detection, there
is no a principled feature selection method that explicitly takes into account
this asymmetric node learning objective. We provide such a boosting algorithm
in this work. It is inspired by the linear asymmetric classifier (LAC) of Wu et
al. in that our boosting algorithm optimizes a similar cost function. The new
totally-corrective boosting algorithm is implemented by the column generation
technique in convex optimization. Experimental results on face detection
suggest that our proposed boosting algorithms can improve the state-of-the-art
methods in detection performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4115</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4115</id><created>2010-05-22</created><authors><author><keyname>Erd&#xe9;lyi</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Piras</keyname><forenames>Lena</forenames></author><author><keyname>Rothe</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Bucklin Voting is Broadly Resistant to Control</title><categories>cs.CC cs.MA</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electoral control models ways of changing the outcome of an election via such
actions as adding/deleting/partitioning either candidates or voters. These
actions modify an election's participation structure and aim at either making a
favorite candidate win (&quot;constructive control&quot;) or prevent a despised candidate
from winning (&quot;destructive control&quot;), which yields a total of 22 standard
control scenarios. To protect elections from such control attempts,
computational complexity has been used to show that electoral control, though
not impossible, is computationally prohibitive. Among natural voting systems
with a polynomial-time winner problem, the two systems with the highest number
of proven resistances to control types (namely 19 out of 22) are
&quot;sincere-strategy preference-based approval voting&quot; (SP-AV, a modification of a
system proposed by Brams and Sanver) and fallback voting. Both are hybrid
systems; e.g., fallback voting combines approval with Bucklin voting. In this
paper, we study the control complexity of Bucklin voting itself and show that
it behaves equally well in terms of control resistance for the 20 cases
investigated so far. As Bucklin voting is a special case of fallback voting,
all resistances shown for Bucklin voting in this paper strengthen the
corresponding resistance for fallback voting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4117</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4117</id><created>2010-05-22</created><authors><author><keyname>Katzgraber</keyname><forenames>Helmut G.</forenames></author></authors><title>Random Numbers in Scientific Computing: An Introduction</title><categories>physics.comp-ph cs.MS cs.NA physics.data-an</categories><comments>lecture at the second international summer school &quot;Modern Computation
  Science&quot;, 9 - 20 August 2010, Oldenburg (Germany), see
  http://www.mcs.uni-oldenburg.de/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random numbers play a crucial role in science and industry. Many numerical
methods require the use of random numbers, in particular the Monte Carlo
method. Therefore it is of paramount importance to have efficient random number
generators. The differences, advantages and disadvantages of true and pseudo
random number generators are discussed with an emphasis on the intrinsic
details of modern and fast pseudo random number generators. Furthermore,
standard tests to verify the quality of the random numbers produced by a given
generator are outlined. Finally, standard scientific libraries with built-in
generators are presented, as well as different approaches to generate
nonuniform random numbers. Potential problems that one might encounter when
using large parallel machines are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4118</identifier>
 <datestamp>2010-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4118</id><created>2010-05-22</created><authors><author><keyname>Paisitkriangkrai</keyname><forenames>Sakrapee</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Zhang</keyname><forenames>Jian</forenames></author></authors><title>Incremental Training of a Detector Using Online Sparse
  Eigen-decomposition</title><categories>cs.CV</categories><comments>14 pages</comments><doi>10.1109/TIP.2010.2053548</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to efficiently and accurately detect objects plays a very crucial
role for many computer vision tasks. Recently, offline object detectors have
shown a tremendous success. However, one major drawback of offline techniques
is that a complete set of training data has to be collected beforehand. In
addition, once learned, an offline detector can not make use of newly arriving
data. To alleviate these drawbacks, online learning has been adopted with the
following objectives: (1) the technique should be computationally and storage
efficient; (2) the updated classifier must maintain its high classification
accuracy. In this paper, we propose an effective and efficient framework for
learning an adaptive online greedy sparse linear discriminant analysis (GSLDA)
model. Unlike many existing online boosting detectors, which usually apply
exponential or logistic loss, our online algorithm makes use of LDA's learning
criterion that not only aims to maximize the class-separation criterion but
also incorporates the asymmetrical property of training data distributions. We
provide a better alternative for online boosting algorithms in the context of
training a visual object detector. We demonstrate the robustness and efficiency
of our methods on handwriting digit and face data sets. Our results confirm
that object detection tasks benefit significantly when trained in an online
manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4122</identifier>
 <datestamp>2010-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4122</id><created>2010-05-22</created><updated>2010-08-08</updated><authors><author><keyname>Prolubnikov</keyname><forenames>A.</forenames></author></authors><title>On a new complete invariant of acyclic graphs</title><categories>cs.CC cs.DM</categories><comments>This paper has been withdrawn by the author due to update</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new complete invariant for acyclic graphs is presented
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4123</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4123</id><created>2010-05-22</created><authors><author><keyname>Soundararajan</keyname><forenames>Shvetha</forenames></author><author><keyname>Arthur</keyname><forenames>James. D.</forenames></author></authors><title>A Structured Framework for Assessing the &quot;Goodness&quot; of Agile Methods</title><categories>cs.SE</categories><comments>4 pages, 3 figures, Research-in-progress paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agile Methods are designed for customization; they offer an organization or a
team the flexibility to adopt a set of principles and practices based on their
culture and values. While that flexibility is consistent with the agile
philosophy, it can lead to the adoption of principles and practices that can be
sub-optimal relative to the desired objectives. We question then, how can one
determine if adopted practices are &quot;in sync&quot; with the identified principles,
and to what extent those principles support organizational objectives? In this
research, we focus on assessing the &quot;goodness&quot; of an agile method adopted by an
organization based on (1) its adequacy, (2) the capability of the organization
to provide the supporting environment to competently implement the method, and
(3) its effectiveness. To guide our assessment, we propose the Objectives,
Principles and Practices (OPP) framework. The design of the OPP framework
revolves around the identification of the agile objectives, principles that
support the achievement of those objectives, and practices that reflect the
&quot;spirit&quot; of those principles. Well-defined linkages between the objectives and
principles, and between the principles and practices are also established to
support the assessment process. We traverse these linkages in a top-down
fashion to assess adequacy and a bottom-up fashion to assess capability and
effectiveness. This is a work-in-progress paper, outlining our proposed
research, preliminary results and future directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4155</identifier>
 <datestamp>2011-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4155</id><created>2010-05-22</created><authors><author><keyname>Solomon</keyname><forenames>Shay</forenames></author></authors><title>An Optimal-Time Construction of Euclidean Sparse Spanners with Tiny
  Diameter</title><categories>cs.CG cs.DS</categories><comments>28 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In STOC'95 \cite{ADMSS95} Arya et al.\ showed that for any set of $n$ points
in $\mathbb R^d$, a $(1+\epsilon)$-spanner with diameter at most 2
(respectively, 3) and $O(n \log n)$ edges (resp., $O(n \log \log n)$ edges) can
be built in $O(n \log n)$ time. Moreover, it was shown in \cite{ADMSS95,NS07}
that for any $k \ge 4$, one can build in $O(n (\log n) 2^k \alpha_k(n))$ time a
$(1+\epsilon)$-spanner with diameter at most $2k$ and $O(n 2^k \alpha_k(n))$
edges. The function $\alpha_k$ is the inverse of a certain function at the
$\lfloor k/2 \rfloor$th level of the primitive recursive hierarchy, where
$\alpha_0(n) = \lceil n/2 \rceil, \alpha_1(n) = \left\lceil \sqrt{n}
\right\rceil, \alpha_2(n) = \lceil \log{n} \rceil, \alpha_3(n) = \lceil
\log\log{n} \rceil, \alpha_4(n) = \log^* n$, \ldots, etc. It is also known
\cite{NS07} that if one allows quadratic time then these bounds can be
improved. Specifically, for any $k \ge 4$, a $(1+\epsilon)$-spanner with
diameter at most $k$ and $O(n k \alpha_k(n))$ edges can be constructed in
$O(n^2)$ time \cite{NS07}.
  A major open problem in this area is whether one can construct within time
$O(n \log n + n k \alpha_k(n))$ a $(1+\epsilon)$-spanner with diameter at most
$k$ and $O(n k \alpha_k(n))$ edges. In this paper we answer this question in
the affirmative. Moreover, in fact, we provide a stronger result. Specifically,
we show that for any $k \ge 4$, a $(1+\epsilon)$-spanner with diameter at most
$k$ and $O(n \alpha_k(n))$ edges can be built in optimal time $O(n \log n)$.
The tradeoff between the diameter and number of edges of our spanners is tight
up to constant factors in the entire range of parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4159</identifier>
 <datestamp>2012-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4159</id><created>2010-05-22</created><updated>2012-04-19</updated><authors><author><keyname>Lin</keyname><forenames>Andrew</forenames></author></authors><title>The Complexity of Manipulating $k$-Approval Elections</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important problem in computational social choice theory is the complexity
of undesirable behavior among agents, such as control, manipulation, and
bribery in election systems. These kinds of voting strategies are often
tempting at the individual level but disastrous for the agents as a whole.
Creating election systems where the determination of such strategies is
difficult is thus an important goal.
  An interesting set of elections is that of scoring protocols. Previous work
in this area has demonstrated the complexity of misuse in cases involving a
fixed number of candidates, and of specific election systems on unbounded
number of candidates such as Borda. In contrast, we take the first step in
generalizing the results of computational complexity of election misuse to
cases of infinitely many scoring protocols on an unbounded number of
candidates. Interesting families of systems include $k$-approval and $k$-veto
elections, in which voters distinguish $k$ candidates from the candidate set.
  Our main result is to partition the problems of these families based on their
complexity. We do so by showing they are polynomial-time computable, NP-hard,
or polynomial-time equivalent to another problem of interest. We also
demonstrate a surprising connection between manipulation in election systems
and some graph theory problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4178</identifier>
 <datestamp>2011-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4178</id><created>2010-05-23</created><updated>2011-01-20</updated><authors><author><keyname>Rashmi</keyname><forenames>K. V.</forenames></author><author><keyname>Shah</keyname><forenames>Nihar B.</forenames></author><author><keyname>Kumar</keyname><forenames>P. Vijay</forenames></author></authors><title>Optimal Exact-Regenerating Codes for Distributed Storage at the MSR and
  MBR Points via a Product-Matrix Construction</title><categories>cs.IT cs.DC cs.NI math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory. Contains 20
  pages, 2 figures</comments><journal-ref>IEEE Transactions on Information Theory, vol. 57, no. 8, pp. 5227
  - 5239, August 2011</journal-ref><doi>10.1109/TIT.2011.2159049</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regenerating codes are a class of distributed storage codes that optimally
trade the bandwidth needed for repair of a failed node with the amount of data
stored per node of the network. Minimum Storage Regenerating (MSR) codes
minimize first, the amount of data stored per node, and then the repair
bandwidth, while Minimum Bandwidth Regenerating (MBR) codes carry out the
minimization in the reverse order. An [n, k, d] regenerating code permits the
data to be recovered by connecting to any k of the n nodes in the network,
while requiring that repair of a failed node be made possible by connecting
(using links of lesser capacity) to any d nodes. Previous, explicit and general
constructions of exact-regenerating codes have been confined to the case n=d+1.
In this paper, we present optimal, explicit constructions of MBR codes for all
feasible values of [n, k, d] and MSR codes for all [n, k, d &gt;= 2k-2], using a
product-matrix framework. The particular product-matrix nature of the
constructions is shown to significantly simplify system operation. To the best
of our knowledge, these are the first constructions of exact-regenerating codes
that allow the number n of nodes in the distributed storage network, to be
chosen independent of the other parameters. The paper also contains a simpler
description, in the product-matrix framework, of a previously constructed MSR
code in which the parameter d satisfies [n=d+1, k, d &gt;= 2k-1].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4200</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4200</id><created>2010-05-23</created><authors><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author><author><keyname>Chu</keyname><forenames>Xiaoli</forenames></author></authors><title>A Robust Beamformer Based on Weighted Sparse Constraint</title><categories>cs.IT math.IT</categories><comments>4 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Applying a sparse constraint on the beam pattern has been suggested to
suppress the sidelobe level of a minimum variance distortionless response
(MVDR) beamformer. In this letter, we introduce a weighted sparse constraint in
the beamformer design to provide a lower sidelobe level and deeper nulls for
interference avoidance, as compared with a conventional MVDR beamformer. The
proposed beamformer also shows improved robustness against the mismatch between
the steering angle and the direction of arrival (DOA) of the desired signal,
caused by imperfect estimation of DOA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4216</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4216</id><created>2010-05-23</created><authors><author><keyname>Babykalpana</keyname><forenames>Y.</forenames></author><author><keyname>ThanushKodi</keyname><forenames>K.</forenames></author></authors><title>Classification of LULC Change Detection using Remotely Sensed Data for
  Coimbatore City, Tamilnadu, India</title><categories>cs.CV</categories><comments>http://www.journalofcomputing.org</comments><journal-ref>Journal of Computing, Volume 2, Issue 5, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maps are used to describe far-off places . It is an aid for navigation and
military strategies. Mapping of the lands are important and the mapping work is
based on (i). Natural resource management &amp; development (ii). Information
technology ,(iii). Environmental development ,(iv). Facility management and
(v). e-governance. The Landuse / Landcover system espoused by almost all
Organisations and scientists, engineers and remote sensing community who are
involved in mapping of earth surface features, is a system which is derived
from the united States Geological Survey (USGS) LULC classification system. The
application of RS and GIS involves influential of homogeneous zones, drift
analysis of land use integration of new area changes or change detection
etc.,National Remote Sensing Agency(NRSA) Govt. of India has devised a
generalized LULC classification system respect to the Indian conditions based
on the various categories of Earth surface features , resolution of available
satellite data, capabilities of sensors and present and future applications.
The profusion information of the earth surface offered by the high resolution
satellite images for remote sensing applications. Using change detection
methodologies to extract the target changes in the areas from high resolution
images and rapidly updates geodatabase information processing.Traditionally,
classification approaches have focused on per-pixel technologies. Pixels within
areas assumed to be automatically homogeneous are analyzed independently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4244</identifier>
 <datestamp>2010-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4244</id><created>2010-05-23</created><updated>2010-12-15</updated><authors><author><keyname>Bei</keyname><forenames>Xiaohui</forenames></author><author><keyname>Huang</keyname><forenames>Zhiyi</forenames></author></authors><title>Bayesian Incentive Compatibility via Fractional Assignments</title><categories>cs.GT</categories><comments>22 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Very recently, Hartline and Lucier studied single-parameter mechanism design
problems in the Bayesian setting. They proposed a black-box reduction that
converted Bayesian approximation algorithms into Bayesian-Incentive-Compatible
(BIC) mechanisms while preserving social welfare. It remains a major open
question if one can find similar reduction in the more important
multi-parameter setting. In this paper, we give positive answer to this
question when the prior distribution has finite and small support. We propose a
black-box reduction for designing BIC multi-parameter mechanisms. The reduction
converts any algorithm into an eps-BIC mechanism with only marginal loss in
social welfare. As a result, for combinatorial auctions with sub-additive
agents we get an eps-BIC mechanism that achieves constant approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4262</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4262</id><created>2010-05-24</created><authors><author><keyname>Chitra</keyname><forenames>K.</forenames></author><author><keyname>Padamavathi</keyname><forenames>G.</forenames></author></authors><title>Classification and Performance of AQM-Based Schemes for Congestion
  Avoidance</title><categories>cs.NI</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Internet faces the problem of congestion due to its increased use. AQM
algorithm is a solution to the problem of congestion control in the Internet.
There are various existing algorithms that have evolved over the past few years
to solve the problem of congestion in IP networks. Congested link causes many
problems such as large delay, underutilization of the link and packet drops in
burst. There are various existing algorithms that have evolved over the past
few years to solve the problem of congestion in IP networks. In this paper,
study of these existing algorithms is done. This paper discusses algorithms
based on various congestion-metrics and classifies them based on certain
factors. This helps in identifying the algorithms that regulate the congestion
more effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4263</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4263</id><created>2010-05-24</created><authors><author><keyname>Thorat</keyname><forenames>S. B.</forenames></author><author><keyname>Nayak</keyname><forenames>S. K.</forenames></author><author><keyname>Dandale</keyname><forenames>Jyoti P</forenames></author></authors><title>Facial Recognition Technology: An analysis with scope in India</title><categories>cs.MA</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A facial recognition system is a computer application for automatically
identifying or verifying a person from a digital image or a video frame from a
video source. One of the way is to do this is by comparing selected facial
features from the image and a facial database.It is typically used in security
systems and can be compared to other biometrics such as fingerprint or eye iris
recognition systems. In this paper we focus on 3-D facial recognition system
and biometric facial recognision system. We do critics on facial recognision
system giving effectiveness and weaknesses. This paper also introduces scope of
recognision system in India.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4264</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4264</id><created>2010-05-24</created><authors><author><keyname>Zehra</keyname><forenames>Najme</forenames></author><author><keyname>Sharma</keyname><forenames>Mansi</forenames></author><author><keyname>Ahuja</keyname><forenames>Somya</forenames></author><author><keyname>Bansal</keyname><forenames>Shubha</forenames></author></authors><title>Bio-Authentication based Secure Transmission System using Steganography</title><categories>cs.CR</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Biometrics deals with identity verification of an individual by using certain
physiological or behavioral features associated with a person. Biometric
identification systems using fingerprints patterns are called AFIS (Automatic
Fingerprint Identification System). In this paper a composite method for
Fingerprint recognition is considered using a combination of Fast Fourier
Transform (FFT) and Sobel Filters for improvement of a poor quality fingerprint
image. Steganography hides messages inside other messages in such a way that an
&quot;adversary&quot; would not even know a secret message were present. The objective of
our paper is to make a bio-secure system. In this paper bio-authentication has
been implemented in terms of finger print recognition and the second part of
the paper is an interactive steganographic system hides the user's data by two
options- creating a songs list or hiding the data in an image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4265</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4265</id><created>2010-05-24</created><authors><author><keyname>Srisailam</keyname><forenames>C.</forenames></author><author><keyname>Tiwari</keyname><forenames>Mukesh</forenames></author><author><keyname>Trivedi</keyname><forenames>Anurag</forenames></author></authors><title>Reduction in iron losses in Indirect Vector-Controlled IM Drive using
  FLC</title><categories>cs.OH</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper describes the use of fuzzy logic controller for efficiency
optimization control of a drive while keeping good dynamic response. At
steady-state light-load condition, the fuzzy controller adaptively adjusts the
excitation current with respect to the torque current to give the minimum total
copper and iron loss. The measured input power such that, for a given load
torque and speed, the drive settles down to the minimum input power, i.e.,
operates at maximum efficiency. The low-frequency pulsating torque due to
decrementation of flux is compensated in a feed forward manner. If the load
torque or speed commands changes, the efficiency search algorithm is abandoned
and the rated flux is established to get the best dynamic response. The drive
system with the proposed efficiency optimization controller has been simulated
with lossy models of converter and machine, and its performance has been
thoroughly investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4266</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4266</id><created>2010-05-24</created><authors><author><keyname>Barskar</keyname><forenames>Raju</forenames></author><author><keyname>Deen</keyname><forenames>Anjana Jayant</forenames></author><author><keyname>Bharti</keyname><forenames>Jyoti</forenames></author><author><keyname>Ahmed</keyname><forenames>Gulfishan Firdose</forenames></author></authors><title>The Algorithm Analysis of E-Commerce Security Issues for Online Payment
  Transaction System in Banking Technology</title><categories>cs.CR</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  E-Commerce offers the banking industry great opportunity, but also creates a
set of new risks and vulnerability such as security threats. Information
security, therefore, is an essential management and technical requirement for
any efficient and effective Payment transaction activities over the internet.
Still, its definition is a complex endeavor due to the constant technological
and business change and requires a coordinated match of algorithm and technical
solutions. Ecommerce is not appropriate to all business transactions and,
within e-commerce there is no one technology that can or should be appropriate
to all requirements. E-commerce is not a new phenomenon; electronic markets,
electronic data interchange and customer e-commerce. The use of electronic data
interchanges as a universal and non-proprietary way of doing business. Through
the electronic transaction the security is the most important phenomena to
enhance the banking transaction security via payment transaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4267</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4267</id><created>2010-05-24</created><authors><author><keyname>Singh</keyname><forenames>Uday Pratap</forenames></author><author><keyname>Jain</keyname><forenames>Sanjeev</forenames></author><author><keyname>Ahmed</keyname><forenames>Gulfishan Firdose</forenames></author></authors><title>Content Base Image Retrieval Using Phong Shading</title><categories>cs.MM cs.IR</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The digital image data is rapidly expanding in quantity and heterogeneity.
The traditional information retrieval techniques does not meet the user's
demand, so there is need to develop an efficient system for content based image
retrieval. Content based image retrieval means retrieval of images from
database on the basis of visual features of image like as color, texture etc.
In our proposed method feature are extracted after applying Phong shading on
input image. Phong shading, flattering out the dull surfaces of the image The
features are extracted using color, texture &amp; edge density methods. Feature
extracted values are used to find the similarity between input query image and
the data base image. It can be measure by the Euclidean distance formula. The
experimental result shows that the proposed approach has a better retrieval
results with phong shading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4268</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4268</id><created>2010-05-24</created><authors><author><keyname>Prasad</keyname><forenames>R Murali</forenames></author><author><keyname>Kumar</keyname><forenames>P. Satish</forenames></author></authors><title>An Adaptive Power Efficient Packet Scheduling Algorithm for Wimax
  Networks</title><categories>cs.NI</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Admission control schemes and scheduling algorithms are designed to offer QoS
services in 802.16/802.16e networks and a number of studies have investigated
these issues. But the channel condition and priority of traffic classes are
very rarely considered in the existing scheduling algorithms. Although a number
of energy saving mechanisms have been proposed for the IEEE 802.16e, to
minimize the power consumption of IEEE 802.16e mobile stations with multiple
real-time connections has not yet been investigated. Moreover, they mainly
consider non real- time connections in IEEE 802.16e networks. In this paper, we
propose to design an adaptive power efficient packet scheduling algorithm that
provides a minimum fair allocation of the channel bandwidth for each packet
flow and additionally minimizes the power consumption. In the adaptive
scheduling algorithm, packets are transmitted as per allotted slots from
different priority of traffic classes adaptively, depending on the channel
condition. Suppose if the buffer size of the high priority traffic queues with
bad channel condition exceeds a threshold, then the priority of those flows
will be increased by adjusting the sleep duty cycle of existing low priority
traffic, to prevent the starvation. By simulation results, we show that our
proposed scheduler achieves better channel utilization while minimizing the
delay and power consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4270</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4270</id><created>2010-05-24</created><authors><author><keyname>Kavitha</keyname><forenames>V.</forenames></author><author><keyname>Punithavalli</keyname><forenames>M.</forenames></author></authors><title>Clustering Time Series Data Stream - A Literature Survey</title><categories>cs.IR</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Mining Time Series data has a tremendous growth of interest in today's world.
To provide an indication various implementations are studied and summarized to
identify the different problems in existing applications. Clustering time
series is a trouble that has applications in an extensive assortment of fields
and has recently attracted a large amount of research. Time series data are
frequently large and may contain outliers. In addition, time series are a
special type of data set where elements have a temporal ordering. Therefore
clustering of such data stream is an important issue in the data mining
process. Numerous techniques and clustering algorithms have been proposed
earlier to assist clustering of time series data streams. The clustering
algorithms and its effectiveness on various applications are compared to
develop a new method to solve the existing problem. This paper presents a
survey on various clustering algorithms available for time series datasets.
Moreover, the distinctiveness and restriction of previous research are
discussed and several achievable topics for future study are recognized.
Furthermore the areas that utilize time series clustering are also summarized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4271</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4271</id><created>2010-05-24</created><authors><author><keyname>Babu</keyname><forenames>K. Delhi</forenames></author><author><keyname>Rajulu</keyname><forenames>P. Govinda</forenames></author><author><keyname>Reddy</keyname><forenames>A. Ramamohana</forenames></author><author><keyname>Kumari</keyname><forenames>A. N. Aruna</forenames></author></authors><title>Selection of Architecture Styles using Analytic Network Process for the
  Optimization of Software Architecture</title><categories>cs.SE</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The continuing process of software systems enlargement in size and complexity
becomes system design extremely important for software production. In this way,
the role of software architecture is significantly important in software
development. It serves as an evaluation and implementation plan for software
development and software evaluation. Consequently, choosing the correct
architecture is a critical issue in software engineering domain.
Moreover,software architecture selection is a multicriteria decision-making
problem in which different goals and objectives must be taken into
consideration. In this paper, more precise and suitable decisions in selection
of architecture styles have been presented by using ANP inference to support
decisions of software architects in order to exploit properties of styles in
the best way to optimize the design of software architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4272</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4272</id><created>2010-05-24</created><authors><author><keyname>Arutchelvan</keyname><forenames>G.</forenames></author><author><keyname>Srivatsa</keyname><forenames>S. K.</forenames></author><author><keyname>Jagannathan</keyname><forenames>R.</forenames></author></authors><title>Inaccuracy Minimization by Partioning Fuzzy Data Sets - Validation of
  Analystical Methodology</title><categories>cs.AI</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In the last two decades, a number of methods have been proposed for
forecasting based on fuzzy time series. Most of the fuzzy time series methods
are presented for forecasting of car road accidents. However, the forecasting
accuracy rates of the existing methods are not good enough. In this paper, we
compared our proposed new method of fuzzy time series forecasting with existing
methods. Our method is based on means based partitioning of the historical data
of car road accidents. The proposed method belongs to the kth order and
time-variant methods. The proposed method can get the best forecasting accuracy
rate for forecasting the car road accidents than the existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4285</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4285</id><created>2010-05-24</created><authors><author><keyname>Karandashev</keyname><forenames>Yakov</forenames></author><author><keyname>Kryzhanovsky</keyname><forenames>Boris</forenames></author><author><keyname>Litinskii</keyname><forenames>Leonid</forenames></author></authors><title>Local Minima of a Quadratic Binary Functional with Quasi-Hebbian
  Connection Matrix</title><categories>cond-mat.dis-nn cs.NE</categories><comments>10 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The local minima of a quadratic functional depending on binary variables are
discussed. An arbitrary connection matrix can be presented in the form of
quasi-Hebbian expansion where each pattern is supplied with its own individual
weight. For such matrices statistical physics methods allow one to derive an
equation describing local minima of the functional. A model where only one
weight differs from other ones is discussed in details. In this case the
above-mention equation can be solved analytically. Obtained results are
confirmed by computer simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4290</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4290</id><created>2010-05-24</created><authors><author><keyname>Sridhar</keyname><forenames>C. S.</forenames></author><author><keyname>ShashiKumar</keyname><forenames>R.</forenames></author><author><keyname>Kumar</keyname><forenames>S. Madhava</forenames></author><author><keyname>Sridhar</keyname><forenames>Manjula</forenames></author><author><keyname>D</keyname><forenames>Varun.</forenames></author></authors><title>E-Speed Governors For Public Transport Vehicles</title><categories>cs.OH</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  An accident is unexpected, unusual, unintended and identifiable external
event which occurs at any place and at any time. The major concern faced by the
government and traffic officials is over speeding at limited speed zones like
hospitals, schools or residential places leading to causalities and more deaths
on the roads. Hence the speed of the vehicles is to be regulated and confined
to the limits as prescribed by the traffic regulations. In this paper we
propose a solution in the form of providing E-speed governor fitted with a
wireless communication system consisting of a Rx which receives the information
regarding the speed regulation for their zones. The TX will be made highly
intelligent and decide when receiver should be made active to regulate the
speed and unwarranted honking from the vehicles which can be deactivated in the
silent zones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4292</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4292</id><created>2010-05-24</created><authors><author><keyname>Rajya</keyname><forenames>Mrigank</forenames></author><author><keyname>Rewri</keyname><forenames>Sonal</forenames></author><author><keyname>Sheoran</keyname><forenames>Swati</forenames></author></authors><title>Application Of Fuzzy System In Segmentation Of MRI Brain Tumor</title><categories>cs.CV</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Segmentation of images holds an important position in the area of image
processing. It becomes more important whi le typically dealing with medical
images where presurgery and post surgery decisions are required for the purpose
of initiating and speeding up the recovery process. Segmentation of 3-D tumor
structures from magnetic resonance images (MRI) is a very challenging problem
due to the variability of tumor geometry and intensity patterns. Level set
evolution combining global smoothness with the flexibility of topology changes
offers significant advantages over the conventional statistical classification
followed by mathematical morphology. Level set evolution with constant
propagation needs to be initialized either completely inside or outside the
tumor and can leak through weak or missing boundary parts. Replacing the
constant propagation term by a statistical force overcomes these limitations
and results in a convergence to a stable solution. Using MR images presenting
tumors, probabilities for background and tumor regions are calculated from a
pre- and post-contrast difference image and mixture modeling fit of the
histogram. The whole image is used for initialization of the level set
evolution to segment the tumor boundaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4298</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4298</id><created>2010-05-24</created><authors><author><keyname>Singh</keyname><forenames>Sameer</forenames></author><author><keyname>Wick</keyname><forenames>Michael</forenames></author><author><keyname>McCallum</keyname><forenames>Andrew</forenames></author></authors><title>Distantly Labeling Data for Large Scale Cross-Document Coreference</title><categories>cs.AI cs.IR cs.LG</categories><comments>16 pages, submitted to ECML 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cross-document coreference, the problem of resolving entity mentions across
multi-document collections, is crucial to automated knowledge base construction
and data mining tasks. However, the scarcity of large labeled data sets has
hindered supervised machine learning research for this task. In this paper we
develop and demonstrate an approach based on ``distantly-labeling'' a data set
from which we can train a discriminative cross-document coreference model. In
particular we build a dataset of more than a million people mentions extracted
from 3.5 years of New York Times articles, leverage Wikipedia for distant
labeling with a generative model (and measure the reliability of such
labeling); then we train and evaluate a conditional random field coreference
model that has factors on cross-document entities as well as mention-pairs.
This coreference model obtains high accuracy in resolving mentions and entities
that are not present in the training data, indicating applicability to
non-Wikipedia data. Given the large amount of data, our work is also an
exercise demonstrating the scalability of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4316</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4316</id><created>2010-05-24</created><authors><author><keyname>Zayyani</keyname><forenames>Hadi</forenames></author><author><keyname>Babaie-Zadeh</keyname><forenames>Massoud</forenames></author><author><keyname>Jutten</keyname><forenames>Christian</forenames></author></authors><title>Bayesian Cram\'{e}r-Rao Bound for Noisy Non-Blind and Blind Compressed
  Sensing</title><categories>cs.IT math.IT</categories><comments>This paper was submitted at 2 June 2009 to IEEE Signal Processing
  Letters and was rejected at 21 August 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the theoretical limitations in reconstructing
sparse signals (in a known complete basis) using compressed sensing framework.
We also divide the CS to non-blind and blind cases. Then, we compute the
Bayesian Cramer-Rao bound for estimating the sparse coefficients while the
measurement matrix elements are independent zero mean random variables.
Simulation results show a large gap between the lower bound and the performance
of the practical algorithms when the number of measurements are low.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4337</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4337</id><created>2010-05-24</created><authors><author><keyname>Stoev</keyname><forenames>Stilian A.</forenames></author><author><keyname>Michailidis</keyname><forenames>George</forenames></author><author><keyname>Vaughan</keyname><forenames>Joel</forenames></author></authors><title>Global Modeling and Prediction of Computer Network Traffic</title><categories>cs.NI math.ST stat.TH</categories><report-no>Department of Statistics, the University of Michigan, Technical
  Report 490</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a probabilistic framework for global modeling of the traffic over
a computer network. This model integrates existing single-link (-flow) traffic
models with the routing over the network to capture the global traffic
behavior. It arises from a limit approximation of the traffic fluctuations as
the time--scale and the number of users sharing the network grow. The resulting
probability model is comprised of a Gaussian and/or a stable, infinite variance
components. They can be succinctly described and handled by certain
'space-time' random fields. The model is validated against simulated and real
data. It is then applied to predict traffic fluctuations over unobserved links
from a limited set of observed links. Further, applications to anomaly
detection and network management are briefly discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4344</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4344</id><created>2010-05-24</created><authors><author><keyname>Stoev</keyname><forenames>Stilian A.</forenames></author><author><keyname>Taqqu</keyname><forenames>Murad S.</forenames></author></authors><title>Max-stable sketches: estimation of Lp-norms, dominance norms and point
  queries for non-negative signals</title><categories>cs.DS cs.DB</categories><report-no>Department of Statistics, the University of Michigan, Technical
  Report 433</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Max-stable random sketches can be computed efficiently on fast streaming
positive data sets by using only sequential access to the data. They can be
used to answer point and Lp-norm queries for the signal. There is an intriguing
connection between the so-called p-stable (or sum-stable) and the max-stable
sketches. Rigorous performance guarantees through error-probability estimates
are derived and the algorithmic implementation is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4363</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4363</id><created>2010-05-24</created><authors><author><keyname>Kzaz</keyname><forenames>Larbi</forenames></author><author><keyname>Elasri</keyname><forenames>Hicham</forenames></author><author><keyname>Sekkaki</keyname><forenames>Abderrahim</forenames></author></authors><title>A model for semantic integration of business components</title><categories>cs.SE</categories><report-no>ISSN:0975-3826 (Online); 0975-4660 (Print)</report-no><journal-ref>Academy &amp; Industry Research Collaboration Center (AIRCC) -
  International journal of computer science &amp; information Technology (IJCSIT),
  07-03-2010</journal-ref><doi>10.1016/S0550-3213(01)00405-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, reusable components are available in several repositories. These last
are certainly conceived for the reusing However, this re-use is not immediate;
it requires, in the fact, to pass through some essential conceptual operations,
among them in particular, research, integration, adaptation, and composition.
We are interested in the present work to the problem of semantic integration of
heterogeneous Business Components. This problem is often put in syntactical
terms, while the real stake is of semantic order. Our contribution concerns a
model proposal for Business components integration as well as resolution method
of semantic naming conflicts, met during the integration of Business
Components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4376</identifier>
 <datestamp>2010-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4376</id><created>2010-05-24</created><authors><author><keyname>Lancichinetti</keyname><forenames>Andrea</forenames></author><author><keyname>Kivela</keyname><forenames>Mikko</forenames></author><author><keyname>Saramaki</keyname><forenames>Jari</forenames></author><author><keyname>Fortunato</keyname><forenames>Santo</forenames></author></authors><title>Characterizing the community structure of complex networks</title><categories>physics.soc-ph cs.IR</categories><comments>15 pages, 20 figures, 4 tables</comments><journal-ref>PLoS One 5(8), e11976 (2010)</journal-ref><doi>10.1371/journal.pone.0011976</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community structure is one of the key properties of complex networks and
plays a crucial role in their topology and function. While an impressive amount
of work has been done on the issue of community detection, very little
attention has been so far devoted to the investigation of communities in real
networks. We present a systematic empirical analysis of the statistical
properties of communities in large information, communication, technological,
biological, and social networks. We find that the mesoscopic organization of
networks of the same category is remarkably similar. This is reflected in
several characteristics of community structure, which can be used as
``fingerprints'' of specific network categories. While community size
distributions are always broad, certain categories of networks consist mainly
of tree-like communities, while others have denser modules. Average path
lengths within communities initially grow logarithmically with community size,
but the growth saturates or slows down for communities larger than a
characteristic size. This behaviour is related to the presence of hubs within
communities, whose roles differ across categories. Also the community
embeddedness of nodes, measured in terms of the fraction of links within their
communities, has a characteristic distribution for each category. Our findings
are verified by the use of two fundamentally different community detection
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4379</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4379</id><created>2010-05-24</created><authors><author><keyname>Snow</keyname><forenames>Zachary</forenames></author><author><keyname>Baelde</keyname><forenames>David</forenames></author><author><keyname>Nadathur</keyname><forenames>Gopalan</forenames></author></authors><title>A Meta-Programming Approach to Realizing Dependently Typed Logic
  Programming</title><categories>cs.LO</categories><acm-class>D.3.2; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dependently typed lambda calculi such as the Logical Framework (LF) can
encode relationships between terms in types and can naturally capture
correspondences between formulas and their proofs. Such calculi can also be
given a logic programming interpretation: the Twelf system is based on such an
interpretation of LF. We consider here whether a conventional logic programming
language can provide the benefits of a Twelf-like system for encoding type and
proof-and-formula dependencies. In particular, we present a simple mapping from
LF specifications to a set of formulas in the higher-order hereditary Harrop
(hohh) language, that relates derivations and proof-search between the two
frameworks. We then show that this encoding can be improved by exploiting
knowledge of the well-formedness of the original LF specifications to elide
much redundant type-checking information. The resulting logic program has a
structure that closely resembles the original specification, thereby allowing
LF specifications to be viewed as hohh meta-programs. Using the Teyjus
implementation of lambdaProlog, we show that our translation provides an
efficient means for executing LF specifications, complementing the ability that
the Twelf system provides for reasoning about them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4394</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4394</id><created>2010-05-24</created><authors><author><keyname>Li</keyname><forenames>Fei</forenames></author></authors><title>Scheduling Packets with Values and Deadlines in Size-bounded Buffers</title><categories>cs.DS</categories><comments>7 pages</comments><doi>10.1007/978-3-642-17458-2_32</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by providing quality-of-service differentiated services in the
Internet, we consider buffer management algorithms for network switches. We
study a multi-buffer model. A network switch consists of multiple size-bounded
buffers such that at any time, the number of packets residing in each
individual buffer cannot exceed its capacity. Packets arrive at the network
switch over time; they have values, deadlines, and designated buffers. In each
time step, at most one pending packet is allowed to be sent and this packet can
be from any buffer. The objective is to maximize the total value of the packets
sent by their respective deadlines. A 9.82-competitive online algorithm has
been provided for this model (Azar and Levy. SWAT 2006), but no offline
algorithms have been known yet. In this paper, We study the offline setting of
the multi-buffer model. Our contributions include a few optimal offline
algorithms for some variants of the model. Each variant has its unique and
interesting algorithmic feature. These offline algorithms help us understand
the model better in designing online algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4395</identifier>
 <datestamp>2010-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4395</id><created>2010-05-24</created><authors><author><keyname>Collins</keyname><forenames>Joseph B.</forenames></author></authors><title>An OpenMath Content Dictionary for Tensor Concepts</title><categories>cs.MS</categories><comments>To appear in The 9th International Conference on Mathematical
  Knowledge Management: MKM 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new OpenMath content dictionary, named tensor1, containing
symbols for the expression of tensor formulas. These symbols support the
expression of non-Cartesian coordinates and invariant, multilinear expressions
in the context of coordinate transformations. While current OpenMath symbols
support the expression of linear algebra formulas using matrices and vectors,
we find that there is an underlying assumption of Cartesian, or standard,
coordinates that makes the expression of general tensor formulas difficult, if
not impossible. In introducing these new OpenMath symbols for the expression of
tensor formulas, we attempt to maintain, as much as possible, consistency with
prior OpenMath symbol definitions for linear algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4405</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4405</id><created>2010-05-19</created><authors><author><keyname>He&#xef;geas</keyname><forenames>Laure</forenames><affiliation>IMAG-INRIA Rh&#xf4;ne-Alpes / GRAVIR</affiliation></author><author><keyname>Luciani</keyname><forenames>Annie</forenames><affiliation>ACROE</affiliation></author><author><keyname>Thollot</keyname><forenames>Jo&#xeb;lle</forenames><affiliation>IMAG-INRIA Rh&#xf4;ne-Alpes / GRAVIR</affiliation></author><author><keyname>Castagn&#xe9;</keyname><forenames>Nicolas</forenames><affiliation>ACROE, ICA</affiliation></author></authors><title>A physically-based particle model of emergent crowd behaviors</title><categories>cs.GR physics.comp-ph</categories><comments>Graphicon 2003, Moscou : Russian Federation (2003)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a modeling process in order to produce a realistic
simulation of crowds in the ancient Greek agora of Argos. This place was a
social theater in which two kinds of collective phenomena took place:
interpersonal interactions (small group discussion and negotiation, etc.) and
global collective phenomena, such as flowing and jamming. In this paper, we
focus on the second type of collective human phenomena, called non-deliberative
emergent crowd phenomena. This is a typical case of collective emergent
self-organization. When a great number of individuals move within a confined
environment and under a common fate, collective structures appear
spontaneously: jamming with inner collapses, organized flowing with queues,
curls, and vortices, propagation effects, etc. These are particularly relevant
features to enhance the realism - more precisely the &quot;truthfulness&quot; - of models
of this kind of collective phenomena. We assume that this truthfulness is
strongly associated with the concept of emergence: evolutions are not
predetermined by the individual characters, but emerge from the interaction of
numerous characters. The evolutions are not repetitive, and evolve on the basis
of small changes. This paper demonstrates that the physically-based interacting
particles system is an adequate candidate to model emergent crowd effects: it
associates a large number of elementary dynamic actors via elementary
non-linear dynamic interactions. Our model of the scene is regulated as a
large, dynamically coupled network of second order differential automata. We
take advantage of symbolic non-photorealistic and efficient visualization to
render the style of the person, rather than the person itself. As an artistic
representation, NPR reinforces the symbolic acceptance of the scene by the
observer, triggering an immediate and intuitive recognition of the scene as a
plausible scene from ancient Greece.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4446</identifier>
 <datestamp>2010-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4446</id><created>2010-05-24</created><authors><author><keyname>Coldridge</keyname><forenames>Jack</forenames></author><author><keyname>Amos</keyname><forenames>Martyn</forenames></author></authors><title>Genetic algorithms and the art of Zen</title><categories>cs.NE cs.AI</categories><comments>Submitted</comments><report-no>2010/5/1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a novel genetic algorithm (GA) solution to a simple
yet challenging commercial puzzle game known as the Zen Puzzle Garden (ZPG). We
describe the game in detail, before presenting a suitable encoding scheme and
fitness function for candidate solutions. We then compare the performance of
the genetic algorithm with that of the A* algorithm. Our results show that the
GA is competitive with informed search in terms of solution quality, and
significantly out-performs it in terms of computational resource requirements.
We conclude with a brief discussion of the implications of our findings for
game solving and other &quot;real world&quot; problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4447</identifier>
 <datestamp>2010-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4447</id><created>2010-05-24</created><authors><author><keyname>Lyaletski</keyname><forenames>Alexander</forenames><affiliation>Kiev National Taras Shevchenko University</affiliation></author><author><keyname>Verchinine</keyname><forenames>Konstantin</forenames><affiliation>Math-Info Department, Paris 12 University</affiliation></author></authors><title>Evidence Algorithm and System for Automated Deduction: A Retrospective
  View</title><categories>cs.AI cs.LO</categories><comments>To appear in The 9th International Conference on Mathematical
  Knowledge Management: MKM 2010</comments><msc-class>01-02 (Primary) 68T15, 03B35 (Secondary)</msc-class><acm-class>F.4.1; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A research project aimed at the development of an automated theorem proving
system was started in Kiev (Ukraine) in early 1960s. The mastermind of the
project, Academician V.Glushkov, baptized it &quot;Evidence Algorithm&quot;, EA. The work
on the project lasted, off and on, more than 40 years. In the framework of the
project, the Russian and English versions of the System for Automated
Deduction, SAD, were constructed. They may be already seen as powerful
theorem-proving assistants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4454</identifier>
 <datestamp>2010-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4454</id><created>2010-05-24</created><authors><author><keyname>Jacob</keyname><forenames>Joseph C.</forenames></author><author><keyname>Katz</keyname><forenames>Daniel S.</forenames></author><author><keyname>Berriman</keyname><forenames>G. Bruce</forenames></author><author><keyname>Good</keyname><forenames>John</forenames></author><author><keyname>Laity</keyname><forenames>Anastasia C.</forenames></author><author><keyname>Deelman</keyname><forenames>Ewa</forenames></author><author><keyname>Kesselman</keyname><forenames>Carl</forenames></author><author><keyname>Singh</keyname><forenames>Gurmeet</forenames></author><author><keyname>Su</keyname><forenames>Mei-Hui</forenames></author><author><keyname>Prince</keyname><forenames>Thomas A.</forenames></author><author><keyname>Williams</keyname><forenames>Roy</forenames></author></authors><title>Montage: a grid portal and software toolkit for science-grade
  astronomical image mosaicking</title><categories>astro-ph.IM cs.DC cs.SE</categories><comments>16 pages, 11 figures</comments><journal-ref>Int. J. Computational Science and Engineering. 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Montage is a portable software toolkit for constructing custom, science-grade
mosaics by composing multiple astronomical images. The mosaics constructed by
Montage preserve the astrometry (position) and photometry (intensity) of the
sources in the input images. The mosaic to be constructed is specified by the
user in terms of a set of parameters, including dataset and wavelength to be
used, location and size on the sky, coordinate system and projection, and
spatial sampling rate. Many astronomical datasets are massive, and are stored
in distributed archives that are, in most cases, remote with respect to the
available computational resources. Montage can be run on both single- and
multi-processor computers, including clusters and grids. Standard grid tools
are used to run Montage in the case where the data or computers used to
construct a mosaic are located remotely on the Internet. This paper describes
the architecture, algorithms, and usage of Montage as both a software toolkit
and as a grid portal. Timing results are provided to show how Montage
performance scales with number of processors on a cluster computer. In
addition, we compare the performance of two methods of running Montage in
parallel on a grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4457</identifier>
 <datestamp>2010-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4457</id><created>2010-05-24</created><authors><author><keyname>Groth</keyname><forenames>Paul</forenames></author><author><keyname>Deelman</keyname><forenames>Ewa</forenames></author><author><keyname>Juve</keyname><forenames>Gideon</forenames></author><author><keyname>Mehta</keyname><forenames>Gaurang</forenames></author><author><keyname>Berriman</keyname><forenames>Bruce</forenames></author></authors><title>Pipeline-Centric Provenance Model</title><categories>astro-ph.IM cs.IR</categories><comments>9 pages, 4 figures</comments><journal-ref>Proceedings of the 4th Workshop on Workflows in Support of
  Large-Scale Science, 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new provenance model which is tailored to a class
of workflow-based applications. We motivate the approach with use cases from
the astronomy community. We generalize the class of applications the approach
is relevant to and propose a pipeline-centric provenance model. Finally, we
evaluate the benefits in terms of storage needed by the approach when applied
to an astronomy application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4461</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4461</id><created>2010-05-24</created><updated>2010-11-06</updated><authors><author><keyname>Nguyen</keyname><forenames>Phong S.</forenames></author><author><keyname>Pfister</keyname><forenames>Henry D.</forenames></author><author><keyname>Narayanan</keyname><forenames>Krishna R.</forenames></author></authors><title>On Multiple Decoding Attempts for Reed-Solomon Codes: A Rate-Distortion
  Approach</title><categories>cs.IT math.IT</categories><comments>to appear in the IEEE Transactions on Information Theory (Special
  Issue on Facets of Coding Theory: from Algorithms to Networks)</comments><journal-ref>IEEE Trans. on Information Theory, vol 57, issue 2 (2011), pages
  668 - 691</journal-ref><doi>10.1109/TIT.2010.2095202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One popular approach to soft-decision decoding of Reed-Solomon (RS) codes is
based on using multiple trials of a simple RS decoding algorithm in combination
with erasing or flipping a set of symbols or bits in each trial. This paper
presents a framework based on rate-distortion (RD) theory to analyze these
multiple-decoding algorithms. By defining an appropriate distortion measure
between an error pattern and an erasure pattern, the successful decoding
condition, for a single errors-and-erasures decoding trial, becomes equivalent
to distortion being less than a fixed threshold. Finding the best set of
erasure patterns also turns into a covering problem which can be solved
asymptotically by rate-distortion theory. Thus, the proposed approach can be
used to understand the asymptotic performance-versus-complexity trade-off of
multiple errors-and-erasures decoding of RS codes.
  This initial result is also extended a few directions. The rate-distortion
exponent (RDE) is computed to give more precise results for moderate
blocklengths. Multiple trials of algebraic soft-decision (ASD) decoding are
analyzed using this framework. Analytical and numerical computations of the RD
and RDE functions are also presented. Finally, simulation results show that
sets of erasure patterns designed using the proposed methods outperform other
algorithms with the same number of decoding trials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4472</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4472</id><created>2010-05-24</created><authors><author><keyname>Cheng</keyname><forenames>Yong</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author></authors><title>Distributive Power Control Algorithm for Multicarrier Interference
  Network over Time-Varying Fading Channels - Tracking Performance Analysis and
  Optimization</title><categories>cs.IT math.IT</categories><comments>To Appear on the IEEE Transaction on Signal Processing</comments><doi>10.1109/TSP.2010.2052044</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed power control over interference limited network has received an
increasing intensity of interest over the past few years. Distributed solutions
(like the iterative water-filling, gradient projection, etc.) have been
intensively investigated under \emph{quasi-static} channels. However, as such
distributed solutions involve iterative updating and explicit message passing,
it is unrealistic to assume that the wireless channel remains unchanged during
the iterations. Unfortunately, the behavior of those distributed solutions
under \emph{time-varying} channels is in general unknown. In this paper, we
shall investigate the distributed scaled gradient projection algorithm (DSGPA)
in a $K$ pairs multicarrier interference network under a finite-state Markov
channel (FSMC) model. We shall analyze the \emph{convergence property} as well
as \emph{tracking performance} of the proposed DSGPA. Our analysis shows that
the proposed DSGPA converges to a limit region rather than a single point under
the FSMC model. We also show that the order of growth of the tracking errors is
given by $\mathcal{O}\(1 \big/ \bar{N}\)$, where $\bar{N}$ is the \emph{average
sojourn time} of the FSMC. Based on the analysis, we shall derive the
\emph{tracking error optimal scaling matrices} via Markov decision process
modeling. We shall show that the tracking error optimal scaling matrices can be
implemented distributively at each transmitter. The numerical results show the
superior performance of the proposed DSGPA over three baseline schemes, such as
the gradient projection algorithm with a constant stepsize.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4496</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4496</id><created>2010-05-25</created><authors><author><keyname>Farid</keyname><forenames>Dewan Md.</forenames><affiliation>University Lumiere Lyon 2 - France</affiliation></author><author><keyname>Harbi</keyname><forenames>Nouria</forenames><affiliation>University Lumiere Lyon 2 - France</affiliation></author><author><keyname>Rahman</keyname><forenames>Mohammad Zahidur</forenames><affiliation>Jahangirnagar University, Bangladesh</affiliation></author></authors><title>Combining Naive Bayes and Decision Tree for Adaptive Intrusion Detection</title><categories>cs.AI</categories><comments>14 Pages, IJNSA</comments><journal-ref>International Journal of Network Security &amp; Its Applications 2.2
  (2010) 12-25</journal-ref><doi>10.5121/ijnsa.2010.2202</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, a new learning algorithm for adaptive network intrusion
detection using naive Bayesian classifier and decision tree is presented, which
performs balance detections and keeps false positives at acceptable level for
different types of network attacks, and eliminates redundant attributes as well
as contradictory examples from training data that make the detection model
complex. The proposed algorithm also addresses some difficulties of data mining
such as handling continuous attribute, dealing with missing attribute values,
and reducing noise in training data. Due to the large volumes of security audit
data as well as the complex and dynamic properties of intrusion behaviours,
several data miningbased intrusion detection techniques have been applied to
network-based traffic data and host-based data in the last decades. However,
there remain various issues needed to be examined towards current intrusion
detection systems (IDS). We tested the performance of our proposed algorithm
with existing learning algorithms by employing on the KDD99 benchmark intrusion
detection dataset. The experimental results prove that the proposed algorithm
achieved high detection rates (DR) and significant reduce false positives (FP)
for different types of network intrusions using limited computational
resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4499</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4499</id><created>2010-05-25</created><authors><author><keyname>Ahmed</keyname><forenames>Eslam Gamal</forenames></author><author><keyname>Shaaban</keyname><forenames>Eman</forenames></author><author><keyname>Hashem</keyname><forenames>Mohamed</forenames></author></authors><title>Lightweight Mutual Authentication Protocol for Low Cost RFID Tags</title><categories>cs.CR</categories><comments>11 Pages, IJNSA</comments><journal-ref>International Journal of Network Security &amp; Its Applications 2.2
  (2010) 27-37</journal-ref><doi>10.5121/ijnsa.2010.2203</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Radio Frequency Identification (RFID) technology one of the most promising
technologies in the field of ubiquitous computing. Indeed, RFID technology may
well replace barcode technology. Although it offers many advantages over other
identification systems, there are also associated security risks that are not
easy to be addressed. When designing a real lightweight authentication protocol
for low cost RFID tags, a number of challenges arise due to the extremely
limited computational, storage and communication abilities of Low-cost RFID
tags. This paper proposes a real mutual authentication protocol for low cost
RFID tags. The proposed protocol prevents passive attacks as active attacks are
discounted when designing a protocol to meet the requirements of low cost RFID
tags. However the implementation of the protocol meets the limited abilities of
low cost RFID tags.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4501</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4501</id><created>2010-05-25</created><authors><author><keyname>Sangeetha</keyname><forenames>S.</forenames><affiliation>Angel College of Engineering, India</affiliation></author><author><keyname>Vaidehi</keyname><forenames>V.</forenames><affiliation>Madras Institute of Technology, India</affiliation></author></authors><title>Fuzzy Aided Application Layer Semantic Intrusion Detection System -
  FASIDS</title><categories>cs.CR</categories><comments>18 Pages, IJNSA</comments><journal-ref>International Journal of Network Security &amp; Its Applications 2.2
  (2010) 39-56</journal-ref><doi>10.5121/ijnsa.2010.2204</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The objective of this is to develop a Fuzzy aided Application layer Semantic
Intrusion Detection System (FASIDS) which works in the application layer of the
network stack. FASIDS consist of semantic IDS and Fuzzy based IDS. Rule based
IDS looks for the specific pattern which is defined as malicious. A
non-intrusive regular pattern can be malicious if it occurs several times with
a short time interval. For detecting such malicious activities, FASIDS is
proposed in this paper. At application layer, HTTP traffic's header and payload
are analyzed for possible intrusion. In the proposed misuse detection module,
the semantic intrusion detection system works on the basis of rules that define
various application layer misuses that are found in the network. An attack
identified by the IDS is based on a corresponding rule in the rule-base. An
event that doesn't make a 'hit' on the rule-base is given to a Fuzzy Intrusion
Detection System (FIDS) for further analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4504</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4504</id><created>2010-05-25</created><authors><author><keyname>Elboukhari</keyname><forenames>Mohamed</forenames><affiliation>FSO - University Mohamed Ist, Morocco</affiliation></author><author><keyname>Azizi</keyname><forenames>Mostafa</forenames><affiliation>ESTO - University Mohamed Ist, Morocco</affiliation></author><author><keyname>Azizi</keyname><forenames>Abdelmalek</forenames><affiliation>Academy Hassan II of Sciences &amp; Technology, Morocco</affiliation></author></authors><title>Analysis of the Security of BB84 by Model Checking</title><categories>cs.CR</categories><comments>12 Pages, IJNSA</comments><journal-ref>International Journal of Network Security &amp; Its Applications 2.2
  (2010) 87-98</journal-ref><doi>10.5121/ijnsa.2010.2207</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Quantum Cryptography or Quantum key distribution (QKD) is a technique that
allows the secure distribution of a bit string, used as key in cryptographic
protocols. When it was noted that quantum computers could break public key
cryptosystems based on number theory extensive studies have been undertaken on
QKD. Based on quantum mechanics, QKD offers unconditionally secure
communication. Now, the progress of research in this field allows the
anticipation of QKD to be available outside of laboratories within the next few
years. Efforts are made to improve the performance and reliability of the
implemented technologies. But several challenges remain despite this big
progress. The task of how to test the apparatuses of QKD For example did not
yet receive enough attention. These devises become complex and demand a big
verification effort. In this paper we are interested in an approach based on
the technique of probabilistic model checking for studying quantum information.
Precisely, we use the PRISM tool to analyze the security of BB84 protocol and
we are focused on the specific security property of eavesdropping detection. We
show that this property is affected by the parameters of quantum channel and
the power of eavesdropper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4505</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4505</id><created>2010-05-25</created><authors><author><keyname>Herberg</keyname><forenames>Ulrich</forenames><affiliation>Ecole Polytechnique, France</affiliation></author><author><keyname>Clausen</keyname><forenames>Thomas</forenames><affiliation>Ecole Polytechnique, France</affiliation></author></authors><title>Security Issues in the Optimized Link State Routing Protocol Version 2
  (OLSRV2)</title><categories>cs.NI</categories><comments>20 Pages, IJNSA</comments><journal-ref>International Journal of Network Security &amp; Its Applications 2.2
  (2010) 162-181</journal-ref><doi>10.5121/ijnsa.2010.2213</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Mobile Ad hoc NETworks (MANETs) are leaving the confines of research
laboratories, to find place in real-world deployments. Outside specialized
domains (military, vehicular, etc.), city-wide communitynetworks are emerging,
connecting regular Internet users with each other, and with the Internet, via
MANETs. Growing to encompass more than a handful of &quot;trusted participants&quot;, the
question of preserving the MANET network connectivity, even when faced with
careless or malicious participants, arises, and must be addressed. A first step
towards protecting a MANET is to analyze the vulnerabilities of the routing
protocol, managing the connectivity. By understanding how the algorithms of the
routing protocol operate, and how these can be exploited by those with ill
intent, countermeasures can be developed, readying MANETs for wider deployment
and use. This paper takes an abstract look at the algorithms that constitute
the Optimized Link State Routing Protocol version 2 (OLSRv2), and identifies
for each protocol element the possible vulnerabilities and attacks -- in a
certain way, provides a &quot;cookbook&quot; for how to best attack an operational OLSRv2
network, or for how to proceed with developing protective countermeasures
against these attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4508</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4508</id><created>2010-05-25</created><updated>2010-09-01</updated><authors><author><keyname>Tiu</keyname><forenames>Alwen F</forenames><affiliation>The Australian National University</affiliation></author><author><keyname>Gore</keyname><forenames>Rajeev</forenames><affiliation>The Australian National University</affiliation></author><author><keyname>Dawson</keyname><forenames>Jeremy</forenames><affiliation>The Australian National University</affiliation></author></authors><title>A Proof Theoretic Analysis of Intruder Theories</title><categories>cs.LO cs.CR</categories><comments>Extended version of RTA 2009 paper</comments><proxy>LMCS</proxy><acm-class>cs.CR</acm-class><journal-ref>Logical Methods in Computer Science, Volume 6, Issue 3 (September
  1, 2010) lmcs:877</journal-ref><doi>10.2168/LMCS-6(3:12)2010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of intruder deduction in security protocol analysis:
that is, deciding whether a given message M can be deduced from a set of
messages Gamma under the theory of blind signatures and arbitrary convergent
equational theories modulo associativity and commutativity (AC) of certain
binary operators. The traditional formulations of intruder deduction are
usually given in natural-deduction-like systems and proving decidability
requires significant effort in showing that the rules are &quot;local&quot; in some
sense. By using the well-known translation between natural deduction and
sequent calculus, we recast the intruder deduction problem as proof search in
sequent calculus, in which locality is immediate. Using standard proof
theoretic methods, such as permutability of rules and cut elimination, we show
that the intruder deduction problem can be reduced, in polynomial time, to the
elementary deduction problem, which amounts to solving certain equations in the
underlying individual equational theories. We show that this result extends to
combinations of disjoint AC-convergent theories whereby the decidability of
intruder deduction under the combined theory reduces to the decidability of
elementary deduction in each constituent theory. To further demonstrate the
utility of the sequent-based approach, we show that, for Dolev-Yao intruders,
our sequent-based techniques can be used to solve the more difficult problem of
solving deducibility constraints, where the sequents to be deduced may contain
gaps (or variables) representing possible messages the intruder may produce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4518</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4518</id><created>2010-05-25</created><updated>2011-03-16</updated><authors><author><keyname>Boufkhad</keyname><forenames>Yacine</forenames></author><author><keyname>Hugel</keyname><forenames>Thomas</forenames></author></authors><title>Estimating Satisfiability</title><categories>cs.DM</categories><comments>31 pages, added the 4.419 upper-bound on the cores</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of estimating the proportion of satisfiable instances of a given
CSP (constraint satisfaction problem) can be tackled through weighting. It
consists in putting onto each solution a non-negative real value based on its
neighborhood in a way that the total weight is at least 1 for each satisfiable
instance. We define in this paper a general weighting scheme for the estimation
of satisfiability of general CSPs. First we give some sufficient conditions for
a weighting system to be correct. Then we show that this scheme allows for an
improvement on the upper bound on the existence of non-trivial cores in 3-SAT
obtained by Maneva and Sinclair (2008) to 4.419. Another more common way of
estimating satisfiability is ordering. This consists in putting a total order
on the domain, which induces an orientation between neighboring solutions in a
way that prevents circuits from appearing, and then counting only minimal
elements. We compare ordering and weighting under various conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4525</identifier>
 <datestamp>2010-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4525</id><created>2010-05-25</created><authors><author><keyname>Elasri</keyname><forenames>Hicham</forenames></author><author><keyname>Kzaz</keyname><forenames>Larbi</forenames></author><author><keyname>Sekkaki</keyname><forenames>Abderrahim</forenames></author></authors><title>Towards an architecture for semantic integration of business components</title><categories>cs.SE</categories><comments>AIM (Association Information and Management), The 14th conference of
  AIM (Association Information and Management), sponsored by the Association
  for Information Systems (AIS), has been held in Marrakech, Morocco between 10
  and 12 of June 2009</comments><doi>10.1016/S0550-3213(01)00405-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, reusable components are available in several repositorys. These are
certainly conceived for re-use. However, this re-use is not immediate, it
requires, in effect, to pass by some essential conceptual operations, among
which in particular, research, integration, adaptation, and composition. We are
interested in the present work to the problem of semantic integration of
heterogeneous Business Components. This problem is often put in syntactical
terms, while the real stake is of semantic order. Our contribution concerns an
architecture proposal for Business components integration and a resolution
method of semantic naming conflicts, met during the integration of Business
Components
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4540</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4540</id><created>2010-05-25</created><updated>2011-03-03</updated><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author><author><keyname>Brandt</keyname><forenames>Felix</forenames></author><author><keyname>Seedig</keyname><forenames>Hans Georg</forenames></author></authors><title>Optimal Partitions in Additively Separable Hedonic Games</title><categories>cs.GT</categories><comments>11 pages; A preliminary version of this work was invited for
  presentation in the session `Cooperative Games and Combinatorial
  Optimization' at the 24th European Conference on Operational Research (EURO
  2010) in Lisbon</comments><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><journal-ref>Artificial Intelligence 195, 2013</journal-ref><doi>10.1016/j.artint.2012.09.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We conduct a computational analysis of fair and optimal partitions in
additively separable hedonic games. We show that, for strict preferences, a
Pareto optimal partition can be found in polynomial time while verifying
whether a given partition is Pareto optimal is coNP-complete, even when
preferences are symmetric and strict. Moreover, computing a partition with
maximum egalitarian or utilitarian social welfare or one which is both Pareto
optimal and individually rational is NP-hard. We also prove that checking
whether there exists a partition which is both Pareto optimal and envy-free is
$\Sigma_{2}^{p}$-complete. Even though an envy-free partition and a Nash stable
partition are both guaranteed to exist for symmetric preferences, checking
whether there exists a partition which is both envy-free and Nash stable is
NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4552</identifier>
 <datestamp>2011-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4552</id><created>2010-05-25</created><authors><author><keyname>Urban</keyname><forenames>Josef</forenames></author><author><keyname>Alama</keyname><forenames>Jesse</forenames></author><author><keyname>Rudnicki</keyname><forenames>Piotr</forenames></author><author><keyname>Geuvers</keyname><forenames>Herman</forenames></author></authors><title>A Wiki for Mizar: Motivation, Considerations, and Initial Prototype</title><categories>cs.DL</categories><comments>To appear in The 9th International Conference on Mathematical
  Knowledge Management: MKM 2010</comments><journal-ref>Intelligent Computer Mathematics 2010, LNCS 6167, pp. 455-469</journal-ref><doi>10.1007/978-3-642-14128-7_38</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal mathematics has so far not taken full advantage of ideas from
collaborative tools such as wikis and distributed version control systems
(DVCS). We argue that the field could profit from such tools, serving both
newcomers and experts alike. We describe a preliminary system for such
collaborative development based on the Git DVCS. We focus, initially, on the
Mizar system and its library of formalized mathematics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4563</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4563</id><created>2010-05-25</created><authors><author><keyname>Guilbaud</keyname><forenames>Claire</forenames><affiliation>ICA</affiliation></author><author><keyname>Luciani</keyname><forenames>Annie</forenames><affiliation>ACROE</affiliation></author><author><keyname>Castagn&#xe9;</keyname><forenames>Nicolas</forenames><affiliation>ACROE</affiliation></author></authors><title>Physically-based particle simulation and visualization of pastes and
  gels</title><categories>cs.GR physics.comp-ph</categories><comments>Graphicon 2003, Moscou : Russie, F\'ed\'eration De (2003)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is focused on the question of simulation and visualiza- tion of 3D
gel and paste dynamic effects. In a first part, we introduce a 3D physically
based particle (or mass-interaction) model, with a small number of masses and
few powerful interaction parameters, which is able to generate the dynamic
features of both gels and pastes. This model proves that the 3D
mass-interaction method is relevant for the simulation of such phenomena,
without an explicit knowledge of their underly- ing physics. In a second part,
we expose an original rendering process, the Flow Structuring Method that
enhances the dynamic properties of the simulation and offers a realistic
visualization. This process ignores all the properties of the underlying
physical model. It leads to a reconstruction of the spatial structure of the
gel (or paste) flow only through an analysis of the output of the simula- tion
which is a set of unorganized points moving in a 3D space. Finally, the paper
presents realistic renderings obtained by using implicit surfaces and
ray-tracing techniques on the Structured Flow previously obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4564</identifier>
 <datestamp>2010-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4564</id><created>2010-05-25</created><authors><author><keyname>Luciani</keyname><forenames>Annie</forenames><affiliation>ACROE, ICA</affiliation></author><author><keyname>Evrard</keyname><forenames>Matthieu</forenames><affiliation>ICA</affiliation></author><author><keyname>Courouss&#xe9;</keyname><forenames>Damien</forenames><affiliation>ICA</affiliation></author><author><keyname>Castagn&#xe9;</keyname><forenames>Nicolas</forenames><affiliation>ACROE, ICA</affiliation></author><author><keyname>Cadoz</keyname><forenames>Claude</forenames><affiliation>ACROE, ICA</affiliation></author><author><keyname>Florens</keyname><forenames>Jean-Loup</forenames><affiliation>ACROE</affiliation></author></authors><title>A basic gesture and motion format for virtual reality multisensory
  applications</title><categories>cs.HC cs.GR cs.MM cs.SD</categories><comments>GRAPP'06, Set\`ubal : Portugal (2006)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question of encoding movements such as those produced by human gestures
may become central in the coming years, given the growing importance of
movement data exchanges between heterogeneous systems and applications (musical
applications, 3D motion control, virtual reality interaction, etc.). For the
past 20 years, various formats have been proposed for encoding movement,
especially gestures. Though, these formats, at different degrees, were designed
in the context of quite specific applications (character animation, motion
capture, musical gesture, biomechanical concerns...). The article introduce a
new file format, called GMS (for 'Gesture and Motion Signal'), with the aim of
being more low-level and generic, by defining the minimal features a format
carrying movement/gesture information needs, rather than by gathering all the
information generally given by the existing formats. The article argues that,
given its growing presence in virtual reality situations, the &quot;gesture signal&quot;
itself must be encoded, and that a specific format is needed. The proposed
format features the inner properties of such signals: dimensionality,
structural features, types of variables, and spatial and temporal properties.
The article first reviews the various situations with multisensory virtual
objects in which gesture controls intervene. The proposed format is then
deduced, as a mean to encode such versatile and variable &quot;gestural and animated
scene&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4584</identifier>
 <datestamp>2010-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4584</id><created>2010-05-06</created><authors><author><keyname>Sivaraman</keyname><forenames>R.</forenames></author><author><keyname>Chandrasekaran</keyname><forenames>R. M.</forenames></author></authors><title>Effective Query Retrieval System In Mobile Business Environment</title><categories>cs.OH</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 7 No. 3, March 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Web Based Query Management System (WBQMS) is a methodology to design and to
implement Mobile Business, in which a server is the gateway to connect
databases with clients which sends requests and receives responses in a
distributive manner. The gateway, which communicates with mobile phone via GSM
Modem, receives the coded queries from users and sends packed results back. The
software which communicates with the gateway system via SHORT MESSAGE, packs
users' requests, IDs and codes, and sends the package to the gateway; then
interprets the packed data for the users to read on a page of GUI. Whenever and
wherever they are, the customer can query the information by sending messages
through the client device which may be mobile phone or PC. The mobile clients
can get the appropriate services through the mobile business architecture in
distributed environment. The messages are secured through the client side
encoding mechanism to avoid the intruders. The gateway system is programmed by
Java, while the software at clients by J2ME and the database is created by
Oracle for reliable and interoperable services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4585</identifier>
 <datestamp>2010-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4585</id><created>2010-05-06</created><authors><author><keyname>Peter</keyname><forenames>S. John</forenames></author><author><keyname>Victor</keyname><forenames>S. P.</forenames></author></authors><title>A Novel Algorithm for Informative Meta Similarity Clusters Using Minimum
  Spanning Tree</title><categories>cs.OH</categories><comments>IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The minimum spanning tree clustering algorithm is capable of detecting
clusters with irregular boundaries. In this paper we propose two minimum
spanning trees based clustering algorithm. The first algorithm produces k
clusters with center and guaranteed intra-cluster similarity. The radius and
diameter of k clusters are computed to find the tightness of k clusters. The
variance of the k clusters are also computed to find the compactness of the
clusters. The second algorithm is proposed to create a dendrogram using the k
clusters as objects with guaranteed inter-cluster similarity. The algorithm is
also finds central cluster from the k number of clusters. The first algorithm
uses divisive approach, where as the second algorithm uses agglomerative
approach. In this paper we used both the approaches to find Informative Meta
similarity clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4592</identifier>
 <datestamp>2011-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4592</id><created>2010-05-25</created><authors><author><keyname>Urban</keyname><forenames>Josef</forenames></author><author><keyname>Sutcliffe</keyname><forenames>Geoff</forenames></author></authors><title>Automated Reasoning and Presentation Support for Formalizing Mathematics
  in Mizar</title><categories>cs.AI</categories><comments>To appear in 10th International Conference on. Artificial
  Intelligence and Symbolic Computation AISC 2010</comments><journal-ref>Intelligent Computer Mathematics 2010, LNCS 6167, pp. 132-146</journal-ref><doi>10.1007/978-3-642-14128-7_12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a combination of several automated reasoning and proof
presentation tools with the Mizar system for formalization of mathematics. The
combination forms an online service called MizAR, similar to the SystemOnTPTP
service for first-order automated reasoning. The main differences to
SystemOnTPTP are the use of the Mizar language that is oriented towards human
mathematicians (rather than the pure first-order logic used in SystemOnTPTP),
and setting the service in the context of the large Mizar Mathematical Library
of previous theorems,definitions, and proofs (rather than the isolated problems
that are solved in SystemOnTPTP). These differences poses new challenges and
new opportunities for automated reasoning and for proof presentation tools.
This paper describes the overall structure of MizAR, and presents the automated
reasoning systems and proof presentation tools that are combined to make MizAR
a useful mathematical service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4616</identifier>
 <datestamp>2010-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4616</id><created>2010-05-25</created><authors><author><keyname>Lu</keyname><forenames>Lunjin</forenames></author></authors><title>Parametrizing Program Analysis by Lifting to Cardinal Power Domains</title><categories>cs.PL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A parametric analysis is an analysis whose input and output are parametrized
with a number of parameters which can be instantiated to abstract properties
after analysis is completed. This paper proposes to use Cousot and Cousot's
Cardinal power domain to capture functional dependencies of analysis output on
its input and obtain a parametric analysis by parametrizing a non-parametric
base analysis. We illustrate the method by parametrizing a $\pos$ based
groundness analysis of logic programs to a parametric groundness analysis. In
addition, a prototype implementation shows that generality of the parametric
groundness analysis comes with a negligible extra cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4652</identifier>
 <datestamp>2010-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4652</id><created>2010-05-25</created><updated>2010-06-23</updated><authors><author><keyname>He</keyname><forenames>Meng</forenames></author><author><keyname>Munro</keyname><forenames>J. Ian</forenames></author></authors><title>Succinct Representations of Dynamic Strings</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rank and select operations over a string of length n from an alphabet of
size $\sigma$ have been used widely in the design of succinct data structures.
In many applications, the string itself need be maintained dynamically,
allowing characters of the string to be inserted and deleted. Under the word
RAM model with word size $w=\Omega(\lg n)$, we design a succinct representation
of dynamic strings using $nH_0 + o(n)\lg\sigma + O(w)$ bits to support rank,
select, insert and delete in $O(\frac{\lg n}{\lg\lg n}(\frac{\lg \sigma}{\lg\lg
n}+1))$ time. When the alphabet size is small, i.e. when $\sigma = O(\polylog
(n))$, including the case in which the string is a bit vector, these operations
are supported in $O(\frac{\lg n}{\lg\lg n})$ time. Our data structures are more
efficient than previous results on the same problem, and we have applied them
to improve results on the design and construction of space-efficient text
indexes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4661</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4661</id><created>2010-05-22</created><updated>2013-04-21</updated><authors><author><keyname>Goldstein</keyname><forenames>Norman J.</forenames></author></authors><title>Nonsingular Efficient Modeling of Rotations in 3-space using three
  components</title><categories>cs.MS cs.CG cs.CR</categories><comments>5 pages, no figures, presented at ICIAM 2011, July 18-21, Vancouver,
  BC</comments><acm-class>G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article introduces yet another representation of rotations in 3-space.
The rotations form a 3-dimensional projective space, which fact has not been
exploited in Computer Science. We use the four affine patches of this
projective space to parametrize the rotations. This affine patch representation
is more compact than quaternions (which require 4 components for calculations),
encompasses the entire rotation group without singularities (unlike the Euler
angles and rotation vector approaches), and requires only ratios of linear or
quadratic polynomials for basic computations (unlike the Euler angles and
rotation vector approaches which require transcendental functions).
  As an example, we derive the differential equation for the integration of
angular velocity using this affine patch representation of rotations. We remark
that the complexity of this equation is the same as the corresponding
quaternion equation, but has advantages over the quaternion approach e.g.
renormalization to unit length is not required, and state space has no dead
directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4695</identifier>
 <datestamp>2010-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4695</id><created>2010-05-25</created><authors><author><keyname>Malik</keyname><forenames>Tanu</forenames></author><author><keyname>Prasad</keyname><forenames>Raghvendra</forenames></author><author><keyname>Patil</keyname><forenames>Sanket</forenames></author><author><keyname>Chaudhary</keyname><forenames>Amitabh</forenames></author><author><keyname>Venkatasubramanian</keyname><forenames>Venkat</forenames></author></authors><title>Providing Scalable Data Services in Ubiquitous Networks</title><categories>cs.DB cs.NI</categories><comments>12pages, 4 figures, 2 algorithms, Workshop for Ubiquitous Data
  Management</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topology is a fundamental part of a network that governs connectivity between
nodes, the amount of data flow and the efficiency of data flow between nodes.
In traditional networks, due to physical limitations, topology remains static
for the course of the network operation. Ubiquitous data networks (UDNs),
alternatively, are more adaptive and can be configured for changes in their
topology. This flexibility in controlling their topology makes them very
appealing and an attractive medium for supporting &quot;anywhere, any place&quot;
communication. However, it raises the problem of designing a dynamic topology.
The dynamic topology design problem is of particular interest to application
service providers who need to provide cost-effective data services on a
ubiquitous network. In this paper we describe algorithms that decide when and
how the topology should be reconfigured in response to a change in the data
communication requirements of the network. In particular, we describe and
compare a greedy algorithm, which is often used for topology reconfiguration,
with a non-greedy algorithm based on metrical task systems. Experiments show
the algorithm based on metrical task system has comparable performance to the
greedy algorithm at a much lower reconfiguration cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4697</identifier>
 <datestamp>2010-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4697</id><created>2010-05-25</created><updated>2010-08-25</updated><authors><author><keyname>Bransen</keyname><forenames>Jeroen</forenames></author></authors><title>The Lambek-Grishin calculus is NP-complete</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Lambek-Grishin calculus LG is the symmetric extension of the
non-associative Lambek calculus NL. In this paper we prove that the
derivability problem for LG is NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4713</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4713</id><created>2010-05-25</created><authors><author><keyname>Martinez</keyname><forenames>Enrique</forenames></author><author><keyname>Monasterio</keyname><forenames>Paul R</forenames></author><author><keyname>Marian</keyname><forenames>Jaime</forenames></author></authors><title>Billion-atom Synchronous Parallel Kinetic Monte Carlo Simulations of
  Critical 3D Ising Systems</title><categories>cond-mat.stat-mech cond-mat.mtrl-sci cs.DS</categories><doi>10.1016/j.jcp.2010.11.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An extension of the synchronous parallel kinetic Monte Carlo (pkMC) algorithm
developed by Martinez {\it et al} [{\it J.\ Comp.\ Phys.} {\bf 227} (2008)
3804] to discrete lattices is presented. The method solves the master equation
synchronously by recourse to null events that keep all processors time clocks
current in a global sense. Boundary conflicts are rigorously solved by adopting
a chessboard decomposition into non-interacting sublattices. We find that the
bias introduced by the spatial correlations attendant to the sublattice
decomposition is within the standard deviation of the serial method, which
confirms the statistical validity of the method. We have assessed the parallel
efficiency of the method and find that our algorithm scales consistently with
problem size and sublattice partition. We apply the method to the calculation
of scale-dependent critical exponents in billion-atom 3D Ising systems, with
very good agreement with state-of-the-art multispin simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4714</identifier>
 <datestamp>2010-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4714</id><created>2010-05-25</created><updated>2010-12-13</updated><authors><author><keyname>De</keyname><forenames>Sushovan</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author></authors><title>Defining and Mining Functional Dependencies in Probabilistic Databases</title><categories>cs.DB</categories><comments>9 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Functional dependencies -- traditional, approximate and conditional are of
critical importance in relational databases, as they inform us about the
relationships between attributes. They are useful in schema normalization, data
rectification and source selection. Most of these were however developed in the
context of deterministic data. Although uncertain databases have started
receiving attention, these dependencies have not been defined for them, nor are
fast algorithms available to evaluate their confidences. This paper defines the
logical extensions of various forms of functional dependencies for
probabilistic databases and explores the connections between them. We propose a
pruning-based exact algorithm to evaluate the confidence of functional
dependencies, a Monte-Carlo based algorithm to evaluate the confidence of
approximate functional dependencies and algorithms for their conditional
counterparts in probabilistic databases. Experiments are performed on both
synthetic and real data evaluating the performance of these algorithms in
assessing the confidence of dependencies and mining them from data. We believe
that having these dependencies and algorithms available for probabilistic
databases will drive adoption of probabilistic data storage in the industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4717</identifier>
 <datestamp>2012-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4717</id><created>2010-05-25</created><updated>2012-06-29</updated><authors><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Lin</keyname><forenames>Qihang</forenames></author><author><keyname>Kim</keyname><forenames>Seyoung</forenames></author><author><keyname>Carbonell</keyname><forenames>Jaime G.</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Smoothing proximal gradient method for general structured sparse
  regression</title><categories>stat.ML cs.LG math.OC stat.AP stat.CO</categories><comments>Published in at http://dx.doi.org/10.1214/11-AOAS514 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS514</report-no><journal-ref>Annals of Applied Statistics 2012, Vol. 6, No. 2, 719-752</journal-ref><doi>10.1214/11-AOAS514</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of estimating high-dimensional regression models
regularized by a structured sparsity-inducing penalty that encodes prior
structural information on either the input or output variables. We consider two
widely adopted types of penalties of this kind as motivating examples: (1) the
general overlapping-group-lasso penalty, generalized from the group-lasso
penalty; and (2) the graph-guided-fused-lasso penalty, generalized from the
fused-lasso penalty. For both types of penalties, due to their nonseparability
and nonsmoothness, developing an efficient optimization method remains a
challenging problem. In this paper we propose a general optimization approach,
the smoothing proximal gradient (SPG) method, which can solve structured sparse
regression problems with any smooth convex loss under a wide spectrum of
structured sparsity-inducing penalties. Our approach combines a smoothing
technique with an effective proximal gradient method. It achieves a convergence
rate significantly faster than the standard first-order methods, subgradient
methods, and is much more scalable than the most widely used interior-point
methods. The efficiency and scalability of our method are demonstrated on both
simulation experiments and real genetic data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4732</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4732</id><created>2010-05-26</created><updated>2015-02-03</updated><authors><author><keyname>Nguyen</keyname><forenames>Nam H.</forenames></author><author><keyname>Drineas</keyname><forenames>Petros</forenames></author><author><keyname>Tran</keyname><forenames>Trac D.</forenames></author></authors><title>Tensor sparsification via a bound on the spectral norm of random tensors</title><categories>math.NA cs.NA</categories><comments>33 pages; Information and Inference (A journal of the IMA), 2014</comments><acm-class>G.1; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an order-$d$ tensor $\tensor A \in \R^{n \times n \times...\times n}$,
we present a simple, element-wise sparsification algorithm that zeroes out all
sufficiently small elements of $\tensor A$, keeps all sufficiently large
elements of $\tensor A$, and retains some of the remaining elements with
probabilities proportional to the square of their magnitudes. We analyze the
approximation accuracy of the proposed algorithm using a powerful inequality
that we derive. This inequality bounds the spectral norm of a random tensor and
is of independent interest. As a result, we obtain novel bounds for the tensor
sparsification problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4752</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4752</id><created>2010-05-26</created><authors><author><keyname>Hiemstra</keyname><forenames>Djoerd</forenames></author><author><keyname>Mihajlovic</keyname><forenames>Vojkan</forenames></author></authors><title>A database approach to information retrieval: The remarkable
  relationship between language models and region models</title><categories>cs.IR cs.DB</categories><comments>Published as CTIT Technical Report 05-35</comments><report-no>TR-CTIT-10-15</report-no><acm-class>H.3.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this report, we unify two quite distinct approaches to information
retrieval: region models and language models. Region models were developed for
structured document retrieval. They provide a well-defined behaviour as well as
a simple query language that allows application developers to rapidly develop
applications. Language models are particularly useful to reason about the
ranking of search results, and for developing new ranking approaches. The
unified model allows application developers to define complex language modeling
approaches as logical queries on a textual database. We show a remarkable
one-to-one relationship between region queries and the language models they
represent for a wide variety of applications: simple ad-hoc search,
cross-language retrieval, video retrieval, and web search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4762</identifier>
 <datestamp>2010-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4762</id><created>2010-05-26</created><authors><author><keyname>Heeren</keyname><forenames>Bastiaan</forenames></author><author><keyname>Jeuring</keyname><forenames>Johan</forenames></author></authors><title>Adapting Mathematical Domain Reasoners</title><categories>cs.MS</categories><comments>To appear in The 9th International Conference on Mathematical
  Knowledge Management: MKM 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mathematical learning environments help students in mastering mathematical
knowledge. Mature environments typically offer thousands of interactive
exercises. Providing feedback to students solving interactive exercises
requires domain reasoners for doing the exercise-specific calculations. Since a
domain reasoner has to solve an exercise in the same way a student should solve
it, the structure of domain reasoners should follow the layered structure of
the mathematical domains. Furthermore, learners, teachers, and environment
builders have different requirements for adapting domain reasoners, such as
providing more details, disallowing or enforcing certain solutions, and
combining multiple mathematical domains in a new domain. In previous work we
have shown how domain reasoners for solving interactive exercises can be
expressed in terms of rewrite strategies, rewrite rules, and views. This paper
shows how users can adapt and configure such domain reasoners to their own
needs. This is achieved by enabling users to explicitly communicate the
components that are used for solving an exercise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4769</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4769</id><created>2010-05-26</created><updated>2013-02-09</updated><authors><author><keyname>Sattari</keyname><forenames>Pegah</forenames></author><author><keyname>Markopoulou</keyname><forenames>Athina</forenames></author><author><keyname>Fragouli</keyname><forenames>Christina</forenames></author><author><keyname>Gjoka</keyname><forenames>Minas</forenames></author></authors><title>A Network Coding Approach to Loss Tomography</title><categories>cs.IT cs.NI math.IT</categories><doi>10.1109/TIT.2012.2236916</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network tomography aims at inferring internal network characteristics based
on measurements at the edge of the network. In loss tomography, in particular,
the characteristic of interest is the loss rate of individual links and
multicast and/or unicast end-to-end probes are typically used. Independently,
recent advances in network coding have shown that there are advantages from
allowing intermediate nodes to process and combine, in addition to just
forward, packets. In this paper, we study the problem of loss tomography in
networks with network coding capabilities. We design a framework for estimating
link loss rates, which leverages network coding capabilities, and we show that
it improves several aspects of tomography including the identifiability of
links, the trade-off between estimation accuracy and bandwidth efficiency, and
the complexity of probe path selection. We discuss the cases of inferring link
loss rates in a tree topology and in a general topology. In the latter case,
the benefits of our approach are even more pronounced compared to standard
techniques, but we also face novel challenges, such as dealing with cycles and
multiple paths between sources and receivers. Overall, this work makes the
connection between active network tomography and network coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4771</identifier>
 <datestamp>2010-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4771</id><created>2010-05-26</created><authors><author><keyname>Farashahi</keyname><forenames>Reza R.</forenames></author><author><keyname>Shparlinski</keyname><forenames>Igor E.</forenames></author></authors><title>Pseudorandom Bits From Points on Elliptic Curves</title><categories>math.NT cs.CR</categories><msc-class>11G05, 11T23, 14G50, 94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\E$ be an elliptic curve over a finite field $\F_{q}$ of $q$ elements,
with $\gcd(q,6)=1$, given by an affine Weierstra\ss\ equation. We also use
$x(P)$ to denote the $x$-component of a point $P = (x(P),y(P))\in \E$. We
estimate character sums of the form $$ \sum_{n=1}^N \chi\(x(nP)x(nQ)\) \quad
\text{and}\quad \sum_{n_1, \ldots, n_k=1}^N \psi\(\sum_{j=1}^k c_j
x\(\(\prod_{i =1}^j n_i\) R\)\) $$ on average over all $\F_q$ rational points
$P$, $Q$ and $R$ on $\E$, where $\chi$ is a quadratic character, $\psi$ is a
nontrivial additive character in $\F_q$ and $(c_1, \ldots, c_k)\in \F_q^k$ is a
non-zero vector. These bounds confirm several recent conjectures of D. Jao, D.
Jetchev and R. Venkatesan, related to extracting random bits from various
sequences of points on elliptic curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4774</identifier>
 <datestamp>2010-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4774</id><created>2010-05-26</created><authors><author><keyname>Sudeendra</keyname><forenames>Sumanth</forenames></author><author><keyname>Saini</keyname><forenames>Megha</forenames></author><author><keyname>Rao</keyname><forenames>Shrisha</forenames></author></authors><title>Fairness in Combinatorial Auctions</title><categories>cs.GT cs.MA</categories><comments>22 pages; extended version of arXiv:0809.2168</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The market economy deals with many interacting agents such as buyers and
sellers who are autonomous intelligent agents pursuing their own interests. One
such multi-agent system (MAS) that plays an important role in auctions is the
combinatorial auctioning system (CAS). We use this framework to define our
concept of fairness in terms of what we call as &quot;basic fairness&quot; and &quot;extended
fairness&quot;. The assumptions of quasilinear preferences and dominant strategies
are taken into consideration while explaining fairness. We give an algorithm to
ensure fairness in a CAS using a Generalized Vickrey Auction (GVA). We use an
algorithm of Sandholm to achieve optimality. Basic and extended fairness are
then analyzed according to the dominant strategy solution concept.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4798</identifier>
 <datestamp>2010-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4798</id><created>2010-05-26</created><updated>2010-07-29</updated><authors><author><keyname>Berka</keyname><forenames>Alexander Victor</forenames></author></authors><title>Introduction to the Report &quot;Interlanguages and Synchronic Models of
  Computation.&quot;</title><categories>cs.PL</categories><comments>Introduction to preprint report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel language system has given rise to promising alternatives to standard
formal and processor network models of computation. An interstring linked with
a abstract machine environment, shares sub-expressions, transfers data, and
spatially allocates resources for the parallel evaluation of dataflow. Formal
models called the a-Ram family are introduced, designed to support interstring
programming languages (interlanguages). Distinct from dataflow, graph
rewriting, and FPGA models, a-Ram instructions are bit level and execute in
situ. They support sequential and parallel languages without the space/time
overheads associated with the Turing Machine and lambda-calculus, enabling
massive programs to be simulated. The devices of one a-Ram model, called the
Synchronic A-Ram, are fully connected and simpler than FPGA LUT's. A compiler
for an interlanguage called Space, has been developed for the Synchronic A-Ram.
Space is MIMD. strictly typed, and deterministic. Barring memory allocation and
compilation, modules are referentially transparent. At a high level of
abstraction, modules exhibit a state transition system, aiding verification.
Data structures and parallel iteration are straightforward to implement, and
allocations of sub-processes and data transfers to resources are implicit.
Space points towards highly connected architectures called Synchronic Engines,
that are more general purpose than systolic arrays and GPUs, and bypass
programmability and conflict issues associated with multicores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4815</identifier>
 <datestamp>2010-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4815</id><created>2010-05-26</created><authors><author><keyname>Artikis</keyname><forenames>Alexander</forenames></author></authors><title>A Formal Specification of Dynamic Protocols for Open Agent Systems</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-agent systems where the agents are developed by parties with competing
interests, and where there is no access to an agent's internal state, are often
classified as `open'. The member agents of such systems may inadvertently fail
to, or even deliberately choose not to, conform to the system specification.
Consequently, it is necessary to specify the normative relations that may exist
between the agents, such as permission, obligation, and institutional power.
The specification of open agent systems of this sort is largely seen as a
design-time activity. Moreover, there is no support for run-time specification
modification. Due to environmental, social, or other conditions, however, it is
often required to revise the specification during the system execution. To
address this requirement, we present an infrastructure for `dynamic'
specifications, that is, specifications that may be modified at run-time by the
agents. The infrastructure consists of well-defined procedures for proposing a
modification of the `rules of the game', as well as decision-making over and
enactment of proposed modifications. We evaluate proposals for rule
modification by modelling a dynamic specification as a metric space, and by
considering the effects of accepting a proposal on system utility. Furthermore,
we constrain the enactment of proposals that do not meet the evaluation
criteria. We employ the action language C+ to formalise dynamic specifications,
and the `Causal Calculator' implementation of C+ to execute the specifications.
We illustrate our infrastructure by presenting a dynamic specification of a
resource-sharing protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4826</identifier>
 <datestamp>2010-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4826</id><created>2010-05-26</created><authors><author><keyname>Margenstern</keyname><forenames>Maurice</forenames></author></authors><title>A new weakly universal cellular automaton in the 3D hyperbolic space
  with two states</title><categories>cs.FL</categories><comments>38p., 25 figures, 8 tables</comments><msc-class>68R05</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show a construction of a weakly universal cellular
automaton in the 3D hyperbolic space with two states. The cellular automaton is
rotation invariant and, moreover, based on a new implementation of a railway
circuit in the dodecagrid,the construction is a truly 3D-one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4834</identifier>
 <datestamp>2010-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4834</id><created>2010-05-26</created><authors><author><keyname>Paolini</keyname><forenames>Enrico</forenames></author><author><keyname>Flanagan</keyname><forenames>Mark F.</forenames></author><author><keyname>Chiani</keyname><forenames>Marco</forenames></author><author><keyname>Fossorier</keyname><forenames>Marc P. C.</forenames></author></authors><title>Spectral Shape of Check-Hybrid GLDPC Codes</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures. Presented at the IEEE ICC 2010, Cape Town, South
  Africa. A minor typo in equation (9) has been corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the asymptotic exponent of both the weight spectrum and
the stopping set size spectrum for a class of generalized low-density
parity-check (GLDPC) codes. Specifically, all variable nodes (VNs) are assumed
to have the same degree (regular VN set), while the check node (CN) set is
assumed to be composed of a mixture of different linear block codes (hybrid CN
set). A simple expression for the exponent (which is also referred to as the
growth rate or the spectral shape) is developed. This expression is consistent
with previous results, including the case where the normalized weight or
stopping set size tends to zero. Furthermore, it is shown how certain symmetry
properties of the local weight distribution at the CNs induce a symmetry in the
overall weight spectral shape function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4844</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4844</id><created>2010-05-26</created><authors><author><keyname>Monniaux</keyname><forenames>David</forenames></author></authors><title>Automatic Modular Abstractions for Template Numerical Constraints</title><categories>cs.LO cs.PL</categories><comments>final version submitted to LMCS</comments><report-no>LMCS-2009-385</report-no><acm-class>F.3.1; F.3.2; F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 6, Issue 3 (July 20,
  2010) lmcs:1015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for automatically generating abstract transformers for
static analysis by abstract interpretation. The method focuses on linear
constraints on programs operating on rational, real or floating-point variables
and containing linear assignments and tests. Given the specification of an
abstract domain, and a program block, our method automatically outputs an
implementation of the corresponding abstract transformer. It is thus a form of
program transformation. In addition to loop-free code, the same method also
applies for obtaining least fixed points as functions of the precondition,
which permits the analysis of loops and recursive functions. The motivation of
our work is data-flow synchronous programming languages, used for building
control-command embedded systems, but it also applies to imperative and
functional programming. Our algorithms are based on quantifier elimination and
symbolic manipulation techniques over linear arithmetic formulas. We also give
less general results for nonlinear constraints and nonlinear program
constructs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4846</identifier>
 <datestamp>2010-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4846</id><created>2010-05-26</created><authors><author><keyname>Aldous</keyname><forenames>David J.</forenames></author></authors><title>When Knowing Early Matters: Gossip, Percolation and Nash Equilibria</title><categories>math.PR cs.GT</categories><comments>This outline version was written in July 2007 to accompany a talk at
  the ICTP workshop &quot;Common Concepts in Statistical Physics and Computer
  Science&quot;. One of the topics (first passage percolation on the $N \times N$
  torus with short and long range interactions) has now been studied rigorously
  by Chatterjee and Durrett (2010)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continually arriving information is communicated through a network of $n$
agents, with the value of information to the $j$'th recipient being a
decreasing function of $j/n$, and communication costs paid by recipient.
Regardless of details of network and communication costs, the social optimum
policy is to communicate arbitrarily slowly. But selfish agent behavior leads
to Nash equilibria which (in the $n \to \infty$ limit) may be efficient (Nash
payoff $=$ social optimum payoff) or wasteful ($0 &lt; $ Nash payoff $&lt;$ social
optimum payoff) or totally wasteful (Nash payoff $=0$). We study the cases of
the complete network (constant communication costs between all agents), the
grid with only nearest-neighbor communication, and the grid with communication
cost a function of distance. The main technical tool is analysis of the
associated first passage percolation process or SI epidemic (representing
spread of one item of information) and in particular its &quot;window width&quot;, the
time interval during which most agents learn the item. Many arguments are just
outlined, not intended as complete rigorous proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4853</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4853</id><created>2010-05-26</created><updated>2010-06-02</updated><authors><author><keyname>Kochman</keyname><forenames>Yuval</forenames></author><author><keyname>Zamir</keyname><forenames>Ram</forenames></author></authors><title>Analog Matching of Colored Sources to Colored Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. Info. theory, July 2008. Revised May 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analog (uncoded) transmission provides a simple and robust scheme for
communicating a Gaussian source over a Gaussian channel under the mean squared
error (MSE) distortion measure. Unfortunately, its performance is usually
inferior to the all-digital, separation-based source-channel coding solution,
which requires exact knowledge of the channel at the encoder. The loss comes
from the fact that except for very special cases, e.g. white source and channel
of matching bandwidth (BW), it is impossible to achieve perfect matching of
source to channel and channel to source by linear means. We show that by
combining prediction and modulo-lattice operations, it is possible to match any
colored Gaussian source to any colored Gaussian noise channel (of possibly
different BW), hence achieve Shannon's optimum attainable performance $R(D)=C$.
Furthermore, when the source and channel BWs are equal (but otherwise their
spectra are arbitrary), this scheme is asymptotically robust in the sense that
for high signal-to-noise ratio a single encoder (independent of the noise
variance) achieves the optimum performance. The derivation is based upon a
recent modulo-lattice modulation scheme for transmitting a Wyner-Ziv source
over a dirty-paper channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4874</identifier>
 <datestamp>2010-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4874</id><created>2010-05-26</created><authors><author><keyname>Scheder</keyname><forenames>Dominik</forenames></author></authors><title>Using a Skewed Hamming Distance to Speed Up Deterministic Local Search</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Schoening presents a simple randomized algorithm for (d,k)-CSP problems with
running time (d(k-1)/k)^n poly(n). Here, d is the number of colors, k is the
size of the constraints, and n is the number of variables. A derandomized
version of this, given by Dantsin et al., achieves a running time of
(dk/(k+1))^n poly(n), inferior to Schoening's. We come up with a simple
modification of the deterministic algorithm, achieving a running time of
(d(k-1)/k * k^d/(k^d-1))^n \poly(n). Though not completely eleminating the gap,
this comes very close to the randomized bound for all but very small values of
d. Our main idea is to define a graph structure on the set of d colors to speed
up local search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4877</identifier>
 <datestamp>2015-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4877</id><created>2010-05-26</created><updated>2015-02-05</updated><authors><author><keyname>Brandt</keyname><forenames>Felix</forenames></author></authors><title>Set-Monotonicity Implies Kelly-Strategyproofness</title><categories>cs.MA</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the strategic manipulation of set-valued social choice
functions according to Kelly's preference extension, which prescribes that one
set of alternatives is preferred to another if and only if all elements of the
former are preferred to all elements of the latter. It is shown that
set-monotonicity---a new variant of Maskin-monotonicity---implies
Kelly-strategyproofness in comprehensive subdomains of the linear domain.
Interestingly, there are a handful of appealing Condorcet extensions---such as
the top cycle, the minimal covering set, and the bipartisan set---that satisfy
set-monotonicity even in the unrestricted linear domain, thereby answering
questions raised independently by Barber\`a (1977) and Kelly (1977).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4882</identifier>
 <datestamp>2010-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4882</id><created>2010-05-26</created><authors><author><keyname>Ghosh</keyname><forenames>Rumi</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author></authors><title>Predicting Influential Users in Online Social Networks</title><categories>cs.CY physics.soc-ph</categories><journal-ref>The fourth SNA-KDD Workshop, Held in conjunction with The 16th ACM
  SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD
  2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Who are the influential people in an online social network? The answer to
this question depends not only on the structure of the network, but also on
details of the dynamic processes occurring on it. We classify these processes
as conservative and non-conservative. A random walk on a network is an example
of a conservative dynamic process, while information spread is
non-conservative. The influence models used to rank network nodes can be
similarly classified, depending on the dynamic process they implicitly emulate.
We claim that in order to correctly rank network nodes, the influence model has
to match the details of the dynamic process. We study a real-world network on
the social news aggregator Digg, which allows users to post and vote for news
stories. We empirically define influence as the number of in-network votes a
user's post generates. This influence measure, and the resulting ranking,
arises entirely from the dynamics of voting on Digg, which represents
non-conservative information flow.
  We then compare predictions of different influence models with this empirical
estimate of influence. The results show that non-conservative models are better
able to predict influential users on Digg. We find that normalized
alpha-centrality metric turns out to be one of the best predictors of
influence. We also present a simple algorithm for computing this metric and the
associated mathematical formulation and analytical proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4895</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4895</id><created>2010-05-26</created><authors><author><keyname>Aubert</keyname><forenames>Sebastien</forenames></author><author><keyname>Mohaisen</keyname><forenames>Manar</forenames></author><author><keyname>Nouvel</keyname><forenames>Fabienne</forenames></author><author><keyname>Chang</keyname><forenames>KyungHi</forenames></author></authors><title>Parallel QR decomposition in LTE-A systems</title><categories>cs.OH cs.IT math.IT</categories><comments>The eleventh IEEE International Workshop on Signal Processing
  Advances for Wireless Communications, 5 pages, 4 figures, 4 algorithms, 1
  table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The QR Decomposition (QRD) of communication channel matrices is a fundamental
prerequisite to several detection schemes in Multiple-Input Multiple-Output
(MIMO) communication systems. Herein, the main feature of the QRD is to
transform the non-causal system into a causal system, where consequently
efficient detection algorithms based on the Successive Interference
Cancellation (SIC) or Sphere Decoder (SD) become possible. Also, QRD can be
used as a light but efficient antenna selection scheme. In this paper, we
address the study of the QRD methods and compare their efficiency in terms of
computational complexity and error rate performance. Moreover, a particular
attention is paid to the parallelism of the QRD algorithms since it reduces the
latency of the matrix factorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4897</identifier>
 <datestamp>2010-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4897</id><created>2010-05-26</created><authors><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>Russell</keyname><forenames>Alexander</forenames></author></authors><title>How close can we come to a parity function when there isn't one?</title><categories>math.CO cs.CC math.GR math.RT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a group G such that there is no homomorphism f:G to {+1,-1}. In that
case, how close can we come to such a homomorphism? We show that if f has zero
expectation, then the probability that f(xy) = f(x) f(y), where x, y are chosen
uniformly and independently from G, is at most 1/2(1+1/sqrt{d}), where d is the
dimension of G's smallest nontrivial irreducible representation. For the
alternating group A_n, for instance, d=n-1. On the other hand, A_n contains a
subgroup isomorphic to S_{n-2}, whose parity function we can extend to obtain
an f for which this probability is 1/2(1+1/{n \choose 2}). Thus the extent to
which f can be &quot;more homomorphic&quot; than a random function from A_n to {+1,-1}
lies between O(n^{-1/2}) and Omega(n^{-2}).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4906</identifier>
 <datestamp>2010-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4906</id><created>2010-05-26</created><authors><author><keyname>Moed</keyname><forenames>Henk F.</forenames></author></authors><title>The Source-Normalized Impact per Paper (SNIP) is a valid and
  sophisticated indicator of journal citation impact</title><categories>cs.DL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a reply to the article &quot;Scopus's Source Normalized Impact per
Paper (SNIP) versus a Journal Impact Factor based on Fractional Counting of
Citations&quot;, published by Loet Leydesdorff and Tobias Opthof (arXiv:1004.3580v2
[cs.DL]). It clarifies the relationship between SNIP and Elsevier's Scopus.
Since Leydesdorff and Opthof's description of SNIP is not complete, it
indicates four key differences between SNIP and the indicator proposed by the
two authors, and argues why the former is more valid than the latter.
Nevertheless, the idea of fractional citation counting deserves further
exploration. The paper discusses difficulties that arise if one attempts to
apply this principle at the level of individual (citing) papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4951</identifier>
 <datestamp>2010-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4951</id><created>2010-05-26</created><updated>2010-06-08</updated><authors><author><keyname>Karmakar</keyname><forenames>Sanjay</forenames></author><author><keyname>Varanasi</keyname><forenames>Mahesh K.</forenames></author></authors><title>The diversity-multiplexing tradeoff of the symmetric MIMO half-duplex
  relay channel</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn.</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper has been withdrawn due to an error in one of the equations in the
extended portion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4963</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4963</id><created>2010-05-26</created><authors><author><keyname>Plangprasopchok</keyname><forenames>Anon</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author><author><keyname>Getoor</keyname><forenames>Lise</forenames></author></authors><title>Integrating Structured Metadata with Relational Affinity Propagation</title><categories>cs.AI</categories><comments>6 Pages, To appear at AAAI Workshop on Statistical Relational AI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structured and semi-structured data describing entities, taxonomies and
ontologies appears in many domains. There is a huge interest in integrating
structured information from multiple sources; however integrating structured
data to infer complex common structures is a difficult task because the
integration must aggregate similar structures while avoiding structural
inconsistencies that may appear when the data is combined. In this work, we
study the integration of structured social metadata: shallow personal
hierarchies specified by many individual users on the SocialWeb, and focus on
inferring a collection of integrated, consistent taxonomies. We frame this task
as an optimization problem with structural constraints. We propose a new
inference algorithm, which we refer to as Relational Affinity Propagation (RAP)
that extends affinity propagation (Frey and Dueck 2007) by introducing
structural constraints. We validate the approach on a real-world social media
dataset, collected from the photosharing website Flickr. Our empirical results
show that our proposed approach is able to construct deeper and denser
structures compared to an approach using only the standard affinity propagation
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4973</identifier>
 <datestamp>2013-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4973</id><created>2010-05-26</created><updated>2012-03-21</updated><authors><author><keyname>Saito</keyname><forenames>Mutsuo</forenames></author><author><keyname>Matsumoto</keyname><forenames>Makoto</forenames></author></authors><title>Variants of Mersenne Twister Suitable for Graphic Processors</title><categories>cs.MS</categories><comments>23 pages, 6 figures</comments><journal-ref>Transactions on Mathematical Software, 39 (2013), 12:1--12:20</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a type of pseudorandom number generator, Mersenne Twister
for Graphic Processor (MTGP), for efficient generation on graphic processessing
units (GPUs). MTGP supports large state sizes such as 11213 bits, and uses the
high parallelism of GPUs in computing many steps of the recursion in parallel.
The second proposal is a parameter-set generator for MTGP, named MTGP Dynamic
Creator (MTGPDC). MT- GPDC creates up to 2^32 distinct parameter sets which
generate sequences with high-dimensional uniformity. This facility is suitable
for a large grid of GPUs where each GPU requires separate random number
streams. MTGP is based on linear recursion over the two-element field, and has
better high-dimensional equidistribution than the Mersenne Twister pseudorandom
number generator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4975</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4975</id><created>2010-05-26</created><authors><author><keyname>de Carvalho</keyname><forenames>Rogerio Atem</forenames></author><author><keyname>Manh&#xe3;es</keyname><forenames>Rodrigo Soares</forenames></author><author><keyname>Silva</keyname><forenames>Fernando Luis de Carvalho e</forenames></author></authors><title>Filling the Gap between Business Process Modeling and Behavior Driven
  Development</title><categories>cs.SE</categories><comments>5 pages, 1 figure, original work</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Behavior Driven Development (NORTH, 2006) is a specification technique that
is growing in acceptance in the Agile methods communities. BDD allows to
securely verify that all functional requirements were treated properly by
source code, by connecting the textual description of these requirements to
tests.
  On the other side, the Enterprise Information Systems (EIS) researchers and
practitioners defends the use of Business Process Modeling (BPM) to, before
defining any part of the system, perform the modeling of the system's
underlying business process. Therefore, it can be stated that, in the case of
EIS, functional requirements are obtained by identifying Use Cases from the
business process models.
  The aim of this paper is, in a narrower perspective, to propose the use of
Finite State Machines (FSM) to model business process and then connect them to
the BDD machinery, thus driving better quality for EIS. In a broader
perspective, this article aims to provoke a discussion on the mapping of the
various BPM notations, since there isn't a real standard for business process
modeling (Moller et al., 2007), to BDD.
  Firstly a historical perspective of the evolution of previous proposals from
which this one emerged will be presented, and then the reasons to change from
Model Driven Development (MDD) to BDD will be presented also in a historical
perspective. Finally the proposal of using FSM, specifically by using UML
Statechart diagrams, will be presented, followed by some conclusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4984</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4984</id><created>2010-05-27</created><authors><author><keyname>Athanasopoulou</keyname><forenames>Eleftheria</forenames></author><author><keyname>Bui</keyname><forenames>Loc</forenames></author><author><keyname>Ji</keyname><forenames>Tianxiong</forenames></author><author><keyname>Srikant</keyname><forenames>R.</forenames></author><author><keyname>Stoylar</keyname><forenames>Alexander</forenames></author></authors><title>Backpressure-based Packet-by-Packet Adaptive Routing in Communication
  Networks</title><categories>cs.NI</categories><comments>13 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Backpressure-based adaptive routing algorithms where each packet is routed
along a possibly different path have been extensively studied in the
literature. However, such algorithms typically result in poor delay performance
and involve high implementation complexity. In this paper, we develop a new
adaptive routing algorithm built upon the widely-studied back-pressure
algorithm. We decouple the routing and scheduling components of the algorithm
by designing a probabilistic routing table which is used to route packets to
per-destination queues. The scheduling decisions in the case of wireless
networks are made using counters called shadow queues. The results are also
extended to the case of networks which employ simple forms of network coding.
In that case, our algorithm provides a low-complexity solution to optimally
exploit the routing-coding tradeoff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4985</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4985</id><created>2010-05-27</created><updated>2015-05-26</updated><authors><author><keyname>Han</keyname><forenames>Shengqian</forenames></author><author><keyname>Yang</keyname><forenames>Chenyang</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author></authors><title>User Scheduling for Cooperative Base Station Transmission Exploiting
  Channel Asymmetry</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study low-signalling overhead scheduling for downlink coordinated
multi-point (CoMP) transmission with multi-antenna base stations (BSs) and
single-antenna users. By exploiting the asymmetric channel feature, i.e., the
pathloss differences towards different BSs, we derive a metric to judge
orthogonality among users only using their average channel gains, based on
which we propose a semi-orthogonal scheduler that can be applied in a two-stage
transmission strategy. Simulation results demonstrate that the proposed
scheduler performs close to the semi-orthogonal scheduler with full channel
information, especially when each BS is with more antennas and the celledge
region is large. Compared with other overhead reduction strategies, the
proposed scheduler requires much less training overhead to achieve the same
cell-average data rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4989</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4989</id><created>2010-05-27</created><authors><author><keyname>Chutchev</keyname><forenames>Evgeny</forenames></author></authors><title>A Formalization of the Turing Test</title><categories>cs.AI</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper offers a mathematical formalization of the Turing test. This
formalization makes it possible to establish the conditions under which some
Turing machine will pass the Turing test and the conditions under which every
Turing machine (or every Turing machine of the special class) will fail the
Turing test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4993</identifier>
 <datestamp>2011-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4993</id><created>2010-05-27</created><updated>2011-03-04</updated><authors><author><keyname>Crampton</keyname><forenames>Jason</forenames></author></authors><title>Practical Constructions for the Efficient Cryptographic Enforcement of
  Interval-Based Access Control Policies</title><categories>cs.CR cs.DS</categories><comments>31 pages, 11 figures, accepted for publication in ACM Transactions on
  Information and System Security; the arXiv version includes some minor
  results that do not appear in the journal version (Proposition 10 and
  Appendix A)</comments><acm-class>D.4.6; K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The enforcement of access control policies using cryptography has received
considerable attention in recent years and the security of such enforcement
schemes is increasingly well understood. Recent work in the area has considered
the efficient enforcement of temporal and geo-spatial access control policies,
and asymptotic results for the time and space complexity of efficient
enforcement schemes have been obtained. However, for practical purposes, it is
useful to have explicit bounds for the complexity of enforcement schemes.
  In this paper, we consider interval-based access control policies, of which
temporal and geo-spatial access control policies are special cases. We define
enforcement schemes for interval-based access control policies for which it is
possible, in almost all cases, to obtain exact values for the schemes'
complexity, thereby subsuming a substantial body of work in the literature.
Moreover, our enforcement schemes are more practical than existing schemes, in
the sense that they operate in the same way as standard cryptographic
enforcement schemes, unlike other efficient schemes in the literature. The main
difference between our approach and earlier work is that we develop techniques
that are specific to the cryptographic enforcement of interval-based access
control policies, rather than applying generic techniques that give rise to
complex constructions and asymptotic bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.4997</identifier>
 <datestamp>2011-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.4997</id><created>2010-05-27</created><authors><author><keyname>Sinha</keyname><forenames>Sitabhra</forenames></author><author><keyname>Ashraf</keyname><forenames>Md Izhar</forenames></author><author><keyname>Pan</keyname><forenames>Raj Kumar</forenames></author><author><keyname>Wells</keyname><forenames>Bryan Kenneth</forenames></author></authors><title>Network analysis of a corpus of undeciphered Indus civilization
  inscriptions indicates syntactic organization</title><categories>cs.CL physics.data-an physics.soc-ph</categories><comments>17 pages (includes 4 page appendix containing Indus sign list), 14
  figures</comments><journal-ref>Computer Speech and Language, 25 (2011) 639-654</journal-ref><doi>10.1016/j.csl.2010.05.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Archaeological excavations in the sites of the Indus Valley civilization
(2500-1900 BCE) in Pakistan and northwestern India have unearthed a large
number of artifacts with inscriptions made up of hundreds of distinct signs. To
date there is no generally accepted decipherment of these sign sequences and
there have been suggestions that the signs could be non-linguistic. Here we
apply complex network analysis techniques to a database of available Indus
inscriptions, with the aim of detecting patterns indicative of syntactic
organization. Our results show the presence of patterns, e.g., recursive
structures in the segmentation trees of the sequences, that suggest the
existence of a grammar underlying these inscriptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5020</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5020</id><created>2010-05-27</created><authors><author><keyname>Vaya</keyname><forenames>Shailesh</forenames></author></authors><title>(Unconditional) Secure Multiparty Computation with Man-in-the-middle
  Attacks</title><categories>cs.CR</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In secure multi-party computation $n$ parties jointly evaluate an $n$-variate
function $f$ in the presence of an adversary which can corrupt up till $t$
parties. Almost all the works that have appeared in the literature so far
assume the presence of authenticated channels between the parties. This
assumption is far from realistic. Two directions of research have been borne
from relaxing this (strong) assumption: (a) The adversary is virtually
omnipotent and can control all the communication channels in the network, (b)
Only a partially connected topology of authenticated channels is guaranteed and
adversary controls a subset of the communication channels in the network.
  This work introduces a new setting for (unconditional) secure multiparty
computation problem which is an interesting intermediate model with respect to
the above well studied models from the literature (by sharing a salient feature
from both the above models). We consider the problem of (unconditional) secure
multi-party computation when 'some' of the communication channels connecting
the parties can be corrupted passively as well as actively. For this setting,
some honest parties may be connected to several other honest parties via
corrupted channels and may not be able to authentically communicate with them.
Such parties may not be assured the canonical guarantees of correctness or
privacy. We present refined definitions of security for this new intermediate
model of unconditional multiparty computation. We show how to adapt protocols
for (Unconditional) secure multiparty computation to realize the definitions
and also argue the tightness of the results achieved by us.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5028</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5028</id><created>2010-05-27</created><authors><author><keyname>Singh</keyname><forenames>Rajesh P</forenames><affiliation>Indian Institute of Technology, Guwahati, India</affiliation></author><author><keyname>Saikia</keyname><forenames>Anupam</forenames><affiliation>Indian Institute of Technology, Guwahati, India</affiliation></author><author><keyname>Sarma</keyname><forenames>B. K.</forenames><affiliation>Indian Institute of Technology, Guwahati, India</affiliation></author></authors><title>Little Dragon Two: An efficient Multivariate Public Key Cryptosystem</title><categories>cs.CR</categories><comments>10 Pages, IJNSA</comments><journal-ref>International Journal of Network Security &amp; Its Applications 2.2
  (2010) 1-10</journal-ref><doi>10.5121/ijnsa.2010.2201</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In 1998 [8], Patarin proposed an efficient cryptosystem called Little Dragon
which was a variant a variant of Matsumoto Imai cryptosystem C*. However
Patarin latter found that Little Dragon cryptosystem is not secure [8], [3]. In
this paper we propose a cryptosystem Little Dragon Two which is as efficient as
Little Dragon cryptosystem but secure against all the known attacks. Like
Little Dragon cryptosystem the public key of Little Dragon Two is mixed type
that is quadratic in plaintext and cipher text variables. So the public key
size of Little Dragon Two is equal to Little Dragon Cryptosystem. Our public
key algorithm is bijective and can be used for both encryption and signatures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5035</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5035</id><created>2010-05-27</created><authors><author><keyname>Edgington</keyname><forenames>Mark</forenames></author><author><keyname>Kassahun</keyname><forenames>Yohannes</forenames></author><author><keyname>Kirchner</keyname><forenames>Frank</forenames></author></authors><title>Dynamic Motion Modelling for Legged Robots</title><categories>cs.RO</categories><doi>10.1109/IROS.2009.5354026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An accurate motion model is an important component in modern-day robotic
systems, but building such a model for a complex system often requires an
appreciable amount of manual effort. In this paper we present a motion model
representation, the Dynamic Gaussian Mixture Model (DGMM), that alleviates the
need to manually design the form of a motion model, and provides a direct means
of incorporating auxiliary sensory data into the model. This representation and
its accompanying algorithms are validated experimentally using an 8-legged
kinematically complex robot, as well as a standard benchmark dataset. The
presented method not only learns the robot's motion model, but also improves
the model's accuracy by incorporating information about the terrain surrounding
the robot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5045</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5045</id><created>2010-05-27</created><authors><author><keyname>Bravetti</keyname><forenames>Mario</forenames></author></authors><title>File Managing and Program Execution in Web Operating Systems</title><categories>cs.SE cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web Operating Systems can be seen as an extension of traditional Operating
Systems where the addresses used to manage files and execute programs (via the
basic load/execution mechanism) are extended from local filesystem path-names
to URLs. A first consequence is that, similarly as in traditional web
technologies, executing a program at a given URL, can be done in two
modalities: either the execution is performed client-side at the invoking
machine (and relative URL addressing in the executed program set to refer to
the invoked URL) or it is performed server-side at the machine addressed by the
invoked URL (as, e.g., for a web service). Moreover in this context, user
identification for access to programs and files and workflow-based composition
of service programs is naturally based on token/session-like mechanisms. We
propose a middleware based on client-server protocols and on a set primitives,
for managing files/resources and executing programs (in the form of
client-side/server-side components/services) in Web Operating Systems. We
formally define the semantics of such middleware via a process algebraic
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5054</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5054</id><created>2010-05-27</created><authors><author><keyname>An</keyname><forenames>HongSun</forenames></author><author><keyname>Mohaisen</keyname><forenames>Manar</forenames></author><author><keyname>Han</keyname><forenames>DongKeol</forenames></author><author><keyname>Chang</keyname><forenames>KyungHi</forenames></author></authors><title>Coordinated transmit and receive processing with adaptive multi-stream
  selection</title><categories>cs.IT math.IT</categories><comments>International Conference on Communications, Networking, and Mobile
  Computing (WiCOM'10): 5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an adaptive coordinated Tx-Rx beamforming scheme
for inter-user interference cancellation, when a base station (BS) communicates
with multiple users that each has multiple receive antennas. The conventional
coordinated Tx-Rx beamforming scheme transmits a fixed number of data streams
for each user regardless of the instantaneous channel states, that is, all the
users, no matter they are with ill-conditioned or well-conditioned channels,
have the same number of data streams. However, in the proposed adaptive
coordinated Tx-Rx beamforming scheme, we adaptively select the number of
streams per user to solve the inefficient problem of the conventional
coordinated Tx-Rx beamforming scheme. As a result, the BER performance is
improved. Simulation results show that the proposed algorithm outperforms the
conventional co-ordinated Tx-Rx beamforming algorithm by 2.5dB at a target BER
of 10^-2
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5060</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5060</id><created>2010-05-27</created><authors><author><keyname>Soundararajan</keyname><forenames>Shvetha</forenames></author><author><keyname>Arthur</keyname><forenames>James D.</forenames></author><author><keyname>Chigani</keyname><forenames>Amine</forenames></author></authors><title>Understanding the Tenets of Agile Software Engineering: Lecturing,
  Exploration and Critical Thinking</title><categories>cs.CY</categories><comments>4 pages, Experience Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of agile principles and practices in software development is becoming
a powerful force in today's workplace. In our quest to develop better products,
therefore, it is imperative that we strive to learn and understand the
application of Agile methods, principles and techniques to the software
development enterprise. Unfortunately, in many educational institutions courses
and projects that emphasize Agile Software Development are minimal. At best,
students have only limited exposure to the agile philosophy, principles and
practices at the graduate and undergraduate levels of education. In an effort
to address this concern, we offered a graduate-level course entitled &quot;Agile
Software Engineering&quot; in the Department of Computer Science at Virginia Tech in
Fall 2009. The primary objectives of the class were to introduce the values,
principles and practices underlying the agile philosophy, and to do so in an
atmosphere that encourages debate and critical thinking. The course was
designed around three central components: (1) teaching the essentials of how
one develops a product within an Agile framework, (2) having invited
presentation by notable industry experts, and (3) having students present and
discuss current research topics and issues. This paper describes our
experiences during the offering of that course, and in particular, the unique
perspectives of the class instructor, the teaching assistant and a student who
was enrolled in the class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5063</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5063</id><created>2010-05-27</created><updated>2010-05-28</updated><authors><author><keyname>Abdallah</keyname><forenames>Yara</forenames></author><author><keyname>Latif</keyname><forenames>Mohamed Abdel</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author><author><keyname>Sultan</keyname><forenames>Ahmed</forenames></author><author><keyname>Gamal</keyname><forenames>Hesham El</forenames></author></authors><title>Keys through ARQ: Theory and Practice</title><categories>cs.IT cs.CR math.IT</categories><comments>15 pages, submitted to IEEE Transactions on Information Forensics and
  Security</comments><journal-ref>IEEE Transactions on Information Forensics and Security, Sep. 2011</journal-ref><doi>10.1109/TIFS.2011.2123093</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a novel framework for sharing secret keys using the
Automatic Repeat reQuest (ARQ) protocol. We first characterize the underlying
information theoretic limits, under different assumptions on the channel
spatial and temporal correlation function. Our analysis reveals a novel role of
&quot;dumb antennas&quot; in overcoming the negative impact of spatial correlation on the
achievable secrecy rates. We further develop an adaptive rate allocation
policy, which achieves higher secrecy rates in temporally correlated channels,
and explicit constructions for ARQ secrecy coding that enjoy low implementation
complexity. Building on this theoretical foundation, we propose a unified
framework for ARQ-based secrecy in Wi-Fi networks. By exploiting the existing
ARQ mechanism in the IEEE 802.11 standard, we develop security overlays that
offer strong security guarantees at the expense of only minor modifications in
the medium access layer. Our numerical results establish the achievability of
non-zero secrecy rates even when the eavesdropper channel is less noisy, on the
average, than the legitimate channel, while our linux-based prototype
demonstrates the efficiency of our ARQ overlays in mitigating all known,
passive and active, Wi-Fi attacks at the expense of a minimal increase in the
link setup time and a small loss in throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5065</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5065</id><created>2010-05-27</created><authors><author><keyname>Mohaisen</keyname><forenames>Manar</forenames></author><author><keyname>Chang</keyname><forenames>KyungHi</forenames></author></authors><title>Upper-lower bounded-complexity QRD-M for spatial multiplexing MIMO-OFDM
  systems</title><categories>cs.IT math.IT</categories><comments>Springer, Wireless Personal Communications Journal (WPC'2010), 13
  pages, 6 figures, 2 tables, 1 algorithm</comments><doi>10.1007/s11277-010-0014-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple-input multiple-output (MIMO) technology applied with orthogonal
frequency division multiplexing (OFDM) is considered as the ultimate solution
to increase channel capacity without any additional spectral resources. At the
receiver side, the challenge resides in designing low complexity detection
algorithms capable of separating independent streams sent simultaneously from
different antennas. In this paper, we introduce an upper-lower
bounded-complexity QRD-M algorithm (ULBC QRD-M). In the proposed algorithm we
solve the problem of high extreme complexity of the conventional sphere
decoding by fixing the upper bound complexity to that of the conventional
QRD-M. On the other hand, ULBC QRD-M intelligently cancels all unnecessary
hypotheses to achieve very low computational requirements. Analyses and
simulation results show that the proposed algorithm achieves the performance of
conventional QRD-M with only 26% of the required computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5114</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5114</id><created>2010-05-27</created><authors><author><keyname>Plangprasopchok</keyname><forenames>Anon</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author><author><keyname>Getoor</keyname><forenames>Lise</forenames></author></authors><title>Growing a Tree in the Forest: Constructing Folksonomies by Integrating
  Structured Metadata</title><categories>cs.AI</categories><comments>10 pages, To appear in the Proceedings of ACM SIGKDD Conference on
  Knowledge Discovery and Data Mining(KDD) 2010</comments><acm-class>H.2.8; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many social Web sites allow users to annotate the content with descriptive
metadata, such as tags, and more recently to organize content hierarchically.
These types of structured metadata provide valuable evidence for learning how a
community organizes knowledge. For instance, we can aggregate many personal
hierarchies into a common taxonomy, also known as a folksonomy, that will aid
users in visualizing and browsing social content, and also to help them in
organizing their own content. However, learning from social metadata presents
several challenges, since it is sparse, shallow, ambiguous, noisy, and
inconsistent. We describe an approach to folksonomy learning based on
relational clustering, which exploits structured metadata contained in personal
hierarchies. Our approach clusters similar hierarchies using their structure
and tag statistics, then incrementally weaves them into a deeper, bushier tree.
We study folksonomy learning using social metadata extracted from the
photo-sharing site Flickr, and demonstrate that the proposed approach addresses
the challenges. Moreover, comparing to previous work, the approach produces
larger, more accurate folksonomies, and in addition, scales better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5115</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5115</id><created>2010-05-27</created><authors><author><keyname>Nguyen-H</keyname><forenames>M.</forenames></author><author><keyname>Zhou</keyname><forenames>C.</forenames></author></authors><title>Improving GPS/INS Integration through Neural Networks</title><categories>cs.NE</categories><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, pp1-6, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Global Positioning Systems (GPS) and Inertial Navigation System (INS)
technology have attracted a considerable importance recently because of its
large number of solutions serving both military as well as civilian
applications. This paper aims to develop a more efficient and especially a
faster method for processing the GPS signal in case of INS signal loss without
losing the accuracy of the data. The conventional or usual method consists of
processing data through a neural network and obtaining accurate positioning
output data. The new or improved method adds selective filtering at the
low-band frequency, the mid-band frequency and the high band frquency, before
processing the GPS data through the neural network, so that the processing time
is decreased significantly while the accuracy remains the same.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5118</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5118</id><created>2010-05-27</created><authors><author><keyname>Chalhoub</keyname><forenames>Gerard</forenames></author><author><keyname>Delobel</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Misson</keyname><forenames>Michel</forenames></author></authors><title>Time Segmentation Approach Allowing QoS and Energy Saving for Wireless
  Sensor Networks</title><categories>cs.NI</categories><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, pp7-14, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks are conceived to monitor a certain application or
physical phenomena and are supposed to function for several years without any
human intervention for maintenance. Thus, the main issue in sensor networks is
often to extend the lifetime of the network by reducing energy consumption. On
the other hand, some applications have high priority traffic that needs to be
transferred within a bounded end-to-end delay while maintaining an energy
efficient behavior. We propose MaCARI, a time segmentation protocol that saves
energy, improves the overall performance of the network and enables quality of
service in terms of guaranteed access to the medium and end-to-end delays. This
time segmentation is achieved by synchronizing the activity of nodes using a
tree-based beacon propagation and allocating activity periods for each cluster
of nodes. The tree-based topology is inspired from the cluster-tree proposed by
the ZigBee standard. The efficiency of our protocol is proven analytically, by
simulation and through real testbed measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5124</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5124</id><created>2010-05-27</created><authors><author><keyname>Kerber</keyname><forenames>Manfred</forenames></author></authors><title>Proofs, proofs, proofs, and proofs</title><categories>cs.AI cs.LO math.HO</categories><comments>10 pages, To appear in The 9th International Conference on
  Mathematical Knowledge Management: MKM 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In logic there is a clear concept of what constitutes a proof and what not. A
proof is essentially defined as a finite sequence of formulae which are either
axioms or derived by proof rules from formulae earlier in the sequence.
Sociologically, however, it is more difficult to say what should constitute a
proof and what not. In this paper we will look at different forms of proofs and
try to clarify the concept of proof in the wider meaning of the term. This has
implications on how proofs should be represented formally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5125</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5125</id><created>2010-05-27</created><authors><author><keyname>Adhicandra</keyname><forenames>Iwan</forenames></author></authors><title>Using AMC and HARQ to Optimize System Capacity and Application Delays in
  WiMAX Networks</title><categories>cs.NI</categories><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, pp15-20, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IEEE 802.16 technology (WiMAX) is a promising technology for providing
last-mile connectivity by radio link due to its high speed data rates, low cost
of deployment, and large coverage area. However, the maximum number of channels
defined in the current system may cause a potential bottleneck and limit the
overall system capacity. The aim of this paper is to compare the impact on
system performance of different solutions used to mitigate the impairments due
to the radio channel. In particular, taking into account the WiMAX system
capacity as well as application delays, the paper presents the simulation
results obtained when a static QPSK 1/2 Modulation and Coding Scheme (MCS) is
adopted. Then, the study is aimed at evaluating the improvements introduced by
the adoption of an adaptive modulation and coding (AMC) and an AMC jointly with
Hybrid Automatic Repeat reQuest (HARQ). Results indicate that the best strategy
is to use an aggressive AMC table with the HARQ.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5129</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5129</id><created>2010-05-27</created><authors><author><keyname>Elmorshidy</keyname><forenames>Ahmed</forenames></author></authors><title>Radio Frequency Identifiers: What are the Possibilities?</title><categories>cs.OH</categories><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, pp21-24, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper defines the components of radio frequency identifiers (RFID). It
also explores the different areas and sectors where RFID can be beneficial. The
paper discusses the uses and advantages of RFID in deference, consumer packaged
goods (CPG), healthcare, logistics, manufacturing, and retail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5130</identifier>
 <datestamp>2010-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5130</id><created>2010-05-27</created><updated>2010-06-01</updated><authors><author><keyname>Suri</keyname><forenames>P. K.</forenames></author><author><keyname>Taneja</keyname><forenames>Kavita</forenames></author></authors><title>Exploring Selfish Trends of Malicious Mobile Devices in MANET</title><categories>cs.CY</categories><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, pp25-30, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The research effort on mobile computing has focused mainly on routing and
usually assumes that all mobile devices (MDs) are cooperative. These
assumptions hold on military or search and rescue operations, where all hosts
are from the same authority and their users have common goals. The application
of mobile ad hoc networks (MANETs) as open networks has emerged recently but
proliferated exponentially. Energy is a valuable commodity in MANETs due to the
limited battery of the portable devices. Batteries typically cannot be replaced
in MANETs, making their lifetime limited. Diverse users, with unlike goals,
share the resources of their devices and ensuring global connectivity comes
very low in their priority. This sort of communities can already be found in
wired networks, namely on peer-to-peer networks. In this scenario, open MANETs
will likely resemble social environments. A group of persons can provide
benefits to each of its members as long as everyone provides his contribution.
For our particular case, each element of a MANET will be called to forward
messages and to participate on routing protocols. A selfish behavior threatens
the entire community and also this behavior is infectious as, other MDs may
also start to perform in the same way. In the extreme, this can take to the
complete sabotage of the network. This paper investigates the prevalent
malicious attacks in MANET and analyzes recent selfish trends in MANET. We
analyzed the respective strengths and vulnerabilities of the existing selfish
behaviour prevention scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5137</identifier>
 <datestamp>2010-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5137</id><created>2010-05-27</created><authors><author><keyname>Hugeng</keyname><forenames>W. Wahab</forenames></author><author><keyname>Gunawan</keyname><forenames>D.</forenames></author></authors><title>Improved Method for Individualization of Head-Related Transfer Functions
  on Horizontal Plane Using Reduced Number of Anthropometric Measurements</title><categories>cs.SD cs.HC</categories><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, pp31-41, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important problem to be solved in modeling head-related impulse responses
(HRIRs) is how to individualize HRIRs so that they are suitable for a listener.
We modeled the entire magnitude head-related transfer functions (HRTFs), in
frequency domain, for sound sources on horizontal plane of 37 subjects using
principal components analysis (PCA). The individual magnitude HRTFs could be
modeled adequately well by a linear combination of only ten orthonormal basis
functions. The goal of this research was to establish multiple linear
regression (MLR) between weights of basis functions obtained from PCA and fewer
anthropometric measurements in order to individualize a given listener's HRTFs
with his or her own anthropomety. We proposed here an improved
individualization method based on MLR of weights of basis functions by
utilizing 8 chosen out of 27 anthropometric measurements. Our objective
experiments' results show a superior performance than that of our previous work
on individualizing minimum phase HRIRs and also better than similar research.
The proposed individualization method shows that the individualized magnitude
HRTFs could approximated well the the original ones with small error. Moving
sound employing the reconstructed HRIRs could be perceived as if it was moving
around the horizontal plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5141</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5141</id><created>2010-05-27</created><updated>2014-05-26</updated><authors><author><keyname>Marteau</keyname><forenames>Pierre-Fran&#xe7;ois</forenames><affiliation>IRISA</affiliation></author><author><keyname>Gibet</keyname><forenames>Sylvie</forenames><affiliation>IRISA</affiliation></author></authors><title>On Recursive Edit Distance Kernels with Application to Time Series
  Classification</title><categories>cs.LG cs.IR</categories><comments>14 pages</comments><proxy>ccsd</proxy><report-no>DRAFT-2013-PositiveDefiniteElasticKernels</report-no><journal-ref>IEEE Transactions on Neural Networks and Learning Systems (2014)
  1-14</journal-ref><doi>10.1109/TNNLS.2014.2333876</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes some extensions to the work on kernels dedicated to
string or time series global alignment based on the aggregation of scores
obtained by local alignments. The extensions we propose allow to construct,
from classical recursive definition of elastic distances, recursive edit
distance (or time-warp) kernels that are positive definite if some sufficient
conditions are satisfied. The sufficient conditions we end-up with are original
and weaker than those proposed in earlier works, although a recursive
regularizing term is required to get the proof of the positive definiteness as
a direct consequence of the Haussler's convolution theorem. The classification
experiment we conducted on three classical time warp distances (two of which
being metrics), using Support Vector Machine classifier, leads to conclude
that, when the pairwise distance matrix obtained from the training data is
\textit{far} from definiteness, the positive definite recursive elastic kernels
outperform in general the distance substituting kernels for the classical
elastic distances we have tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5142</identifier>
 <datestamp>2010-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5142</id><created>2010-05-27</created><updated>2010-12-10</updated><authors><author><keyname>Terraf</keyname><forenames>Pedro S&#xe1;nchez</forenames></author></authors><title>Unprovability of the Logical Characterization of Bisimulation</title><categories>cs.LO</categories><comments>Extended introduction and comments; extra section on semi-pullbacks;
  11 pages Some background details added; extra example on the non-locality of
  state bisimilarity; 14 pages</comments><msc-class>03E15, 28A05, 60Jxx</msc-class><acm-class>G.3; F.4.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We quickly review labelled Markov processes (LMP) and provide a
counterexample showing that in general measurable spaces, event bisimilarity
and state bisimilarity differ in LMP. This shows that the logic in Desharnais
[*] does not characterize state bisimulation in non-analytic measurable spaces.
Furthermore we show that, under current foundations of Mathematics, such
logical characterization is unprovable for spaces that are projections of a
coanalytic set. Underlying this construction there is a proof that stationary
Markov processes over general measurable spaces do not have semi-pullbacks.
([*] J. Desharnais, Labelled Markov Processes. School of Computer Science.
McGill University, Montr\'eal (1999))
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5170</identifier>
 <datestamp>2010-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5170</id><created>2010-05-25</created><authors><author><keyname>Bouboulis</keyname><forenames>P.</forenames></author></authors><title>Wirtinger's Calculus in general Hilbert Spaces</title><categories>cs.LG math.CV</categories><comments>Report completed for the department of Informatics and
  Telecommunications of the University of Athens</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present report, has been inspired by the need of the author and its
colleagues to understand the underlying theory of Wirtinger's Calculus and to
further extend it to include the kernel case. The aim of the present manuscript
is twofold: a) it endeavors to provide a more rigorous presentation of the
related material, focusing on aspects that the author finds more insightful and
b) it extends the notions of Wirtinger's calculus on general Hilbert spaces
(such as Reproducing Hilbert Kernel Spaces).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5180</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5180</id><created>2010-05-27</created><authors><author><keyname>Moghaddam</keyname><forenames>Saeed</forenames></author><author><keyname>Helmy</keyname><forenames>Ahmed</forenames></author><author><keyname>Ranka</keyname><forenames>Sanjay</forenames></author><author><keyname>Somaiya</keyname><forenames>Manas</forenames></author></authors><title>Data-driven Co-clustering Model of Internet Usage in Large Mobile
  Societies</title><categories>cs.NI</categories><comments>10 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Design and simulation of future mobile networks will center around human
interests and behavior. We propose a design paradigm for mobile networks driven
by realistic models of users' on-line behavior, based on mining of billions of
wireless-LAN records. We introduce a systematic method for large-scale
multi-dimensional coclustering of web activity for thousands of mobile users at
79 locations. We find surprisingly that users can be consistently modeled using
ten clusters with disjoint profiles. Access patterns from multiple locations
show differential user behavior. This is the first study to obtain such
detailed results for mobile Internet usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5181</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5181</id><created>2010-05-27</created><authors><author><keyname>Burfoot</keyname><forenames>Daniel</forenames></author></authors><title>Compression Rate Method for Empirical Science and Application to
  Computer Vision</title><categories>cs.CV</categories><comments>37 pages, 3 figures</comments><acm-class>I.4.0; I.4.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This philosophical paper proposes a modified version of the scientific
method, in which large databases are used instead of experimental observations
as the necessary empirical ingredient. This change in the source of the
empirical data allows the scientific method to be applied to several aspects of
physical reality that previously resisted systematic interrogation. Under the
new method, scientific theories are compared by instantiating them as
compression programs, and examining the codelengths they achieve on a database
of measurements related to a phenomenon of interest. Because of the
impossibility of compressing random data, &quot;real world&quot; data can only be
compressed by discovering and exploiting the empirical structure it exhibits.
The method also provides a new way of thinking about two longstanding issues in
the philosophy of science: the problem of induction and the problem of
demarcation.
  The second part of the paper proposes to reformulate computer vision as an
empirical science of visual reality, by applying the new method to large
databases of natural images. The immediate goal of the proposed reformulation
is to repair the chronic difficulties in evaluation experienced by the field of
computer vision. The reformulation should bring a wide range of benefits,
including a substantially increased degree of methodological rigor, the ability
to justify complex theories without overfitting, a scalable evaluation
paradigm, and the potential to make systematic progress. A crucial argument is
that the change is not especially drastic, because most computer vision tasks
can be reformulated as specialized image compression techniques. Finally, a
concrete proposal is discussed in which a database is produced by recording
from a roadside video camera, and compression is achieved by developing a
computational understanding of the appearance of moving cars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5183</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5183</id><created>2010-05-27</created><authors><author><keyname>Berka</keyname><forenames>Alexander Victor</forenames></author></authors><title>Interlanguages and synchronic models of computation</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel language system has given rise to promising alternatives to standard
formal and processor network models of computation. An interstring linked with
a abstract machine environment, shares sub-expressions, transfers data, and
spatially allocates resources for the parallel evaluation of dataflow. Formal
models called the a-Ram family are introduced, designed to support interstring
programming languages (interlanguages). Distinct from dataflow, graph
rewriting, and FPGA models, a-Ram instructions are bit level and execute in
situ. They support sequential and parallel languages without the space/time
overheads associated with the Turing Machine and l-calculus, enabling massive
programs to be simulated. The devices of one a-Ram model, called the Synchronic
A-Ram, are fully connected and simpler than FPGA LUT's. A compiler for an
interlanguage called Space, has been developed for the Synchronic A-Ram. Space
is MIMD. strictly typed, and deterministic. Barring memory allocation and
compilation, modules are referentially transparent. At a high level of
abstraction, modules exhibit a state transition system, aiding verification.
Data structures and parallel iteration are straightforward to implement, and
allocations of sub-processes and data transfers to resources are implicit.
Space points towards highly connected architectures called Synchronic Engines,
that scale in a GALS manner. Synchronic Engines are more general purpose than
systolic arrays and GPUs, and bypass programmability and conflict issues
associated with multicores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5194</identifier>
 <datestamp>2011-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5194</id><created>2010-05-27</created><updated>2010-10-01</updated><authors><author><keyname>Wood</keyname><forenames>David R.</forenames></author><author><keyname>Linusson</keyname><forenames>Svante</forenames></author></authors><title>Thomassen's Choosability Argument Revisited</title><categories>math.CO cs.DM</categories><msc-class>05C83, 05C15</msc-class><journal-ref>SIAM J. Discrete Mathematics 24(4):1632-1637, 2010</journal-ref><doi>10.1137/100796649</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thomassen (1994) proved that every planar graph is 5-choosable. This result
was generalised by {\v{S}}krekovski (1998) and He et al. (2008), who proved
that every $K_5$-minor-free graph is 5-choosable. Both proofs rely on the
characterisation of $K_5$-minor-free graphs due to Wagner (1937). This paper
proves the same result without using Wagner's structure theorem or even planar
embeddings. Given that there is no structure theorem for graphs with no
$K_6$-minor, we argue that this proof suggests a possible approach for
attacking the Hadwiger Conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5197</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5197</id><created>2010-05-27</created><updated>2012-09-01</updated><authors><author><keyname>Slivkins</keyname><forenames>Aleksandrs</forenames></author><author><keyname>Radlinski</keyname><forenames>Filip</forenames></author><author><keyname>Gollapudi</keyname><forenames>Sreenivas</forenames></author></authors><title>Ranked bandits in metric spaces: learning optimally diverse rankings
  over large document collections</title><categories>cs.LG cs.DS</categories><comments>This is the full version of a paper in ICML 2010, with full proofs
  and a significantly revised presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most learning to rank research has assumed that the utility of different
documents is independent, which results in learned ranking functions that
return redundant results. The few approaches that avoid this have rather
unsatisfyingly lacked theoretical foundations, or do not scale. We present a
learning-to-rank formulation that optimizes the fraction of satisfied users,
with several scalable algorithms that explicitly takes document similarity and
ranking context into account. Our formulation is a non-trivial common
generalization of two multi-armed bandit models from the literature: &quot;ranked
bandits&quot; (Radlinski et al., ICML 2008) and &quot;Lipschitz bandits&quot; (Kleinberg et
al., STOC 2008). We present theoretical justifications for this approach, as
well as a near-optimal algorithm. Our evaluation adds optimizations that
improve empirical performance, and shows that our algorithms learn orders of
magnitude more quickly than previous approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5223</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5223</id><created>2010-05-28</created><authors><author><keyname>Dubois</keyname><forenames>Swan</forenames><affiliation>LIP6, INRIA Rocquencourt</affiliation></author><author><keyname>Masuzawa</keyname><forenames>Toshimitsu</forenames><affiliation>Department of Information and Computer Sciences, Osaka University</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LIP6</affiliation></author></authors><title>On Byzantine Containment Properties of the $min+1$ Protocol</title><categories>cs.DC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-stabilization is a versatile approach to fault-tolerance since it
permits a distributed system to recover from any transient fault that
arbitrarily corrupts the contents of all memories in the system. Byzantine
tolerance is an attractive feature of distributed systems that permits to cope
with arbitrary malicious behaviors. We consider the well known problem of
constructing a breadth-first spanning tree in this context. Combining these two
properties proves difficult: we demonstrate that it is impossible to contain
the impact of Byzantine nodes in a strictly or strongly stabilizing manner. We
then adopt the weaker scheme of topology-aware strict stabilization and we
present a similar weakening of strong stabilization. We prove that the
classical $min+1$ protocol has optimal Byzantine containment properties with
respect to these criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5227</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5227</id><created>2010-05-28</created><authors><author><keyname>Schreiber</keyname><forenames>Michael</forenames></author></authors><title>Twenty Hirsch index variants and other indicators giving more or less
  preference to highly cited papers</title><categories>physics.soc-ph cs.DL</categories><comments>19 pages, including 6 tables and 3 figures accepted for publication
  in Annalen der Physik Berlin</comments><journal-ref>Ann. Phys. (Berlin) 522, 536-554 (2010)</journal-ref><doi>10.1002/andp.201000046</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hirsch index or h-index is widely used to quantify the impact of an
individual's scientific research output, determining the highest number h of a
scientist's papers that received at least h citations. Several variants of the
index have been proposed in order to give more or less preference to highly
cited papers. I analyse the citation records of 26 physicists discussing
various suggestions, in particular A, e, f, g, h(2), h_w, h_T, \hbar, m, {\pi},
R, s, t, w, and maxprod. The total number of all and of all cited publications
as well as the highest and the average number of citations are also compared.
Advantages and disadvantages of these indices and indicators are discussed.
Correlation coefficients are determined quantifying which indices and
indicators yield similar and which yield more deviating rankings of the 26
datasets. For 6 datasets the determination of the indices and indicators is
visualized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5232</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5232</id><created>2010-05-28</created><updated>2010-05-31</updated><authors><author><keyname>Kohlhase</keyname><forenames>Michael</forenames></author><author><keyname>Rabe</keyname><forenames>Florian</forenames></author><author><keyname>Zholudev</keyname><forenames>Vyacheslav</forenames></author></authors><title>Towards MKM in the Large: Modular Representation and Scalable Software
  Architecture</title><categories>cs.OH</categories><comments>To appear in The 9th International Conference on Mathematical
  Knowledge Management: MKM 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MKM has been defined as the quest for technologies to manage mathematical
knowledge. MKM &quot;in the small&quot; is well-studied, so the real problem is to scale
up to large, highly interconnected corpora: &quot;MKM in the large&quot;. We contend that
advances in two areas are needed to reach this goal. We need representation
languages that support incremental processing of all primitive MKM operations,
and we need software architectures and implementations that implement these
operations scalably on large knowledge bases.
  We present instances of both in this paper: the MMT framework for modular
theory-graphs that integrates meta-logical foundations, which forms the base of
the next OMDoc version; and TNTBase, a versioned storage system for XML-based
document formats. TNTBase becomes an MMT database by instantiating it with
special MKM operations for MMT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5241</identifier>
 <datestamp>2010-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5241</id><created>2010-05-28</created><authors><author><keyname>Boukhobza</keyname><forenames>Jalil</forenames><affiliation>LESTER</affiliation></author><author><keyname>Claude</keyname><forenames>Timsit</forenames><affiliation>PRISM</affiliation></author></authors><title>Simulation de traces r\'eelles d'E/S disque de PC</title><categories>cs.PF cs.OS</categories><proxy>ccsd</proxy><journal-ref>RenPar'17 / SympA'2006 / CFSE'5 / JC'2006, Canet en Roussillon :
  France (2006)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Under Windows operating system, existing I/O benchmarking tools does not
allow a developer to efficiently define a file access strategy according to the
applications' constraints. This is essentially due to the fact that the
existing tools do allow only a restricted set of I/O workloads that does not
generally correspond to the target applications. To cope with this problem, we
designed and implemented a precise I/O simulator allowing to simulate whatever
real I/O trace on a given defined architecture, and in which most of file and
disk cache strategies, their interactions and the detailed storage system
architecture are implemented. Simulation results on different workloads and
architectures show a very high degree of precision. In fact, the mean error
rate as compared to real measures is of about 6% with a maximum of 10% on
global throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5253</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5253</id><created>2010-05-28</created><authors><author><keyname>Guadarrama</keyname><forenames>Sergio</forenames><affiliation>European Centre for Soft Computing</affiliation></author><author><keyname>Pancho</keyname><forenames>David P.</forenames><affiliation>European Centre for Soft Computing</affiliation></author></authors><title>Using Soft Constraints To Learn Semantic Models Of Descriptions Of
  Shapes</title><categories>cs.CL cs.AI cs.HC cs.LG</categories><comments>8 pages, 8 figures, WCCI'10 Conference</comments><report-no>FSC 2009-22</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The contribution of this paper is to provide a semantic model (using soft
constraints) of the words used by web-users to describe objects in a language
game; a game in which one user describes a selected object of those composing
the scene, and another user has to guess which object has been described. The
given description needs to be non ambiguous and accurate enough to allow other
users to guess the described shape correctly.
  To build these semantic models the descriptions need to be analyzed to
extract the syntax and words' classes used. We have modeled the meaning of
these descriptions using soft constraints as a way for grounding the meaning.
  The descriptions generated by the system took into account the context of the
object to avoid ambiguous descriptions, and allowed users to guess the
described object correctly 72% of the times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5268</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5268</id><created>2010-05-28</created><authors><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>An Empirical Study of the Manipulability of Single Transferable Voting</title><categories>cs.AI cs.GT cs.MA</categories><comments>To appear in Proceedings of the 19th European Conference on
  Artificial Intelligence (ECAI 2010)</comments><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voting is a simple mechanism to combine together the preferences of multiple
agents. Agents may try to manipulate the result of voting by mis-reporting
their preferences. One barrier that might exist to such manipulation is
computational complexity. In particular, it has been shown that it is NP-hard
to compute how to manipulate a number of different voting rules. However,
NP-hardness only bounds the worst-case complexity. Recent theoretical results
suggest that manipulation may often be easy in practice. In this paper, we
study empirically the manipulability of single transferable voting (STV) to
determine if computational complexity is really a barrier to manipulation. STV
was one of the first voting rules shown to be NP-hard. It also appears one of
the harder voting rules to manipulate. We sample a number of distributions of
votes including uniform and real world elections. In almost every election in
our experiments, it was easy to compute how a single agent could manipulate the
election or to prove that manipulation by a single agent was impossible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5270</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5270</id><created>2010-05-28</created><authors><author><keyname>Katsirelos</keyname><forenames>George</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Symmetries of Symmetry Breaking Constraints</title><categories>cs.AI</categories><comments>To appear in Proceedings of the 19th European Conference on
  Artificial Intelligence (ECAI 2010). Revises workshop paper that appears at
  SymCon 2009</comments><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symmetry is an important feature of many constraint programs. We show that
any problem symmetry acting on a set of symmetry breaking constraints can be
used to break symmetry. Different symmetries pick out different solutions in
each symmetry class. This simple but powerful idea can be used in a number of
different ways. We describe one application within model restarts, a search
technique designed to reduce the conflict between symmetry breaking and the
branching heuristic. In model restarts, we restart search periodically with a
random symmetry of the symmetry breaking constraints. Experimental results show
that this symmetry breaking technique is effective in practice on some standard
benchmark problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5271</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5271</id><created>2010-05-28</created><authors><author><keyname>Sabucedo</keyname><forenames>Luis Alvarez</forenames></author><author><keyname>Rifon</keyname><forenames>Luis Anido</forenames></author></authors><title>A Restful Approach for Managing Citizen profiles Using A Semantic
  Support</title><categories>cs.IR</categories><comments>18 Pages, IJDMS</comments><journal-ref>International Journal of Database Management Systems 2.2 (2010)
  1-18</journal-ref><doi>10.5121/ijdms.2010.2201</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Several steps are missing in the current high-speed race towards the holistic
support of citizen needs in the domain of eGovernment. This paper is focused on
how to provide support for the citizen profile. This profile, in a wide sense,
includes personal information as well documents in possession of the citizen.
This also involves the provision of those mechanisms required to publish,
access and submit the convenient information to a Public Administration in due
curse of a transactional services provided with the last one. Main features of
the system are related to interoperability and possibilities for its inclusion
in a cost effective manner in already developed platforms. To make that
possible, this approach will take full advantage of semantic technologies and
the RESTful paradigm to design the entire system. The paper presents the
overall system with some notes on the deployment of the solution for its
further reuse in similar contexts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5273</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5273</id><created>2010-05-28</created><updated>2010-09-06</updated><authors><author><keyname>Sei</keyname><forenames>Tomonari</forenames></author><author><keyname>Takayama</keyname><forenames>Nobuki</forenames></author><author><keyname>Takemura</keyname><forenames>Akimichi</forenames></author><author><keyname>Nakayama</keyname><forenames>Hiromasa</forenames></author><author><keyname>Nishiyama</keyname><forenames>Kenta</forenames></author><author><keyname>Noro</keyname><forenames>Masayuki</forenames></author><author><keyname>Ohara</keyname><forenames>Katsuyoshi</forenames></author></authors><title>Holonomic Gradient Descent and its Application to Fisher-Bingham
  Integral</title><categories>cs.SC math.OC math.ST stat.TH</categories><comments>23 pages, 1 figure</comments><journal-ref>Advances in Applied Mathematics 47 (2011) 639-658</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new algorithm to find local maximum and minimum of a holonomic
function and apply it for the Fisher-Bingham integral on the sphere $S^n$,
which is used in the directional statistics. The method utilizes the theory and
algorithms of holonomic systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5278</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5278</id><created>2010-05-28</created><updated>2010-08-18</updated><authors><author><keyname>Jonsson</keyname><forenames>Peter A.</forenames></author><author><keyname>Nordlander</keyname><forenames>Johan</forenames></author></authors><title>Positive Supercompilation for a Higher-Order Call-By-Value Language</title><categories>cs.PL</categories><proxy>Logical Methods In Computer Science</proxy><acm-class>D.3.4; D.3.2</acm-class><journal-ref>Logical Methods in Computer Science, Volume 6, Issue 3 (August 18,
  2010) lmcs:1038</journal-ref><doi>10.2168/LMCS-6(3:5)2010</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Previous deforestation and supercompilation algorithms may introduce
accidental termination when applied to call-by-value programs. This hides
looping bugs from the programmer, and changes the behavior of a program
depending on whether it is optimized or not. We present a supercompilation
algorithm for a higher-order call-by-value language and prove that the
algorithm both terminates and preserves termination properties. This algorithm
utilizes strictness information to decide whether to substitute or not and
compares favorably with previous call-by-name transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5337</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5337</id><created>2010-05-28</created><authors><author><keyname>Byl</keyname><forenames>Marten F.</forenames></author><author><keyname>Demers</keyname><forenames>James T.</forenames></author><author><keyname>Rietman</keyname><forenames>Edward A.</forenames></author></authors><title>Using a Kernel Adatron for Object Classification with RCS Data</title><categories>cs.LG stat.ML</categories><comments>This material is based upon work supported by US Army Space &amp; Missile
  Command under Contract Number W9113M-07-C-0204. Any opinions, findings and
  conclusions or recommendations expressed in this material are those of the
  authors and do not necessarily re flect the views of US Army Space &amp; Missile
  Command</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapid identification of object from radar cross section (RCS) signals is
important for many space and military applications. This identification is a
problem in pattern recognition which either neural networks or support vector
machines should prove to be high-speed. Bayesian networks would also provide
value but require significant preprocessing of the signals. In this paper, we
describe the use of a support vector machine for object identification from
synthesized RCS data. Our best results are from data fusion of X-band and
S-band signals, where we obtained 99.4%, 95.3%, 100% and 95.6% correct
identification for cylinders, frusta, spheres, and polygons, respectively. We
also compare our results with a Bayesian approach and show that the SVM is
three orders of magnitude faster, as measured by the number of floating point
operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5348</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5348</id><created>2010-05-28</created><authors><author><keyname>Lei</keyname><forenames>Ming</forenames></author><author><keyname>Del Moral</keyname><forenames>Pierre</forenames></author><author><keyname>Baehr</keyname><forenames>Christophe</forenames></author></authors><title>Error Analysis of Approximated PCRLBs for Nonlinear Dynamics</title><categories>stat.AP cs.IT math.IT math.NA</categories><comments>5 pages, 4 figures;</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In practical nonlinear filtering, the assessment of achievable filtering
performance is important. In this paper, we focus on the problem of efficiently
approximate the posterior Cramer-Rao lower bound (CRLB) in a recursive manner.
By using Gaussian assumptions, two types of approximations for calculating the
CRLB are proposed: An exact model using the state estimate as well as a
Taylor-series-expanded model using both of the state estimate and its error
covariance, are derived. Moreover, the difference between the two approximated
CRLBs is also formulated analytically. By employing the particle filter (PF)
and the unscented Kalman filter (UKF) to compute, simulation results reveal
that the approximated CRLB using mean-covariance-based model outperforms that
using the mean-based exact model. It is also shown that the theoretical
difference between the estimated CRLBs can be improved through an improved
filtering method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5361</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5361</id><created>2010-04-26</created><authors><author><keyname>Gupta</keyname><forenames>Akash Kumar</forenames></author><author><keyname>Kumar</keyname><forenames>Sanjeet</forenames></author></authors><title>VHDL Implementation of different Turbo Encoder using Log-MAP Decoder</title><categories>cs.IT math.IT</categories><journal-ref>Journal of Telecommunications, Volume 2, Issue 1, p49-53, April
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Turbo code is a great achievement in the field of communication system. It
can be created by connecting a turbo encoder and a decoder serially. A Turbo
encoder is build with parallel concatenation of two simple convolutional codes.
By varying the number of memory element (encoder configuration), code rate (1/2
or 1/3), block size of data and iteration, we can achieve better BER
performance. Turbo code also consists of interleaver unit and its BER
performance also depends on interleaver size. Turbo Decoder can be implemented
using different algorithm, but Log -MAP decoding algorithm is less
computationaly complex with respect to MAP (maximux a posteriori) algorithm,
without compromising its BER performance, nearer to Shannon limit. A register
transfer level (RTL) turbo encoder is designed and simulated using VHDL (Very
high speed integrated circuit Hardware Description Language). In this paper
VHDL model of different turbo encoder are implemented using Log MAP decoder and
its performance are compared and verified with corresponding MATLAB simulated
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5367</identifier>
 <datestamp>2010-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5367</id><created>2010-05-28</created><authors><author><keyname>Yeow</keyname><forenames>Wai-Leong</forenames></author><author><keyname>Westphal</keyname><forenames>C&#xe9;dric</forenames></author><author><keyname>Kozat</keyname><forenames>Ula&#x15f; C.</forenames></author></authors><title>Designing and Embedding Reliable Virtual Infrastructures</title><categories>cs.NI</categories><report-no>DCL-TR-2010-15</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a virtualized infrastructure where physical resources are shared, a single
physical server failure will terminate several virtual servers and crippling
the virtual infrastructures which contained those virtual servers. In the worst
case, more failures may cascade from overloading the remaining servers. To
guarantee some level of reliability, each virtual infrastructure, at
instantiation, should be augmented with backup virtual nodes and links that
have sufficient capacities. This ensures that, when physical failures occur,
sufficient computing resources are available and the virtual network topology
is preserved. However, in doing so, the utilization of the physical
infrastructure may be greatly reduced. This can be circumvented if backup
resources are pooled and shared across multiple virtual infrastructures, and
intelligently embedded in the physical infrastructure. These techniques can
reduce the physical footprint of virtual backups while guaranteeing
reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5375</identifier>
 <datestamp>2012-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5375</id><created>2010-05-28</created><updated>2011-06-20</updated><authors><author><keyname>Fiziev</keyname><forenames>Plamen P.</forenames></author><author><keyname>Staicova</keyname><forenames>Denitsa R.</forenames></author></authors><title>Two-dimensional generalization of the Muller root-finding algorithm and
  its applications</title><categories>cs.NA astro-ph.IM gr-qc hep-th</categories><comments>21 pages, 3 figures, 4 tables; Amendments. Typos corrected. New
  sections and figures added. New comments on the application of the method in
  systems featuring confluent Heun functions, including the QNM of the Kerr
  black hole. Expanded numerical testing of the algorithm on simple systems;
  Internal Report, Sofia University, 2011</comments><report-no>SU-TH/1-05-2010</report-no><msc-class>G.1.0, G.1.5</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new algorithm for solving a system of two nonlinear
transcendental equations with two complex variables based on the Muller
algorithm. The two-dimensional Muller algorithm is tested on systems of
different type and is found to work comparably to Newton's method and Broyden's
method in many cases. The new algorithm is particularly useful in systems
featuring the Heun functions whose complexity may make the already known
algorithms not efficient enough or not working at all. In those specific cases,
the new algorithm gives distinctly better results than the other two methods.
As an example for its application in physics, the new algorithm was used to
find the quasi-normal modes (QNM) of Schwarzschild black hole described by the
Regge-Wheeler equation. The numerical results obtained by our method are
compared with the already published QNM frequencies and are found to coincide
to a great extent with them. Also discussed are the QNM of the Kerr black hole,
described by the Teukolsky Master equation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5384</identifier>
 <datestamp>2010-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5384</id><created>2010-05-28</created><authors><author><keyname>Gaburov</keyname><forenames>Evghenii</forenames></author><author><keyname>B&#xe9;dorf</keyname><forenames>Jeroen</forenames></author><author><keyname>Zwart</keyname><forenames>Simon Portegies</forenames></author></authors><title>Gravitational tree-code on graphics processing units: implementation in
  CUDA</title><categories>astro-ph.IM cs.DC</categories><comments>9 pages, 8 figures. Accepted for publication at International
  Conference on Computational Science 2010</comments><doi>10.1016/j.procs.2010.04.124</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new very fast tree-code which runs on massively parallel
Graphical Processing Units (GPU) with NVIDIA CUDA architecture. The
tree-construction and calculation of multipole moments is carried out on the
host CPU, while the force calculation which consists of tree walks and
evaluation of interaction list is carried out on the GPU. In this way we
achieve a sustained performance of about 100GFLOP/s and data transfer rates of
about 50GB/s. It takes about a second to compute forces on a million particles
with an opening angle of $\theta \approx 0.5$. The code has a convenient user
interface and is freely available for use\footnote{{\tt
http://castle.strw.leidenuniv.nl/software/octgrav.html}}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5395</identifier>
 <datestamp>2010-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5395</id><created>2010-05-28</created><updated>2010-06-08</updated><authors><author><keyname>Sorge</keyname><forenames>Manuel</forenames></author></authors><title>Algorithmic Aspects of Golomb Ruler Construction</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Golomb rulers and their construction. Common rulers feature marks
at every unit measure, distances can often be measured with numerous pairs of
marks. On Golomb rulers, for every distance there are at most two marks
measuring it. The construction of optimal---with respect to shortest length for
given number of marks or maximum number of marks for given length---is
nontrivial, various problems regarding this are NP-complete. We give a
simplified hardness proof for one of them. We use a hypergraph characterization
of rulers and Golomb rulers to illuminate structural properties. This gives
rise to a problem kernel in a fixed-parameter approach to a construction
problem. We also take a short look at the practical implications of these
considerations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5412</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5412</id><created>2010-05-28</created><authors><author><keyname>Li</keyname><forenames>Jiangyuan</forenames></author><author><keyname>Petropulu</keyname><forenames>Athina P.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>On Cooperative Beamforming Based on Second-Order Statistics of Channel
  State Information</title><categories>cs.IT math.IT</categories><comments>30 pages, 9 figures</comments><doi>10.1109/TSP.2010.2094614</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative beamforming in relay networks is considered, in which a source
transmits to its destination with the help of a set of cooperating nodes. The
source first transmits locally. The cooperating nodes that receive the source
signal retransmit a weighted version of it in an amplify-and-forward (AF)
fashion. Assuming knowledge of the second-order statistics of the channel state
information, beamforming weights are determined so that the signal-to-noise
ratio (SNR) at the destination is maximized subject to two different power
constraints, i.e., a total (source and relay) power constraint, and individual
relay power constraints. For the former constraint, the original problem is
transformed into a problem of one variable, which can be solved via Newton's
method. For the latter constraint, the original problem is transformed into a
homogeneous quadratically constrained quadratic programming (QCQP) problem. In
this case, it is shown that when the number of relays does not exceed three the
global solution can always be constructed via semidefinite programming (SDP)
relaxation and the matrix rank-one decomposition technique. For the cases in
which the SDP relaxation does not generate a rank one solution, two methods are
proposed to solve the problem: the first one is based on the coordinate descent
method, and the second one transforms the QCQP problem into an infinity norm
maximization problem in which a smooth finite norm approximation can lead to
the solution using the augmented Lagrangian method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5413</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5413</id><created>2010-05-28</created><authors><author><keyname>Kostitsyna</keyname><forenames>Irina</forenames></author><author><keyname>Polishchuk</keyname><forenames>Valentin</forenames></author></authors><title>Simple Wriggling is Hard unless You Are a Fat Hippo</title><categories>cs.CG</categories><comments>A shorter version is to be presented at FUN 2010</comments><doi>10.1007/978-3-642-13122-6_27</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that it is NP-hard to decide whether two points in a polygonal
domain with holes can be connected by a wire. This implies that finding any
approximation to the shortest path for a long snake amidst polygonal obstacles
is NP-hard. On the positive side, we show that snake's problem is
&quot;length-tractable&quot;: if the snake is &quot;fat&quot;, i.e., its length/width ratio is
small, the shortest path can be computed in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5432</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5432</id><created>2010-05-29</created><authors><author><keyname>H</keyname><forenames>Spits Warnars H. L.</forenames></author></authors><title>Attribute oriented induction with star schema</title><categories>cs.DB</categories><comments>23 Pages, IJDMS</comments><journal-ref>International Journal of Database Management Systems 2.2 (2010)
  20-42</journal-ref><doi>10.5121/ijdms.2010.2202</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper will propose a novel star schema attribute induction as a new
attribute induction paradigm and as improving from current attribute oriented
induction. A novel star schema attribute induction will be examined with
current attribute oriented induction based on characteristic rule and using non
rule based concept hierarchy by implementing both of approaches. In novel star
schema attribute induction some improvements have been implemented like
elimination threshold number as maximum tuples control for generalization
result, there is no ANY as the most general concept, replacement the role
concept hierarchy with concept tree, simplification for the generalization
strategy steps and elimination attribute oriented induction algorithm. Novel
star schema attribute induction is more powerful than the current attribute
oriented induction since can produce small number final generalization tuples
and there is no ANY in the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5433</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5433</id><created>2010-05-29</created><authors><author><keyname>Arfaoui</keyname><forenames>Nouha</forenames></author><author><keyname>Akaichi</keyname><forenames>Jalel</forenames></author></authors><title>A Data Warehouse Assistant Design System Based on Clover Model</title><categories>cs.DB</categories><comments>15 Pages, IJDMS</comments><journal-ref>International Journal of Database Management Systems 2.2 (2010)
  57-71</journal-ref><doi>10.5121/ijdms.2010.2204</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Nowadays, Data Warehouse (DW) plays a crucial role in the process of decision
making. However, their design remains a very delicate and difficult task either
for expert or users. The goal of this paper is to propose a new approach based
on the clover model, destined to assist users to design a DW. The proposed
approach is based on two main steps. The first one aims to guide users in their
choice of DW schema model. The second one aims to finalize the chosen model by
offering to the designer views related to former successful DW design
experiences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5434</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5434</id><created>2010-05-29</created><authors><author><keyname>Keshavamurthy</keyname><forenames>B. N.</forenames></author><author><keyname>Sharma</keyname><forenames>Mitesh</forenames></author><author><keyname>Toshniwal</keyname><forenames>Durga</forenames></author></authors><title>Efficient Support Coupled Frequent Pattern Mining Over Progressive
  Databases</title><categories>cs.DB</categories><comments>10 Pages, IJDMS</comments><journal-ref>International Journal of Database Management Systems 2.2 (2010)
  73-82</journal-ref><doi>10.5121/ijdms.2010.2205</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  There have been many recent studies on sequential pattern mining. The
sequential pattern mining on progressive databases is relatively very new, in
which we progressively discover the sequential patterns in period of interest.
Period of interest is a sliding window continuously advancing as the time goes
by. As the focus of sliding window changes, the new items are added to the
dataset of interest and obsolete items are removed from it and become up to
date. In general, the existing proposals do not fully explore the real world
scenario, such as items associated with support in data stream applications
such as market basket analysis. Thus mining important knowledge from supported
frequent items becomes a non trivial research issue. Our proposed novel
approach efficiently mines frequent sequential pattern coupled with support
using progressive mining tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5435</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5435</id><created>2010-05-29</created><authors><author><keyname>Singh</keyname><forenames>Y. Jayanta</forenames><affiliation>117th October University, Libya</affiliation></author><author><keyname>Singh</keyname><forenames>Yumnam Somananda</forenames><affiliation>Institute of Management Studies &amp; I. T</affiliation></author><author><keyname>Gaikwad</keyname><forenames>Ashok</forenames><affiliation>Institute of Management Studies &amp; I. T</affiliation></author><author><keyname>Mehrotra</keyname><forenames>S. C.</forenames><affiliation>Dr. B. A. Marathwada University, India</affiliation></author></authors><title>Dynamic management of transactions in distributed real-time processing
  system</title><categories>cs.DC</categories><comments>10 Pages,IJDMS</comments><journal-ref>International Journal of Database Management Systems 2.2 (2010)
  161-170</journal-ref><doi>10.5121/ijdms.2010.2210</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Managing the transactions in real time distributed computing system is not
easy, as it has heterogeneously networked computers to solve a single problem.
If a transaction runs across some different sites, it may commit at some sites
and may failure at another site, leading to an inconsistent transaction. The
complexity is increase in real time applications by placing deadlines on the
response time of the database system and transactions processing. Such a system
needs to process Transactions before these deadlines expired. A series of
simulation study have been performed to analyze the performance under different
transaction management under conditions such as different workloads,
distribution methods, execution mode-distribution and parallel etc. The
scheduling of data accesses are done in order to meet their deadlines and to
minimize the number of transactions that missed deadlines. A new concept is
introduced to manage the transactions in dynamic ways rather than setting
computing parameters in static ways. With this approach, the system gives a
significant improvement in performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5436</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5436</id><created>2010-05-29</created><authors><author><keyname>Dakshayini</keyname><forenames>M.</forenames><affiliation>Dr.MGR University, India</affiliation></author><author><keyname>Nair</keyname><forenames>T. R. Gopala Krishnan</forenames><affiliation>Research and Industry Incubation Centre - Bangalore, India</affiliation></author></authors><title>Client-to-Client Streaming Scheme for VOD Applications</title><categories>cs.MM</categories><comments>10 Pages, IJMA</comments><journal-ref>International journal of Multimedia &amp; Its Applications 2.2 (2010)
  46-55</journal-ref><doi>10.5121/ijma.2010.2204</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we propose an efficient client-to-client streaming approach to
cooperatively stream the video using chaining technique with unicast
communication among the clients. This approach considers two major issues of
VoD 1) Prefix caching scheme to accommodate more number of videos closer to
client, so that the request-service delay for the user can be minimized. 2)
Cooperative proxy and client chaining scheme for streaming the videos using
unicasting. This approach minimizes the client rejection rate and bandwidth
requirement on server to proxy and proxy to client path. Our simulation results
show that the proposed approach achieves reduced client waiting time and
optimal prefix caching of videos minimizing server to proxy path bandwidth
usage by utilizing the client to client bandwidth, which is occasionally used
when compared to busy server to proxy path bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5437</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5437</id><created>2010-05-29</created><authors><author><keyname>Rao</keyname><forenames>Ch. Srinivasa</forenames><affiliation>Sri Sai Aditya Institute of Science &amp; Technology, India</affiliation></author><author><keyname>Kumar</keyname><forenames>S. Srinivas</forenames><affiliation>JNTUK, India</affiliation></author><author><keyname>Mohan</keyname><forenames>B. Chandra</forenames><affiliation>Bapatla Engineering College, India</affiliation></author></authors><title>Content Based Image Retrieval Using Exact Legendre Moments and Support
  Vector Machine</title><categories>cs.CV</categories><comments>11 Pages, IJMA</comments><journal-ref>International journal of Multimedia &amp; Its Applications 2.2 (2010)
  69-79</journal-ref><doi>10.5121/ijma.2010.2206</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Content Based Image Retrieval (CBIR) systems based on shape using invariant
image moments, viz., Moment Invariants (MI) and Zernike Moments (ZM) are
available in the literature. MI and ZM are good at representing the shape
features of an image. However, non-orthogonality of MI and poor reconstruction
of ZM restrict their application in CBIR. Therefore, an efficient and
orthogonal moment based CBIR system is needed. Legendre Moments (LM) are
orthogonal, computationally faster, and can represent image shape features
compactly. CBIR system using Exact Legendre Moments (ELM) for gray scale images
is proposed in this work. Superiority of the proposed CBIR system is observed
over other moment based methods, viz., MI and ZM in terms of retrieval
efficiency and retrieval time. Further, the classification efficiency is
improved by employing Support Vector Machine (SVM) classifier. Improved
retrieval results are obtained over existing CBIR algorithm based on Stacked
Euler Vector (SERVE) combined with Modified Moment Invariants (MMI).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5438</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5438</id><created>2010-05-29</created><authors><author><keyname>King</keyname><forenames>Raddad Al</forenames></author><author><keyname>Hameurlain</keyname><forenames>Abdelkader</forenames></author><author><keyname>Morvan</keyname><forenames>Franck</forenames></author></authors><title>Query Routing and Processing in Peer-To-Peer Data Sharing Systems</title><categories>cs.DB</categories><comments>24 Pages, IJDMS</comments><journal-ref>International Journal of Database Management Systems 2.2 (2010)
  116-139</journal-ref><doi>10.5121/ijdms.2010.2208</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Sharing musical files via the Internet was the essential motivation of early
P2P systems. Despite of the great success of the P2P file sharing systems,
these systems support only &quot;simple&quot; queries. The focus in such systems is how
to carry out an efficient query routing in order to find the nodes storing a
desired file. Recently, several research works have been made to extend P2P
systems to be able to share data having a fine granularity (i.e. atomic
attribute) and to process queries written with a highly expressive language
(i.e. SQL). These works have led to the emergence of P2P data sharing systems
that represent a new generation of P2P systems and, on the other hand, a next
stage in a long period of the database research area. ? The characteristics of
P2P systems (e.g. large-scale, node autonomy and instability) make impractical
to have a global catalog that represents often an essential component in
traditional database systems. Usually, such a catalog stores information about
data, schemas and data sources. Query routing and processing are two problems
affected by the absence of a global catalog. Locating relevant data sources and
generating a close to optimal execution plan become more difficult. In this
paper, we concentrate our study on proposed solutions for the both problems.
Furthermore, selected case studies of main P2P data sharing systems are
analyzed and compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5439</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5439</id><created>2010-05-29</created><authors><author><keyname>Al-Rahayfeh</keyname><forenames>Amer A.</forenames></author><author><keyname>Abuzneid</keyname><forenames>Abdelshakour A.</forenames></author></authors><title>Detection of Bleeding in Wireless Capsule Endoscopy Images Using Range
  Ratio Color</title><categories>cs.CV</categories><comments>10 Pages, IJMA</comments><journal-ref>International journal of Multimedia &amp; Its Applications 2.2 (2010)
  1-10</journal-ref><doi>10.5121/ijma.2010.2201</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Wireless Capsule Endoscopy (WCE) is device to detect abnormalities in
colon,esophagus,small intestinal and stomach, to distinguish bleeding in WCE
images from non bleeding is a hard job by human reviewing and very time
consuming. Consequently, automation for classifying bleeding frames not only
will expedite the process but will reduce the burden on the doctors. Using the
purity of the red color we can detect the Bleeding areas in WCE images. But, we
could find various intensity of red color values in different parts of the
small intestinal,so it is not enough to depend on the red color feature alone.
We select RGB(Red,Green,Blue) because it takes raw level values and it is easy
to use. In this paper we will put range ratio color for each of R,G,and B.
Therefore, we divide each image into multiple pixels and apply the range ratio
color condition for each pixel. Then we count the number of the pixels that
achieved our condition. If the number of pixels grater than zero, then the
frame is classified as a bleeding type. Otherwise, it is a non-bleeding. Our
experimental results show that this method could achieve a very high accuracy
in detecting bleeding images for the different parts of the small intestinal
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5440</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5440</id><created>2010-05-29</created><authors><author><keyname>Kumar</keyname><forenames>Surender</forenames><affiliation>Haryana College of Tech. &amp; Mgmt. India</affiliation></author><author><keyname>Chauhan</keyname><forenames>R. K.</forenames><affiliation>Kurukshetra University, India</affiliation></author><author><keyname>Kumar</keyname><forenames>Parveen</forenames><affiliation>MIET - Meerut, India</affiliation></author></authors><title>A Low Overhead Minimum Process Global Snapshop Collection Algorithm for
  Mobile Distributed System</title><categories>cs.DC</categories><comments>19 Pages, IJMA</comments><journal-ref>International journal of Multimedia &amp; Its Applications 2.2 (2010)
  12-30</journal-ref><doi>10.5121/ijma.2010.2202</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Coordinated checkpointing is an effective fault tolerant technique in
distributed system as it avoids the domino effect and require minimum storage
requirement. Most of the earlier coordinated checkpoint algorithms block their
computation during checkpointing and forces minimum-process or non-blocking but
forces all nodes to takes checkpoint even though many of them may not be
necessary or non-blocking minimum-process but takes useless checkpoints or
reduced useless checkpoint but has higher synchronization message overhead or
has high checkpoint request propagation time. Hence in mobile distributed
systems there is a great need of minimizing the number of communication message
and checkpointing overhead as it raise new issues such as mobility, low
bandwidth of wireless channels, frequently disconnections, limited battery
power and lack of reliable stable storage on mobile nodes. In this paper, we
propose a minimum-process coordinated checkpointing algorithm for mobile
distributed system where no useless checkpoints are taken, no blocking of
processes takes place and enforces a minimum-number of processes to take
checkpoints. Our algorithm imposes low memory and computation overheads on MH's
and low communication overheads on wireless channels. It avoids awakening of an
MH if it is not required to take its checkpoint and has reduced latency time as
each process involved in a global checkpoint can forward its own decision
directly to the checkpoint initiator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5444</identifier>
 <datestamp>2010-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5444</id><created>2010-05-29</created><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Eugene Garfield and Algorithmic Historiography: Co-Words, Co-Authors,
  and Journal Names</title><categories>cs.DL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithmic historiography was proposed by Eugene Garfield in collaboration
with Irving Sher in the 1960s, but further developed only recently into
HistCite^{TM} with Alexander Pudovkin. As in history writing, HistCite^{TM}
reconstructs by drawing intellectual lineages. In addition to cited references,
however, documents can be attributed a multitude of other variables such as
title words, keywords, journal names, author names, and even full texts. New
developments in multidimensional scaling (MDS) enable us not only to visualize
these patterns at each moment of time, but also to animate them over time.
Using title words, co-authors, and journal names in Garfield's oeuvre, the
method is demonstrated and further developed in this paper (and in the
animation at http://www.leydesdorff.net/garfield/animation). The variety and
substantive content of the animation enables us to write, visualize, and
animate the author's intellectual history.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5448</identifier>
 <datestamp>2012-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5448</id><created>2010-05-29</created><authors><author><keyname>Kumar</keyname><forenames>Shailesh</forenames></author><author><keyname>Rao</keyname><forenames>Shrisha</forenames></author></authors><title>Failover in cellular automata</title><categories>cs.AI nlin.CG</categories><comments>16 pages, 15 figures and associated video at
  http://dl.dropbox.com/u/7553694/failover_demo.avi and simulation at
  http://dl.dropbox.com/u/7553694/failover_simulation.jar</comments><journal-ref>Physics Procedia, vol. 22, pp. 557--564, 2011</journal-ref><doi>10.1016/j.phpro.2011.11.086</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A cellular automata (CA) configuration is constructed that exhibits emergent
failover. The configuration is based on standard Game of Life rules. Gliders
and glider-guns form the core messaging structure in the configuration. The
blinker is represented as the basic computational unit, and it is shown how it
can be recreated in case of a failure. Stateless failover using primary-backup
mechanism is demonstrated. The details of the CA components used in the
configuration and its working are described, and a simulation of the complete
configuration is also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5449</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5449</id><created>2010-05-29</created><updated>2011-11-03</updated><authors><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Raman</keyname><forenames>Venkatesh</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>Bidimensionality and EPTAS</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bidimensionality theory is a powerful framework for the development of
metaalgorithmic techniques. It was introduced by Demaine et al. as a tool to
obtain sub-exponential time parameterized algorithms for problems on H-minor
free graphs. Demaine and Hajiaghayi extended the theory to obtain PTASs for
bidimensional problems, and subsequently improved these results to EPTASs.
Fomin et. al related the theory to the existence of linear kernels for
parameterized problems. In this paper we revisit bidimensionality theory from
the perspective of approximation algorithms and redesign the framework for
obtaining EPTASs to be more powerful, easier to apply and easier to understand.
  Two of the most widely used approaches to obtain PTASs on planar graphs are
the Lipton-Tarjan separator based approach, and Baker's approach. Demaine and
Hajiaghayi strengthened both approaches using bidimensionality and obtained
EPTASs for a multitude of problems. We unify the two strenghtened approaches to
combine the best of both worlds. At the heart of our framework is a
decomposition lemma which states that for &quot;most&quot; bidimensional problems, there
is a polynomial time algorithm which given an H-minor-free graph G as input and
an e &gt; 0 outputs a vertex set X of size e * OPT such that the treewidth of G n
X is f(e). Here, OPT is the objective function value of the problem in question
and f is a function depending only on e. This allows us to obtain EPTASs on
(apex)-minor-free graphs for all problems covered by the previous framework, as
well as for a wide range of packing problems, partial covering problems and
problems that are neither closed under taking minors, nor contractions. To the
best of our knowledge for many of these problems including cycle packing,
vertex-h-packing, maximum leaf spanning tree, and partial r-dominating set no
EPTASs on planar graphs were previously known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5462</identifier>
 <datestamp>2010-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5462</id><created>2010-05-29</created><updated>2010-06-12</updated><authors><author><keyname>Mirzal</keyname><forenames>Andri</forenames></author><author><keyname>Furukawa</keyname><forenames>Masashi</forenames></author></authors><title>On the clustering aspect of nonnegative matrix factorization</title><categories>cs.LG</categories><comments>4 pages, no figure, to appear in ICEIE 2010</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper provides a theoretical explanation on the clustering aspect of
nonnegative matrix factorization (NMF). We prove that even without imposing
orthogonality nor sparsity constraint on the basis and/or coefficient matrix,
NMF still can give clustering results, thus providing a theoretical support for
many works, e.g., Xu et al. [1] and Kim et al. [2], that show the superiority
of the standard NMF as a clustering method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5466</identifier>
 <datestamp>2010-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5466</id><created>2010-05-29</created><authors><author><keyname>Buk</keyname><forenames>Solomiya</forenames></author></authors><title>Quantitative parametrization of texts written by Ivan Franko: An attempt
  of the project</title><categories>cs.CL</categories><comments>20 pages, in Ukrainian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the article, the project of quantitative parametrization of all texts by
Ivan Franko is manifested. It can be made only by using modern computer
techniques after the frequency dictionaries for all Franko's works are
compiled. The paper describes the application spheres, methodology, stages,
principles and peculiarities in the compilation of the frequency dictionary of
the second half of the 19th century - the beginning of the 20th century. The
relation between the Ivan Franko frequency dictionary, explanatory dictionary
of writer's language and text corpus is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5475</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5475</id><created>2010-05-29</created><authors><author><keyname>Bradonjic</keyname><forenames>Milan</forenames></author><author><keyname>Hagberg</keyname><forenames>Aric</forenames></author><author><keyname>Hengartner</keyname><forenames>Nicolas W.</forenames></author><author><keyname>Percus</keyname><forenames>Allon G.</forenames></author></authors><title>Component Evolution in General Random Intersection Graphs</title><categories>cs.DM math.CO math.PR</categories><doi>10.1007/978-3-642-18009-5_5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random intersection graphs (RIGs) are an important random structure with
applications in social networks, epidemic networks, blog readership, and
wireless sensor networks. RIGs can be interpreted as a model for large randomly
formed non-metric data sets. We analyze the component evolution in general
RIGs, and give conditions on existence and uniqueness of the giant component.
Our techniques generalize existing methods for analysis of component evolution:
we analyze survival and extinction properties of a dependent, inhomogeneous
Galton-Watson branching process on general RIGs. Our analysis relies on
bounding the branching processes and inherits the fundamental concepts of the
study of component evolution in Erd\H{o}s-R\'enyi graphs. The major challenge
comes from the underlying structure of RIGs, which involves its both the set of
nodes and the set of attributes, as well as the set of different probabilities
among the nodes and attributes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5489</identifier>
 <datestamp>2010-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5489</id><created>2010-05-29</created><authors><author><keyname>Jucovschi</keyname><forenames>Constantin</forenames></author><author><keyname>Kohlhase</keyname><forenames>Michael</forenames></author></authors><title>sTeXIDE: An Integrated Development Environment for sTeX Collections</title><categories>cs.OH</categories><comments>To appear in The 9th International Conference on Mathematical
  Knowledge Management: MKM 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Authoring documents in MKM formats like OMDoc is a very tedious task. After
years of working on a semantically annotated corpus of sTeX documents (GenCS),
we identified a set of common, time-consuming subtasks, which can be supported
in an integrated authoring environment. We have adapted the modular Eclipse IDE
into sTeXIDE, an authoring solution for enhancing productivity in contributing
to sTeX based corpora. sTeXIDE supports context-aware command completion,
module management, semantic macro retrieval, and theory graph navigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5490</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5490</id><created>2010-05-29</created><updated>2010-06-06</updated><authors><author><keyname>Formato</keyname><forenames>Richard A.</forenames></author></authors><title>On the Utility of Directional Information for Repositioning Errant
  Probes in Central Force Optimization</title><categories>cs.OH</categories><comments>Ver. 2, 6 June 2010 (Fig. 1 improved for clarity; minor typos
  corrected)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Central Force Optimization is a global search and optimization algorithm that
searches a decision space by flying &quot;probes&quot; whose trajectories are
deterministically computed using two equations of motion. Because it is
possible for a probe to fly outside the domain of feasible solutions, a simple
errant probe retrieval method has been used previously that does not include
the directional information contained in a probe's acceleration vector. This
note investigates the effect of adding directionality to the &quot;repositioning
factor&quot; approach. As a general proposition, it appears that doing so does not
improve convergence speed or accuracy. In fact, adding directionality to the
original errant probe retrieval scheme appears to be highly inadvisable.
Nevertheless, there may be alternative probe retrieval schemes that do benefit
from directional information, and the results reported here may assist in or
encourage their development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5507</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5507</id><created>2010-05-30</created><authors><author><keyname>Chatterji</keyname><forenames>Samaresh</forenames></author><author><keyname>Gandhi</keyname><forenames>Ratnik</forenames></author></authors><title>An Algebraic Approach for Computing Equilibria of a Subclass of Finite
  Normal Form Games</title><categories>cs.GT cs.SC</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Nash equilibrium has become important solution concept for analyzing the
decision making in Game theory. In this paper, we consider the problem of
computing Nash equilibria of a subclass of generic finite normal form games. We
define &quot;rational payoff irrational equilibria games&quot; to be the games with all
rational payoffs and all irrational equilibria. We present a purely algebraic
method for computing all Nash equilibria of these games that uses knowledge of
Galois groups. Some results, showing properties of the class of games, and an
example to show working of the method concludes the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5513</identifier>
 <datestamp>2010-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5513</id><created>2010-05-30</created><authors><author><keyname>Ailon</keyname><forenames>Nir</forenames></author><author><keyname>Liberty</keyname><forenames>Edo</forenames></author></authors><title>Almost Optimal Unrestricted Fast Johnson-Lindenstrauss Transform</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problems of random projections and sparse reconstruction have much in
common and individually received much attention. Surprisingly, until now they
progressed in parallel and remained mostly separate. Here, we employ new tools
from probability in Banach spaces that were successfully used in the context of
sparse reconstruction to advance on an open problem in random pojection. In
particular, we generalize and use an intricate result by Rudelson and Vershynin
for sparse reconstruction which uses Dudley's theorem for bounding Gaussian
processes. Our main result states that any set of $N = \exp(\tilde{O}(n))$ real
vectors in $n$ dimensional space can be linearly mapped to a space of dimension
$k=O(\log N\polylog(n))$, while (1) preserving the pairwise distances among the
vectors to within any constant distortion and (2) being able to apply the
transformation in time $O(n\log n)$ on each vector. This improves on the best
known $N = \exp(\tilde{O}(n^{1/2}))$ achieved by Ailon and Liberty and $N =
\exp(\tilde{O}(n^{1/3}))$ by Ailon and Chazelle.
  The dependence in the distortion constant however is believed to be
suboptimal and subject to further investigation. For constant distortion, this
settles the open question posed by these authors up to a $\polylog(n)$ factor
while considerably simplifying their constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5514</identifier>
 <datestamp>2011-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5514</id><created>2010-05-30</created><authors><author><keyname>Delveroudis</keyname><forenames>Yannis</forenames></author><author><keyname>Lekeas</keyname><forenames>Paraskevas V.</forenames></author></authors><title>Managing Semantic Loss during Query Reformulation in Peer Data
  Management Systems</title><categories>cs.DB</categories><comments>SWOD '07 Proceedings of the 2007 IEEE International Workshop on
  Databases for Next Generation Researchers</comments><doi>10.1109/SWOD.2007.353199</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we deal with the notion of semantic loss in Peer Data
Management Systems (PDMS) queries. We define such a notion and we give a
mechanism that discovers semantic loss in a PDMS network. Next, we propose an
algorithm that addresses the problem of restoring such a loss. Further
evaluation of our proposed algorithm is an ongoing work
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5516</identifier>
 <datestamp>2010-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5516</id><created>2010-05-30</created><updated>2010-06-06</updated><authors><author><keyname>Brenes</keyname><forenames>David J.</forenames></author><author><keyname>Gayo-Avello</keyname><forenames>Daniel</forenames></author><author><keyname>Garcia</keyname><forenames>Rodrigo</forenames></author></authors><title>On the Fly Query Entity Decomposition Using Snippets</title><categories>cs.IR</categories><comments>Extended version of paper submitted to CERI 2010</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  One of the most important issues in Information Retrieval is inferring the
intents underlying users' queries. Thus, any tool to enrich or to better
contextualized queries can proof extremely valuable. Entity extraction,
provided it is done fast, can be one of such tools. Such techniques usually
rely on a prior training phase involving large datasets. That training is
costly, specially in environments which are increasingly moving towards real
time scenarios where latency to retrieve fresh informacion should be minimal.
In this paper an `on-the-fly' query decomposition method is proposed. It uses
snippets which are mined by means of a na\&quot;ive statistical algorithm. An
initial evaluation of such a method is provided, in addition to a discussion on
its applicability to different scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5520</identifier>
 <datestamp>2010-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5520</id><created>2010-05-30</created><updated>2010-12-12</updated><authors><author><keyname>Cheilaris</keyname><forenames>Panagiotis</forenames></author><author><keyname>Smorodinsky</keyname><forenames>Shakhar</forenames></author><author><keyname>Sulovsk&#xfd;</keyname><forenames>Marek</forenames></author></authors><title>The potential to improve the choice: list conflict-free coloring for
  geometric hypergraphs</title><categories>math.CO cs.CG cs.DM cs.DS</categories><comments>16 pages; we simplify and extend our previous results using a new
  potential method; new coauthor (Marek Sulovsk\'y) added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a geometric hypergraph (or a range-space) $H=(V,\cal E)$, a coloring of
its vertices is said to be conflict-free if for every hyperedge $S \in \cal E$
there is at least one vertex in $S$ whose color is distinct from the colors of
all other vertices in $S$. The study of this notion is motivated by frequency
assignment problems in wireless networks. We study the list-coloring (or
choice) version of this notion. In this version, each vertex is associated with
a set of (admissible) colors and it is allowed to be colored only with colors
from its set. List coloring arises naturally in the context of wireless
networks.
  Our main result is a list coloring algorithm based on a new potential method.
The algorithm produces a stronger unique-maximum coloring, in which colors are
positive integers and the maximum color in every hyperedge occurs uniquely. As
a corollary, we provide asymptotically sharp bounds on the size of the lists
required to assure the existence of such unique-maximum colorings for many
geometric hypergraphs (e.g., discs or pseudo-discs in the plane or points with
respect to discs). Moreover, we provide an algorithm, such that, given a family
of lists with the appropriate sizes, computes such a coloring from these lists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5525</identifier>
 <datestamp>2012-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5525</id><created>2010-05-30</created><updated>2012-02-13</updated><authors><author><keyname>Karapetyan</keyname><forenames>Daniel</forenames></author><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author></authors><title>Efficient Local Search Algorithms for Known and New Neighborhoods for
  the Generalized Traveling Salesman Problem</title><categories>cs.DS</categories><comments>29 pages</comments><journal-ref>European Journal of Operational Research 219 (2012) 234-251</journal-ref><doi>10.1016/j.ejor.2012.01.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Generalized Traveling Salesman Problem (GTSP) is a well-known
combinatorial optimization problem with a host of applications. It is an
extension of the Traveling Salesman Problem (TSP) where the set of cities is
partitioned into so-called clusters, and the salesman has to visit every
cluster exactly once.
  While the GTSP is a very important combinatorial optimization problem and is
well studied in many aspects, the local search algorithms used in the
literature are mostly basic adaptations of simple TSP heuristics. Hence, a
thorough and deep research of the neighborhoods and local search algorithms
specific to the GTSP is required.
  We formalize the procedure of adaptation of a TSP neighborhood for the GTSP
and classify all other existing and some new GTSP neighborhoods. For every
neighborhood, we provide efficient exploration algorithms that are often
significantly faster than the ones known from the literature. Finally, we
compare different local search implementations empirically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5543</identifier>
 <datestamp>2011-04-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5543</id><created>2010-05-30</created><updated>2011-04-04</updated><authors><author><keyname>Davidson</keyname><forenames>Susan B.</forenames></author><author><keyname>Khanna</keyname><forenames>Sanjeev</forenames></author><author><keyname>Milo</keyname><forenames>Tova</forenames></author><author><keyname>Panigrahi</keyname><forenames>Debmalya</forenames></author><author><keyname>Roy</keyname><forenames>Sudeepa</forenames></author></authors><title>Provenance Views for Module Privacy</title><categories>cs.DB cs.DS</categories><report-no>UPenn Tech Report MS-CIS-10-22</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific workflow systems increasingly store provenance information about
the module executions used to produce a data item, as well as the parameter
settings and intermediate data items passed between module executions. However,
authors/owners of workflows may wish to keep some of this information
confidential. In particular, a module may be proprietary, and users should not
be able to infer its behavior by seeing mappings between all data inputs and
outputs. The problem we address in this paper is the following: Given a
workflow, abstractly modeled by a relation R, a privacy requirement \Gamma and
costs associated with data. The owner of the workflow decides which data
(attributes) to hide, and provides the user with a view R' which is the
projection of R over attributes which have not been hidden. The goal is to
minimize the cost of hidden data while guaranteeing that individual modules are
\Gamma -private. We call this the &quot;secureview&quot; problem. We formally define the
problem, study its complexity, and offer algorithmic solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5553</identifier>
 <datestamp>2010-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5553</id><created>2010-05-30</created><updated>2010-10-29</updated><authors><author><keyname>Korchenko</keyname><forenames>Oleksandr</forenames></author><author><keyname>Vasiliu</keyname><forenames>Yevhen</forenames></author><author><keyname>Gnatyuk</keyname><forenames>Sergiy</forenames></author></authors><title>Modern Quantum Technologies of Information Security</title><categories>cs.CR</categories><comments>11 pages, 3 figures, 80 references, text is corrected</comments><msc-class>94A60, 81P94</msc-class><acm-class>D.4.6</acm-class><journal-ref>Aviation. Vilnius: Technika, 2010, Vol. 14, No. 2 , p. 58-69</journal-ref><doi>10.3846/aviation.2010.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the systematisation and classification of modern quantum
technologies of information security against cyber-terrorist attack are carried
out. The characteristic of the basic directions of quantum cryptography from
the viewpoint of the quantum technologies used is given. A qualitative analysis
of the advantages and disadvantages of concrete quantum protocols is made. The
current status of the problem of practical quantum cryptography use in
telecommunication networks is considered. In particular, a short review of
existing commercial systems of quantum key distribution is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5556</identifier>
 <datestamp>2010-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5556</id><created>2010-05-30</created><updated>2010-06-03</updated><authors><author><keyname>Iqbal</keyname><forenames>Ridwan Al</forenames></author></authors><title>Empirical learning aided by weak domain knowledge in the form of feature
  importance</title><categories>cs.LG cs.AI cs.NE</categories><comments>9 pages, 1 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard hybrid learners that use domain knowledge require stronger knowledge
that is hard and expensive to acquire. However, weaker domain knowledge can
benefit from prior knowledge while being cost effective. Weak knowledge in the
form of feature relative importance (FRI) is presented and explained. Feature
relative importance is a real valued approximation of a feature's importance
provided by experts. Advantage of using this knowledge is demonstrated by IANN,
a modified multilayer neural network algorithm. IANN is a very simple
modification of standard neural network algorithm but attains significant
performance gains. Experimental results in the field of molecular biology show
higher performance over other empirical learning algorithms including standard
backpropagation and support vector machines. IANN performance is even
comparable to a theory refinement system KBANN that uses stronger domain
knowledge. This shows Feature relative importance can improve performance of
existing empirical learning algorithms significantly with minimal effort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5574</identifier>
 <datestamp>2010-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5574</id><created>2010-05-30</created><authors><author><keyname>Xing</keyname><forenames>Chengwen</forenames></author><author><keyname>Ma</keyname><forenames>Shaodan</forenames></author><author><keyname>Wu</keyname><forenames>Yik-Chung</forenames></author><author><keyname>Ng</keyname><forenames>Tung-Sang</forenames></author></authors><title>Robust Beamforming for Amplify-and-Forward MIMO Relay Systems Based on
  Quadratic Matrix Programming</title><categories>cs.IT math.IT</categories><comments>Proceedings of IEEE International Conference on Acoustics, Speech,
  and Signal Processing (ICASSP'2010), U.S.A</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, robust transceiver design based on minimum-mean-square-error
(MMSE) criterion for dual-hop amplify-and-forward MIMO relay systems is
investigated. The channel estimation errors are modeled as Gaussian random
variables, and then the effect are incorporated into the robust transceiver
based on the Bayesian framework. An iterative algorithm is proposed to jointly
design the precoder at the source, the forward matrix at the relay and the
equalizer at the destination, and the joint design problem can be efficiently
solved by quadratic matrix programming (QMP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5577</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5577</id><created>2010-05-30</created><updated>2012-09-17</updated><authors><author><keyname>Xing</keyname><forenames>Chengwen</forenames></author><author><keyname>Ma</keyname><forenames>Shaodan</forenames></author><author><keyname>Wu</keyname><forenames>Yik-Chung</forenames></author><author><keyname>Ng</keyname><forenames>Tung-Sang</forenames></author></authors><title>Transceiver Design for Dual-Hop Non-regenerative MIMO-OFDM Relay Systems
  Under Channel Uncertainties</title><categories>cs.IT math.IT</categories><comments>30 pages, 6 figures, IEEE Transactions on Signal Processing, The
  Final Version</comments><doi>10.1109/TSP.2010.2070797</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, linear transceiver design for dual-hop non-regenerative
(amplify-and-forward (AF)) MIMO-OFDM systems under channel estimation errors is
investigated. Second order moments of channel estimation errors in the two hops
are first deduced. Then based on the Bayesian framework, joint design of linear
forwarding matrix at the relay and equalizer at the destination under channel
estimation errors is proposed to minimize the total mean-square-error (MSE) of
the output signal at the destination. The optimal designs for both correlated
and uncorrelated channel estimation errors are considered. The relationship
with existing algorithms is also disclosed. Moreover, this design is extended
to the joint design involving source precoder design. Simulation results show
that the proposed design outperforms the design based on estimated channel
state information only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5581</identifier>
 <datestamp>2010-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5581</id><created>2010-05-30</created><updated>2010-10-29</updated><authors><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi-Hua</forenames></author></authors><title>Multi-View Active Learning in the Non-Realizable Case</title><categories>cs.LG</categories><comments>22 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sample complexity of active learning under the realizability assumption
has been well-studied. The realizability assumption, however, rarely holds in
practice. In this paper, we theoretically characterize the sample complexity of
active learning in the non-realizable case under multi-view setting. We prove
that, with unbounded Tsybakov noise, the sample complexity of multi-view active
learning can be $\widetilde{O}(\log\frac{1}{\epsilon})$, contrasting to
single-view setting where the polynomial improvement is the best possible
achievement. We also prove that in general multi-view setting the sample
complexity of active learning with unbounded Tsybakov noise is
$\widetilde{O}(\frac{1}{\epsilon})$, where the order of $1/\epsilon$ is
independent of the parameter in Tsybakov noise, contrasting to previous
polynomial bounds where the order of $1/\epsilon$ is related to the parameter
in Tsybakov noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5582</identifier>
 <datestamp>2011-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5582</id><created>2010-05-31</created><updated>2011-08-25</updated><authors><author><keyname>Aybat</keyname><forenames>Necdet Serhat</forenames></author><author><keyname>Iyengar</keyname><forenames>Garud</forenames></author></authors><title>A First-order Augmented Lagrangian Method for Compressed Sensing</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a first-order augmented Lagrangian algorithm (FAL) for solving the
basis pursuit problem. FAL computes a solution to this problem by inexactly
solving a sequence of L1-regularized least squares sub-problems. These
sub-problems are solved using an infinite memory proximal gradient algorithm
wherein each update reduces to &quot;shrinkage&quot; or constrained &quot;shrinkage&quot;. We show
that FAL converges to an optimal solution of the basis pursuit problem whenever
the solution is unique, which is the case with very high probability for
compressed sensing problems. We construct a parameter sequence such that the
corresponding FAL iterates are eps-feasible and eps-optimal for all eps&gt;0
within O(log(1/eps)) FAL iterations. Moreover, FAL requires at most O(1/eps)
matrix-vector multiplications of the form Ax or A^Ty to compute an
eps-feasible, eps-optimal solution. We show that FAL can be easily extended to
solve the basis pursuit denoising problem when there is a non-trivial level of
noise on the measurements. We report the results of numerical experiments
comparing FAL with the state-of-the-art algorithms for both noisy and noiseless
compressed sensing problems. A striking property of FAL that we observed in the
numerical experiments with randomly generated instances when there is no
measurement noise was that FAL always correctly identifies the support of the
target signal without any thresholding or post-processing, for moderately small
error tolerance values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5584</identifier>
 <datestamp>2010-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5584</id><created>2010-05-31</created><authors><author><keyname>Sly</keyname><forenames>Allan</forenames></author></authors><title>Computational Transition at the Uniqueness Threshold</title><categories>cs.CC math-ph math.MP math.PR</categories><comments>33 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hardcore model is a model of lattice gas systems which has received much
attention in statistical physics, probability theory and theoretical computer
science. It is the probability distribution over independent sets $I$ of a
graph weighted proportionally to $\lambda^{|I|}$ with fugacity parameter
$\lambda$. We prove that at the uniqueness threshold of the hardcore model on
the $d$-regular tree, approximating the partition function becomes
computationally hard on graphs of maximum degree $d$.
  Specifically, we show that unless NP$=$RP there is no polynomial time
approximation scheme for the partition function (the sum of such weighted
independent sets) on graphs of maximum degree $d$ for fugacity $\lambda_c(d) &lt;
\lambda &lt; \lambda_c(d) + \epsilon(d)$ where $\lambda_c =
\frac{(d-1)^{d-1}}{(d-2)^d}$ is the uniqueness threshold on the $d$-regular
tree and $\epsilon(d)&gt;0$. Weitz produced an FPTAS for approximating the
partition function when $0&lt;\lambda &lt; \lambda_c(d)$ so this result demonstrates
that the computational threshold exactly coincides with the statistical physics
phase transition thus confirming the main conjecture of [28]. We further
analyze the special case of $\lambda=1, d=6$ and show there is no polynomial
time algorithm for approximately counting independent sets on graphs of maximum
degree $d= 6$ which is optimal.
  Our proof is based on specially constructed random bi-partite graphs which
act as gadgets in a reduction to MAX-CUT. Building on the second moment method
analysis of [28] and combined with an analysis of the reconstruction problem on
the tree our proof establishes a strong version of 'replica' method heuristics
developed by theoretical physicists. The result establishes the first rigorous
correspondence between the hardness of approximate counting and sampling with
statistical physics phase transitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5591</identifier>
 <datestamp>2010-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5591</id><created>2010-05-31</created><authors><author><keyname>Shieh</keyname><forenames>Min-Zheng</forenames></author><author><keyname>Tsai</keyname><forenames>Shi-Chun</forenames></author></authors><title>On the minimum weight problem of permutation codes under Chebyshev
  distance</title><categories>cs.IT math.IT</categories><comments>5 pages. ISIT 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Permutation codes of length $n$ and distance $d$ is a set of permutations on
$n$ symbols, where the distance between any two elements in the set is at least
$d$. Subgroup permutation codes are permutation codes with the property that
the elements are closed under the operation of composition. In this paper,
under the distance metric $\ell_{\infty}$-norm, we prove that finding the
minimum weight codeword for subgroup permutation code is NP-complete. Moreover,
we show that it is NP-hard to approximate the minimum weight within the factor
$7/6-\epsilon$ for any $\epsilon&gt;0$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5596</identifier>
 <datestamp>2010-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5596</id><created>2010-05-31</created><authors><author><keyname>Constant</keyname><forenames>Matthieu</forenames><affiliation>IGM-LabInfo</affiliation></author><author><keyname>Tolone</keyname><forenames>Elsa</forenames><affiliation>IGM-LabInfo</affiliation></author></authors><title>A generic tool to generate a lexicon for NLP from Lexicon-Grammar tables</title><categories>cs.CL</categories><proxy>ccsd</proxy><journal-ref>Actes du 27e Colloque international sur le lexique et la grammaire
  (L'Aquila, 10-13 septembre 2008). Seconde partie, Michele De Gioia (Ed.)
  (2010) pages 79-93</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lexicon-Grammar tables constitute a large-coverage syntactic lexicon but they
cannot be directly used in Natural Language Processing (NLP) applications
because they sometimes rely on implicit information. In this paper, we
introduce LGExtract, a generic tool for generating a syntactic lexicon for NLP
from the Lexicon-Grammar tables. It is based on a global table that contains
undefined information and on a unique extraction script including all
operations to be performed for all tables. We also present an experiment that
has been conducted to generate a new lexicon of French verbs and predicative
nouns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5602</identifier>
 <datestamp>2011-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5602</id><created>2010-05-31</created><updated>2011-05-16</updated><authors><author><keyname>Aubry</keyname><forenames>Yves</forenames><affiliation>IML</affiliation></author><author><keyname>Godin</keyname><forenames>Jean-Christophe</forenames><affiliation>IML</affiliation></author><author><keyname>Togni</keyname><forenames>Olivier</forenames><affiliation>Le2i</affiliation></author></authors><title>Choosability of a weighted path and free-choosability of a cycle</title><categories>math.CO cs.DM</categories><comments>9 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $G$ with a list of colors $L(v)$ and weight $w(v)$ for each vertex
$v$ is $(L,w)$-colorable if one can choose a subset of $w(v)$ colors from
$L(v)$ for each vertex $v$, such that adjacent vertices receive disjoint color
sets. In this paper, we give necessary and sufficient conditions for a weighted
path to be $(L,w)$-colorable for some list assignments $L$. Furthermore, we
solve the problem of the free-choosability of a cycle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5603</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5603</id><created>2010-05-31</created><updated>2014-12-26</updated><authors><author><keyname>Ryabko</keyname><forenames>Daniil</forenames><affiliation>INRIA Lille</affiliation></author></authors><title>On the Relation between Realizable and Nonrealizable Cases of the
  Sequence Prediction Problem</title><categories>cs.LG cs.IT math.IT math.ST stat.TH</categories><comments>Earlier conference version appeared as: &quot;Sequence prediction in
  realizable and non-realizable cases,&quot; Conference on Learning Theory (2010)
  119-131</comments><proxy>ccsd</proxy><journal-ref>Journal of Machine Learning Research, vol. 12: 2161-2180, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sequence $x_1,\dots,x_n,\dots$ of discrete-valued observations is generated
according to some unknown probabilistic law (measure) $\mu$. After observing
each outcome, one is required to give conditional probabilities of the next
observation. The realizable case is when the measure $\mu$ belongs to an
arbitrary but known class $\mathcal C$ of process measures. The non-realizable
case is when $\mu$ is completely arbitrary, but the prediction performance is
measured with respect to a given set $\mathcal C$ of process measures. We are
interested in the relations between these problems and between their solutions,
as well as in characterizing the cases when a solution exists and finding these
solutions. We show that if the quality of prediction is measured using the
total variation distance, then these problems coincide, while if it is measured
using the expected average KL divergence, then they are different. For some of
the formalizations we also show that when a solution exists, it can be obtained
as a Bayes mixture over a countable subset of $\mathcal C$. We also obtain
several characterization of those sets $\mathcal C$ for which solutions to the
considered problems exist. As an illustration to the general results obtained,
we show that a solution to the non-realizable case of the sequence prediction
problem exists for the set of all finite-memory processes, but does not exist
for the set of all stationary processes.
  It should be emphasized that the framework is completely general: the
processes measures considered are not required to be i.i.d., mixing,
stationary, or to belong to any parametric family.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5606</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5606</id><created>2010-05-31</created><authors><author><keyname>Kesavaraja</keyname><forenames>D.</forenames><affiliation>Sivanthi Aditanar College of Engineering, India</affiliation></author><author><keyname>Balasubramanian</keyname><forenames>R.</forenames><affiliation>Manonmaniam Sundaranar University, India</affiliation></author><author><keyname>Sasireka</keyname><forenames>D.</forenames><affiliation>PSN College of Engineering and Technology, India</affiliation></author></authors><title>Implementation of a Cloud Data Server (CDS) for Providing Secure Service
  in E-Business</title><categories>cs.DC</categories><comments>12 Pages, IJDMS 2010</comments><journal-ref>International Journal of Database Management Systems 2.2 (2010)
  44-55</journal-ref><doi>10.5121/ijdms.2010.2203</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Cloud Data Servers is the novel approach for providing secure service to
e-business .Millions of users are surfing the Cloud for various purposes,
therefore they need highly safe and persistent services. Usually hackers target
particular Operating Systems or a Particular Controller. Inspiteof several
ongoing researches Conventional Web Servers and its Intrusion Detection System
might not be able to detect such attacks. So we implement a Cloud Data Server
with Session Controller Architecture using Redundancy and Disconnected Data
Access Mechanism. In this paper, we generate the hash code using MD5 algorithm.
With the help of which we can circumvent even the attacks, which are undefined
by traditional Systems .we implement Cloud Data Sever using Java and Hash Code
backup Management using My SQL. Here we Implement AES Algorithm for providing
more Security for the hash Code. The CDS using the Virtual Controller controls
and monitors the Connections and modifications of the page so as to prevent
malicious users from hacking the website. In the proposed approach an activity
analyzer takes care of intimating the administrator about possible intrusions
and the counter measures required to tackle them. The efficiency ratio of our
approach is 98.21% compared with similar approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5608</identifier>
 <datestamp>2010-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5608</id><created>2010-05-31</created><authors><author><keyname>Finkel</keyname><forenames>Olivier</forenames><affiliation>ELM</affiliation></author></authors><title>On Infinitary Rational Relations and Borel Sets</title><categories>cs.LO cs.CC math.LO</categories><proxy>ccsd</proxy><journal-ref>Fourth International Conference on Discrete Mathematics and
  Theoretical Computer Science DMTCS'03, 7 - 12 July 2003, Dijon, France.,
  France (2003)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove in this paper that there exists some infinitary rational relations
which are Sigma^0_3-complete Borel sets and some others which are
Pi^0_3-complete. This implies that there exists some infinitary rational
relations which are Delta^0_4-sets but not (Sigma^0_3U Pi^0_3)-sets. These
results give additional answers to questions of Simonnet and of Lescow and
Thomas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5610</identifier>
 <datestamp>2010-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5610</id><created>2010-05-31</created><updated>2010-06-11</updated><authors><author><keyname>Emiris</keyname><forenames>Ioannis Z.</forenames><affiliation>DI</affiliation></author><author><keyname>Mourrain</keyname><forenames>Bernard</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Tsigaridas</keyname><forenames>Elias</forenames></author></authors><title>The DMM bound: multivariate (aggregate) separation bounds</title><categories>cs.SC</categories><proxy>ccsd</proxy><journal-ref>International Symposium on Symbolic and Algebraic Computation
  (ISSAC), Munich : Germany (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we derive aggregate separation bounds, named after
Davenport-Mahler-Mignotte (\dmm), on the isolated roots of polynomial systems,
specifically on the minimum distance between any two such roots. The bounds
exploit the structure of the system and the height of the sparse (or toric)
resultant by means of mixed volume, as well as recent advances on aggregate
root bounds for univariate polynomials, and are applicable to arbitrary
positive dimensional systems. We improve upon Canny's gap theorem
\cite{c-crmp-87} by a factor of $\OO(d^{n-1})$, where $d$ bounds the degree of
the polynomials, and $n$ is the number of variables. One application is to the
bitsize of the eigenvalues and eigenvectors of an integer matrix, which also
yields a new proof that the problem is polynomial. We also compare against
recent lower bounds on the absolute value of the root coordinates by Brownawell
and Yap \cite{by-issac-2009}, obtained under the hypothesis there is a
0-dimensional projection. Our bounds are in general comparable, but exploit
sparseness; they are also tighter when bounding the value of a positive
polynomial over the simplex. For this problem, we also improve upon the bounds
in \cite{bsr-arxix-2009,jp-arxiv-2009}. Our analysis provides a precise
asymptotic upper bound on the number of steps that subdivision-based algorithms
perform in order to isolate all real roots of a polynomial system. This leads
to the first complexity bound of Milne's algorithm \cite{Miln92} in 2D.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5613</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5613</id><created>2010-05-31</created><authors><author><keyname>Khan</keyname><forenames>Murtaza Ali</forenames><affiliation>Royal University for Women, Bahrain</affiliation></author></authors><title>An Automated Algorithm for Approximation of Temporal Video Data Using
  Linear B'EZIER Fitting</title><categories>cs.MM</categories><comments>14 Pages, IJMA 2010</comments><journal-ref>International journal of Multimedia &amp; Its Applications 2.2 (2010)
  81-94</journal-ref><doi>10.5121/ijma.2010.2207</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents an efficient method for approximation of temporal video
data using linear Bezier fitting. For a given sequence of frames, the proposed
method estimates the intensity variations of each pixel in temporal dimension
using linear Bezier fitting in Euclidean space. Fitting of each segment ensures
upper bound of specified mean squared error. Break and fit criteria is employed
to minimize the number of segments required to fit the data. The proposed
method is well suitable for lossy compression of temporal video data and
automates the fitting process of each pixel. Experimental results show that the
proposed method yields good results both in terms of objective and subjective
quality measurement parameters without causing any blocking artifacts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5614</identifier>
 <datestamp>2010-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5614</id><created>2010-05-31</created><authors><author><keyname>Pign&#xe9;</keyname><forenames>Yoann</forenames><affiliation>ILIAS</affiliation></author><author><keyname>Casteigts</keyname><forenames>Arnaud</forenames><affiliation>SITE</affiliation></author><author><keyname>Guinand</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>LITIS</affiliation></author><author><keyname>Chaumette</keyname><forenames>Serge</forenames><affiliation>LaBRI</affiliation></author></authors><title>Construction et maintien d'une for\^et couvrante dans un r\'eseau
  dynamique</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>12\`emes Rencontres Francophones sur les Aspects Algorithmiques de
  T\'el\'ecommunications (AlgoTel), Belle Dune : France (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we introduce the principles of an algorithm that constructs and
maintains a spanning forest in a mobile telecommunication network-a MANET. The
algorithm is based on the random walk of a token and is entirely decentralized.
A probability analysis is performed when the network is static. Then we show
that performances can be slightly enhanced when adding a memory process in the
walk on the token.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5623</identifier>
 <datestamp>2010-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5623</id><created>2010-05-31</created><authors><author><keyname>Barcenas</keyname><forenames>Everardo</forenames><affiliation>INRIA Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Geneves</keyname><forenames>Pierre</forenames><affiliation>INRIA Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Layaida</keyname><forenames>Nabil</forenames><affiliation>INRIA Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Schmitt</keyname><forenames>Alan</forenames><affiliation>INRIA Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author></authors><title>A Tree Logic with Graded Paths and Nominals</title><categories>cs.LO</categories><proxy>ccsd</proxy><report-no>RR-7251</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regular tree grammars and regular path expressions constitute core constructs
widely used in programming languages and type systems. Nevertheless, there has
been little research so far on reasoning frameworks for path expressions where
node cardinality constraints occur along a path in a tree. We present a logic
capable of expressing deep counting along paths which may include arbitrary
recursive forward and backward navigation. The counting extensions can be seen
as a generalization of graded modalities that count immediate successor nodes.
While the combination of graded modalities, nominals, and inverse modalities
yields undecidable logics over graphs, we show that these features can be
combined in a tree logic decidable in exponential time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5628</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5628</id><created>2010-05-31</created><authors><author><keyname>Scholtes</keyname><forenames>Ingo</forenames></author></authors><title>Distributed Creation and Adaptation of Random Scale-Free Overlay
  Networks</title><categories>cs.NI</categories><comments>Under Review for SASO 2010</comments><acm-class>C.2.1</acm-class><journal-ref>SASO 2010, Proceedings of 4th IEEE International Conference on
  Self-Adaptive and Self-Organizing Systems (SASO), 2010</journal-ref><doi>10.1109/SASO.2010.45</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random scale-free overlay topologies provide a number of properties like for
example high resilience against failures of random nodes, small (average)
diameter as well as good expansion and congestion characteristics that make
them interesting for the use in large-scale distributed systems. A number of
these properties have been shown to be influenced by the exponent \gamma of
their degree distribution P(k) ~ k^{-\gamma}. In this article, we present a
distributed rewiring scheme that is suitable to effectuate scale-free overlay
topologies with an adjustable exponent. The scheme uses a biased random walk
strategy to sample new endpoints of edges being rewired and relies on a simple
equilibrium model for scale-free networks. The bias of the random walk strategy
can be tuned to produce random scale-free networks with arbitrary degree
distribution exponents greater than two. We argue that the rewiring strategy
can be implemented in a distributed fashion based on a node's information about
its immediate neighbors. We present both analytical arguments as well as
results that have been obtained using an implementation of the proposed
protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5630</identifier>
 <datestamp>2013-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5630</id><created>2010-05-31</created><authors><author><keyname>Johnen</keyname><forenames>Colette</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Lavallee</keyname><forenames>Ivan</forenames><affiliation>LAISC</affiliation></author><author><keyname>Lavault</keyname><forenames>Christian</forenames><affiliation>LIPN</affiliation></author></authors><title>Reliable Self-Stabilizing Communication for Quasi Rendezvous</title><categories>cs.DC</categories><proxy>ccsd</proxy><journal-ref>Studia Informatica Universalis 1, 1 (2002) 59-76</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents three self-stabilizing protocols for basic fair and
reliable link communication primitives. We assume a link-register communication
model under read/write atomicity, where every process can read from but cannot
write into its neighbours' registers. The first primitive guarantees that any
process writes a new value in its register(s) only after all its neighbours
have read the previous value, whatever the initial scheduling of processes'
actions. The second primitive implements a &quot;weak rendezvous&quot; communication
mechanism by using an alternating bit protocol: whenever a process
consecutively writes n values (possibly the same ones) in a register, each
neighbour is guaranteed to read each value from the register at least once. On
the basis of the previous protocol, the third primitive implements a &quot;quasi
rendezvous&quot;: in words, this primitive ensures furthermore that there exists
exactly one reading between two writing operations All protocols are
self-stabilizing and run in asynchronous arbitrary networks. The goal of the
paper is in handling each primitive by a separate procedure, which can be used
as a &quot;black box&quot; in more involved self-stabilizing protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5631</identifier>
 <datestamp>2010-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5631</id><created>2010-05-31</created><authors><author><keyname>Auger</keyname><forenames>Anne</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Hansen</keyname><forenames>Nikolaus</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Zerpa</keyname><forenames>Jorge M. Perez</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Ros</keyname><forenames>Raymond</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Schoenauer</keyname><forenames>Marc</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author></authors><title>Experimental Comparisons of Derivative Free Optimization Algorithms</title><categories>cs.NA</categories><comments>8th International Symposium on Experimental Algorithms, Dortmund :
  Germany (2009)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the performances of the quasi-Newton BFGS algorithm, the
NEWUOA derivative free optimizer, the Covariance Matrix Adaptation Evolution
Strategy (CMA-ES), the Differential Evolution (DE) algorithm and Particle Swarm
Optimizers (PSO) are compared experimentally on benchmark functions reflecting
important challenges encountered in real-world optimization problems.
Dependence of the performances in the conditioning of the problem and
rotational invariance of the algorithms are in particular investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5633</identifier>
 <datestamp>2010-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5633</id><created>2010-05-31</created><authors><author><keyname>Finkel</keyname><forenames>Olivier</forenames><affiliation>ELM</affiliation></author></authors><title>On Omega Context Free Languages which are Borel Sets of Infinite Rank</title><categories>cs.LO math.LO</categories><comments>The supremum of the set of Borel ranks of omega-context-free
  languages is actually greater than the first non-recursive ordinal. This has
  been proved later in a paper &quot;Borel Ranks and Wadge Degrees of Omega Context
  Free Languages&quot; published in the journal Mathematical Structures in Computer
  Science (2006)</comments><proxy>ccsd</proxy><journal-ref>Theoretical Computer Science 299 (1-3) (2003) 327-346</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is a continuation of the study of topological properties of omega
context free languages (omega-CFL). We proved before that the class of
omega-CFL exhausts the hierarchy of Borel sets of finite rank, and that there
exist some omega-CFL which are analytic but non Borel sets. We prove here that
there exist some omega context free languages which are Borel sets of infinite
(but not finite) rank, giving additional answer to questions of Lescow and
Thomas [Logical Specifications of Infinite Computations, In:&quot;A Decade of
Concurrency&quot;, Springer LNCS 803 (1994), 583-621].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5635</identifier>
 <datestamp>2010-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5635</id><created>2010-05-31</created><authors><author><keyname>Finkel</keyname><forenames>Olivier</forenames><affiliation>ELM</affiliation></author></authors><title>An Effective Extension of the Wagner Hierarchy to Blind Counter Automata</title><categories>cs.LO cs.FL math.LO</categories><proxy>ccsd</proxy><journal-ref>Computer Science Logic , 15th International Workshop, CSL 2001,
  10th Annual Conference of the European Association for Computer Science
  Logic, Paris, September 10-13, 2001., France (2001)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extension of the Wagner hierarchy to blind counter automata accepting
infinite words with a Muller acceptance condition is effective. We determine
precisely this hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5638</identifier>
 <datestamp>2011-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5638</id><created>2010-05-31</created><updated>2011-06-29</updated><authors><author><keyname>Chapouly</keyname><forenames>Marianne</forenames></author><author><keyname>Mirrahimi</keyname><forenames>Mazyar</forenames></author></authors><title>Distributed source identification for wave equations: an observer-based
  approach (full paper)</title><categories>math.OC cs.SY math.AP</categories><comments>26 pages, 2 figures</comments><msc-class>35L20, 35R30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the 1D wave equation where the spatial domain is a
bounded interval. Assuming the initial conditions to be known, we are here
interested in identifying an unknown source term, while we take the Neumann
derivative of the solution on one of the boundaries as the measurement output.
Applying a back-and-forth iterative scheme and constructing well-chosen
observers, we retrieve the source term from the measurement output in the
minimal observation time. We further provide an extension of the method to the
case of wave equations with N dimensional spatial domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5648</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5648</id><created>2010-05-31</created><updated>2010-06-29</updated><authors><author><keyname>Endrullis</keyname><forenames>Joerg</forenames><affiliation>Vrije Universiteit Amsterdam</affiliation></author><author><keyname>Hendriks</keyname><forenames>Dimitri</forenames><affiliation>Vrije Universiteit Amsterdam</affiliation></author></authors><title>Transforming Outermost into Context-Sensitive Rewriting</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>D.1.1, D.3.1, F.4.1, F.4.2, I.1.1, I.1.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 6, Issue 2 (June 29,
  2010) lmcs:1105</journal-ref><doi>10.2168/LMCS-6(2:5)2010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define two transformations from term rewriting systems (TRSs) to
context-sensitive TRSs in such a way that termination of the target system
implies outermost termination of the original system. In the transformation
based on 'context extension', each outermost rewrite step is modeled by exactly
one step in the transformed system. This transformation turns out to be
complete for the class of left-linear TRSs. The second transformation is called
`dynamic labeling' and results in smaller sized context-sensitive TRSs. Here
each modeled step is adjoined with a small number of auxiliary steps. As a
result state-of-the-art termination methods for context-sensitive rewriting
become available for proving termination of outermost rewriting. Both
transformations have been implemented in Jambox, making it the most successful
tool in the category of outermost rewriting of the last edition of the annual
termination competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5662</identifier>
 <datestamp>2010-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5662</id><created>2010-05-31</created><authors><author><keyname>Bergstra</keyname><forenames>Jan A.</forenames></author><author><keyname>Bethke</keyname><forenames>Inge</forenames></author></authors><title>On the contribution of backward jumps to instruction sequence
  expressiveness</title><categories>cs.LO</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the expressiveness of backward jumps in a framework of
formalized sequential programming called program algebra. We show that - if
expressiveness is measured in terms of the computability of partial Boolean
functions - then backward jumps are superfluous. If we, however, want to
prevent explosion of the length of programs, then backward jumps are essential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5674</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5674</id><created>2010-05-31</created><updated>2010-07-01</updated><authors><author><keyname>Shavitt</keyname><forenames>Yuval</forenames></author><author><keyname>Zilberman</keyname><forenames>Noa</forenames></author></authors><title>A Study of Geolocation Databases</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The geographical location of Internet IP addresses has an importance both for
academic research and commercial applications. Thus, both commercial and
academic databases and tools are available for mapping IP addresses to
geographic locations. Evaluating the accuracy of these mapping services is
complex since obtaining diverse large scale ground truth is very hard. In this
work we evaluate mapping services using an algorithm that groups IP addresses
to PoPs, based on structure and delay. This way we are able to group close to
100,000 IP addresses world wide into groups that are known to share a
geo-location with high confidence. We provide insight into the strength and
weaknesses of IP geolocation databases, and discuss their accuracy and
encountered anomalies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5685</identifier>
 <datestamp>2010-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5685</id><created>2010-05-31</created><authors><author><keyname>Romero</keyname><forenames>Ana</forenames></author><author><keyname>Sergeraert</keyname><forenames>Francis</forenames></author></authors><title>Discrete Vector Fields and Fundamental Algebraic Topology</title><categories>math.AT cs.DM</categories><comments>53 pages</comments><msc-class>18G30, 18G40, 55P20, 55T10, 55T20, 55T35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show in this text how the most important homology equivalences of
fundamental Algebraic Topology can be obtained as reductions associated to
discrete vector fields. Mainly the homology equivalences whose existence --
most often non-constructive -- is proved by the main spectral sequences, the
Serre and Eilenberg-Moore spectral sequences. On the contrary, the constructive
existence is here systematically looked for and obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5697</identifier>
 <datestamp>2010-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5697</id><created>2010-05-31</created><authors><author><keyname>Jung</keyname><forenames>Alexander</forenames></author><author><keyname>Ben-Haim</keyname><forenames>Zvika</forenames></author><author><keyname>Hlawatsch</keyname><forenames>Franz</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Unbiased Estimation of a Sparse Vector in White Gaussian Noise</title><categories>math.ST cs.IT math.IT stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider unbiased estimation of a sparse nonrandom vector corrupted by
additive white Gaussian noise. We show that while there are infinitely many
unbiased estimators for this problem, none of them has uniformly minimum
variance. Therefore, we focus on locally minimum variance unbiased (LMVU)
estimators. We derive simple closed-form lower and upper bounds on the variance
of LMVU estimators or, equivalently, on the Barankin bound (BB). Our bounds
allow an estimation of the threshold region separating the low-SNR and high-SNR
regimes, and they indicate the asymptotic behavior of the BB at high SNR. We
also develop numerical lower and upper bounds which are tighter than the
closed-form bounds and thus characterize the BB more accurately. Numerical
studies compare our characterization of the BB with established biased
estimation schemes, and demonstrate that while unbiased estimators perform
poorly at low SNR, they may perform better than biased estimators at high SNR.
An interesting conclusion of our analysis is that the high-SNR behavior of the
BB depends solely on the value of the smallest nonzero component of the sparse
vector, and that this type of dependence is also exhibited by the performance
of certain practical estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5698</identifier>
 <datestamp>2012-06-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5698</id><created>2010-05-31</created><updated>2012-06-25</updated><authors><author><keyname>Menton</keyname><forenames>Curtis</forenames></author></authors><title>Normalized Range Voting Broadly Resists Control</title><categories>cs.GT cs.CC</categories><report-no>TR956</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the behavior of Range Voting and Normalized Range Voting with
respect to electoral control. Electoral control encompasses attempts from an
election chair to alter the structure of an election in order to change the
outcome. We show that a voting system resists a case of control by proving that
performing that case of control is computationally infeasible. Range Voting is
a natural extension of approval voting, and Normalized Range Voting is a simple
variant which alters each vote to maximize the potential impact of each voter.
We show that Normalized Range Voting has among the largest number of control
resistances among natural voting systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5712</identifier>
 <datestamp>2010-06-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5712</id><created>2010-05-31</created><authors><author><keyname>Vabishchevich</keyname><forenames>Petr N.</forenames></author></authors><title>SM stability for time-dependent problems</title><categories>cs.NA</categories><comments>12 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various classes of stable finite difference schemes can be constructed to
obtain a numerical solution. It is important to select among all stable schemes
such a scheme that is optimal in terms of certain additional criteria. In this
study, we use a simple boundary value problem for a one-dimensional parabolic
equation to discuss the selection of an approximation with respect to time. We
consider the pure diffusion equation, the pure convective transport equation
and combined convection-diffusion phenomena. Requirements for the
unconditionally stable finite difference schemes are formulated that are
related to retaining the main features of the differential problem. The concept
of SM stable finite difference scheme is introduced. The starting point are
difference schemes constructed on the basis of the various Pad$\acute{e}$
approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5718</identifier>
 <datestamp>2012-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5718</id><created>2010-05-31</created><updated>2011-07-25</updated><authors><author><keyname>Caticha</keyname><forenames>Nestor</forenames></author><author><keyname>Vicente</keyname><forenames>Renato</forenames></author></authors><title>Agent-based Social Psychology: from Neurocognitive Processes to Social
  Data</title><categories>physics.soc-ph cs.SI q-bio.NC</categories><comments>11 pages, 4 figures, 2 C codes, to appear in Advances in Complex
  Systems</comments><journal-ref>Advances in Complex Systems, Vol. 14, No. 5 (2011) 711--731</journal-ref><doi>10.1142/S0219525911003190</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Moral Foundation Theory states that groups of different observers may rely on
partially dissimilar sets of moral foundations, thereby reaching different
moral valuations. The use of functional imaging techniques has revealed a
spectrum of cognitive styles with respect to the differential handling of novel
or corroborating information that is correlated to political affiliation. Here
we characterize the collective behavior of an agent-based model whose inter
individual interactions due to information exchange in the form of opinions are
in qualitative agreement with experimental neuroscience data. The main
conclusion derived connects the existence of diversity in the cognitive
strategies and statistics of the sets of moral foundations and suggests that
this connection arises from interactions between agents. Thus a simple
interacting agent model, whose interactions are in accord with empirical data
on conformity and learning processes, presents statistical signatures
consistent with moral judgment patterns of conservatives and liberals as
obtained by survey studies of social psychology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5732</identifier>
 <datestamp>2012-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5732</id><created>2010-05-31</created><updated>2010-06-03</updated><authors><author><keyname>Afrati</keyname><forenames>Foto</forenames></author><author><keyname>Kyritsis</keyname><forenames>Victor</forenames></author><author><keyname>Lekeas</keyname><forenames>Paraskevas V.</forenames></author><author><keyname>Souliou</keyname><forenames>Dora</forenames></author></authors><title>A New Framework for Join Product Skew</title><categories>cs.DB</categories><doi>10.1007/978-3-642-27392-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different types of data skew can result in load imbalance in the context of
parallel joins under the shared nothing architecture. We study one important
type of skew, join product skew (JPS). A static approach based on frequency
classes is proposed which takes for granted the data distribution of join
attribute values. It comes from the observation that the join selectivity can
be expressed as a sum of products of frequencies of the join attribute values.
As a consequence, an appropriate assignment of join sub-tasks, that takes into
consideration the magnitude of the frequency products can alleviate the join
product skew. Motivated by the aforementioned remark, we propose an algorithm,
called Handling Join Product Skew (HJPS), to handle join product skew.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1005.5734</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1005.5734</id><created>2010-05-31</created><authors><author><keyname>Koetter</keyname><forenames>Ralf</forenames></author><author><keyname>Ma</keyname><forenames>Jun</forenames></author><author><keyname>Vardy</keyname><forenames>Alexander</forenames></author></authors><title>The Re-Encoding Transformation in Algebraic List-Decoding of
  Reed-Solomon Codes</title><categories>cs.IT math.IT</categories><comments>15 pages, 3 tables</comments><msc-class>94B35</msc-class><acm-class>E.4; H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main computational steps in algebraic soft-decoding, as well as
Sudan-type list-decoding, of Reed-Solomon codes are bivariate polynomial
interpolation and factorization. We introduce a computational technique, based
upon re-encoding and coordinate transformation, that significantly reduces the
complexity of the bivariate interpolation procedure. This re-encoding and
coordinate transformation converts the original interpolation problem into
another reduced interpolation problem, which is orders of magnitude smaller
than the original one. A rigorous proof is presented to show that the two
interpolation problems are indeed equivalent. An efficient factorization
procedure that applies directly to the reduced interpolation problem is also
given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0030</identifier>
 <datestamp>2010-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0030</id><created>2010-05-31</created><authors><author><keyname>Gaboardi</keyname><forenames>Marco</forenames></author><author><keyname>Marion</keyname><forenames>Jean-Yves</forenames></author><author><keyname>Della Rocca</keyname><forenames>Simona Ronchi</forenames></author></authors><title>An Implicit Characterization of PSPACE</title><categories>cs.LO cs.PL</categories><acm-class>F.3.3; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a type system for an extension of lambda calculus with a
conditional construction, named STAB, that characterizes the PSPACE class. This
system is obtained by extending STA, a type assignment for lambda-calculus
inspired by Lafont's Soft Linear Logic and characterizing the PTIME class. We
extend STA by means of a ground type and terms for booleans and conditional.
The key issue in the design of the type system is to manage the contexts in the
rule for conditional in an additive way. Thanks to this rule, we are able to
program polynomial time Alternating Turing Machines. From the well-known result
APTIME = PSPACE, it follows that STAB is complete for PSPACE. Conversely,
inspired by the simulation of Alternating Turing machines by means of
Deterministic Turing machine, we introduce a call-by-name evaluation machine
with two memory devices in order to evaluate programs in polynomial space. As
far as we know, this is the first characterization of PSPACE that is based on
lambda calculus and light logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0051</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0051</id><created>2010-05-31</created><updated>2011-07-03</updated><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author><author><keyname>Delahaye</keyname><forenames>Jean-Paul</forenames></author><author><keyname>Gaucherel</keyname><forenames>Cedric</forenames></author></authors><title>Image Characterization and Classification by Physical Complexity</title><categories>cs.CC cs.IT math.IT</categories><comments>30 pages, 21 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for estimating the complexity of an image based on
Bennett's concept of logical depth. Bennett identified logical depth as the
appropriate measure of organized complexity, and hence as being better suited
to the evaluation of the complexity of objects in the physical world. Its use
results in a different, and in some sense a finer characterization than is
obtained through the application of the concept of Kolmogorov complexity alone.
We use this measure to classify images by their information content. The method
provides a means for classifying and evaluating the complexity of objects by
way of their visual representations. To the authors' knowledge, the method and
application inspired by the concept of logical depth presented herein are being
proposed and implemented for the first time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0054</identifier>
 <datestamp>2011-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0054</id><created>2010-06-01</created><updated>2011-06-18</updated><authors><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author><author><keyname>Wen</keyname><forenames>Fei</forenames></author><author><keyname>Xu</keyname><forenames>Jia</forenames></author><author><keyname>Peng</keyname><forenames>Yingning</forenames></author></authors><title>Anti-measurement Matrix Uncertainty Sparse Signal Recovery for
  Compressive Sensing</title><categories>cs.IT math.IT math.NA stat.AP</categories><comments>13 pages, 3 figures; Accepted by International Journal of the
  Physical Sciences</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive sensing (CS) is a technique for estimating a sparse signal from
the random measurements and the measurement matrix. Traditional sparse signal
recovery methods have seriously degeneration with the measurement matrix
uncertainty (MMU). Here the MMU is modeled as a bounded additive error. An
anti-uncertainty constraint in the form of a mixed L2 and L1 norm is deduced
from the sparse signal model with MMU. Then we combine the sparse constraint
with the anti-uncertainty constraint to get an anti-uncertainty sparse signal
recovery operator. Numerical simulations demonstrate that the proposed operator
has a better reconstructing performance with the MMU than traditional methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0056</identifier>
 <datestamp>2010-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0056</id><created>2010-06-01</created><authors><author><keyname>Yang</keyname><forenames>Ruiming</forenames></author><author><keyname>Wan</keyname><forenames>Qun</forenames></author><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author><author><keyname>Yang</keyname><forenames>Wanlin</forenames></author></authors><title>Inter-atom Interference Mitigation for Sparse Signal Reconstruction
  Using Semi-blindly Weighted Minimum Variance Distortionless Response</title><categories>cs.IT math.IT math.NA</categories><comments>3 pages, 2 figures, accepted by The 17th International Conference on
  Telecommunications 2010 (ICT2010)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The feasibility of sparse signal reconstruction depends heavily on the
inter-atom interference of redundant dictionary. In this paper, a semi-blindly
weighted minimum variance distortionless response (SBWMVDR) is proposed to
mitigate the inter-atom interference. Examples of direction of arrival
estimation are presented to show that the orthogonal match pursuit (OMP) based
on SBWMVDR performs better than the ordinary OMP algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0109</identifier>
 <datestamp>2010-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0109</id><created>2010-06-01</created><authors><author><keyname>Bouyukliev</keyname><forenames>Iliya</forenames></author><author><keyname>Jakobsson</keyname><forenames>Erik</forenames></author></authors><title>Results on Binary Linear Codes With Minimum Distance 8 and 10</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory, May 2010 To
  be presented at the ACCT 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All codes with minimum distance 8 and codimension up to 14 and all codes with
minimum distance 10 and codimension up to 18 are classified. Nonexistence of
codes with parameters [33,18,8] and [33,14,10] is proved. This leads to 8 new
exact bounds for binary linear codes. Primarily two algorithms considering the
dual codes are used, namely extension of dual codes with a proper coordinate,
and a fast algorithm for finding a maximum clique in a graph, which is modified
to find a maximum set of vectors with the right dependency structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0153</identifier>
 <datestamp>2010-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0153</id><created>2010-06-01</created><authors><author><keyname>Buk</keyname><forenames>Solomiya</forenames></author></authors><title>Ivan Franko's novel Dlja domashnjoho ohnyshcha (For the Hearth) in the
  light of the frequency dictionary</title><categories>cs.CL</categories><comments>11 pages, in Ukrainian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the article, the methodology and the principles of the compilation of the
Frequency dictionary for Ivan Franko's novel Dlja domashnjoho ohnyshcha (For
the Hearth) are described. The following statistical parameters of the novel
vocabulary are obtained: variety, exclusiveness, concentration indexes,
correlation between word rank and text coverage, etc. The main quantitative
characteristics of Franko's novels Perekhresni stezhky (The Cross-Paths) and
Dlja domashnjoho ohnyshcha are compared on the basis of their frequency
dictionaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0168</identifier>
 <datestamp>2010-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0168</id><created>2010-06-01</created><authors><author><keyname>Pianykh</keyname><forenames>Oleg</forenames></author></authors><title>Perfusion Linearity and Its Applications</title><categories>cs.CE</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Perfusion analysis computes blood flow parameters (blood volume, blood flow,
mean transit time) from the observed flow of contrast agent, passing through
the patient's vascular system. Perfusion deconvolution has been widely accepted
as the principal numerical tool for perfusion analysis, and is used routinely
in clinical applications. This extensive use of perfusion in clinical
decision-making makes numerical stability and robustness of perfusion
computations vital for accurate diagnostics and patient safety. The main goal
of this paper is to propose a novel approach for validating numerical
properties of perfusion algorithms. The approach is based on Perfusion
Linearity Property (PLP), which we find in perfusion deconvolution, as well as
in many other perfusion techniques. PLP allows one to study perfusion values as
weighted averages of the original imaging data. This, in turn, uncovers hidden
problems with the existing deconvolution techniques, and may be used to suggest
more reliable computational approaches and methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0170</identifier>
 <datestamp>2010-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0170</id><created>2010-06-01</created><authors><author><keyname>Kampf</keyname><forenames>Sabine</forenames></author><author><keyname>Bossert</keyname><forenames>Martin</forenames></author></authors><title>A Fast Generalized Minimum Distance Decoder for Reed-Solomon Codes Based
  on the Extended Euclidean Algorithm</title><categories>cs.IT math.IT</categories><comments>ISIT Austin 2010</comments><doi>10.1109/ISIT.2010.5513705</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method to determine a set of basis polynomials from the
extended Euclidean algorithm that allows Generalized Minimum Distance decoding
of Reed-Solomon codes with a complexity of O(nd).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0193</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0193</id><created>2010-06-01</created><updated>2010-06-02</updated><authors><author><keyname>Becker</keyname><forenames>Johanna</forenames></author><author><keyname>Csizmadia</keyname><forenames>Zsolt</forenames></author><author><keyname>Laugier</keyname><forenames>Alexandre</forenames></author><author><keyname>Szab&#xf3;</keyname><forenames>J&#xe1;cint</forenames></author><author><keyname>Szeg&#xf6;</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author></authors><title>Balancing congestion for unsplittable routing on a bidirected ring</title><categories>cs.NI</categories><msc-class>90B10, 90C10, 94C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a bidirected ring with capacities and a demand graph, we present an
approximation algorithm to the problem of finding the minimum $\alpha$ such
that there exists a feasible unsplittable routing of the demands after
multiplying each capacity by $\alpha$. We also give an approximation scheme to
the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0195</identifier>
 <datestamp>2012-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0195</id><created>2010-06-01</created><updated>2012-02-09</updated><authors><author><keyname>Kakhbod</keyname><forenames>Ali</forenames></author><author><keyname>Teneketzis</keyname><forenames>Demosthenis</forenames></author></authors><title>Power Allocation and Spectrum Sharing in Multi-User, Multi-Channel
  Systems with Strategic Users</title><categories>math.OC cs.GT</categories><journal-ref>IEEE Transactions on Automatic Control (TAC). vol 57, no. 7, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the decentralized power allocation and spectrum sharing problem
in multi-user, multi-channel systems with strategic users. We present a
mechanism/game form that has the following desirable features. (1) It is
individually rational. (2) It is budget balanced at every Nash equilibrium of
the game induced by the game form as well as off equilibrium. (3) The
allocation corresponding to every Nash equilibrium (NE) of the game induced by
the mechanism is a Lindahl allocation, that is a weakly Pareto optimal
allocation. Our proposed game form/mechanism achieves all the above desirable
properties without any assumption about, concavity, differentiability,
monotonicity, or quasi-linearity of the users' utility functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0200</identifier>
 <datestamp>2011-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0200</id><created>2010-06-01</created><updated>2011-06-26</updated><authors><author><keyname>Koutschan</keyname><forenames>Christoph</forenames></author><author><keyname>Zeilberger</keyname><forenames>Doron</forenames></author></authors><title>The 1958 Pekeris-Accad-WEIZAC Ground-Breaking Collaboration that
  Computed Ground States of Two-Electron Atoms (and its 2010 Redux)</title><categories>cs.SC quant-ph</categories><comments>8 pages, 2 photos, final version as it appeared in the journal</comments><journal-ref>The Mathematical Intelligencer 33(2), pp. 52-57, 2011</journal-ref><doi>10.1007/s00283-010-9192-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to appreciate how well off we mathematicians and scientists are
today, with extremely fast hardware and lots and lots of memory, as well as
with powerful software, both for numeric and symbolic computation, it may be a
good idea to go back to the early days of electronic computers and compare how
things went then. We have chosen, as a case study, a problem that was
considered a huge challenge at the time. Namely, we looked at C.L. Pekeris's
seminal 1958 work on the ground state energies of two-electron atoms. We went
through all the computations ab initio with today's software and hardware, with
a special emphasis on the symbolic computations which in 1958 had to be made by
hand, and which nowadays can be automated and generalized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0220</identifier>
 <datestamp>2010-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0220</id><created>2010-06-01</created><authors><author><keyname>Creignou</keyname><forenames>Nadia</forenames></author><author><keyname>Meier</keyname><forenames>Arne</forenames></author><author><keyname>Thomas</keyname><forenames>Michael</forenames></author><author><keyname>Vollmer</keyname><forenames>Heribert</forenames></author></authors><title>The Complexity of Reasoning for Fragments of Autoepistemic Logic</title><categories>cs.LO cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autoepistemic logic extends propositional logic by the modal operator L. A
formula that is preceded by an L is said to be &quot;believed&quot;. The logic was
introduced by Moore 1985 for modeling an ideally rational agent's behavior and
reasoning about his own beliefs. In this paper we analyze all Boolean fragments
of autoepistemic logic with respect to the computational complexity of the
three most common decision problems expansion existence, brave reasoning and
cautious reasoning. As a second contribution we classify the computational
complexity of counting the number of stable expansions of a given knowledge
base. To the best of our knowledge this is the first paper analyzing the
counting problem for autoepistemic logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0234</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0234</id><created>2010-06-01</created><updated>2011-10-23</updated><authors><author><keyname>Gomez-Rodriguez</keyname><forenames>Manuel</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author><author><keyname>Krause</keyname><forenames>Andreas</forenames></author></authors><title>Inferring Networks of Diffusion and Influence</title><categories>cs.DS cs.SI physics.soc-ph stat.ML</categories><comments>Short version appeared in ACM SIGKDD International Conference on
  Knowledge Discovery and Data Mining (KDD), 2010. Long version submitted to
  ACM Transactions on Knowledge Discovery from Data (TKDD)</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information diffusion and virus propagation are fundamental processes taking
place in networks. While it is often possible to directly observe when nodes
become infected with a virus or adopt the information, observing individual
transmissions (i.e., who infects whom, or who influences whom) is typically
very difficult. Furthermore, in many applications, the underlying network over
which the diffusions and propagations spread is actually unobserved. We tackle
these challenges by developing a method for tracing paths of diffusion and
influence through networks and inferring the networks over which contagions
propagate. Given the times when nodes adopt pieces of information or become
infected, we identify the optimal network that best explains the observed
infection times. Since the optimization problem is NP-hard to solve exactly, we
develop an efficient approximation algorithm that scales to large datasets and
finds provably near-optimal networks.
  We demonstrate the effectiveness of our approach by tracing information
diffusion in a set of 170 million blogs and news articles over a one year
period to infer how information flows through the online media space. We find
that the diffusion network of news for the top 1,000 media sites and blogs
tends to have a core-periphery structure with a small set of core media sites
that diffuse information to the rest of the Web. These sites tend to have
stable circles of influence with more general news media sites acting as
connectors between them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0240</identifier>
 <datestamp>2010-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0240</id><created>2010-06-01</created><updated>2010-12-22</updated><authors><author><keyname>Zhao</keyname><forenames>Pengkai</forenames></author></authors><title>Performance of a Concurrent Link SDMA MAC under Practical PHY Operating
  Conditions</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Space Division Multiple Access (SDMA) based Medium Access Control (MAC)
protocols have been proposed to enable concurrent communications and improve
link throughput in Multi-Input Multi-Output (MIMO) Ad Hoc networks. For the
most part, the works appearing in the literature make idealized and simplifying
assumptions about the underlying physical layer as well as some aspects of the
link adaptation protocol. The result is that the performance predicted by such
works may not necessarily be a good predictor of actual performance in a fully
deployed system. In this paper we look to introduce elements into the SDMA MAC
concept that would allow us to better predict their performance under realistic
operating conditions. Using a generic SDMA-MAC we look at how the network sum
throughput changes with the introduction of the following: $(a)$ use of the
more practical MMSE algorithm instead of the zero-forcing or SVD based nulling
algorithms used for receive beamnulling; $(b)$ impact of channel estimation
errors; $(c)$ introduction of link adaptation mechanism specifically designed
for concurrent SDMA MACs; $(d)$ incorporation of TX beamforming along with RX
beamnulling. Following on the transmission window during which concurrent
transmissions are allowed by the MAC, we qualify the impact of each of these
four elements in isolation. At the conclusion, the performance of a system that
incorporates elements $a-d$ is presented and compared against the baseline
system, showing an improvement of up to 5x in the overall network sum
throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0245</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0245</id><created>2010-06-01</created><authors><author><keyname>Li</keyname><forenames>Shizheng</forenames></author><author><keyname>Ramamoorthy</keyname><forenames>Aditya</forenames></author></authors><title>Improved compression of network coding vectors using erasure decoding
  and list decoding</title><categories>cs.IT math.IT</categories><comments>3 pages. Accepted by IEEE Communications Letters. First submitted:
  18-Sep-2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Practical random network coding based schemes for multicast include a header
in each packet that records the transformation between the sources and the
terminal. The header introduces an overhead that can be significant in certain
scenarios. In previous work, parity check matrices of error control codes along
with error decoding were used to reduce this overhead. In this work we propose
novel packet formats that allow us to use erasure decoding and list decoding.
Both schemes have a smaller overhead compared to the error decoding based
scheme, when the number of sources combined in a packet is not too small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0247</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0247</id><created>2010-06-01</created><authors><author><keyname>Fraczek</keyname><forenames>Wojciech</forenames></author><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Szczypiorski</keyname><forenames>Krzysztof</forenames></author></authors><title>Stream Control Transmission Protocol Steganography</title><categories>cs.CR</categories><comments>6 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stream Control Transmission Protocol (SCTP) is a new transport layer protocol
that is due to replace TCP (Transmission Control Protocol) and UDP (User
Datagram Protocol) protocols in future IP networks. Currently, it is
implemented in such operating systems like BSD, Linux, HP-UX or Sun Solaris. It
is also supported in Cisco network devices operating system (Cisco IOS) and may
be used in Windows. This paper describes potential steganographic methods that
may be applied to SCTP and may pose a threat to network security. Proposed
methods utilize new, characteristic SCTP features like multi-homing and
multistreaming. Identified new threats and suggested countermeasures may be
used as a supplement to RFC 5062, which describes security attacks in SCTP
protocol and can induce further standard modifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0259</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0259</id><created>2010-06-01</created><authors><author><keyname>Cluzeau</keyname><forenames>Mathieu</forenames></author><author><keyname>Finiasz</keyname><forenames>Matthieu</forenames></author><author><keyname>Tillich</keyname><forenames>Jean-Pierre</forenames></author></authors><title>Methods for the Reconstruction of Parallel Turbo Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two new algorithms for the reconstruction of turbo codes from a
noisy intercepted bitstream. With these algorithms, we were able to reconstruct
various turbo codes with realistic parameter sizes. To the best of our
knowledge, these are the first algorithms able to recover the whole permutation
of a turbo code in the presence of high noise levels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0271</identifier>
 <datestamp>2011-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0271</id><created>2010-06-01</created><updated>2011-08-08</updated><authors><author><keyname>Hodas</keyname><forenames>Nathan O.</forenames></author></authors><title>The Quality of Oscillations in Overdamped Networks</title><categories>cond-mat.stat-mech cs.SI math-ph math.MP math.SP nlin.PS physics.bio-ph q-bio.MN</categories><comments>5 pages, 3 figures, version 8.8.11</comments><msc-class>60J27</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The second law of thermodynamics implies that no macroscopic system may
oscillate indefinitely without consuming energy. The question of the number of
possible oscillations and the coherent quality of these oscillations remain
unanswered. This paper proves the upper-bounds on the number and quality of
such oscillations when the system in question is homogeneously driven and has a
discrete network of states. In a closed system, the maximum number of
oscillations is bounded by the number of states in the network. In open
systems, the size of the network bounds the quality factor of oscillation. This
work also explores how the quality factor of macrostate oscillations, such as
would be observed in chemical reactions, are bounded by the smallest equivalent
loop of the network, not the size of the entire system. The consequences of
this limit are explored in the context of chemical clocks and limit cycles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0274</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0274</id><created>2010-06-01</created><authors><author><keyname>Li</keyname><forenames>Nan</forenames></author><author><keyname>Cushing</keyname><forenames>William</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author><author><keyname>Yoon</keyname><forenames>Sungwook</forenames></author></authors><title>Learning Probabilistic Hierarchical Task Networks to Capture User
  Preferences</title><categories>cs.AI</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose automatically learning probabilistic Hierarchical Task Networks
(pHTNs) in order to capture a user's preferences on plans, by observing only
the user's behavior. HTNs are a common choice of representation for a variety
of purposes in planning, including work on learning in planning. Our
contributions are (a) learning structure and (b) representing preferences. In
contrast, prior work employing HTNs considers learning method preconditions
(instead of structure) and representing domain physics or search control
knowledge (rather than preferences). Initially we will assume that the observed
distribution of plans is an accurate representation of user preference, and
then generalize to the situation where feasibility constraints frequently
prevent the execution of preferred plans. In order to learn a distribution on
plans we adapt an Expectation-Maximization (EM) technique from the discipline
of (probabilistic) grammar induction, taking the perspective of task reductions
as productions in a context-free grammar over primitive actions. To account for
the difference between the distributions of possible and preferred plans we
subsequently modify this core EM technique, in short, by rescaling its input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0277</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0277</id><created>2010-06-01</created><authors><author><keyname>Wang</keyname><forenames>Meng</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author><author><keyname>Tang</keyname><forenames>Ao</forenames></author></authors><title>The Limits of Error Correction with lp Decoding</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure. ISIT 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An unknown vector f in R^n can be recovered from corrupted measurements y =
Af + e where A^(m*n)(m&gt;n) is the coding matrix if the unknown error vector e is
sparse. We investigate the relationship of the fraction of errors and the
recovering ability of lp-minimization (0 &lt; p &lt;= 1) which returns a vector x
minimizing the &quot;lp-norm&quot; of y - Ax. We give sharp thresholds of the fraction of
errors that determine the successful recovery of f. If e is an arbitrary
unknown vector, the threshold strictly decreases from 0.5 to 0.239 as p
increases from 0 to 1. If e has fixed support and fixed signs on the support,
the threshold is 2/3 for all p in (0, 1), while the threshold is 1 for
l1-minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0284</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0284</id><created>2010-06-01</created><authors><author><keyname>Ota</keyname><forenames>Takahiro</forenames></author><author><keyname>Morita</keyname><forenames>Hiroyoshi</forenames></author></authors><title>Asymptotic Optimality of Antidictionary Codes</title><categories>cs.IT math.IT</categories><comments>5 pages, to appear in the proceedings of 2010 IEEE International
  Symposium on Information Theory (ISIT2010)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An antidictionary code is a lossless compression algorithm using an
antidictionary which is a set of minimal words that do not occur as substrings
in an input string. The code was proposed by Crochemore et al. in 2000, and its
asymptotic optimality has been proved with respect to only a specific
information source, called balanced binary source that is a binary Markov
source in which a state transition occurs with probability 1/2 or 1. In this
paper, we prove the optimality of both static and dynamic antidictionary codes
with respect to a stationary ergodic Markov source on finite alphabet such that
a state transition occurs with probability $p (0 &lt; p \leq 1)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0289</identifier>
 <datestamp>2010-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0289</id><created>2010-06-01</created><updated>2010-10-14</updated><authors><author><keyname>Lorenzetti</keyname><forenames>Carlos M.</forenames></author><author><keyname>Cecchini</keyname><forenames>Roc&#xed;o L.</forenames></author><author><keyname>Maguitman</keyname><forenames>Ana G.</forenames></author><author><keyname>Bencz&#xfa;r</keyname><forenames>Andr&#xe1;s A.</forenames></author></authors><title>M\'{e}todos para la Selecci\'{o}n y el Ajuste de Caracter\'{i}sticas en
  el Problema de la Detecci\'{o}n de Spam</title><categories>cs.IR cs.AI</categories><comments>5 pages, 1 figure, Workshop de Investigadores en Ciencias de la
  Computaci\'{o}n, WICC 2010, pp 48-52</comments><msc-class>68P20</msc-class><acm-class>H.3.3</acm-class><journal-ref>Workshop de Investigadores en Ciencias de la Computacion, WICC
  2010, El Calafate, Santa Cruz, Argentina</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The email is used daily by millions of people to communicate around the globe
and it is a mission-critical application for many businesses. Over the last
decade, unsolicited bulk email has become a major problem for email users. An
overwhelming amount of spam is flowing into users' mailboxes daily. In 2004, an
estimated 62% of all email was attributed to spam. Spam is not only frustrating
for most email users, it strains the IT infrastructure of organizations and
costs businesses billions of dollars in lost productivity. In recent years,
spam has evolved from an annoyance into a serious security threat, and is now a
prime medium for phishing of sensitive information, as well the spread of
malicious software. This work presents a first approach to attack the spam
problem. We propose an algorithm that will improve a classifier's results by
adjusting its training set data. It improves the document's vocabulary
representation by detecting good topic descriptors and discriminators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0291</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0291</id><created>2010-06-02</created><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Devroye</keyname><forenames>Luc</forenames></author><author><keyname>L&#xf6;ffler</keyname><forenames>Maarten</forenames></author><author><keyname>Snoeyink</keyname><forenames>Jack</forenames></author><author><keyname>Verma</keyname><forenames>Vishal</forenames></author></authors><title>The dilation of the Delaunay triangulation is greater than {\pi}/2</title><categories>cs.CG</categories><comments>12 pages, 6 figures, invited to the special edition of Computational
  Geometry Theory and Applications for papers from CCCG 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the Delaunay triangulation T of a set P of points in the plane as a
Euclidean graph, in which the weight of every edge is its length. It has long
been conjectured that the dilation in T of any pair p, p \in P, which is the
ratio of the length of the shortest path from p to p' in T over the Euclidean
distance ||pp'||, can be at most {\pi}/2 \approx 1.5708. In this paper, we show
how to construct point sets in convex position with dilation &gt; 1.5810 and in
general position with dilation &gt; 1.5846. Furthermore, we show that a
sufficiently large set of points drawn independently from any distribution will
in the limit approach the worst-case dilation for that distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0304</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0304</id><created>2010-06-02</created><authors><author><keyname>Babaie-Zadeh</keyname><forenames>Massoud</forenames></author><author><keyname>Jutten</keyname><forenames>Christian</forenames></author></authors><title>On the stable recovery of the sparsest overcomplete representations in
  presence of noise</title><categories>cs.IT math.IT</categories><comments>Accepted in IEEE Trans on SP on 4 May 2010. (c) 2010 IEEE. Personal
  use of this material is permitted. Permission from IEEE must be obtained for
  all other users, including reprinting/republishing this material for
  advertising or promotional purposes, creating new collective works for resale
  or redistribution to servers or lists, or reuse of any copyrighted components
  of this work in other works</comments><doi>10.1109/TSP.2010.2052357</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let x be a signal to be sparsely decomposed over a redundant dictionary A,
i.e., a sparse coefficient vector s has to be found such that x=As. It is known
that this problem is inherently unstable against noise, and to overcome this
instability, the authors of [Stable Recovery; Donoho et.al., 2006] have
proposed to use an &quot;approximate&quot; decomposition, that is, a decomposition
satisfying ||x - A s|| &lt; \delta, rather than satisfying the exact equality x =
As. Then, they have shown that if there is a decomposition with ||s||_0 &lt;
(1+M^{-1})/2, where M denotes the coherence of the dictionary, this
decomposition would be stable against noise. On the other hand, it is known
that a sparse decomposition with ||s||_0 &lt; spark(A)/2 is unique. In other
words, although a decomposition with ||s||_0 &lt; spark(A)/2 is unique, its
stability against noise has been proved only for highly more restrictive
decompositions satisfying ||s||_0 &lt; (1+M^{-1})/2, because usually (1+M^{-1})/2
&lt;&lt; spark(A)/2.
  This limitation maybe had not been very important before, because ||s||_0 &lt;
(1+M^{-1})/2 is also the bound which guaranties that the sparse decomposition
can be found via minimizing the L1 norm, a classic approach for sparse
decomposition. However, with the availability of new algorithms for sparse
decomposition, namely SL0 and Robust-SL0, it would be important to know whether
or not unique sparse decompositions with (1+M^{-1})/2 &lt; ||s||_0 &lt; spark(A)/2
are stable. In this paper, we show that such decompositions are indeed stable.
In other words, we extend the stability bound from ||s||_0 &lt; (1+M^{-1})/2 to
the whole uniqueness range ||s||_0 &lt; spark(A)/2. In summary, we show that &quot;all
unique sparse decompositions are stably recoverable&quot;. Moreover, we see that
sparser decompositions are &quot;more stable&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0308</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0308</id><created>2010-06-02</created><authors><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Beloglazov</keyname><forenames>Anton</forenames></author><author><keyname>Abawajy</keyname><forenames>Jemal</forenames></author></authors><title>Energy-Efficient Management of Data Center Resources for Cloud
  Computing: A Vision, Architectural Elements, and Open Challenges</title><categories>cs.DC</categories><comments>12 pages, 5 figures,Proceedings of the 2010 International Conference
  on Parallel and Distributed Processing Techniques and Applications (PDPTA
  2010), Las Vegas, USA, July 12-15, 2010</comments><acm-class>C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is offering utility-oriented IT services to users worldwide.
Based on a pay-as-you-go model, it enables hosting of pervasive applications
from consumer, scientific, and business domains. However, data centers hosting
Cloud applications consume huge amounts of energy, contributing to high
operational costs and carbon footprints to the environment. Therefore, we need
Green Cloud computing solutions that can not only save energy for the
environment but also reduce operational costs. This paper presents vision,
challenges, and architectural elements for energy-efficient management of Cloud
computing environments. We focus on the development of dynamic resource
provisioning and allocation algorithms that consider the synergy between
various data center infrastructures (i.e., the hardware, power units, cooling
and software), and holistically work to boost data center energy efficiency and
performance. In particular, this paper proposes (a) architectural principles
for energy-efficient management of Clouds; (b) energy-efficient resource
allocation policies and scheduling algorithms considering quality-of-service
expectations, and devices power usage characteristics; and (c) a novel software
technology for energy-efficient management of Clouds. We have validated our
approach by conducting a set of rigorous performance evaluation study using the
CloudSim toolkit. The results demonstrate that Cloud computing model has
immense potential as it offers significant performance gains as regards to
response time and cost saving under dynamic workload scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0312</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0312</id><created>2010-06-02</created><authors><author><keyname>Ho</keyname><forenames>Siu-Wai</forenames></author></authors><title>Markov Lemma for Countable Alphabets</title><categories>cs.IT math.IT</categories><comments>5 pages, ISIT2010, Austin</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Strong typicality and the Markov lemma have been used in the proofs of
several multiterminal source coding theorems. Since these two tools can be
applied to finite alphabets only, the results proved by them are subject to the
same limitation. Recently, a new notion of typicality, namely unified
typicality, has been defined. It can be applied to both finite or countably
infinite alphabets, and it retains the asymptotic equipartition property and
the structural properties of strong typicality. In this paper, unified
typicality is used to derive a version of the Markov lemma which works on both
finite or countably infinite alphabets so that many results in multiterminal
source coding can readily be extended. Furthermore, a simple way to verify
whether some sequences are jointly typical is shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0330</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0330</id><created>2010-06-02</created><authors><author><keyname>Schenk</keyname><forenames>Andreas</forenames></author><author><keyname>Fischer</keyname><forenames>Robert F. H.</forenames></author></authors><title>Soft-Output Sphere Decoder for Multiple-Symbol Differential Detection of
  Impulse-Radio Ultra-Wideband</title><categories>cs.IT math.IT</categories><comments>accepted for presentation at 2010 IEEE International Symposium on
  Information Theory (ISIT 2010), Austin (TX), USA, as paper #1061</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power efficiency of noncoherent receivers for impulse-radio ultra-wideband
(IR-UWB) transmission systems can significantly be improved, on the one hand,
by employing multiple-symbol differential detection (MSDD), and, on the other
hand, by providing reliability information to the subsequent channel decoder.
In this paper, we combine these two techniques. Incorporating the computation
of the soft information into a single-tree-search sphere decoder (SD), the
application of this soft-output MSDD in a typical IR-UWB system imposes only a
moderate complexity increase at, however, improved performance over hard-output
MSDD, and in particular, over conventional symbol-by-symbol noncoherent
differential detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0334</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0334</id><created>2010-06-02</created><authors><author><keyname>Costa</keyname><forenames>Rui A.</forenames></author><author><keyname>Langberg</keyname><forenames>Michael</forenames></author><author><keyname>Barros</keyname><forenames>Jo&#xe3;o</forenames></author></authors><title>One-Shot Capacity of Discrete Channels</title><categories>cs.IT math.IT</categories><comments>ISIT 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shannon defined channel capacity as the highest rate at which there exists a
sequence of codes of block length $n$ such that the error probability goes to
zero as $n$ goes to infinity. In this definition, it is implicit that the block
length, which can be viewed as the number of available channel uses, is
unlimited. This is not the case when the transmission power must be
concentrated on a single transmission, most notably in military scenarios with
adversarial conditions or delay-tolerant networks with random short encounters.
A natural question arises: how much information can we transmit in a single use
of the channel? We give a precise characterization of the one-shot capacity of
discrete channels, defined as the maximum number of bits that can be
transmitted in a single use of a channel with an error probability that does
not exceed a prescribed value. This capacity definition is shown to be useful
and significantly different from the zero-error problem statement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0355</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0355</id><created>2010-06-02</created><authors><author><keyname>Patra</keyname><forenames>Manas K</forenames></author><author><keyname>Braunstein</keyname><forenames>Samuel L</forenames></author></authors><title>An algebraic approach to information theory</title><categories>cs.IT math.IT</categories><comments>5 pages, to be presented at Int. Symp. Inf. Th. (ISIT), 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes an algebraic model for classical information theory. We
first give an algebraic model of probability theory. Information theoretic
constructs are based on this model. In addition to theoretical insights
provided by our model one obtains new computational and analytical tools.
Several important theorems of classical probability and information theory are
presented in the algebraic framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0375</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0375</id><created>2010-06-02</created><authors><author><keyname>Buhmann</keyname><forenames>Joachim M.</forenames></author></authors><title>Information theoretic model validation for clustering</title><categories>cs.IT cs.LG math.IT stat.ML</categories><comments>9 pages, 2 figures, International Symposium on Information Theory
  2010 (ISIT10 E-Mo-4.2), June 13-18 in Austin, TX}</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model selection in clustering requires (i) to specify a suitable clustering
principle and (ii) to control the model order complexity by choosing an
appropriate number of clusters depending on the noise level in the data. We
advocate an information theoretic perspective where the uncertainty in the
measurements quantizes the set of data partitionings and, thereby, induces
uncertainty in the solution space of clusterings. A clustering model, which can
tolerate a higher level of fluctuations in the measurements than alternative
models, is considered to be superior provided that the clustering solution is
equally informative. This tradeoff between \emph{informativeness} and
\emph{robustness} is used as a model selection criterion. The requirement that
data partitionings should generalize from one data set to an equally probable
second data set gives rise to a new notion of structure induced information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0379</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0379</id><created>2010-06-02</created><authors><author><keyname>Brown</keyname><forenames>J. David</forenames></author><author><keyname>Abouei</keyname><forenames>Jamshid</forenames></author><author><keyname>Plataniotis</keyname><forenames>Konstantinos N.</forenames></author><author><keyname>Pasupathy</keyname><forenames>Subbarayan</forenames></author></authors><title>Adaptive Demodulation in Differentially Coherent Phase Systems: Design
  and Performance Analysis</title><categories>cs.IT math.IT</categories><comments>25 pages, 11 Figures, submitted to IEEE Transactions on
  Communications, June 1, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive Demodulation (ADM) is a newly proposed rate-adaptive system which
operates without requiring Channel State Information (CSI) at the transmitter
(unlike adaptive modulation) by using adaptive decision region boundaries at
the receiver and encoding the data with a rateless code. This paper addresses
the design and performance of an ADM scheme for two common differentially
coherent schemes: M-DPSK (M-ary Differential Phase Shift Keying) and M-DAPSK
(M-ary Differential Amplitude and Phase Shift Keying) operating over AWGN and
Rayleigh fading channels. The optimal method for determining the most reliable
bits for a given differential detection scheme is presented. In addition,
simple (near-optimal) implementations are provided for recovering the most
reliable bits from a received pair of differentially encoded symbols for
systems using 16-DPSK and 16- DAPSK. The new receivers offer the advantages of
a rate-adaptive system, without requiring CSI at the transmitter and a coherent
phase reference at the receiver. Bit error analysis for the ADM system in both
cases is presented along with numerical results of the spectral efficiency for
the rate-adaptive systems operating over a Rayleigh fading channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0385</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0385</id><created>2010-06-01</created><authors><author><keyname>Werbos</keyname><forenames>Paul J.</forenames></author></authors><title>Brain-Like Stochastic Search: A Research Challenge and Funding
  Opportunity</title><categories>cs.AI</categories><comments>Plenary talk at IEEE Conference on Evolutionary Computing 1999,
  extended in 2010 with new appendix</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Brain-Like Stochastic Search (BLiSS) refers to this task: given a family of
utility functions U(u,A), where u is a vector of parameters or task
descriptors, maximize or minimize U with respect to u, using networks (Option
Nets) which input A and learn to generate good options u stochastically. This
paper discusses why this is crucial to brain-like intelligence (an area funded
by NSF) and to many applications, and discusses various possibilities for
network design and training. The appendix discusses recent research, relations
to work on stochastic optimization in operations research, and relations to
engineering-based approaches to understanding neocortex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0386</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0386</id><created>2010-06-02</created><authors><author><keyname>Rashwan</keyname><forenames>Haitham</forenames></author><author><keyname>Gabidulin</keyname><forenames>Ernst M.</forenames></author><author><keyname>Honary</keyname><forenames>Bahram</forenames></author></authors><title>A Smart Approach for GPT Cryptosystem Based on Rank Codes</title><categories>cs.IT cs.CR math.IT</categories><comments>5 pages. to appear in Proceedings of IEEE ISIT2010</comments><report-no>#1223: ISIT 2010</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The concept of Public- key cryptosystem was innovated by McEliece's
cryptosystem. The public key cryptosystem based on rank codes was presented in
1991 by Gabidulin -Paramonov-Trejtakov(GPT). The use of rank codes in
cryptographic applications is advantageous since it is practically impossible
to utilize combinatoric decoding. This has enabled using public keys of a
smaller size. Respective structural attacks against this system were proposed
by Gibson and recently by Overbeck. Overbeck's attacks break many versions of
the GPT cryptosystem and are turned out to be either polynomial or exponential
depending on parameters of the cryptosystem. In this paper, we introduce a new
approach, called the Smart approach, which is based on a proper choice of the
distortion matrix X. The Smart approach allows for withstanding all known
attacks even if the column scrambler matrix P over the base field Fq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0392</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0392</id><created>2010-06-02</created><authors><author><keyname>Galatolo</keyname><forenames>Stefano</forenames><affiliation>Dipartiento di matematica applicata, Universita di Pisa</affiliation></author><author><keyname>Hoyrup</keyname><forenames>Mathieu</forenames><affiliation>LORIA, Vandoeuvre-l es-Nancy, France</affiliation></author><author><keyname>Rojas</keyname><forenames>Crist&#xf3;bal</forenames><affiliation>Fields Institute, Toronto, Canada</affiliation></author></authors><title>Computing the speed of convergence of ergodic averages and pseudorandom
  points in computable dynamical systems</title><categories>cs.NA cs.CE cs.LO</categories><proxy>EPTCS</proxy><acm-class>G.0; G.3</acm-class><journal-ref>EPTCS 24, 2010, pp. 7-18</journal-ref><doi>10.4204/EPTCS.24.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A pseudorandom point in an ergodic dynamical system over a computable metric
space is a point which is computable but its dynamics has the same statistical
behavior as a typical point of the system.
  It was proved in [Avigad et al. 2010, Local stability of ergodic averages]
that in a system whose dynamics is computable the ergodic averages of
computable observables converge effectively. We give an alternative, simpler
proof of this result.
  This implies that if also the invariant measure is computable then the
pseudorandom points are a set which is dense (hence nonempty) on the support of
the invariant measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0393</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0393</id><created>2010-06-02</created><authors><author><keyname>Battenfeld</keyname><forenames>Ingo</forenames><affiliation>TU Dortmund</affiliation></author></authors><title>A domain-theoretic investigation of posets of sub-sigma-algebras
  (extended abstract)</title><categories>cs.LO math.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 24, 2010, pp. 19-28</journal-ref><doi>10.4204/EPTCS.24.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a measurable space (X, M) there is a (Galois) connection between
sub-sigma-algebras of M and equivalence relations on X. On the other hand
equivalence relations on X are closely related to congruences on stochastic
relations. In recent work, Doberkat has examined lattice properties of posets
of congruences on a stochastic relation and motivated a domain-theoretic
investigation of these ordered sets. Here we show that the posets of
sub-sigma-algebras of a measurable space do not enjoy desired domain-theoretic
properties and that our counterexamples can be applied to the set of smooth
equivalence relations on an analytic space, thus giving a rather unsatisfactory
answer to Doberkat's question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0394</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0394</id><created>2010-06-02</created><authors><author><keyname>Bauer</keyname><forenames>Matthew S.</forenames><affiliation>Arcadia University</affiliation></author><author><keyname>Zheng</keyname><forenames>Xizhong</forenames><affiliation>Arcadia University</affiliation></author></authors><title>On the Weak Computability of Continuous Real Functions</title><categories>cs.CC</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 24, 2010, pp. 29-40</journal-ref><doi>10.4204/EPTCS.24.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computable analysis, sequences of rational numbers which effectively
converge to a real number x are used as the (rho-) names of x. A real number x
is computable if it has a computable name, and a real function f is computable
if there is a Turing machine M which computes f in the sense that, M accepts
any rho-name of x as input and outputs a rho-name of f(x) for any x in the
domain of f. By weakening the effectiveness requirement of the convergence and
classifying the converging speeds of rational sequences, several interesting
classes of real numbers of weak computability have been introduced in
literature, e.g., in addition to the class of computable real numbers (EC), we
have the classes of semi-computable (SC), weakly computable (WC), divergence
bounded computable (DBC) and computably approximable real numbers (CA). In this
paper, we are interested in the weak computability of continuous real functions
and try to introduce an analogous classification of weakly computable real
functions. We present definitions of these functions by Turing machines as well
as by sequences of rational polygons and prove these two definitions are not
equivalent. Furthermore, we explore the properties of these functions, and
among others, show their closure properties under arithmetic operations and
composition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0395</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0395</id><created>2010-06-02</created><authors><author><keyname>Brattka</keyname><forenames>Vasco</forenames><affiliation>University of Cape Town</affiliation></author><author><keyname>Pauly</keyname><forenames>Arno</forenames><affiliation>University of Cambridge</affiliation></author></authors><title>Computation with Advice</title><categories>cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 24, 2010, pp. 41-55</journal-ref><doi>10.4204/EPTCS.24.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computation with advice is suggested as generalization of both computation
with discrete advice and Type-2 Nondeterminism. Several embodiments of the
generic concept are discussed, and the close connection to Weihrauch
reducibility is pointed out. As a novel concept, computability with random
advice is studied; which corresponds to correct solutions being guessable with
positive probability. In the framework of computation with advice, it is
possible to define computational complexity for certain concepts of
hypercomputation. Finally, some examples are given which illuminate the
interplay of uniform and non-uniform techniques in order to investigate both
computability with advice and the Weihrauch lattice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0396</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0396</id><created>2010-06-02</created><authors><author><keyname>Calvert</keyname><forenames>Wesley</forenames><affiliation>Murray State University</affiliation></author><author><keyname>Kramer</keyname><forenames>Ken</forenames><affiliation>Queens College &amp; CUNY Graduate Center</affiliation></author><author><keyname>Miller</keyname><forenames>Russell</forenames><affiliation>Queens College &amp; CUNY Graduate Center</affiliation></author></authors><title>The Cardinality of an Oracle in Blum-Shub-Smale Computation</title><categories>cs.LO</categories><proxy>EPTCS</proxy><acm-class>F.1.1</acm-class><journal-ref>EPTCS 24, 2010, pp. 56-66</journal-ref><doi>10.4204/EPTCS.24.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the relation of BSS-reducibility on subsets of the real numbers.
The question was asked recently (and anonymously) whether it is possible for
the halting problem H in BSS-computation to be BSS-reducible to a countable
set. Intuitively, it seems that a countable set ought not to contain enough
information to decide membership in a reasonably complex (uncountable) set such
as H. We confirm this intuition, and prove a more general theorem linking the
cardinality of the oracle set to the cardinality, in a local sense, of the set
which it computes. We also mention other recent results on BSS-computation and
algebraic real numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0397</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0397</id><created>2010-06-02</created><authors><author><keyname>Cenzer</keyname><forenames>Douglas</forenames><affiliation>University of Florida</affiliation></author><author><keyname>Brodhead</keyname><forenames>Paul</forenames><affiliation>Virginia State University</affiliation></author></authors><title>Effective Capacity and Randomness of Closed Sets</title><categories>cs.LO cs.IT math.IT math.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 24, 2010, pp. 67-76</journal-ref><doi>10.4204/EPTCS.24.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the connection between measure and capacity for the space of
nonempty closed subsets of {0,1}*. For any computable measure, a computable
capacity T may be defined by letting T(Q) be the measure of the family of
closed sets which have nonempty intersection with Q. We prove an effective
version of Choquet's capacity theorem by showing that every computable capacity
may be obtained from a computable measure in this way. We establish conditions
that characterize when the capacity of a random closed set equals zero or is
&gt;0. We construct for certain measures an effectively closed set with positive
capacity and with Lebesgue measure zero.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0398</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0398</id><created>2010-06-02</created><updated>2011-09-01</updated><authors><author><keyname>G&#xe4;rtner</keyname><forenames>Tobias</forenames><affiliation>Universit&#xe4;t des Saarlandes</affiliation></author><author><keyname>Ziegler</keyname><forenames>Martin</forenames><affiliation>TU Darmstadt</affiliation></author></authors><title>Real Analytic Machines and Degrees</title><categories>cs.LO math.LO</categories><comments>20 pages. A preliminary version of this work had appeared in Proc.
  CCA 2011, EPTCS vol.24, arXiv:1006.0398v1</comments><proxy>LMCS</proxy><acm-class>F.4.1, F.1.1, G.1.0</acm-class><journal-ref>Logical Methods in Computer Science, Volume 7, Issue 3 (September
  2, 2011) lmcs:1019</journal-ref><doi>10.2168/LMCS-7(3:11)2011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study and compare in two degree-theoretic ways (iterated Halting oracles
analogous to Kleene's arithmetical hierarchy and the Borel hierarchy of
descriptive set theory) the capabilities and limitations of three models of
analytic computation: BSS machines (aka real-RAM) and strongly/weakly analytic
machines as introduced by Hotz et. al. (1995).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0399</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0399</id><created>2010-06-02</created><authors><author><keyname>Gregoriades</keyname><forenames>Vassilios</forenames><affiliation>Technische Universitaet Darmstadt</affiliation></author></authors><title>The descriptive set-theoretic complexity of the set of points of
  continuity of a multi-valued function (Extended Abstract)</title><categories>cs.CC</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 24, 2010, pp. 92-100</journal-ref><doi>10.4204/EPTCS.24.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we treat a notion of continuity for a multi-valued function F
and we compute the descriptive set-theoretic complexity of the set of all x for
which F is continuous at x. We give conditions under which the latter set is
either a G_\delta set or the countable union of G_\delta sets. Also we provide
a counterexample which shows that the latter result is optimum under the same
conditions. Moreover we prove that those conditions are necessary in order to
obtain that the set of points of continuity of F is Borel i.e., we show that if
we drop some of the previous conditions then there is a multi-valued function F
whose graph is a Borel set and the set of points of continuity of F is not a
Borel set. Finally we give some analogue results regarding a stronger notion of
continuity for a multi-valued function. This article is motivated by a question
of M. Ziegler in &quot;Real Computation with Least Discrete Advice: A Complexity
Theory of Nonuniform Computability with Applications to Linear Algebra&quot;,
(submitted).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0400</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0400</id><created>2010-06-02</created><authors><author><keyname>Lu</keyname><forenames>Dianchen</forenames><affiliation>Jiangsu University</affiliation></author><author><keyname>Wang</keyname><forenames>Qingyan</forenames><affiliation>Jiangsu University</affiliation></author><author><keyname>Zheng</keyname><forenames>Rui</forenames><affiliation>Jiangsu University</affiliation></author></authors><title>Computing the Solutions of the Combined Korteweg-de Vries Equation by
  Turing Machines</title><categories>cs.CC</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 24, 2010, pp. 101-105</journal-ref><doi>10.4204/EPTCS.24.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the computability of the initial value problem of the
Combined KdV equation. It is shown that, for any integer s&gt;2, the nonlinear
solution operator which maps an initial condition data to the solution of the
Combined KdV equation can be computed by a Turing machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0401</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0401</id><created>2010-06-02</created><authors><author><keyname>M&#xfc;ller</keyname><forenames>Norbert Th.</forenames><affiliation>Universit&#xe4;t Trier, Germany</affiliation></author><author><keyname>Korovina</keyname><forenames>Margarita</forenames><affiliation>University Manchester, UK</affiliation></author></authors><title>Making big steps in trajectories</title><categories>cs.MS cs.NA</categories><proxy>EPTCS</proxy><acm-class>G.4; G.1.7</acm-class><journal-ref>EPTCS 24, 2010, pp. 106-119</journal-ref><doi>10.4204/EPTCS.24.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the solution of initial value problems within the context of
hybrid systems and emphasise the use of high precision approximations (in
software for exact real arithmetic). We propose a novel algorithm for the
computation of trajectories up to the area where discontinuous jumps appear,
applicable for holomorphic flow functions. Examples with a prototypical
implementation illustrate that the algorithm might provide results with higher
precision than well-known ODE solvers at a similar computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0402</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0402</id><created>2010-06-02</created><authors><author><keyname>Rettinger</keyname><forenames>Robert</forenames></author></authors><title>A Local to Global Principle for the Complexity of Riemann Mappings
  (Extended Abstract)</title><categories>cs.CC cs.LO cs.NA</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 24, 2010, pp. 120-129</journal-ref><doi>10.4204/EPTCS.24.16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the computational complexity of Riemann mappings can be bounded
by the complexity needed to compute conformal mappings locally at boundary
points. As a consequence we get first formally proven upper bounds for
Schwarz-Christoffel mappings and, more generally, Riemann mappings of domains
with piecewise analytic boundaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0403</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0403</id><created>2010-06-02</created><authors><author><keyname>Shen</keyname><forenames>Yuping</forenames><affiliation>Institute of Logic and Cognition, Sun Yat-sen University</affiliation></author><author><keyname>Zhao</keyname><forenames>Xishun</forenames><affiliation>Institute of Logic and Cognition, Sun Yat-sen University</affiliation></author></authors><title>NP-Logic Systems and Model-Equivalence Reductions</title><categories>cs.LO cs.CC</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 24, 2010, pp. 130-138</journal-ref><doi>10.4204/EPTCS.24.17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the existence of model-equivalence reduction
between NP-logic systems which are logic systems with model existence problem
in NP. It is shown that among all NP-systems with model checking problem in NP,
the existentially quantified propositional logic (\exists PF) is maximal with
respect to poly-time model-equivalent reduction. However, \exists PF seems not
a maximal NP-system in general because there exits a NP-system with model
checking problem D^P-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0404</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0404</id><created>2010-06-02</created><authors><author><keyname>Spandl</keyname><forenames>Christoph</forenames></author></authors><title>Computational Complexity of Iterated Maps on the Interval (Extended
  Abstract)</title><categories>cs.MS</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 24, 2010, pp. 139-150</journal-ref><doi>10.4204/EPTCS.24.18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The exact computation of orbits of discrete dynamical systems on the interval
is considered. Therefore, a multiple-precision floating point approach based on
error analysis is chosen and a general algorithm is presented. The correctness
of the algorithm is shown and the computational complexity is analyzed. As a
main result, the computational complexity measure considered here is related to
the Ljapunow exponent of the dynamical system under consideration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0405</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0405</id><created>2010-06-02</created><authors><author><keyname>Steinke</keyname><forenames>Thomas</forenames></author><author><keyname>Sainudiin</keyname><forenames>Raazesh</forenames></author></authors><title>A Rigorous Extension of the Sch\&quot;onhage-Strassen Integer Multiplication
  Algorithm Using Complex Interval Arithmetic</title><categories>cs.NA cs.CR cs.DS</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 24, 2010, pp. 151-159</journal-ref><doi>10.4204/EPTCS.24.19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiplication of n-digit integers by long multiplication requires O(n^2)
operations and can be time-consuming. In 1970 A. Schoenhage and V. Strassen
published an algorithm capable of performing the task with only O(n log(n))
arithmetic operations over the complex field C; naturally, finite-precision
approximations to C are used and rounding errors need to be accounted for.
Overall, using variable-precision fixed-point numbers, this results in an
O(n(log(n))^(2+Epsilon))-time algorithm. However, to make this algorithm more
efficient and practical we need to make use of hardware-based floating-point
numbers. How do we deal with rounding errors? and how do we determine the
limits of the fixed-precision hardware? Our solution is to use interval
arithmetic to guarantee the correctness of results and determine the hardware's
limits. We examine the feasibility of this approach and are able to report that
75,000-digit base-256 integers can be handled using double-precision
containment sets. This clearly demonstrates that our approach has practical
potential; however, at this stage, our implementation does not yet compete with
commercial ones, but we are able to demonstrate the feasibility of this
technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0406</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0406</id><created>2010-06-02</created><authors><author><keyname>Wu</keyname><forenames>Yongcheng</forenames><affiliation>Nanjing University of Information Science and Technology</affiliation></author></authors><title>Complete Multi-Representations of Sets in a Computable Measure Space</title><categories>cs.CC</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 24, 2010, pp. 160-166</journal-ref><doi>10.4204/EPTCS.24.20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent paper, two multi-representations for the measurable sets in a
computable measure space have been introduced, which prove to be topologically
complete w.r.t. certain topological properties. In this contribution, we show
them recursively complete w.r.t. computability of measure and set-theoretical
operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0407</identifier>
 <datestamp>2012-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0407</id><created>2010-06-02</created><updated>2011-01-13</updated><authors><author><keyname>Drineas</keyname><forenames>Petros</forenames></author><author><keyname>Zouzias</keyname><forenames>Anastasios</forenames></author></authors><title>A Note on Element-wise Matrix Sparsification via a Matrix-valued
  Bernstein Inequality</title><categories>cs.DS</categories><comments>8 pages</comments><doi>10.1016/j.ipl.2011.01.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an n x n matrix A, we present a simple, element-wise sparsification
algorithm that zeroes out all sufficiently small elements of A and then retains
some of the remaining elements with probabilities proportional to the square of
their magnitudes. We analyze the approximation accuracy of the proposed
algorithm using a recent, elegant non-commutative Bernstein inequality, and
compare our bounds with all existing (to the best of our knowledge)
element-wise matrix sparsification algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0408</identifier>
 <datestamp>2010-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0408</id><created>2010-06-02</created><updated>2010-09-09</updated><authors><author><keyname>Hinkelmann</keyname><forenames>Franziska</forenames></author><author><keyname>Murrugarra</keyname><forenames>David</forenames></author><author><keyname>Jarrah</keyname><forenames>Abdul Salam</forenames></author><author><keyname>Laubenbacher</keyname><forenames>Reinhard</forenames></author></authors><title>A Mathematical Framework for Agent Based Models of Complex Biological
  Networks</title><categories>q-bio.QM cs.MA physics.bio-ph</categories><comments>To appear in Bulletin of Mathematical Biology</comments><doi>10.1007/S11538-010-9582-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agent-based modeling and simulation is a useful method to study biological
phenomena in a wide range of fields, from molecular biology to ecology. Since
there is currently no agreed-upon standard way to specify such models it is not
always easy to use published models. Also, since model descriptions are not
usually given in mathematical terms, it is difficult to bring mathematical
analysis tools to bear, so that models are typically studied through
simulation. In order to address this issue, Grimm et al. proposed a protocol
for model specification, the so-called ODD protocol, which provides a standard
way to describe models. This paper proposes an addition to the ODD protocol
which allows the description of an agent-based model as a dynamical system,
which provides access to computational and theoretical tools for its analysis.
The mathematical framework is that of algebraic models, that is, time-discrete
dynamical systems with algebraic structure. It is shown by way of several
examples how this mathematical specification can help with model analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0423</identifier>
 <datestamp>2010-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0423</id><created>2010-06-01</created><authors><author><keyname>Denise</keyname><forenames>Alain</forenames><affiliation>LRI, IGM</affiliation></author><author><keyname>Ponty</keyname><forenames>Yann</forenames><affiliation>LIX</affiliation></author><author><keyname>Termier</keyname><forenames>Michel</forenames><affiliation>IGM</affiliation></author></authors><title>Controlled non uniform random generation of decomposable structures</title><categories>cs.DM</categories><proxy>ccsd</proxy><journal-ref>Journal of Theoretical Computer Science (TCS) 411, 40-42 (2010)
  0304-3975</journal-ref><doi>10.1016/j.tcs.2010.05.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a class of decomposable combinatorial structures, using different
types of atoms $\Atoms = \{\At_1,\ldots ,\At_{|{\Atoms}|}\}$. We address the
random generation of such structures with respect to a size $n$ and a targeted
distribution in $k$ of its \emph{distinguished} atoms. We consider two
variations on this problem. In the first alternative, the targeted distribution
is given by $k$ real numbers $\TargFreq_1, \ldots, \TargFreq_k$ such that $0 &lt;
\TargFreq_i &lt; 1$ for all $i$ and $\TargFreq_1+\cdots+\TargFreq_k \leq 1$. We
aim to generate random structures among the whole set of structures of a given
size $n$, in such a way that the {\em expected} frequency of any distinguished
atom $\At_i$ equals $\TargFreq_i$. We address this problem by weighting the
atoms with a $k$-tuple $\Weights$ of real-valued weights, inducing a weighted
distribution over the set of structures of size $n$. We first adapt the
classical recursive random generation scheme into an algorithm taking
$\bigO{n^{1+o(1)}+mn\log{n}}$ arithmetic operations to draw $m$ structures from
the $\Weights$-weighted distribution. Secondly, we address the analytical
computation of weights such that the targeted frequencies are achieved
asymptotically, i. e. for large values of $n$. We derive systems of functional
equations whose resolution gives an explicit relationship between $\Weights$
and $\TargFreq_1, \ldots, \TargFreq_k$. Lastly, we give an algorithm in
$\bigO{k n^4}$ for the inverse problem, {\it i.e.} computing the frequencies
associated with a given $k$-tuple $\Weights$ of weights, and an optimized
version in $\bigO{k n^2}$ in the case of context-free languages. This allows
for a heuristic resolution of the weights/frequencies relationship suitable for
complex specifications. In the second alternative, the targeted distribution is
given by a $k$ natural numbers $n_1, \ldots, n_k$ such that
$n_1+\cdots+n_k+r=n$ where $r \geq 0$ is the number of undistinguished atoms.
The structures must be generated uniformly among the set of structures of size
$n$ that contain {\em exactly} $n_i$ atoms $\At_i$ ($1 \leq i \leq k$). We give
a $\bigO{r^2\prod_{i=1}^k n_i^2 +m n k \log n}$ algorithm for generating $m$
structures, which simplifies into a $\bigO{r\prod_{i=1}^k n_i +m n}$ for
regular specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0448</identifier>
 <datestamp>2010-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0448</id><created>2010-06-02</created><authors><author><keyname>Gregor</keyname><forenames>Karo</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Emergence of Complex-Like Cells in a Temporal Product Network with Local
  Receptive Fields</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new neural architecture and an unsupervised algorithm for
learning invariant representations from temporal sequence of images. The system
uses two groups of complex cells whose outputs are combined multiplicatively:
one that represents the content of the image, constrained to be constant over
several consecutive frames, and one that represents the precise location of
features, which is allowed to vary over time but constrained to be sparse. The
architecture uses an encoder to extract features, and a decoder to reconstruct
the input from the features. The method was applied to patches extracted from
consecutive movie frames and produces orientation and frequency selective units
analogous to the complex cells in V1. An extension of the method is proposed to
train a network composed of units with local receptive field spread over a
large image of arbitrary size. A layer of complex cells, subject to sparsity
constraints, pool feature units over overlapping local neighborhoods, which
causes the feature units to organize themselves into pinwheel patterns of
orientation-selective receptive fields, similar to those observed in the
mammalian visual cortex. A feed-forward encoder efficiently computes the
feature representation of full images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0469</identifier>
 <datestamp>2011-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0469</id><created>2010-06-02</created><updated>2011-01-25</updated><authors><author><keyname>Zuckerman</keyname><forenames>David</forenames></author></authors><title>Pseudorandom Financial Derivatives</title><categories>q-fin.CP cs.CC</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arora, Barak, Brunnermeier, and Ge showed that taking computational
complexity into account, a dishonest seller could dramatically increase the
lemon costs of a family of financial derivatives. We show that if the seller is
required to construct derivatives of a certain form, then this phenomenon
disappears. In particular, we define and construct pseudorandom derivative
families, for which lemon placement only slightly affects the values of the
derivatives. Our constructions use expander graphs. We study our derivatives in
a more general setting than Arora et al. In particular, we analyze entire
collateralized debt obligations (CDOs) when the underlying assets can have
significant dependencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0475</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0475</id><created>2010-06-02</created><authors><author><keyname>Chernov</keyname><forenames>Alexey</forenames></author><author><keyname>Vovk</keyname><forenames>Vladimir</forenames></author></authors><title>Prediction with Advice of Unknown Number of Experts</title><categories>cs.LG</categories><comments>22 pages; draft version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the framework of prediction with expert advice, we consider a recently
introduced kind of regret bounds: the bounds that depend on the effective
instead of nominal number of experts. In contrast to the NormalHedge bound,
which mainly depends on the effective number of experts and also weakly depends
on the nominal one, we obtain a bound that does not contain the nominal number
of experts at all. We use the defensive forecasting method and introduce an
application of defensive forecasting to multivalued supermartingales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0495</identifier>
 <datestamp>2010-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0495</id><created>2010-06-02</created><authors><author><keyname>Szczypiorski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author></authors><title>Hiding Data in OFDM Symbols of IEEE 802.11 Networks</title><categories>cs.CR</categories><comments>6 pages, 8 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new steganographic method called WiPad (Wireless
Padding). It is based on the insertion of hidden data into the padding of
frames at the physical layer of WLANs (Wireless Local Area Networks). A
performance analysis based on a Markov model, previously introduced and
validated by the authors in [10], is provided for the method in relation to the
IEEE 802.11 a/g standards. Its results prove that maximum steganographic
bandwidth for WiPad is as high as 1.1 Mbit/s for data frames and 0.44 Mbit/s
for acknowledgment (ACK) frames. To the authors' best knowledge this is the
most capacious of all the known steganographic network channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0496</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0496</id><created>2010-06-02</created><updated>2012-06-17</updated><authors><author><keyname>Karmakar</keyname><forenames>Sanjay</forenames></author><author><keyname>Varanasi</keyname><forenames>Mahesh K.</forenames></author></authors><title>The diversity-multiplexing tradeoff of the MIMO Z interference channel</title><categories>cs.IT math.IT</categories><comments>Submitted to the Transactions of Information Theory, 34 pages, 6
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fundamental generalized diversity-multiplexing tradeoff (GDMT) of the
quasi-static fading MIMO Z interference channel (Z-IC) is established for the
general Z-IC with an arbitrary number of antennas at each node under the
assumptions of full channel state information at the transmitters (CSIT) and a
short-term average power constraint. In the GDMT framework, the direct link
signal-to-noise ratios (SNR) and cross-link interference-to-noise ratio (INR)
are allowed to vary so that their ratios relative to a nominal SNR in the dB
scale, i.e., the SNR/INR exponents, are fixed. It is shown that a simple
Han-Kobayashi message-splitting/partial interference decoding scheme that uses
only partial CSIT -- in which the second transmitter's signal depends only on
its cross-link channel matrix and the first user's transmit signal doesn't need
any CSIT whatsoever -- can achieve the full-CSIT GDMT of the MIMO Z-IC. The
GDMT of the MIMO Z-IC under the No-CSIT assumption is also obtained for some
range of multiplexing gains. The size of this range depends on the numbers of
antennas at the four nodes and the SNR and INR exponents of the direct and
cross links, respectively. For certain classes of channels including those in
which the interfered receiver has more antennas than do the other nodes, or
when the INR exponent is greater than a certain threshold, the GDMT of the MIMO
Z-IC under the No-CSIT assumption is completely characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0517</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0517</id><created>2010-06-02</created><updated>2010-08-04</updated><authors><author><keyname>Buckley</keyname><forenames>Andy</forenames></author><author><keyname>Whalley</keyname><forenames>Mike</forenames></author></authors><title>HepData reloaded: reinventing the HEP data archive</title><categories>hep-ex cs.DL hep-ph physics.data-an</categories><comments>7 pages, 3 figures, Presented at 13th International Workshop on
  Advanced Computing and Analysis Techniques in Physics Research (ACAT 2010),
  February 22-27, 2010, Jaipur, India</comments><journal-ref>PoS ACAT2010:067,2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the status of the HepData database system, following a major
re-development in time for the advent of LHC data. The new HepData system
benefits from use of modern database and programming language technologies, as
well as a variety of high-quality tools for interfacing the data sources and
their presentation, primarily via the Web. The new back-end provides much more
flexible and semantic data representations than before, on which new external
applications can be built to respond to the data demands of the LHC
experimental era. The HepData re-development was largely motivated by a desire
to have a single source of reference data for Monte Carlo validation and tuning
tools, whose status and connection to HepData we also briefly review.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0526</identifier>
 <datestamp>2010-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0526</id><created>2010-06-02</created><authors><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author><author><keyname>Ghosh</keyname><forenames>Rumi</forenames></author><author><keyname>Kang</keyname><forenames>Jeon Hyung</forenames></author></authors><title>Centrality Metric for Dynamic Networks</title><categories>cs.CY physics.soc-ph</categories><comments>in KDD workshop on Mining and Learning in Graphs (MLG)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Centrality is an important notion in network analysis and is used to measure
the degree to which network structure contributes to the importance of a node
in a network. While many different centrality measures exist, most of them
apply to static networks. Most networks, on the other hand, are dynamic in
nature, evolving over time through the addition or deletion of nodes and edges.
A popular approach to analyzing such networks represents them by a static
network that aggregates all edges observed over some time period. This
approach, however, under or overestimates centrality of some nodes. We address
this problem by introducing a novel centrality metric for dynamic network
analysis. This metric exploits an intuition that in order for one node in a
dynamic network to influence another over some period of time, there must exist
a path that connects the source and destination nodes through intermediaries at
different times. We demonstrate on an example network that the proposed metric
leads to a very different ranking than analysis of an equivalent static
network. We use dynamic centrality to study a dynamic citations network and
contrast results to those reached by static network analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0542</identifier>
 <datestamp>2010-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0542</id><created>2010-06-02</created><authors><author><keyname>Liu</keyname><forenames>Chun-Hung</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Multicast Capacity Scaling of Wireless Networks with Multicast Outage</title><categories>cs.IT math.IT</categories><comments>1 figure, 5 pages. IEEE International Symposium on Information Theory
  (ISIT), June 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multicast transmission has several distinctive traits as opposed to more
commonly studied unicast networks. Specially, these include (i) identical
packets must be delivered successfully to several nodes, (ii) outage could
simultaneously happen at different receivers, and (iii) the multicast rate is
dominated by the receiver with the weakest link in order to minimize outage and
retransmission. To capture these key traits, we utilize a Poisson cluster
process consisting of a distinct Poisson point process (PPP) for the
transmitters and receivers, and then define the multicast transmission capacity
(MTC) as the maximum achievable multicast rate times the number of multicast
clusters per unit volume, accounting for outages and retransmissions. Our main
result shows that if $\tau$ transmission attempts are allowed in a multicast
cluster, the MTC is $\Theta\left(\rho k^{x}\log(k)\right)$ where $\rho$ and $x$
are functions of $\tau$ depending on the network size and density, and $k$ is
the average number of the intended receivers in a cluster. We also show that an
appropriate number of retransmissions can significantly enhance the MTC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0544</identifier>
 <datestamp>2010-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0544</id><created>2010-06-02</created><authors><author><keyname>Hong</keyname><forenames>Jun-pyo</forenames></author><author><keyname>Choi</keyname><forenames>Wan</forenames></author></authors><title>Capacity scaling law by multiuser diversity in cognitive radio systems</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures, ISIT2010 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes the multiuser diversity gain in a cognitive radio (CR)
system where secondary transmitters opportunistically utilize the spectrum
licensed to primary users only when it is not occupied by the primary users. To
protect the primary users from the interference caused by the missed detection
of primary transmissions in the secondary network, minimum average throughput
of the primary network is guaranteed by transmit power control at the secondary
transmitters. The traffic dynamics of a primary network are also considered in
our analysis. We derive the average achievable capacity of the secondary
network and analyze its asymptotic behaviors to characterize the multiuser
diversity gains in the CR system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0551</identifier>
 <datestamp>2010-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0551</id><created>2010-06-03</created><authors><author><keyname>Zheng</keyname><forenames>Xizhong</forenames><affiliation>Arcadia University</affiliation></author><author><keyname>Zhong</keyname><forenames>Ning</forenames><affiliation>University of Cincinnati</affiliation></author></authors><title>Proceedings Seventh International Conference on Computability and
  Complexity in Analysis</title><categories>cs.CC cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 24, 2010</journal-ref><doi>10.4204/EPTCS.24</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume of the Electronic Proceedings in Theoretical Computer Science
(EPTCS) contains extended abstracts of talks to be presented at the Seventh
International Conference on Computability and Complexity in Analysis (CCA 2010)
that will take place in Zhenjiang, China, June 21-25, 2010. This conference is
the seventeenth event in the series of CCA annual meetings. The CCA conferences
are aimed at promoting the study and advancement of the theory of computability
and complexity over real-valued data and its application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0575</identifier>
 <datestamp>2010-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0575</id><created>2010-06-03</created><authors><author><keyname>Butnaru</keyname><forenames>Bogdan</forenames><affiliation>PRISM</affiliation></author><author><keyname>Nguyen</keyname><forenames>Benjamin</forenames><affiliation>PRISM</affiliation></author><author><keyname>Gardarin</keyname><forenames>Georges</forenames><affiliation>PRISM</affiliation></author><author><keyname>Yeh</keyname><forenames>Laurent</forenames><affiliation>PRISM</affiliation></author></authors><title>XQ2P: Efficient XQuery P2P Time Series Processing</title><categories>cs.DB</categories><proxy>ccsd</proxy><journal-ref>Bases de Donn\'ees Avanc\'ees (D\'emonstration), Namur : Belgium
  (2009)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this demonstration, we propose a model for the management of XML time
series (TS), using the new XQuery 1.1 window operator. We argue that
centralized computation is slow, and demonstrate XQ2P, our prototype of
efficient XQuery P2P TS computation in the context of financial analysis of
large data sets (&gt;1M values).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0576</identifier>
 <datestamp>2010-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0576</id><created>2010-06-03</created><authors><author><keyname>Gardarin</keyname><forenames>Georges</forenames><affiliation>PRISM</affiliation></author><author><keyname>Nguyen</keyname><forenames>Benjamin</forenames><affiliation>PRISM</affiliation></author><author><keyname>Yeh</keyname><forenames>Laurent</forenames><affiliation>PRISM</affiliation></author><author><keyname>Zeitouni</keyname><forenames>Karine</forenames><affiliation>PRISM</affiliation></author><author><keyname>Butnaru</keyname><forenames>Bogdan</forenames><affiliation>PRISM</affiliation></author><author><keyname>Sandu-Popa</keyname><forenames>Iulian</forenames><affiliation>PRISM</affiliation></author></authors><title>Gestion efficace de s\'eries temporelles en P2P: Application \`a
  l'analyse technique et l'\'etude des objets mobiles</title><categories>cs.DB</categories><proxy>ccsd</proxy><journal-ref>Bases de Donn\'ees Avanc\'ees, Namur : Belgium (2009)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a simple generic model to manage time series. A
time series is composed of a calendar with a typed value for each calendar
entry. Although the model could support any kind of XML typed values, in this
paper we focus on real numbers, which are the usual application. We define
basic vector space operations (plus, minus, scale), and also relational-like
and application oriented operators to manage time series. We show the interest
of this generic model on two applications: (i) a stock investment helper; (ii)
an ecological transport management system. Stock investment requires
window-based operations while trip management requires complex queries. The
model has been implemented and tested in PHP, Java, and XQuery. We show
benchmark results illustrating that the computing of 5000 series of over
100.000 entries in length - common requirements for both applications - is
difficult on classical centralized PCs. In order to serve a community of users
sharing time series, we propose a P2P implementation of time series by dividing
them in segments and providing optimized algorithms for operator expression
computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0619</identifier>
 <datestamp>2010-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0619</id><created>2010-06-03</created><authors><author><keyname>He</keyname><forenames>Yuan Yuan</forenames></author><author><keyname>Dey</keyname><forenames>Subhrakanti</forenames></author></authors><title>Spectrum Sharing in Cognitive Radio with Quantized Channel Information</title><categories>cs.IT math.IT math.OC</categories><comments>30 pages, 8 figures</comments><msc-class>94A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a wideband spectrum sharing system where a secondary user can
share a number of orthogonal frequency bands where each band is licensed to an
individual primary user. We address the problem of optimum secondary transmit
power allocation for its ergodic capacity maximization subject to an average
sum (across the bands) transmit power constraint and individual average
interference constraints on the primary users. The major contribution of our
work lies in considering quantized channel state information (CSI)(for the
vector channel space consisting of all secondary-to-secondary and
secondary-to-primary channels) at the secondary transmitter. It is assumed that
a band manager or a cognitive radio service provider has access to the full CSI
information from the secondary and primary receivers and designs (offline) an
optimal power codebook based on the statistical information (channel
distributions) of the channels and feeds back the index of the codebook to the
secondary transmitter for every channel realization in real-time, via a
delay-free noiseless limited feedback channel. A modified Generalized
Lloyds-type algorithm (GLA) is designed for deriving the optimal power
codebook. An approximate quantized power allocation (AQPA) algorithm is also
presented, that performs very close to its GLA based counterpart for large
number of feedback bits and is significantly faster. We also present an
extension of the modified GLA based quantized power codebook design algorithm
for the case when the feedback channel is noisy. Numerical studies illustrate
that with only 3-4 bits of feedback, the modified GLA based algorithms provide
secondary ergodic capacity very close to that achieved by full CSI and with
only as little as 4 bits of feedback, AQPA provides a comparable performance,
thus making it an attractive choice for practical implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0644</identifier>
 <datestamp>2010-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0644</id><created>2010-06-03</created><authors><author><keyname>Tian</keyname><forenames>Chao</forenames></author><author><keyname>Diggavi</keyname><forenames>Suhas</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author></authors><title>The Achievable Distortion Region of Bivariate Gaussian Source on
  Gaussian Broadcast Channel</title><categories>cs.IT math.IT</categories><comments>13 pages, 4 figures, submitted to Trans. Information Theory; partial
  result to be presented at ISIT 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a complete characterization of the achievable distortion region
for the problem of sending a bivariate Gaussian source over bandwidth-matched
Gaussian broadcast channels, where each receiver is interested in only one
component of the source. This setting naturally generalizes the simple single
Gaussian source bandwidth-matched broadcast problem for which the uncoded
scheme is known to be optimal. We show that a hybrid scheme can achieve the
optimum for the bivariate case, but neither an uncoded scheme alone nor a
separation-based scheme alone is sufficient. We further show that in this joint
source channel coding setting, the Gaussian setting is the worst scenario among
the sources and channel noises with the same covariances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0646</identifier>
 <datestamp>2010-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0646</id><created>2010-06-03</created><authors><author><keyname>Kraidy</keyname><forenames>Ghassan M.</forenames></author><author><keyname>Boutros</keyname><forenames>Joseph J.</forenames></author><author><keyname>F&#xe0;bregas</keyname><forenames>Albert Guill&#xe9;n i</forenames></author></authors><title>Irregular Turbo Codes in Block-Fading Channels</title><categories>cs.IT math.IT</categories><comments>to be presented at the IEEE International Symposium on Information
  Theory, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study irregular binary turbo codes over non-ergodic block-fading channels.
We first propose an extension of channel multiplexers initially designed for
regular turbo codes. We then show that, using these multiplexers, irregular
turbo codes that exhibit a small decoding threshold over the ergodic
Gaussian-noise channel perform very close to the outage probability on
block-fading channels, from both density evolution and finite-length
perspectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0659</identifier>
 <datestamp>2010-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0659</id><created>2010-06-03</created><authors><author><keyname>Sayir</keyname><forenames>Jossy</forenames></author></authors><title>EXIT Chart Approximations using the Role Model Approach</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures, to be presented at the IEEE Symposium on
  Information Theory (ISIT 2010) in Austin, Texas, June 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extrinsic Information Transfer (EXIT) functions can be measured by
statistical methods if the message alphabet size is moderate or if messages are
true a-posteriori distributions. We propose an approximation we call mixed
information that constitutes a lower bound for the true EXIT function and can
be estimated by statistical methods even when the message alphabet is large and
histogram-based approaches are impractical, or when messages are not true
probability distributions and time-averaging approaches are not applicable. We
illustrate this with the hypothetical example of a rank-only message passing
decoder for which it is difficult to compute or measure EXIT functions in the
conventional way. We show that the role model approach (arXiv:0809.1300) can be
used to optimize post-processing for the decoder and that it coincides with
Monte Carlo integration in the non-parametric case. It is guaranteed to tend
towards the optimal Bayesian post-processing estimator and can be applied in a
blind setup with unknown code-symbols to optimize the check-node operation for
non-binary Low-Density Parity-Check (LDPC) decoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0670</identifier>
 <datestamp>2010-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0670</id><created>2010-06-03</created><authors><author><keyname>Accomazzi</keyname><forenames>Alberto</forenames></author></authors><title>Astronomy 3.0 Style</title><categories>astro-ph.IM cs.DL</categories><comments>9 pages, 2 figures, to appear in Library and Information Services in
  Astronomy VI, ASP Conference Proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the next decade we will witness the development of a new infrastructure
in support of data-intensive scientific research, which includes Astronomy.
This new networked environment will offer both challenges and opportunities to
our community and has the potential to transform the way data are described,
curated and preserved. Based on the lessons learned during the development and
management of the ADS, a case is made for adopting the emerging technologies
and practices of the Semantic Web to support the way Astronomy research will be
conducted. Examples of how small, incremental steps can, in the aggregate, make
a significant difference in the provision and repurposing of astronomical data
are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0673</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0673</id><created>2010-06-03</created><updated>2014-09-30</updated><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Doyen</keyname><forenames>Laurent</forenames></author><author><keyname>Gimbert</keyname><forenames>Hugo</forenames></author><author><keyname>Henzinger</keyname><forenames>Thomas A.</forenames></author></authors><title>Randomness for Free</title><categories>cs.GT cs.LO</categories><doi>10.1007/978-3-642-15155-2_23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two-player zero-sum games on graphs. These games can be
classified on the basis of the information of the players and on the mode of
interaction between them. On the basis of information the classification is as
follows: (a) partial-observation (both players have partial view of the game);
(b) one-sided complete-observation (one player has complete observation); and
(c) complete-observation (both players have complete view of the game). On the
basis of mode of interaction we have the following classification: (a)
concurrent (both players interact simultaneously); and (b) turn-based (both
players interact in turn). The two sources of randomness in these games are
randomness in transition function and randomness in strategies. In general,
randomized strategies are more powerful than deterministic strategies, and
randomness in transitions gives more general classes of games. In this work we
present a complete characterization for the classes of games where randomness
is not helpful in: (a) the transition function probabilistic transition can be
simulated by deterministic transition); and (b) strategies (pure strategies are
as powerful as randomized strategies). As consequence of our characterization
we obtain new undecidability results for these games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0701</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0701</id><created>2010-06-03</created><authors><author><keyname>Zimand</keyname><forenames>Marius</forenames></author></authors><title>Impossibility of independence amplification in Kolmogorov complexity
  theory</title><categories>cs.CC</categories><doi>10.1007/978-3-642-15155-2_61</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies randomness extraction from sources with bounded
independence and the issue of independence amplification of sources, using the
framework of Kolmogorov complexity. The dependency of strings $x$ and $y$ is
${\rm dep}(x,y) = \max\{C(x) - C(x \mid y), C(y) - C(y\mid x)\}$, where
$C(\cdot)$ denotes the Kolmogorov complexity. It is shown that there exists a
computable Kolmogorov extractor $f$ such that, for any two $n$-bit strings with
complexity $s(n)$ and dependency $\alpha(n)$, it outputs a string of length
$s(n)$ with complexity $s(n)- \alpha(n)$ conditioned by any one of the input
strings. It is proven that the above are the optimal parameters a Kolmogorov
extractor can achieve. It is shown that independence amplification cannot be
effectively realized. Specifically, if (after excluding a trivial case) there
exist computable functions $f_1$ and $f_2$ such that ${\rm dep}(f_1(x,y),
f_2(x,y)) \leq \beta(n)$ for all $n$-bit strings $x$ and $y$ with ${\rm
dep}(x,y) \leq \alpha(n)$, then $\beta(n) \geq \alpha(n) - O(\log n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0706</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0706</id><created>2010-06-03</created><updated>2010-08-25</updated><authors><author><keyname>Barguno</keyname><forenames>Luis</forenames><affiliation>UPC Barcelona</affiliation></author><author><keyname>Godoy</keyname><forenames>Guillem</forenames><affiliation>UPC Barcelona</affiliation></author><author><keyname>Huntingford</keyname><forenames>Eduard</forenames><affiliation>UPC Barcelona</affiliation></author><author><keyname>Tiwari</keyname><forenames>Ashish</forenames><affiliation>SRI International</affiliation></author></authors><title>Termination of Rewriting with Right-Flat Rules Modulo Permutative
  Theories</title><categories>cs.LO</categories><comments>20 pages</comments><proxy>LMCS</proxy><acm-class>F.4.2</acm-class><journal-ref>Logical Methods in Computer Science, Volume 6, Issue 3 (August 25,
  2010) lmcs:821</journal-ref><doi>10.2168/LMCS-6(3:8)2010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present decidability results for termination of classes of term rewriting
systems modulo permutative theories. Termination and innermost termination
modulo permutative theories are shown to be decidable for term rewrite systems
(TRS) whose right-hand side terms are restricted to be shallow (variables occur
at depth at most one) and linear (each variable occurs at most once). Innermost
termination modulo permutative theories is also shown to be decidable for
shallow TRS. We first show that a shallow TRS can be transformed into a flat
(only variables and constants occur at depth one) TRS while preserving
termination and innermost termination. The decidability results are then proved
by showing that (a) for right-flat right-linear (flat) TRS, non-termination
(respectively, innermost non-termination) implies non-termination starting from
flat terms, and (b) for right-flat TRS, the existence of non-terminating
derivations starting from a given term is decidable. On the negative side, we
show PSPACE-hardness of termination and innermost termination for shallow
right-linear TRS, and undecidability of termination for flat TRS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0711</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0711</id><created>2010-06-03</created><authors><author><keyname>Ramasco</keyname><forenames>Jose J.</forenames></author><author><keyname>de la Lama</keyname><forenames>Marta S.</forenames></author><author><keyname>Lopez</keyname><forenames>Eduardo</forenames></author><author><keyname>Boettcher</keyname><forenames>Stefan</forenames></author></authors><title>Optimization of transport protocols with path-length constraints in
  complex networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.NI</categories><comments>8 pages, 8 figures</comments><journal-ref>Physical Review E 82, 036119 (2010)</journal-ref><doi>10.1103/PhysRevE.82.036119</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a protocol optimization technique that is applicable to both
weighted or unweighted graphs. Our aim is to explore by how much a small
variation around the Shortest Path or Optimal Path protocols can enhance
protocol performance. Such an optimization strategy can be necessary because
even though some protocols can achieve very high traffic tolerance levels, this
is commonly done by enlarging the path-lengths, which may jeopardize
scalability. We use ideas borrowed from Extremal Optimization to guide our
algorithm, which proves to be an effective technique. Our method exploits the
degeneracy of the paths or their close-weight alternatives, which significantly
improves the scalability of the protocols in comparison to Shortest Paths or
Optimal Paths protocols, keeping at the same time almost intact the length or
weight of the paths. This characteristic ensures that the optimized routing
protocols are composed of paths that are quick to traverse, avoiding negative
effects in data communication due to path-length increases that can become
specially relevant when information losses are present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0719</identifier>
 <datestamp>2010-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0719</id><created>2010-06-03</created><updated>2010-07-02</updated><authors><author><keyname>Bajwa</keyname><forenames>Waheed U.</forenames></author><author><keyname>Calderbank</keyname><forenames>Robert</forenames></author><author><keyname>Jafarpour</keyname><forenames>Sina</forenames></author></authors><title>Why Gabor Frames? Two Fundamental Measures of Coherence and Their Role
  in Model Selection</title><categories>math.ST cs.IT math.IT stat.ML stat.TH</categories><comments>31 pages, 4 figures; This paper is a full-length journal version of a
  shorter paper that was presented at the IEEE International Symposium on
  Information Theory, Austin, TX, June 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies non-asymptotic model selection for the general case of
arbitrary design matrices and arbitrary nonzero entries of the signal. In this
regard, it generalizes the notion of incoherence in the existing literature on
model selection and introduces two fundamental measures of coherence---termed
as the worst-case coherence and the average coherence---among the columns of a
design matrix. It utilizes these two measures of coherence to provide an
in-depth analysis of a simple, model-order agnostic one-step thresholding (OST)
algorithm for model selection and proves that OST is feasible for exact as well
as partial model selection as long as the design matrix obeys an easily
verifiable property. One of the key insights offered by the ensuing analysis in
this regard is that OST can successfully carry out model selection even when
methods based on convex optimization such as the lasso fail due to the rank
deficiency of the submatrices of the design matrix. In addition, the paper
establishes that if the design matrix has reasonably small worst-case and
average coherence then OST performs near-optimally when either (i) the energy
of any nonzero entry of the signal is close to the average signal energy per
nonzero entry or (ii) the signal-to-noise ratio in the measurement system is
not too high. Finally, two other key contributions of the paper are that (i) it
provides bounds on the average coherence of Gaussian matrices and Gabor frames,
and (ii) it extends the results on model selection using OST to low-complexity,
model-order agnostic recovery of sparse signals with arbitrary nonzero entries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0741</identifier>
 <datestamp>2011-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0741</id><created>2010-06-03</created><authors><author><keyname>Chebotarev</keyname><forenames>P. Yu.</forenames></author><author><keyname>Loginov</keyname><forenames>A. K.</forenames></author><author><keyname>Tsodikova</keyname><forenames>Ya. Yu.</forenames></author><author><keyname>Lezina</keyname><forenames>Z. M.</forenames></author><author><keyname>Borzenko</keyname><forenames>V. I.</forenames></author></authors><title>Analysis of Collectivism and Egoism Phenomena within the Context of
  Social Welfare</title><categories>cs.MA cs.SI math.OC</categories><comments>12 pages, 5 figures. Translated from Russian. Original Russian Text
  published in Problemy Upravleniya, 2008, No. 4, pp. 30-37</comments><msc-class>91B12, 91B70</msc-class><journal-ref>Automation and Remote Control 71(2010) No.6 1196-1207</journal-ref><doi>10.1134/S0005117910060202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comparative benefits provided by the basic social strategies including
collectivism and egoism are investigated within the framework of democratic
decision-making. In particular, we study the mechanism of growing &quot;snowball&quot; of
cooperation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0744</identifier>
 <datestamp>2013-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0744</id><created>2010-06-03</created><updated>2013-08-16</updated><authors><author><keyname>Gebauer</keyname><forenames>Heidi</forenames></author><author><keyname>Szabo</keyname><forenames>Tibor</forenames></author><author><keyname>Tardos</keyname><forenames>Gabor</forenames></author></authors><title>The Local Lemma Is Tight for SAT</title><categories>math.CO cs.CC cs.DM</categories><comments>37 pages</comments><acm-class>G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct unsatisfiable k-CNF formulas where every clause has k distinct
literals and every variable appears in at most (2/e + o(1))2^{k}/k clauses. The
Lopsided Local Lemma, applied with assignment of random values according to
counterintuitive probabilities, shows that our result is asymptotically best
possible. The determination of this extremal function is particularly important
as it represents the value where the k-SAT problem exhibits its complexity
hardness jump: from having every instance being a YES-instance it becomes
NP-hard just by allowing each variable to occur in one more clause. The
asymptotics of other related extremal functions are also determined. Let l(k)
denote the maximum number, such that every k-CNF formula with each clause
containing k distinct literals and each clause having a common variable with at
most l(k) other clauses, is satisfiable. We establish that the lower bound on
l(k) obtained from the Local Lemma is asymptotically optimal, i.e., l(k) = (1/e
+ o(1))2^{k}. The construction of our unsatisfiable CNF-formulas is based on
the binary tree approach of [16] and thus the constructed formulas are in the
class MU(1)of minimal unsatisfiable formulas having one more clauses than
variables. To obtain the asymptotically optimal binary trees we consider a
continuous approximation of the problem, set up a differential equation and
estimate its solution. The trees are then obtained through a discretization of
this solution. The binary trees constructed also give asymptotically precise
answers for seemingly unrelated problems like the European Tenure Game
introduced by Doerr [9] and the search problem with bounded number of
consecutive lies, considered in a problem of the 2012 IMO contest. As yet
another consequence we slightly improve two bounds related to the Neighborhood
Conjecture of Beck.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0758</identifier>
 <datestamp>2012-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0758</id><created>2010-06-03</created><updated>2011-06-10</updated><authors><author><keyname>Fong</keyname><forenames>David</forenames></author><author><keyname>Saunders</keyname><forenames>Michael</forenames></author></authors><title>LSMR: An iterative algorithm for sparse least-squares problems</title><categories>cs.MS math.NA</categories><comments>21 pages</comments><report-no>SOL 2010-2</report-no><msc-class>15A06, 65F10, 65F20, 65F22, 65F25, 65F35, 65F50, 93E24</msc-class><doi>10.1016/j.ijheatmasstransfer.2011.12.029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An iterative method LSMR is presented for solving linear systems $Ax=b$ and
least-squares problem $\min \norm{Ax-b}_2$, with $A$ being sparse or a fast
linear operator. LSMR is based on the Golub-Kahan bidiagonalization process. It
is analytically equivalent to the MINRES method applied to the normal equation
$A\T Ax = A\T b$, so that the quantities $\norm{A\T r_k}$ are monotonically
decreasing (where $r_k = b - Ax_k$ is the residual for the current iterate
$x_k$). In practice we observe that $\norm{r_k}$ also decreases monotonically.
Compared to LSQR, for which only $\norm{r_k}$ is monotonic, it is safer to
terminate LSMR early. Improvements for the new iterative method in the presence
of extra available memory are also explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0763</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0763</id><created>2010-06-03</created><authors><author><keyname>Jibril</keyname><forenames>Mubarak</forenames></author><author><keyname>tomlinson</keyname><forenames>Martin</forenames></author><author><keyname>Ahmed</keyname><forenames>Mohammed Zaki</forenames></author><author><keyname>Tjhai</keyname><forenames>Cen</forenames></author></authors><title>Good Codes From Generalised Algebraic Geometry Codes</title><categories>cs.IT math.IT</categories><comments>3 pages, to be presented at the IEEE Symposium on Information Theory
  (ISIT 2010) in Austin, Texas, June 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algebraic geometry codes or Goppa codes are defined with places of degree
one. In constructing generalised algebraic geometry codes places of higher
degree are used. In this paper we present 41 new codes over GF(16) which
improve on the best known codes of the same length and rate. The construction
method uses places of small degree with a technique originally published over
10 years ago for the construction of generalised algebraic geometry codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0773</identifier>
 <datestamp>2014-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0773</id><created>2010-06-03</created><authors><author><keyname>Lee</keyname><forenames>Jon</forenames></author><author><keyname>Onn</keyname><forenames>Shmuel</forenames></author><author><keyname>Romanchuk</keyname><forenames>Lyubov</forenames></author><author><keyname>Weismantel</keyname><forenames>Robert</forenames></author></authors><title>The Quadratic Graver Cone, Quadratic Integer Minimization, and
  Extensions</title><categories>math.OC cs.DM cs.DS math.CO</categories><msc-class>05A, 15A, 51M, 52A, 52B, 52C, 62H, 68Q, 68R, 68U, 68W, 90B, 90C</msc-class><journal-ref>Mathematical Programming, 136:301--323, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the nonlinear integer programming problem of minimizing a
quadratic function over the integer points in variable dimension satisfying a
system of linear inequalities. We show that when the Graver basis of the matrix
defining the system is given, and the quadratic function lies in a suitable
{\em dual Graver cone}, the problem can be solved in polynomial time. We
discuss the relation between this cone and the cone of positive semidefinite
matrices, and show that none contains the other. So we can minimize in
polynomial time some non-convex and some (including all separable) convex
quadrics.
  We conclude by extending our results to efficient integer minimization of
multivariate polynomial functions of arbitrary degree lying in suitable cones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0778</identifier>
 <datestamp>2011-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0778</id><created>2010-06-04</created><updated>2011-10-01</updated><authors><author><keyname>Gamal</keyname><forenames>Aly El</forenames></author><author><keyname>Koyluoglu</keyname><forenames>O. Ozan</forenames></author><author><keyname>Youssef</keyname><forenames>Moustafa</forenames></author><author><keyname>Gamal</keyname><forenames>Hesham El</forenames></author></authors><title>The Two-Way Wiretap Channel: Achievable Regions and Experimental Results</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers the two-way wiretap channel in which two legitimate
users, Alice and Bob, wish to exchange messages securely in the presence of a
passive eavesdropper Eve. In the full-duplex scenario, where each node can
transmit and receive simultaneously, we obtain new achievable secrecy rate
regions based on the idea of allowing the two users to jointly optimize their
channel prefixing distributions and binning codebooks in addition to key
sharing. The new regions are shown to be strictly larger than the known ones
for a wide class of discrete memoryless and Gaussian channels. In the
half-duplex case, where a user can only transmit or receive on any given degree
of freedom, we introduce the idea of randomized scheduling and establish the
significant gain it offers in terms of the achievable secrecy sum-rate. We
further develop an experimental setup based on a IEEE 802.15.4-enabled sensor
boards, and use this testbed to show that one can exploit the two-way nature of
the communication, via appropriately randomizing the transmit power levels and
transmission schedule, to introduce significant ambiguity at a noiseless Eve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0795</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0795</id><created>2010-06-04</created><authors><author><keyname>Salamanca</keyname><forenames>Luis</forenames></author><author><keyname>Murillo-Fuentes</keyname><forenames>Juan Jos&#xe9;</forenames></author><author><keyname>P&#xe9;rez-Cruz</keyname><forenames>Fernando</forenames></author></authors><title>Channel Decoding with a Bayesian Equalizer</title><categories>cs.IT math.IT</categories><comments>5 pages, 6 figures, ISIT 2010 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-density parity-check (LPDC) decoders assume the channel estate
information (CSI) is known and they have the true a posteriori probability
(APP) for each transmitted bit. But in most cases of interest, the CSI needs to
be estimated with the help of a short training sequence and the LDPC decoder
has to decode the received word using faulty APP estimates. In this paper, we
study the uncertainty in the CSI estimate and how it affects the bit error rate
(BER) output by the LDPC decoder. To improve these APP estimates, we propose a
Bayesian equalizer that takes into consideration not only the uncertainty due
to the noise in the channel, but also the uncertainty in the CSI estimate,
reducing the BER after the LDPC decoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0806</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0806</id><created>2010-06-04</created><authors><author><keyname>Fiore</keyname><forenames>Marco</forenames></author><author><keyname>Casetti</keyname><forenames>Claudio</forenames></author><author><keyname>Chiasserini</keyname><forenames>Carla Fabiana</forenames></author><author><keyname>Papadimitratos</keyname><forenames>Panagiotis</forenames></author></authors><title>Secure Neighbor Position Discovery in VANETs</title><categories>cs.CR</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many significant functionalities of vehicular ad hoc networks (VANETs)
require that nodes have knowledge of the positions of other vehicles, and
notably of those within communication range. However, adversarial nodes could
provide false position information or disrupt the acquisition of such
information. Thus, in VANETs, the discovery of neighbor positions should be
performed in a secure manner. In spite of a multitude of security protocols in
the literature, there is no secure discovery protocol for neighbors positions.
We address this problem in our paper: we design a distributed protocol that
relies solely on information exchange among one-hop neighbors, we analyze its
security properties in presence of one or multiple (independent or colluding)
adversaries, and we evaluate its performance in a VANET environment using
realistic mobility traces. We show that our protocol can be highly effective in
detecting falsified position information, while maintaining a low rate of false
positive detections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0809</identifier>
 <datestamp>2011-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0809</id><created>2010-06-04</created><updated>2011-09-06</updated><authors><author><keyname>Grabowski</keyname><forenames>Szymon</forenames></author><author><keyname>Bieniecki</keyname><forenames>Wojciech</forenames></author></authors><title>Tight and simple Web graph compression</title><categories>cs.DS</categories><comments>15 pages</comments><msc-class>68U35</msc-class><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysing Web graphs has applications in determining page ranks, fighting Web
spam, detecting communities and mirror sites, and more. This study is however
hampered by the necessity of storing a major part of huge graphs in the
external memory, which prevents efficient random access to edge (hyperlink)
lists. A number of algorithms involving compression techniques have thus been
presented, to represent Web graphs succinctly but also providing random access.
Those techniques are usually based on differential encodings of the adjacency
lists, finding repeating nodes or node regions in the successive lists, more
general grammar-based transformations or 2-dimensional representations of the
binary matrix of the graph. In this paper we present two Web graph compression
algorithms. The first can be seen as engineering of the Boldi and Vigna (2004)
method. We extend the notion of similarity between link lists, and use a more
compact encoding of residuals. The algorithm works on blocks of varying size
(in the number of input lines) and sacrifices access time for better
compression ratio, achieving more succinct graph representation than other
algorithms reported in the literature. The second algorithm works on blocks of
the same size, in the number of input lines, and its key mechanism is merging
the block into a single ordered list. This method achieves much more attractive
space-time tradeoffs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0813</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0813</id><created>2010-06-04</created><authors><author><keyname>Bergstra</keyname><forenames>J. A.</forenames></author><author><keyname>Middelburg</keyname><forenames>C. A.</forenames></author></authors><title>On the definition of a theoretical concept of an operating system</title><categories>cs.OS</categories><comments>8 pages</comments><acm-class>D.4.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We dwell on how a definition of a theoretical concept of an operating system,
suitable to be incorporated in a mathematical theory of operating systems,
could look like. This is considered a valuable preparation for the development
of a mathematical theory of operating systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0831</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0831</id><created>2010-06-04</created><authors><author><keyname>Abdalla</keyname><forenames>Mahmoud I. A.</forenames></author></authors><title>Treatment the Effects of Studio Wall Resonance and Coincidence Phenomena
  for Recording Noisy Speech Via FPGA Digital Filter</title><categories>cs.SD</categories><comments>Submitted to Journal of Telecommunications, see
  http://sites.google.com/site/journaloftelecommunications/volume-2-issue-2-may-2010</comments><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, pp42-48, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work introduces an economic solution for the problems of sound
insulation of recording studios. Sound insulation at wall resonance frequency
is weak. Instead of acoustical treatment, a digital filter is used to eliminate
the effects of wall resonance and coincidence phenomena on recording of speech.
Sound insulation of studio is measured to calculate the wall resonance
frequency and the coincidence frequency. Pole /zero placement technique is used
to calculate the IIR filter coefficients. The digital filter is designed,
simulated and implemented. The proposed system is used to treat these problems
and it is shown to be effective in recording the noisy speech. In this work
digital signal processing is used instead of the acoustic treatment to
eliminate the effect of noise at the studio wall resonance. This technique is
cheap and effective in canceling the noise at the desired frequencies. Field
Programmable Gate Array (FPGA) is used for hardware implementation of the
proposed filter structure which provides fast and cheap solution for processing
real time audio signals. The implementation is carried out using Spartan chip
from Xinlinx achieving higher performance than commercially available software
solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0834</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0834</id><created>2010-06-04</created><authors><author><keyname>Sari</keyname><forenames>L.</forenames></author><author><keyname>Wibisono</keyname><forenames>G.</forenames></author><author><keyname>Gunawan</keyname><forenames>D.</forenames></author></authors><title>Performance of RCPC-Encoded V-BLAST MIMO In Nakagami-m Fading Channel</title><categories>cs.NI</categories><comments>Submitted to Journal of Telecommunications, see
  http://sites.google.com/site/journaloftelecommunications/volume-2-issue-2-may-2010</comments><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, pp49-57, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple Input Multiple Output (MIMO) wireless communication link has been
theoretically proven to be reliable and capable of achieving high capacity.
However, these two advantageous characteristics tend to be addressed separately
in many major researches. Researches on various approaches to attain both
characteristics in a single MIMO system are still on-going and an established
approach is yet to be concluded. To address this problem, in this paper a
Vertical Bell Laboratories Layered Space-Time (V-BLAST) MIMO enhanced with
Rate-Compatible Convolutional (RCPC) codes with Zero Forcing (ZF) and Minimum
Mean Squared Error (MMSE)-based detection is proposed. The analytical BER of
the system is presented and numerically analyzed. The system performance is
analyzed in Nakagami-m fading channel, which provides accuracy and flexibility
in matching the signals statistics compared to other fading models. The
complexity which arises in the calculations of the RCPC codes parameters is
significantly reduced by using equivalent convolutional codes. Results show
that the use of high-rate code allows for bandwidth efficiency and at the same
time does not severely degrades the system performance. It is also shown that
the MMSE-based system outperforms the conventional ZF-based system especially
in the low Eb/N0 region and in severe fading conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0836</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0836</id><created>2010-06-04</created><authors><author><keyname>kumar</keyname><forenames>Harsh</forenames></author><author><keyname>Srivastava</keyname><forenames>Shweta</forenames></author></authors><title>Rectangular and Circular Antenna Design on Thick Substrate</title><categories>cs.OH</categories><comments>Submitted to Journal of Telecommunications, see
  http://sites.google.com/site/journaloftelecommunications/volume-2-issue-2-may-2010</comments><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, pp58-63, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave technology being an emerging area is still very undeveloped.
A substantial research needs to be done in this area as its applications are
numerous. In the present endeavor, a rectangular patch antenna is designed on
thick substrate and simulated using SONNET software, also a novel analysis
technique is developed for circular patch antenna for millimeter wave
frequency. The antenna is designed at 39 GHz on thick substrate and has been
analyzed and simulated.The results of the theoretical analysis are in good
agreement with the simulated results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0839</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0839</id><created>2010-06-04</created><authors><author><keyname>Mohanna</keyname><forenames>S.</forenames></author><author><keyname>Farahbakhsh</keyname><forenames>A.</forenames></author><author><keyname>Tavakoli</keyname><forenames>S.</forenames></author></authors><title>Mutual Coupling Reduction in Two-Dimensional Array of Microstrip
  Antennas Using Concave Rectangular Patches</title><categories>cs.OH</categories><comments>Submitted to Journal of Telecommunications, see
  http://sites.google.com/site/journaloftelecommunications/volume-2-issue-2-may-2010</comments><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, pp64-69, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using concave rectangular patches, a new solution to reduce mutual coupling
and return loss in two-dimensional array of microstrip antennas is proposed.
The effect of width and length concavity on mutual coupling and return loss is
studied. Also, the patch parameters as well as the amounts of width and length
concavity are optimized using an enhanced genetic algorithm. Simulation results
show that the resulting array antenna has low amounts of mutual coupling and
return loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0840</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0840</id><created>2010-06-04</created><authors><author><keyname>Mohammed</keyname><forenames>Usama S.</forenames></author><author><keyname>Hamada</keyname><forenames>H. A.</forenames></author></authors><title>Image transmission over OFDM channel with rate allocation scheme and
  minimum peak-toaverage power ratio</title><categories>cs.NI</categories><comments>Submitted to Journal of Telecommunications, see
  http://sites.google.com/site/journaloftelecommunications/volume-2-issue-2-may-2010</comments><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, pp70-78, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes new scheme for efficient rate allocation in conjunction
with reducing peak-to-average power ratio (PAPR) in orthogonal
frequency-division multiplexing (OFDM) systems. Modification of the set
partitioning in hierarchical trees (SPIHT) image coder is proposed to generate
four different groups of bit-stream relative to its significances. The
significant bits, the sign bits, the set bits and the refinement bits are
transmitted in four different groups. The proposed method for reducing the PAPR
utilizes twice the unequal error protection (UEP), using the Read-Solomon codes
(RS), in conjunction with bit-rate allocation and selective interleaving to
provide minimum PAPR. The output bit-stream from the source code (SPIHT) will
be started by the most significant types of bits (first group of bits). The
optimal unequal error protection (UEP) of the four groups is proposed based on
the channel destortion. The proposed structure provides significant improvement
in bit error rate (BER) performance. Performed computer simulations have shown
that the proposed scheme outperform the performance of most of the recent PAPR
reduction techniques in most cases. Moreover, the simulation results indicate
that the proposed scheme provides significantly better PSNR performance in
comparison to well-known robust coding schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0841</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0841</id><created>2010-06-04</created><authors><author><keyname>Reza</keyname><forenames>A. Galib</forenames></author><author><keyname>Tan</keyname><forenames>S. C.</forenames></author><author><keyname>Abbou</keyname><forenames>F. M.</forenames></author></authors><title>Performance Evaluation of Two-Stage Shared FDL Optical Packet Switch
  using Contention Resolution Scheme with Packet Releasing Priority</title><categories>cs.NI</categories><comments>Submitted to Journal of Telecommunications, see
  http://sites.google.com/site/journaloftelecommunications/volume-2-issue-2-may-2010</comments><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, pp79-82, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a two-stage optical packet switch with second stage of
recirculate switch of FDL to reduce the number of the FDL used in the switch
for contention resolution. The contention resolution scheme with priority in
packet releasing from FDL is tested in the two-stage switch for performance
evaluation. Simulation result shows that zero packet loss rate achievable with
{\i}&lt; 0.8 for 32x 32 two-stage switch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0843</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0843</id><created>2010-06-04</created><authors><author><keyname>Sinha</keyname><forenames>Nirmalendu Bikas</forenames></author><author><keyname>kumar</keyname><forenames>Prosenjit</forenames></author><author><keyname>Mitra</keyname><forenames>M.</forenames></author></authors><title>Capacity Optimized For Multicarrier OFDM-MIMO Antenna Systems</title><categories>cs.NI</categories><comments>Submitted to Journal of Telecommunications, see
  http://sites.google.com/site/journaloftelecommunications/volume-2-issue-2-may-2010</comments><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, pp83-90, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by MIMO broad-band fading channel model, in this section we deals
with the capacity behaviour of wireless MIMO and OFDM based spatial
multiplexing systems in broad-band fading environments for the case where the
channel is unknown at the transmitter and perfectly known at the receiver. This
influence the propagation and system parameters on ergodic capacity, we
furthermore demonstrate that, unlike the single-input single-output (SISO)
case, delay spread channels may provide advantage over flat-fading channels not
only in terms of outage capacity but also in terms of ergodic capacity.
Therefore, MIMO delay spread channels will in general provide both higher
diversity gain and higher multiplexing gain than MIMO flat-fading channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0844</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0844</id><created>2010-06-04</created><authors><author><keyname>Garcia</keyname><forenames>J.</forenames></author><author><keyname>Zhou</keyname><forenames>C.</forenames></author></authors><title>Improving GPS Precision and Processing Time using Parallel and
  Reduced-Length Wiener Filters</title><categories>cs.OH</categories><comments>Submitted to Journal of Telecommunications, see
  http://sites.google.com/site/journaloftelecommunications/volume-2-issue-2-may-2010</comments><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, pp91-98, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing GPS precision at low cost has always been a challenge for the
manufacturers of the GPS receivers. This paper proposes the use of a Wiener
filter for increasing precision in substitution of traditional GPS/INS fusion
systems, which require expensive inertial systems. In this paper, we first
implement and compare three GPS signal processing schemes: a Kalman filter, a
neural network and a Wiener filter and compare them in terms of precision and
the processing time. To further reduce the processing time of Wiener filter, we
propose parallel and reduced-length implementations. Finally, we calculate the
sampling frequency that would be required in every Wiener scheme in order to
obtain the same total processing time as the Kalman filter and the neural
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0845</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0845</id><created>2010-06-04</created><authors><author><keyname>Dahmouni</keyname><forenames>H.</forenames></author><author><keyname>Ghazi</keyname><forenames>H. El</forenames></author><author><keyname>Bonacci</keyname><forenames>D.</forenames></author><author><keyname>Sanso</keyname><forenames>B.</forenames></author><author><keyname>Girard</keyname><forenames>A.</forenames></author></authors><title>Imprvoing QoS of all-IP Generation of Pre-WiMax Networks Using
  Delay-Jitter Model</title><categories>cs.NI</categories><comments>Submitted to Journal of Telecommunications, see
  http://sites.google.com/site/journaloftelecommunications/volume-2-issue-2-may-2010</comments><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, pp99-103, May
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The topic of this paper is the evaluation of QoS parameters in live Pre-Wimax
environments. The main contribution is the validation of an analytical
delay-jitter behavior model. These models can be used in optimization
algorithms in order to provide opportunistic and reliable all-IP networks. It
allows understanding the impact of the jitter constraints on the throughput and
packet loss in wireless systems. However, we show that the real-time QoS
requirements of real-time and interactive services can be avoided to a large
degree by controlling only the packet delay-jitter in a fixed and mobile
environment. The QoS metrics have been computed from live measurements in a
Pre-Wimax realistic environment (Toulouse/Blagnac Airport).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0846</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0846</id><created>2010-06-04</created><authors><author><keyname>Elmorshidy</keyname><forenames>Ahmed</forenames></author></authors><title>Holographic Projection Technology: The World is Changing</title><categories>cs.CY</categories><comments>Submitted to Journal of Telecommunications, see
  http://sites.google.com/site/journaloftelecommunications/volume-2-issue-2-may-2010</comments><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, pp104-112, May
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research papers examines the new technology of Holographic Projections.
It highlights the importance and need of this technology and how it represents
the new wave in the future of technology and communications, the different
application of the technology, the fields of life it will dramatically affect
including business, education, telecommunication and healthcare. The paper also
discusses the future of holographic technology and how it will prevail in the
coming years highlighting how it will also affect and reshape many other fields
of life, technologies and businesses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0848</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0848</id><created>2010-06-04</created><authors><author><keyname>Kang</keyname><forenames>A S</forenames></author><author><keyname>Sharma</keyname><forenames>Vishal</forenames></author></authors><title>Analysis of Beaulieu Pulse Shaping Family Based FIR Filter for WCDMA</title><categories>cs.NI</categories><comments>Submitted to Journal of Telecommunications, see
  http://sites.google.com/site/journaloftelecommunications/volume-2-issue-2-may-2010</comments><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, p113-125, May
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analysis and simulation of transmit and receive pulse shaping filter is
an important aspect of digital wireless communication since it has a direct
effect on error probabilities. Pulse shaping for wireless communication over
time as well as frequency selective channels is the need of hour for 3G and 4G
systems. The pulse shaping filter is a useful means to shape the signal
spectrum and avoid interferences. Basically digital filters are used to modify
the characteristics of signal in time and frequency domain and have been
recognized as primary digital signal processing operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0849</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0849</id><created>2010-06-04</created><authors><author><keyname>Fyson</keyname><forenames>Nick</forenames></author><author><keyname>De Bie</keyname><forenames>Tijl</forenames></author><author><keyname>Cristianini</keyname><forenames>Nello</forenames></author></authors><title>Reconstruction of Causal Networks by Set Covering</title><categories>cs.DS stat.ML</categories><comments>Under consideration for the ECML PKDD 2010 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for the reconstruction of networks, based on the order of
nodes visited by a stochastic branching process. Our algorithm reconstructs a
network of minimal size that ensures consistency with the data. Crucially, we
show that global consistency with the data can be achieved through purely local
considerations, inferring the neighbourhood of each node in turn. The
optimisation problem solved for each individual node can be reduced to a Set
Covering Problem, which is known to be NP-hard but can be approximated well in
practice. We then extend our approach to account for noisy data, based on the
Minimum Description Length principle. We demonstrate our algorithms on
synthetic data, generated by an SIR-like epidemiological model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0850</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0850</id><created>2010-06-04</created><authors><author><keyname>Singh</keyname><forenames>Hardeep</forenames></author><author><keyname>Singh</keyname><forenames>Jasvir</forenames></author><author><keyname>Mian</keyname><forenames>M.</forenames></author></authors><title>Simulink based VoIP Analysis</title><categories>cs.NI</categories><comments>Submitted to Journal of Telecommunications, see
  http://sites.google.com/site/journaloftelecommunications/volume-2-issue-2-may-2010</comments><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, p126-130, May
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voice communication over internet not be possible without a reliable data
network, this was first available when distributed network topologies were used
in conjunction with data packets. Early network used single centre node network
in which a single workstation (Server) is responsible for the communication.
This posed problems as if there was a fault with the centre node, (workstation)
nothing would work. This problem was solved by the distributed system in which
reliability increases by spreading the load between many nodes. The idea of
packet switching &amp; distributed network were combined, this combination were
increased reliability, speed &amp; responsible for voice communication over
internet, Voice-over-IP (VoIP)These data packets travel through a
packet-switched network such as the Internet and arrive at their destination
where they are decompressed using a compatible Codec (audio coder/decoder) and
converted back to analogue audio. This paper deals with the Simulink
architecture for VoIP network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0856</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0856</id><created>2010-06-04</created><authors><author><keyname>Ali</keyname><forenames>Mahdi</forenames></author><author><keyname>Kachouri</keyname><forenames>Abdennacer</forenames></author><author><keyname>Samet</keyname><forenames>Mounir</forenames></author></authors><title>Novel method for planar microstrip antenna matching impedance</title><categories>cs.OH</categories><comments>Submitted to Journal of Telecommunications, see
  http://sites.google.com/site/journaloftelecommunications/volume-2-issue-2-may-2010</comments><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, p131-138, May
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Because all microstrip antennas have to be matched to the standard generator
impedance or load, the input impedance matching method for antenna is
particularly important. In this paper a new methodology in achieving matching
impedance of a planar microstrip antenna for wireless application is described.
The method is based on embedding an Interdigital capacitor. The fine results
obtained by using a microstrip Interdigital capacitor for matching antenna
impedance led to an efficient method to improve array antenna performance. In
fact, a substantial saving on the whole surfaces as well as enhancement of the
gain, the directivity and the power radiated was achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0859</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0859</id><created>2010-06-04</created><authors><author><keyname>Khalaj-Amirhosseini</keyname><forenames>M.</forenames></author><author><keyname>Akbarzadeh-Jahromi</keyname><forenames>S. A.</forenames></author></authors><title>To Optimally Design Microstrip Nonuniform Transmission Lines as Lowpass
  Filters</title><categories>cs.OH</categories><comments>Submitted to Journal of Telecommunications, see
  http://sites.google.com/site/journaloftelecommunications/volume-2-issue-2-may-2010</comments><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, p139-142, May
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method is proposed to optimally design the Microstrip Nonuniform
Transmission Line (MNTLs) as lowpass filters. Some electrical and physical
restrictions are used to design MNTLs. To optimally design the MNTLs, their
strip width is expanded as truncated Fourier series, firstly. Then, the optimum
values of the coefficients of the series are obtained through an optimization
approach. The performance of the proposed structure is studied by design and
fabrication of two lowpass filters of cutoff frequency 2.0 GHz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0860</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0860</id><created>2010-06-04</created><authors><author><keyname>Shet</keyname><forenames>N. S. V.</forenames></author><author><keyname>Chandrasekaran</keyname><forenames>K.</forenames></author><author><keyname>Shet</keyname><forenames>K. C.</forenames></author></authors><title>Implementation of Handoff through wireless access point Techniques</title><categories>cs.NI</categories><comments>Submitted to Journal of Telecommunications, see
  http://sites.google.com/site/journaloftelecommunications/volume-2-issue-2-may-2010</comments><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, p143-146, May
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Handoff has become an inevitable part of wireless cellular communication,
Soon users will carry small portable handheld devices which will incorporate
the computer, phone, camera, GPS, personal control module etc. This paper
proposes a new scheme to deal with seam less roaming and reduce failed
handoffs. The simulation is done using software called Qualnet meant for
wireless communication. The results clearly indicate the advantages of this new
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0861</identifier>
 <datestamp>2010-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0861</id><created>2010-06-04</created><authors><author><keyname>Chawla</keyname><forenames>Sonal</forenames></author><author><keyname>Singla</keyname><forenames>R. K.</forenames></author></authors><title>Mechanism for Learning Object retrieval supporting adaptivity</title><categories>cs.CY</categories><comments>Submitted to Journal of Telecommunications, see
  http://sites.google.com/site/journaloftelecommunications/volume-2-issue-2-may-2010</comments><journal-ref>Journal of Telecommunications,Volume 2, Issue 2, p147-152, May
  2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today\^as world designing adaptable course material requires new technical
knowledge which involves a need for a uniform protocol that allows organizing
resources with emphasis on quality and Learning. This can be achieved by
bundling the resources in a known and prescribed fashion called Learning
objects. Learning Objects are composed of two aspects namely &quot;Learning&quot; and
&quot;Object&quot;. The Learning aspect of Learning objects refers to Education. Since
Education is a process so the primary aim of learning objects tends to be
facilitating acquisition, assessment and conversion of content into Learning
objects while fostering the assimilation of these Learning objects into
learning modules and instruction. The Object part of Learning objects relates
to the Digital Electronic format of the resources i.e. to say that it deals
with the physical resource that forms the Learning objects. The objects in LOs
are analogous to objects used in object-oriented modeling (OOM). The analogy
helps visualize how LOs will be packaged, processed and transported across the
digital library as well as utilized in course building. OOM concepts such as
encapsulation, classification, polymorphism, inheritance and reuse can be
borrowed to describe the operations on LOs in the digital library. Thus, the
aim of this paper is threefolds. Firstly, to discuss the background of this
research and the concept of Learning Objects. Secondly, to provide a framework
for adaptive mechanism for the retrieval of Learning Objects and thirdly to
highlight the benefits that this new proposed framework shall bring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0866</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0866</id><created>2010-06-04</created><authors><author><keyname>Tzeng</keyname><forenames>Shing-Kwei</forenames><affiliation>Kainan University, Taiwan and</affiliation></author><author><keyname>Huang</keyname><forenames>Chih-Fang</forenames><affiliation>Yuan Ze University, Taiwan</affiliation></author></authors><title>A Study on the Interactive &quot;HOPSCOTCH&quot; Game for the Children Using
  Computer Music Techniques</title><categories>cs.SD</categories><comments>13 Pages</comments><journal-ref>International journal of Multimedia &amp; Its Applications 2.2 (2010)
  32-44</journal-ref><doi>10.5121/ijma.2010.2203</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  &quot;Hopscotch&quot; is a world-wide game for children to play since the times in the
ancient Roman Empire and China. Here we present a study mainly focused on the
research and discussion of the application on the children's well-know
edutainment via the physical interactive design to provide the sensing of the
times for the conventional hopscotch, which is a new type of experiment for the
technology aided edutainment. The innovated hopscotch music game involves the
sound samples of various animals and the characters of cartoon, and the
algorithmic composition via the development of the music technology based
interactive game, to gradually make the children perceive the world of digits,
sound, and music. It can guide the growing children's personality and character
from disorder into clarity. Furthermore, the traditional teaching materials can
be improved via the implementation of the electrical sensing devices,
electrical I/O module, and the computer music program Max/MSP, to integrate the
interactive computer music with the interactive and immersive soundscapes
composition, and the teaching tool with educational gaming is completely
accomplished eventually.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0869</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0869</id><created>2010-06-04</created><authors><author><keyname>Shi</keyname><forenames>Hao</forenames><affiliation>Victoria University, Australia</affiliation></author></authors><title>An Interactive Zoo Guide: A Case Study of Collaborative Learning</title><categories>cs.CY</categories><comments>11 Pages</comments><journal-ref>International journal of Multimedia &amp; Its Applications 2.2 (2010)
  57-67</journal-ref><doi>10.5121/ijma.2010.2205</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Real Industry Projects and team work can have a great impact on student
learning but providing these activities requires significant commitment from
academics. It requires several years planning implementing to create a
collaborative learning environment that mimics the real world ICT (Information
and Communication Technology) industry workplace. In this project, staff from
all the three faculties, namely the Faculty of Health, Engineering and Science,
Faculty of Arts, Education and Human Development, and Faculty of Business and
Law in higher education work together to establish a detailed project
management plan and to develop the unit guidelines for participating students.
The proposed project brings together students from business, multimedia and
computer science degrees studying their three project-based units within each
faculty to work on a relatively large IT project with our industry partner,
Melbourne Zoo. This paper presents one multimedia software project accomplished
by one of the multi-discipline student project teams. The project was called
'Interactive ZooOz Guide' and developed on a GPS-enabled PDA device in 2007.
The developed program allows its users to navigate through the Zoo via an
interactive map and provides multimedia information of animals on hotspots at
the 'Big Cats' section of the Zoo so that it enriches user experience at the
Zoo. A recent development in zoo applications is also reviewed. This paper is
also intended to encourage academia to break boundaries to enhance students'
learning beyond classroom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0871</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0871</id><created>2010-06-04</created><authors><author><keyname>Lutz</keyname><forenames>Tobias</forenames></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames></author><author><keyname>Hausl</keyname><forenames>Christoph</forenames></author></authors><title>Capacity for Half-Duplex Line Networks with Two Sources</title><categories>cs.IT math.IT</categories><comments>Proceedings of the IEEE International Symposium on Information
  Theory, Austin, TX, USA, June 12 - 18, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus is on noise-free half-duplex line networks with two sources where
the first node and either the second node or the second-last node in the
cascade act as sources. In both cases, we establish the capacity region of
rates at which both sources can transmit independent information to a common
sink. The achievability scheme presented for the first case is constructive
while the achievability scheme for the second case is based on a random coding
argument.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0876</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0876</id><created>2010-06-04</created><authors><author><keyname>Gouider</keyname><forenames>Mohamed Salah</forenames></author><author><keyname>Farhat</keyname><forenames>Amine</forenames></author></authors><title>Building a Data Warehouse for National Social Security Fund of the
  Republic of Tunisia</title><categories>cs.DB</categories><comments>13 pages</comments><journal-ref>International Journal of Database Management Systems 2.2 (2010)
  102-114</journal-ref><doi>10.5121/ijdms.2010.2207</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The amounts of data available to decision makers are increasingly important,
given the network availability, low cost storage and diversity of applications.
To maximize the potential of these data within the National Social Security
Fund (NSSF) in Tunisia, we have built a data warehouse as a multidimensional
database, cleaned, homogenized, historicized and consolidated. We used Oracle
Warehouse Builder to extract, transform and load the source data into the Data
Warehouse, by applying the KDD process. We have implemented the Data Warehouse
as an Oracle OLAP. The knowledge extraction has been performed using the Oracle
Discoverer tool. This allowed users to take maximum advantage of knowledge as a
regular report or as ad hoc queries. We started by implementing the main topic
for this public institution, accounting for the movements of insured persons.
The great success that has followed the completion of this work has encouraged
the NSSF to complete the achievement of other topics of interest within the
NSSF. We suggest in the near future to use Multidimensional Data Mining to
extract hidden knowledge and that are not predictable by the OLAP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0878</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0878</id><created>2010-06-04</created><authors><author><keyname>Shi</keyname><forenames>Hao</forenames></author></authors><title>Developing E-Learning Materials for Software Development Course</title><categories>cs.CY</categories><comments>7 pages</comments><journal-ref>International Journal of Managing Information Technology 2.2
  (2010) 15-21</journal-ref><doi>10.5121/ijmit.2010.2202</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Software Development is a core second-year course currently offered to
undergraduate students at Victoria University at its five local and
international campuses. The project aims to redesign the existing course
curriculum to support student-centred teaching and learning. It is intended to
provide a learning context in which learners can reflect on new material,
discuss their tentative understandings with others, actively search for new
information, develop skills in communication and collaboration, and build
conceptual connections to their existing knowledge base. The key feature of the
cross-campus curriculum innovation is the use of Blackboard, short for
Blackboard Learning System, to assist in course content organization and online
delivery. A well-defined and integrated case study is used throughout the
course to provide realistic practical experience of software development. It
allows students to take control of their own learning while at the same time
providing support to those students who have particular learning difficulties.
In this paper, the developed curriculum and the learning outcome are described.
The e-Learning material and various Blackboard tools used for teaching and
learning activities are presented. Finally, conclusion is drawn from classroom
experience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0880</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0880</id><created>2010-06-04</created><authors><author><keyname>Hu</keyname><forenames>Jinwei</forenames></author></authors><title>Expressiveness of a Provenance-Enabled Authorization Logic</title><categories>cs.LO</categories><comments>8 pages</comments><journal-ref>International Journal of Managing Information Technology 2.2
  (2010) 23-30</journal-ref><doi>10.5121/ijmit.2010.2203</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In distributed environments, access control decisions depend on statements of
multiple agents rather than only one central trusted party. However, existing
policy languages put few emphasis on authorization provenances. The capability
of managing these provenances is important and useful in various security areas
such as computer auditing and authorization recycling. Based on our previously
proposed logic, we present several case studies of this logic. By doing this,
we show its expressiveness and usefulness in security arena.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0888</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0888</id><created>2010-06-04</created><authors><author><keyname>Shen</keyname><forenames>Yuan</forenames></author><author><keyname>Win</keyname><forenames>Moe Z.</forenames></author></authors><title>Fundamental Limits of Wideband Localization - Part I: A General
  Framework</title><categories>cs.IT cs.NI math.IT</categories><comments>To appear in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of positional information is of great importance in many
commercial, public safety, and military applications. The coming years will see
the emergence of location-aware networks with sub-meter accuracy, relying on
accurate range measurements provided by wide bandwidth transmissions. In this
two-part paper, we determine the fundamental limits of localization accuracy of
wideband wireless networks in harsh multipath environments. We first develop a
general framework to characterize the localization accuracy of a given node
here and then extend our analysis to cooperative location-aware networks in
Part II.
  In this paper, we characterize localization accuracy in terms of a
performance measure called the squared position error bound (SPEB), and
introduce the notion of equivalent Fisher information to derive the SPEB in a
succinct expression. This methodology provides insights into the essence of the
localization problem by unifying localization information from individual
anchors and information from a priori knowledge of the agent's position in a
canonical form. Our analysis begins with the received waveforms themselves
rather than utilizing only the signal metrics extracted from these waveforms,
such as time-of-arrival and received signal strength. Hence, our framework
exploits all the information inherent in the received waveforms, and the
resulting SPEB serves as a fundamental limit of localization accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0890</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0890</id><created>2010-06-04</created><authors><author><keyname>Shen</keyname><forenames>Yuan</forenames></author><author><keyname>Wymeersch</keyname><forenames>Henk</forenames></author><author><keyname>Win</keyname><forenames>Moe Z.</forenames></author></authors><title>Fundamental Limits of Wideband Localization - Part II: Cooperative
  Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>To appear in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of positional information is of great importance in many
commercial, governmental, and military applications. Localization is commonly
accomplished through the use of radio communication between mobile devices
(agents) and fixed infrastructure (anchors). However, precise determination of
agent positions is a challenging task, especially in harsh environments due to
radio blockage or limited anchor deployment. In these situations, cooperation
among agents can significantly improve localization accuracy and reduce
localization outage probabilities. A general framework of analyzing the
fundamental limits of wideband localization has been developed in Part I of the
paper. Here, we build on this framework and establish the fundamental limits of
wideband cooperative location-aware networks. Our analysis is based on the
waveforms received at the nodes, in conjunction with Fisher information
inequality. We provide a geometrical interpretation of equivalent Fisher
information for cooperative networks. This approach allows us to succinctly
derive fundamental performance limits and their scaling behaviors, and to treat
anchors and agents in a unified way from the perspective of localization
accuracy. Our results yield important insights into how and when cooperation is
beneficial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0902</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0902</id><created>2010-06-04</created><authors><author><keyname>Tomescu</keyname><forenames>Alexandru I.</forenames></author></authors><title>On cycles through two arcs in strong multipartite tournaments</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A multipartite tournament is an orientation of a complete $c$-partite graph.
In [L. Volkmann, A remark on cycles through an arc in strongly connected
multipartite tournaments, Appl. Math. Lett. 20 (2007) 1148--1150], Volkmann
proved that a strongly connected $c$-partite tournament with $c \ge 3$ contains
an arc that belongs to a directed cycle of length $m$ for every $m \in \{3, 4,
\ldots, c\}$. He also conjectured the existence of three arcs with this
property. In this note, we prove the existence of two such arcs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0964</identifier>
 <datestamp>2010-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0964</id><created>2010-06-04</created><authors><author><keyname>Chatterjee</keyname><forenames>Debdeep</forenames></author><author><keyname>Wong</keyname><forenames>Tan F.</forenames></author><author><keyname>Oyman</keyname><forenames>Ozgur</forenames></author></authors><title>On Achievable Rate Regions for Half-Duplex Causal Cognitive Radio
  Channels</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures, ISIT 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coding for the causal cognitive radio channel, with the cognitive source
subjected to a half-duplex constraint, is studied. A discrete memoryless
channel model incorporating the half-duplex constraint is presented, and a new
achievable rate region is derived for this channel. It is proved that this rate
region contains the previously known causal achievable rate region of
\cite{Devroye06} for Gaussian channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0991</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0991</id><created>2010-06-04</created><authors><author><keyname>Harik</keyname><forenames>Georges</forenames></author><author><keyname>Shazeer</keyname><forenames>Noam</forenames></author></authors><title>Variational Program Inference</title><categories>cs.AI</categories><report-no>HSL-000001</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a framework for representing a variety of interesting problems
as inference over the execution of probabilistic model programs. We represent a
&quot;solution&quot; to such a problem as a guide program which runs alongside the model
program and influences the model program's random choices, leading the model
program to sample from a different distribution than from its priors. Ideally
the guide program influences the model program to sample from the posteriors
given the evidence. We show how the KL- divergence between the true posterior
distribution and the distribution induced by the guided model program can be
efficiently estimated (up to an additive constant) by sampling multiple
executions of the guided model program. In addition, we show how to use the
guide program as a proposal distribution in importance sampling to
statistically prove lower bounds on the probability of the evidence and on the
probability of a hypothesis and the evidence. We can use the quotient of these
two bounds as an estimate of the conditional probability of the hypothesis
given the evidence. We thus turn the inference problem into a heuristic search
for better guide programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.0992</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.0992</id><created>2010-06-04</created><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author><author><keyname>Zvesper</keyname><forenames>Jonathan</forenames></author></authors><title>From Lawvere to Brandenburger-Keisler: interactive forms of
  diagonalization and self-reference</title><categories>math.LO cs.GT</categories><comments>To appear in the Proceedings of LOFT 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the Brandenburger-Keisler paradox in epistemic game theory, which
is a `two-person version of Russell's paradox'. Our aim is to understand how it
relates to standard one-person arguments, and why the `believes-assumes'
modality used in the argument arises.
  We recast it as a fixpoint result, which can be carried out in any regular
category, and show how it can be reduced to a relational form of the one-person
diagonal argument due to Lawvere. We give a compositional account, which leads
to simple multi-agent generalizations.
  We also outline a general approach to the construction of assumption complete
models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1003</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1003</id><created>2010-06-04</created><updated>2012-03-28</updated><authors><author><keyname>Friedrich</keyname><forenames>Tobias</forenames></author><author><keyname>Levine</keyname><forenames>Lionel</forenames></author></authors><title>Fast simulation of large-scale growth models</title><categories>math.PR cond-mat.stat-mech cs.DM cs.DS math.CO</categories><comments>27 pages, 9 figures. To appear in Random Structures &amp; Algorithms</comments><msc-class>82C24, 05C81, 05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algorithm that computes the final state of certain growth models
without computing all intermediate states. Our technique is based on a &quot;least
action principle&quot; which characterizes the odometer function of the growth
process. Starting from an approximation for the odometer, we successively
correct under- and overestimates and provably arrive at the correct final
state.
  Internal diffusion-limited aggregation (IDLA) is one of the models amenable
to our technique. The boundary fluctuations in IDLA were recently proved to be
at most logarithmic in the size of the growth cluster, but the constant in
front of the logarithm is still not known. As an application of our method, we
calculate the size of fluctuations over two orders of magnitude beyond previous
simulations, and use the results to estimate this constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1010</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1010</id><created>2010-06-04</created><authors><author><keyname>Pavlovic</keyname><forenames>Dusko</forenames></author></authors><title>Geometry of abstraction in quantum computation</title><categories>quant-ph cs.LO math.CT</categories><comments>28 pages, 42 figures; Clifford Lectures 2008 (main speaker Samson
  Abramsky)</comments><report-no>Oxford University Computing Laboratory RR-09-13</report-no><journal-ref>Proceedings of Symposia in Applied Mathematics vol. 71 (2012)
  233--267</journal-ref><doi>10.1090/psapm/071</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum algorithms are sequences of abstract operations, performed on
non-existent computers. They are in obvious need of categorical semantics. We
present some steps in this direction, following earlier contributions of
Abramsky, Coecke and Selinger. In particular, we analyze function abstraction
in quantum computation, which turns out to characterize its classical
interfaces. Some quantum algorithms provide feasible solutions of important
hard problems, such as factoring and discrete log (which are the building
blocks of modern cryptography). It is of a great practical interest to
precisely characterize the computational resources needed to execute such
quantum algorithms. There are many ideas how to build a quantum computer. Can
we prove some necessary conditions? Categorical semantics help with such
questions. We show how to implement an important family of quantum algorithms
using just abelian groups and relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1011</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1011</id><created>2010-06-04</created><authors><author><keyname>Pavlovic</keyname><forenames>Dusko</forenames></author></authors><title>Relating toy models of quantum computation: comprehension,
  complementarity and dagger mix autonomous categories</title><categories>quant-ph cs.LO math-ph math.CT math.MP</categories><comments>21 pages, 6 figures; Proceedings of Quantum Physics and Logic, Oxford
  8-9 April 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Toy models have been used to separate important features of quantum
computation from the rich background of the standard Hilbert space model.
Category theory, on the other hand, is a general tool to separate components of
mathematical structures, and analyze one layer at a time. It seems natural to
combine the two approaches, and several authors have already pursued this idea.
We explore *categorical comprehension construction* as a tool for adding
features to toy models. We use it to comprehend quantum propositions and
probabilities within the basic model of finite-dimensional Hilbert spaces. We
also analyze complementary quantum observables over the category of sets and
relations. This leads into the realm of *test spaces*, a well-studied model. We
present one of many possible extensions of this model, enabled by the
comprehension construction. Conspicuously, all models obtained in this way
carry the same categorical structure, *extending* the familiar dagger compact
framework with the complementation operations. We call the obtained structure
*dagger mix autonomous*, because it extends mix autonomous categories, popular
in computer science, in a similar way like dagger compact structure extends
compact categories. Dagger mix autonomous categories seem to arise quite
naturally in quantum computation, as soon as complementarity is viewed as a
part of the global structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1017</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1017</id><created>2010-06-04</created><authors><author><keyname>Thampi</keyname><forenames>Sabu M.</forenames></author><author><keyname>K</keyname><forenames>Chandra Sekaran</forenames></author></authors><title>An Enhanced Search Technique for Managing Partial Coverage and Free
  Riding in P2P Networks</title><categories>cs.NI</categories><comments>9 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a Q-learning based scheme for managing the partial
coverage problem and the ill-effects of free riding in unstructured P2P
networks. Based on various parameter values collected during query routing,
reward for the actions are computed and these rewards are used for updating the
corresponding Q-values of peers. Thus, the routing is done through only nodes
which have shown high performance in the past. Simulation experiments are
conducted in several times and the results are plotted. Results show that the
proposed scheme effectively manages free riders, generates high hit ratio,
reduces network traffic and manages partial coverage problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1018</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1018</id><created>2010-06-04</created><authors><author><keyname>Thampi</keyname><forenames>Sabu M.</forenames></author><author><keyname>K</keyname><forenames>Chandra Sekaran</forenames></author></authors><title>`Q-Feed' - An Effective Solution for the Free-riding Problem in
  Unstructured P2P Networks</title><categories>cs.NI</categories><comments>14 pages, 10 figures</comments><journal-ref>International Journal of Digital Multimedia Broadcasting, 2010</journal-ref><doi>10.1155/2010/793591</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a solution for reducing the ill effects of free-riders in
decentralised unstructured P2P networks. An autonomous replication scheme is
proposed to improve the availability and enhance system performance. Q-learning
is widely employed in different situations to improve the accuracy in decision
making by each peer. Based on the performance of neighbours of a peer, every
neighbour is awarded different levels of ranks. At the same time a
low-performing node is allowed to improve its rank in different ways.
Simulation results show that Q-learning based free riding control mechanism
effectively limits the services received by free-riders and also encourages the
low-performing neighbours to improve their position. The popular files are
autonomously replicated to nodes possessing required parameters. Due to this
improvement of quantity of popular files, free riders are given opportunity to
lift their position for active participation in the network for sharing files.
Q-feed effectively manages queries from free riders and reduces network traffic
significantly
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1019</identifier>
 <datestamp>2010-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1019</id><created>2010-06-05</created><updated>2010-08-31</updated><authors><author><keyname>Liu</keyname><forenames>Jian</forenames></author><author><keyname>Chiu</keyname><forenames>Dah Ming</forenames></author></authors><title>Mathematical Modeling of Competition in Sponsored Search Market</title><categories>cs.GT</categories><comments>A short version would appear at 2010 Workshop on the Economics of
  Networks, Systems, and Computation (NetEcon '10)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sponsored search mechanisms have drawn much attention from both academic
community and industry in recent years since the seminal papers of [13] and
[14]. However, most of the existing literature concentrates on the mechanism
design and analysis within the scope of only one search engine in the market.
In this paper we propose a mathematical framework for modeling the interaction
of publishers, advertisers and end users in a competitive market. We first
consider the monopoly market model and provide optimal solutions for both ex
ante and ex post cases, which represents the long-term and short-term revenues
of search engines respectively. We then analyze the strategic behaviors of end
users and advertisers under duopoly and prove the existence of equilibrium for
both search engines to co-exist from ex-post perspective. To show the more
general ex ante results, we carry out extensive simulations under different
parameter settings. Our analysis and observation in this work can provide
useful insight in regulating the sponsored search market and protecting the
interests of advertisers and end users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1024</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1024</id><created>2010-06-05</created><authors><author><keyname>Wang</keyname><forenames>Xuepeng</forenames></author><author><keyname>Bai</keyname><forenames>Baoming</forenames></author><author><keyname>Ma</keyname><forenames>Xiao</forenames></author></authors><title>A Low-Complexity Joint Detection-Decoding Algorithm for Nonbinary
  LDPC-Coded Modulation Systems</title><categories>cs.IT math.IT</categories><comments>5 pages, 8 figures, to be presented at the IEEE Symposium on
  Information Theory (ISIT 2010) in Austin, Texas, June 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a low-complexity joint detection-decoding algorithm
for nonbinary LDPC codedmodulation systems. The algorithm combines
hard-decision decoding using the message-passing strategy with the signal
detector in an iterative manner. It requires low computational complexity,
offers good system performance and has a fast rate of decoding convergence.
Compared to the q-ary sum-product algorithm (QSPA), it provides an attractive
candidate for practical applications of q-ary LDPC codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1029</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1029</id><created>2010-06-05</created><authors><author><keyname>Kastrin</keyname><forenames>Andrej</forenames></author><author><keyname>Peterlin</keyname><forenames>Borut</forenames></author><author><keyname>Hristovski</keyname><forenames>Dimitar</forenames></author></authors><title>Chi-square-based scoring function for categorization of MEDLINE
  citations</title><categories>cs.IR stat.AP stat.ML</categories><comments>34 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objectives: Text categorization has been used in biomedical informatics for
identifying documents containing relevant topics of interest. We developed a
simple method that uses a chi-square-based scoring function to determine the
likelihood of MEDLINE citations containing genetic relevant topic. Methods: Our
procedure requires construction of a genetic and a nongenetic domain document
corpus. We used MeSH descriptors assigned to MEDLINE citations for this
categorization task. We compared frequencies of MeSH descriptors between two
corpora applying chi-square test. A MeSH descriptor was considered to be a
positive indicator if its relative observed frequency in the genetic domain
corpus was greater than its relative observed frequency in the nongenetic
domain corpus. The output of the proposed method is a list of scores for all
the citations, with the highest score given to those citations containing MeSH
descriptors typical for the genetic domain. Results: Validation was done on a
set of 734 manually annotated MEDLINE citations. It achieved predictive
accuracy of 0.87 with 0.69 recall and 0.64 precision. We evaluated the method
by comparing it to three machine learning algorithms (support vector machines,
decision trees, na\&quot;ive Bayes). Although the differences were not statistically
significantly different, results showed that our chi-square scoring performs as
good as compared machine learning algorithms. Conclusions: We suggest that the
chi-square scoring is an effective solution to help categorize MEDLINE
citations. The algorithm is implemented in the BITOLA literature-based
discovery support system as a preprocessor for gene symbol disambiguation
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1030</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1030</id><created>2010-06-05</created><authors><author><keyname>Kastrin</keyname><forenames>Andrej</forenames></author><author><keyname>Peterlin</keyname><forenames>Borut</forenames></author></authors><title>Rasch-based high-dimensionality data reduction and class prediction with
  applications to microarray gene expression data</title><categories>cs.AI stat.AP stat.ME stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Class prediction is an important application of microarray gene expression
data analysis. The high-dimensionality of microarray data, where number of
genes (variables) is very large compared to the number of samples (obser-
vations), makes the application of many prediction techniques (e.g., logistic
regression, discriminant analysis) difficult. An efficient way to solve this
prob- lem is by using dimension reduction statistical techniques. Increasingly
used in psychology-related applications, Rasch model (RM) provides an appealing
framework for handling high-dimensional microarray data. In this paper, we
study the potential of RM-based modeling in dimensionality reduction with
binarized microarray gene expression data and investigate its prediction ac-
curacy in the context of class prediction using linear discriminant analysis.
Two different publicly available microarray data sets are used to illustrate a
general framework of the approach. Performance of the proposed method is
assessed by re-randomization scheme using principal component analysis (PCA) as
a benchmark method. Our results show that RM-based dimension reduction is as
effective as PCA-based dimension reduction. The method is general and can be
applied to the other high-dimensional data problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1031</identifier>
 <datestamp>2010-07-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1031</id><created>2010-06-05</created><updated>2010-07-23</updated><authors><author><keyname>Lust</keyname><forenames>Thibaut</forenames></author><author><keyname>Teghem</keyname><forenames>Jacques</forenames></author></authors><title>Multiobjective decomposition of integer matrices: application to
  radiotherapy</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following problem: to decompose a nonnegative integer matrix
into a linear combination of binary matrices that respect the consecutive ones
prop- erty. This problem occurs in the radiotherapy treatment of cancer. The
nonnegative integer matrix corresponds to fields giving the different radiation
beams that a linear accelerator has to send throughout the body of a patient.
Due to the in- homogeneous dose levels, leaves from a multi-leaf collimator are
used between the accelerator and the body of the patient to block the
radiations. The leaves positions can be represented by segments, that are
binary matrices with the consecutive ones property. The aim is to find
efficient decompositions that simultaneously minimize the irradiation time, the
cardinality of the decomposition and the setup-time to configure the multi-leaf
collimator at each step of the decomposition. We propose for this NP-hard
multiobjective combinatorial problem a heuristic, based on the adaptation of
the two-phase Pareto local search. Experiments are carried out on different
size instances and the results are reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1032</identifier>
 <datestamp>2010-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1032</id><created>2010-06-05</created><authors><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author><author><keyname>van Eck</keyname><forenames>Nees Jan</forenames></author><author><keyname>Noyons</keyname><forenames>Ed C. M.</forenames></author></authors><title>A unified approach to mapping and clustering of bibliometric networks</title><categories>cs.DL physics.data-an physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the analysis of bibliometric networks, researchers often use mapping and
clustering techniques in a combined fashion. Typically, however, mapping and
clustering techniques that are used together rely on very different ideas and
assumptions. We propose a unified approach to mapping and clustering of
bibliometric networks. We show that the VOS mapping technique and a weighted
and parameterized variant of modularity-based clustering can both be derived
from the same underlying principle. We illustrate our proposed approach by
producing a combined mapping and clustering of the most frequently cited
publications that appeared in the field of information science in the period
1999-2008.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1043</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1043</id><created>2010-06-05</created><authors><author><keyname>Kounchev</keyname><forenames>Ognyan</forenames></author><author><keyname>Kalaglarsky</keyname><forenames>Damyan</forenames></author></authors><title>Polyharmonic Daubechies type wavelets in Image Processing and Astronomy,
  I</title><categories>math.NA cs.NA</categories><comments>6 pages, prepared for the ACM proceedings of CompSysTech 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new family of multivariate wavelets which are obtained by
&quot;polyharmonic subdivision&quot;. They generalize directly the original compactly
supported Daubechies wavelets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1055</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1055</id><created>2010-06-05</created><authors><author><keyname>Samid</keyname><forenames>Gideon</forenames></author></authors><title>Shannon Revisited: Considering a More Tractable Expression to Measure
  and Manage Intractability, Uncertainty, Risk, Ignorance, and Entropy</title><categories>cs.IT math.IT</categories><report-no>CLN: 10-500</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building on Shannon's lead, let's consider a more malleable expression for
tracking uncertainty, and states of &quot;knowledge available&quot; vs. &quot;knowledge
missing,&quot; to better practice innovation, improve risk management, and
successfully measure progress of intractable undertakings. Shannon's formula,
and its common replacements (Renyi, Tsallis) compute to increased knowledge
whenever two competing choices, however marginal, exchange probability
measures. Such and other distortions are corrected by anchoring knowledge to a
reference challenge. Entropy then expresses progress towards meeting that
challenge. We introduce an 'interval of interest' outside which all probability
changes should be ignored. The resultant formula for Missing Acquirable
Relevant Knowledge (MARK) serves as a means to optimize intractable activities
involving knowledge acquisition, such as research, development, risk
management, and opportunity exploitation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1057</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1057</id><created>2010-06-05</created><authors><author><keyname>Gabidulin</keyname><forenames>Ernst M.</forenames></author><author><keyname>Rashwan</keyname><forenames>Haitham</forenames></author><author><keyname>Honary</keyname><forenames>Bahram</forenames></author></authors><title>On improving security of GPT cryptosystems</title><categories>cs.CR cs.IT math.IT</categories><comments>5 pages. submitted ISIT 2009.Processed on IEEE ISIT2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The public key cryptosystem based on rank error correcting codes (the GPT
cryptosystem) was proposed in 1991. Use of rank codes in cryptographic
applications is advantageous since it is practically impossible to utilize
combinatoric decoding. This enabled using public keys of a smaller size.
Several attacks against this system were published, including Gibson's attacks
and more recently Overbeck's attacks. A few modifications were proposed
withstanding Gibson's attack but at least one of them was broken by the
stronger attacks by Overbeck. A tool to prevent Overbeck's attack is presented
in [12]. In this paper, we apply this approach to other variants of the GPT
cryptosystem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1074</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1074</id><created>2010-06-05</created><authors><author><keyname>Monnerville</keyname><forenames>M.</forenames></author><author><keyname>S&#xe9;mah</keyname><forenames>G.</forenames></author></authors><title>Youpi, a Web-based Astronomical Image Processing Pipeline</title><categories>cs.DC</categories><comments>4 pages, 2 figures, to appear in the proceedings of ADASS XIX, Oct
  4-8 2009, Sapporo, Japan (ASP Conf. Series)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Youpi stands for &quot;YOUpi is your processing PIpeline&quot;. It is a portable, easy
to use web application providing high level functionalities to perform data
reduction on scientific FITS images. It is built on top of open source
processing tools that are released to the community by Terapix, in order to
organize your data on a computer cluster, to manage your processing jobs in
real time and to facilitate teamwork by allowing fine-grain sharing of results
and data. On the server side, Youpi is written in the Python programming
language and uses the Django web framework. On the client side, Ajax techniques
are used along with the Prototype and script.aculo.us Javascript librairies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1080</identifier>
 <datestamp>2010-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1080</id><created>2010-06-06</created><authors><author><keyname>Rodriguez</keyname><forenames>Marko A.</forenames></author><author><keyname>Pepe</keyname><forenames>Alberto</forenames></author><author><keyname>Shinavier</keyname><forenames>Joshua</forenames></author></authors><title>The Dilated Triple</title><categories>cs.AI</categories><report-no>LA-UR-08-03927</report-no><acm-class>I.2.4</acm-class><journal-ref>Emergent Web Intelligence: Advanced Semantic Technologies,
  Advanced Information and Knowledge Processing series, pages 3-16,
  ISBN:978-1-84996-076-2, Springer-Verlag, June 2010</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The basic unit of meaning on the Semantic Web is the RDF statement, or
triple, which combines a distinct subject, predicate and object to make a
definite assertion about the world. A set of triples constitutes a graph, to
which they give a collective meaning. It is upon this simple foundation that
the rich, complex knowledge structures of the Semantic Web are built. Yet the
very expressiveness of RDF, by inviting comparison with real-world knowledge,
highlights a fundamental shortcoming, in that RDF is limited to statements of
absolute fact, independent of the context in which a statement is asserted.
This is in stark contrast with the thoroughly context-sensitive nature of human
thought. The model presented here provides a particularly simple means of
contextualizing an RDF triple by associating it with related statements in the
same graph. This approach, in combination with a notion of graph similarity, is
sufficient to select only those statements from an RDF graph which are
subjectively most relevant to the context of the requesting process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1104</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1104</id><created>2010-06-06</created><authors><author><keyname>Rice</keyname><forenames>Jacqueline E.</forenames></author><author><keyname>Kent</keyname><forenames>Kenneth B.</forenames></author></authors><title>Systolic Array Technique for Determining Common Approximate Substrings</title><categories>cs.DS</categories><comments>Submitted to Journal of Computer Science and Engineering, see
  http://sites.google.com/site/jcseuk/volume-1-issue-1-may-2010</comments><journal-ref>Journal of Computer Science and Engineering, Volume 1, Issue 1,
  p1-9, May 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A technique using a systolic array structure is proposed for solving the
common approximate substring (CAS) problem. This approach extends the technique
introduced in earlier work from the computation of the edit-distance between
two strings to the more encompassing CAS problem. A comparison to existing work
is given, and the technique presented is validated and analyzed based on
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1117</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1117</id><created>2010-06-06</created><authors><author><keyname>Cohen</keyname><forenames>Hagai</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author></authors><title>On the hardness of distance oracle for sparse graph</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that set-intersection is harder than distance oracle on
sparse graphs. Given a collection of total size n which consists of m sets
drawn from universe U, the set-intersection problem is to build a data
structure which can answer whether two sets have any intersection. A distance
oracle is a data structure which can answer distance queries on a given graph.
We show that if one can build distance oracle for sparse graph G=(V,E), which
requires s(|V|,|E|) space and answers a (2-\epsilon,c)-approximate distance
query in time t(|V|,|E|) where (2-\epsilon) is a multiplicative error and c is
a constant additive error, then, set-intersection can be solved in t(m+|U|,n)
time using s(m+|U|,n) space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1126</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1126</id><created>2010-06-06</created><authors><author><keyname>Haller</keyname><forenames>Kirk</forenames></author><author><keyname>John</keyname><forenames>Audrey Lee-St.</forenames></author><author><keyname>Sitharam</keyname><forenames>Meera</forenames></author><author><keyname>Streinu</keyname><forenames>Ileana</forenames></author><author><keyname>White</keyname><forenames>Neil</forenames></author></authors><title>Body-and-cad Geometric Constraint Systems</title><categories>cs.CG</categories><comments>33 pages, to appear in Computational Geometry: Theory and
  Applications (an abbreviated version appeared in: 24th Annual ACM Symposium
  on Applied Computing, Technical Track on Geometric Constraints and Reasoning
  GCR'09, Honolulu, HI, 2009)</comments><msc-class>68R10, 05C85, 05C50</msc-class><acm-class>I.3.5; J.6; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by constraint-based CAD software, we develop the foundation for the
rigidity theory of a very general model: the body-and-cad structure, composed
of rigid bodies in 3D constrained by pairwise coincidence, angular and distance
constraints. We identify 21 relevant geometric constraints and develop the
corresponding infinitesimal rigidity theory for these structures. The classical
body-and-bar rigidity model can be viewed as a body-and-cad structure that uses
only one constraint from this new class. As a consequence, we identify a new,
necessary, but not sufficient, counting condition for minimal rigidity of
body-and-cad structures: nested sparsity. This is a slight generalization of
the well-known sparsity condition of Maxwell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1129</identifier>
 <datestamp>2010-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1129</id><created>2010-06-06</created><updated>2010-08-22</updated><authors><author><keyname>Pestov</keyname><forenames>Vladimir</forenames></author></authors><title>Predictive PAC learnability: a paradigm for learning from exchangeable
  input data</title><categories>cs.LG</categories><comments>5 pages, latex, a postprint correcting a typo in the main definition
  4.1</comments><msc-class>68T05</msc-class><acm-class>I.2.6</acm-class><journal-ref>Proc. 2010 IEEE International Conference on Granular Computing
  (GrC 2010), San Jose, CA, August 14-16, 2010, IEEE Computer Society, Los
  Alamitos, 2010, pp. 387-391, Symposium on Foundations and Practice of Data
  Mining</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exchangeable random variables form an important and well-studied
generalization of i.i.d. variables, however simple examples show that no
nontrivial concept or function classes are PAC learnable under general
exchangeable data inputs $X_1,X_2,\ldots$. Inspired by the work of Berti and
Rigo on a Glivenko--Cantelli theorem for exchangeable inputs, we propose a new
paradigm, adequate for learning from exchangeable data: predictive PAC
learnability. A learning rule $\mathcal L$ for a function class $\mathscr F$ is
predictive PAC if for every $\e,\delta&gt;0$ and each function $f\in {\mathscr
F}$, whenever $\abs{\sigma}\geq s(\delta,\e)$, we have with confidence
$1-\delta$ that the expected difference between $f(X_{n+1})$ and the image of
$f\vert\sigma$ under $\mathcal L$ does not exceed $\e$ conditionally on
$X_1,X_2,\ldots,X_n$. Thus, instead of learning the function $f$ as such, we
are learning to a given accuracy $\e$ the predictive behaviour of $f$ at the
future points $X_i(\omega)$, $i&gt;n$ of the sample path. Using de Finetti's
theorem, we show that if a universally separable function class $\mathscr F$ is
distribution-free PAC learnable under i.i.d. inputs, then it is
distribution-free predictive PAC learnable under exchangeable inputs, with a
slightly worse sample complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1138</identifier>
 <datestamp>2014-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1138</id><created>2010-06-06</created><updated>2014-08-12</updated><authors><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author><author><keyname>Sridharan</keyname><forenames>Karthik</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Online Learning via Sequential Complexities</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of sequential prediction and provide tools to study
the minimax value of the associated game. Classical statistical learning theory
provides several useful complexity measures to study learning with i.i.d. data.
Our proposed sequential complexities can be seen as extensions of these
measures to the sequential setting. The developed theory is shown to yield
precise learning guarantees for the problem of sequential prediction. In
particular, we show necessary and sufficient conditions for online learnability
in the setting of supervised learning. Several examples show the utility of our
framework: we can establish learnability without having to exhibit an explicit
online learning algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1149</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1149</id><created>2010-06-06</created><authors><author><keyname>Karmakar</keyname><forenames>Sanjay</forenames></author><author><keyname>Varanasi</keyname><forenames>Mahesh K.</forenames></author></authors><title>The diversity-multiplexing tradeoff of the symmetric MIMO 2-user
  interference channel</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures. To be presented at ISIT 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fundamental diversity-multiplexing tradeoff (DMT) of the quasi-static
fading, symmetric $2$-user MIMO interference channel (IC) with channel state
information at the transmitters (CSIT) and a short term average power
constraint is obtained. The general case is considered where the
interference-to-noise ratio (INR) at each receiver scales differently from the
signal-to-noise ratio (SNR) at the receivers. The achievability of the DMT is
proved by showing that a simple Han-Kobayashi coding scheme can achieve a rate
region which is within a constant (independent of SNR) number of bits from a
set of upper bounds to the capacity region of the IC. In general, only part of
the DMT curve with CSIT can be achieved by coding schemes which do not use any
CSIT (No-CSIT). A result in this paper establishes a threshold for the INR
beyond which the DMT with CSIT coincides with that with No-CSIT. Our result
also settles one of the conjectures made in~\cite{EaOlCv}. Furthermore, the
fundamental DMT of a class of non-symmetric ICs with No-CSIT is also obtained
wherein the two receivers have different numbers of antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1162</identifier>
 <datestamp>2010-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1162</id><created>2010-06-06</created><updated>2010-06-10</updated><authors><author><keyname>Nguyen</keyname><forenames>Khoa D.</forenames></author><author><keyname>Rasmussen</keyname><forenames>Lars K.</forenames></author><author><keyname>Fabregas</keyname><forenames>Albert Guillen i</forenames></author><author><keyname>Letzepis</keyname><forenames>Nick</forenames></author></authors><title>MIMO ARQ with Multi-bit Feedback: Outage Analysis</title><categories>cs.IT math.IT</categories><comments>28 pages, 7 figures; submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the asymptotic outage performance of incremental redundancy
automatic repeat request (INR-ARQ) transmission over the multiple-input
multiple-output (MIMO) block-fading channels with discrete input
constellations. We first show that transmission with random codes using a
discrete signal constellation across all transmit antennas achieves the optimal
outage diversity given by the Singleton bound. We then analyze the optimal
SNR-exponent and outage diversity of INR-ARQ transmission over the MIMO
block-fading channel. We show that a significant gain in outage diversity is
obtained by providing more than one bit feedback at each ARQ round. Thus, the
outage performance of INR-ARQ transmission can be remarkably improved with
minimal additional overhead. A suboptimal feedback and power adaptation rule,
which achieves the optimal outage diversity, is proposed for MIMO INR-ARQ,
demonstrating the benefits provided by multi-bit feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1165</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1165</id><created>2010-06-06</created><authors><author><keyname>Soldo</keyname><forenames>Fabio</forenames></author><author><keyname>Argyraki</keyname><forenames>Katerina</forenames></author><author><keyname>Markopoulou</keyname><forenames>Athina</forenames></author></authors><title>Optimal Source-Based Filtering of Malicious Traffic</title><categories>cs.NI</categories><comments>Conference version appeared in Infocom 2009. Journal version
  submitted to ToN</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of blocking malicious traffic on the
Internet, via source-based filtering. In particular, we consider filtering via
access control lists (ACLs): these are already available at the routers today
but are a scarce resource because they are stored in the expensive ternary
content addressable memory (TCAM). Aggregation (by filtering source prefixes
instead of individual IP addresses) helps reduce the number of filters, but
comes also at the cost of blocking legitimate traffic originating from the
filtered prefixes. We show how to optimally choose which source prefixes to
filter, for a variety of realistic attack scenarios and operators' policies. In
each scenario, we design optimal, yet computationally efficient, algorithms.
Using logs from Dshield.org, we evaluate the algorithms and demonstrate that
they bring significant benefit in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1172</identifier>
 <datestamp>2010-06-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1172</id><created>2010-06-07</created><authors><author><keyname>Talari</keyname><forenames>Ali</forenames></author><author><keyname>Rahnavard</keyname><forenames>Nazanin</forenames></author></authors><title>Distributed Rateless Codes with UEP Property</title><categories>cs.IT math.IT</categories><comments>Accepted, 2010 IEEE International Symposium on Information Theory
  (ISIT 2010)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When multiple sources of data need to transmit their rateless coded symbols
through a single relay to a common destination, a distributed rateless code
instead of several separate conventional rateless codes can be employed to
encode the input symbols to increase the transmission efficiency and
flexibility.
  In this paper, we propose distributed rateless codes DU-rateless that can
provide unequal error protection (UEP) for distributed sources with different
data block lengths and different importance levels. We analyze our proposed
DU-rateless code employing And-Or tree analysis technique. Next, we design
several sets of optimum DU-rateless codes for various setups employing
multi-objective genetic algorithms and evaluate their performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1177</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1177</id><created>2010-06-07</created><authors><author><keyname>Addepallil</keyname><forenames>Srirangam V</forenames></author><author><keyname>Andersen</keyname><forenames>Per</forenames></author><author><keyname>Barnes</keyname><forenames>George L</forenames></author></authors><title>Efficient Resource Matching in Heterogeneous Grid Using Resource Vector</title><categories>cs.DC</categories><comments>10 pages</comments><journal-ref>International Journal of Computer Science and Information
  Technology 2.3 (2010) 1-10</journal-ref><doi>10.5121/ijcsit.2010.2301</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, a method for efficient scheduling to obtain optimum job
throughput in a distributed campus grid environment is presented; Traditional
job schedulers determine job scheduling using user and job resource attributes.
User attributes are related to current usage, historical usage, user priority
and project access. Job resource attributes mainly comprise of soft
requirements (compilers, libraries) and hard requirements like memory, storage
and interconnect. A job scheduler dispatches jobs to a resource if a job's hard
and soft requirements are met by a resource. In current scenario during
execution of a job, if a resource becomes unavailable, schedulers are presented
with limited options, namely re-queuing job or migrating job to a different
resource. Both options are expensive in terms of data and compute time. These
situations can be avoided, if the often ignored factor, availability time of a
resource in a grid environment is considered. We propose resource rank
approach, in which jobs are dispatched to a resource which has the highest rank
among all resources that match the job's requirement. The results show that our
approach can increase throughput of many serial / monolithic jobs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1178</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1178</id><created>2010-06-07</created><authors><author><keyname>Chen</keyname><forenames>Chao</forenames><affiliation>Indiana University - Purdue University, USA</affiliation></author><author><keyname>Pomalaza-Raez</keyname><forenames>Carlos</forenames><affiliation>Indiana University - Purdue University, USA</affiliation></author></authors><title>Implementing and Evaluating a Wireless Body Sensor System for Automated
  Physiological Data Acquisition at Home</title><categories>cs.OH</categories><comments>15 pages</comments><journal-ref>International Journal of Computer Science and Information
  Technology 2.3 (2010) 24-38</journal-ref><doi>10.5121/ijcsit.2010.2303</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Advances in embedded devices and wireless sensor networks have resulted in
new and inexpensive health care solutions. This paper describes the
implementation and the evaluation of a wireless body sensor system that
monitors human physiological data at home. Specifically, a waist-mounted
triaxial accelerometer unit is used to record human movements. Sampled data are
transmitted using an IEEE 802.15.4 wireless transceiver to a data logger unit.
The wearable sensor unit is light, small, and consumes low energy, which allows
for inexpensive and unobtrusive monitoring during normal daily activities at
home. The acceleration measurement tests show that it is possible to classify
different human motion through the acceleration reading. The 802.15.4 wireless
signal quality is also tested in typical home scenarios. Measurement results
show that even with interference from nearby IEEE 802.11 signals and microwave
ovens, the data delivery performance is satisfactory and can be improved by
selecting an appropriate channel. Moreover, we found that the wireless signal
can be attenuated by housing materials, home appliances, and even plants.
Therefore, the deployment of wireless body sensor systems at home needs to take
all these factors into consideration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1179</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1179</id><created>2010-06-07</created><authors><author><keyname>Marimuthu</keyname><forenames>C. N.</forenames><affiliation>Maharaja Engineering College, India</affiliation></author><author><keyname>Thangaraj</keyname><forenames>P.</forenames><affiliation>Kongu Engineering College, India</affiliation></author><author><keyname>Ramesan</keyname><forenames>Aswathy</forenames><affiliation>Maharaja Engineering College, India</affiliation></author></authors><title>Low Power Shift and Add Multiplier Design</title><categories>cs.AR</categories><comments>11 pages</comments><journal-ref>International Journal of Computer Science and Information
  Technology 2.3 (2010) 12-22</journal-ref><doi>10.5121/ijcsit.2010.2302</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Today every circuit has to face the power consumption issue for both portable
device aiming at large battery life and high end circuits avoiding cooling
packages and reliability issues that are too complex. It is generally accepted
that during logic synthesis power tracks well with area. This means that a
larger design will generally consume more power. The multiplier is an important
kernel of digital signal processors. Because of the circuit complexity, the
power consumption and area are the two important design considerations of the
multiplier. In this paper a low power low area architecture for the shift and
add multiplier is proposed. For getting the low power low area architecture,
the modifications made to the conventional architecture consist of the
reduction in switching activities of the major blocks of the multiplier, which
includes the reduction in switching activity of the adder and counter. This
architecture avoids the shifting of the multiplier register. The simulation
result for 8 bit multipliers shows that the proposed low power architecture
lowers the total power consumption by 35.25% and area by 52.72 % when compared
to the conventional architecture. Also the reduction in power consumption
increases with the increase in bit width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1006.1182</identifier>
 <datestamp>2010-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1006.1182</id><created>2010-06-07</created><authors><author><keyname>Hasan</keyname><forenames>K. M. Azharul</forenames></author><author><keyname>Hasan</keyname><forenames>Mohammad Sabbir</forenames></author></authors><title>A Parsing Scheme for Finding the Design Pattern and Reducing the
  Development Cost of Reusable Object Oriented Software</title><categories>cs.SE</categories><comments>15 pages</comments><journal-ref>International Journal of Computer Science and Information
  Technology 2.3 (2010) 40-54</journal-ref><doi>10.5121/ijcsit.2010.2304</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Because of the importance of object oriented methodologies, the research in
developing new measure for object oriented system development is getting
increased focus. The most of the metrics need to find the interactions between
the objects and modules for developing necessary metric and an influential
software measure that is attracting the software developers, designers and
researchers. In this paper a new interactions are defined for object oriented
system. Using these interactions, a parser is developed to analyze the existing
architecture of the software. Within the design model, it is necessary for
design classes to collaborate with one another. However, collaboration should
be kept to an acceptable minimum i.e. better designing practice will introduce
low coupling. If a design model is highly coupled, the system is difficult to
implement, to test and to maintain overtime. In case of enhancing software, we
need to introduce or remove module and in that case coupling is the most
important factor to be considered because unnecessary coupling may make the
system unstable and may cause reduction in the system's performance. So
coupling is thought to be a desirable goal in software construction, leading to
better values for external software qualities such as maintainability,
reusability and so on. To test this hypothesis, a good measure of class
coupling is needed. In this paper, based on the developed tool called Design
Analyzer we propose a methodology to reuse an existing system with the
objective of enhancing an existing Object oriented system keeping the coupling
as low as possible.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="13000" completeListSize="102538">1122234|14001</resumptionToken>
</ListRecords>
</OAI-PMH>
