<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:49:24Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|79001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03144</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03144</id><created>2015-06-09</created><updated>2015-08-13</updated><authors><author><keyname>Schiebinger</keyname><forenames>Geoffrey</forenames></author><author><keyname>Robeva</keyname><forenames>Elina</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author></authors><title>Superresolution without Separation</title><categories>math.OC cs.IT math.IT</categories><comments>23 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a theoretical analysis of diffraction-limited
superresolution, demonstrating that arbitrarily close point sources can be
resolved in ideal situations. Precisely, we assume that the incoming signal is
a linear combination of M shifted copies of a known waveform with unknown
shifts and amplitudes, and one only observes a finite collection of evaluations
of this signal. We characterize properties of the base waveform such that the
exact translations and amplitudes can be recovered from 2M + 1 observations.
This recovery is achieved by solving a a weighted version of basis pursuit over
a continuous dictionary. Our methods combine classical polynomial interpolation
techniques with contemporary tools from compressed sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03147</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03147</id><created>2015-06-09</created><authors><author><keyname>Gameiro</keyname><forenames>Marcio</forenames></author><author><keyname>Hiraoka</keyname><forenames>Yasuaki</forenames></author><author><keyname>Obayashi</keyname><forenames>Ippei</forenames></author></authors><title>Continuation of Point Clouds via Persistence Diagrams</title><categories>math.NA cs.CG math.AT math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a mathematical and algorithmic framework for the
continuation of point clouds by persistence diagrams. A key property used in
the method is that the persistence map, which assigns a persistence diagram to
a point cloud, is differentiable. This allows us to apply the Newton-Raphson
continuation method in this setting. Given an original point cloud $P$, its
persistence diagram $D$, and a target persistence diagram $D'$, we gradually
move from $D$ to $D'$, by successively computing intermediate point clouds
until we finally find a point cloud $P'$ having $D'$ as its persistence
diagram. Our method can be applied to a wide variety of situations in
topological data analysis where it is necessary to solve an inverse problem,
from persistence diagrams to point cloud data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03152</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03152</id><created>2015-06-09</created><authors><author><keyname>Shi</keyname><forenames>Zhan</forenames></author><author><keyname>Nurdin</keyname><forenames>Hendra I.</forenames></author></authors><title>Entanglement in a linear coherent feedback chain of nondegenerate
  optical parametric amplifiers</title><categories>quant-ph cs.SY</categories><comments>29 pages, 11 figures, to appear in Quantum Information and
  Computation (QIC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with linear quantum networks of $N$ nondegenerate
optical parametric amplifiers (NOPAs), with $N$ up to 6, which are
interconnected in a coherent feedback chain. Each network connects two
communicating parties (Alice and Bob) over two transmission channels. In
previous work we have shown that a dual-NOPA coherent feedback network
generates better Einstein-Podolsky-Rosen (EPR) entanglement (i.e., more
two-mode squeezing) between its two outgoing Gaussian fields than a single
NOPA, when the same total pump power is consumed and the systems undergo the
same transmission losses over the same distance. This paper aims to analyze
stability, EPR entanglement between two outgoing fields of interest, and
bipartite entanglement of two-mode Gaussian states of cavity modes of the
$N$-NOPA networks under the effect of transmission and amplification losses, as
well as time delays. It is numerically shown that, in the absence of losses and
delays, the network with more NOPAs in the chain requires less total pump power
to generate the same degree of EPR entanglement. Moreover, we report on the
internal entanglement synchronization that occurs in the steady state between
certain pairs of Gaussian oscillator modes inside the NOPA cavities of the
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03159</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03159</id><created>2015-06-10</created><updated>2015-10-31</updated><authors><author><keyname>Tran</keyname><forenames>Dustin</forenames></author><author><keyname>Blei</keyname><forenames>David M.</forenames></author><author><keyname>Airoldi</keyname><forenames>Edoardo M.</forenames></author></authors><title>Copula variational inference</title><categories>stat.ML cs.LG stat.CO stat.ME</categories><comments>Appears in Neural Information Processing Systems, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a general variational inference method that preserves dependency
among the latent variables. Our method uses copulas to augment the families of
distributions used in mean-field and structured approximations. Copulas model
the dependency that is not captured by the original variational distribution,
and thus the augmented variational family guarantees better approximations to
the posterior. With stochastic optimization, inference on the augmented
distribution is scalable. Furthermore, our strategy is generic: it can be
applied to any inference procedure that currently uses the mean-field or
structured approach. Copula variational inference has many advantages: it
reduces bias; it is less sensitive to local optima; it is less sensitive to
hyperparameters; and it helps characterize and interpret the dependency among
the latent variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03160</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03160</id><created>2015-06-10</created><authors><author><keyname>Lee</keyname><forenames>Donghyuk</forenames></author><author><keyname>Pekhimenko</keyname><forenames>Gennady</forenames></author><author><keyname>Khan</keyname><forenames>Samira</forenames></author><author><keyname>Ghose</keyname><forenames>Saugata</forenames></author><author><keyname>Mutlu</keyname><forenames>Onur</forenames></author></authors><title>Simultaneous Multi Layer Access: A High Bandwidth and Low Cost
  3D-Stacked Memory Interface</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Limited memory bandwidth is a critical bottleneck in modern systems.
3D-stacked DRAM enables higher bandwidth by leveraging wider
Through-Silicon-Via (TSV) channels, but today's systems cannot fully exploit
them due to the limited internal bandwidth of DRAM. DRAM reads a whole row
simultaneously from the cell array to a row buffer, but can transfer only a
fraction of the data from the row buffer to peripheral IO circuit, through a
limited and expensive set of wires referred to as global bitlines. In presence
of wider memory channels, the major bottleneck becomes the limited data
transfer capacity through these global bitlines. Our goal in this work is to
enable higher bandwidth in 3D-stacked DRAM without the increased cost of adding
more global bitlines. We instead exploit otherwise-idle resources, such as
global bitlines, already existing within the multiple DRAM layers by accessing
the layers simultaneously. Our architecture, Simultaneous Multi Layer Access
(SMLA), provides higher bandwidth by aggregating the internal bandwidth of
multiple layers and transferring the available data at a higher IO frequency.
  To implement SMLA, simultaneous data transfer from multiple layers through
the same IO TSVs requires coordination between layers to avoid channel
conflict. We first study coordination by static partitioning, which we call
Dedicated-IO, that assigns groups of TSVs to each layer. We then provide a
simple, yet sophisticated mechanism, called Cascaded-IO, which enables
simultaneous access to each layer by time-multiplexing the IOs. By operating at
a frequency proportional to the number of layers, SMLA provides a higher
bandwidth (4X for a four-layer stacked DRAM). Our evaluations show that SMLA
provides significant performance improvement and energy reduction (55%/18% on
average for multi-programmed workloads, respectively) over a baseline
3D-stacked DRAM with very low area overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03163</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03163</id><created>2015-06-10</created><updated>2015-06-21</updated><authors><author><keyname>Naidan</keyname><forenames>Bilegsaikhan</forenames></author><author><keyname>Boytsov</keyname><forenames>Leonid</forenames></author><author><keyname>Nyberg</keyname><forenames>Eric</forenames></author></authors><title>Permutation Search Methods are Efficient, Yet Faster Search is Possible</title><categories>cs.LG cs.DB cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey permutation-based methods for approximate k-nearest neighbor
search. In these methods, every data point is represented by a ranked list of
pivots sorted by the distance to this point. Such ranked lists are called
permutations. The underpinning assumption is that, for both metric and
non-metric spaces, the distance between permutations is a good proxy for the
distance between original points. Thus, it should be possible to efficiently
retrieve most true nearest neighbors by examining only a tiny subset of data
points whose permutations are similar to the permutation of a query. We further
test this assumption by carrying out an extensive experimental evaluation where
permutation methods are pitted against state-of-the art benchmarks (the
multi-probe LSH, the VP-tree, and proximity-graph based retrieval) on a variety
of realistically large data set from the image and textual domain. The focus is
on the high-accuracy retrieval methods for generic spaces. Additionally, we
assume that both data and indices are stored in main memory. We find
permutation methods to be reasonably efficient and describe a setup where these
methods are most useful. To ease reproducibility, we make our software and data
sets publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03167</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03167</id><created>2015-06-10</created><updated>2016-01-25</updated><authors><author><keyname>Kindler</keyname><forenames>Guy</forenames></author><author><keyname>O'Donnell</keyname><forenames>Ryan</forenames></author><author><keyname>Witmer</keyname><forenames>David</forenames></author></authors><title>Remarks on the Most Informative Function Conjecture at fixed mean</title><categories>cs.IT cs.CC math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2013, Courtade and Kumar posed the following problem: Let $\boldsymbol{x}
\sim \{\pm 1\}^n$ be uniformly random, and form $\boldsymbol{y} \sim \{\pm
1\}^n$ by negating each bit of $\boldsymbol{x}$ independently with probability
$\alpha$. Is it true that the mutual information $I(f(\boldsymbol{x})
\mathbin{;} \boldsymbol{y})$ is maximized among $f:\{\pm 1\}^n \to \{\pm 1\}$
by $f(x) = x_1$? We do not resolve this problem. Instead, we make a couple of
observations about the fixed-mean version of the conjecture. We show that
Courtade and Kumar's stronger Lex Conjecture fails for small noise rates. We
also prove a continuous version of the conjecture on the sphere and show that
it implies the previously-known analogue for Gaussian space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03171</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03171</id><created>2015-06-10</created><updated>2016-02-01</updated><authors><author><keyname>Yasunaga</keyname><forenames>Kenji</forenames></author></authors><title>Correction of Samplable Additive Errors</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the correctability of efficiently samplable errors. Specifically, we
consider the setting in which errors are efficiently samplable without the
knowledge of the code or the transmitted codeword, and the error rate is not
bounded. It is shown that for every flat distribution $Z$ over $\{0,1\}^n$ with
support of size $2^m$, there is a code that corrects $Z$ with optimal rate $1 -
m/n$ and the decoding complexity $O(n^32^m)$. If the support of $Z$ forms a
linear subspace, there is a linear code that corrects $Z$ with rate $1 - m/n$
by polynomial-time decoding. We show that there is an oracle relative to which
there is a samplable flat distribution $Z$ that is not pseudorandom, but
uncorrectable by polynomial-time coding schemes of rate $1 - m/n - \omega(\log
n/n)$. The result implies the difficulty of correcting every samplable
distribution $Z$ even when $Z$ is not pseudorandom. Finally, we show that the
existence of one-way functions is necessary to derive impossibility results for
coding schemes of rate less than $1 - m/n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03172</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03172</id><created>2015-06-10</created><authors><author><keyname>Gopalkrishnan</keyname><forenames>Manoj</forenames></author></authors><title>A Scheme for Molecular Computation of Maximum Likelihood Estimators for
  Log-Linear Models</title><categories>cs.NE math.ST q-bio.MN stat.TH</categories><comments>12 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a scheme for computing Maximum Likelihood Estimators for
Log-Linear models using reaction networks, and prove its correctness. Our
scheme exploits the toric structure of equilibrium points of reaction networks.
This allows an efficient encoding of the problem, and reveals how reaction
networks are naturally suited to statistical inference tasks. Our scheme is
relevant to molecular programming, an emerging discipline that views molecular
interactions as computational primitives for the synthesis of sophisticated
behaviors. In addition, such a scheme may provide a template to understand how
biochemical signaling pathways integrate extensive information about their
environment and history.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03181</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03181</id><created>2015-06-10</created><updated>2015-08-31</updated><authors><author><keyname>Haque</keyname><forenames>Mohammad Shihabul</forenames></author><author><keyname>Peddersen</keyname><forenames>Jorgen</forenames></author><author><keyname>Janapsatya</keyname><forenames>Andhi</forenames></author><author><keyname>Parameswaran</keyname><forenames>Sri</forenames></author></authors><title>DEW: A Fast Level 1 Cache Simulation Approach for Embedded Processors
  with FIFO Replacement Policy</title><categories>cs.AR</categories><doi>10.1109/DATE.2010.5457153</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing the speed of cache simulation to obtain hit/miss rates en- ables
performance estimation, cache exploration for embedded sys- tems and energy
estimation. Previously, such simulations, particu- larly exact approaches, have
been exclusively for caches which uti- lize the least recently used (LRU)
replacement policy. In this paper, we propose a new, fast and exact cache
simulation method for the First In First Out(FIFO) replacement policy. This
method, called DEW, is able to simulate multiple level 1 cache configurations
(dif- ferent set sizes, associativities, and block sizes) with FIFO replace-
ment policy. DEW utilizes a binomial tree based representation of cache
configurations and a novel searching method to speed up sim- ulation over
single cache simulators like Dinero IV. Depending on different cache block
sizes and benchmark applications, DEW oper- ates around 8 to 40 times faster
than Dinero IV. Dinero IV compares 2.17 to 19.42 times more cache ways than DEW
to determine accu- rate miss rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03182</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03182</id><created>2015-06-10</created><updated>2015-08-31</updated><authors><author><keyname>Haque</keyname><forenames>Mohammad Shihabul</forenames></author><author><keyname>Kumar</keyname><forenames>Akash</forenames></author><author><keyname>Ha</keyname><forenames>Yajun</forenames></author><author><keyname>Wu</keyname><forenames>Qiang</forenames></author><author><keyname>Luo</keyname><forenames>Shaobo</forenames></author></authors><title>TRISHUL: A Single-pass Optimal Two-level Inclusive Data Cache Hierarchy
  Selection Process for Real-time MPSoCs</title><categories>cs.AR</categories><doi>10.1109/ASPDAC.2013.6509615</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hitherto discovered approaches analyze the execution time of a real time
application on all the possible cache hierarchy setups to find the application
specific optimal two level inclusive data cache hierarchy to reduce cost, space
and energy consumption while satisfying the time deadline in real time
Multiprocessor Systems on Chip. These brute force like approaches can take
years to complete. Alternatively, memory access trace driven crude estimation
methods can find a cache hierarchy quickly by compromising the accuracy of
results. In this article, for the first time, we propose a fast and accurate
trace driven approach to find the optimal real time application specific two
level inclusive data cache hierarchy. Our proposed approach TRISHUL predicts
the optimal cache hierarchy performance first and then utilizes that
information to find the optimal cache hierarchy quickly. TRISHUL can suggest a
cache hierarchy, which has up to 128 times smaller size, up to 7 times faster
compared to the suggestion of the state of the art crude trace driven two level
inclusive cache hierarchy selection approach for the application traces
analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03184</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03184</id><created>2015-06-10</created><authors><author><keyname>Zhou</keyname><forenames>Xinyu</forenames></author><author><keyname>Zhou</keyname><forenames>Shuchang</forenames></author><author><keyname>Yao</keyname><forenames>Cong</forenames></author><author><keyname>Cao</keyname><forenames>Zhimin</forenames></author><author><keyname>Yin</keyname><forenames>Qi</forenames></author></authors><title>ICDAR 2015 Text Reading in the Wild Competition</title><categories>cs.CV</categories><comments>3 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, text detection and recognition in natural scenes are becoming
increasing popular in the computer vision community as well as the document
analysis community. However, majority of the existing ideas, algorithms and
systems are specifically designed for English. This technical report presents
the final results of the ICDAR 2015 Text Reading in the Wild (TRW 2015)
competition, which aims at establishing a benchmark for assessing detection and
recognition algorithms devised for both Chinese and English scripts and
providing a playground for researchers from the community. In this article, we
describe in detail the dataset, tasks, evaluation protocols and participants of
this competition, and report the performance of the participating methods.
Moreover, promising directions for future research are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03186</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03186</id><created>2015-06-10</created><updated>2015-08-31</updated><authors><author><keyname>Haque</keyname><forenames>Mohammad Shihabul</forenames></author><author><keyname>Peddersen</keyname><forenames>Jorgen</forenames></author><author><keyname>Parameswaran</keyname><forenames>Sri</forenames></author></authors><title>CIPARSim: Cache Intersection Property Assisted Rapid Single-pass FIFO
  Cache Simulation Technique</title><categories>cs.AR</categories><doi>10.1109/ICCAD.2011.6105316</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, for the first time, we introduce a cache property called the
Intersection Property that helps to reduce singlepass simulation time in a
manner similar to inclusion property. An intersection property defines
conditions that if met, prove a particular element exists in larger caches,
thus avoiding further search time. We have discussed three such intersection
properties for caches using the FIFO replacement policy in this paper. A rapid
singlepass FIFO cache simulator CIPARSim has also been proposed. CIPARSim is
the first singlepass simulator dependent on the FIFO cache properties to reduce
simulation time significantly. CIPARSim simulation time was up to 5 times
faster compared to the state of the art singlepass FIFO cache simulator for the
cache configurations tested. CIPARSim produces the cache hit and miss rates of
an application accurately on various cache configurations. During simulation,
CIPARSim intersection properties alone predict up to 90% of the total hits,
reducing simulationtime immensely
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03193</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03193</id><created>2015-06-10</created><updated>2015-08-31</updated><authors><author><keyname>Haque</keyname><forenames>Mohammad Shihabul</forenames></author><author><keyname>Li</keyname><forenames>Ang</forenames></author><author><keyname>Kumar</keyname><forenames>Akash</forenames></author><author><keyname>Wei</keyname><forenames>Qingsong</forenames></author></authors><title>Accelerating Non-volatile/Hybrid Processor Cache Design Space
  Exploration for Application Specific Embedded Systems</title><categories>cs.AR</categories><doi>10.1109/ASPDAC.2015.7059045</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we propose a technique to accelerate nonvolatile or hybrid
of volatile and nonvolatile processor cache design space exploration for
application specific embedded systems. Utilizing a novel cache behavior
modeling equation and a new accurate cache miss prediction mechanism, our
proposed technique can accelerate NVM or hybrid FIFO processor cache design
space exploration for SPEC CPU 2000 applications up to 249 times compared to
the conventional approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03214</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03214</id><created>2015-06-10</created><authors><author><keyname>Luciano</keyname><forenames>Patrick</forenames></author><author><keyname>Rebai</keyname><forenames>Ismail</forenames></author><author><keyname>Lemaire</keyname><forenames>Vincent</forenames></author></authors><title>Increasing loyalty using predictive modeling in Business-to-Business
  Telecommunication</title><categories>cs.OH cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Customer Relationship Management (CRM) is a key element of modern marketing
strategies. One of the most practical way to build useful knowledge on
customers in a CRM system to produce scores to forecast churn behavior,
propensity to subscribe to a new service... In AMEA zone (Asia, Middle East and
Africa zone), the context of fierce competition may represent a higher
percentage, and particularly in B2B market. But by contrast, to our knowledge,
no scientific papers were dedicated and published to detail the way to improve
loyalty in B2B Telco market. If we can assume at low segments similarities
between B2B and B2C, some research is required in order to model B2B user
behavior versus B2C behavior. This problematic stands actual as &quot;Bring Your own
Device&quot; (BYOD) becomes more and more trendy. Answering business requirements,
our team applied some B2C predictive tools with adapting them to B2B in an AMEA
country Orange affiliate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03227</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03227</id><created>2015-06-10</created><updated>2016-02-11</updated><authors><author><keyname>Guerrini</keyname><forenames>Eleonora</forenames></author><author><keyname>Meneghetti</keyname><forenames>Alessio</forenames></author><author><keyname>Sala</keyname><forenames>Massimiliano</forenames></author></authors><title>On optimal nonlinear systematic codes</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1502.07379</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most bounds on the size of codes hold for any code, whether linear or not.
Notably, the Griesmer bound holds only in the linear case and so optimal linear
codes are not necessarily optimal codes. In this paper we identify code
parameters $(q,d,k)$, namely field size, minimum distance and dimension, for
which the Griesmer bound holds also in the (systematic) nonlinear case.
Moreover, we show that the Griesmer bound does not necessarily hold for a
systematic code by explicit construction of a family of optimal systematic
binary codes. On the other hand, we are able to provide some versions of the
Griesmer bound holding for all systematic codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03229</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03229</id><created>2015-06-10</created><updated>2015-06-22</updated><authors><author><keyname>Golosio</keyname><forenames>Bruno</forenames></author><author><keyname>Cangelosi</keyname><forenames>Angelo</forenames></author><author><keyname>Gamotina</keyname><forenames>Olesya</forenames></author><author><keyname>Masala</keyname><forenames>Giovanni Luca</forenames></author></authors><title>A cognitive neural architecture able to learn and communicate through
  natural language</title><categories>cs.CL</categories><comments>The source code of the software, the User Guide and the datasets used
  for its validation are available in the ANNABELL web site at
  https://github.com/golosio/annabell/wiki</comments><journal-ref>PLoS ONE 10 (2015) e0140866</journal-ref><doi>10.1371/journal.pone.0140866</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communicative interactions involve a kind of procedural knowledge that is
used by the human brain for processing verbal and nonverbal inputs and for
language production. Although considerable work has been done on modeling human
language abilities, it has been difficult to bring them together to a
comprehensive tabula rasa system compatible with current knowledge of how
verbal information is processed in the brain. This work presents a cognitive
system, entirely based on a large-scale neural architecture, which was
developed to shed light on the procedural knowledge involved in language
elaboration. The main component of this system is the central executive, which
is a supervising system that coordinates the other components of the working
memory. In our model, the central executive is a neural network that takes as
input the neural activation states of the short-term memory and yields as
output mental actions, which control the flow of information among the working
memory components through neural gating mechanisms. The proposed system is
capable of learning to communicate through natural language starting from
tabula rasa, without any a priori knowledge of the structure of phrases,
meaning of words, role of the different classes of words, only by interacting
with a human through a text-based interface, using an open-ended incremental
learning process. It is able to learn nouns, verbs, adjectives, pronouns and
other word classes, and to use them in expressive language. The model was
validated on a corpus of 1587 input sentences, based on literature on early
language assessment, at the level of about 4-years old child, and produced 521
output sentences, expressing a broad range of language processing
functionalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03236</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03236</id><created>2015-06-10</created><authors><author><keyname>Wang</keyname><forenames>Ligong</forenames></author><author><keyname>Wornell</keyname><forenames>Gregory</forenames></author><author><keyname>Zheng</keyname><forenames>Lizhong</forenames></author></authors><title>Fundamental Limits of Communication with Low Probability of Detection</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory. Full version of
  a paper to be presented at ISIT 2015 in Hong Kong</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of communication over a discrete memoryless
channel (DMC) or an additive white Gaussian noise (AWGN) channel subject to the
constraint that the probability that an adversary who observes the channel
outputs can detect the communication is low. Specifically, the relative entropy
between the output distributions when a codeword is transmitted and when no
input is provided to the channel must be sufficiently small. For a DMC whose
output distribution induced by the &quot;off&quot; input symbol is not a mixture of the
output distributions induced by other input symbols, it is shown that the
maximum amount of information that can be transmitted under this criterion
scales like the square root of the blocklength. The same is true for the AWGN
channel. Exact expressions for the scaling constant are also derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03237</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03237</id><created>2015-06-10</created><authors><author><keyname>Olaniyi</keyname><forenames>Mikail</forenames></author><author><keyname>Omotosho</keyname><forenames>Adebayo</forenames></author><author><keyname>Oluwatosin</keyname><forenames>Esther</forenames></author><author><keyname>Adegoke</keyname><forenames>Michael</forenames></author><author><keyname>Akinmukomi</keyname><forenames>Tomi</forenames></author></authors><title>Students Exeat Monitoring System Using Fingerprint Biometric
  Authentication and Mobile Short Message Service</title><categories>cs.CY</categories><comments>The Don Bosco International Journal of ICT and Youth Development 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exeat is a generic term commonly used to describe a period of absence from a
centre of learning either for entire day, or parts of a day for appointments,
interviews, open days and other fixtures in privately owned academic
environment. The current method of monitoring students movement is inefficient
and brings difficulty to the University Halls management checking students exit
or entry into the halls of residence as well as impersonation. By using nexus
combination of Ubiquitous Mobile Computing Technology through Mobile Short
Message Service and biometric fingerprint approach exeat management and
monitoring is quick and easy. Result after testing of the designed and
simulated system shows that exeat monitoring systems is less prone to forgery
as stakeholders are carried along, capable of preventing impersonation among
students, and provide absolute electronic compliance to the policy of issuing
exeat to students in the University Halls of Residence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03250</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03250</id><created>2015-06-10</created><authors><author><keyname>Pang</keyname><forenames>Jun</forenames><affiliation>University of Luxembourg</affiliation></author><author><keyname>Liu</keyname><forenames>Yang</forenames><affiliation>Nanyang Technological University</affiliation></author><author><keyname>Mauw</keyname><forenames>Sjouke</forenames><affiliation>University of Luxembourg</affiliation></author></authors><title>Proceedings 4th International Workshop on Engineering Safety and
  Security Systems</title><categories>cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 184, 2015</journal-ref><doi>10.4204/EPTCS.184</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present volume contains the proceedings of the Fourth International
Workshop on Engineering Safety and Security Systems (ESSS'15). The workshop was
held in Oslo, Norway, on June 22nd, 2015, as a satellite event of the 20th
International Symposium on Formal Methods (FM'15).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03254</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03254</id><created>2015-06-10</created><authors><author><keyname>Steinbring</keyname><forenames>Jannik</forenames></author><author><keyname>Pander</keyname><forenames>Martin</forenames></author><author><keyname>Hanebeck</keyname><forenames>Uwe D.</forenames></author></authors><title>The Smart Sampling Kalman Filter with Symmetric Samples</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonlinear Kalman Filters are powerful and widely-used techniques when trying
to estimate the hidden state of a stochastic nonlinear dynamic system. In this
paper, we extend the Smart Sampling Kalman Filter (S2KF) with a new point
symmetric Gaussian sampling scheme. This not only improves the S2KF's
estimation quality, but also reduces the time needed to compute the required
optimal Gaussian samples drastically. Moreover, we improve the numerical
stability of the sample computation, which allows us to accurately approximate
a thousand-dimensional Gaussian distribution using tens of thousands of
optimally placed samples. We evaluate the new symmetric S2KF by computing
higher-order moments of standard normal distributions and investigate the
estimation quality of the S2KF when dealing with symmetric measurement
equations. Finally, extended object tracking based on many measurements per
time step is considered. This high-dimensional estimation problem shows the
advantage of the S2KF being able to use an arbitrary number of samples
independent of the state dimension, in contrast to other state-of-the-art
sample-based Kalman Filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03257</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03257</id><created>2015-06-10</created><authors><author><keyname>Navarro-Colorado</keyname><forenames>Borja</forenames></author><author><keyname>Saquete</keyname><forenames>Estela</forenames></author></authors><title>Combining Temporal Information and Topic Modeling for Cross-Document
  Event Ordering</title><categories>cs.CL</categories><comments>5 pages</comments><journal-ref>Proceedings of the 9th International Workshop on Semantic
  Evaluation (SemEval 2015) http://www.aclweb.org/anthology/S/S15/</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building unified timelines from a collection of written news articles
requires cross-document event coreference resolution and temporal relation
extraction. In this paper we present an approach event coreference resolution
according to: a) similar temporal information, and b) similar semantic
arguments. Temporal information is detected using an automatic temporal
information system (TIPSem), while semantic information is represented by means
of LDA Topic Modeling. The evaluation of our approach shows that it obtains the
highest Micro-average F-score results in the SemEval2015 Task 4: TimeLine:
Cross-Document Event Ordering (25.36\% for TrackB, 23.15\% for SubtrackB), with
an improvement of up to 6\% in comparison to the other systems. However, our
experiment also showed some draw-backs in the Topic Modeling approach that
degrades performance of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03262</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03262</id><created>2015-06-10</created><authors><author><keyname>Boucher</keyname><forenames>Christina</forenames></author><author><keyname>Bowe</keyname><forenames>Alexander</forenames></author><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Manzini</keyname><forenames>Giovanni</forenames></author><author><keyname>Sir&#xe9;n</keyname><forenames>Jouni</forenames></author></authors><title>Relative Select</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the problem of storing coloured de Bruijn graphs, we show how,
if we can already support fast select queries on one string, then we can store
a little extra information and support fairly fast select queries on a similar
string.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03264</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03264</id><created>2015-06-10</created><authors><author><keyname>Indiveri</keyname><forenames>Giacomo</forenames></author><author><keyname>Liu</keyname><forenames>Shih-Chii</forenames></author></authors><title>Memory and information processing in neuromorphic systems</title><categories>cs.NE</categories><comments>Submitted to Proceedings of IEEE, review of recently proposed
  neuromorphic computing platforms and systems</comments><doi>10.1109/JPROC.2015.2444094</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A striking difference between brain-inspired neuromorphic processors and
current von Neumann processors architectures is the way in which memory and
processing is organized. As Information and Communication Technologies continue
to address the need for increased computational power through the increase of
cores within a digital processor, neuromorphic engineers and scientists can
complement this need by building processor architectures where memory is
distributed with the processing. In this paper we present a survey of
brain-inspired processor architectures that support models of cortical networks
and deep neural networks. These architectures range from serial clocked
implementations of multi-neuron systems to massively parallel asynchronous ones
and from purely digital systems to mixed analog/digital systems which implement
more biological-like models of neurons and synapses together with a suite of
adaptation and learning mechanisms analogous to the ones found in biological
nervous systems. We describe the advantages of the different approaches being
pursued and present the challenges that need to be addressed for building
artificial neural processing systems that can display the richness of behaviors
seen in biological systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03265</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03265</id><created>2015-06-10</created><updated>2015-11-09</updated><authors><author><keyname>Ceccarello</keyname><forenames>Matteo</forenames></author><author><keyname>Pietracaprina</keyname><forenames>Andrea</forenames></author><author><keyname>Pucci</keyname><forenames>Geppino</forenames></author><author><keyname>Upfal</keyname><forenames>Eli</forenames></author></authors><title>A Practical Parallel Algorithm for Diameter Approximation of Massive
  Weighted Graphs</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a space and time efficient practical parallel algorithm for
approximating the diameter of massive weighted undirected graphs on distributed
platforms supporting a MapReduce-like abstraction. The core of the algorithm is
a weighted graph decomposition strategy generating disjoint clusters of bounded
weighted radius. Theoretically, our algorithm uses linear space and yields a
polylogarithmic approximation guarantee; moreover, for important practical
classes of graphs, it runs in a number of rounds asymptotically smaller than
those required by the natural approximation provided by the state-of-the-art
$\Delta$-stepping SSSP algorithm, which is its only practical linear-space
competitor in the aforementioned computational scenario. We complement our
theoretical findings with an extensive experimental analysis on large benchmark
graphs, which demonstrates that our algorithm attains substantial improvements
on a number of key performance indicators with respect to the aforementioned
competitor, while featuring a similar approximation ratio (a small constant
less than 1.4, as opposed to the polylogarithmic theoretical bound).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03266</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03266</id><created>2015-06-10</created><authors><author><keyname>Gabbay</keyname><forenames>Dov</forenames></author><author><keyname>Gabbay</keyname><forenames>Michael</forenames></author></authors><title>The Attack as Strong Negation, Part I</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We add strong negation $N$ to classical logic and interpret the attack
relation of &quot;$x$ attacks $y$&quot; in argumentation as $(x\to Ny)$. We write a
corresponding object level (using $N$ only) classical theory for each
argumentation network and show that the classical models of this theory
correspond exactly to the complete extensions of the argumentation network. We
show by example how this approach simplifies the study of abstract
argumentation networks. We compare with other translations of abstract
argumentation networks into logic, such as classical predicate logic or modal
logics, or logic programming, and we also compare with Abstract Dialectical
Frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03271</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03271</id><created>2015-06-10</created><updated>2015-11-03</updated><authors><author><keyname>Neu</keyname><forenames>Gergely</forenames></author></authors><title>Explore no more: Improved high-probability regret bounds for
  non-stochastic bandits</title><categories>cs.LG stat.ML</categories><comments>To appear at NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work addresses the problem of regret minimization in non-stochastic
multi-armed bandit problems, focusing on performance guarantees that hold with
high probability. Such results are rather scarce in the literature since
proving them requires a large deal of technical effort and significant
modifications to the standard, more intuitive algorithms that come only with
guarantees that hold on expectation. One of these modifications is forcing the
learner to sample arms from the uniform distribution at least
$\Omega(\sqrt{T})$ times over $T$ rounds, which can adversely affect
performance if many of the arms are suboptimal. While it is widely conjectured
that this property is essential for proving high-probability regret bounds, we
show in this paper that it is possible to achieve such strong results without
this undesirable exploration component. Our result relies on a simple and
intuitive loss-estimation strategy called Implicit eXploration (IX) that allows
a remarkably clean analysis. To demonstrate the flexibility of our technique,
we derive several improved high-probability bounds for various extensions of
the standard multi-armed bandit framework. Finally, we conduct a simple
experiment that illustrates the robustness of our implicit exploration
technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03282</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03282</id><created>2015-06-10</created><updated>2015-10-30</updated><authors><author><keyname>Bougeret</keyname><forenames>Marin</forenames></author><author><keyname>Duvilli&#xe9;</keyname><forenames>Guillerme</forenames></author><author><keyname>Giroudeau</keyname><forenames>Rodolphe</forenames></author><author><keyname>Watrigant</keyname><forenames>R&#xe9;mi</forenames></author></authors><title>Multidimensional Binary Vector Assignment problem: standard, structural
  and above guarantee parameterizations</title><categories>cs.DS</categories><comments>17 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we focus on the parameterized complexity of the
Multidimensional Binary Vector Assignment problem (called \BVA). An input of
this problem is defined by $m$ disjoint sets $V^1, V^2, \dots, V^m$, each
composed of $n$ binary vectors of size $p$. An output is a set of $n$ disjoint
$m$-tuples of vectors, where each $m$-tuple is obtained by picking one vector
from each set $V^i$. To each $m$-tuple we associate a $p$ dimensional vector by
applying the bit-wise AND operation on the $m$ vectors of the tuple. The
objective is to minimize the total number of zeros in these $n$ vectors. mBVA
can be seen as a variant of multidimensional matching where hyperedges are
implicitly locally encoded via labels attached to vertices, but was originally
introduced in the context of integrated circuit manufacturing.
  We provide for this problem FPT algorithms and negative results ($ETH$-based
results, $W$[2]-hardness and a kernel lower bound) according to several
parameters: the standard parameter $k$ (\textit{i.e.} the total number of
zeros), as well as two parameters above some guaranteed values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03284</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03284</id><created>2015-06-10</created><updated>2015-11-02</updated><authors><author><keyname>Zheng</keyname><forenames>Jun</forenames></author><author><keyname>Zhu</keyname><forenames>Guchuan</forenames></author></authors><title>In-domain control of a heat equation: an approach combining
  zero-dynamics inverse and differential flatness</title><categories>math.OC cs.SY</categories><comments>Preprint of an original research paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the set-point control problem of a heat equation with
in-domain actuation. The proposed scheme is based on the framework of zero
dynamics inverse combined with flat system control. Moreover, the set-point
control is cast into a motion planing problem of a multiple-input, multiple-out
system, which is solved by a Green's function-based reference trajectory
decomposition. The validity of the proposed method is assessed through
convergence and solvability analysis of the control algorithm. The performance
of the developed control scheme and the viability of the proposed approach are
confirmed by numerical simulation of a representative system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03289</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03289</id><created>2015-06-10</created><authors><author><keyname>de Brito</keyname><forenames>J. B.</forenames></author><author><keyname>Filho</keyname><forenames>C. I. N. Sampaio</forenames></author><author><keyname>Moreira</keyname><forenames>A. A.</forenames></author><author><keyname>Andrade</keyname><forenames>J. S.</forenames><suffix>Jr</suffix></author></authors><title>Characterizing the intrinsic correlations of scale-free networks</title><categories>physics.soc-ph cs.SI</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Very often, when studying topological or dynamical properties of random
scale-free networks, it is tacitly assumed that degree-degree correlations are
not present. However, simple constraints, such as the absence of multiple edges
and self-loops, can give rise to intrinsic correlations in these structures. In
the same way that Fermionic correlations in thermodynamic systems are relevant
only in the limit of low temperature, the intrinsic correlations in scale-free
networks are relevant only when the extreme values for the degrees grow faster
than the square-root of the network size. In this situation, these correlations
can significantly affect the dependence of the average degree of the nearest
neighbors of a given vertice on this vertices's degree. Here, we introduce an
analytical approach that is capable to predict the functional form of this
property. Moreover, our results indicate that random scale-free networks models
are not self-averaging, that is, the second moment of their degree distribution
may vary orders of magnitude among different realizations. Finally, we argue
that the intrinsic correlations investigated here may have profound impact on
the critical properties of random scale-free networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03301</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03301</id><created>2015-06-10</created><authors><author><keyname>Galun</keyname><forenames>Meirav</forenames></author><author><keyname>Amir</keyname><forenames>Tal</forenames></author><author><keyname>Hassner</keyname><forenames>Tal</forenames></author><author><keyname>Basri</keyname><forenames>Ronen</forenames></author><author><keyname>Lipman</keyname><forenames>Yaron</forenames></author></authors><title>Wide baseline stereo matching with convex bounded-distortion constraints</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding correspondences in wide baseline setups is a challenging problem.
Existing approaches have focused largely on developing better feature
descriptors for correspondence and on accurate recovery of epipolar line
constraints. This paper focuses on the challenging problem of finding
correspondences once approximate epipolar constraints are given. We introduce a
novel method that integrates a deformation model. Specifically, we formulate
the problem as finding the largest number of corresponding points related by a
bounded distortion map that obeys the given epipolar constraints. We show that,
while the set of bounded distortion maps is not convex, the subset of maps that
obey the epipolar line constraints is convex, allowing us to introduce an
efficient algorithm for matching. We further utilize a robust cost function for
matching and employ majorization-minimization for its optimization. Our
experiments indicate that our method finds significantly more accurate maps
than existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03311</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03311</id><created>2015-06-10</created><authors><author><keyname>Avrachenkov</keyname><forenames>Konstantin</forenames><affiliation>MAESTRO</affiliation></author><author><keyname>Singh</keyname><forenames>Vikas Vikram</forenames><affiliation>MAESTRO</affiliation></author></authors><title>Stochastic Coalitional Better-response Dynamics and Strong Nash
  Equilibrium</title><categories>cs.GT cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider coalition formation among players in an n-player finite strategic
game over infinite horizon. At each time a randomly formed coalition makes a
joint deviation from a current action profile such that at new action profile
all players from the coalition are strictly benefited. Such deviations define a
coalitional better-response (CBR) dynamics that is in general stochastic. The
CBR dynamics either converges to a strong Nash equilibrium or stucks in a
closed cycle. We also assume that at each time a selected coalition makes
mistake in deviation with small probability that add mutations (perturbations)
into CBR dynamics. We prove that all strong Nash equilibria and closed cycles
are stochastically stable, i.e., they are selected by perturbed CBR dynamics as
mutations vanish. Similar statement holds for strict strong Nash equilibrium.
We apply CBR dynamics to the network formation games and we prove that all
strongly stable networks and closed cycles are stochastically stable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03319</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03319</id><created>2015-06-10</created><updated>2015-07-16</updated><authors><author><keyname>Nam</keyname><forenames>Junyoung</forenames></author></authors><title>Capacity Bounds for the $K$-User Gaussian Interference Channel</title><categories>cs.IT math.IT</categories><comments>Presented in part at ISIT 2015 and submitted to IEEE Transactions on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity region of the $K$-user Gaussian interference channel (GIC) is a
long-standing open problem and even capacity outer bounds are little known in
general. A significant progress on degrees-of-freedom (DoF) analysis, a
first-order capacity approximation, for the $K$-user GIC has provided new
important insights into the problem of interest in the high signal-to-noise
ratio (SNR) limit. However, such capacity approximation has been observed to
have some limitations in predicting the capacity at finite SNR. In this work,
we develop a new upper-bounding technique that utilizes the so-called
change-of-interference approach in conjunction with the well-known genie-aided
approach. Based on this technique, we derive new upper bounds on the sum
capacity of the three-user GIC with constant, complex channel coefficients and
then generalize to the $K$-user case to provide a more accurate and concrete
perspective on sum-rate behavior at finite SNR. We also provide closed-form
expressions of our upper bounds on the capacity of the $K$-user symmetric GIC
for any $K$. In light of our results, some sum-rate behavior at finite SNR is
in line with the insights given by the existing DoF results, while some others
are not. In particular, the well-known $K/2$ DoF achievable for almost all
constant real channel coefficients turns out to be not embodied in a
considerable performance gain over a certain range of the cross-channel
coefficient in the $K$-user symmetric real case, no matter how large $K$ is.
Moreover, the impact of phase offset between the direct-channel coefficient and
the cross-channel coefficients on the capacity is investigated for the
three-user complex GIC. As a consequence, we provide new findings that could
not be predicted by the prior works on DoF of GICs. Finally, outer bounds on
the capacity region of the $K$-user GIC are obtained by extending the proposed
upper bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03324</identifier>
 <datestamp>2015-07-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03324</id><created>2015-06-10</created><updated>2015-07-16</updated><authors><author><keyname>Nam</keyname><forenames>Junyoung</forenames></author></authors><title>New Outer Bounds on the Capacity of the Two-User Gaussian Interference
  Channel</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The best outer bound on the capacity region of the two-user Gaussian
interference channel (GIC) is known to be the intersection of regions of
various bounds including genie-aided outer bounds, in which a genie provides
noisy input signals to the intended receiver. In this paper, we propose a new
upper-bounding technique that utilizes noisy observation of interfering signals
as auxiliary random variables. In order to derive new capacity bounds, we also
introduce a conditional version of the worst additive noise lemma. The
resulting outer bounds are shown to be tighter than the existing bounds for a
certain range of channel parameters in the weak interference regime. We show
that the rate gap between the time division lower bound (or inner bound) and
the upper bounds (or outer bounds) is fairly small for practical medium-to-high
values of signal-to-noise ratio (SNR) in the weak interference regime of the
symmetric real GIC. This result leaves a somewhat marginal performance benefit
to interference management schemes, compared to the simple time division scheme
in the symmetric real case. On a positive side, we further investigate the
impact of phase offset of direct-channel and cross-channel coefficients on the
capacity of the asymmetric complex GIC. It turns out that sophisticated
interference management schemes still have a meaningful opportunity to
noticeably improve the sum-rate performance in this general case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03325</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03325</id><created>2015-06-10</created><authors><author><keyname>Pohl</keyname><forenames>Christoph</forenames></author><author><keyname>Schlierkamp</keyname><forenames>Kathrin</forenames></author><author><keyname>Hof</keyname><forenames>Hans-Joachim</forenames></author></authors><title>BREW: A Breakable Web Application for IT-Security Classroom Use</title><categories>cs.CY</categories><comments>15 pages in European Conference on Software Engineering Education
  2014</comments><journal-ref>European Conference on Software Engineering Education : ECSEE
  2014, 27th and 28th November 2014,191-205</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents BREW (Breakable Web Application), a tool for teaching IT
Security. BREWs main teaching targets are identification and exploitation of
vulnerabilities, using technologies and methodologies for software auditing and
testing, and bug detection, fixation, and writing of secure code. Main
advantages of BREW include that it is easy to apply in practice, it is a
perfect tool to create and retain motivation, it corresponds to the demands of
the psychology of learning, and it can be used for a heterogeneous group of
students. BREW has been success- fully used for teaching IT Security in Germany
as well as on an Erasmus Project with international student groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03338</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03338</id><created>2015-06-10</created><updated>2015-11-16</updated><authors><author><keyname>Gu</keyname><forenames>Shixiang</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author><author><keyname>Turner</keyname><forenames>Richard E.</forenames></author></authors><title>Neural Adaptive Sequential Monte Carlo</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequential Monte Carlo (SMC), or particle filtering, is a popular class of
methods for sampling from an intractable target distribution using a sequence
of simpler intermediate distributions. Like other importance sampling-based
methods, performance is critically dependent on the proposal distribution: a
bad proposal can lead to arbitrarily inaccurate estimates of the target
distribution. This paper presents a new method for automatically adapting the
proposal using an approximation of the Kullback-Leibler divergence between the
true posterior and the proposal distribution. The method is very flexible,
applicable to any parameterized proposal distribution and it supports online
and batch variants. We use the new framework to adapt powerful proposal
distributions with rich parameterizations based upon neural networks leading to
Neural Adaptive Sequential Monte Carlo (NASMC). Experiments indicate that NASMC
significantly improves inference in a non-linear state space model
outperforming adaptive proposal methods including the Extended Kalman and
Unscented Particle Filters. Experiments also indicate that improved inference
translates into improved parameter learning when NASMC is used as a subroutine
of Particle Marginal Metropolis Hastings. Finally we show that NASMC is able to
train a latent variable recurrent neural network (LV-RNN) achieving results
that compete with the state-of-the-art for polymorphic music modelling. NASMC
can be seen as bridging the gap between adaptive SMC methods and the recent
work in scalable, black-box variational inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03340</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03340</id><created>2015-06-10</created><updated>2015-11-19</updated><authors><author><keyname>Hermann</keyname><forenames>Karl Moritz</forenames></author><author><keyname>Ko&#x10d;isk&#xfd;</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Grefenstette</keyname><forenames>Edward</forenames></author><author><keyname>Espeholt</keyname><forenames>Lasse</forenames></author><author><keyname>Kay</keyname><forenames>Will</forenames></author><author><keyname>Suleyman</keyname><forenames>Mustafa</forenames></author><author><keyname>Blunsom</keyname><forenames>Phil</forenames></author></authors><title>Teaching Machines to Read and Comprehend</title><categories>cs.CL cs.AI cs.NE</categories><comments>Appears in: Advances in Neural Information Processing Systems 28
  (NIPS 2015). 14 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Teaching machines to read natural language documents remains an elusive
challenge. Machine reading systems can be tested on their ability to answer
questions posed on the contents of documents that they have seen, but until now
large scale training and test datasets have been missing for this type of
evaluation. In this work we define a new methodology that resolves this
bottleneck and provides large scale supervised reading comprehension data. This
allows us to develop a class of attention based deep neural networks that learn
to read real documents and answer complex questions with minimal prior
knowledge of language structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03350</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03350</id><created>2015-06-10</created><authors><author><keyname>Gaspar</keyname><forenames>Ivan</forenames></author><author><keyname>Matth&#xe9;</keyname><forenames>Maximilian</forenames></author><author><keyname>Michailow</keyname><forenames>Nicola</forenames></author><author><keyname>Mendes</keyname><forenames>Luciano Leonel</forenames></author><author><keyname>Zhang</keyname><forenames>Dan</forenames></author><author><keyname>Fettweis</keyname><forenames>Gerhard</forenames></author></authors><title>GFDM Transceiver using Precoded Data and Low-complexity Multiplication
  in Time Domain</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Wireless Communication Letter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future wireless communication systems are demanding a more flexible physical
layer. GFDM is a block filtered multicarrier modulation scheme proposed to add
multiple degrees of freedom and cover other waveforms in a single framework. In
this paper, GFDM modulation and demodulation will be presented as a
frequency-domain circular convolution, allowing for a reduction of the
implementation complexity when MF, ZF and MMSE filters are employed in the
inner and outer receiver operation. Also, precoding is introduced to further
increase GFDM flexibility, addressing a wider set of applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03352</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03352</id><created>2015-06-10</created><updated>2015-07-06</updated><authors><author><keyname>Mochaourab</keyname><forenames>Rami</forenames></author><author><keyname>Bengtsson</keyname><forenames>Mats</forenames></author></authors><title>Linear Transceiver Optimization in Multicell MIMO Based on the
  Generalized Benders Decomposition</title><categories>cs.IT math.IT</categories><comments>Published in Proc. of IEEE International Workshop on Signal
  Processing Advances in Wireless Communications (SPAWC '15), 5 pages, 2
  tables, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the maximum sum rate optimization problem in the multiple-input
multiple-output interfering broadcast channel. The multiple-antenna
transmitters and receivers are assumed to have perfect channel state
information. In this setting, finding the optimal linear transceiver design is
an NP-hard problem. We show that a reformulation of the problem renders the
application of generalized Benders decomposition suitable. The decomposition
provides us with an optimization structure which we exploit to apply two
different optimization approaches. While one approach is guaranteed to converge
to a local optimum of the original problem, the other approach hinges on
techniques which can be promising for devising a global optimization method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03353</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03353</id><created>2015-06-10</created><authors><author><keyname>Zhang</keyname><forenames>Qi</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author><author><keyname>McKay</keyname><forenames>Matthew</forenames></author><author><keyname>Morales-Jimenez</keyname><forenames>David</forenames></author><author><keyname>Zhu</keyname><forenames>Hongbo</forenames></author></authors><title>Power Allocation Schemes for Multicell Massive MIMO Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the sum-rate gains brought by power allocation
strategies in multicell massive multipleinput multiple-output systems, assuming
time-division duplex transmission. For both uplink and downlink, we derive
tractable expressions for the achievable rate with zero-forcing receivers and
precoders respectively. To avoid high complexity joint optimization across the
network, we propose a scheduling mechanism for power allocation, where in a
single time slot, only cells that do not interfere with each other adjust their
transmit powers. Based on this, corresponding transmit power allocation
strategies are derived, aimed at maximizing the sum rate per-cell. These
schemes are shown to bring considerable gains over equal power allocation for
practical antenna configurations (e.g., up to a few hundred). However, with
fixed number of users (N), these gains diminish as M turns to infinity, and
equal power allocation becomes optimal. A different conclusion is drawn for the
case where both M and N grow large together, in which case: (i) improved rates
are achieved as M grows with fixed M/N ratio, and (ii) the relative gains over
the equal power allocation diminish as M/N grows. Moreover, we also provide
applicable values of M/N under an acceptable power allocation gain threshold,
which can be used as to determine when the proposed power allocation schemes
yield appreciable gains, and when they do not. From the network point of view,
the proposed scheduling approach can achieve almost the same performance as the
joint power allocation after one scheduling round, with much reduced
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03355</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03355</id><created>2015-06-10</created><authors><author><keyname>Honda</keyname><forenames>Junya</forenames></author></authors><title>Exact Asymptotics for the Random Coding Error Probability</title><categories>cs.IT math.IT</categories><comments>Full version of the paper in ISIT2015 with some corrections and
  refinements</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Error probabilities of random codes for memoryless channels are considered in
this paper. In the area of communication systems, admissible error probability
is very small and it is sometimes more important to discuss the relative gap
between the achievable error probability and its bound than to discuss the
absolute gap. Scarlett et al. derived a good upper bound of a random coding
union bound based on the technique of saddlepoint approximation but it is not
proved that the relative gap of their bound converges to zero. This paper
derives a new bound on the achievable error probability in this viewpoint for a
class of memoryless channels. The derived bound is strictly smaller than that
by Scarlett et al. and its relative gap with the random coding error
probability (not a union bound) vanishes as the block length increases for a
fixed coding rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03358</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03358</id><created>2015-06-10</created><authors><author><keyname>Lang</keyname><forenames>Lukas F.</forenames></author><author><keyname>Scherzer</keyname><forenames>Otmar</forenames></author></authors><title>Optical Flow on Evolving Sphere-Like Surfaces</title><categories>math.OC cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we consider optical flow on evolving Riemannian 2-manifolds
which can be parametrised from the 2-sphere. Our main motivation is to estimate
cell motion in time-lapse volumetric microscopy images depicting fluorescently
labelled cells of a live zebrafish embryo. We exploit the fact that the
recorded cells float on the surface of the embryo and allow for the extraction
of an image sequence together with a sphere-like surface. We solve the
resulting variational problem by means of a Galerkin method based on vector
spherical harmonics and present numerical results computed from the
aforementioned microscopy data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03363</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03363</id><created>2015-06-10</created><authors><author><keyname>Clark</keyname><forenames>Tony</forenames></author><author><keyname>Sammut</keyname><forenames>Paul</forenames></author><author><keyname>Willans</keyname><forenames>James</forenames></author></authors><title>Super-Languages: Developing Languages and Applications with XMF (Second
  Edition)</title><categories>cs.SE cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this book is to introduce the language XMF. This is done by
defining the language, providing some examples of applications that can be
written directly in the XOCL language that comes with XMF, and then by showing
how XMF can be used for language engineering. The main focus of this book is on
language engineering by example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03365</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03365</id><created>2015-06-10</created><updated>2015-06-19</updated><authors><author><keyname>Yu</keyname><forenames>Fisher</forenames></author><author><keyname>Zhang</keyname><forenames>Yinda</forenames></author><author><keyname>Song</keyname><forenames>Shuran</forenames></author><author><keyname>Seff</keyname><forenames>Ari</forenames></author><author><keyname>Xiao</keyname><forenames>Jianxiong</forenames></author></authors><title>LSUN: Construction of a Large-scale Image Dataset using Deep Learning
  with Humans in the Loop</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The state-of-the-art visual recognition algorithms are all data-hungry,
requiring a huge amount of labeled image data to optimize millions of
parameters. While there has been remarkable progress in algorithm and system
design, the labeled datasets used by these models are quickly becoming outdated
in terms of size. To overcome the bottleneck of human labeling speed during
dataset construction, we propose to amplify human effort using deep learning
with humans in the loop. Our procedure comes equipped with precision and recall
guarantees to ensure labeling quality, reaching the same level of performance
as fully manual annotation. To demonstrate the power of our annotation
procedure and enable further progress of visual recognition, we construct a
scene-centric database called &quot;LSUN&quot; containing millions of labeled images in
each scene category. We experiment with popular deep nets using our dataset and
obtain a substantial performance gain with the same model trained using our
larger training set. All data and source code will be available online upon
acceptance of the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03366</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03366</id><created>2015-06-10</created><authors><author><keyname>Clark</keyname><forenames>Tony</forenames></author></authors><title>Processing XML for Domain Specific Languages</title><categories>cs.FL cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  XML is a standard and universal language for representing information. XML
processing is supported by two key frameworks: DOM and SAX. SAX is efficient,
but leaves the developer to encode much of the processing. This paper
introduces a language for expressing XML-based languages via grammars that can
be used to process XML documents and synthesize arbitrary values. The language
is declarative and shields the developer from SAX implementation details. The
language is specified and an efficient implementation is defined as an abstract
machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03371</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03371</id><created>2015-06-10</created><authors><author><keyname>Kariotoglou</keyname><forenames>Nikolaos</forenames></author><author><keyname>Kamgarpour</keyname><forenames>Maryam</forenames></author><author><keyname>Summers</keyname><forenames>Tyler H.</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>Upper bounds for the reach-avoid probability via robust optimization</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider finite horizon reach-avoid problems for discrete time stochastic
systems. Our goal is to construct upper bound functions for the reach-avoid
probability by means of tractable convex optimization problems. We achieve this
by restricting attention to the span of Gaussian radial basis functions and
imposing structural assumptions on the transition kernel of the stochastic
processes as well as the target and safe sets of the reach-avoid problem. In
particular, we require the kernel to be written as a Gaussian mixture density
with each mean of the distribution being affine in the current state and input
and the target and safe sets to be written as intersections of quadratic
inequalities. Taking advantage of these structural assumptions, we formulate a
recursion of semidefinite programs where each step provides an upper bound to
the value function of the reach- avoid problem. The upper bounds provide a
performance metric to which any suboptimal control policy can be compared, and
can themselves be used to construct suboptimal control policies. We illustrate
via numerical examples that even if the resulting bounds are conservative, the
associated control policies achieve higher reach-avoid probabilities than
heuristic controllers for problems of large state-input space dimensions (more
than 20). The results presented in this paper, far exceed the limits of current
approximation methods for reach-avoid problems in the specific class of
stochastic systems considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03374</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03374</id><created>2015-06-10</created><authors><author><keyname>Agrawal</keyname><forenames>Shipra</forenames></author><author><keyname>Devanur</keyname><forenames>Nikhil R.</forenames></author><author><keyname>Li</keyname><forenames>Lihong</forenames></author></authors><title>Contextual Bandits with Global Constraints and Objective</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the contextual version of a multi-armed bandit problem with
global convex constraints and concave objective function. In each round, the
outcome of pulling an arm is a context-dependent vector, and the global
constraints require the average of these vectors to lie in a certain convex
set. The objective is a concave function of this average vector. The learning
agent competes with an arbitrary set of context-dependent policies. This
problem is a common generalization of problems considered by Badanidiyuru et
al. (2014) and Agrawal and Devanur (2014), with important applications. We give
computationally efficient algorithms with near-optimal regret, generalizing the
approach of Agarwal et al. (2014) for the non-constrained version of the
problem. For the special case of budget constraints our regret bounds match
those of Badanidiyuru et al. (2014), answering their main open question of
obtaining a computationally efficient algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03377</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03377</id><created>2015-06-10</created><authors><author><keyname>Soh</keyname><forenames>Sieteng</forenames></author><author><keyname>Lin</keyname><forenames>Gongqi</forenames></author><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>Connectivity in Social Networks</title><categories>cs.SI</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The value of a social network is generally determined by its size and the
connectivity of its nodes. But since some of the nodes may be fake ones and
others that are dormant, the question of validating the node counts by
statistical tests becomes important. In this paper we propose the use of the
Benford's distribution to check on the trustworthiness of the connectivity
statistics. Our experiments using statistics of both symmetric and asymmetric
networks show that when the accumulation processes are random, the convergence
to Benford's law is significantly better, and therefore this fact can be used
to distinguish between processes which are randomly generated and those with
internal dependencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03378</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03378</id><created>2015-06-10</created><authors><author><keyname>Liu</keyname><forenames>Che-Yu</forenames></author><author><keyname>Li</keyname><forenames>Lihong</forenames></author></authors><title>On the Prior Sensitivity of Thompson Sampling</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The empirically successful Thompson Sampling algorithm for stochastic bandits
has drawn much interest in understanding its theoretical properties. One
important benefit of the algorithm is that it allows domain knowledge to be
conveniently encoded as a prior distribution to balance exploration and
exploitation more effectively. While it is generally believed that the
algorithm's regret is low (high) when the prior is good (bad), little is known
about the exact dependence. In this paper, we fully characterize the
algorithm's worst-case dependence of regret on the choice of prior, focusing on
a special yet representative case. These results also provide insights into the
general sensitivity of the algorithm to the choice of priors. In particular,
with $p$ being the prior probability mass of the true reward-generating model,
we prove $O(\sqrt{T/p})$ and $O(\sqrt{(1-p)T})$ regret upper bounds for the
bad- and good-prior cases, respectively, as well as \emph{matching} lower
bounds. Our proofs rely on the discovery of a fundamental property of Thompson
Sampling and make heavy use of martingale theory, both of which appear novel in
the literature, to the best of our knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03379</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03379</id><created>2015-06-10</created><updated>2015-09-21</updated><authors><author><keyname>Brunskill</keyname><forenames>Emma</forenames></author><author><keyname>Li</keyname><forenames>Lihong</forenames></author></authors><title>The Online Coupon-Collector Problem and Its Application to Lifelong
  Reinforcement Learning</title><categories>cs.LG cs.AI</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transferring knowledge across a sequence of related tasks is an important
challenge in reinforcement learning (RL). Despite much encouraging empirical
evidence, there has been little theoretical analysis. In this paper, we study a
class of lifelong RL problems: the agent solves a sequence of tasks modeled as
finite Markov decision processes (MDPs), each of which is from a finite set of
MDPs with the same state/action sets and different transition/reward functions.
Motivated by the need for cross-task exploration in lifelong learning, we
formulate a novel online coupon-collector problem and give an optimal
algorithm. This allows us to develop a new lifelong RL algorithm, whose overall
sample complexity in a sequence of tasks is much smaller than single-task
learning, even if the sequence of tasks is generated by an adversary. Benefits
of the algorithm are demonstrated in simulated problems, including a recently
introduced human-robot interaction problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03380</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03380</id><created>2015-06-10</created><authors><author><keyname>Clark</keyname><forenames>Tony</forenames></author><author><keyname>Kramer</keyname><forenames>Dean</forenames></author><author><keyname>Oussena</keyname><forenames>Samia</forenames></author></authors><title>Model Driven Reactive Applications</title><categories>cs.SE cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reactive applications (rapps) are of interest because of the explosion of
mobile, tablet and web-based platforms. The complexity and proliferation of
implementation technologies makes it attractive to use model-driven techniques
to develop rapp systems. This article proposes a domain specific language for
rapps consisting of stereotyped class models for the structure of the
application and state machine models for the application behaviour. The models
are given a semantics in terms of a transformation to a calculus called Widget.
The languages are introduced using an example application for mobile phones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03381</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03381</id><created>2015-06-10</created><authors><author><keyname>Clark</keyname><forenames>Tony</forenames></author></authors><title>Meta-Packages: Painless Domain Specific Languages</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain Specific Languages are used to provide a tailored modelling notation
for a specific application domain. There are currently two main approaches to
DSLs: standard notations that are tailored by adding simple properties; new
notations that are designed from scratch. There are problems with both of these
approaches which can be addressed by providing access to a small meta-language
based on packages and classes. A meta-modelling approach based on meta-packages
allows a wide range of DSLs to be defined in a standard way. The DSLs can be
processed using standard object-based extension at the meta-level and existing
tooling can easily be defined to adapt to the new languages. This paper
introduces the concept of meta-packages and provides a simple example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03382</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03382</id><created>2015-06-10</created><authors><author><keyname>Cai</keyname><forenames>T. Tony</forenames></author><author><keyname>Li</keyname><forenames>Xiaodong</forenames></author><author><keyname>Ma</keyname><forenames>Zongming</forenames></author></authors><title>Optimal Rates of Convergence for Noisy Sparse Phase Retrieval via
  Thresholded Wirtinger Flow</title><categories>math.ST cs.IT math.IT math.NA stat.ML stat.TH</categories><comments>28 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the noisy sparse phase retrieval problem: recovering a
sparse signal $x \in \mathbb{R}^p$ from noisy quadratic measurements $y_j =
(a_j' x )^2 + \epsilon_j$, $j=1, \ldots, m$, with independent sub-exponential
noise $\epsilon_j$. The goals are to understand the effect of the sparsity of
$x$ on the estimation precision and to construct a computationally feasible
estimator to achieve the optimal rates. Inspired by the Wirtinger Flow [12]
proposed for noiseless and non-sparse phase retrieval, a novel thresholded
gradient descent algorithm is proposed and it is shown to adaptively achieve
the minimax optimal rates of convergence over a wide range of sparsity levels
when the $a_j$'s are independent standard Gaussian random vectors, provided
that the sample size is sufficiently large compared to the sparsity of $x$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03392</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03392</id><created>2015-06-10</created><updated>2015-06-12</updated><authors><author><keyname>Matsumoto</keyname><forenames>Stephanos</forenames></author><author><keyname>Reischuk</keyname><forenames>Raphael M.</forenames></author><author><keyname>Szalachowski</keyname><forenames>Pawel</forenames></author><author><keyname>Kim</keyname><forenames>Tiffany Hyun-Jin</forenames></author><author><keyname>Perrig</keyname><forenames>Adrian</forenames></author></authors><title>Designing a Global Authentication Infrastructure</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of scaling authentication for naming, routing, and
end-entity certification to a global environment in which authentication
policies and users' sets of trust roots vary widely. The current mechanisms for
authenticating names (DNSSEC), routes (BGPSEC), and end-entity certificates
(TLS) do not support a coexistence of authentication policies, affect the
entire Internet when compromised, cannot update trust root information
efficiently, and do not provide users with the ability to make flexible trust
decisions. We propose a Scalable Authentication Infrastructure for
Next-generation Trust (SAINT), which partitions the Internet into groups with
common, local trust roots, and isolates the effects of a compromised trust
root. SAINT requires groups with direct routing connections to cross-sign each
other for authentication purposes, allowing diverse authentication policies
while keeping all entities globally verifiable. SAINT makes trust root
management a central part of the network architecture, enabling trust root
updates within seconds and allowing users to make flexible trust decisions.
SAINT operates without a significant performance penalty and can be deployed
alongside existing infrastructures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03394</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03394</id><created>2015-06-10</created><authors><author><keyname>Everett</keyname><forenames>Evan</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>Spatial Self-Interference Isolation for In-Band Full-Duplex Wireless: A
  Degrees-of-Freedom Analysis</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The challenge to in-band full-duplex wireless communication is managing
self-interference. Many designs have employed spatial isolation mechanisms,
such as shielding or multi-antenna beamforming, to isolate the
self-interference wave from the receiver. Such spatial isolation methods are
effective, but by confining the transmit and receive signals to a subset of the
available space, the full spatial resources of the channel be under-utilized,
expending a cost that may nullify the net benefit of operating in full-duplex
mode. In this paper we leverage an antenna-theory-based channel model to
analyze the spatial degrees of freedom available to a full-duplex capable base
station, and observe that whether or not spatial isolation out-performs
time-division (i.e. half-duplex) depends heavily on the geometric distribution
of scatterers. Unless the angular spread of the objects that scatter to the
intended users is overlapped by the spread of objects that backscatter to the
base station, then spatial isolation outperforms time division, otherwise time
division may be optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03398</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03398</id><created>2015-06-10</created><authors><author><keyname>Clark</keyname><forenames>Tony</forenames></author></authors><title>A General Architecture for Heterogeneous Language Engineering and
  Projectional Editor Support</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tool support for language engineering has typically prioritises concrete
syntax over abstract syntax by providing meta-languages for expressing concrete
syntax and then mapping concrete to abstract structures. Text-based languages
are usually specified using a BNF-like language used to generate a syntax-aware
editor that includes features such as keyword completion. Similarly, graphical
languages are defined using a declarative graphical syntax language, producing
an editor that supports features such as shapes, graphs and edges. Projectional
editors invert traditional approaches by prioritising abstract over concrete
syntax. This paper describes a projectional meta-tool architecture, including
general purpose abstract and concrete meta-languages, that uses declarative
rules to integrate the syntax and tool support for a range of heterogeneous
languages. The architecture has been implemented in Racket and the paper
illustrates the architecture with concrete examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03401</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03401</id><created>2015-06-10</created><authors><author><keyname>Pokhriyal</keyname><forenames>Neeti</forenames></author><author><keyname>Dong</keyname><forenames>Wen</forenames></author><author><keyname>Govindaraju</keyname><forenames>Venu</forenames></author></authors><title>Virtual Networks and Poverty Analysis in Senegal</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Do today's communication technologies hold potential to alleviate poverty?
The mobile phone's accessibility and use allows us with an unprecedented volume
of data on social interactions, mobility and more. Can this data help us better
understand, characterize and alleviate poverty in one of the poorest nations in
the world. Our study is an attempt in this direction. We discuss two concepts,
which are both interconnected and immensely useful for securing the important
link between mobile accessibility and poverty.
  First, we use the cellular-communications data to construct virtual
connectivity maps for Senegal, which are then correlated with the poverty
indicators to learn a model. Our model predicts poverty index at any spatial
resolution. Thus, we generate Poverty Maps for Senegal at an unprecedented
finer resolution. Such maps are essential for understanding what characterizes
poverty in a certain region, and how it differentiates from other regions, for
targeted responses for the demographic of the population that is most needy. An
interesting fact, that is empirically proved by our methodology, is that a
large portion of all communication, and economic activity in Senegal is
concentrated in Dakar, leaving many other regions marginalized.
  Second, we study how user behavioral statistics, gathered from
cellular-communications, correlate with the poverty indicators. Can this
relationship be learnt as a model to generate poverty maps at a finer
resolution? Surprisingly, this relationship can give us an alternate poverty
map, that is solely based on the user behavior. Since poverty is a complex
phenomenon, poverty maps showcasing multiple perspectives, such as ours,
provide policymakers with better insights for effective responses for poverty
eradication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03405</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03405</id><created>2015-06-10</created><updated>2015-09-22</updated><authors><author><keyname>Tall</keyname><forenames>Abdoulaye</forenames></author><author><keyname>Altman</keyname><forenames>Zwi</forenames></author><author><keyname>Altman</keyname><forenames>Eitan</forenames></author></authors><title>Self-optimizing load balancing with backhaul-constrained radio access
  networks</title><categories>cs.NI</categories><comments>Wireless Communications Letters, IEEE, 2015</comments><doi>10.1109/LWC.2015.2477499</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-Organizing Network (SON) technology aims at autonomously deploying,
optimizing and repairing the Radio Access Networks (RAN). SON algorithms
typically use Key Performance Indicators (KPIs) from the RAN. It is shown that
in certain cases, it is essential to take into account the impact of the
backhaul state in the design of the SON algorithm. We revisit the Base Station
(BS) load definition taking into account the backhaul state. We provide an
analytical formula for the load along with a simple estimator for both elastic
and guaranteed bit-rate (GBR) traffic. We incorporate the proposed load
estimator in a self-optimized load balancing algorithm. Simulation results for
a backhaul constrained heterogeneous network illustrate how the correct load
definition can guarantee a proper operation of the SON algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03407</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03407</id><created>2015-06-10</created><authors><author><keyname>No</keyname><forenames>Albert</forenames></author><author><keyname>Ingber</keyname><forenames>Amir</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Strong Successive Refinability and Rate-Distortion-Complexity Tradeoff</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the second order asymptotics (source dispersion) of the
successive refinement problem. Similarly to the classical definition of a
successively refinable source, we say that a source is strongly successively
refinable if successive refinement coding can achieve the second order optimum
rate (including the dispersion terms) at both decoders. We establish a
sufficient condition for strong successive refinability. We show that any
discrete source under Hamming distortion and the Gaussian source under
quadratic distortion are strongly successively refinable.
  We also demonstrate how successive refinement ideas can be used in
point-to-point lossy compression problems in order to reduce complexity. We
give two examples, the binary-Hamming and Gaussian-quadratic cases, in which a
layered code construction results in a low complexity scheme that attains
optimal performance. For example, when the number of layers grows with the
block length $n$, we show how to design an $O(n^{\log(n)})$ algorithm that
asymptotically achieves the rate-distortion bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03410</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03410</id><created>2015-06-10</created><authors><author><keyname>Tomita</keyname><forenames>Tyler M.</forenames></author><author><keyname>Maggioni</keyname><forenames>Mauro</forenames></author><author><keyname>Vogelstein</keyname><forenames>Joshua T.</forenames></author></authors><title>Randomer Forests</title><categories>stat.ML cs.LG</categories><comments>9 pages</comments><msc-class>68T10</msc-class><acm-class>I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random forests (RF) is a popular general purpose classifier that has been
shown to outperform many other classifiers on a variety of datasets. The
widespread use of random forests can be attributed to several factors, some of
which include its excellent empirical performance, scale and unit invariance,
robustness to outliers, time and space complexity, and interpretability. While
RF has many desirable qualities, one drawback is its sensitivity to rotations
and other operations that &quot;mix&quot; variables. In this work, we establish a
generalized forest building scheme, linear threshold forests. Random forests
and many other currently existing decision forest algorithms can be viewed as
special cases of this scheme. With this scheme in mind, we propose a few
special cases which we call randomer forests (RerFs). RerFs are linear
threshold forest that exhibit all of the nice properties of RF, in addition to
approximate affine invariance. In simulated datasets designed for RF to do
well, we demonstrate that RerF outperforms RF. We also demonstrate that one
particular variant of RerF is approximately affine invariant. Lastly, in an
evaluation on 121 benchmark datasets, we observe that RerF outperforms RF. We
therefore putatively propose that RerF be considered a replacement for RF as
the general purpose classifier of choice. Open source code is available at
http://ttomita.github.io/RandomerForest/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03412</identifier>
 <datestamp>2015-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03412</id><created>2015-06-10</created><updated>2015-11-16</updated><authors><author><keyname>Ithapu</keyname><forenames>Vamsi K.</forenames></author><author><keyname>Ravi</keyname><forenames>Sathya</forenames></author><author><keyname>Singh</keyname><forenames>Vikas</forenames></author></authors><title>Convergence rates for pretraining and dropout: Guiding learning
  parameters using network structure</title><categories>cs.LG cs.CV cs.NE math.OC stat.ML</categories><comments>25 pages. arXiv admin note: substantial text overlap with
  arXiv:1502.03537 The paper has been withdrawn due to an error in the proof of
  theorem 3.1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised pretraining and dropout have been well studied, especially with
respect to regularization and output consistency. However, our understanding
about the explicit convergence rates of the parameter estimates, and their
dependence on the learning (like denoising and dropout rate) and structural
(like depth and layer lengths) aspects of the network is less mature. An
interesting question in this context is to ask if the network structure could
&quot;guide&quot; the choices of such learning parameters. In this work, we explore these
gaps between network structure, the learning mechanisms and their interaction
with parameter convergence rates. We present a way to address these issues
based on the backpropagation convergence rates for general nonconvex objectives
using first-order information. We then incorporate two learning mechanisms into
this general framework -- denoising autoencoder and dropout, and subsequently
derive the convergence rates of deep networks. Building upon these bounds, we
provide insights into the choices of learning parameters and network sizes that
achieve certain levels of convergence accuracy. The results derived here
support existing empirical observations, and we also conduct a set of
experiments to evaluate them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03425</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03425</id><created>2015-06-10</created><authors><author><keyname>Choromanski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Kumar</keyname><forenames>Sanjiv</forenames></author><author><keyname>Liu</keyname><forenames>Xiaofeng</forenames></author></authors><title>Fast Online Clustering with Randomized Skeleton Sets</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new fast online clustering algorithm that reliably recovers
arbitrary-shaped data clusters in high throughout data streams. Unlike the
existing state-of-the-art online clustering methods based on k-means or
k-medoid, it does not make any restrictive generative assumptions. In addition,
in contrast to existing nonparametric clustering techniques such as DBScan or
DenStream, it gives provable theoretical guarantees. To achieve fast
clustering, we propose to represent each cluster by a skeleton set which is
updated continuously as new data is seen. A skeleton set consists of weighted
samples from the data where weights encode local densities. The size of each
skeleton set is adapted according to the cluster geometry. The proposed
technique automatically detects the number of clusters and is robust to
outliers. The algorithm works for the infinite data stream where more than one
pass over the data is not feasible. We provide theoretical guarantees on the
quality of the clustering and also demonstrate its advantage over the existing
state-of-the-art on several datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03428</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03428</id><created>2015-06-10</created><authors><author><keyname>Ramos</keyname><forenames>Marcus V. M.</forenames></author><author><keyname>de Queiroz</keyname><forenames>Ruy J. G. B.</forenames></author></authors><title>Formalization of closure properties for context-free grammars</title><categories>cs.FL</categories><journal-ref>Preliminary Proceedings of the 9th Workshop on Logical and
  Semantic Frameworks, with Applications, LSFA'14 (2014), pp. 187-198</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context-free language theory is a well-established area of mathematics,
relevant to computer science foundations and technology. This paper presents
the preliminary results of an ongoing formalization project using context-free
grammars and the Coq proof assistant. The results obtained so far include the
representation of context-free grammars, the description of algorithms for some
operations on them (union, concatenation and closure) and the proof of related
theorems (e.g. the correctness of these algorithms). A brief survey of related
works is presented, as well as plans for further development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03437</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03437</id><created>2015-06-10</created><authors><author><keyname>Hassan-Moghaddam</keyname><forenames>Sepideh</forenames></author><author><keyname>Jovanovi&#x107;</keyname><forenames>Mihailo R.</forenames></author></authors><title>An interior-point method for topology identification and optimal design
  of noisy networks</title><categories>math.OC cs.SY</categories><comments>15 pages; 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of optimal topology identification and design of the
corresponding edge weights for stochastically-forced undirected networks. By
introducing $\ell_1$-regularization into the optimal control formulation aimed
at minimizing the steady-state variance amplification, this problem can be cast
as a semidefinite program. Standard interior-point method solvers can be used
to efficiently compute the optimal solution for small and medium size networks.
In this paper, we develop a customized algorithm, based on infeasible
primal-dual interior-point method, that is well-suited for large problems. The
search direction is obtained using the direct method based on Cholesky
factorization and an inexact iterative method based on the preconditioned
conjugate gradients (PCG). We illustrate that both of these significantly
outperform the general-purpose solvers and that the PCG method can solve the
problems with hundreds of thousands of edges in the controller graph in several
minutes, on a PC. We also exploit structure of connected resistive networks and
demonstrate how additional edges can be systematically added in order to
minimize the total effective resistance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03471</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03471</id><created>2015-06-10</created><authors><author><keyname>Zyskind</keyname><forenames>Guy</forenames></author><author><keyname>Nathan</keyname><forenames>Oz</forenames></author><author><keyname>Pentland</keyname><forenames>Alex</forenames></author></authors><title>Enigma: Decentralized Computation Platform with Guaranteed Privacy</title><categories>cs.CR cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A peer-to-peer network, enabling different parties to jointly store and run
computations on data while keeping the data completely private. Enigma's
computational model is based on a highly optimized version of secure
multi-party computation, guaranteed by a verifiable secret-sharing scheme. For
storage, we use a modified distributed hashtable for holding secret-shared
data. An external blockchain is utilized as the controller of the network,
manages access control, identities and serves as a tamper-proof log of events.
Security deposits and fees incentivize operation, correctness and fairness of
the system. Similar to Bitcoin, Enigma removes the need for a trusted third
party, enabling autonomous control of personal data. For the first time, users
are able to share their data with cryptographic guarantees regarding their
privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03473</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03473</id><created>2015-06-10</created><authors><author><keyname>Petersen</keyname><forenames>Holger</forenames></author></authors><title>A Non-Oblivious Reduction of Counting Ones to Multiplication</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm counting the number of ones in a binary word is presented
running in time $O(\log\log b)$ where $b$ is the number of ones. The operations
available include bit-wise logical operations and multiplication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03475</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03475</id><created>2015-06-10</created><authors><author><keyname>Hou</keyname><forenames>Yuqing</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author></authors><title>Image Tag Completion and Refinement by Subspace Clustering and Matrix
  Completion</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tag-based image retrieval (TBIR) has drawn much attention in recent years due
to the explosive amount of digital images and crowdsourcing tags. However, the
TBIR applications still suffer from the deficient and inaccurate tags provided
by users. Inspired by the subspace clustering methods, we formulate the tag
completion problem in a subspace clustering model which assumes that images are
sampled from subspaces, and complete the tags using the state-of-the-art Low
Rank Representation (LRR) method. And we propose a matrix completion algorithm
to further refine the tags. Our empirical results on multiple benchmark
datasets for image annotation show that the proposed algorithm outperforms
state-of-the-art approaches when handling missing and noisy tags.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03478</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03478</id><created>2015-06-10</created><updated>2015-09-18</updated><authors><author><keyname>Theis</keyname><forenames>Lucas</forenames></author><author><keyname>Bethge</keyname><forenames>Matthias</forenames></author></authors><title>Generative Image Modeling Using Spatial LSTMs</title><categories>stat.ML cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling the distribution of natural images is challenging, partly because of
strong statistical dependencies which can extend over hundreds of pixels.
Recurrent neural networks have been successful in capturing long-range
dependencies in a number of problems but only recently have found their way
into generative image models. We here introduce a recurrent image model based
on multi-dimensional long short-term memory units which are particularly suited
for image modeling due to their spatial structure. Our model scales to images
of arbitrary size and its likelihood is computationally tractable. We find that
it outperforms the state of the art in quantitative comparisons on several
image datasets and produces promising results when used for texture synthesis
and inpainting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03479</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03479</id><created>2015-06-10</created><updated>2015-09-15</updated><authors><author><keyname>Wan</keyname><forenames>Cheng</forenames></author></authors><title>Strategic decentralization in binary choice composite congestion games</title><categories>cs.GT</categories><comments>27 pages, 2 figures</comments><msc-class>91A10, 91A18, 90B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies strategic decentralization in binary choice composite
network congestion games. A player decentralizes if she lets some autonomous
agents to decide respectively how to send different parts of her stock from the
origin to the destination. This paper shows that, with convex, strictly
increasing and differentiable arc cost functions, an atomic splittable player
always has an optimal unilateral decentralization strategy. Besides, unilateral
decentralization gives her the same advantage as being the leader in a
Stackelberg congestion game. Finally, unilateral decentralization of an atomic
player has a negative impact on the social cost and on the costs of the other
players at the equilibrium of the congestion game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03482</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03482</id><created>2015-06-10</created><authors><author><keyname>Feldt</keyname><forenames>Robert</forenames></author><author><keyname>Poulding</keyname><forenames>Simon</forenames></author><author><keyname>Clark</keyname><forenames>David</forenames></author><author><keyname>Yoo</keyname><forenames>Shin</forenames></author></authors><title>Test Set Diameter: Quantifying the Diversity of Sets of Test Cases</title><categories>cs.SE</categories><comments>In submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common and natural intuition among software testers is that test cases need
to differ if a software system is to be tested properly and its quality
ensured. Consequently, much research has gone into formulating distance
measures for how test cases, their inputs and/or their outputs differ. However,
common to these proposals is that they are data type specific and/or calculate
the diversity only between pairs of test inputs, traces or outputs.
  We propose a new metric to measure the diversity of sets of tests: the test
set diameter (TSDm). It extends our earlier, pairwise test diversity metrics
based on recent advances in information theory regarding the calculation of the
normalized compression distance (NCD) for multisets. An advantage is that TSDm
can be applied regardless of data type and on any test-related information, not
only the test inputs. A downside is the increased computational time compared
to competing approaches.
  Our experiments on four different systems show that the test set diameter can
help select test sets with higher structural and fault coverage than random
selection even when only applied to test inputs. This can enable early test
design and selection, prior to even having a software system to test, and
complement other types of test automation and analysis. We argue that this
quantification of test set diversity creates a number of opportunities to
better understand software quality and provides practical ways to increase it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03486</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03486</id><created>2015-06-10</created><updated>2016-03-01</updated><authors><author><keyname>Balsubramani</keyname><forenames>Akshay</forenames></author><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author></authors><title>Sequential Nonparametric Testing with the Law of the Iterated Logarithm</title><categories>stat.ML cs.LG math.ST stat.ME stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new algorithmic framework for sequential hypothesis testing with
i.i.d. data, which includes A/B testing, nonparametric two-sample testing, and
independence testing as special cases. It is novel in several ways: (a) it
takes linear time and constant space to compute on the fly, (b) it has the same
power guarantee as a non-sequential version of the test with the same
computational constraints up to a small factor, and (c) it accesses only as
many samples as are required - its stopping time adapts to the unknown
difficulty of the problem. All our test statistics are constructed to be
zero-mean martingales under the null hypothesis, and the rejection threshold is
governed by a uniform non-asymptotic law of the iterated logarithm (LIL). For
the case of nonparametric two-sample mean testing, we also provide a finite
sample power analysis, and the first non-asymptotic stopping time calculations
for this class of problems. We verify our predictions for type I and II errors
and stopping times using simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03487</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03487</id><created>2015-06-10</created><updated>2015-08-26</updated><authors><author><keyname>Wieting</keyname><forenames>John</forenames></author><author><keyname>Bansal</keyname><forenames>Mohit</forenames></author><author><keyname>Gimpel</keyname><forenames>Kevin</forenames></author><author><keyname>Livescu</keyname><forenames>Karen</forenames></author><author><keyname>Roth</keyname><forenames>Dan</forenames></author></authors><title>From Paraphrase Database to Compositional Paraphrase Model and Back</title><categories>cs.CL</categories><comments>2015 TACL paper updated with an appendix describing new 300
  dimensional embeddings. Submitted 1/2015. Accepted 2/2015. Published 6/2015</comments><journal-ref>TACL Vol 3 (2015) pg 345-358</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Paraphrase Database (PPDB; Ganitkevitch et al., 2013) is an extensive
semantic resource, consisting of a list of phrase pairs with (heuristic)
confidence estimates. However, it is still unclear how it can best be used, due
to the heuristic nature of the confidences and its necessarily incomplete
coverage. We propose models to leverage the phrase pairs from the PPDB to build
parametric paraphrase models that score paraphrase pairs more accurately than
the PPDB's internal scores while simultaneously improving its coverage. They
allow for learning phrase embeddings as well as improved word embeddings.
Moreover, we introduce two new, manually annotated datasets to evaluate
short-phrase paraphrasing models. Using our paraphrase model trained using
PPDB, we achieve state-of-the-art results on standard word and bigram
similarity tasks and beat strong baselines on our new short phrase paraphrase
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03489</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03489</id><created>2015-06-10</created><authors><author><keyname>Cummings</keyname><forenames>Rachel</forenames></author><author><keyname>Ioannidis</keyname><forenames>Stratis</forenames></author><author><keyname>Ligett</keyname><forenames>Katrina</forenames></author></authors><title>Truthful Linear Regression</title><categories>cs.GT cs.DS stat.ML</categories><comments>To appear in Proceedings of the 28th Annual Conference on Learning
  Theory (COLT 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of fitting a linear model to data held by individuals
who are concerned about their privacy. Incentivizing most players to truthfully
report their data to the analyst constrains our design to mechanisms that
provide a privacy guarantee to the participants; we use differential privacy to
model individuals' privacy losses. This immediately poses a problem, as
differentially private computation of a linear model necessarily produces a
biased estimation, and existing approaches to design mechanisms to elicit data
from privacy-sensitive individuals do not generalize well to biased estimators.
We overcome this challenge through an appropriate design of the computation and
payment scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03493</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03493</id><created>2015-06-10</created><authors><author><keyname>Schein</keyname><forenames>Aaron</forenames></author><author><keyname>Paisley</keyname><forenames>John</forenames></author><author><keyname>Blei</keyname><forenames>David M.</forenames></author><author><keyname>Wallach</keyname><forenames>Hanna</forenames></author></authors><title>Bayesian Poisson Tensor Factorization for Inferring Multilateral
  Relations from Sparse Dyadic Event Counts</title><categories>stat.ML cs.AI cs.LG cs.SI stat.AP</categories><comments>To appear in Proceedings of the 21st ACM SIGKDD Conference of
  Knowledge Discovery and Data Mining (KDD 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a Bayesian tensor factorization model for inferring latent group
structures from dynamic pairwise interaction patterns. For decades, political
scientists have collected and analyzed records of the form &quot;country $i$ took
action $a$ toward country $j$ at time $t$&quot;---known as dyadic events---in order
to form and test theories of international relations. We represent these event
data as a tensor of counts and develop Bayesian Poisson tensor factorization to
infer a low-dimensional, interpretable representation of their salient
patterns. We demonstrate that our model's predictive performance is better than
that of standard non-negative tensor factorization methods. We also provide a
comparison of our variational updates to their maximum likelihood counterparts.
In doing so, we identify a better way to form point estimates of the latent
factors than that typically used in Bayesian Poisson matrix factorization.
Finally, we showcase our model as an exploratory analysis tool for political
scientists. We show that the inferred latent factor matrices capture
interpretable multilateral relations that both conform to and inform our
knowledge of international affairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03495</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03495</id><created>2015-06-10</created><authors><author><keyname>Chino</keyname><forenames>Daniel Y. T.</forenames></author><author><keyname>Avalhais</keyname><forenames>Letricia P. S.</forenames></author><author><keyname>Rodrigues</keyname><forenames>Jose F.</forenames><suffix>Jr.</suffix></author><author><keyname>Traina</keyname><forenames>Agma J. M.</forenames></author></authors><title>BoWFire: Detection of Fire in Still Images by Integrating Pixel Color
  and Texture Analysis</title><categories>cs.CV</categories><comments>8 pages, Proceedings of the 28th SIBGRAPI Conference on Graphics,
  Patterns and Images, IEEE Press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emergency events involving fire are potentially harmful, demanding a fast and
precise decision making. The use of crowdsourcing image and videos on crisis
management systems can aid in these situations by providing more information
than verbal/textual descriptions. Due to the usual high volume of data,
automatic solutions need to discard non-relevant content without losing
relevant information. There are several methods for fire detection on video
using color-based models. However, they are not adequate for still image
processing, because they can suffer on high false-positive results. These
methods also suffer from parameters with little physical meaning, which makes
fine tuning a difficult task. In this context, we propose a novel fire
detection method for still images that uses classification based on color
features combined with texture classification on superpixel regions. Our method
uses a reduced number of parameters if compared to previous works, easing the
process of fine tuning the method. Results show the effectiveness of our method
of reducing false-positives while its precision remains compatible with the
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03498</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03498</id><created>2015-06-10</created><updated>2016-01-28</updated><authors><author><keyname>Saade</keyname><forenames>Alaa</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Matrix Completion from Fewer Entries: Spectral Detectability and Rank
  Estimation</title><categories>cond-mat.dis-nn cs.LG stat.ML</categories><comments>NIPS Conference 2015</comments><journal-ref>Advances in Neural Information Processing Systems (NIPS 2015) 28,
  pages 1261--1269</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The completion of low rank matrices from few entries is a task with many
practical applications. We consider here two aspects of this problem:
detectability, i.e. the ability to estimate the rank $r$ reliably from the
fewest possible random entries, and performance in achieving small
reconstruction error. We propose a spectral algorithm for these two tasks
called MaCBetH (for Matrix Completion with the Bethe Hessian). The rank is
estimated as the number of negative eigenvalues of the Bethe Hessian matrix,
and the corresponding eigenvectors are used as initial condition for the
minimization of the discrepancy between the estimated matrix and the revealed
entries. We analyze the performance in a random matrix setting using results
from the statistical mechanics of the Hopfield neural network, and show in
particular that MaCBetH efficiently detects the rank $r$ of a large $n\times m$
matrix from $C(r)r\sqrt{nm}$ entries, where $C(r)$ is a constant close to $1$.
We also evaluate the corresponding root-mean-square error empirically and show
that MaCBetH compares favorably to other existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03500</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03500</id><created>2015-06-10</created><updated>2015-11-23</updated><authors><author><keyname>Lazaridou</keyname><forenames>Angeliki</forenames></author><author><keyname>Nguyen</keyname><forenames>Dat Tien</forenames></author><author><keyname>Bernardi</keyname><forenames>Raffaella</forenames></author><author><keyname>Baroni</keyname><forenames>Marco</forenames></author></authors><title>Unveiling the Dreams of Word Embeddings: Towards Language-Driven Image
  Generation</title><categories>cs.CV cs.CL</categories><comments>A 6-page version to appear at the Multimodal Machine Learning NIPS
  2015 Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce language-driven image generation, the task of generating an
image visualizing the semantic contents of a word embedding, e.g., given the
word embedding of grasshopper, we generate a natural image of a grasshopper. We
implement a simple method based on two mapping functions. The first takes as
input a word embedding (as produced, e.g., by the word2vec toolkit) and maps it
onto a high-level visual space (e.g., the space defined by one of the top
layers of a Convolutional Neural Network). The second function maps this
abstract visual representation to pixel space, in order to generate the target
image. Several user studies suggest that the current system produces images
that capture general visual properties of the concepts encoded in the word
embedding, such as color or typical environment, and are sufficient to
discriminate between general categories of objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03504</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03504</id><created>2015-06-10</created><updated>2015-11-02</updated><authors><author><keyname>Bachman</keyname><forenames>Philip</forenames></author><author><keyname>Precup</keyname><forenames>Doina</forenames></author></authors><title>Data Generation as Sequential Decision Making</title><categories>cs.LG stat.ML</categories><comments>Accepted for publication at Advances in Neural Information Processing
  Systems (NIPS) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We connect a broad class of generative models through their shared reliance
on sequential decision making. Motivated by this view, we develop extensions to
an existing model, and then explore the idea further in the context of data
imputation -- perhaps the simplest setting in which to investigate the relation
between unconditional and conditional generative modelling. We formulate data
imputation as an MDP and develop models capable of representing effective
policies for it. We construct the models using neural networks and train them
using a form of guided policy search. Our models generate predictions through
an iterative process of feedback and refinement. We show that this approach can
learn effective policies for imputation problems of varying difficulty and
across multiple datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03506</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03506</id><created>2015-06-10</created><authors><author><keyname>Herlihy</keyname><forenames>Maurice</forenames></author><author><keyname>Saraph</keyname><forenames>Vikram</forenames></author></authors><title>The Relative Power of Composite Loop Agreement Tasks</title><categories>cs.DC</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Loop agreement is a family of wait-free tasks that includes set agreement and
simplex agreement, and was used to prove the undecidability of wait-free
solvability of distributed tasks by read/write memory. Herlihy and Rajsbaum
defined the algebraic signature of a loop agreement task, which consists of a
group and a distinguished element. They used the algebraic signature to
characterize the relative power of loop agreement tasks. In particular, they
showed that one task implements another exactly when there is a homomorphism
between their respective signatures sending one distinguished element to the
other. In this paper, we extend the previous result by defining the composition
of multiple loop agreement tasks to create a new one with the same combined
power. We generalize the original algebraic characterization of relative power
to compositions of tasks. In this way, we can think of loop agreement tasks in
terms of their basic building blocks. We also investigate a category-theoretic
perspective of loop agreement by defining a category of loops, showing that the
algebraic signature is a functor, and proving that our definition of task
composition is the &quot;correct&quot; one, in a categorical sense.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03509</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03509</id><created>2015-06-10</created><updated>2015-06-18</updated><authors><author><keyname>Huang</keyname><forenames>Furong</forenames></author><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author></authors><title>Convolutional Dictionary Learning through Tensor Factorization</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensor methods have emerged as a powerful paradigm for consistent learning of
many latent variable models such as topic models, independent component
analysis and dictionary learning. Model parameters are estimated via CP
decomposition of the observed higher order input moments. However, in many
domains, additional invariances such as shift invariances exist, enforced via
models such as convolutional dictionary learning. In this paper, we develop
novel tensor decomposition algorithms for parameter estimation of convolutional
models. Our algorithm is based on the popular alternating least squares method,
but with efficient projections onto the space of stacked circulant matrices.
Our method is embarrassingly parallel and consists of simple operations such as
fast Fourier transforms and matrix multiplications. Our algorithm converges to
the dictionary much faster and more accurately compared to the alternating
minimization over filters and activation maps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03518</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03518</id><created>2015-06-10</created><updated>2016-01-29</updated><authors><author><keyname>Okano</keyname><forenames>Kunihisa</forenames></author><author><keyname>Ishii</keyname><forenames>Hideaki</forenames></author></authors><title>Stabilization of uncertain systems using quantized and lossy
  observations and uncertain control inputs</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a stabilization problem of an uncertain system in
a networked control setting. Due to the network, the measurements are quantized
to finite-bit signals and may be randomly lost in the communication. We study
uncertain autoregressive systems whose state and input parameters vary within
given intervals. We derive conditions for making the plant output to be mean
square stable, characterizing limitations on data rate, packet loss
probabilities, and magnitudes of uncertainty. It is shown that a specific class
of nonuniform quantizers can achieve stability with a lower data rate compared
with the common uniform one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03521</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03521</id><created>2015-06-10</created><updated>2015-10-06</updated><authors><author><keyname>Oymak</keyname><forenames>Samet</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author><author><keyname>Soltanolkotabi</keyname><forenames>Mahdi</forenames></author></authors><title>Isometric sketching of any set via the Restricted Isometry Property</title><categories>cs.IT cs.DS math.IT math.PR math.ST stat.ML stat.TH</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show that for the purposes of dimensionality reduction
certain class of structured random matrices behave similarly to random Gaussian
matrices. This class includes several matrices for which matrix-vector multiply
can be computed in log-linear time, providing efficient dimensionality
reduction of general sets. In particular, we show that using such matrices any
set from high dimensions can be embedded into lower dimensions with near
optimal distortion. We obtain our results by connecting dimensionality
reduction of any set to dimensionality reduction of sparse vectors via a
chaining argument.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03523</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03523</id><created>2015-06-10</created><updated>2015-11-05</updated><authors><author><keyname>Hegarty</keyname><forenames>Fintan</forenames></author><author><keyname>Cath&#xe1;in</keyname><forenames>Padraig &#xd3;</forenames></author><author><keyname>Zhao</keyname><forenames>Yunbin</forenames></author></authors><title>Sparsification of Matrices and Compressed Sensing</title><categories>cs.IT math.IT</categories><comments>12 pages, 4 figures</comments><msc-class>94A12, 94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing is a relatively new signal processing technique whereby
the limits proposed by the Shannon-Nyquist theorem can be exceeded under
certain conditions imposed upon the signal. Such conditions occur in many
real-world scenarios, and compressed sensing has emerging applications in
medical imaging, big data, and statistics. Finding practical matrix
constructions and computationally efficient recovery algorithms for compressed
sensing is an area of intense research interest. Many probabilistic matrix
constructions have been proposed, and it is now well known that matrices with
entries drawn from a suitable probability distribution are essentially optimal
for compressed sensing. Potential applications have motivated the search for
constructions of sparse compressed sensing matrices (i.e. matrices containing
few non-zero entries). Various constructions have been proposed, and
simulations suggest that their performance is comparable to that of dense
matrices. In this paper extensive simulations are presented which suggest that
sparsification leads to a marked improvement in compressed sensing performance
for a large class of matrix constructions and for many different recovery
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03528</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03528</id><created>2015-06-10</created><authors><author><keyname>Amani</keyname><forenames>Mahdi</forenames></author><author><keyname>Lai</keyname><forenames>Kevin A.</forenames></author><author><keyname>Tarjan</keyname><forenames>Robert E.</forenames></author></authors><title>Amortized Rotation Cost in AVL Trees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An AVL tree is the original type of balanced binary search tree. An insertion
in an $n$-node AVL tree takes at most two rotations, but a deletion in an
$n$-node AVL tree can take $\Theta(\log n)$. A natural question is whether
deletions can take many rotations not only in the worst case but in the
amortized case as well. A sequence of $n$ successive deletions in an $n$-node
tree takes $O(n)$ rotations, but what happens when insertions are intermixed
with deletions? Heaupler, Sen, and Tarjan conjectured that alternating
insertions and deletions in an $n$-node AVL tree can cause each deletion to do
$\Omega(\log n)$ rotations, but they provided no construction to justify their
claim. We provide such a construction: we show that, for infinitely many $n$,
there is a set $E$ of {\it expensive} $n$-node AVL trees with the property
that, given any tree in $E$, deleting a certain leaf and then reinserting it
produces a tree in $E$, with the deletion having done $\Theta(\log n)$
rotations. One can do an arbitrary number of such expensive deletion-insertion
pairs. The difficulty in obtaining such a construction is that in general the
tree produced by an expensive deletion-insertion pair is not the original tree.
Indeed, if the trees in $E$ have even height $k$, $2^{k/2}$ deletion-insertion
pairs are required to reproduce the original tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03540</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03540</id><created>2015-06-10</created><authors><author><keyname>Kim</keyname><forenames>Kangjin</forenames></author><author><keyname>Campbell</keyname><forenames>Joe</forenames></author><author><keyname>Duong</keyname><forenames>William</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Fainekos</keyname><forenames>Georgios</forenames></author></authors><title>DisCoF$^+$: Asynchronous DisCoF with Flexible Decoupling for Cooperative
  Pathfinding in Distributed Systems</title><categories>cs.RO</categories><comments>9 pages, 3 figures, in Proceedings of the IEEE International
  Conference on Automation Science and Engineering, Aug. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our prior work, we outlined an approach, named DisCoF, for cooperative
pathfinding in distributed systems with limited sensing and communication
range. Contrasting to prior works on cooperative pathfinding with completeness
guarantees, which often assume the access to global information, DisCoF does
not make this assumption. The implication is that at any given time in DisCoF,
the robots may not all be aware of each other, which is often the case in
distributed systems. As a result, DisCoF represents an inherently online
approach since coordination can only be realized in an opportunistic manner
between robots that are within each other's sensing and communication range.
However, there are a few assumptions made in DisCoF to facilitate a formal
analysis, which must be removed to work with distributed multi-robot platforms.
In this paper, we present DisCoF$^+$, which extends DisCoF by enabling an
asynchronous solution, as well as providing flexible decoupling between robots
for performance improvement. We also extend the formal results of DisCoF to
DisCoF$^+$. Furthermore, we evaluate our implementation of DisCoF$^+$ and
demonstrate a simulation of it running in a distributed multi-robot
environment. Finally, we compare DisCoF$^+$ with DisCoF in terms of plan
quality and planning performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03549</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03549</id><created>2015-06-11</created><authors><author><keyname>Sun</keyname><forenames>Qiyu</forenames></author><author><keyname>Tang</keyname><forenames>Wai-Shing</forenames></author></authors><title>Nonlinear frames and sparse reconstructions in Banach spaces</title><categories>cs.IT math.FA math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the first part of this paper, we consider nonlinear extension of frame
theory by introducing bi-Lipschitz maps $F$ between Banach spaces. Our linear
model of bi-Lipschitz maps is the analysis operator associated with Hilbert
frames, $p$-frames, Banach frames, g-frames and fusion frames. In general
Banach space setting, stable algorithm to reconstruct a signal $x$ from its
noisy measurement $F(x)+\epsilon$ may not exist. In this paper, we establish
exponential convergence of two iterative reconstruction algorithms when $F$ is
not too far from some bounded below linear operator with bounded
pseudo-inverse, and when $F$ is a well-localized map between two Banach spaces
with dense Hilbert subspaces. The crucial step to prove the later conclusion is
a novel fixed point theorem for a well-localized map on a Banach space.
  In the second part of this paper, we consider stable reconstruction of sparse
signals in a union ${\bf A}$ of closed linear subspaces of a Hilbert space
${\bf H}$ from their nonlinear measurements. We create an optimization
framework called sparse approximation triple $({\bf A}, {\bf M}, {\bf H})$, and
show that the minimizer $$x^*={\rm argmin}_{\hat x\in {\mathbf M}\ {\rm with} \
\|F(\hat x)-F(x^0)\|\le \epsilon} \|\hat x\|_{\mathbf M}$$ provides a
suboptimal approximation to the original sparse signal $x^0\in {\bf A}$ when
the measurement map $F$ has the sparse Riesz property and almost linear
property on ${\mathbf A}$. The above two new properties is also discussed in
this paper when $F$ is not far away from a linear measurement operator $T$
having the restricted isometry property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03551</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03551</id><created>2015-06-11</created><authors><author><keyname>Khonsari</keyname><forenames>Ahmad</forenames></author><author><keyname>Shariatpanahi</keyname><forenames>Seyed Pooya</forenames></author><author><keyname>Diyanat</keyname><forenames>Abolfazl</forenames></author><author><keyname>Shafiei</keyname><forenames>Hossein</forenames></author></authors><title>On the Feasibility of Wireless Interconnects for High-throughput Data
  Centers</title><categories>cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data Centers (DCs) are required to be scalable to large data sets so as to
accommodate ever increasing demands of resource-limited embedded and mobile
devices. Thanks to the availability of recent high data rate millimeter-wave
frequency spectrum such as 60GHz and due to the favorable attributes of this
technology, wireless DC (WDC) exhibits the potentials of being a promising
solution especially for small to medium scale DCs. This paper investigates the
problem of throughput scalability of WDCs using the established theory of the
asymptotic throughput of wireless multi-hop networks that are primarily
proposed for homogeneous traffic conditions. The rate-heterogeneous traffic
distribution of a data center however, requires the asymptotic heterogeneous
throughput knowledge of a wireless network in order to study the performance
and feasibility of WDCs for practical purposes. To answer these questions this
paper presents a lower bound for the throughput scalability of a multi-hop
rate-heterogeneous network when traffic generation rates of all nodes are
similar, except one node. We demonstrate that the throughput scalability of
conventional multi-hopping and the spatial reuse of the above bi-rate network
is inefficient and henceforth develop a speculative 2-partitioning scheme that
improves the network throughput scaling potentials. A better lower bound of the
throughput is then obtained. Finally, we obtain the throughput scaling of an
i.i.d. rate-heterogeneous network and obtain its lower bound. Again we propose
a speculative 2-partitioning scheme to achieve a network with higher throughput
in terms of improved lower bound. All of the obtained results have been
verified using simulation experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03553</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03553</id><created>2015-06-11</created><authors><author><keyname>Arcile</keyname><forenames>Johan</forenames><affiliation>Laboratoire IBISC, Universit&#xe9; d'Evry-Val d'Essonne</affiliation></author><author><keyname>Didier</keyname><forenames>Jean-Yves</forenames><affiliation>Laboratoire IBISC, Universit&#xe9; d'Evry-Val d'Essonne</affiliation></author><author><keyname>Klaudel</keyname><forenames>Hanna</forenames><affiliation>Laboratoire IBISC, Universit&#xe9; d'Evry-Val d'Essonne</affiliation></author><author><keyname>Devillers</keyname><forenames>Raymond</forenames><affiliation>D&#xe9;partement d'Informatique, Universit&#xe9; Libre de Bruxelles</affiliation></author><author><keyname>Rataj</keyname><forenames>Artur</forenames><affiliation>Institute of Theoretical and Applied Computer Science</affiliation></author></authors><title>Indefinite waitings in MIRELA systems</title><categories>cs.SE cs.LO</categories><comments>In Proceedings ESSS 2015, arXiv:1506.03250</comments><proxy>EPTCS</proxy><acm-class>D.2.4</acm-class><journal-ref>EPTCS 184, 2015, pp. 5-18</journal-ref><doi>10.4204/EPTCS.184.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MIRELA is a high-level language and a rapid prototyping framework dedicated
to systems where virtual and digital objects coexist in the same environment
and interact in real time. Its semantics is given in the form of networks of
timed automata, which can be checked using symbolic methods. This paper shows
how to detect various kinds of indefinite waitings in the components of such
systems. The method is experimented using the PRISM model checker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03554</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03554</id><created>2015-06-11</created><authors><author><keyname>Busard</keyname><forenames>Simon</forenames><affiliation>Universit&#xe9; catholique de Louvain</affiliation></author><author><keyname>Cappart</keyname><forenames>Quentin</forenames><affiliation>Universit&#xe9; catholique de Louvain</affiliation></author><author><keyname>Limbr&#xe9;e</keyname><forenames>Christophe</forenames><affiliation>Universit&#xe9; catholique de Louvain</affiliation></author><author><keyname>Pecheur</keyname><forenames>Charles</forenames><affiliation>Universit&#xe9; catholique de Louvain</affiliation></author><author><keyname>Schaus</keyname><forenames>Pierre</forenames><affiliation>Universit&#xe9; catholique de Louvain</affiliation></author></authors><title>Verification of railway interlocking systems</title><categories>cs.SE cs.LO</categories><comments>In Proceedings ESSS 2015, arXiv:1506.03250</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 184, 2015, pp. 19-31</journal-ref><doi>10.4204/EPTCS.184.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the railway domain, an interlocking is a computerised system that controls
the railway signalling objects in order to allow a safe operation of the train
traffic. Each interlocking makes use of particular data, called application
data, that reflects the track layout of the station under control. The
verification and validation of the application data are performed manually and
is thus error-prone and costly. In this paper, we explain how we built an
executable model in NuSMV of a railway interlocking based on the application
data. We also detail the tool that we have developed in order to translate the
application data into our model automatically. Finally we show how we could
verify a realistic set of safety properties on a real-size station model by
customizing the existing model-checking algorithm with PyNuSMV a Python library
based on NuSMV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03555</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03555</id><created>2015-06-11</created><authors><author><keyname>Kromodimoeljo</keyname><forenames>Sentot</forenames><affiliation>University of Queensland</affiliation></author><author><keyname>Lindsay</keyname><forenames>Peter A.</forenames><affiliation>University of Queensland</affiliation></author></authors><title>Automatic Generation of Minimal Cut Sets</title><categories>cs.LO</categories><comments>In Proceedings ESSS 2015, arXiv:1506.03250</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 184, 2015, pp. 33-47</journal-ref><doi>10.4204/EPTCS.184.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A cut set is a collection of component failure modes that could lead to a
system failure. Cut Set Analysis (CSA) is applied to critical systems to
identify and rank system vulnerabilities at design time. Model checking tools
have been used to automate the generation of minimal cut sets but are generally
based on checking reachability of system failure states. This paper describes a
new approach to CSA using a Linear Temporal Logic (LTL) model checker called BT
Analyser that supports the generation of multiple counterexamples. The approach
enables a broader class of system failures to be analysed, by generalising from
failure state formulae to failure behaviours expressed in LTL. The traditional
approach to CSA using model checking requires the model or system failure to be
modified, usually by hand, to eliminate already-discovered cut sets, and the
model checker to be rerun, at each step. By contrast, the new approach works
incrementally and fully automatically, thereby removing the tedious and
error-prone manual process and resulting in significantly reduced computation
time. This in turn enables larger models to be checked. Two different
strategies for using BT Analyser for CSA are presented. There is generally no
single best strategy for model checking: their relative efficiency depends on
the model and property being analysed. Comparative results are given for the
A320 hydraulics case study in the Behavior Tree modelling language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03556</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03556</id><created>2015-06-11</created><authors><author><keyname>M&#xf6;hlmann</keyname><forenames>Eike</forenames><affiliation>Carl von Ossietzky University of Oldenburg</affiliation></author><author><keyname>Theel</keyname><forenames>Oliver</forenames><affiliation>Carl von Ossietzky University of Oldenburg</affiliation></author></authors><title>Breaking Dense Structures: Proving Stability of Densely Structured
  Hybrid Systems</title><categories>cs.SE cs.LO cs.SY</categories><comments>In Proceedings ESSS 2015, arXiv:1506.03250</comments><proxy>EPTCS</proxy><acm-class>D.2.4; Design; Verification</acm-class><journal-ref>EPTCS 184, 2015, pp. 49-63</journal-ref><doi>10.4204/EPTCS.184.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abstraction and refinement is widely used in software development. Such
techniques are valuable since they allow to handle even more complex systems.
One key point is the ability to decompose a large system into subsystems,
analyze those subsystems and deduce properties of the larger system. As
cyber-physical systems tend to become more and more complex, such techniques
become more appealing.
  In 2009, Oehlerking and Theel presented a (de-)composition technique for
hybrid systems. This technique is graph-based and constructs a Lyapunov
function for hybrid systems having a complex discrete state space. The
technique consists of (1) decomposing the underlying graph of the hybrid system
into subgraphs, (2) computing multiple local Lyapunov functions for the
subgraphs, and finally (3) composing the local Lyapunov functions into a
piecewise Lyapunov function. A Lyapunov function can serve multiple purposes,
e.g., it certifies stability or termination of a system or allows to construct
invariant sets, which in turn may be used to certify safety and security.
  In this paper, we propose an improvement to the decomposing technique, which
relaxes the graph structure before applying the decomposition technique. Our
relaxation significantly reduces the connectivity of the graph by exploiting
super-dense switching. The relaxation makes the decomposition technique more
efficient on one hand and on the other allows to decompose a wider range of
graph structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03557</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03557</id><created>2015-06-11</created><authors><author><keyname>Pang</keyname><forenames>Linna</forenames><affiliation>McMaster University</affiliation></author><author><keyname>Wang</keyname><forenames>Chen-Wei</forenames><affiliation>McMaster University</affiliation></author><author><keyname>Lawford</keyname><forenames>Mark</forenames><affiliation>McMaster University</affiliation></author><author><keyname>Wassyng</keyname><forenames>Alan</forenames><affiliation>McMaster University</affiliation></author><author><keyname>Newell</keyname><forenames>Josh</forenames><affiliation>Systemware Innovation Corporation</affiliation></author><author><keyname>Chow</keyname><forenames>Vera</forenames><affiliation>Systemware Innovation Corporation</affiliation></author><author><keyname>Tremaine</keyname><forenames>David</forenames><affiliation>Systemware Innovation Corporation</affiliation></author></authors><title>Formal Verification of Real-Time Function Blocks Using PVS</title><categories>cs.SE cs.LO</categories><comments>In Proceedings ESSS 2015, arXiv:1506.03250</comments><proxy>EPTCS</proxy><acm-class>D.2.4; F.4.1</acm-class><journal-ref>EPTCS 184, 2015, pp. 65-79</journal-ref><doi>10.4204/EPTCS.184.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A critical step towards certifying safety-critical systems is to check their
conformance to hard real-time requirements. A promising way to achieve this is
by building the systems from pre-verified components and verifying their
correctness in a compositional manner. We previously reported a formal approach
to verifying function blocks (FBs) using tabular expressions and the PVS proof
assistant. By applying our approach to the IEC 61131-3 standard of Programmable
Logic Controllers (PLCs), we constructed a repository of precise specification
and reusable (proven) theorems of feasibility and correctness for FBs. However,
we previously did not apply our approach to verify FBs against timing
requirements, since IEC 61131-3 does not define composite FBs built from
timers. In this paper, based on our experience in the nuclear domain, we
conduct two realistic case studies, consisting of the software requirements and
the proposed FB implementations for two subsystems of an industrial control
system. The implementations are built from IEC 61131-3 FBs, including the
on-delay timer. We find issues during the verification process and suggest
solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03558</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03558</id><created>2015-06-11</created><authors><author><keyname>Wang</keyname><forenames>Chen-Wei</forenames><affiliation>York University</affiliation></author><author><keyname>Ostroff</keyname><forenames>Jonathan S.</forenames><affiliation>York University</affiliation></author><author><keyname>Hudon</keyname><forenames>Simon</forenames><affiliation>York University</affiliation></author></authors><title>Using Indexed and Synchronous Events to Model and Validate
  Cyber-Physical Systems</title><categories>cs.SE cs.LO</categories><comments>In Proceedings ESSS 2015, arXiv:1506.03250</comments><proxy>EPTCS</proxy><acm-class>D.2.4</acm-class><journal-ref>EPTCS 184, 2015, pp. 81-95</journal-ref><doi>10.4204/EPTCS.184.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Timed Transition Models (TTMs) are event-based descriptions for modelling,
specifying, and verifying discrete real-time systems. An event can be
spontaneous, fair, or timed with specified bounds. TTMs have a textual syntax,
an operational semantics, and an automated tool supporting linear-time temporal
logic. We extend TTMs and its tool with two novel modelling features for
writing high-level specifications: indexed events and synchronous events.
Indexed events allow for concise description of behaviour common to a set of
actors. The indexing construct allows us to select a specific actor and to
specify a temporal property for that actor. We use indexed events to validate
the requirements of a train control system. Synchronous events allow developers
to decompose simultaneous state updates into actions of separate events. To
specify the intended data flow among synchronized actions, we use primed
variables to reference the post-state (i.e., one resulted from taking the
synchronized actions). The TTM tool automatically infers the data flow from
synchronous events, and reports errors on inconsistencies due to circular data
flow. We use synchronous events to validate part of the requirements of a
nuclear shutdown system. In both case studies, we show how the new notation
facilitates the formal validation of system requirements, and use the TTM tool
to verify safety, liveness, and real-time properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03562</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03562</id><created>2015-06-11</created><authors><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author><author><keyname>Mignosi</keyname><forenames>Filippo</forenames></author></authors><title>Words with the Maximum Number of Abelian Squares</title><categories>cs.DM cs.FL math.CO</categories><comments>To appear in the proceedings of WORDS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An abelian square is the concatenation of two words that are anagrams of one
another. A word of length $n$ can contain $\Theta(n^2)$ distinct factors that
are abelian squares. We study infinite words such that the number of abelian
square factors of length $n$ grows quadratically with $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03588</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03588</id><created>2015-06-11</created><authors><author><keyname>Chen</keyname><forenames>Liqun</forenames></author><author><keyname>Li</keyname><forenames>Qin</forenames></author><author><keyname>Martin</keyname><forenames>Keith M.</forenames></author><author><keyname>Ng</keyname><forenames>Siaw-Lynn</forenames></author></authors><title>Private reputation retrieval in public - a privacy-aware announcement
  scheme for VANETs</title><categories>cs.CR</categories><comments>This paper is a preprint of a paper submitted to IET Information
  Security and is subject to Institution of Engineering and Technology
  Copyright. If accepted, the copy of record will be available at IET Digital
  Library</comments><msc-class>94A62</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An announcement scheme is a system that facilitates vehicles to broadcast
road-related information in vehicular ad hoc networks (VANETs) in order to
improve road safety and efficiency. Here we propose a new cryptographic
primitive for public updating of reputation score based on the
Boneh-Boyen-Shacham short group signature scheme. This allows private
reputation score retrieval without a secure channel. Using this we devise a
privacy-aware announcement scheme using reputation systems which is reliable,
auditable and robust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03599</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03599</id><created>2015-06-11</created><authors><author><keyname>Dasgupta</keyname><forenames>Sakyasingha</forenames></author><author><keyname>Goldschmidt</keyname><forenames>Dennis</forenames></author><author><keyname>W&#xf6;rg&#xf6;tter</keyname><forenames>Florentin</forenames></author><author><keyname>Manoonpong</keyname><forenames>Poramate</forenames></author></authors><title>Distributed Recurrent Neural Forward Models with Synaptic Adaptation for
  Complex Behaviors of Walking Robots</title><categories>cs.NE cs.RO q-bio.NC</categories><comments>26 pages, 10 figures</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Walking animals, like stick insects, cockroaches or ants, demonstrate a
fascinating range of locomotive abilities and complex behaviors. The locomotive
behaviors can consist of a variety of walking patterns along with adaptation
that allow the animals to deal with changes in environmental conditions, like
uneven terrains, gaps, obstacles etc. Biological study has revealed that such
complex behaviors are a result of a combination of biome- chanics and neural
mechanism thus representing the true nature of embodied interactions. While the
biomechanics helps maintain flexibility and sustain a variety of movements, the
neural mechanisms generate movements while making appropriate predictions
crucial for achieving adaptation. Such predictions or planning ahead can be
achieved by way of in- ternal models that are grounded in the overall behavior
of the animal. Inspired by these findings, we present here, an artificial
bio-inspired walking system which effectively com- bines biomechanics (in terms
of the body and leg structures) with the underlying neural mechanisms. The
neural mechanisms consist of 1) central pattern generator based control for
generating basic rhythmic patterns and coordinated movements, 2) distributed
(at each leg) recurrent neural network based adaptive forward models with
efference copies as internal models for sensory predictions and instantaneous
state estimations, and 3) searching and elevation control for adapting the
movement of an individual leg to deal with different environmental conditions.
Using simulations we show that this bio-inspired approach with adaptive
internal models allows the walking robot to perform complex loco- motive
behaviors as observed in insects, including walking on undulated terrains,
crossing large gaps as well as climbing over high obstacles...
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03604</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03604</id><created>2015-06-11</created><authors><author><keyname>Zheng</keyname><forenames>Chengshi</forenames></author><author><keyname>Schwarz</keyname><forenames>Andreas</forenames></author><author><keyname>Kellermann</keyname><forenames>Walter</forenames></author><author><keyname>Li</keyname><forenames>Xiaodong</forenames></author></authors><title>Binaural coherent-to-diffuse-ratio estimation for dereverberation using
  an ITD model</title><categories>cs.SD</categories><comments>accepted for EUSIPCO 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most previously proposed dual-channel coherent-to-diffuse-ratio (CDR)
estimators are based on a free-field model. When used for binaural signals,
e.g., for dereverberation in binaural hearing aids, their performance may
degrade due to the influence of the head, even when the direction-of-arrival of
the desired speaker is exactly known. In this paper, the head shadowing effect
is taken into account for CDR estimation by using a simplified model for the
frequency-dependent interaural time difference and a model for the binaural
coherence of the diffuse noise field. Evaluation of CDR-based dereverberation
with measured binaural impulse responses indicates that the proposed binaural
CDR estimators can improve PESQ scores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03607</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03607</id><created>2015-06-11</created><updated>2015-09-23</updated><authors><author><keyname>Ch&#xe9;ron</keyname><forenames>Guilhem</forenames></author><author><keyname>Laptev</keyname><forenames>Ivan</forenames></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames></author></authors><title>P-CNN: Pose-based CNN Features for Action Recognition</title><categories>cs.CV</categories><comments>ICCV, December 2015, Santiago, Chile</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work targets human action recognition in video. While recent methods
typically represent actions by statistics of local video features, here we
argue for the importance of a representation derived from human pose. To this
end we propose a new Pose-based Convolutional Neural Network descriptor (P-CNN)
for action recognition. The descriptor aggregates motion and appearance
information along tracks of human body parts. We investigate different schemes
of temporal aggregation and experiment with P-CNN features obtained both for
automatically estimated and manually annotated human poses. We evaluate our
method on the recent and challenging JHMDB and MPII Cooking datasets. For both
datasets our method shows consistent improvement over the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03610</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03610</id><created>2015-06-11</created><updated>2015-06-22</updated><authors><author><keyname>Nichita</keyname><forenames>Florin F.</forenames></author></authors><title>Yang-Baxter Equations, Computational Methods and Applications</title><categories>cs.CE math-ph math.MP</categories><comments>12 pages</comments><msc-class>16T10, 16T25, 17B01, 17B60, 17C05, 17C50, 17D99, 65D20, 65L09,
  68R99, 97M10, 97M80, 97N50, 97N80, 97P20, 97R20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational methods are an important tool for solving the Yang-Baxter
equations(in small dimensions), for classifying (unifying) structures, and for
solving related problems. This paper is an account of some of the latest
developments on the Yang-Baxter equation, its set-theoretical version, and its
applications. We construct new set-theoretical solutions for the Yang-Baxter
equation. Unification theories and other results are proposed or proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03611</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03611</id><created>2015-06-11</created><authors><author><keyname>Kramer</keyname><forenames>Stephan C</forenames></author><author><keyname>Piggott</keyname><forenames>Matthew D</forenames></author></authors><title>A correction to the enhanced bottom drag parameterisation of tidal
  turbines</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hydrodynamic modelling is an important tool for the development of tidal
stream energy projects. Many hydrodynamic models incorporate the effect of
tidal turbines through an enhanced bottom drag. In this paper we show that
although for coarse grid resolutions (kilometre scale) the resulting force
exerted on the flow agrees well with the theoretical value, the force starts
decreasing with decreasing grid sizes when these become smaller than the length
scale of the wake recovery. This is because the assumption that the upstream
velocity can be approximated by the local model velocity, is no longer valid.
Using linear momentum actuator disc theory however, we derive a relationship
between these two velocities and formulate a correction to the enhanced bottom
drag formulation that consistently applies a force that remains closed to the
theoretical value, for all grid sizes down to the turbine scale. In addition, a
better understanding of the relation between the model, upstream, and actual
turbine velocity, as predicted by actuator disc theory, leads to an improved
estimate of the usefully extractable energy. We show how the corrections can be
applied (demonstrated here for the models MIKE 21 and Fluidity) by a simple
modification of the drag coefficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03613</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03613</id><created>2015-06-11</created><authors><author><keyname>Konstantinidis</keyname><forenames>Georgios</forenames></author><author><keyname>Kehagias</keyname><forenames>Athanasios</forenames></author></authors><title>Simultaneously Moving Cops and Robbers</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the concurrent cops and robber (CCCR) game. CCCR
follows the same rules as the classical, turn-based game, except for the fact
that the players move simultaneously. The cops' goal is to capture the robber
and the concurrent cop number of a graph is defined the minimum number of cops
which guarantees capture. For the variant in which it it required to capture
the robber in the shortest possible time, we let time to capture be the payoff
function of CCCR; the (game theoretic) value of CCCR is the optimal capture
time and (cop and robber) time optimal strategies are the ones which achieve
the value. In this paper we prove the following.
  (1) For every graph G, the concurrent cop number is equal to the &quot;classical&quot;
cop number.
  (2) For every graph G, CCCR has a value, the cops have an optimal strategy
and, for every epsilon&gt;0, the robber has an epsilon-optimal strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03623</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03623</id><created>2015-06-11</created><authors><author><keyname>Xiao</keyname><forenames>Han</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaoyan</forenames></author></authors><title>Max-Entropy Feed-Forward Clustering Neural Network</title><categories>cs.LG</categories><comments>This paper has been published in ICANN 2015</comments><journal-ref>ICANN 2015: International Conference on Artificial Neural
  Networks, Amsterdam, The Netherlands, (May 14-15, 2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The outputs of non-linear feed-forward neural network are positive, which
could be treated as probability when they are normalized to one. If we take
Entropy-Based Principle into consideration, the outputs for each sample could
be represented as the distribution of this sample for different clusters.
Entropy-Based Principle is the principle with which we could estimate the
unknown distribution under some limited conditions. As this paper defines two
processes in Feed-Forward Neural Network, our limited condition is the
abstracted features of samples which are worked out in the abstraction process.
And the final outputs are the probability distribution for different clusters
in the clustering process. As Entropy-Based Principle is considered into the
feed-forward neural network, a clustering method is born. We have conducted
some experiments on six open UCI datasets, comparing with a few baselines and
applied purity as the measurement . The results illustrate that our method
outperforms all the other baselines that are most popular clustering methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03624</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03624</id><created>2015-06-11</created><authors><author><keyname>Mankowitz</keyname><forenames>Daniel J.</forenames></author><author><keyname>Mann</keyname><forenames>Timothy A.</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Bootstrapping Skills</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The monolithic approach to policy representation in Markov Decision Processes
(MDPs) looks for a single policy that can be represented as a function from
states to actions. For the monolithic approach to succeed (and this is not
always possible), a complex feature representation is often necessary since the
policy is a complex object that has to prescribe what actions to take all over
the state space. This is especially true in large domains with complicated
dynamics. It is also computationally inefficient to both learn and plan in MDPs
using a complex monolithic approach. We present a different approach where we
restrict the policy space to policies that can be represented as combinations
of simpler, parameterized skills---a type of temporally extended action, with a
simple policy representation. We introduce Learning Skills via Bootstrapping
(LSB) that can use a broad family of Reinforcement Learning (RL) algorithms as
a &quot;black box&quot; to iteratively learn parametrized skills. Initially, the learned
skills are short-sighted but each iteration of the algorithm allows the skills
to bootstrap off one another, improving each skill in the process. We prove
that this bootstrapping process returns a near-optimal policy. Furthermore, our
experiments demonstrate that LSB can solve MDPs that, given the same
representational power, could not be solved by a monolithic approach. Thus,
planning with learned skills results in better policies without requiring
complex policy representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03625</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03625</id><created>2015-06-11</created><authors><author><keyname>Feinerer</keyname><forenames>Ingo</forenames></author><author><keyname>Franconi</keyname><forenames>Enrico</forenames></author><author><keyname>Guagliardo</keyname><forenames>Paolo</forenames></author></authors><title>Lossless Selection Views under Conditional Domain Constraints</title><categories>cs.DB</categories><comments>16 pages, 1 figure, 1 table. Full version with complete proofs.
  Instead of this version, please cite
  http://dx.doi.org/10.1109/TKDE.2014.2334327</comments><journal-ref>IEEE Transactions on Knowledge and Data Engineering, vol. 27, no.
  2, pp. 504-517, February 2015</journal-ref><doi>10.1109/TKDE.2014.2334327</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A set of views defined by selection queries splits a database relation into
sub-relations, each containing a subset of the original rows. This
decomposition into horizontal fragments is lossless when the initial relation
can be reconstructed from the fragments by union. In this paper, we consider
horizontal decomposition in a setting where some of the attributes in the
database schema are interpreted over a specific domain, on which a set of
special predicates and functions is defined.
  We study losslessness in the presence of integrity constraints on the
database schema. We consider the class of conditional domain constraints
(CDCs), which restrict the values that the interpreted attributes may take
whenever a certain condition holds on the non-interpreted ones, and investigate
lossless horizontal decomposition under CDCs in isolation, as well as in
combination with functional and unary inclusion dependencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03626</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03626</id><created>2015-06-11</created><authors><author><keyname>Xiao</keyname><forenames>Han</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaoyan</forenames></author></authors><title>Margin-Based Feed-Forward Neural Network Classifiers</title><categories>cs.LG</categories><comments>This paper has been published in ICANN 2015: International Conference
  on Artificial Neural Networks, Amsterdam, The Netherlands, (May 14-15, 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Margin-Based Principle has been proposed for a long time, it has been proved
that this principle could reduce the structural risk and improve the
performance in both theoretical and practical aspects. Meanwhile, feed-forward
neural network is a traditional classifier, which is very hot at present with a
deeper architecture. However, the training algorithm of feed-forward neural
network is developed and generated from Widrow-Hoff Principle that means to
minimize the squared error. In this paper, we propose a new training algorithm
for feed-forward neural networks based on Margin-Based Principle, which could
effectively promote the accuracy and generalization ability of neural network
classifiers with less labelled samples and flexible network. We have conducted
experiments on four UCI open datasets and achieved good results as expected. In
conclusion, our model could handle more sparse labelled and more high-dimension
dataset in a high accuracy while modification from old ANN method to our method
is easy and almost free of work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03645</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03645</id><created>2015-06-11</created><authors><author><keyname>Mastrandrea</keyname><forenames>Rossana</forenames></author><author><keyname>Fournet</keyname><forenames>Julie</forenames></author><author><keyname>Barrat</keyname><forenames>Alain</forenames></author></authors><title>Contact patterns in a high school: a comparison between data collected
  using wearable sensors, contact diaries and friendship surveys</title><categories>physics.soc-ph cs.SI</categories><journal-ref>PLoS ONE 10(9): e0136497 (2015)</journal-ref><doi>10.1371/journal.pone.0136497</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given their importance in shaping social networks and determining how
information or diseases propagate in a population, human interactions are the
subject of many data collection efforts. To this aim, different methods are
commonly used, from diaries and surveys to wearable sensors. These methods show
advantages and limitations but are rarely compared in a given setting. As
surveys targeting friendship relations might suffer less from memory biases
than contact diaries, it is also interesting to explore how daily contact
patterns compare with friendship relations and with online social links. Here
we make progresses in these directions by leveraging data from a French high
school: face-to-face contacts measured by two concurrent methods, sensors and
diaries; self-reported friendship surveys; Facebook links. We compare the data
sets and find that most short contacts are not reported in diaries while long
contacts have larger reporting probability, with a general tendency to
overestimate durations. Measured contacts corresponding to reported friendship
can have durations of any length but all long contacts correspond to reported
friendships. Online links not associated to reported friendships correspond to
short face-to-face contacts, highlighting the different nature of reported
friendships and online links. Diaries and surveys suffer from a low sampling
rate, showing the higher acceptability of sensor-based platform. Despite the
biases, we found that the overall structure of the contact network, i.e., the
mixing patterns between classes, is correctly captured by both self-reported
contacts and friendships networks. Overall, diaries and surveys tend to yield a
correct picture of the structural organization of the contact network, albeit
with much less links, and give access to a sort of backbone of the contact
network corresponding to the strongest links in terms of cumulative durations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03647</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03647</id><created>2015-06-11</created><updated>2015-08-13</updated><authors><author><keyname>Goedgebeur</keyname><forenames>Jan</forenames></author><author><keyname>Schaudt</keyname><forenames>Oliver</forenames></author></authors><title>Exhaustive generation of $k$-critical $\mathcal H$-free graphs</title><categories>math.CO cs.DM</categories><comments>17 pages, improved girth results. arXiv admin note: text overlap with
  arXiv:1504.06979</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an algorithm for generating all $k$-critical $\mathcal H$-free
graphs, based on a method of Ho\`{a}ng et al. Using this algorithm, we prove
that there are only finitely many $4$-critical $(P_7,C_k)$-free graphs, for
both $k=4$ and $k=5$. We also show that there are only finitely many
$4$-critical graphs $(P_8,C_4)$-free graphs. For each case of these cases we
also give the complete lists of critical graphs and vertex-critical graphs.
These results generalize previous work by Hell and Huang, and yield certifying
algorithms for the $3$-colorability problem in the respective classes.
  Moreover, we prove that for every $t$, the class of 4-critical planar
$P_t$-free graphs is finite. We also determine all 27 4-critical planar
$(P_7,C_6)$-free graphs.
  We also prove that every $P_{10}$-free graph of girth at least five is
3-colorable, and determine the smallest 4-chromatic $P_{12}$-free graph of
girth five. Moreover, we show that every $P_{13}$-free graph of girth at least
six and every $P_{16}$-free graph of girth at least seven is 3-colorable. This
strengthens results of Golovach et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03648</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03648</id><created>2015-06-11</created><updated>2015-10-18</updated><authors><author><keyname>Pathak</keyname><forenames>Deepak</forenames></author><author><keyname>Kr&#xe4;henb&#xfc;hl</keyname><forenames>Philipp</forenames></author><author><keyname>Darrell</keyname><forenames>Trevor</forenames></author></authors><title>Constrained Convolutional Neural Networks for Weakly Supervised
  Segmentation</title><categories>cs.CV cs.LG</categories><comments>12 pages, ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to learn a dense pixel-wise labeling from image-level
tags. Each image-level tag imposes constraints on the output labeling of a
Convolutional Neural Network (CNN) classifier. We propose Constrained CNN
(CCNN), a method which uses a novel loss function to optimize for any set of
linear constraints on the output space (i.e. predicted label distribution) of a
CNN. Our loss formulation is easy to optimize and can be incorporated directly
into standard stochastic gradient descent optimization. The key idea is to
phrase the training objective as a biconvex optimization for linear models,
which we then relax to nonlinear deep networks. Extensive experiments
demonstrate the generality of our new learning framework. The constrained loss
yields state-of-the-art results on weakly supervised semantic image
segmentation. We further demonstrate that adding slightly more supervision can
greatly improve the performance of the learning algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03656</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03656</id><created>2015-06-11</created><authors><author><keyname>Hajri</keyname><forenames>Salah Eddine</forenames></author><author><keyname>Assaad</keyname><forenames>Mohamad</forenames></author></authors><title>An Exclusion zone for Massive MIMO With Underlay D2D Communication</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fifth generation networks will incorporate a variety of new features in
wireless networks such as data offloading, D2D communication, and Massive MIMO.
Massive MIMO is specially appealing since it achieves huge gains while enabling
simple processing like MRC receivers. It suffers, though, from a major
shortcoming refereed to as pilot contamination. In this paper we propose a
frame-work in which, a D2D underlaid Massive MIMO system is implemented and we
will prove that this scheme can reduce the pilot contamination problem while
enabling an optimization of the system spectral efficiency. The D2D
communication will help maintain the network coverage while allowing a better
channel estimation to be performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03658</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03658</id><created>2015-06-11</created><authors><author><keyname>Wang</keyname><forenames>Xiaozhe</forenames></author><author><keyname>Chiang</keyname><forenames>Hsiao-Dong</forenames></author><author><keyname>Wang</keyname><forenames>Jianhui</forenames></author><author><keyname>Liu</keyname><forenames>Hui</forenames></author><author><keyname>Wang</keyname><forenames>Tao</forenames></author></authors><title>Long-Term Stability Analysis of Power Systems with Wind Power Based on
  Stochastic Differential Equations: Model Development and Foundations</title><categories>cs.SY</categories><comments>The paper has been submitted to IEEE Transactions on Sustainable
  Energy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the variable wind power is incorporated into the dynamic model
for long-term stability analysis. A theory-based method is proposed for power
systems with wind power to conduct long-term stability analysis, which is able
to provide accurate stability assessments with fast simulation speed.
Particularly, the theoretical foundation for the proposed approximation
approach is presented. The accuracy and efficiency of the method are
illustrated by several numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03662</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03662</id><created>2015-06-11</created><updated>2016-02-26</updated><authors><author><keyname>Hofmann</keyname><forenames>Thomas</forenames></author><author><keyname>Lucchi</keyname><forenames>Aurelien</forenames></author><author><keyname>Lacoste-Julien</keyname><forenames>Simon</forenames></author><author><keyname>McWilliams</keyname><forenames>Brian</forenames></author></authors><title>Variance Reduced Stochastic Gradient Descent with Neighbors</title><categories>cs.LG math.OC stat.ML</categories><comments>Appears in: Advances in Neural Information Processing Systems 28
  (NIPS 2015). 13 pages</comments><msc-class>90C06, 90C25, 68T05</msc-class><acm-class>G.1.6; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet its
slow convergence can be a computational bottleneck. Variance reduction
techniques such as SAG, SVRG and SAGA have been proposed to overcome this
weakness, achieving linear convergence. However, these methods are either based
on computations of full gradients at pivot points, or on keeping per data point
corrections in memory. Therefore speed-ups relative to SGD may need a minimal
number of epochs in order to materialize. This paper investigates algorithms
that can exploit neighborhood structure in the training data to share and
re-use information about past stochastic gradients across data points, which
offers advantages in the transient optimization phase. As a side-product we
provide a unified convergence analysis for a family of variance reduction
algorithms, which we call memorization algorithms. We provide experimental
results supporting our theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03663</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03663</id><created>2015-06-11</created><authors><author><keyname>Hinrichs</keyname><forenames>Christian</forenames></author><author><keyname>Sonnenschein</keyname><forenames>Michael</forenames></author></authors><title>Design, Analysis and Evaluation of Control Algorithms for Applications
  in Smart Grids</title><categories>cs.SY cs.MA cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many countries, the currently observable transformation of the power
supply system from a centrally controlled system towards a complex &quot;system of
systems&quot;, comprising lots of autonomously interacting components, leads to a
significant amount of research regarding novel control concepts. To facilitate
the structured development of such approaches regarding the criticality of the
targeted system, the research and development of a distributed control concept
is demonstrated by employing an integrated methodology comprising both the
Smart Grids Architecture Model framework (SGAM) and the Smart Grid Algorithm
Engineering process model (SGAE). Along the way, a taxonomy of evaluation
criteria and evaluation methods for such approaches is presented. For the whole
paper, the Dynamic Virtual Power Plants business case (DVPP) serves as
motivating example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03668</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03668</id><created>2015-06-11</created><authors><author><keyname>Dashdorj</keyname><forenames>Zolzaya</forenames></author><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author></authors><title>Impact of the spatial context on human communication activity</title><categories>cs.SI physics.soc-ph</categories><comments>12 pages, 11 figures</comments><msc-class>68</msc-class><acm-class>H.1.2; H.2.8; H.3.3; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Technology development produces terabytes of data generated by hu- man
activity in space and time. This enormous amount of data often called big data
becomes crucial for delivering new insights to decision makers. It contains
behavioral information on different types of human activity influenced by many
external factors such as geographic infor- mation and weather forecast. Early
recognition and prediction of those human behaviors are of great importance in
many societal applications like health-care, risk management and urban
planning, etc. In this pa- per, we investigate relevant geographical areas
based on their categories of human activities (i.e., working and shopping)
which identified from ge- ographic information (i.e., Openstreetmap). We use
spectral clustering followed by k-means clustering algorithm based on TF/IDF
cosine simi- larity metric. We evaluate the quality of those observed clusters
with the use of silhouette coefficients which are estimated based on the
similari- ties of the mobile communication activity temporal patterns. The area
clusters are further used to explain typical or exceptional communication
activities. We demonstrate the study using a real dataset containing 1 million
Call Detailed Records. This type of analysis and its application are important
for analyzing the dependency of human behaviors from the external factors and
hidden relationships and unknown correlations and other useful information that
can support decision-making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03676</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03676</id><created>2015-06-11</created><authors><author><keyname>Christiani</keyname><forenames>Tobias</forenames></author><author><keyname>Pagh</keyname><forenames>Rasmus</forenames></author><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>From Independence to Expansion and Back Again</title><categories>cs.DS</categories><comments>An extended abstract of this paper was accepted to The 47th ACM
  Symposium on Theory of Computing (STOC 2015). Copyright ACM</comments><acm-class>E.1; E.2; G.2.2; F.2.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following fundamental problems: (1) Constructing
$k$-independent hash functions with a space-time tradeoff close to Siegel's
lower bound. (2) Constructing representations of unbalanced expander graphs
having small size and allowing fast computation of the neighbor function. It is
not hard to show that these problems are intimately connected in the sense that
a good solution to one of them leads to a good solution to the other one. In
this paper we exploit this connection to present efficient, recursive
constructions of $k$-independent hash functions (and hence expanders with a
small representation). While the previously most efficient construction
(Thorup, FOCS 2013) needed time quasipolynomial in Siegel's lower bound, our
time bound is just a logarithmic factor from the lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03681</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03681</id><created>2015-06-11</created><authors><author><keyname>Akhter</keyname><forenames>Fatema</forenames></author></authors><title>A Novel Approach for Image Steganography in Spatial Domain</title><categories>cs.MM cs.CR</categories><journal-ref>Global Journal of Computer Science and Technology, Volume 13,
  Issue 7, pp. 1-6 Year 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new approach for hiding information in digital image in
spatial domain. In this approach three bits of message is embedded in a pixel
using Lucas number system but only one bit plane is allowed for alternation.
The experimental results show that the proposed method has the larger capacity
of embedding data, high peak signal to noise ratio compared to existing methods
and is hardly detectable for steganolysis algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03684</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03684</id><created>2015-06-11</created><authors><author><keyname>Morgenstern</keyname><forenames>Jamie</forenames></author><author><keyname>Roughgarden</keyname><forenames>Tim</forenames></author></authors><title>The Pseudo-Dimension of Near-Optimal Auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a general approach, rooted in statistical learning
theory, to learning an approximately revenue-maximizing auction from data. We
introduce $t$-level auctions to interpolate between simple auctions, such as
welfare maximization with reserve prices, and optimal auctions, thereby
balancing the competing demands of expressivity and simplicity. We prove that
such auctions have small representation error, in the sense that for every
product distribution $F$ over bidders' valuations, there exists a $t$-level
auction with small $t$ and expected revenue close to optimal. We show that the
set of $t$-level auctions has modest pseudo-dimension (for polynomial $t$) and
therefore leads to small learning error. One consequence of our results is
that, in arbitrary single-parameter settings, one can learn a mechanism with
expected revenue arbitrarily close to optimal from a polynomial number of
samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03688</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03688</id><created>2015-06-11</created><authors><author><keyname>Adiceam</keyname><forenames>Faustin</forenames></author><author><keyname>Beresnevich</keyname><forenames>Victor</forenames></author><author><keyname>Levesley</keyname><forenames>Jason</forenames></author><author><keyname>Velani</keyname><forenames>Sanju</forenames></author><author><keyname>Zorin</keyname><forenames>Evgeniy</forenames></author></authors><title>Diophantine Approximation and applications in Interference Alignment</title><categories>math.NT cs.IT math.IT</categories><comments>44 pages</comments><msc-class>Primary 11J83, Secondary 11J13, 11K60, 94A12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is motivated by recent applications of Diophantine approximation
in electronics, in particular, in the rapidly developing area of Interference
Alignment. Some remarkable advances in this area give substantial credit to the
fundamental Khintchine-Groshev Theorem and, in particular, to its far reaching
generalisation for submanifolds of a Euclidean space. With a view towards the
aforementioned applications, here we introduce and prove quantitative explicit
generalisations of the Khintchine-Groshev Theorem for non-degenerate
submanifolds of $\mathbb{R}^n$. The importance of such quantitative statements
is explicitly discussed in Section 4.7.1 of Jafar's monograph `Interference
Alignment - A New Look at Signal Dimensions in a Communication Network',
Foundations and Trends in Communications and Information Theory, Vol. 7, no. 1,
2010.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03693</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03693</id><created>2015-06-11</created><updated>2015-12-02</updated><authors><author><keyname>Meeds</keyname><forenames>Edward</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Optimization Monte Carlo: Efficient and Embarrassingly Parallel
  Likelihood-Free Inference</title><categories>cs.LG stat.ML</categories><comments>NIPS 2015 camera ready</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an embarrassingly parallel, anytime Monte Carlo method for
likelihood-free models. The algorithm starts with the view that the
stochasticity of the pseudo-samples generated by the simulator can be
controlled externally by a vector of random numbers u, in such a way that the
outcome, knowing u, is deterministic. For each instantiation of u we run an
optimization procedure to minimize the distance between summary statistics of
the simulator and the data. After reweighing these samples using the prior and
the Jacobian (accounting for the change of volume in transforming from the
space of summary statistics to the space of parameters) we show that this
weighted ensemble represents a Monte Carlo estimate of the posterior
distribution. The procedure can be run embarrassingly parallel (each node
handling one sample) and anytime (by allocating resources to the worst
performing sample). The procedure is validated on six experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03694</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03694</id><created>2015-06-11</created><updated>2015-06-19</updated><authors><author><keyname>Chrupa&#x142;a</keyname><forenames>Grzegorz</forenames></author><author><keyname>K&#xe1;d&#xe1;r</keyname><forenames>&#xc1;kos</forenames></author><author><keyname>Alishahi</keyname><forenames>Afra</forenames></author></authors><title>Learning language through pictures</title><categories>cs.CL</categories><comments>To appear at ACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Imaginet, a model of learning visually grounded representations of
language from coupled textual and visual input. The model consists of two Gated
Recurrent Unit networks with shared word embeddings, and uses a multi-task
objective by receiving a textual description of a scene and trying to
concurrently predict its visual representation and the next word in the
sentence. Mimicking an important aspect of human language learning, it acquires
meaning representations for individual words from descriptions of visual
scenes. Moreover, it learns to effectively use sequential structure in semantic
interpretation of multi-word phrases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03701</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03701</id><created>2015-06-11</created><updated>2015-08-11</updated><authors><author><keyname>Egger</keyname><forenames>Katharina</forenames></author><author><keyname>Majdak</keyname><forenames>Piotr</forenames></author><author><keyname>Laback</keyname><forenames>Bernhard</forenames></author></authors><title>Channel Interaction and Current Level Affect Across-Electrode
  Integration of Interaural Time Differences in Bilateral Cochlear-Implant
  Listeners</title><categories>cs.SD</categories><comments>submitted to JARO on 3 November 2014, Resubmitted revised version on
  1 June 2015, Resubmitted revised version on 3 August 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensitivity to ITDs is important for sound localization. Normal-hearing
listeners benefit from across-frequency processing, as seen with improved ITD
thresholds when consistent ITD cues are presented over a range of frequency
channels compared to when ITD information is only presented in a single
frequency channel. This study aimed to clarify whether cochlear-implant (CI)
listeners can make use of similar processing when being stimulated with
multiple interaural electrode pairs transmitting consistent ITD information.
ITD thresholds for unmodulated, 100-pulse-per-second pulse trains were measured
in seven bilateral CI listeners using research interfaces. Consistent ITDs were
presented at either one or two electrode pairs at different current levels,
allowing for comparisons at either constant level per component electrode or
equal overall loudness. Different tonotopic distances between the pairs were
tested in order to clarify the potential influence of channel interaction.
Comparison of ITD thresholds between double pairs and the respective single
pairs revealed systematic effects of tonotopic separation and current level. At
constant levels, performance with double-pair stimulation improved compared to
single-pair stimulation, but only for large tonotopic separation. Comparisons
at equal overall loudness revealed no benefit from presenting ITD information
at two electrode pairs for any tonotopic spacing. Irrespective of
electrode-pair configuration, ITD sensitivity improved with increasing current
level. Hence, the improved ITD sensitivity for double pairs found for a large
tonotopic separation and constant current levels seems to be due to increased
loudness. The overall data suggest that CI listeners can benefit from combining
consistent ITD information across multiple electrodes, provided sufficient
stimulus levels and that stimulating electrode pairs are widely spaced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03705</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03705</id><created>2015-06-11</created><updated>2015-06-12</updated><authors><author><keyname>Mroueh</keyname><forenames>Youssef</forenames></author><author><keyname>Rennie</keyname><forenames>Steven</forenames></author><author><keyname>Goel</keyname><forenames>Vaibhava</forenames></author></authors><title>Random Maxout Features</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose and study random maxout features, which are
constructed by first projecting the input data onto sets of randomly generated
vectors with Gaussian elements, and then outputing the maximum projection value
for each set. We show that the resulting random feature map, when used in
conjunction with linear models, allows for the locally linear estimation of the
function of interest in classification tasks, and for the locally linear
embedding of points when used for dimensionality reduction or data
visualization. We derive generalization bounds for learning that assess the
error in approximating locally linear functions by linear functions in the
maxout feature space, and empirically evaluate the efficacy of the approach on
the MNIST and TIMIT classification tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03710</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03710</id><created>2015-06-11</created><authors><author><keyname>Cappai</keyname><forenames>Alberto</forenames></author><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames></author></authors><title>On Equivalences, Metrics, and Polynomial Time (Long Version)</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactive behaviors are ubiquitous in modern cryptography, but are also
present in $\lambda$-calculi, in the form of higher-order constructions.
Traditionally, however, typed $\lambda$-calculi simply do not fit well into
cryptography, being both deterministic and too powerful as for the complexity
of functions they can express. We study interaction in a $\lambda$-calculus for
probabilistic polynomial time computable functions. In particular, we show how
notions of context equivalence and context metric can both be characterized by
way of traces when defined on linear contexts. We then give evidence on how
this can be turned into a proof methodology for computational
indistinguishability, a key notion in modern cryptography. We also hint at what
happens if a more general notion of a context is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03724</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03724</id><created>2015-06-11</created><authors><author><keyname>Chee</keyname><forenames>Yeow Meng</forenames></author><author><keyname>Kiah</keyname><forenames>Han Mao</forenames></author><author><keyname>Purkayastha</keyname><forenames>Punarbasu</forenames></author><author><keyname>Sol&#xe9;</keyname><forenames>Patrick</forenames></author></authors><title>Product Construction of Affine Codes</title><categories>cs.IT cs.DM math.IT</categories><comments>13 pages, to appear in SIAM Journal on Discrete Mathematics</comments><msc-class>94B05, 94B60 (Primary), 94B20 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary matrix codes with restricted row and column weights are a desirable
method of coded modulation for power line communication. In this work, we
construct such matrix codes that are obtained as products of affine codes -
cosets of binary linear codes. Additionally, the constructions have the
property that they are systematic. Subsequently, we generalize our construction
to irregular product of affine codes, where the component codes are affine
codes of different rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03726</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03726</id><created>2015-06-11</created><updated>2016-02-18</updated><authors><author><keyname>Grenet</keyname><forenames>Bruno</forenames></author></authors><title>Lacunaryx: Computing bounded-degree factors of lacunary polynomials</title><categories>cs.MS cs.SC</categories><comments>6 pages</comments><journal-ref>ACM Communications in Computer Algebra 49(4), 2015</journal-ref><doi>10.1145/2893803.2893807</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we report on an implementation in the free software Mathemagix
of lacunary factorization algorithms, distributed as a library called
Lacunaryx. These algorithms take as input a polynomial in sparse
representation, that is as a list of nonzero monomials, and an integer $d$, and
compute its irreducible degree-$\le d$ factors. The complexity of these
algorithms is polynomial in the sparse size of the input polynomial and $d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03728</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03728</id><created>2015-06-11</created><authors><author><keyname>Hoffmann</keyname><forenames>Udo</forenames></author><author><keyname>Kleist</keyname><forenames>Linda</forenames></author><author><keyname>Miltzow</keyname><forenames>Tillmann</forenames></author></authors><title>Upper and Lower Bounds on Long Dual-Paths in Line Arrangements</title><categories>math.CO cs.CG cs.DM</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a line arrangement $\cal A$ with $n$ lines, we show that there exists a
path of length $n^2/3 - O(n)$ in the dual graph of $\cal A$ formed by its
faces. This bound is tight up to lower order terms. For the bicolored version,
we describe an example of a line arrangement with $3k$ blue and $2k$ red lines
with no alternating path longer than $14k$. Further, we show that any line
arrangement with $n$ lines has a coloring such that it has an alternating path
of length $\Omega (n^2/ \log n)$. Our results also hold for pseudoline
arrangements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03729</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03729</id><created>2015-06-11</created><authors><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author><author><keyname>Sandon</keyname><forenames>Colin</forenames></author></authors><title>Recovering communities in the general stochastic block model without
  knowing the parameters</title><categories>math.PR cs.IT cs.LG cs.SI math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1503.00609</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most recent developments on the stochastic block model (SBM) rely on the
knowledge of the model parameters, or at least on the number of communities.
This paper introduces efficient algorithms that do not require such knowledge
and yet achieve the optimal information-theoretic tradeoffs identified in
[AS15] for linear size communities. The results are three-fold: (i) in the
constant degree regime, an algorithm is developed that requires only a
lower-bound on the relative sizes of the communities and detects communities
with an optimal accuracy scaling for large degrees; (ii) in the regime where
degrees are scaled by $\omega(1)$ (diverging degrees), this is enhanced into a
fully agnostic algorithm that only takes the graph in question and
simultaneously learns the model parameters (including the number of
communities) and detects communities with accuracy $1-o(1)$, with an overall
quasi-linear complexity; (iii) in the logarithmic degree regime, an agnostic
algorithm is developed that learns the parameters and achieves the optimal
CH-limit for exact recovery, in quasi-linear time. These provide the first
algorithms affording efficiency, universality and information-theoretic
optimality for strong and weak consistency in the general SBM with linear size
communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03736</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03736</id><created>2015-06-11</created><updated>2015-11-18</updated><authors><author><keyname>Ndiaye</keyname><forenames>Eugene</forenames></author><author><keyname>Fercoq</keyname><forenames>Olivier</forenames></author><author><keyname>Gramfort</keyname><forenames>Alexandre</forenames></author><author><keyname>Salmon</keyname><forenames>Joseph</forenames></author></authors><title>GAP Safe screening rules for sparse multi-task and multi-class models</title><categories>stat.ML cs.LG math.OC stat.CO</categories><comments>in Proceedings of the 29-th Conference on Neural Information
  Processing Systems (NIPS), 2015</comments><msc-class>68Uxx, 49N15, 62Jxx, 68Q32, 62-04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High dimensional regression benefits from sparsity promoting regularizations.
Screening rules leverage the known sparsity of the solution by ignoring some
variables in the optimization, hence speeding up solvers. When the procedure is
proven not to discard features wrongly the rules are said to be \emph{safe}. In
this paper we derive new safe rules for generalized linear models regularized
with $\ell_1$ and $\ell_1/\ell_2$ norms. The rules are based on duality gap
computations and spherical safe regions whose diameters converge to zero. This
allows to discard safely more variables, in particular for low regularization
parameters. The GAP Safe rule can cope with any iterative solver and we
illustrate its performance on coordinate descent for multi-task Lasso, binary
and multinomial logistic regression, demonstrating significant speed ups on all
tested datasets with respect to previous safe rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03744</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03744</id><created>2015-06-11</created><updated>2015-06-13</updated><authors><author><keyname>Tiwari</keyname><forenames>Shashank</forenames></author><author><keyname>Das</keyname><forenames>Suvra Sekhar</forenames></author><author><keyname>Bandyopadhyay</keyname><forenames>Kalyan Kumar</forenames></author></authors><title>Precoded GFDM System to Combat Inter Carrier Interference : Performance
  Analysis</title><categories>cs.IT math.IT</categories><comments>accepted in IET Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The expected operating scenarios of 5G pose a great challenge to orthogonal
frequency division multiplexing (OFDM) which has poor out of band (OoB)
spectral properties, stringent synchronization requirements, and large symbol
duration. Generalized frequency division multiplexing (GFDM) which is the focus
of this work, has been suggested in the literature as one of the possible
solutions to meet 5G requirements. In this work, the analytical performance
evaluation of MMSE receiver for GFDM is presented. We also proposed precoding
techniques to enhance the performance of GFDM. A simplified expression of SINR
for MMSE receiver of GFDM is derived using special properties related to the
modulation matrix of GFDM, which are described in this work. This SINR is used
to evaluate the BER performance. Precoding schemes are proposed to reduce
complexity of GFDM-MMSE receiver without compromising on the performance. Block
Inverse Discrete Fourier Transform (BIDFT) and Discrete Fourier Transform (DFT)
based precoding schemes are found to outperform GFDM-MMSE receiver due to
frequency diversity gain while having complexity similar to zero-forcing
receiver of GFDM. It is shown that both BIDFT and DFT-based precoding schemes
reduce peak to average power ratio (PAPR) significantly. Computational
complexity of different transmitters and receivers of precoded and uncoded GFDM
is also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03753</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03753</id><created>2015-06-11</created><updated>2015-06-20</updated><authors><author><keyname>Gupta</keyname><forenames>Udit</forenames></author></authors><title>Application of Multi factor authentication in Internet of Things domain</title><categories>cs.CR</categories><comments>6 pages, 1 table</comments><doi>10.5120/ijca2015905221</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Authentication forms the gateway to any secure system. Together with
integrity, confidentiality and authorization it helps in preventing any sort of
intrusions into the system. Up until a few years back password based
authentication was the most common form of authentication to any secure
network. But with the advent of more sophisticated technologies this form of
authentication although still widely used has become insecure. Furthermore,
with the rise of 'Internet of Things' where the number of devices would grow
manifold it would be infeasible for user to remember innumerable passwords.
Therefore, it's important to address this concern by devising ways in which
multiple forms of authentication would be required to gain access to any smart
devices and at the same time its usability would be high. In this paper, a
methodology is discussed as to what kind of authentication mechanisms could be
deployed in internet of things (IOT).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03760</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03760</id><created>2015-06-11</created><authors><author><keyname>Chitnis</keyname><forenames>Rajesh</forenames></author><author><keyname>Esfandiari</keyname><forenames>Hossein</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>Khandekar</keyname><forenames>Rohit</forenames></author><author><keyname>Kortsarz</keyname><forenames>Guy</forenames></author><author><keyname>Seddighin</keyname><forenames>Saeed</forenames></author></authors><title>A Tight Algorithm for Strongly Connected Steiner Subgraph On Two
  Terminals With Demands</title><categories>cs.DS cs.CC cs.DM</categories><comments>An extended abstract appeared in IPEC '14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an edge-weighted directed graph $G=(V,E)$ on $n$ vertices and a set
$T=\{t_1, t_2, \ldots, t_p\}$ of $p$ terminals, the objective of the \scss
($p$-SCSS) problem is to find an edge set $H\subseteq E$ of minimum weight such
that $G[H]$ contains an $t_{i}\rightarrow t_j$ path for each $1\leq i\neq j\leq
p$. In this paper, we investigate the computational complexity of a variant of
$2$-SCSS where we have demands for the number of paths between each terminal
pair. Formally, the \sharinggeneral problem is defined as follows: given an
edge-weighted directed graph $G=(V,E)$ with weight function $\omega:
E\rightarrow \mathbb{R}^{\geq 0}$, two terminal vertices $s, t$, and integers
$k_1, k_2$ ; the objective is to find a set of $k_1$ paths $F_1, F_2, \ldots,
F_{k_1}$ from $s\leadsto t$ and $k_2$ paths $B_1, B_2, \ldots, B_{k_2}$ from
$t\leadsto s$ such that $\sum_{e\in E} \omega(e)\cdot \phi(e)$ is minimized,
where $\phi(e)= \max \Big\{|\{i\in [k_1] : e\in F_i\}|\ ,\ |\{j\in [k_2] : e\in
B_j\}|\Big\}$. For each $k\geq 1$, we show the following:
  \begin{itemize}
  \item The \sharing problem can be solved in $n^{O(k)}$ time.
  \item A matching lower bound for our algorithm: the \sharing problem does not
have an $f(k)\cdot n^{o(k)}$ algorithm for any computable function $f$, unless
the Exponential Time Hypothesis (ETH) fails.
  \end{itemize}
  Our algorithm for \sharing relies on a structural result regarding an optimal
solution followed by using the idea of a &quot;token game&quot; similar to that of
Feldman and Ruhl. We show with an example that the structural result does not
hold for the \sharinggeneral problem if $\min\{k_1, k_2\}\geq 2$. Therefore
\sharing is the most general problem one can attempt to solve with our
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03767</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03767</id><created>2015-06-11</created><authors><author><keyname>Rippel</keyname><forenames>Oren</forenames></author><author><keyname>Snoek</keyname><forenames>Jasper</forenames></author><author><keyname>Adams</keyname><forenames>Ryan P.</forenames></author></authors><title>Spectral Representations for Convolutional Neural Networks</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discrete Fourier transforms provide a significant speedup in the computation
of convolutions in deep learning. In this work, we demonstrate that, beyond its
advantages for efficient computation, the spectral domain also provides a
powerful representation in which to model and train convolutional neural
networks (CNNs).
  We employ spectral representations to introduce a number of innovations to
CNN design. First, we propose spectral pooling, which performs dimensionality
reduction by truncating the representation in the frequency domain. This
approach preserves considerably more information per parameter than other
pooling strategies and enables flexibility in the choice of pooling output
dimensionality. This representation also enables a new form of stochastic
regularization by randomized modification of resolution. We show that these
methods achieve competitive results on classification and approximation tasks,
without using any dropout or max-pooling.
  Finally, we demonstrate the effectiveness of complex-coefficient spectral
parameterization of convolutional filters. While this leaves the underlying
model unchanged, it results in a representation that greatly facilitates
optimization. We observe on a variety of popular CNN configurations that this
leads to significantly faster convergence during training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03771</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03771</id><created>2015-06-11</created><authors><author><keyname>Gomez</keyname><forenames>Javier V.</forenames></author><author><keyname>Alvarez</keyname><forenames>David</forenames></author><author><keyname>Garrido</keyname><forenames>Santiago</forenames></author><author><keyname>Moreno</keyname><forenames>Luis</forenames></author></authors><title>Fast Methods for Eikonal Equations: an Experimental Survey</title><categories>cs.NA cs.RO</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The Fast Marching Method is a very popular algorithm to compute
times-of-arrival maps (distances map measured in time units). Since their
proposal in 1995, it has been applied to many different applications such as
robotics, medical computer vision, fluid simulation, etc. Many alternatives
have been proposed with two main objectives: to reduce its computational time
and to improve its accuracy. In this paper, we collect the main approaches
which improve the computational time of the standard Fast Marching Method,
focusing on single-threaded methods and isotropic environments. 9 different
methods are studied under a common mathematical framework and experimentally in
representative environments: Fast Marching Method with binary heap, Fast
Marching Method with Fibonacci Heap, Simplified Fast Marching Method, Untidy
Fast Marching Method, Fast Iterative Method, Group Marching Method, Fast
Sweeping Method, Lock Sweeping Method and Double Dynamic Queue Method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03774</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03774</id><created>2015-06-11</created><authors><author><keyname>Liang</keyname><forenames>Jingwen</forenames></author><author><keyname>Sankar</keyname><forenames>Lalitha</forenames></author><author><keyname>Kosut</keyname><forenames>Oliver</forenames></author></authors><title>Vulnerability Analysis and Consequences of False Data Injection Attack
  on Power System State Estimation</title><categories>cs.SY cs.CR</categories><comments>9 pages, 7 figures. A version of this manuscript was submitted to the
  IEEE Transactions on Power Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An unobservable false data injection (FDI) attack on AC state estimation (SE)
is introduced and its consequences on the physical system are studied. With a
focus on understanding the physical consequences of FDI attacks, a bi-level
optimization problem is introduced whose objective is to maximize the physical
line flows subsequent to an FDI attack on DC SE. The maximization is subject to
constraints on both attacker resources (size of attack) and attack detection
(limiting load shifts) as well as those required by DC optimal power flow (OPF)
following SE. The resulting attacks are tested on a more realistic non-linear
system model using AC state estimation and ACOPF, and it is shown that, with an
appropriately chosen sub-network, the attacker can overload transmission lines
with moderate shifts of load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03775</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03775</id><created>2015-06-11</created><authors><author><keyname>Biyani</keyname><forenames>Prakhar</forenames></author><author><keyname>Caragea</keyname><forenames>Cornelia</forenames></author><author><keyname>Bhamidipati</keyname><forenames>Narayan</forenames></author></authors><title>Entity-Specific Sentiment Classification of Yahoo News Comments</title><categories>cs.CL cs.IR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sentiment classification is widely used for product reviews and in online
social media such as forums, Twitter, and blogs. However, the problem of
classifying the sentiment of user comments on news sites has not been addressed
yet. News sites cover a wide range of domains including politics, sports,
technology, and entertainment, in contrast to other online social sites such as
forums and review sites, which are specific to a particular domain. A user
associated with a news site is likely to post comments on diverse topics (e.g.,
politics, smartphones, and sports) or diverse entities (e.g., Obama, iPhone, or
Google). Classifying the sentiment of users tied to various entities may help
obtain a holistic view of their personality, which could be useful in
applications such as online advertising, content personalization, and political
campaign planning. In this paper, we formulate the problem of entity-specific
sentiment classification of comments posted on news articles in Yahoo News and
propose novel features that are specific to news comments. Experimental results
show that our models outperform state-of-the-art baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03777</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03777</id><created>2015-06-10</created><authors><author><keyname>Xu</keyname><forenames>Siyao</forenames></author></authors><title>Reversible Logic Synthesis with Minimal Usage of Ancilla Bits</title><categories>cs.ET quant-ph</categories><comments>50 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible logic has attracted much research interest over the last few
decades, especially due to its application in quantum computing. In the
construction of reversible gates from basic gates, ancilla bits are commonly
used to remove restrictions on the type of gates that a certain set of basic
gates generates. With unlimited ancilla bits, many gates (such as Toffoli and
Fredkin) become universal reversible gates. However, ancilla bits can be
expensive to implement, thus making the problem of minimizing necessary ancilla
bits a practical topic.
  This thesis explores the problem of reversible logic synthesis using a single
base gate and a few ancilla bits. Two base gates are discussed: a variation of
the 3-bit Toffoli gate and the original 3-bit Fredkin gate. There are three
main results associated with these two gates: i) the variated Toffoli gate can
generate all n-bit reversible gates using 1 ancilla bit, ii) the variated
Toffoli can generate all n-bit reversible gates that are even permutations
using no ancilla bit, iii) the Fredkin gate can generate all n-bit conservative
reversible gates using 1 ancilla bit. Prior to this paper, the best known
result for general universality requires three basic gates, and the best known
result for conservative universality needs 5 ancilla bits. The second result is
trivially optimal. For the first and the third result, we explicitly prove
their optimality: the variated Toffoli cannot generate all n-bit reversible
gates without using any extra input lines, and the Fredkin gate cannot generate
all n-bit conservative reversible gates without using extra input lines. We
also explore a stronger version of the second converse by introducing a new
concept called borrowed bits, and prove that the Fredkin gate cannot generate
all n-bit conservative reversible gates without ancilla bits, even with an
unlimited number of borrowed bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03782</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03782</id><created>2015-06-11</created><authors><author><keyname>Firer</keyname><forenames>Marcelo</forenames></author><author><keyname>Walker</keyname><forenames>Judy L.</forenames></author></authors><title>Matched Metrics and Channels</title><categories>cs.IT math.CO math.IT</categories><msc-class>68P30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most common decision criteria for decoding are maximum likelihood
decoding and nearest neighbor decoding. It is well-known that maximum
likelihood decoding coincides with nearest neighbor decoding with respect to
the Hamming metric on the binary symmetric channel. In this work we study
channels and metrics for which those two criteria do and do not coincide for
general codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03792</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03792</id><created>2015-06-11</created><authors><author><keyname>Mahmood</keyname><forenames>Rafid</forenames></author><author><keyname>Badr</keyname><forenames>Ahmed</forenames></author><author><keyname>Khisti</keyname><forenames>Ashish</forenames></author></authors><title>Convolutional Codes with Maximum Column Sum Rank for Network Streaming</title><categories>cs.IT math.IT</categories><comments>15 pages, presented in part at ISIT 2015, submitted to IEEE
  Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The column Hamming distance of a convolutional code determines the error
correction capability when streaming over a class of packet erasure channels.
We show that the column sum rank parallels column Hamming distance when
streaming over a network with link failures. We prove rank analogues of several
known column Hamming distance properties and introduce a new family of
convolutional codes that maximize the column sum rank up to the code memory.
Our construction involves finding a class of super-regular matrices that
preserve this property after multiplication with non-singular block diagonal
matrices in the ground field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03797</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03797</id><created>2015-06-11</created><authors><author><keyname>Cavanna</keyname><forenames>Nicholas J.</forenames></author><author><keyname>Jahanseir</keyname><forenames>Mahmoodreza</forenames></author><author><keyname>Sheehy</keyname><forenames>Donald R.</forenames></author></authors><title>A Geometric Perspective on Sparse Filtrations</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a geometric perspective on sparse filtrations used in topological
data analysis. This new perspective leads to much simpler proofs, while also
being more general, applying equally to Rips filtrations and Cech filtrations
for any convex metric. We also give an algorithm for finding the simplices in
such a filtration and prove that the vertex removal can be implemented as a
sequence of elementary edge collapses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03799</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03799</id><created>2015-06-11</created><authors><author><keyname>Jourabloo</keyname><forenames>Amin</forenames></author><author><keyname>Liu</keyname><forenames>Xiaoming</forenames></author></authors><title>Pose-Invariant 3D Face Alignment</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face alignment aims to estimate the locations of a set of landmarks for a
given image. This problem has received much attention as evidenced by the
recent advancement in both the methodology and performance. However, most of
the existing works neither explicitly handle face images with arbitrary poses,
nor perform large-scale experiments on non-frontal and profile face images. In
order to address these limitations, this paper proposes a novel face alignment
algorithm that estimates both 2D and 3D landmarks and their 2D visibilities for
a face image with an arbitrary pose. By integrating a 3D deformable model, a
cascaded coupled-regressor approach is designed to estimate both the camera
projection matrix and the 3D landmarks. Furthermore, the 3D model also allows
us to automatically estimate the 2D landmark visibilities via surface normals.
We gather a substantially larger collection of all-pose face images to evaluate
our algorithm and demonstrate superior performances than the state-of-the-art
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03805</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03805</id><created>2015-06-11</created><updated>2015-10-15</updated><authors><author><keyname>Lakshminarayanan</keyname><forenames>Balaji</forenames></author><author><keyname>Roy</keyname><forenames>Daniel M.</forenames></author><author><keyname>Teh</keyname><forenames>Yee Whye</forenames></author></authors><title>Mondrian Forests for Large-Scale Regression when Uncertainty Matters</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real-world regression problems demand a measure of the uncertainty
associated with each prediction. Standard decision forests deliver efficient
state-of-the-art predictive performance, but high-quality uncertainty estimates
are lacking. Gaussian processes (GPs) deliver uncertainty estimates, but
scaling GPs to large-scale data sets comes at the cost of approximating the
uncertainty estimates. We extend Mondrian forests, first proposed by
Lakshminarayanan et al. (2014) for classification problems, to the large-scale
non-parametric regression setting. Using a novel hierarchical Gaussian prior
that dovetails with the Mondrian forest framework, we obtain principled
uncertainty estimates, while still retaining the computational advantages of
decision forests. Through a combination of illustrative examples, real-world
large-scale datasets, and Bayesian optimization benchmarks, we demonstrate that
Mondrian forests outperform approximate GPs on large-scale regression tasks and
deliver better-calibrated uncertainty assessments than decision-forest-based
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03828</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03828</id><created>2015-06-11</created><authors><author><keyname>Mitre-Hern&#xe1;ndez</keyname><forenames>Hugo A.</forenames></author><author><keyname>Mora-Soto</keyname><forenames>Arturo</forenames></author><author><keyname>L&#xf3;pez-Portillo</keyname><forenames>H&#xe9;ctor P&#xe9;rez</forenames></author><author><keyname>Lara-Alvarez</keyname><forenames>Carlos</forenames></author></authors><title>Strategies for fostering Knowledge Management Programs in Public
  Organizations</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge Management (KM) is an approach to achieving strategic objectives by
visualizing, sharing, and using intangible resources of an organization and its
stakeholders. There are many studies that analyze specific factors for the
successful implementation of KM programs, and the evaluation of such factors is
considered a strategic tool for Public Organizations (POs) for efficiently
directing the implementation of a KM program. Nevertheless, there are cultural
problems such as weak trust, bad collaboration; technological problems as KM
systems difficult to use, nor interconnected or interoperable; and strategic
problems as political changes, not inter-administration continuity or lack of
political willingness. In this research we provide an overview of the key
factors for facilitating the implementation of KM programs. To this end, we
conducted a Systematic Literature Review (SLR) of success factors to create a
KM program for POs. Analyzing the twenty related studies we summarize benefits
and quality attributes of KM. Moreover, we obtained from an ongoing project
evaluation results of the KM factors ranked cultural, technological and
strategic. The results show that the Mexican POs have strategic and
technological issues such as: a misalignment between knowledge management and
organizational goals, it seems that POs are barely reusing their knowledge to
execute their daily activities or to take decisions, and is not possible to
know who and how a knowledge asset has been used. Finally, according to the
gaps and difficulties of the SLR, we provide strategies to successfully
implement KM programs in Mexico.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03830</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03830</id><created>2015-06-11</created><authors><author><keyname>Mosso</keyname><forenames>Juan Manuel R.</forenames></author></authors><title>Ciberseguridad Inteligente</title><categories>cs.CR</categories><comments>21 pages, in Spanish</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet Economy has a strong dependency on cyberspace. This raises
security risk scenarios due to the increasing number of vulnerabilities and the
increased frequency and sophistication of cyber attacks, especially with the
advent of advanced threats of APT type. This paper presents a model of
Intelligent Cybersecurity (ICS) for detect, deny, disrupt, degrade, deceive and
destroy enemy capabilities in cyberspace. This is achieved through the
conceptual and technical development of a Capacity for Cyber Intelligence (CCI)
which aims to interfere destructively C2 capabilities of the adversary,
penetrating its decision loops with the speed necessary to displace him to a
reactive posture. Finally, unlike the security models raised classically, the
concept of ICSI suggests that the advantage in the conflict can be obtained by
defense and not always by the attacker. As theoretical support, the &quot;Offensive
System Reference Model&quot; (OSRM) is presented, which is used to think cyber
conflict at all levels, from a perspective coordinated and synchronized with
the rest of the traditional forces under the present set; and a justification
of the capacity from the modern perspective C2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03837</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03837</id><created>2015-06-11</created><authors><author><keyname>Zhang</keyname><forenames>Weinan</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author></authors><title>Statistical Arbitrage Mining for Display Advertising</title><categories>cs.GT cs.DB</categories><comments>In the proceedings of the 21st ACM SIGKDD international conference on
  Knowledge discovery and data mining (KDD 2015)</comments><doi>10.1145/2783258.2783269</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We study and formulate arbitrage in display advertising. Real-Time Bidding
(RTB) mimics stock spot exchanges and utilises computers to algorithmically buy
display ads per impression via a real-time auction. Despite the new automation,
the ad markets are still informationally inefficient due to the heavily
fragmented marketplaces. Two display impressions with similar or identical
effectiveness (e.g., measured by conversion or click-through rates for a
targeted audience) may sell for quite different prices at different market
segments or pricing schemes. In this paper, we propose a novel data mining
paradigm called Statistical Arbitrage Mining (SAM) focusing on mining and
exploiting price discrepancies between two pricing schemes. In essence, our
SAMer is a meta-bidder that hedges advertisers' risk between CPA (cost per
action)-based campaigns and CPM (cost per mille impressions)-based ad
inventories; it statistically assesses the potential profit and cost for an
incoming CPM bid request against a portfolio of CPA campaigns based on the
estimated conversion rate, bid landscape and other statistics learned from
historical data. In SAM, (i) functional optimisation is utilised to seek for
optimal bidding to maximise the expected arbitrage net profit, and (ii) a
portfolio-based risk management solution is leveraged to reallocate bid volume
and budget across the set of campaigns to make a risk and return trade-off. We
propose to jointly optimise both components in an EM fashion with high
efficiency to help the meta-bidder successfully catch the transient statistical
arbitrage opportunities in RTB. Both the offline experiments on a real-world
large-scale dataset and online A/B tests on a commercial platform demonstrate
the effectiveness of our proposed solution in exploiting arbitrage in various
model settings and market environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03838</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03838</id><created>2015-06-11</created><authors><author><keyname>Chen</keyname><forenames>Jiehua</forenames></author><author><keyname>Pruhs</keyname><forenames>Kirk</forenames></author><author><keyname>Woeginger</keyname><forenames>Gerhard J.</forenames></author></authors><title>The one-dimensional Euclidean domain: Finitely many obstructions are not
  enough</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that one-dimensional Euclidean preference profiles can not be
characterized in terms of finitely many forbidden substructures. This result is
in strong contrast to the case of single-peaked and single-crossing preference
profiles, for which such finite characterizations have been derived in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03843</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03843</id><created>2015-06-11</created><authors><author><keyname>Ivan</keyname><forenames>Szabolcs</forenames></author></authors><title>Algebraic characterization of temporal logics on forests</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We associate a temporal logic $\mathrm{FL}(\mathcal{L})$ to class
$\mathcal{L}$ of (regular) forest languages where a forest is an ordered finite
sum of finite unranked trees. Under a natural assumption of the set
$\mathcal{L}$ of modalities we derive an algebraic characterization of the
forest languages definable in $\mathrm{FL}(\mathcal{L})$, in terms of the
iterated &quot;Moore product&quot; of forest automata. Using this characterization we
derive a polynomial-time algorithm to decide definability of the fragment
$\mathrm{EF}^*$ of $\mathrm{CTL}$, evaluated on forests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03844</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03844</id><created>2015-06-11</created><updated>2015-07-04</updated><authors><author><keyname>Bedo</keyname><forenames>Marcos</forenames></author><author><keyname>Blanco</keyname><forenames>Gustavo</forenames></author><author><keyname>Oliveira</keyname><forenames>Willian</forenames></author><author><keyname>Cazzolato</keyname><forenames>Mirela</forenames></author><author><keyname>Costa</keyname><forenames>Alceu</forenames></author><author><keyname>Rodrigues</keyname><forenames>Jose</forenames></author><author><keyname>Traina</keyname><forenames>Agma</forenames></author><author><keyname>Traina</keyname><forenames>Caetano</forenames><suffix>Jr</suffix></author></authors><title>Techniques for effective and efficient fire detection from social media
  images</title><categories>cs.CV</categories><comments>12 pages, Proceedings of the International Conference on Enterprise
  Information Systems. Specifically: Marcos Bedo, Gustavo Blanco, Willian
  Oliveira, Mirela Cazzolato, Alceu Costa, Jose Rodrigues, Agma Traina, Caetano
  Traina, 2015, Techniques for effective and efficient fire detection from
  social media images, ICEIS, 34-45</comments><journal-ref>Int Conf on Enterp Inf Systems 34-45 SCITEPRESS (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media could provide valuable information to support decision making in
crisis management, such as in accidents, explosions and fires. However, much of
the data from social media are images, which are uploaded in a rate that makes
it impossible for human beings to analyze them. Despite the many works on image
analysis, there are no fire detection studies on social media. To fill this
gap, we propose the use and evaluation of a broad set of content-based image
retrieval and classification techniques for fire detection. Our main
contributions are: (i) the development of the Fast-Fire Detection method
(FFDnR), which combines feature extractor and evaluation functions to support
instance-based learning, (ii) the construction of an annotated set of images
with ground-truth depicting fire occurrences -- the FlickrFire dataset, and
(iii) the evaluation of 36 efficient image descriptors for fire detection.
Using real data from Flickr, our results showed that FFDnR was able to achieve
a precision for fire detection comparable to that of human annotators.
Therefore, our work shall provide a solid basis for further developments on
monitoring images from social media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03847</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03847</id><created>2015-06-11</created><authors><author><keyname>Rodrigues</keyname><forenames>Jose</forenames></author><author><keyname>Tong</keyname><forenames>Hanghang</forenames></author><author><keyname>Traina</keyname><forenames>Agma</forenames></author><author><keyname>Faloutsos</keyname><forenames>Christos</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author></authors><title>GMine: A System for Scalable, Interactive Graph Visualization and Mining</title><categories>cs.SI</categories><comments>4 pages, Proceedings of the 32nd international conference on Very
  Large Data Bases, 2006. Specifically: Jose Rodrigues, Hanghang Tong, Agma
  Traina, Christos Faloutsos, Jure Leskovec (2006), GMine: A System for
  Scalable, Interactive Graph Visualization and Mining; 32nd International
  Conference on Very Large Data Bases (VLDB) 1195-1198</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several graph visualization tools exist. However, they are not able to handle
large graphs, and/or they do not allow interaction. We are interested on large
graphs, with hundreds of thousands of nodes. Such graphs bring two challenges:
the first one is that any straightforward interactive manipulation will be
prohibitively slow. The second one is sensory overload: even if we could plot
and replot the graph quickly, the user would be overwhelmed with the vast
volume of information because the screen would be too cluttered as nodes and
edges overlap each other. GMine system addresses both these issues, by using
summarization and multi-resolution. GMine offers multi-resolution graph
exploration by partitioning a given graph into a hierarchy of
com-munities-within-communities and storing it into a novel R-tree-like
structure which we name G-Tree. GMine offers summarization by implementing an
innovative subgraph extraction algorithm and then visualizing its output.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03852</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03852</id><created>2015-06-11</created><authors><author><keyname>Hu</keyname><forenames>Shell X.</forenames></author><author><keyname>Williams</keyname><forenames>Christopher K. I.</forenames></author><author><keyname>Todorovic</keyname><forenames>Sinisa</forenames></author></authors><title>Tree-Cut for Probabilistic Image Segmentation</title><categories>stat.ML cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new probabilistic generative model for image
segmentation, i.e. the task of partitioning an image into homogeneous regions.
Our model is grounded on a mid-level image representation, called a region
tree, in which regions are recursively split into subregions until superpixels
are reached. Given the region tree, image segmentation is formalized as
sampling cuts in the tree from the model. Inference for the cuts is exact, and
formulated using dynamic programming. Our tree-cut model can be tuned to sample
segmentations at a particular scale of interest out of many possible multiscale
image segmentations. This generalizes the common notion that there should be
only one correct segmentation per image. Also, it allows moving beyond the
standard single-scale evaluation, where the segmentation result for an image is
averaged against the corresponding set of coarse and fine human annotations, to
conduct a scale-specific evaluation. Our quantitative results are comparable to
those of the leading gPb-owt-ucm method, with the notable advantage that we
additionally produce a distribution over all possible tree-consistent
segmentations of the image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03857</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03857</id><created>2015-06-11</created><authors><author><keyname>Lu</keyname><forenames>Wei</forenames></author><author><keyname>Di Renzo</keyname><forenames>Marco</forenames></author></authors><title>Stochastic Geometry Modeling of Cellular Networks: Analysis, Simulation
  and Experimental Validation</title><categories>cs.IT math.IT</categories><comments>submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the increasing heterogeneity and deployment density of emerging
cellular networks, new flexible and scalable approaches for their modeling,
simulation, analysis and optimization are needed. Recently, a new approach has
been proposed: it is based on the theory of point processes and it leverages
tools from stochastic geometry for tractable system-level modeling, performance
evaluation and optimization. In this paper, we investigate the accuracy of this
emerging abstraction for modeling cellular networks, by explicitly taking
realistic base station locations, building footprints, spatial blockages and
antenna radiation patterns into account. More specifically, the base station
locations and the building footprints are taken from two publicly available
databases from the United Kingdom. Our study confirms that the abstraction
model based on stochastic geometry is capable of accurately modeling the
communication performance of cellular networks in dense urban environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03865</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03865</id><created>2015-06-11</created><authors><author><keyname>Piva</keyname><forenames>Breno</forenames></author><author><keyname>de Souza</keyname><forenames>Cid C.</forenames></author></authors><title>Counterexample for the 2-approximation of finding partitions of
  rectilinear polygons with minimum stabbing number</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a counterexample for the approximation algorithm proposed
by Durocher and Mehrabi [1] for the general problem of finding a rectangular
partition of a rectilinear polygon with minimum stabbing number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03872</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03872</id><created>2015-06-11</created><updated>2015-06-18</updated><authors><author><keyname>Ballard</keyname><forenames>Grey</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author><author><keyname>Kolda</keyname><forenames>Tamara G.</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author></authors><title>Diamond Sampling for Approximate Maximum All-pairs Dot-product (MAD)
  Search</title><categories>cs.SI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two sets of vectors, $A = \{{a_1}, \dots, {a_m}\}$ and
$B=\{{b_1},\dots,{b_n}\}$, our problem is to find the top-$t$ dot products,
i.e., the largest $|{a_i}\cdot{b_j}|$ among all possible pairs. This is a
fundamental mathematical problem that appears in numerous data applications
involving similarity search, link prediction, and collaborative filtering. We
propose a sampling-based approach that avoids direct computation of all $mn$
dot products. We select diamonds (i.e., four-cycles) from the weighted
tripartite representation of $A$ and $B$. The probability of selecting a
diamond corresponding to pair $(i,j)$ is proportional to $({a_i}\cdot{b_j})^2$,
amplifying the focus on the largest-magnitude entries. Experimental results
indicate that diamond sampling is orders of magnitude faster than direct
computation and requires far fewer samples than any competing approach. We also
apply diamond sampling to the special case of maximum inner product search, and
get significantly better results than the state-of-the-art hashing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03874</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03874</id><created>2015-06-11</created><authors><author><keyname>Geneson</keyname><forenames>Jesse T.</forenames></author><author><keyname>Tian</keyname><forenames>Peter M.</forenames></author></authors><title>Extremal Functions of Forbidden Multidimensional Matrices</title><categories>math.CO cs.DM</categories><msc-class>05D99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pattern avoidance is a central topic in graph theory and combinatorics.
Pattern avoidance in matrices has applications in computer science and
engineering, such as robot motion planning and VLSI circuit design. A
$d$-dimensional zero-one matrix $A$ avoids another $d$-dimensional zero-one
matrix $P$ if no submatrix of $A$ can be transformed to $P$ by changing some
ones to zeros. A fundamental problem is to study the maximum number of nonzero
entries in a $d$-dimensional $n \times \cdots \times n$ matrix that avoids $P$.
This maximum number, denoted by $f(n,P,d)$, is called the extremal function.
  We advance the extremal theory of matrices in two directions. The methods
that we use come from combinatorics, probability, and analysis. Firstly, we
obtain non-trivial lower and upper bounds on $f(n,P,d)$ when $n$ is large for
every $d$-dimensional block permutation matrix $P$. We establish the tight
bound $\Theta(n^{d-1})$ on $f(n,P,d)$ for every $d$-dimensional tuple
permutation matrix $P$. This tight bound has the lowest possible order that an
extremal function of a nontrivial matrix can ever achieve. Secondly, we show
that $f(n,P,d)$ is super-homogeneous for a class of matrices $P$. We use this
super-homogeneity to show that the limit inferior of the sequence $\{ {f(n,P,d)
\over n^{d-1}}\}$ has a lower bound $2^{\Omega(k^{1/ d})}$ for a family of $k
\times \cdots \times k$ permutation matrices $P$. We also improve the upper
bound on the limit superior from $2^{O(k \log k)}$ to $2^{O(k)}$ for all $k
\times \cdots \times k$ permutation matrices and show that the new upper bound
also holds for tuple permutation matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03877</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03877</id><created>2015-06-11</created><updated>2016-01-03</updated><authors><author><keyname>Bornschein</keyname><forenames>Jorg</forenames></author><author><keyname>Shabanian</keyname><forenames>Samira</forenames></author><author><keyname>Fischer</keyname><forenames>Asja</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Training Bidirectional Helmholtz Machines</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient unsupervised training and inference in deep generative models
remains a challenging problem. One basic approach, called Helmholtz machine,
involves training a top-down directed generative model together with a
bottom-up auxiliary model that is trained to help perform approximate
inference. Recent results indicate that better results can be obtained with
better approximate inference procedures. Instead of employing more powerful
procedures, we here propose to regularize the generative model to stay close to
the class of distributions that can be efficiently inverted by the approximate
inference model. We achieve this by interpreting both the top-down and the
bottom-up directed models as approximate inference distributions and by
defining the model distribution to be the geometric mean of these two. We
present a lower-bound for the likelihood of this model and we show that
optimizing this bound regularizes the model so that the Bhattacharyya distance
between the bottom-up and top-down approximate distributions is minimized. We
demonstrate that we can use this approach to fit generative models with many
layers of hidden binary stochastic variables to complex training distributions
and hat this method prefers significantly deeper architectures while it
supports orders of magnitude more efficient approximate inference than other
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03879</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03879</id><created>2015-06-11</created><updated>2015-06-14</updated><authors><author><keyname>Xu</keyname><forenames>Ji</forenames></author><author><keyname>Wang</keyname><forenames>Guoyin</forenames></author></authors><title>Leading Tree in DPCLUS and Its Impact on Building Hierarchies</title><categories>cs.AI</categories><comments>11 Pages, 5 figures. It is a very fundamental topic with respect to
  the research (clustering by fast search and find of density peaks)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reveals the tree structure as an intermediate result of clustering
by fast search and find of density peaks (DPCLUS), and explores the power of
using this tree to perform hierarchical clustering. The array used to hold the
index of the nearest higher-densitied object for each object can be transformed
into a Leading Tree (LT), in which each parent node P leads its child nodes to
join the same cluster as P itself, and the child nodes are sorted by their
gamma values in descendant order to accelerate the disconnecting of root in
each subtree. There are two major advantages with the LT: One is dramatically
reducing the running time of assigning noncenter data points to their cluster
ID, because the assigning process is turned into just disconnecting the links
from each center to its parent. The other is that the tree model for
representing clusters is more informative. Because we can check which objects
are more likely to be selected as centers in finer grained clustering, or which
objects reach to its center via less jumps. Experiment results and analysis
show the effectiveness and efficiency of the assigning process with an LT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03883</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03883</id><created>2015-06-11</created><authors><author><keyname>Berwanger</keyname><forenames>Dietmar</forenames></author><author><keyname>Mathew</keyname><forenames>Anup Basil</forenames></author><author><keyname>Bogaard</keyname><forenames>Marie van den</forenames></author></authors><title>Hierarchical Information Patterns and Distributed Strategy Synthesis</title><categories>cs.GT cs.DC</categories><msc-class>91A06, 93C41</msc-class><acm-class>C.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Infinite games with imperfect information tend to be undecidable unless the
information flow is severely restricted. One fundamental decidable case occurs
when there is a total ordering among players, such that each player has access
to all the information that the following ones receive.
  In this paper we consider variations of this hierarchy principle for
synchronous games with perfect recall, and identify new decidable classes for
which the distributed synthesis problem is solvable with finite-state
strategies. In particular, we show that decidability is maintained when the
information hierarchy may change along the play, or when transient phases
without hierarchical information are allowed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03885</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03885</id><created>2015-06-11</created><authors><author><keyname>Berwanger</keyname><forenames>Dietmar</forenames></author><author><keyname>Bogaard</keyname><forenames>Marie van den</forenames></author></authors><title>Games with Delays. A Frankenstein Approach</title><categories>cs.GT</categories><msc-class>91A20, 93C41</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate infinite games on finite graphs where the information flow is
perturbed by nondeterministic signalling delays. It is known that such
perturbations make synthesis problems virtually unsolvable, in the general
case. On the classical model where signals are attached to states, tractable
cases are rare and difficult to identify.
  Here, we propose a model where signals are detached from control states, and
we identify a subclass on which equilibrium outcomes can be preserved, even if
signals are delivered with a delay that is finitely bounded. To offset the
perturbation, our solution procedure combines responses from a collection of
virtual plays following an equilibrium strategy in the instant- signalling game
to synthesise, in a Frankenstein manner, an equivalent equilibrium strategy for
the delayed-signalling game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03899</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03899</id><created>2015-06-12</created><authors><author><keyname>Liao</keyname><forenames>Yiyi</forenames></author><author><keyname>Kodagoda</keyname><forenames>Sarath</forenames></author><author><keyname>Wang</keyname><forenames>Yue</forenames></author><author><keyname>Shi</keyname><forenames>Lei</forenames></author><author><keyname>Liu</keyname><forenames>Yong</forenames></author></authors><title>Place classification with a graph regularized deep neural network model</title><categories>cs.RO cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Place classification is a fundamental ability that a robot should possess to
carry out effective human-robot interactions. It is a nontrivial classification
problem which has attracted many research. In recent years, there is a high
exploitation of Artificial Intelligent algorithms in robotics applications.
Inspired by the recent successes of deep learning methods, we propose an
end-to-end learning approach for the place classification problem. With the
deep architectures, this methodology automatically discovers features and
contributes in general to higher classification accuracies. The pipeline of our
approach is composed of three parts. Firstly, we construct multiple layers of
laser range data to represent the environment information in different levels
of granularity. Secondly, each layer of data is fed into a deep neural network
model for classification, where a graph regularization is imposed to the deep
architecture for keeping local consistency between adjacent samples. Finally,
the predicted labels obtained from all the layers are fused based on confidence
trees to maximize the overall confidence. Experimental results validate the
effective- ness of our end-to-end place classification framework in which both
the multi-layer structure and the graph regularization promote the
classification performance. Furthermore, results show that the features
automatically learned from the raw input range data can achieve competitive
results to the features constructed based on statistical and geometrical
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03914</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03914</id><created>2015-06-12</created><authors><author><keyname>Zechner</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Marussig</keyname><forenames>Benjamin</forenames></author><author><keyname>Beer</keyname><forenames>Gernot</forenames></author><author><keyname>Fries</keyname><forenames>Thomas-Peter</forenames></author></authors><title>The Isogeometric Nystr\&quot;om Method</title><categories>cs.NA math.NA</categories><comments>21 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the isogeometric Nystr\&quot;om method is presented. It's
outstanding features are: it allows the analysis of domains described by many
different geometrical mapping methods in computer aided geometric design and it
requires only pointwise function evaluations just like isogeometric collocation
methods. The analysis of the computational domain is carried out by means of
boundary integral equations, therefor only the boundary representation is
required. The method is thoroughly integrated into the isogeometric framework.
For example, the regularization of the arising singular integrals performed
with local correction as well as the interpolation of the pointwise existing
results are carried out by means of Bezier elements.
  The presented isogeometric Nystr\&quot;om method is applied to practical problems
solved by the Laplace and the Lame-Navier equation. Numerical tests show higher
order convergence in two and three dimensions. It is concluded that the
presented approach provides a simple and flexible alternative to currently used
methods for solving boundary integral equations, but has some limitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03929</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03929</id><created>2015-06-12</created><updated>2015-08-31</updated><authors><author><keyname>Tseliou</keyname><forenames>Georgia</forenames></author><author><keyname>Adelantado</keyname><forenames>Ferran</forenames></author><author><keyname>Verikoukis</keyname><forenames>Christos</forenames></author></authors><title>Scalable RAN Virtualization in Multi-Tenant LTE-A Heterogeneous Networks
  (Extended version)</title><categories>cs.NI</categories><comments>40 pages (including Appendices), Accepted for publication in the IEEE
  Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular communications are evolving to facilitate the current and expected
increasing needs of Quality of Service (QoS), high data rates and diversity of
offered services. Towards this direction, Radio Access Network (RAN)
virtualization aims at providing solutions of mapping virtual network elements
onto radio resources of the existing physical network. This paper proposes the
Resources nEgotiation for NEtwork Virtualization (RENEV) algorithm, suitable
for application in Heterogeneous Networks (HetNets) in Long Term
Evolution-Advanced (LTE-A) environments, consisting of a macro evolved NodeB
(eNB) overlaid with small cells. By exploiting Radio Resource Management (RRM)
principles, RENEV achieves slicing and on demand delivery of resources.
Leveraging the multi-tenancy approach, radio resources are transferred in terms
of physical radio Resource Blocks (RBs) among multiple heterogeneous base
stations, interconnected via the X2 interface. The main target is to deal with
traffic variations in geographical dimension. All signaling design
considerations under the current Third Generation Partnership Project (3GPP)
LTE-A architecture are also investigated. Analytical studies and simulation
experiments are conducted to evaluate RENEV in terms of network's throughput as
well as its additional signaling overhead. Moreover we show that RENEV can be
applied independently on top of already proposed schemes for RAN virtualization
to improve their performance. The results indicate that significant merits are
achieved both from network's and users' perspective as well as that it is a
scalable solution for different number of small cells.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03930</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03930</id><created>2015-06-12</created><authors><author><keyname>Konecny</keyname><forenames>Jan</forenames></author><author><keyname>Krupka</keyname><forenames>Michal</forenames></author></authors><title>Complete relations on fuzzy complete lattices</title><categories>cs.LO</categories><comments>Preprint submitted to Fuzzy Sets and Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the notion of complete binary relation on complete lattice to
residuated lattice valued ordered sets and show its properties. Then we focus
on complete fuzzy tolerances on fuzzy complete lattices and prove they are in
one-to-one correspondence with extensive isotone Galois connections. Finally,
we prove that fuzzy complete lattice, factorized by a complete fuzzy tolerance,
is again a fuzzy complete lattice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03932</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03932</id><created>2015-06-12</created><authors><author><keyname>Zhan</keyname><forenames>Xiu-Xiu</forenames></author><author><keyname>Liu</keyname><forenames>Chuang</forenames></author><author><keyname>Zhou</keyname><forenames>Ge</forenames></author><author><keyname>Zhang</keyname><forenames>Zi-Ke</forenames></author><author><keyname>Sun</keyname><forenames>Gui-Quan</forenames></author><author><keyname>Zhu</keyname><forenames>Jonathan J. H.</forenames></author></authors><title>Mutual Feedback Between Epidemic Spreading and Information Diffusion</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The impact that information diffusion has on epidemic spreading has recently
attracted much attention. As a disease begins to spread in the population,
information about the disease is transmitted to others, which in turn has an
effect on the spread of disease. In this paper, using empirical results of the
propagation of H7N9 and information about the disease, we clearly show that the
spreading dynamics of the two-types of processes influence each other. We build
a mathematical model in which both types of spreading dynamics are described
using the SIS process in order to illustrate the influence of information
diffusion on epidemic spreading. Both the simulation results and the pairwise
analysis reveal that information diffusion can increase the threshold of an
epidemic outbreak, decrease the final fraction of infected individuals and
significantly decrease the rate at which the epidemic propagates. Additionally,
we find that the multi-outbreak phenomena of epidemic spreading, along with the
impact of information diffusion, is consistent with the empirical results.
These findings highlight the requirement to maintain social awareness of
diseases even when the epidemics seem to be under control in order to prevent a
subsequent outbreak. These results may shed light on the in-depth understanding
of the interplay between the dynamics of epidemic spreading and information
diffusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03935</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03935</id><created>2015-06-12</created><updated>2015-09-16</updated><authors><author><keyname>Ramezanpour</keyname><forenames>A.</forenames></author><author><keyname>Moghimi-Araghi</keyname><forenames>S.</forenames></author></authors><title>Statistical physics of loopy interactions: Independent-loop
  approximation and beyond</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.DM cs.DS</categories><comments>21 pages, 6 figures, 2 appendices, typos corrected</comments><journal-ref>Phys. Rev. E 92, 032112 (2015)</journal-ref><doi>10.1103/PhysRevE.92.032112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an interacting system of spin variables on a loopy interaction
graph, identified by a tree graph and a set of loopy interactions. We start
from a high-temperature expansion for loopy interactions represented by a sum
of nonnegative contributions from all the possible frustration-free loop
configurations. We then compute the loop corrections using different
approximations for the nonlocal loop interactions induced by the spin
correlations in the tree graph. For distant loopy interactions, we can exploit
the exponential decay of correlations in the tree interaction graph to compute
loop corrections within an independent-loop approximation. Higher orders of the
approximation are obtained by considering the correlations between the nearby
loopy interactions involving larger number of spin variables. In particular,
the sum over the loop configurations can be computed &quot;exactly&quot; by the belief
propagation algorithm in the low orders of the approximation as long as the
loopy interactions have a tree structure. These results might be useful in
developing more accurate and convergent message-passing algorithms exploiting
the structure of loopy interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03936</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03936</id><created>2015-06-12</created><authors><author><keyname>Majd</keyname><forenames>Mahshid</forenames></author><author><keyname>Shoeleh</keyname><forenames>Farzaneh</forenames></author></authors><title>A Novel Hybrid Approach for Cephalometric Landmark Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cephalometric analysis has an important role in dentistry and especially in
orthodontics as a treatment planning tool to gauge the size and special
relationships of the teeth, jaws and cranium. The first step of using such
analyses is localizing some important landmarks known as cephalometric
landmarks on craniofacial in x-ray image. The past decade has seen a growing
interest in automating this process. In this paper, a novel hybrid approach is
proposed for automatic detection of cephalometric landmarks. Here, the
landmarks are categorized into three main sets according to their anatomical
characteristics and usage in well-known cephalometric analyses. Consequently,
to have a reliable and accurate detection system, three methods named edge
tracing, weighted template matching, and analysis based estimation are
designed, each of which is consistent and well-suited for one category. Edge
tracing method is suggested to predict those landmarks which are located on
edges. Weighted template matching method is well-suited for landmarks located
in an obvious and specific structure which can be extracted or searchable in a
given x-ray image. The last but not the least method is named analysis based
estimation. This method is based on the fact that in cephalometric analyses the
relations between landmarks are used and the locations of some landmarks are
never used individually. Therefore the third suggested method has a novelty in
estimating the desired relations directly. The effectiveness of the proposed
approach is compared with the state of the art methods and the results were
promising especially in real world applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03942</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03942</id><created>2015-06-12</created><authors><author><keyname>Lu</keyname><forenames>Longfei</forenames></author></authors><title>Optimal $\gamma$ and $C$ for $\epsilon$-Support Vector Regression with
  RBF Kernels</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this study is to investigate the efficient determination of
$C$ and $\gamma$ for Support Vector Regression with RBF or mahalanobis kernel
based on numerical and statistician considerations, which indicates the
connection between $C$ and kernels and demonstrates that the deviation of
geometric distance of neighbour observation in mapped space effects the predict
accuracy of $\epsilon$-SVR. We determinate the arrange of $\gamma$ &amp; $C$ and
propose our method to choose their best values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03943</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03943</id><created>2015-06-12</created><updated>2015-10-22</updated><authors><author><keyname>Blanqui</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>INRIA</affiliation></author><author><keyname>Jouannaud</keyname><forenames>Jean-Pierre</forenames><affiliation>Ecole Polytechnique, Universit&#xe9; Paris-Sud, Tsinghua University</affiliation></author><author><keyname>Rubio</keyname><forenames>Albert</forenames><affiliation>Technical University of Catalonia</affiliation></author></authors><title>The computability path ordering</title><categories>cs.LO</categories><proxy>LMCS</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at carrying out termination proofs for simply typed
higher-order calculi automatically by using ordering comparisons. To this end,
we introduce the computability path ordering (CPO), a recursive relation on
terms obtained by lifting a precedence on function symbols. A first version,
core CPO, is essentially obtained from the higher-order recursive path ordering
(HORPO) by eliminating type checks from some recursive calls and by
incorporating the treatment of bound variables as in the com-putability
closure. The well-foundedness proof shows that core CPO captures the essence of
computability arguments \'a la Tait and Girard, therefore explaining its name.
We further show that no further type check can be eliminated from its recursive
calls without loosing well-foundedness, but for one for which we found no
counterexample yet. Two extensions of core CPO are then introduced which allow
one to consider: the first, higher-order inductive types; the second, a
precedence in which some function symbols are smaller than application and
abstraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03944</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03944</id><created>2015-06-12</created><authors><author><keyname>Kozlov</keyname><forenames>D. N.</forenames></author></authors><title>Combinatorial topology of the standard chromatic subdivision and Weak
  Symmetry Breaking for 6 processes</title><categories>cs.DC math.CO</categories><journal-ref>Springer INdAM series, volume on configuration spaces, 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study a family of discrete configuration spaces, the
so-called protocol complexes, which are of utmost importance in theoretical
distributed computing. Specifically, we consider questions of the existance of
compliant binary labelings on the vertices of iterated standard chromatic
subdivisions of an n-simplex. The existance of such labelings is equivalent to
the existance of distributed protocols solving Weak Symmetry Breaking task in
the standard computational model.
  As a part of our formal model, we introduce function sb(n), defined for
natural numbers n, called the symmetry breaking function. From the geometric
point of view sb(n) denotes the minimal number of iterations of the standard
chromatic subdivision of an (n-1)-simplex, which is needed for the compliant
binary labeling to exist. From the point of distributed computing, the function
sb(n) measures the minimal number of rounds in a protocol solving the Weak
Symmetry Breaking task.
  In addition to the development of combinatorial topology, which is applicable
in a broader context, our main contribution is the proof of new bounds for the
function sb(n). Accordingly, the bulk of the paper is taken up by in-depth
analysis of the structure of adjacency graph on the set of n-simplices in
iterated standard chromatic subdivision of an n-simplex. On the algorithmic
side, we provide the first distributed protocol solving Weak Symmetry Breaking
task in the layered immediate snapshot computational model for some number of
processes.
  It is well known, that the smallest number of processes for which Weak
Symmetry Breaking task is solvable is 6. Based on our analysis, we are able to
find a very fast explicit protocol, solving the Weak Symmetry Breaking for 6
processes using only 3 rounds. Furthermore, we show that no protocol can solve
Weak Symmetry Breaking in fewer than 2 rounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03949</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03949</id><created>2015-06-12</created><authors><author><keyname>Uiterwijk</keyname><forenames>Jos</forenames></author><author><keyname>Barton</keyname><forenames>Michael</forenames></author></authors><title>New Results for Domineering from Combinatorial Game Theory Endgame
  Databases</title><categories>cs.AI cs.GT</categories><doi>10.1016/j.tcs.2015.05.017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have constructed endgame databases for all single-component positions up
to 15 squares for Domineering, filled with exact Combinatorial Game Theory
(CGT) values in canonical form. The most important findings are as follows.
  First, as an extension of Conway's [8] famous Bridge Splitting Theorem for
Domineering, we state and prove another theorem, dubbed the Bridge Destroying
Theorem for Domineering. Together these two theorems prove very powerful in
determining the CGT values of large positions as the sum of the values of
smaller fragments, but also to compose larger positions with specified values
from smaller fragments. Using the theorems, we then prove that for any dyadic
rational number there exist Domineering positions with that value.
  Second, we investigate Domineering positions with infinitesimal CGT values,
in particular ups and downs, tinies and minies, and nimbers. In the databases
we find many positions with single or double up and down values, but no ups and
downs with higher multitudes. However, we prove that such single-component ups
and downs easily can be constructed. Further, we find Domineering positions
with 11 different tinies and minies values. For each we give an example. Next,
for nimbers we find many Domineering positions with values up to *3. This is
surprising, since Drummond-Cole [10] suspected that no *2 and *3 positions in
standard Domineering would exist. We show and characterize many *2 and *3
positions. Finally, we give some Domineering positions with values interesting
for other reasons.
  Third, we have investigated the temperature of all positions in our
databases. There appears to be exactly one position with temperature 2 (as
already found before) and no positions with temperature larger than 2. This
supports Berlekamp's conjecture that 2 is the highest possible temperature in
Domineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03950</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03950</id><created>2015-06-12</created><updated>2015-06-16</updated><authors><author><keyname>Bichhawat</keyname><forenames>Abhishek</forenames></author><author><keyname>Rajani</keyname><forenames>Vineet</forenames></author><author><keyname>Garg</keyname><forenames>Deepak</forenames></author><author><keyname>Hammer</keyname><forenames>Christian</forenames></author></authors><title>Generalizing Permissive-Upgrade in Dynamic Information Flow Analysis</title><categories>cs.CR cs.PL</categories><doi>10.1145/2637113.2637116</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Preventing implicit information flows by dynamic program analysis requires
coarse approximations that result in false positives, because a dynamic monitor
sees only the executed trace of the program. One widely deployed method is the
no-sensitive-upgrade check, which terminates a program whenever a variable's
taint is upgraded (made more sensitive) due to a control dependence on tainted
data. Although sound, this method is restrictive, e.g., it terminates the
program even if the upgraded variable is never used subsequently. To counter
this, Austin and Flanagan introduced the permissive-upgrade check, which allows
a variable upgrade due to control dependence, but marks the variable
&quot;partially-leaked&quot;. The program is stopped later if it tries to use the
partially-leaked variable. Permissive-upgrade handles the dead-variable
assignment problem and remains sound. However, Austin and Flanagan develop
permissive-upgrade only for a two-point (low-high) security lattice and
indicate a generalization to pointwise products of such lattices. In this
paper, we develop a non-trivial and non-obvious generalization of
permissive-upgrade to arbitrary lattices. The key difficulty lies in finding a
suitable notion of partial leaks that is both sound and permissive and in
developing a suitable definition of memory equivalence that allows an inductive
proof of soundness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03963</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03963</id><created>2015-06-12</created><authors><author><keyname>Qin</keyname><forenames>Xiaolin</forenames></author><author><keyname>Tang</keyname><forenames>Juan</forenames></author><author><keyname>Feng</keyname><forenames>Yong</forenames></author><author><keyname>Bachmann</keyname><forenames>Bernhard</forenames></author><author><keyname>Fritzson</keyname><forenames>Peter</forenames></author></authors><title>Efficient algorithm for computing large scale systems of differential
  algebraic equations</title><categories>cs.NA cs.SY</categories><comments>16 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In many mathematical models of physical phenomenons and engineering fields,
such as electrical circuits or mechanical multibody systems, which generate the
differential algebraic equations (DAEs) systems naturally. In general, the
feature of DAEs is a sparse large scale system of fully nonlinear and high
index. To make use of its sparsity, this paper provides a simple and efficient
algorithm for computing the large scale DAEs system. We exploit the shortest
augmenting path algorithm for finding maximum value transversal (MVT) as well
as block triangular forms (BTF). We also present the extended signature matrix
method with the block fixed point iteration and its complexity results.
Furthermore, a range of nontrivial problems are demonstrated by our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03973</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03973</id><created>2015-06-12</created><updated>2015-10-10</updated><authors><author><keyname>Biziou-van-Pol</keyname><forenames>Laura</forenames></author><author><keyname>Haenen</keyname><forenames>Jana</forenames></author><author><keyname>Novaro</keyname><forenames>Arianna</forenames></author><author><keyname>Liberman</keyname><forenames>Andr&#xe9;s Occhipinti</forenames></author><author><keyname>Capraro</keyname><forenames>Valerio</forenames></author></authors><title>Does telling white lies signal pro-social preferences?</title><categories>physics.soc-ph cs.GT q-bio.PE</categories><comments>Forthcoming in Judgment and Decision Making</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The opportunity to tell a white lie (i.e., a lie that benefits another
person) generates a moral conflict between two opposite moral dictates, one
pushing towards telling always the truth and the other pushing towards helping
others. Here we study how people resolve this moral conflict. What does telling
a white lie signal about a person's pro-social tendencies? To answer this
question, we conducted a two-stage 2x2 experiment. In the first stage, we used
a Deception Game to measure aversion to telling a Pareto white lie (i.e., a lie
that helps both the liar and the listener), and aversion to telling an
altruistic white lie (i.e., a lie that helps the listener at the expense of the
liar). In the second stage we measured altruistic tendencies using a Dictator
Game and cooperative tendencies using a Prisoner's dilemma. We found three
major results: (i) both altruism and cooperation are positively correlated with
aversion to telling a Pareto white lie; (ii) both altruism and cooperation are
negatively correlated with aversion to telling an altruistic white lie; (iii)
men are more likely than women to tell an altruistic white lie, but not to tell
a Pareto white lie. Our results shed light on the moral conflit between
pro-sociality and truth-telling. In particular, the first finding suggests that
a significant proportion of people have non-distributional notions of what the
right thing to do is: irrespective of their economic consequences, they tell
the truth, they cooperate, they share their money.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03995</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03995</id><created>2015-06-12</created><authors><author><keyname>Kol&#xe1;&#x159;</keyname><forenames>Martin</forenames></author><author><keyname>Hradi&#x161;</keyname><forenames>Michal</forenames></author><author><keyname>Zem&#x10d;&#xed;k</keyname><forenames>Pavel</forenames></author></authors><title>Technical Report: Image Captioning with Semantically Similar Images</title><categories>cs.CV</categories><comments>3 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents our submission to the MS COCO Captioning Challenge 2015.
The method uses Convolutional Neural Network activations as an embedding to
find semantically similar images. From these images, the most typical caption
is selected based on unigram frequencies. Although the method received low
scores with automated evaluation metrics and in human assessed average
correctness, it is competitive in the ratio of captions which pass the Turing
test and which are assessed as better or equal to human captions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03997</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03997</id><created>2015-06-12</created><authors><author><keyname>Wittmann</keyname><forenames>Markus</forenames></author><author><keyname>Zeiser</keyname><forenames>Thomas</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>Short Note on Costs of Floating Point Operations on current x86-64
  Architectures: Denormals, Overflow, Underflow, and Division by Zero</title><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simple floating point operations like addition or multiplication on
normalized floating point values can be computed by current AMD and Intel
processors in three to five cycles. This is different for denormalized numbers,
which appear when an underflow occurs and the value can no longer be
represented as a normalized floating-point value. Here the costs are about two
magnitudes higher.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.03998</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.03998</id><created>2015-06-12</created><authors><author><keyname>Ferdowsi</keyname><forenames>Sohrab</forenames></author><author><keyname>Voloshynovskiy</keyname><forenames>Svyatoslav</forenames></author><author><keyname>Kostadinov</keyname><forenames>Dimche</forenames></author></authors><title>Sparse Multi-layer Image Approximation: Facial Image Compression</title><categories>cs.CV cs.IT math.IT</categories><comments>Submitted to the MLSP 2015</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We propose a scheme for multi-layer representation of images. The problem is
first treated from an information-theoretic viewpoint where we analyze the
behavior of different sources of information under a multi-layer data
compression framework and compare it with a single-stage (shallow) structure.
We then consider the image data as the source of information and link the
proposed representation scheme to the problem of multi-layer dictionary
learning for visual data. For the current work we focus on the problem of image
compression for a special class of images where we report a considerable
performance boost in terms of PSNR at high compression ratios in comparison
with the JPEG2000 codec.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04002</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04002</id><created>2015-06-12</created><authors><author><keyname>Shoeleh</keyname><forenames>Farzaneh</forenames></author><author><keyname>Majd</keyname><forenames>Mahshid</forenames></author><author><keyname>Hamzeh</keyname><forenames>Ali</forenames></author><author><keyname>Hashemi</keyname><forenames>Sattar</forenames></author></authors><title>Knowledge Representation in Learning Classifier Systems: A Review</title><categories>cs.NE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge representation is a key component to the success of all rule based
systems including learning classifier systems (LCSs). This component brings
insight into how to partition the problem space what in turn seeks prominent
role in generalization capacity of the system as a whole. Recently, knowledge
representation component has received great deal of attention within data
mining communities due to its impacts on rule based systems in terms of
efficiency and efficacy. The current work is an attempt to find a comprehensive
and yet elaborate view into the existing knowledge representation techniques in
LCS domain in general and XCS in specific. To achieve the objectives, knowledge
representation techniques are grouped into different categories based on the
classification approach in which they are incorporated. In each category, the
underlying rule representation schema and the format of classifier condition to
support the corresponding representation are presented. Furthermore, a precise
explanation on the way that each technique partitions the problem space along
with the extensive experimental results is provided. To have an elaborated view
on the functionality of each technique, a comparative analysis of existing
techniques on some conventional problems is provided. We expect this survey to
be of interest to the LCS researchers and practitioners since it provides a
guideline for choosing a proper knowledge representation technique for a given
problem and also opens up new streams of research on this topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04006</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04006</id><created>2015-06-12</created><updated>2015-07-06</updated><authors><author><keyname>Vahdati</keyname><forenames>Sahar</forenames></author><author><keyname>Karim</keyname><forenames>Farah</forenames></author><author><keyname>Huang</keyname><forenames>Jyun-Yao</forenames></author><author><keyname>Lange</keyname><forenames>Christoph</forenames></author></authors><title>Mapping Large Scale Research Metadata to Linked Data: A Performance
  Comparison of HBase, CSV and XML</title><categories>cs.DB cs.DL cs.PF</categories><comments>Accepted in 0th Metadata and Semantics Research Conference</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  OpenAIRE, the Open Access Infrastructure for Research in Europe, comprises a
database of all EC FP7 and H2020 funded research projects, including metadata
of their results (publications and datasets). These data are stored in an HBase
NoSQL database, post-processed, and exposed as HTML for human consumption, and
as XML through a web service interface. As an intermediate format to facilitate
statistical computations, CSV is generated internally. To interlink the
OpenAIRE data with related data on the Web, we aim at exporting them as Linked
Open Data (LOD). The LOD export is required to integrate into the overall data
processing workflow, where derived data are regenerated from the base data
every day. We thus faced the challenge of identifying the best-performing
conversion approach.We evaluated the performances of creating LOD by a
MapReduce job on top of HBase, by mapping the intermediate CSV files, and by
mapping the XML output.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04013</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04013</id><created>2015-06-12</created><updated>2016-01-21</updated><authors><author><keyname>Y&#xfc;ksel</keyname><forenames>Serdar</forenames></author></authors><title>Stationary and Ergodic Properties of Stochastic Non-Linear Systems
  Controlled over Communication Channels</title><categories>math.PR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the following problem: Given a stochastic
non-linear system controlled over a noisy channel, what is the largest class of
channels for which there exist coding and control policies so that the closed
loop system is stochastically stable? Stochastic stability notions considered
are stationarity, ergodicity or asymptotic mean stationarity. We do not
restrict the state space to be compact, for example systems considered can be
driven by unbounded noise. Necessary and sufficient conditions are obtained for
a large class of systems and channels. A generalization of Bode's Integral
Formula for a large class of non-linear systems and information channels is
obtained. The findings generalize existing results for linear systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04014</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04014</id><created>2015-06-12</created><updated>2015-12-15</updated><authors><author><keyname>Piddock</keyname><forenames>Stephen</forenames></author><author><keyname>Montanaro</keyname><forenames>Ashley</forenames></author></authors><title>The complexity of antiferromagnetic interactions and 2D lattices</title><categories>quant-ph cs.CC</categories><comments>35 pages, 11 figures; v2 added references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimation of the minimum eigenvalue of a quantum Hamiltonian can be
formalised as the Local Hamiltonian problem. We study the natural special case
of the Local Hamiltonian problem where the same 2-local interaction, with
differing weights, is applied across each pair of qubits. First we consider
antiferromagnetic/ferromagnetic interactions, where the weights of the terms in
the Hamiltonian are restricted to all be of the same sign. We show that for
symmetric 2-local interactions with no 1-local part, the problem is either
QMA-complete or in StoqMA. In particular the antiferromagnetic Heisenberg and
antiferromagnetic XY interactions are shown to be QMA-complete. We also prove
StoqMA-completeness of the antiferromagnetic transverse field Ising model.
Second, we study the Local Hamiltonian problem under the restriction that the
interaction terms can only be chosen to lie on a particular graph. We prove
that nearly all of the QMA-complete 2-local interactions remain QMA-complete
when restricted to a 2D square lattice. Finally we consider both restrictions
at the same time and discover that, with the exception of the antiferromagnetic
Heisenberg interaction, all of the interactions which are QMA-complete with
positive coefficients remain QMA-complete when restricted to a 2D triangular
lattice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04036</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04036</id><created>2015-06-12</created><updated>2015-07-08</updated><authors><author><keyname>Mart&#xed;nez-Pe&#xf1;as</keyname><forenames>Umberto</forenames></author></authors><title>On the similarities between generalized rank and Hamming weights and
  their applications to network coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rank weights and generalized rank weights have been proven to characterize
error and erasure correction, and information leakage in linear network coding,
in the same way as the Hamming weights and generalized Hamming weights describe
classical error and erasure correction, and information leakage in secret
sharing. Many similarities between both cases have been studied in the
literature. Although many definitions, results and proofs are similar in both
cases, it might seem that they are essentially different. The aim of this paper
is to further relate both weights and generalized weights, show that the
results and proofs in both cases are actually essentially the same, and see the
significance of these similarities in network coding. Some of the new results
in the rank case also have new consequences in the Hamming case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04047</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04047</id><created>2015-06-12</created><authors><author><keyname>Etesami</keyname><forenames>Seyed Rasoul</forenames></author><author><keyname>Basar</keyname><forenames>Tamer</forenames></author></authors><title>Approximation Algorithm for the Binary-Preference Capacitated Selfish
  Replication Game and a Tight Bound on its Price of Anarchy</title><categories>cs.GT cs.DM cs.MA math.CO math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the capacitated selfish replication (CSR) game with binary
preferences, over general undirected networks. We first show that such games
have an associated ordinary potential function, and hence always admit a
pure-strategy Nash equilibrium (NE). Further, when the minimum degree of the
network and the number of resources are of the same order, there exists an
exact polynomial time algorithm which can find a NE. Following this, we study
the price of anarchy of such games, and show that it is bounded above by 3; we
further provide some instances for which the price of anarchy is at least 2. We
develop a quasi-polynomial algorithm O(n^2D^{ln n}), where n is the number of
players and D is the diameter of the network, which can find, in a distributed
manner, an allocation profile that is within a constant factor of the optimal
allocation, and hence of any pure-strategy NE of the game. Proof of this result
uses a novel potential function. We further show that when the underlying
network has a tree structure, every global optimal allocation is a NE, which
can be reached in only linear time. We formulate the optimal solutions and NE
points of the CSR game using integer linear programs, and provide an upper
bound for the minimum social cost of the game using a probabilistic argument.
Finally, we introduce the LCSR game as a localized version of the CSR game,
wherein the actions of the players are restricted to only their close
neighborhoods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04051</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04051</id><created>2015-06-12</created><authors><author><keyname>Maddalena</keyname><forenames>Lucia</forenames></author><author><keyname>Petrosino</keyname><forenames>Alfredo</forenames></author></authors><title>Towards Benchmarking Scene Background Initialization</title><categories>cs.CV</categories><comments>6 pages, SBI dataset, SBMI2015 Workshop</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Given a set of images of a scene taken at different times, the availability
of an initial background model that describes the scene without foreground
objects is the prerequisite for a wide range of applications, ranging from
video surveillance to computational photography. Even though several methods
have been proposed for scene background initialization, the lack of a common
groundtruthed dataset and of a common set of metrics makes it difficult to
compare their performance. To move first steps towards an easy and fair
comparison of these methods, we assembled a dataset of sequences frequently
adopted for background initialization, selected or created ground truths for
quantitative evaluation through a selected suite of metrics, and compared
results obtained by some existing methods, making all the material publicly
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04059</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04059</id><created>2015-06-12</created><authors><author><keyname>Dartois</keyname><forenames>Luc</forenames></author><author><keyname>Reynier</keyname><forenames>Pierre-Alain</forenames></author></authors><title>Aperiodic Transducers</title><categories>cs.FL</categories><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that languages definable using first-order logic with order
predicate (FO) are exactly those recognized by a finite-state automaton whose
transition monoid is aperiodic. This result has recently been lifted to FO
definable string-to-string functions, with equivalent representations by means
of aperiodic deterministic two-way transducers and aperiodic (1-bounded)
streaming string transducers (SST). In this paper, we study transformations
between these two models of transducers, and prove that they preserve the
aperiodicity of transducers. We also provide a transformation from k-bounded
SST to 1-bounded SST, and show it preserves aperiodicity. As a corollary, we
obtain that FO definable string-to-string functions are equivalent to SST whose
transition monoid is finite and aperiodic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04079</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04079</id><created>2015-06-12</created><updated>2016-01-22</updated><authors><author><keyname>Rezaee</keyname><forenames>Milad</forenames></author><author><keyname>Mirmohseni</keyname><forenames>Mahtab</forenames></author><author><keyname>Aref</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Energy Harvesting Systems with Continuous Energy and Data Arrivals: the
  Optimal Offline and a Heuristic Online Algorithms</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting has been developed as an effective technology for
communication systems in order to extend the lifetime of these systems. In this
work, we consider a singleuser energy harvesting wireless communication system,
in which arrival data and harvested energy curves are modeled as continuous
functions. For the single-user model, our first goal is to find an offline
algorithm, which maximizes the amount of data which is transmitted to the
receiver node by a given deadline. If more than one scheme exists that
transmits the maximum data, we choose the one with minimum utilized energy at
the transmitter node. Next, we propose an online algorithm for this system. We
also consider a multi-hop energy harvesting wireless communication system in a
full-duplex mode and find the optimal offline algorithm to maximize the
throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04082</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04082</id><created>2015-06-12</created><authors><author><keyname>Ron</keyname><forenames>Aviv</forenames></author><author><keyname>Shulman-Peleg</keyname><forenames>Alexandra</forenames></author><author><keyname>Bronshtein</keyname><forenames>Emanuel</forenames></author></authors><title>No SQL, No Injection? Examining NoSQL Security</title><categories>cs.CR</categories><comments>In Proceedings of the 9th Workshop on Web 2.0 Security and Privacy
  (W2SP) 2015</comments><proxy>Mike Just</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NoSQL data storage systems have become very popular due to their scalability
and ease of use. This paper examines the maturity of security measures for
NoSQL databases, addressing their new query and access mechanisms. For example
the emergence of new query formats makes the old SQL injection techniques
irrelevant, but are NoSQL databases immune to injection in general? The answer
is NO. Here we present a few techniques for attacking NoSQL databases such as
injections and CSRF. We analyze the source of these vulnerabilities and present
methodologies to mitigate the attacks. We show that this new vibrant
technological area lacks the security measures and awareness which have
developed over the years in traditional RDBMS SQL systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04089</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04089</id><created>2015-06-12</created><updated>2015-12-17</updated><authors><author><keyname>Mei</keyname><forenames>Hongyuan</forenames></author><author><keyname>Bansal</keyname><forenames>Mohit</forenames></author><author><keyname>Walter</keyname><forenames>Matthew R.</forenames></author></authors><title>Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to
  Action Sequences</title><categories>cs.CL cs.AI cs.LG cs.NE cs.RO</categories><comments>To appear at AAAI 2016 (and an extended version of a NIPS 2015
  Multimodal Machine Learning workshop paper)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a neural sequence-to-sequence model for direction following, a
task that is essential to realizing effective autonomous agents. Our
alignment-based encoder-decoder model with long short-term memory recurrent
neural networks (LSTM-RNN) translates natural language instructions to action
sequences based upon a representation of the observable world state. We
introduce a multi-level aligner that empowers our model to focus on sentence
&quot;regions&quot; salient to the current world state by using multiple abstractions of
the input sentence. In contrast to existing methods, our model uses no
specialized linguistic resources (e.g., parsers) or task-specific annotations
(e.g., seed lexicons). It is therefore generalizable, yet still achieves the
best results reported to-date on a benchmark single-sentence dataset and
competitive results for the limited-training multi-sentence setting. We analyze
our model through a series of ablations that elucidate the contributions of the
primary components of our model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04092</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04092</id><created>2015-06-12</created><updated>2015-09-04</updated><authors><author><keyname>Dujmovi&#x107;</keyname><forenames>Vida</forenames></author></authors><title>The Utility of Untangling</title><categories>cs.CG math.CO</categories><comments>10 pages, 0 figures</comments><msc-class>68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we show how techniques developed for untangling planar graphs by
Bose et al. [Discrete &amp; Computational Geometry 42(4): 570-585 (2009)] and Goaoc
et al. [Discrete &amp; Com- putational Geometry 42(4): 542-569 (2009)] imply new
results about some recent graph drawing models. These include column planarity,
universal point subsets, and partial simultaneous geometric embeddings (with or
without mappings). Some of these results answer open problems posed in previous
papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04093</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04093</id><created>2015-06-12</created><authors><author><keyname>Zhu</keyname><forenames>Zhanxing</forenames></author><author><keyname>Storkey</keyname><forenames>Amos J.</forenames></author></authors><title>Adaptive Stochastic Primal-Dual Coordinate Descent for Separable Saddle
  Point Problems</title><categories>stat.ML cs.LG</categories><comments>Accepted by ECML/PKDD2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a generic convex-concave saddle point problem with separable
structure, a form that covers a wide-ranged machine learning applications.
Under this problem structure, we follow the framework of primal-dual updates
for saddle point problems, and incorporate stochastic block coordinate descent
with adaptive stepsize into this framework. We theoretically show that our
proposal of adaptive stepsize potentially achieves a sharper linear convergence
rate compared with the existing methods. Additionally, since we can select
&quot;mini-batch&quot; of block coordinates to update, our method is also amenable to
parallel processing for large-scale data. We apply the proposed method to
regularized empirical risk minimization and show that it performs comparably
or, more often, better than state-of-the-art methods on both synthetic and
real-world data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04094</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04094</id><created>2015-06-10</created><authors><author><keyname>Lange</keyname><forenames>Christoph</forenames></author><author><keyname>Shekarpour</keyname><forenames>Saeedeh</forenames></author><author><keyname>Auer</keyname><forenames>Soren</forenames></author></authors><title>The WDAqua ITN: Answering Questions using Web Data</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WDAqua is a Marie Curie Innovative Training Network (ITN) and is funded under
EU grant number 642795 and runs from January 2015 to December 2018. WDAqua aims
at advancing the state of the art by intertwining training, research and
innovation efforts, centered around one service: data-driven question
answering. Question answering is immediately useful to a wide audience of end
users, and we will demonstrate this in settings including e-commerce, public
sector information, publishing and smart cities. Question answering also covers
web science and data science broadly, leading to transferrable research results
and to transferrable skills of the researchers who have finished our training
programme. To ensure that our research improves question answering overall,
every individual research project connects at least two of these steps.
Intersectional secondments (within a consortium covering academia, research
institutes and industrial research as well as network-wide workshops, R and D
challenges and innovation projects further balance ground-breaking research and
the needs of society and industry. Training-wise these offers equip early stage
researchers with the expertise and transferable technical and non-technical
skills that will allow them to pursue a successful career as an academic,
decision maker, practitioner or entrepreneur.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04103</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04103</id><created>2015-06-12</created><authors><author><keyname>Fruchter</keyname><forenames>Nathaniel</forenames></author><author><keyname>Miao</keyname><forenames>Hsin</forenames></author><author><keyname>Stevenson</keyname><forenames>Scott</forenames></author><author><keyname>Balebako</keyname><forenames>Rebecca</forenames></author></authors><title>Variations in Tracking in Relation to Geographic Location</title><categories>cs.CR cs.CY</categories><comments>In Proceedings of the 9th Workshop on Web 2.0 Security and Privacy
  (W2SP) 2015</comments><proxy>Mike Just</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different countries have different privacy regulatory models. These models
impact the perspectives and laws surrounding internet privacy. However, little
is known about how effective the regulatory models are when it comes to
limiting online tracking and advertising activity. In this paper, we propose a
method for investigating tracking behavior by analyzing cookies and HTTP
requests from browsing sessions originating in different countries. We collect
browsing data from visits to top websites in various countries that utilize
different regulatory models. We found that there are significant differences in
tracking activity between different countries using several metrics. We also
suggest various ways to extend this study which may yield a more complete
representation of tracking from a global perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04104</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04104</id><created>2015-06-12</created><authors><author><keyname>Kontaxis</keyname><forenames>Georgios</forenames></author><author><keyname>Chew</keyname><forenames>Monica</forenames></author></authors><title>Tracking Protection in Firefox For Privacy and Performance</title><categories>cs.CR</categories><comments>In Proceedings of the 9th Workshop on Web 2.0 Security and Privacy
  (W2SP) 2015. Awarded Best Paper prize at workshop</comments><proxy>Mike Just</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Tracking Protection in the Mozilla Firefox web browser. Tracking
Protection is a new privacy technology to mitigate invasive tracking of users'
online activity by blocking requests to tracking domains. We evaluate our
approach and demonstrate a 67.5% reduction in the number of HTTP cookies set
during a crawl of the Alexa top 200 news sites. Since Firefox does not download
and render content from tracking domains, Tracking Protection also enjoys
performance benefits of a 44% median reduction in page load time and 39%
reduction in data usage in the Alexa top 200 news sites.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04105</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04105</id><created>2015-06-12</created><authors><author><keyname>Piekarska</keyname><forenames>Marta</forenames></author><author><keyname>Zhou</keyname><forenames>Yun</forenames></author><author><keyname>Strohmeier</keyname><forenames>Dominik</forenames></author><author><keyname>Raake</keyname><forenames>Alexander</forenames></author></authors><title>Because we care: Privacy Dashboard on Firefox OS</title><categories>cs.CR cs.HC</categories><comments>In Proceedings of the 9th Workshop on Web 2.0 Security and Privacy
  (W2SP) 2015</comments><proxy>Mike Just</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the Privacy Dashboard -- a tool designed to inform
and empower the people using mobile devices, by introducing features such as
Remote Privacy Protection, Backup, Adjustable Location Accuracy, Permission
Control and Secondary-User Mode. We have implemented our solution on FirefoxOS
and conducted user studies to verify the usefulness and usability of our tool.
The paper starts with a discussion of different aspects of mobile privacy, how
users perceive it and how much they are willing to give up for better
usability. Then we describe the tool in detail, presenting what incentives
drove us to certain design decisions. During our studies we tried to understand
how users interact with the system and what are their priorities. We have
verified our hypothesis, and the impact of the educational aspects on the
decisions about the privacy settings. We show that by taking a user-centric
development of privacy extensions we can reduce the gap between protection and
usability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04107</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04107</id><created>2015-06-12</created><authors><author><keyname>Akkus</keyname><forenames>Istemi Ekin</forenames></author><author><keyname>Weaver</keyname><forenames>Nicholas</forenames></author></authors><title>The Case for a General and Interaction-based Third-party Cookie Policy</title><categories>cs.CR cs.CY</categories><comments>In Proceedings of the 9th Workshop on Web 2.0 Security and Privacy
  (W2SP) 2015</comments><proxy>Mike Just</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The privacy implications of third-party tracking is a well-studied problem.
Recent research has shown that besides data aggregators and behavioral
advertisers, online social networks also act as trackers via social widgets.
Existing cookie policies are not enough to solve these problems, pushing users
to employ blacklist-based browser extensions to prevent such tracking.
Unfortunately, such approaches require maintaining and distributing blacklists,
which are often too general and adversely affect non-tracking services for
advertisements and analytics. In this paper, we propose and advocate for a
general third-party cookie policy that prevents third-party tracking with
cookies and preserves the functionality of social widgets without requiring a
blacklist and adversely affecting non-tracking services. We implemented a
proof-of-concept of our policy as browser extensions for Mozilla Firefox and
Google Chrome. To date, our extensions have been downloaded about 11.8K times
and have over 2.8K daily users combined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04110</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04110</id><created>2015-06-12</created><authors><author><keyname>Levy</keyname><forenames>Amit</forenames></author><author><keyname>Corrigan-Gibbs</keyname><forenames>Henry</forenames></author><author><keyname>Boneh</keyname><forenames>Dan</forenames></author></authors><title>Stickler: Defending Against Malicious CDNs in an Unmodified Browser</title><categories>cs.CR</categories><comments>In Proceedings of the 9th Workshop on Web 2.0 Security and Privacy
  (W2SP) 2015</comments><proxy>Mike Just</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Website publishers can derive enormous performance benefits and cost savings
by directing traffic to their sites through content distribution networks
(CDNs). However, publishers who use CDNs today must trust their CDN not to
modify the site's JavaScript, CSS, images or other media en route to end users.
A CDN that violates this trust could inject ads into websites, downsample media
to save bandwidth or, worse, inject malicious JavaScript code to steal user
secrets it could not otherwise access. We present Stickler, a system for
website publishers that guarantees the end-to-end authenticity of content
served to end users while simultaneously allowing publishers to reap the
benefits of CDNs. Crucially, Stickler achieves these guarantees without
requiring modifications to the browser.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04111</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04111</id><created>2015-06-12</created><authors><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Shirley</keyname><forenames>Kenneth</forenames></author></authors><title>Breaking Bad: Detecting malicious domains using word segmentation</title><categories>cs.CR</categories><comments>In Proceedings of the 9th Workshop on Web 2.0 Security and Privacy
  (W2SP) 2015</comments><proxy>Mike Just</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, vulnerable hosts and maliciously registered domains have
been frequently involved in mobile attacks. In this paper, we explore the
feasibility of detecting malicious domains visited on a cellular network based
solely on lexical characteristics of the domain names. In addition to using
traditional quantitative features of domain names, we also use a word
segmentation algorithm to segment the domain names into individual words to
greatly expand the size of the feature set. Experiments on a sample of
real-world data from a large cellular network show that using word segmentation
improves our ability to detect malicious domains relative to approaches without
segmentation, as measured by misclassification rates and areas under the ROC
curve. Furthermore, the results are interpretable, allowing one to discover
(with little supervision or tuning required) which words are used most often to
attract users to malicious domains. Such a lightweight approach could be
performed in near-real time when a device attempts to visit a domain. This
approach can complement (rather than substitute) other more expensive and
time-consuming approaches to similar problems that use richer feature sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04112</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04112</id><created>2015-06-12</created><authors><author><keyname>Niemietz</keyname><forenames>Marcus</forenames></author><author><keyname>Schwenk</keyname><forenames>Joerg</forenames></author></authors><title>Owning Your Home Network: Router Security Revisited</title><categories>cs.CR cs.NI</categories><comments>In Proceedings of the 9th Workshop on Web 2.0 Security and Privacy
  (W2SP) 2015</comments><proxy>Mike Just</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the Web interfaces of several DSL home routers
that can be used to manage their settings via a Web browser. Our goal is to
change these settings by using primary XSS and UI redressing attacks. This
study evaluates routers from 10 different manufacturers (TP-Link, Netgear,
Huawei, D-Link, Linksys, LogiLink, Belkin, Buffalo, Fritz!Box, and Asus). We
were able to circumvent the security of all of them. To demonstrate how all
devices are able to be attacked, we show how to do fast fingerprinting attacks.
Furthermore, we provide countermeasures to make administration interfaces and
therefore the use of routers more secure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04113</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04113</id><created>2015-06-12</created><authors><author><keyname>Weiss</keyname><forenames>Mor</forenames></author><author><keyname>Rozenberg</keyname><forenames>Boris</forenames></author><author><keyname>Barham</keyname><forenames>Muhammad</forenames></author></authors><title>Practical Solutions For Format-Preserving Encryption</title><categories>cs.CR</categories><comments>In Proceedings of the 9th Workshop on Web 2.0 Security and Privacy
  (W2SP) 2015</comments><proxy>Mike Just</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Format Preserving Encryption (FPE) schemes encrypt a plaintext into a
ciphertext while preserving its format (e.g., a valid social-security number is
encrypted into a valid social-security number), thus allowing encrypted data to
be stored and used in the same manner as unencrypted data. Motivated by the
always-increasing use of cloud-computing and memory delegation, which require
preserving both plaintext format and privacy, several FPE schemes for general
formats have been previously suggested. However, current solutions are both
insecure and inefficient in practice. We propose an efficient FPE scheme with
optimal security. Our scheme includes an efficient method of representing
general (complex) formats, and provides efficient encryption and decryption
algorithms that do not require an expensive set-up. During encryption, only
format-specific properties are preserved, while all message-specific properties
remain hidden, thus guaranteeing data privacy. As experimental results show
that in many cases large formats domains cannot be encrypted efficiently, we
extend our scheme to support large formats, by imposing a user-defined bound on
the maximal format size, thus obtaining a flexible security-efficiency tradeoff
and the best possible security (under the size limitation).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04115</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04115</id><created>2015-06-12</created><authors><author><keyname>Syverson</keyname><forenames>Paul</forenames></author><author><keyname>Boyce</keyname><forenames>Griffin</forenames></author></authors><title>Genuine onion: Simple, Fast, Flexible, and Cheap Website Authentication</title><categories>cs.CR</categories><comments>In Proceedings of the 9th Workshop on Web 2.0 Security and Privacy
  (W2SP) 2015</comments><proxy>Mike Just</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tor is a communications infrastructure widely used for unfettered and
anonymous access to Internet websites. Tor is also used to access sites on the
.onion virtual domain. The focus of .onion use and discussion has traditionally
been on the offering of hidden services, services that separate their
reachability from the identification of their IP addresses. We argue that Tor's
.onion system can be used to provide an entirely separate benefit: basic
website authentication. We also argue that not only can onionsites provide
website authentication, but doing so is easy, fast, cheap, flexible and secure
when compared to alternatives such as the standard use of TLS with
certificates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04118</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04118</id><created>2015-06-12</created><authors><author><keyname>Campello</keyname><forenames>Antonio</forenames></author><author><keyname>Vaishampayan</keyname><forenames>Vinay A.</forenames></author></authors><title>A Generalization of Montucla's Rectangle-to-Rectangle Dissection to
  Higher Dimensions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dissections of polytopes are a well-studied subject by geometers as well as
recreational mathematicians. A recent application in coding theory arises from
the problem of parameterizing binary vectors of constant Hamming weight which
has been shown previously to be equivalent to the problem of dissecting a
tetrahedron to a brick. Applications of dissections also arise in problems
related to the construction of analog codes.
  Here we consider the rectangle-to-rectangle dissection due to Montucla.
Montucla's dissection is first reinterpreted in terms of the Two Tile Theorem.
Based on this, a cube-to-brick dissection is developed in $\mathbb{R}^n$. We
present a linear time algorithm (in $n$) that computes the dissection, i.e.
determines a point in the cube given a point in a specific realization of the
brick. An application of this algorithm to a previously reported analog coding
scheme is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04129</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04129</id><created>2015-06-12</created><authors><author><keyname>Korkin</keyname><forenames>Igor</forenames></author><author><keyname>Nesterov</keyname><forenames>Ivan</forenames></author></authors><title>Applying Memory Forensics to Rootkit Detection</title><categories>cs.CR</categories><comments>25 pages, 3 figures, 8 tables. Paper presented at the Proceedings of
  the 9th annual Conference on Digital Forensics, Security and Law (CDFSL),
  115-141, Richmond, VA, USA. (2014, May 28-29)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Volatile memory dump and its analysis is an essential part of digital
forensics. Among a number of various software and hardware approaches for
memory dumping there are authors who point out that some of these approaches
are not resilient to various anti-forensic techniques, and others that require
a reboot or are highly platform dependent. New resilient tools have certain
disadvantages such as low speed or vulnerability to rootkits which directly
manipulate kernel structures e.g. page tables. A new memory forensic system -
Malware Analysis System for Hidden Knotty Anomalies (MASHKA) is described in
this paper. It is resilient to popular anti-forensic techniques. The system can
be used for doing a wide range of memory forensics tasks. This paper describes
how to apply the system for research and detection of kernel mode rootkits and
also presents analysis of the most popular anti-rootkit tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04130</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04130</id><created>2015-06-12</created><authors><author><keyname>Agrawal</keyname><forenames>Harsh</forenames></author><author><keyname>Mathialagan</keyname><forenames>Clint Solomon</forenames></author><author><keyname>Goyal</keyname><forenames>Yash</forenames></author><author><keyname>Chavali</keyname><forenames>Neelima</forenames></author><author><keyname>Banik</keyname><forenames>Prakriti</forenames></author><author><keyname>Mohapatra</keyname><forenames>Akrit</forenames></author><author><keyname>Osman</keyname><forenames>Ahmed</forenames></author><author><keyname>Batra</keyname><forenames>Dhruv</forenames></author></authors><title>CloudCV: Large Scale Distributed Computer Vision as a Cloud Service</title><categories>cs.CV cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are witnessing a proliferation of massive visual data. Unfortunately
scaling existing computer vision algorithms to large datasets leaves
researchers repeatedly solving the same algorithmic, logistical, and
infrastructural problems. Our goal is to democratize computer vision; one
should not have to be a computer vision, big data and distributed computing
expert to have access to state-of-the-art distributed computer vision
algorithms. We present CloudCV, a comprehensive system to provide access to
state-of-the-art distributed computer vision algorithms as a cloud service
through a Web Interface and APIs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04131</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04131</id><created>2015-06-12</created><authors><author><keyname>Korkin</keyname><forenames>Igor</forenames></author></authors><title>Two Challenges of Stealthy Hypervisors Detection: Time Cheating and Data
  Fluctuations</title><categories>cs.CR stat.AP stat.CO stat.OT</categories><comments>25 pages, 7 figures, 8 tables. Paper presented at the Proceedings of
  the 10th Annual Conference on Digital Forensics, Security and Law (CDFSL),
  33-57, Daytona Beach, Florida, USA (2015, May 18-21)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Hardware virtualization technologies play a significant role in cyber
security. On the one hand these technologies enhance security levels, by
designing a trusted operating system. On the other hand these technologies can
be taken up into modern malware which is rather hard to detect. None of the
existing methods is able to efficiently detect a hypervisor in the face of
countermeasures such as time cheating, temporary self uninstalling, memory
hiding etc. New hypervisor detection methods which will be described in this
paper can detect a hypervisor under these countermeasures and even count
several nested ones. These novel approaches rely on the new statistical
analysis of time discrepancies by examination of a set of instructions, which
are unconditionally intercepted by a hypervisor. Reliability was achieved
through the comprehensive analysis of the collected data despite its
fluctuation. These offered methods were comprehensively assessed in both Intel
and AMD CPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04132</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04132</id><created>2015-06-12</created><updated>2015-11-18</updated><authors><author><keyname>Li</keyname><forenames>Yingzhen</forenames></author><author><keyname>Hernandez-Lobato</keyname><forenames>Jose Miguel</forenames></author><author><keyname>Turner</keyname><forenames>Richard E.</forenames></author></authors><title>Stochastic Expectation Propagation</title><categories>stat.ML cs.LG</categories><comments>Published at NIPS 2015. 18 pages including supplementary</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expectation propagation (EP) is a deterministic approximation algorithm that
is often used to perform approximate Bayesian parameter learning. EP
approximates the full intractable posterior distribution through a set of local
approximations that are iteratively refined for each datapoint. EP can offer
analytic and computational advantages over other approximations, such as
Variational Inference (VI), and is the method of choice for a number of models.
The local nature of EP appears to make it an ideal candidate for performing
Bayesian learning on large models in large-scale dataset settings. However, EP
has a crucial limitation in this context: the number of approximating factors
needs to increase with the number of data-points, N, which often entails a
prohibitively large memory overhead. This paper presents an extension to EP,
called stochastic expectation propagation (SEP), that maintains a global
posterior approximation (like VI) but updates it in a local way (like EP).
Experiments on a number of canonical learning problems using synthetic and
real-world datasets indicate that SEP performs almost as well as full EP, but
reduces the memory consumption by a factor of $N$. SEP is therefore ideally
suited to performing approximate Bayesian learning in the large model, large
dataset setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04135</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04135</id><created>2015-06-12</created><authors><author><keyname>De Myttenaere</keyname><forenames>Arnaud</forenames><affiliation>SAMM, Viadeo</affiliation></author><author><keyname>Golden</keyname><forenames>Boris</forenames><affiliation>Viadeo</affiliation></author><author><keyname>Grand</keyname><forenames>B&#xe9;n&#xe9;dicte Le</forenames><affiliation>CRI</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>Reducing offline evaluation bias of collaborative filtering algorithms</title><categories>cs.IR cs.LG stat.ML</categories><comments>European Symposium on Artificial Neural Networks, Computational
  Intelligence and Machine Learning (ESANN), Apr 2015, Bruges, Belgium.
  pp.137-142, 2015, Proceedings of the 23-th European Symposium on Artificial
  Neural Networks, Computational Intelligence and Machine Learning (ESANN 2015)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommendation systems have been integrated into the majority of large online
systems to filter and rank information according to user profiles. It thus
influences the way users interact with the system and, as a consequence, bias
the evaluation of the performance of a recommendation algorithm computed using
historical data (via offline evaluation). This paper presents a new application
of a weighted offline evaluation to reduce this bias for collaborative
filtering algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04147</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04147</id><created>2015-06-12</created><updated>2015-06-18</updated><authors><author><keyname>Andreas</keyname><forenames>Jacob</forenames></author><author><keyname>Rabinovich</keyname><forenames>Maxim</forenames></author><author><keyname>Klein</keyname><forenames>Dan</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>On the accuracy of self-normalized log-linear models</title><categories>stat.ML cs.CL cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Calculation of the log-normalizer is a major computational obstacle in
applications of log-linear models with large output spaces. The problem of fast
normalizer computation has therefore attracted significant attention in the
theoretical and applied machine learning literature. In this paper, we analyze
a recently proposed technique known as &quot;self-normalization&quot;, which introduces a
regularization term in training to penalize log normalizers for deviating from
zero. This makes it possible to use unnormalized model scores as approximate
probabilities. Empirical evidence suggests that self-normalization is extremely
effective, but a theoretical understanding of why it should work, and how
generally it can be applied, is largely lacking. We prove generalization bounds
on the estimated variance of normalizers and upper bounds on the loss in
accuracy due to self-normalization, describe classes of input distributions
that self-normalize easily, and construct explicit examples of high-variance
input distributions. Our theoretical results make predictions about the
difficulty of fitting self-normalized models to several classes of
distributions, and we conclude with empirical validation of these predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04161</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04161</id><created>2015-06-12</created><authors><author><keyname>Monniaux</keyname><forenames>David</forenames><affiliation>VERIMAG - IMAG</affiliation></author><author><keyname>Alberti</keyname><forenames>Francesco</forenames></author></authors><title>A simple abstraction of arrays and maps by program translation</title><categories>cs.PL cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach for the static analysis of programs handling arrays,
with a Galois connection between the semantics of the array program and
semantics of purely scalar operations. The simplest way to implement it is by
automatic, syntactic transformation of the array program into a scalar program
followed analysis of the scalar program with any static analysis technique
(abstract interpretation, acceleration, predicate abstraction,.. .). The
scalars invariants thus obtained are translated back onto the original program
as universally quantified array invariants. We illustrate our approach on a
variety of examples, leading to the &quot; Dutch flag &quot; algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04176</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04176</id><created>2015-06-12</created><authors><author><keyname>De Myttenaere</keyname><forenames>Arnaud</forenames><affiliation>SAMM</affiliation></author><author><keyname>Golden</keyname><forenames>Boris</forenames><affiliation>Viadeo</affiliation></author><author><keyname>Grand</keyname><forenames>B&#xe9;n&#xe9;dicte Le</forenames><affiliation>CRI</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>Using the Mean Absolute Percentage Error for Regression Models</title><categories>stat.ML cs.LG</categories><comments>European Symposium on Artificial Neural Networks, Computational
  Intelligence and Machine Learning (ESANN), Apr 2015, Bruges, Belgium. 2015,
  Proceedings of the 23-th European Symposium on Artificial Neural Networks,
  Computational Intelligence and Machine Learning (ESANN 2015)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study in this paper the consequences of using the Mean Absolute Percentage
Error (MAPE) as a measure of quality for regression models. We show that
finding the best model under the MAPE is equivalent to doing weighted Mean
Absolute Error (MAE) regression. We show that universal consistency of
Empirical Risk Minimization remains possible using the MAPE instead of the MAE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04177</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04177</id><created>2015-06-12</created><authors><author><keyname>Rabenoro</keyname><forenames>Tsirizo</forenames><affiliation>SAMM</affiliation></author><author><keyname>Lacaille</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>SAMM</affiliation></author><author><keyname>Cottrell</keyname><forenames>Marie</forenames><affiliation>SAMM</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>Search Strategies for Binary Feature Selection for a Naive Bayes
  Classifier</title><categories>stat.ML cs.LG</categories><proxy>ccsd</proxy><journal-ref>European Symposium on Artificial Neural Networks, Computational
  Intelligence and Machine Learning (ESANN), Apr 2015, Bruges, Belgium.
  pp.291-296, 2015, Proceedings of the 23-th European Symposium on Artificial
  Neural Networks, Computational Intelligence and Machine Learning (ESANN 2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compare in this paper several feature selection methods for the Naive
Bayes Classifier (NBC) when the data under study are described by a large
number of redundant binary indicators. Wrapper approaches guided by the NBC
estimation of the classification error probability out-perform filter
approaches while retaining a reasonable computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04182</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04182</id><created>2015-06-12</created><authors><author><keyname>Reuillon</keyname><forenames>Romain</forenames><affiliation>ISC-PIF</affiliation></author><author><keyname>Leclaire</keyname><forenames>Mathieu</forenames><affiliation>ISC-PIF, GC</affiliation></author><author><keyname>Passerat-Palmbach</keyname><forenames>Jonathan</forenames><affiliation>BioMedIA</affiliation></author></authors><title>Model Exploration Using OpenMOLE - a workflow engine for large scale
  distributed design of experiments and parameter tuning</title><categories>cs.DC</categories><comments>IEEE High Performance Computing and Simulation conference 2015, Jun
  2015, Amsterdam, Netherlands</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  OpenMOLE is a scientific workflow engine with a strong emphasis on workload
distribution. Workflows are designed using a high level Domain Specific
Language (DSL) built on top of Scala. It exposes natural parallelism constructs
to easily delegate the workload resulting from a workflow to a wide range of
distributed computing environments. In this work, we briefly expose the strong
assets of OpenMOLE and demonstrate its efficiency at exploring the parameter
set of an agent simulation model. We perform a multi-objective optimisation on
this model using computationally expensive Genetic Algorithms (GA). OpenMOLE
hides the complexity of designing such an experiment thanks to its DSL, and
transparently distributes the optimisation process. The example shows how an
initialisation of the GA with a population of 200,000 individuals can be
evaluated in one hour on the European Grid Infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04184</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04184</id><created>2015-06-12</created><authors><author><keyname>Bodirsky</keyname><forenames>Manuel</forenames></author><author><keyname>Mamino</keyname><forenames>Marcello</forenames></author></authors><title>Max-Closed Semilinear Constraint Satisfaction</title><categories>cs.CC</categories><comments>29 pages, 2 figures</comments><msc-class>68Q25, 15A80, 52B55</msc-class><acm-class>F.4.1; F.2.2; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A semilinear relation S is max-closed if it is preserved by taking the
componentwise maximum. The constraint satisfaction problem for max-closed
semilinear constraints is at least as hard as determining the winner in Mean
Payoff Games, a notorious problem of open computational complexity. Mean Payoff
Games are known to be in the intersection of NP and co-NP, which is not known
for max-closed semilinear constraints. Semilinear relations that are max-closed
and additionally closed under translations have been called tropically convex
in the literature. One of our main results is a new duality for open tropically
convex relations, which puts the CSP for tropically convex semilinaer
constraints in general into NP intersected co-NP. This extends the
corresponding complexity result for and-or precedence constraints aka the
max-atoms problem. To this end, we present a characterization of max-closed
semilinear relations in terms of syntactically restricted first-order logic,
and another characterization in terms of a finite set of relations L that allow
primitive positive definitions of all other relations in the class. We also
present a subclass of max-closed constraints where the CSP is in P; this class
generalizes the class of max-closed constraints over finite domains, and the
feasibility problem for max-closed linear inequalities. Finally, we show that
the class of max-closed semilinear constraints is maximal in the sense that as
soon as a single relation that is not max-closed is added to L, the CSP becomes
NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04188</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04188</id><created>2015-06-12</created><updated>2015-06-21</updated><authors><author><keyname>Landerreche</keyname><forenames>Esteban</forenames></author><author><keyname>Fern&#xe1;ndez-Duque</keyname><forenames>David</forenames></author></authors><title>A case study in almost-perfect security for unconditionally secure
  communication</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Russian cards problem, Alice, Bob and Cath draw $a$, $b$ and $c$
cards, respectively, from a publicly known deck. Alice and Bob must then
communicate their cards to each other without Cath learning who holds a single
card. Solutions in the literature provide weak security, where Cath does not
know with certainty who holds each card that is not hers, or perfect security,
where Cath learns no probabilistic information about who holds any given card
from Alice and Bob's exchange. We propose an intermediate notion, which we call
$\varepsilon$-strong security, where the probabilities perceived by Cath may
only change by a factor of $\varepsilon$. We then show that a mild variant of
the so-called geometric strategy gives $\varepsilon$-strong safety for
arbitrarily small $\varepsilon$ and appropriately chosen values of $a,b,c$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04189</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04189</id><created>2015-06-12</created><updated>2015-06-20</updated><authors><author><keyname>Fotouhi</keyname><forenames>Babak</forenames></author><author><keyname>Momeni</keyname><forenames>Naghmeh</forenames></author></authors><title>Multiplex Networks with Intrinsic Fitness: Modeling the Merit-Fame
  Interplay via Latent Layers</title><categories>cs.SI cond-mat.stat-mech cs.MA physics.soc-ph</categories><journal-ref>Chaos, Solitons &amp; Fractals 80 (2015): 83-89</journal-ref><doi>10.1016/j.chaos.2015.06.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of growing multiplex networks with intrinsic fitness
and inter-layer coupling. The model comprises two layers; one that incorporates
fitness and another in which attachments are preferential. In the first layer,
attachment probabilities are proportional to fitness values, and in the second
layer, proportional to the sum of degrees in both layers. We provide analytical
closed-form solutions for the joint distributions of fitness and degrees. We
also derive closed-form expressions for the expected value of the degree as a
function of fitness. The model alleviates two shortcomings that are present in
the current models of growing multiplex networks: homogeneity of connections,
and homogeneity of fitness. In this paper, we posit and analyze a growth model
that is heterogeneous in both senses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04191</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04191</id><created>2015-06-12</created><authors><author><keyname>Deng</keyname><forenames>Zhiwei</forenames></author><author><keyname>Zhai</keyname><forenames>Mengyao</forenames></author><author><keyname>Chen</keyname><forenames>Lei</forenames></author><author><keyname>Liu</keyname><forenames>Yuhao</forenames></author><author><keyname>Muralidharan</keyname><forenames>Srikanth</forenames></author><author><keyname>Roshtkhari</keyname><forenames>Mehrsan Javan</forenames></author><author><keyname>Mori</keyname><forenames>Greg</forenames></author></authors><title>Deep Structured Models For Group Activity Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a deep neural-network-based hierarchical graphical model
for individual and group activity recognition in surveillance scenes. Deep
networks are used to recognize the actions of individual people in a scene.
Next, a neural-network-based hierarchical graphical model refines the predicted
labels for each class by considering dependencies between the classes. This
refinement step mimics a message-passing step similar to inference in a
probabilistic graphical model. We show that this approach can be effective in
group activity recognition, with the deep graphical model improving recognition
rates over baseline methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04198</identifier>
 <datestamp>2015-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04198</id><created>2015-06-12</created><updated>2015-10-21</updated><authors><author><keyname>Balkanski</keyname><forenames>Eric</forenames></author><author><keyname>Hartline</keyname><forenames>Jason D.</forenames></author></authors><title>Bayesian Budget Feasibility with Posted Pricing</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of budget feasible mechanism design proposed by
Singer (2010), but in a Bayesian setting. A principal has a public value for
hiring a subset of the agents and a budget, while the agents have private costs
for being hired. We consider both additive and submodular value functions of
the principal. We show that there are simple, practical, ex post budget
balanced posted pricing mechanisms that approximate the value obtained by the
Bayesian optimal mechanism that is budget balanced only in expectation. A main
motivating application for this work is the crowdsourcing large projects, e.g.,
on Mechanical Turk, where workers are drawn from a large population and posted
pricing is standard. Our analysis methods relate to contention resolution
schemes in submodular optimization of Vondrak et al. (2011) and the correlation
gap analysis of Yan (2011).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04200</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04200</id><created>2015-06-12</created><updated>2015-08-25</updated><authors><author><keyname>Berlin</keyname><forenames>Konstantin</forenames></author><author><keyname>Slater</keyname><forenames>David</forenames></author><author><keyname>Saxe</keyname><forenames>Joshua</forenames></author></authors><title>Malicious Behavior Detection using Windows Audit Logs</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As antivirus and network intrusion detection systems have increasingly proven
insufficient to detect advanced threats, large security operations centers have
moved to deploy endpoint-based sensors that provide deeper visibility into
low-level events across their enterprises. Unfortunately, for many
organizations in government and industry, the installation, maintenance, and
resource requirements of these newer solutions pose barriers to adoption and
are perceived as risks to organizations' missions.
  To mitigate this problem we investigated the utility of agentless detection
of malicious endpoint behavior, using only the standard build-in Windows audit
logging facility as our signal. We found that Windows audit logs, while
emitting manageable sized data streams on the endpoints, provide enough
information to allow robust detection of malicious behavior. Audit logs provide
an effective, low-cost alternative to deploying additional expensive
agent-based breach detection systems in many government and industrial
settings, and can be used to detect, in our tests, 83% percent of malware
samples with a 0.1% false positive rate. They can also supplement already
existing host signature-based antivirus solutions, like Kaspersky, Symantec,
and McAfee, detecting, in our testing environment, 78% of malware missed by
those antivirus systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04205</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04205</id><created>2015-06-12</created><updated>2015-08-22</updated><authors><author><keyname>Tanter</keyname><forenames>&#xc9;ric</forenames></author><author><keyname>Tabareau</keyname><forenames>Nicolas</forenames></author></authors><title>Gradual Certified Programming in Coq</title><categories>cs.PL</categories><comments>DLS'15 final version, Proceedings of the ACM Dynamic Languages
  Symposium (DLS 2015)</comments><acm-class>D.3.3, F.3.1</acm-class><doi>10.1145/2816707.2816710</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expressive static typing disciplines are a powerful way to achieve
high-quality software. However, the adoption cost of such techniques should not
be under-estimated. Just like gradual typing allows for a smooth transition
from dynamically-typed to statically-typed programs, it seems desirable to
support a gradual path to certified programming. We explore gradual certified
programming in Coq, providing the possibility to postpone the proofs of
selected properties, and to check &quot;at runtime&quot; whether the properties actually
hold. Casts can be integrated with the implicit coercion mechanism of Coq to
support implicit cast insertion a la gradual typing. Additionally, when
extracting Coq functions to mainstream languages, our encoding of casts
supports lifting assumed properties into runtime checks. Much to our surprise,
it is not necessary to extend Coq in any way to support gradual certified
programming. A simple mix of type classes and axioms makes it possible to bring
gradual certified programming to Coq in a straightforward manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04209</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04209</id><created>2015-06-12</created><updated>2015-10-28</updated><authors><author><keyname>Huang</keyname><forenames>Kejun</forenames></author><author><keyname>Sidiropoulos</keyname><forenames>Nicholas D.</forenames></author><author><keyname>Liavas</keyname><forenames>Athanasios P.</forenames></author></authors><title>A Flexible and Efficient Algorithmic Framework for Constrained Matrix
  and Tensor Factorization</title><categories>stat.ML cs.LG math.OC stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general algorithmic framework for constrained matrix and tensor
factorization, which is widely used in signal processing and machine learning.
The new framework is a hybrid between alternating optimization (AO) and the
alternating direction method of multipliers (ADMM): each matrix factor is
updated in turn, using ADMM, hence the name AO-ADMM. This combination can
naturally accommodate a great variety of constraints on the factor matrices,
and almost all possible loss measures for the fitting. Computation caching and
warm start strategies are used to ensure that each update is evaluated
efficiently, while the outer AO framework exploits recent developments in block
coordinate descent (BCD)-type methods which help ensure that every limit point
is a stationary point, as well as faster and more robust convergence in
practice. Three special cases are studied in detail: non-negative matrix/tensor
factorization, constrained matrix/tensor completion, and dictionary learning.
Extensive simulations and experiments with real data are used to showcase the
effectiveness and broad applicability of the proposed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04214</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04214</id><created>2015-06-12</created><updated>2015-09-19</updated><authors><author><keyname>Shi</keyname><forenames>Xingjian</forenames></author><author><keyname>Chen</keyname><forenames>Zhourong</forenames></author><author><keyname>Wang</keyname><forenames>Hao</forenames></author><author><keyname>Yeung</keyname><forenames>Dit-Yan</forenames></author><author><keyname>Wong</keyname><forenames>Wai-kin</forenames></author><author><keyname>Woo</keyname><forenames>Wang-chun</forenames></author></authors><title>Convolutional LSTM Network: A Machine Learning Approach for
  Precipitation Nowcasting</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of precipitation nowcasting is to predict the future rainfall
intensity in a local region over a relatively short period of time. Very few
previous studies have examined this crucial and challenging weather forecasting
problem from the machine learning perspective. In this paper, we formulate
precipitation nowcasting as a spatiotemporal sequence forecasting problem in
which both the input and the prediction target are spatiotemporal sequences. By
extending the fully connected LSTM (FC-LSTM) to have convolutional structures
in both the input-to-state and state-to-state transitions, we propose the
convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model
for the precipitation nowcasting problem. Experiments show that our ConvLSTM
network captures spatiotemporal correlations better and consistently
outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for
precipitation nowcasting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04215</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04215</id><created>2015-06-12</created><authors><author><keyname>Khochtali</keyname><forenames>Mohamed</forenames></author><author><keyname>Roche</keyname><forenames>Daniel S.</forenames></author><author><keyname>Tian</keyname><forenames>Xisen</forenames></author></authors><title>Parallel sparse interpolation using small primes</title><categories>cs.SC cs.DC</categories><comments>Accepted to PASCO 2015</comments><msc-class>68W30 (Primary) 68W10, 68Q10, 12Y05 (Secondary)</msc-class><acm-class>F.2.1; G.3; G.4; I.1.2</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  To interpolate a supersparse polynomial with integer coefficients, two
alternative approaches are the Prony-based &quot;big prime&quot; technique, which acts
over a single large finite field, or the more recently-proposed &quot;small primes&quot;
technique, which reduces the unknown sparse polynomial to many low-degree dense
polynomials. While the latter technique has not yet reached the same
theoretical efficiency as Prony-based methods, it has an obvious potential for
parallelization. We present a heuristic &quot;small primes&quot; interpolation algorithm
and report on a low-level C implementation using FLINT and MPI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04217</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04217</id><created>2015-06-12</created><updated>2015-06-21</updated><authors><author><keyname>Lee</keyname><forenames>Ching-pei</forenames></author></authors><title>On the Equivalence of CoCoA+ and DisDCA</title><categories>cs.LG</categories><comments>This article is withdrawn by the author because this is actually a
  known fact</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this document, we show that the algorithm CoCoA+ (Ma et al., ICML, 2015)
under the setting used in their experiments, which is also the best setting
suggested by the authors that proposed this algorithm, is equivalent to the
practical variant of DisDCA (Yang, NIPS, 2013).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04220</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04220</id><created>2015-06-12</created><authors><author><keyname>Chandu</keyname><forenames>Drona Pratap</forenames></author></authors><title>Improved Greedy Algorithm for Set Covering Problem</title><categories>cs.DS</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a greedy algorithm named as Big step greedy set cover
algorithm to compute approximate minimum set cover. The Big step greedy
algorithm, in each step selects p sets such that the union of selected p sets
contains greatest number of uncovered elements and adds the selected p sets to
partial set cover. The process of adding p sets is repeated until all the
elements are covered. When p=1 it behaves like the classical greedy algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04226</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04226</id><created>2015-06-13</created><authors><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author></authors><title>Student Teaching and Research Laboratory Focusing on Brain-computer
  Interface Paradigms - A Creative Environment for Computer Science Students -</title><categories>q-bio.NC cs.HC cs.RO</categories><comments>4 pages, 4 figures, accepted for EMBC 2015, IEEE copyright</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents an applied concept of a brain-computer interface (BCI)
student research laboratory (BCI-LAB) at the Life Science Center of TARA,
University of Tsukuba, Japan. Several successful case studies of the student
projects are reviewed together with the BCI Research Award 2014 winner case.
The BCI-LAB design and project-based teaching philosophy is also explained.
Future teaching and research directions summarize the review.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04228</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04228</id><created>2015-06-13</created><authors><author><keyname>Iliev</keyname><forenames>Grigor</forenames></author><author><keyname>Borisova</keyname><forenames>Nadezhda</forenames></author><author><keyname>Karashtranova</keyname><forenames>Elena</forenames></author><author><keyname>Kostadinova</keyname><forenames>Dafina</forenames></author></authors><title>A Publicly Available Cross-Platform Lemmatizer for Bulgarian</title><categories>cs.CL</categories><comments>5 pages, Sixth International Scientific Conference - FMNS2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our dictionary-based lemmatizer for the Bulgarian language presented here is
distributed as free software, publicly available to download and use under the
GPL v3 license. The presented software is written entirely in Java and is
distributed as a GATE plugin. To our best knowledge, at the time of writing
this article, there are not any other free lemmatization tools specifically
targeting the Bulgarian language. The presented lemmatizer is a work in
progress and currently yields an accuracy of about 95% in comparison to the
manually annotated corpus BulTreeBank-Morph, which contains 273933 tokens.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04229</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04229</id><created>2015-06-13</created><authors><author><keyname>Karashtranova</keyname><forenames>Elena</forenames></author><author><keyname>Iliev</keyname><forenames>Grigor</forenames></author><author><keyname>Borisova</keyname><forenames>Nadezhda</forenames></author><author><keyname>Chankova</keyname><forenames>Yana</forenames></author><author><keyname>Atanasova</keyname><forenames>Irena</forenames></author></authors><title>Evaluation of the Accuracy of the BGLemmatizer</title><categories>cs.CL</categories><comments>5 pages, Sixth International Scientific Conference - FMNS2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reveals the results of an analysis of the accuracy of developed
software for automatic lemmatization for the Bulgarian language. This
lemmatization software is written entirely in Java and is distributed as a GATE
plugin. Certain statistical methods are used to define the accuracy of this
software. The results of the analysis show 95% lemmatization accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04233</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04233</id><created>2015-06-13</created><authors><author><keyname>Peng</keyname><forenames>Mugen</forenames></author><author><keyname>Yan</keyname><forenames>Shi</forenames></author><author><keyname>Zhang</keyname><forenames>Kecheng</forenames></author><author><keyname>Wang</keyname><forenames>Chonggang</forenames></author></authors><title>Fog Computing based Radio Access Networks: Issues and Challenges</title><categories>cs.IT cs.NI math.IT</categories><comments>21 pages, 7 figures, accepted by IEEE Networks Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fog computing based radio access network (F-RAN) is presented in this
article as a promising paradigm for the fifth generation (5G) wireless
communication system to provide high spectral and energy efficiency. The core
idea is to take full advantages of local radio signal processing, cooperative
radio resource management, and distributed storing capabilities in edge
devices, which can decrease the heavy burden on fronthaul and avoid large-scale
radio signal processing in the centralized baseband unit pool. This article
comprehensively presents the system architecture and key techniques of F-RANs.
In particular, key techniques and their corresponding solutions, including
transmission mode selection and interference suppression, are discussed. Open
issues in terms of edge caching, software-defined networking, and network
function virtualization, are also identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04243</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04243</id><created>2015-06-13</created><authors><author><keyname>Shi</keyname><forenames>Yuanming</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author><author><keyname>Bai</keyname><forenames>Bo</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author></authors><title>Large-Scale Convex Optimization for Ultra-Dense Cloud-RAN</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Wireless Commun. Mag., June 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The heterogeneous cloud radio access network (Cloud-RAN) provides a
revolutionary way to densify radio access networks. It enables centralized
coordination and signal processing for efficient interference management and
flexible network adaptation. Thus, it can resolve the main challenges for
next-generation wireless networks, including higher energy efficiency and
spectral efficiency, higher cost efficiency, scalable connectivity, and low
latency. In this article, we shall provide an algorithmic thinking on the new
design challenges for the dense heterogeneous Cloud-RAN based on convex
optimization. As problem sizes scale up with the network size, we will
demonstrate that it is critical to take unique structures of design problems
and inherent characteristics of wireless channels into consideration, while
convex optimization will serve as a powerful tool for such purposes. Network
power minimization and channel state information acquisition will be used as
two typical examples to demonstrate the effectiveness of convex optimization
methods. We will then present a two-stage framework to solve general
large-scale convex optimization problems, which is amenable to parallel
implementation in the cloud data center.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04251</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04251</id><created>2015-06-13</created><updated>2015-07-27</updated><authors><author><keyname>Ismaili</keyname><forenames>Anisse</forenames></author></authors><title>Efficiency in Multi-objective Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a multi-objective game, each agent individually evaluates each overall
action-profile on multiple objectives. I generalize the price of anarchy to
multi-objective games and provide a polynomial-time algorithm to assess it.
This work asserts that policies on tobacco promote a higher economic
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04255</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04255</id><created>2015-06-13</created><authors><author><keyname>Chen</keyname><forenames>Yongxin</forenames></author><author><keyname>Georgiou</keyname><forenames>Tryphon T.</forenames></author><author><keyname>Pavon</keyname><forenames>Michele</forenames></author></authors><title>Entropic and displacement interpolation: a computational approach using
  the Hilbert metric</title><categories>math.OC cs.SY</categories><comments>20 pages, 7 figures</comments><msc-class>47H07, 47H09, 60J25, 34A34, 49J20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monge-Kantorovich optimal mass transport (OMT) provides a blueprint for
geometries in the space of positive densities -- it quantifies the cost of
transporting a mass distribution into another. In particular, it provides
natural options for interpolation of distributions (displacement interpolation)
and for modeling flows. As such it has been the cornerstone of recent
developments in physics, probability theory, image processing, time-series
analysis, and several other fields. In spite of extensive work and theoretical
developments, the computation of OMT for large scale problems has remained a
challenging task. An alternative framework for interpolating distributions,
rooted in statistical mechanics and large deviations, is that of Schroedinger
bridges (entropic interpolation). This may be seen as a stochastic
regularization of OMT and can be cast as the stochastic control problem of
steering the probability density of the state-vector of a dynamical system
between two marginals. In this approach, however, the actual computation of
flows had hardly received any attention. In recent work on Schroedinger bridges
for Markov chains and quantum evolutions, we noted that the solution can be
efficiently obtained from the fixed-point of a map which is contractive in the
Hilbert metric. Thus, the purpose of this paper is to show that a similar
approach can be taken in the context of diffusion processes which i) leads to a
new proof of a classical result on Schroedinger bridges and ii) provides an
efficient computational scheme for both, Schroedinger bridges and OMT. We
illustrate this new computational approach by obtaining interpolation of
densities in representative examples such as interpolation of images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04257</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04257</id><created>2015-06-13</created><authors><author><keyname>Malloy</keyname><forenames>Matthew L.</forenames></author><author><keyname>Alfeld</keyname><forenames>Scott</forenames></author><author><keyname>Barford</keyname><forenames>Paul</forenames></author></authors><title>Contamination Estimation via Convex Relaxations</title><categories>cs.IT cs.LG math.IT math.OC</categories><comments>To appear, ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying anomalies and contamination in datasets is important in a wide
variety of settings. In this paper, we describe a new technique for estimating
contamination in large, discrete valued datasets. Our approach considers the
normal condition of the data to be specified by a model consisting of a set of
distributions. Our key contribution is in our approach to contamination
estimation. Specifically, we develop a technique that identifies the minimum
number of data points that must be discarded (i.e., the level of contamination)
from an empirical data set in order to match the model to within a specified
goodness-of-fit, controlled by a p-value. Appealing to results from large
deviations theory, we show a lower bound on the level of contamination is
obtained by solving a series of convex programs. Theoretical results guarantee
the bound converges at a rate of $O(\sqrt{\log(p)/p})$, where p is the size of
the empirical data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04263</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04263</id><created>2015-06-13</created><authors><author><keyname>Xu</keyname><forenames>Kuang</forenames></author></authors><title>Necessity of Future Information in Admission Control</title><categories>math.PR cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the necessity of predictive information in a class of queueing
admission control problems, where a system manager is allowed to divert
incoming jobs up to a fixed rate, in order to minimize the queueing delay
experienced by the admitted jobs.
  Spencer et al. (2014) show that the system's delay performance can be
significantly improved by having access to future information in the form of a
lookahead window, during which the times of future arrivals and services are
revealed. They prove that, while delay under an optimal online policy diverges
to infinity in the heavy-traffic regime, it can stay bounded by making use of
future information. However, the diversion polices of Spencer et al. (2014)
require the length of the lookahead window to grow to infinity at a non-trivial
rate in the heavy-traffic regime, and it remained open whether substantial
performance improvement could still be achieved with less future information.
  We resolve this question to a large extent by establishing an asymptotically
tight lower bound on how much future information is necessary to achieve
superior performance, which matches the upper bound of Spencer et al. (2014) up
to a constant multiplicative factor. Our result hence demonstrates that the
system's heavy-traffic delay performance is highly sensitive to the amount of
future information available. Our proof is based on analyzing certain excursion
probabilities of the input sample paths, and exploiting a connection between a
policy's diversion decisions and subsequent server idling, which may be of
independent interest for related dynamic resource allocation problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04265</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04265</id><created>2015-06-13</created><authors><author><keyname>Chakraborty</keyname><forenames>Sinjan</forenames></author><author><keyname>Kumar</keyname><forenames>Vineet</forenames></author></authors><title>A Study and Implementation of RSA Cryptosystem</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This project involves an implementation of the Rivest Shamir Adleman (RSA)
encryption algorithm in C. It consists of generation of two random prime
numbers and a number co- prime to phi(n) also called euler toitent function.
These three are used to generate a public key and private key. The user has to
enter a message which is encrypted by the public key. The algorithm also
decrypts the generated cipher text with the help of the private key and returns
the plain text message which was encrypted earlier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04268</identifier>
 <datestamp>2015-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04268</id><created>2015-06-13</created><authors><author><keyname>Waclawczyk</keyname><forenames>Tomasz</forenames></author></authors><title>A consistent solution of the reinitialization equation in the
  conservative level-set method</title><categories>cs.NA physics.comp-ph physics.flu-dyn</categories><comments>the paper is currently under review after the first revision in the
  Journal of Computational Physics (72 pages)</comments><journal-ref>J. Comput. Phys. 299 (2015) 487-525</journal-ref><doi>10.1016/j.jcp.2015.06.029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new re-initialization method for the conservative level-set
function is put forward. First, it has been shown that the re-initialization
and advection equations of the conservative level-set function are
mathematically equivalent to the re-initialization and advection equations of
the localized signed distance function. Next, a new discretization for the
spatial derivatives of the conservative level-set function has been proposed.
This new discretization is consistent with the re-initialization procedure and
it guarantees a second-order convergence rate of the interface curvature on
gradually refined grids. The new re-initialization method does not introduce
artificial deformations to stationary and non-stationary interfaces, even when
the number of re-initialization steps is large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04272</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04272</id><created>2015-06-13</created><authors><author><keyname>Pu</keyname><forenames>Fuan</forenames></author><author><keyname>Luo</keyname><forenames>Jian</forenames></author><author><keyname>Zhang</keyname><forenames>Yulai</forenames></author><author><keyname>Luo</keyname><forenames>Guiming</forenames></author></authors><title>Attacker and Defender Counting Approach for Abstract Argumentation</title><categories>cs.AI</categories><comments>7 pages, 2 figures;conference CogSci 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Dung's abstract argumentation, arguments are either acceptable or
unacceptable, given a chosen notion of acceptability. This gives a coarse way
to compare arguments. In this paper, we propose a counting approach for a more
fine-gained assessment to arguments by counting the number of their respective
attackers and defenders based on argument graph and argument game. An argument
is more acceptable if the proponent puts forward more number of defenders for
it and the opponent puts forward less number of attackers against it. We show
that our counting model has two well-behaved properties: normalization and
convergence. Then, we define a counting semantics based on this model, and
investigate some general properties of the semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04283</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04283</id><created>2015-06-13</created><authors><author><keyname>Molu</keyname><forenames>Mehdi M.</forenames></author></authors><title>Series Representation of Modified Bessel Functions and Its Application
  in AF Cooperative systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using fractional-calculus mathematics, a novel approach is introduced to
rewrite modified Bessel functions in series form using simple elementary
functions. Then, a statistical characterization of the total receive-SNR at the
destination, corresponding to the source-relay-destination and the
source-destination link SNR, is provided for a general relaying scenario, in
which the destination exploits a maximum ratio combining (MRC) receiver. Using
the novel statistical model for the total receive SNR at the destination,
accurate and simple analytical expressions for the outage probability, the bit
error probability and the ergodic capacity are obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04299</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04299</id><created>2015-06-13</created><updated>2015-09-19</updated><authors><author><keyname>Salimi</keyname><forenames>Babak</forenames></author><author><keyname>Bertossi</keyname><forenames>Leopoldo</forenames></author></authors><title>Query-Answer Causality in Databases: Abductive Diagnosis and
  View-Updates</title><categories>cs.DB cs.AI</categories><comments>To appear in Proc. UAI Causal Inference Workshop, 2015. One example
  was fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Causality has been recently introduced in databases, to model, characterize
and possibly compute causes for query results (answers). Connections between
query causality and consistency-based diagnosis and database repairs (wrt.
integrity constrain violations) have been established in the literature. In
this work we establish connections between query causality and abductive
diagnosis and the view-update problem. The unveiled relationships allow us to
obtain new complexity results for query causality -the main focus of our work-
and also for the two other areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04303</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04303</id><created>2015-06-13</created><authors><author><keyname>Deka</keyname><forenames>Deepjyoti</forenames></author><author><keyname>Baldick</keyname><forenames>Ross</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>One Breaker is Enough: Hidden Topology Attacks on Power Grids</title><categories>cs.CR math.OC</categories><comments>5 pages, 5 figures, Accepted to the IEEE PES General Meeting 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A coordinated cyber-attack on grid meter readings and breaker statuses can
lead to incorrect state estimation that can subsequently destabilize the grid.
This paper studies cyber-attacks by an adversary that changes breaker statuses
on transmission lines to affect the estimation of the grid topology. The
adversary, however, is incapable of changing the value of any meter data and
can only block recorded measurements on certain lines from being transmitted to
the control center. The proposed framework, with limited resource requirements
as compared to standard data attacks, thus extends the scope of cyber-attacks
to grids secure from meter corruption. We discuss necessary and sufficient
conditions for feasible attacks using a novel graph-coloring based analysis and
show that an optimal attack requires breaker status change at only ONE
transmission line. The potency of our attack regime is demonstrated through
simulations on IEEE test cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04304</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04304</id><created>2015-06-13</created><updated>2015-06-16</updated><authors><author><keyname>Maitin-Shepard</keyname><forenames>Jeremy</forenames><affiliation>UC Berkeley</affiliation><affiliation>Google</affiliation></author><author><keyname>Jain</keyname><forenames>Viren</forenames><affiliation>Google</affiliation></author><author><keyname>Januszewski</keyname><forenames>Michal</forenames><affiliation>Google</affiliation></author><author><keyname>Li</keyname><forenames>Peter</forenames><affiliation>Google</affiliation></author><author><keyname>Kornfeld</keyname><forenames>J&#xf6;rgen</forenames><affiliation>Max-Planck-Institute for Medical Research</affiliation></author><author><keyname>Buhmann</keyname><forenames>Julia</forenames><affiliation>Max-Planck-Institute for Medical Research</affiliation></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames><affiliation>UC Berkeley</affiliation></author></authors><title>Combinatorial Energy Learning for Image Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new machine learning approach for image segmentation, based on
a joint energy model over image features and novel local binary shape
descriptors. These descriptors compactly represent rich shape information at
multiple scales, including interactions between multiple objects. Our approach,
which does not rely on any hand-designed features, reflects the inherent
combinatorial nature of dense image segmentation problems. We propose efficient
algorithms for learning deep neural networks to model the joint energy, and for
local optimization of this energy in the space of supervoxel agglomerations.
  We demonstrate the effectiveness of our approach on 3-D biological data,
where rich shape information appears to be critical for resolving ambiguities.
On two challenging 3-D electron microscopy datasets highly relevant to ongoing
efforts towards large-scale dense mapping of neural circuits, we achieve
state-of-the-art segmentation accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04319</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04319</id><created>2015-06-13</created><authors><author><keyname>Leventi-Peetz</keyname><forenames>A. -M.</forenames></author><author><keyname>Peetz</keyname><forenames>J. -V.</forenames></author></authors><title>Generating S-Box Multivariate Quadratic Equation Systems And Estimating
  Algebraic Attack Resistance Aided By SageMath</title><categories>cs.CR</categories><comments>Ca. 20 pages, contains SageMath code listings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Methods are presented to derive with the aid of the computer mathematics
software system SageMath the Multivariate Quadratic equation systems (MQ) for
the input and output bit variables of a cryptographic S-box starting from its
algebraic expressions. Motivation to this work were the results of recent
articles which we have verified and extended in an original way, to our
knowledge, not yet published elsewhere. At the same time we present results
contrary to the published ones which cast serious doubts on the suitability of
previously presented formulas, supposed to quantify the resistance of S-boxes
against algebraic attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04320</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04320</id><created>2015-06-13</created><authors><author><keyname>Swenson</keyname><forenames>B.</forenames></author><author><keyname>Kar</keyname><forenames>S.</forenames></author><author><keyname>Xavier</keyname><forenames>J.</forenames></author></authors><title>A Computationally Efficient Implementation of Fictitious Play for
  Large-Scale Games</title><categories>math.OC cs.GT math.PR</categories><comments>Submitted for publication. Initial Submission: Jun. 2015. 15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is concerned with distributed learning and optimization in
large-scale settings. The well-known Fictitious Play (FP) algorithm has been
shown to achieve Nash equilibrium learning in certain classes of multi-agent
games. However, FP can be computationally difficult to implement when the
number of players is large. Sampled FP is a variant of FP that mitigates the
computational difficulties arising in FP by using a Monte-Carlo (i.e.,
sampling-based) approach. The Sampled FP algorithm has been studied both as a
tool for distributed learning and as an optimization heuristic for large-scale
problems. Despite its computational advantages, a shortcoming of Sampled FP is
that the number of samples that must be drawn in each round of the algorithm
grows without bound (on the order of $\sqrt{t}$, where $t$ is the round of the
repeated play). In this paper we propose Computationally Efficient Sampled FP
(CESFP)---a variant of Sampled FP in which only one sample need be drawn each
round of the algorithm (a substantial reduction from $O(\sqrt{t})$ samples per
round, as required in Sampled FP). CESFP operates using a
stochastic-approximation type rule to estimate the expected utility from round
to round. It is proven that the CESFP algorithm achieves Nash equilibrium
learning in the same sense as classical FP and Sampled FP. Simulation results
suggest that the convergence rate of CESFP (in terms of repeated-play
iterations) is similar to that of Sampled FP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04322</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04322</id><created>2015-06-13</created><updated>2016-02-15</updated><authors><author><keyname>Ahmed</keyname><forenames>Nesreen K.</forenames></author><author><keyname>Neville</keyname><forenames>Jennifer</forenames></author><author><keyname>Rossi</keyname><forenames>Ryan A.</forenames></author><author><keyname>Duffield</keyname><forenames>Nick</forenames></author><author><keyname>Willke</keyname><forenames>Theodore L.</forenames></author></authors><title>Graphlet Decomposition: Framework, Algorithms, and Applications</title><categories>cs.SI cs.DC cs.IR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From social science to biology, numerous applications often rely on graphlets
for intuitive and meaningful characterization of networks at both the global
macro-level as well as the local micro-level. While graphlets have witnessed a
tremendous success and impact in a variety of domains, there has yet to be a
fast and efficient approach for computing the frequencies of these subgraph
patterns. However, existing methods are not scalable to large networks with
millions of nodes and edges, which impedes the application of graphlets to new
problems that require large-scale network analysis. To address these problems,
we propose a fast, efficient, and parallel algorithm for counting graphlets of
size k={3,4}-nodes that take only a fraction of the time to compute when
compared with the current methods used. The proposed graphlet counting
algorithms leverages a number of proven combinatorial arguments for different
graphlets. For each edge, we count a few graphlets, and with these counts along
with the combinatorial arguments, we obtain the exact counts of others in
constant time. On a large collection of 300+ networks from a variety of
domains, our graphlet counting strategies are on average 460x faster than
current methods. This brings new opportunities to investigate the use of
graphlets on much larger networks and newer applications as we show in the
experiments. To the best of our knowledge, this paper provides the largest
graphlet computations to date as well as the largest systematic investigation
on over 300+ networks from a variety of domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04326</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04326</id><created>2015-06-13</created><authors><author><keyname>Pramukkul</keyname><forenames>Pensri</forenames></author><author><keyname>Svenkeson</keyname><forenames>Adam</forenames></author><author><keyname>West</keyname><forenames>Bruce J.</forenames></author><author><keyname>Grigolini</keyname><forenames>Paolo</forenames></author></authors><title>The Value of Conflict in Stable Social Networks</title><categories>physics.soc-ph cs.SI</categories><comments>5 pages, 3 figures, 1 supplement</comments><doi>10.1209/0295-5075/111/58003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A cooperative network model of sociological interest is examined to determine
the sensitivity of the global dynamics to having a fraction of the members
behaving uncooperatively, that is, being in conflict with the majority. We
study a condition where in the absence of these uncooperative individuals, the
contrarians, the control parameter exceeds a critical value and the network is
frozen in a state of consensus. The network dynamics change with variations in
the percentage of contrarians, resulting in a balance between the value of the
control parameter and the percentage of those in conflict with the majority. We
show that the transmission of information from a network $B$ to a network $A$,
with a small fraction of lookout members in $A$ who adopt the behavior of $B$,
becomes maximal when both networks are assigned the same critical percentage of
contrarians.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04330</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04330</id><created>2015-06-13</created><authors><author><keyname>Lukovszki</keyname><forenames>Tamas</forenames></author><author><keyname>Schmid</keyname><forenames>Stefan</forenames></author></authors><title>Online Admission Control and Embedding of Service Chains</title><categories>cs.NI</categories><comments>early version of SIROCCO 2015 paper</comments><acm-class>C.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The virtualization and softwarization of modern computer networks enables the
definition and fast deployment of novel network services called service chains:
sequences of virtualized network functions (e.g., firewalls, caches, traffic
optimizers) through which traffic is routed between source and destination.
This paper attends to the problem of admitting and embedding a maximum number
of service chains, i.e., a maximum number of source-destination pairs which are
routed via a sequence of to-be-allocated, capacitated network functions. We
consider an Online variant of this maximum Service Chain Embedding Problem,
short OSCEP, where requests arrive over time, in a worst-case manner. Our main
contribution is a deterministic O(log L)-competitive online algorithm, under
the assumption that capacities are at least logarithmic in L. We show that this
is asymptotically optimal within the class of deterministic and randomized
online algorithms. We also explore lower bounds for offline approximation
algorithms, and prove that the offline problem is APX-hard for unit capacities
and small L &gt; 2, and even Poly-APX-hard in general, when there is no bound on
L. These approximation lower bounds may be of independent interest, as they
also extend to other problems such as Virtual Circuit Routing. Finally, we
present an exact algorithm based on 0-1 programming, implying that the general
offline SCEP is in NP and by the above hardness results it is NP-complete for
constant L.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04333</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04333</id><created>2015-06-13</created><updated>2015-06-16</updated><authors><author><keyname>Bikakis</keyname><forenames>Nikos</forenames></author><author><keyname>Liagouris</keyname><forenames>John</forenames></author><author><keyname>Krommyda</keyname><forenames>Maria</forenames></author><author><keyname>Papastefanatos</keyname><forenames>George</forenames></author><author><keyname>Sellis</keyname><forenames>Timos</forenames></author></authors><title>Towards Scalable Visual Exploration of Very Large RDF Graphs</title><categories>cs.HC cs.DB</categories><comments>12th Extended Semantic Web Conference (ESWC 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we outline our work on developing a disk-based infrastructure
for efficient visualization and graph exploration operations over very large
graphs. The proposed platform, called graphVizdb, is based on a novel technique
for indexing and storing the graph. Particularly, the graph layout is indexed
with a spatial data structure, i.e., an R-tree, and stored in a database. In
runtime, user operations are translated into efficient spatial operations
(i.e., window queries) in the backend.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04334</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04334</id><created>2015-06-13</created><updated>2015-06-28</updated><authors><author><keyname>Buys</keyname><forenames>Jan</forenames></author><author><keyname>Blunsom</keyname><forenames>Phil</forenames></author></authors><title>A Bayesian Model for Generative Transition-based Dependency Parsing</title><categories>cs.CL</categories><comments>Depling 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple, scalable, fully generative model for transition-based
dependency parsing with high accuracy. The model, parameterized by Hierarchical
Pitman-Yor Processes, overcomes the limitations of previous generative models
by allowing fast and accurate inference. We propose an efficient decoding
algorithm based on particle filtering that can adapt the beam size to the
uncertainty in the model while jointly predicting POS tags and parse trees. The
UAS of the parser is on par with that of a greedy discriminative baseline. As a
language model, it obtains better perplexity than a n-gram model by performing
semi-supervised learning over a large unlabelled corpus. We show that the model
is able to generate locally and syntactically coherent sentences, opening the
door to further applications in language generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04338</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04338</id><created>2015-06-13</created><authors><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Lin</keyname><forenames>Haiting</forenames></author><author><keyname>Kang</keyname><forenames>Sing Bing</forenames></author><author><keyname>Yu</keyname><forenames>Jingyi</forenames></author></authors><title>Resolving Scale Ambiguity Via XSlit Aspect Ratio Analysis</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In perspective cameras, images of a frontal-parallel 3D object preserve its
aspect ratio invariant to its depth. Such an invariance is useful in
photography but is unique to perspective projection. In this paper, we show
that alternative non-perspective cameras such as the crossed-slit or XSlit
cameras exhibit a different depth-dependent aspect ratio (DDAR) property that
can be used to 3D recovery. We first conduct a comprehensive analysis to
characterize DDAR, infer object depth from its AR, and model recoverable depth
range, sensitivity, and error. We show that repeated shape patterns in real
Manhattan World scenes can be used for 3D reconstruction using a single XSlit
image. We also extend our analysis to model slopes of lines. Specifically,
parallel 3D lines exhibit depth-dependent slopes (DDS) on their images which
can also be used to infer their depths. We validate our analyses using real
XSlit cameras, XSlit panoramas, and catadioptric mirrors. Experiments show that
DDAR and DDS provide important depth cues and enable effective single-image
scene reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04340</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04340</id><created>2015-06-13</created><authors><author><keyname>Pandey</keyname><forenames>Rohit</forenames></author><author><keyname>Zhou</keyname><forenames>Yingbo</forenames></author><author><keyname>Govindaraju</keyname><forenames>Venu</forenames></author></authors><title>Deep Secure Encoding: An Application to Face Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present Deep Secure Encoding: a framework for secure
classification using deep neural networks, and apply it to the task of
biometric template protection for faces. Using deep convolutional neural
networks (CNNs), we learn a robust mapping of face classes to high entropy
secure codes. These secure codes are then hashed using standard hash functions
like SHA-256 to generate secure face templates. The efficacy of the approach is
shown on two face databases, namely, CMU-PIE and Extended Yale B, where we
achieve state of the art matching performance, along with cancelability and
high security with no unrealistic assumptions. Furthermore, the scheme can work
in both identification and verification modes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04349</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04349</id><created>2015-06-14</created><authors><author><keyname>Hern&#xe1;ndez-Orozco</keyname><forenames>Santiago</forenames></author><author><keyname>Hern&#xe1;ndez-Quiroz</keyname><forenames>Francisco</forenames></author><author><keyname>Zenil</keyname><forenames>Hector</forenames></author><author><keyname>Sieg</keyname><forenames>Wilfried</forenames></author></authors><title>Rare Speed-up in Automatic Theorem Proving Reveals Tradeoff Between
  Computational Time and Information Value</title><categories>cs.LO cs.AI</categories><comments>14 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that strategies implemented in automatic theorem proving involve an
interesting tradeoff between execution speed, proving speedup/computational
time and usefulness of information. We advance formal definitions for these
concepts by way of a notion of normality related to an expected (optimal)
theoretical speedup when adding useful information (other theorems as axioms),
as compared with actual strategies that can be effectively and efficiently
implemented. We propose the existence of an ineluctable tradeoff between this
normality and computational time complexity. The argument quantifies the
usefulness of information in terms of (positive) speed-up. The results disclose
a kind of no-free-lunch scenario and a tradeoff of a fundamental nature. The
main theorem in this paper together with the numerical experiment---undertaken
using two different automatic theorem provers AProS and Prover9 on random
theorems of propositional logic---provide strong theoretical and empirical
arguments for the fact that finding new useful information for solving a
specific problem (theorem) is, in general, as hard as the problem (theorem)
itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04350</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04350</id><created>2015-06-14</created><updated>2015-11-18</updated><authors><author><keyname>Gopalan</keyname><forenames>Parikshit</forenames></author><author><keyname>Kane</keyname><forenames>Daniel</forenames></author><author><keyname>Meka</keyname><forenames>Raghu</forenames></author></authors><title>Pseudorandomness via the discrete Fourier transform</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach to constructing unconditional pseudorandom
generators against classes of functions that involve computing a linear
function of the inputs. We give an explicit construction of a pseudorandom
generator that fools the discrete Fourier transforms of linear functions with
seed-length that is nearly logarithmic (up to polyloglog factors) in the input
size and the desired error parameter. Our result gives a single pseudorandom
generator that fools several important classes of tests computable in logspace
that have been considered in the literature, including halfspaces (over general
domains), modular tests and combinatorial shapes. For all these classes, our
generator is the first that achieves near logarithmic seed-length in both the
input length and the error parameter. Getting such a seed-length is a natural
challenge in its own right, which needs to be overcome in order to derandomize
RL - a central question in complexity theory.
  Our construction combines ideas from a large body of prior work, ranging from
a classical construction of [NN93] to the recent gradually increasing
independence paradigm of [KMN11, CRSW13, GMRTV12], while also introducing some
novel analytic machinery which might find other applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04352</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04352</id><created>2015-06-14</created><updated>2015-06-26</updated><authors><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Hu</keyname><forenames>Kai</forenames></author><author><keyname>Yin</keyname><forenames>Baolin</forenames></author></authors><title>Internet Traffic Matrix Structural Analysis Based on Multi-Resolution
  RPCA</title><categories>cs.NI</categories><comments>18 pages, in Chinese. This unpublished manuscript is an improvement
  on our previous papers in references [12] and [13]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet traffic matrix plays a significant roll in network operation and
management, therefore, the structural analysis of traffic matrix, which
decomposes different traffic components of this high-dimensional traffic
dataset, is quite valuable to some network applications. In this study, based
on the Robust Principal Component Analysis (RPCA) theory, a novel traffic
matrix structural analysis approach named Multi-Resolution RPCA is created,
which utilizes the wavelet multi-resolution analysis. Firstly, we build the
Multi-Resolution Traffic Matrix Decomposition Model (MR-TMDM), which
characterizes the smoothness of the deterministic traffic by its wavelet
coefficients. Secondly, based on this model, we improve the Stable Principal
Component Pursuit (SPCP), propose a new traffic matrix decomposition method
named SPCP-MRC with Multi-Resolution Constraints, and design its numerical
algorithm. Specifically, we give and prove the closed-form solution to a
sub-problem in the algorithm. Lastly, we evaluate different traffic
decomposition methods by multiple groups of simulated traffic matrices
containing different kinds of anomalies and distinct noise levels. It is
demonstrated that SPCP-MRC, compared with other methods, achieves more accurate
and more reasonable traffic decompositions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04356</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04356</id><created>2015-06-14</created><authors><author><keyname>Rajkovi&#x107;</keyname><forenames>Milan</forenames></author><author><keyname>Milovanovi&#x107;</keyname><forenames>Milo&#x161;</forenames></author></authors><title>The Artists who Forged Themselves: Detecting Creativity in Art</title><categories>cs.CV q-bio.NC</categories><comments>26 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Creativity and the understanding of cognitive processes involved in the
creative process are relevant to all of human activities. Comprehension of
creativity in the arts is of special interest due to the involvement of many
scientific and non scientific disciplines. Using digital representation of
paintings, we show that creative process in painting art may be objectively
recognized within the mathematical framework of self organization, a process
characteristic of nonlinear dynamic systems and occurring in natural and social
sciences. Unlike the artist identification process or the recognition of
forgery, which presupposes the knowledge of the original work, our method
requires no prior knowledge on the originality of the work of art. The original
paintings are recognized as realizations of the creative process which, in
general, is shown to correspond to self-organization of texture features which
determine the aesthetic complexity of the painting. The method consists of the
wavelet based statistical digital image processing and the measure of
statistical complexity which represents the minimal (average) information
necessary for optimal prediction. The statistical complexity is based on the
properly defined causal states with optimal predictive properties. Two
different time concepts related to the works of art are introduced: the
internal time and the artistic time. The internal time of the artwork is
determined by the span of causal dependencies between wavelet coefficients
while the artistic time refers to the internal time during which complexity
increases where complexity refers to compositional, aesthetic and structural
arrangement of texture features. The method is illustrated by recognizing the
original paintings from the copies made by the artists themselves, including
the works of the famous surrealist painter Ren\'{e} Magritte.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04359</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04359</id><created>2015-06-14</created><authors><author><keyname>Lei</keyname><forenames>Yunwen</forenames></author><author><keyname>Dogan</keyname><forenames>&#xdc;r&#xfc;n</forenames></author><author><keyname>Binder</keyname><forenames>Alexander</forenames></author><author><keyname>Kloft</keyname><forenames>Marius</forenames></author></authors><title>Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to
  Novel Algorithms</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the generalization performance of multi-class
classification algorithms, for which we obtain, for the first time, a
data-dependent generalization error bound with a logarithmic dependence on the
class size, substantially improving the state-of-the-art linear dependence in
the existing data-dependent generalization analysis. The theoretical analysis
motivates us to introduce a new multi-class classification machine based on
$\ell_p$-norm regularization, where the parameter $p$ controls the complexity
of the corresponding bounds. We derive an efficient optimization algorithm
based on Fenchel duality theory. Benchmarks on several real-world datasets show
that the proposed algorithm can achieve significant accuracy gains over the
state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04364</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04364</id><created>2015-06-14</created><authors><author><keyname>Lei</keyname><forenames>Yunwen</forenames></author><author><keyname>Binder</keyname><forenames>Alexander</forenames></author><author><keyname>Dogan</keyname><forenames>&#xdc;r&#xfc;n</forenames></author><author><keyname>Kloft</keyname><forenames>Marius</forenames></author></authors><title>Localized Multiple Kernel Learning---A Convex Approach</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a localized approach to multiple kernel learning that, in contrast
to prevalent approaches, can be formulated as a convex optimization problem
over a given cluster structure. From which we obtain the first generalization
error bounds for localized multiple kernel learning and derive an efficient
optimization algorithm based on the Fenchel dual representation. Experiments on
real-world datasets from the application domains of computational biology and
computer vision show that the convex approach to localized multiple kernel
learning can achieve higher prediction accuracies than its global and
non-convex local counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04365</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04365</id><created>2015-06-14</created><authors><author><keyname>Chen</keyname><forenames>Kuan-Yu</forenames></author><author><keyname>Liu</keyname><forenames>Shih-Hung</forenames></author><author><keyname>Wang</keyname><forenames>Hsin-Min</forenames></author><author><keyname>Chen</keyname><forenames>Berlin</forenames></author><author><keyname>Chen</keyname><forenames>Hsin-Hsi</forenames></author></authors><title>Leveraging Word Embeddings for Spoken Document Summarization</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Owing to the rapidly growing multimedia content available on the Internet,
extractive spoken document summarization, with the purpose of automatically
selecting a set of representative sentences from a spoken document to concisely
express the most important theme of the document, has been an active area of
research and experimentation. On the other hand, word embedding has emerged as
a newly favorite research subject because of its excellent performance in many
natural language processing (NLP)-related tasks. However, as far as we are
aware, there are relatively few studies investigating its use in extractive
text or speech summarization. A common thread of leveraging word embeddings in
the summarization process is to represent the document (or sentence) by
averaging the word embeddings of the words occurring in the document (or
sentence). Then, intuitively, the cosine similarity measure can be employed to
determine the relevance degree between a pair of representations. Beyond the
continued efforts made to improve the representation of words, this paper
focuses on building novel and efficient ranking models based on the general
word embedding methods for extractive speech summarization. Experimental
results demonstrate the effectiveness of our proposed methods, compared to
existing state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04366</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04366</id><created>2015-06-14</created><authors><author><keyname>Franz</keyname><forenames>Arthur</forenames></author></authors><title>Artificial general intelligence through recursive data compression and
  grounded reasoning: a position paper</title><categories>cs.AI</categories><comments>27 pages, 3 figures, position paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a tentative outline for the construction of an
artificial, generally intelligent system (AGI). It is argued that building a
general data compression algorithm solving all problems up to a complexity
threshold should be the main thrust of research. A measure for partial progress
in AGI is suggested. Although the details are far from being clear, some
general properties for a general compression algorithm are fleshed out. Its
inductive bias should be flexible and adapt to the input data while constantly
searching for a simple, orthogonal and complete set of hypotheses explaining
the data. It should recursively reduce the size of its representations thereby
compressing the data increasingly at every iteration.
  Abstract Based on that fundamental ability, a grounded reasoning system is
proposed. It is argued how grounding and flexible feature bases made of
hypotheses allow for resourceful thinking. While the simulation of
representation contents on the mental stage accounts for much of the power of
propositional logic, compression leads to simple sets of hypotheses that allow
the detection and verification of universally quantified statements.
  Abstract Together, it is highlighted how general compression and grounded
reasoning could account for the birth and growth of first concepts about the
world and the commonsense reasoning about them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04374</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04374</id><created>2015-06-14</created><authors><author><keyname>Nakaizumi</keyname><forenames>Chisaki</forenames></author><author><keyname>Makino</keyname><forenames>Shoji</forenames></author><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author></authors><title>Head-related Impulse Response Cues for Spatial Auditory Brain-computer
  Interface</title><categories>q-bio.NC cs.HC</categories><comments>4 pages, 4 figures, accepted for EMBC 2015, IEEE copyright</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This study provides a comprehensive test of a head-related impulse response
(HRIR) cues for a spatial auditory brain-computer interface (saBCI) speller
paradigm. We present a comparison with the conventional virtual sound
headphone-based spatial auditory modality. We propose and optimize the three
types of sound spatialization settings using a variable elevation in order to
evaluate the HRIR efficacy for the saBCI. Three experienced and seven naive BCI
users participated in the three experimental setups based on ten presented
Japanese syllables. The obtained EEG auditory evoked potentials (AEP) resulted
with encouragingly good and stable P300 responses in online BCI experiments.
Our case study indicated that users could perceive elevation in the saBCI
experiments generated using the HRIR measured from a general head model. The
saBCI accuracy and information transfer rate (ITR) scores have been improved
comparing to the classical horizontal plane-based virtual spatial sound
reproduction modality, as far as the healthy users in the current pilot study
are concerned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04380</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04380</id><created>2015-06-14</created><updated>2016-02-19</updated><authors><author><keyname>Dujmovi&#x107;</keyname><forenames>Vida</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Wood</keyname><forenames>David R.</forenames></author></authors><title>Structure of Graphs with Locally Restricted Crossings</title><categories>math.CO cs.CG cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider relations between the size, treewidth, and local crossing number
(maximum number of crossings per edge) of graphs embedded on topological
surfaces. We show that an $n$-vertex graph embedded on a surface of genus $g$
with at most $k$ crossings per edge has treewidth $O(\sqrt{(g+1)(k+1)n})$ and
layered treewidth $O((g+1)k)$, and that these bounds are tight up to a constant
factor. As a special case, the $k$-planar graphs with $n$ vertices have
treewidth $O(\sqrt{(k+1)n})$ and layered treewidth $O(k+1)$, which are tight
bounds that improve a previously known $O((k+1)^{3/4}n^{1/2})$ treewidth bound.
Analogous results are proved for map graphs defined with respect to any
surface. Finally, we show that for $g&lt;m$, every $m$-edge graph can be embedded
on a surface of genus~$g$ with $O((m/(g+1))\log^2 g)$ crossings per edge, which
is tight to a polylogarithmic factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04383</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04383</id><created>2015-06-14</created><updated>2015-08-10</updated><authors><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author><author><keyname>N&#xf6;llenburg</keyname><forenames>Martin</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author></authors><title>Drawing Large Graphs by Multilevel Maxent-Stress Optimization</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drawing large graphs appropriately is an important step for the visual
analysis of data from real-world networks. Here we present a novel multilevel
algorithm to compute a graph layout with respect to a recently proposed metric
that combines layout stress and entropy. As opposed to previous work, we do not
solve the linear systems of the maxent-stress metric with a typical numerical
solver. Instead we use a simple local iterative scheme within a multilevel
approach. To accelerate local optimization, we approximate long-range forces
and use shared-memory parallelism. Our experiments validate the high potential
of our approach, which is particularly appealing for dynamic graphs. In
comparison to the previously best maxent-stress optimizer, which is sequential,
our parallel implementation is on average 30 times faster already for static
graphs (and still faster if executed on one thread) while producing a
comparable solution quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04384</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04384</id><created>2015-06-14</created><authors><author><keyname>Khan</keyname><forenames>Khalid</forenames></author><author><keyname>Lobiyal</keyname><forenames>D. K.</forenames></author></authors><title>Performance evaluation of different optimization techniques for coverage
  and connectivity control in backbone based wireless networks</title><categories>cs.NI</categories><comments>15 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, performance evaluation of Newton-Raphson and Conjugate
Gradient method has been studied in comparison to Steepest Decent method for
coverage and connectivity control in backbone based wireless networks. In order
to design such wireless networks, the main challenge is to ensure network
requirements such as network coverage and connectivity. To optimize coverage
and connectivity, backbone nodes will be repositioned by the use of mobility
control based on above mentioned methods. Thus the network get self organized
which autonomously achieve energy minimizing configuration. Furthermore by
simulation using MATLAB R2010a, methods are compared on the basis of optimized
cost, number of iterations and elapsed time i.e. total time taken to execute
the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04391</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04391</id><created>2015-06-14</created><updated>2015-12-21</updated><authors><author><keyname>Pasquier</keyname><forenames>Thomas F. J. -M.</forenames></author><author><keyname>Singh</keyname><forenames>Jatinder</forenames></author><author><keyname>Eyers</keyname><forenames>David</forenames></author><author><keyname>Bacon</keyname><forenames>Jean</forenames></author></authors><title>CamFlow: Managed Data-sharing for Cloud Services</title><categories>cs.CR cs.DC</categories><comments>14 pages, 8 figures</comments><acm-class>D.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model of cloud services is emerging whereby a few trusted providers manage
the underlying hardware and communications whereas many companies build on this
infrastructure to offer higher level, cloud-hosted PaaS services and/or SaaS
applications. From the start, strong isolation between cloud tenants was seen
to be of paramount importance, provided first by virtual machines (VM) and
later by containers, which share the operating system (OS) kernel. Increasingly
it is the case that applications also require facilities to effect isolation
and protection of data managed by those applications. They also require
flexible data sharing with other applications, often across the traditional
cloud-isolation boundaries; for example, when government provides many related
services for its citizens on a common platform. Similar considerations apply to
the end-users of applications. But in particular, the incorporation of cloud
services within `Internet of Things' architectures is driving the requirements
for both protection and cross-application data sharing.
  These concerns relate to the management of data. Traditional access control
is application and principal/role specific, applied at policy enforcement
points, after which there is no subsequent control over where data flows; a
crucial issue once data has left its owner's control by cloud-hosted
applications and within cloud-services. Information Flow Control (IFC), in
addition, offers system-wide, end-to-end, flow control based on the properties
of the data. We discuss the potential of cloud-deployed IFC for enforcing
owners' dataflow policy with regard to protection and sharing, as well as
safeguarding against malicious or buggy software. In addition, the audit log
associated with IFC provides transparency, giving configurable system-wide
visibility over data flows. [...]
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04395</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04395</id><created>2015-06-14</created><updated>2015-12-20</updated><authors><author><keyname>He</keyname><forenames>Pan</forenames></author><author><keyname>Huang</keyname><forenames>Weilin</forenames></author><author><keyname>Qiao</keyname><forenames>Yu</forenames></author><author><keyname>Loy</keyname><forenames>Chen Change</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Reading Scene Text in Deep Convolutional Sequences</title><categories>cs.CV</categories><comments>To appear in the 13th AAAI Conference on Artificial Intelligence
  (AAAI-16), 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a Deep-Text Recurrent Network (DTRN) that regards scene text
reading as a sequence labelling problem. We leverage recent advances of deep
convolutional neural networks to generate an ordered high-level sequence from a
whole word image, avoiding the difficult character segmentation problem. Then a
deep recurrent model, building on long short-term memory (LSTM), is developed
to robustly recognize the generated CNN sequences, departing from most existing
approaches recognising each character independently. Our model has a number of
appealing properties in comparison to existing scene text recognition methods:
(i) It can recognise highly ambiguous words by leveraging meaningful context
information, allowing it to work reliably without either pre- or
post-processing; (ii) the deep CNN feature is robust to various image
distortions; (iii) it retains the explicit order information in word image,
which is essential to discriminate word strings; (iv) the model does not depend
on pre-defined dictionary, and it can process unknown words and arbitrary
strings. Codes for the DTRN will be available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04416</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04416</id><created>2015-06-14</created><updated>2015-11-06</updated><authors><author><keyname>Korattikara</keyname><forenames>Anoop</forenames></author><author><keyname>Rathod</keyname><forenames>Vivek</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author></authors><title>Bayesian Dark Knowledge</title><categories>cs.LG stat.ML</categories><comments>final version submitted to NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of Bayesian parameter estimation for deep neural
networks, which is important in problem settings where we may have little data,
and/ or where we need accurate posterior predictive densities, e.g., for
applications involving bandits or active learning. One simple approach to this
is to use online Monte Carlo methods, such as SGLD (stochastic gradient
Langevin dynamics). Unfortunately, such a method needs to store many copies of
the parameters (which wastes memory), and needs to make predictions using many
versions of the model (which wastes time).
  We describe a method for &quot;distilling&quot; a Monte Carlo approximation to the
posterior predictive density into a more compact form, namely a single deep
neural network. We compare to two very recent approaches to Bayesian neural
networks, namely an approach based on expectation propagation [Hernandez-Lobato
and Adams, 2015] and an approach based on variational Bayes [Blundell et al.,
2015]. Our method performs better than both of these, is much simpler to
implement, and uses less computation at test time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04417</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04417</id><created>2015-06-14</created><authors><author><keyname>McGregor</keyname><forenames>Andrew</forenames></author><author><keyname>Tench</keyname><forenames>David</forenames></author><author><keyname>Vorotnikova</keyname><forenames>Sofya</forenames></author><author><keyname>Vu</keyname><forenames>Hoa T.</forenames></author></authors><title>Densest Subgraph in Dynamic Graph Streams</title><categories>cs.DS</categories><comments>To appear in MFCS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of approximating the densest subgraph
in the dynamic graph stream model. In this model of computation, the input
graph is defined by an arbitrary sequence of edge insertions and deletions and
the goal is to analyze properties of the resulting graph given memory that is
sub-linear in the size of the stream. We present a single-pass algorithm that
returns a $(1+\epsilon)$ approximation of the maximum density with high
probability; the algorithm uses $O(\epsilon^{-2} n \polylog n)$ space,
processes each stream update in $\polylog (n)$ time, and uses $\poly(n)$
post-processing time where $n$ is the number of nodes. The space used by our
algorithm matches the lower bound of Bahmani et al.~(PVLDB 2012) up to a
poly-logarithmic factor for constant $\epsilon$. The best existing results for
this problem were established recently by Bhattacharya et al.~(STOC 2015). They
presented a $(2+\epsilon)$ approximation algorithm using similar space and
another algorithm that both processed each update and maintained a
$(4+\epsilon)$ approximation of the current maximum density in $\polylog (n)$
time per-update.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04422</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04422</id><created>2015-06-14</created><updated>2015-06-18</updated><authors><author><keyname>Pinto</keyname><forenames>Rafael</forenames></author><author><keyname>Engel</keyname><forenames>Paulo</forenames></author></authors><title>A Fast Incremental Gaussian Mixture Model</title><categories>cs.LG</categories><comments>10 pages, no figures, draft submission to Plos One</comments><acm-class>I.2.6</acm-class><doi>10.1371/journal.pone.0141942</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work builds upon previous efforts in online incremental learning, namely
the Incremental Gaussian Mixture Network (IGMN). The IGMN is capable of
learning from data streams in a single-pass by improving its model after
analyzing each data point and discarding it thereafter. Nevertheless, it
suffers from the scalability point-of-view, due to its asymptotic time
complexity of $\operatorname{O}\bigl(NKD^3\bigr)$ for $N$ data points, $K$
Gaussian components and $D$ dimensions, rendering it inadequate for
high-dimensional data. In this paper, we manage to reduce this complexity to
$\operatorname{O}\bigl(NKD^2\bigr)$ by deriving formulas for working directly
with precision matrices instead of covariance matrices. The final result is a
much faster and scalable algorithm which can be applied to high dimensional
tasks. This is confirmed by applying the modified algorithm to high-dimensional
classification datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04423</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04423</id><created>2015-06-14</created><updated>2015-09-27</updated><authors><author><keyname>Knauer</keyname><forenames>Kolja</forenames></author><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author></authors><title>Graph drawings with one bend and few slopes</title><categories>cs.CG cs.DM math.CO</categories><comments>minor revision. arXiv admin note: text overlap with arXiv:1205.2548</comments><msc-class>05C62, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider drawings of graphs in the plane in which edges are represented by
polygonal paths with at most one bend and the number of different slopes used
by all segments of these paths is small. We prove that
$\lceil\frac{\Delta}{2}\rceil$ edge slopes suffice for outerplanar drawings of
outerplanar graphs with maximum degree $\Delta\geq 3$. This matches the obvious
lower bound. We also show that $\lceil\frac{\Delta}{2}\rceil+1$ edge slopes
suffice for drawings of general graphs, improving on the previous bound of
$\Delta+1$. Furthermore, we improve previous upper bounds on the number of
slopes needed for planar drawings of planar and bipartite planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04428</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04428</id><created>2015-06-14</created><authors><author><keyname>Cohen</keyname><forenames>Gil</forenames></author></authors><title>Two-Source Dispersers for Polylogarithmic Entropy and Improved Ramsey
  Graphs</title><categories>math.CO cs.CC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In his 1947 paper that inaugurated the probabilistic method, Erd\H{o}s proved
the existence of $2\log{n}$-Ramsey graphs on $n$ vertices. Matching Erd\H{o}s'
result with a constructive proof is a central problem in combinatorics, that
has gained a significant attention in the literature. The state of the art
result was obtained in the celebrated paper by Barak, Rao, Shaltiel and
Wigderson [Ann. Math'12], who constructed a
$2^{2^{(\log\log{n})^{1-\alpha}}}$-Ramsey graph, for some small universal
constant $\alpha &gt; 0$.
  In this work, we significantly improve the result of Barak~\etal and
construct $2^{(\log\log{n})^c}$-Ramsey graphs, for some universal constant $c$.
In the language of theoretical computer science, our work resolves the problem
of explicitly constructing two-source dispersers for polylogarithmic entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04429</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04429</id><created>2015-06-14</created><authors><author><keyname>Desmedt</keyname><forenames>Yvo</forenames></author><author><keyname>Erotokritou</keyname><forenames>Stelios</forenames></author></authors><title>Making Code Voting Secure against Insider Threats using Unconditionally
  Secure MIX Schemes and Human PSMT Protocols</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Code voting was introduced by Chaum as a solution for using a possibly
infected-by-malware device to cast a vote in an electronic voting application.
Chaum's work on code voting assumed voting codes are physically delivered to
voters using the mail system, implicitly requiring to trust the mail system.
This is not necessarily a valid assumption to make - especially if the mail
system cannot be trusted. When conspiring with the recipient of the cast
ballots, privacy is broken.
  It is clear to the public that when it comes to privacy, computers and
&quot;secure&quot; communication over the Internet cannot fully be trusted. This
emphasizes the importance of using: (1) Unconditional security for secure
network communication. (2) Reduce reliance on untrusted computers.
  In this paper we explore how to remove the mail system trust assumption in
code voting. We use PSMT protocols (SCN 2012) where with the help of visual
aids, humans can carry out $\mod 10$ addition correctly with a 99\% degree of
accuracy. We introduce an unconditionally secure MIX based on the combinatorics
of set systems.
  Given that end users of our proposed voting scheme construction are humans we
\emph{cannot use} classical Secure Multi Party Computation protocols.
  Our solutions are for both single and multi-seat elections achieving:
\begin{enumerate}[i)]
  \item An anonymous and perfectly secure communication network secure against
a $t$-bounded passive adversary used to deliver voting,
  \item The end step of the protocol can be handled by a human to evade the
threat of malware. \end{enumerate} We do not focus on active adversaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04440</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04440</id><created>2015-06-14</created><authors><author><keyname>Kaplan</keyname><forenames>Nathan</forenames></author><author><keyname>Petrow</keyname><forenames>Ian</forenames></author></authors><title>Traces of Hecke Operators and Refined Weight Enumerators of Reed-Solomon
  Codes</title><categories>math.NT cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the quadratic residue weight enumerators of the dual projective
Reed-Solomon codes of dimensions $5$ and $q-4$ over the finite field
$\mathbb{F}_q$. Our main results are formulas for the coefficients of the the
quadratic residue weight enumerators for such codes. If $q=p^v$ and we fix $v$
and vary $p$ then our formulas for the coefficients of the dimension $q-4$ code
involve only polynomials in $p$ and the trace of the $q$-th and $(q/p^2)$-th
Hecke operators acting on spaces of cusp forms for the congruence groups
$\operatorname{SL}_2 (\mathbb{Z}), \Gamma_0(2)$, and $\Gamma_0(4)$. The main
tool we use is the Eichler-Selberg trace formula, which gives along the way a
variation of a theorem of Birch on the distribution of rational point counts
for elliptic curves with prescribed $2$-torsion over a fixed finite field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04444</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04444</id><created>2015-06-14</created><authors><author><keyname>Zhang</keyname><forenames>Shuai</forenames></author><author><keyname>Yin</keyname><forenames>Penghang</forenames></author><author><keyname>Xin</keyname><forenames>Jack</forenames></author></authors><title>Transformed Schatten-1 Iterative Thresholding Algorithms for Matrix Rank
  Minimization and Applications</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a non-convex low-rank promoting penalty function, the transformed
Schatten-1 (TS1), and its applications in matrix completion. The TS1 penalty,
as a matrix quasi-norm defined on its singular values, interpolates the rank
and the nuclear norm through a nonnegative parameter a. We consider the
unconstrained TS1 regularized low-rank matrix recovery problem and develop a
fixed point representation for its global minimizer. The TS1 thresholding
functions are in closed analytical form for all parameter values. The TS1
threshold values differ in subcritical (supercritical) parameter regime where
the TS1 threshold functions are continuous (discontinuous). We propose TS1
iterative thresholding algorithms and compare them with some state-of-the-art
algorithms on matrix completion test problems. For problems with known rank, a
fully adaptive TS1 iterative thresholding algorithm consistently performs the
best under different conditions with ground truth matrix being multivariate
Gaussian at varying covariance. For problems with unknown rank, TS1 algorithms
with an additional rank estimation procedure approach the level of IRucL-q
which is an iterative reweighted algorithm, non-convex in nature and best in
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04447</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04447</id><created>2015-06-14</created><authors><author><keyname>Hu</keyname><forenames>Qinran</forenames></author><author><keyname>Li</keyname><forenames>Fangxing</forenames></author></authors><title>An Optimal Framework for Residential Load Aggregator</title><categories>cs.CE</categories><comments>to be submitted to IEEE Trans. on Smart Grid</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the development of intelligent demand-side management with automatic
control, distributed populations of large residential loads, such as air
conditioners (ACs) and electrical water heaters (EWHs), have the opportunities
to provide effective demand-side ancillary services for load serving entities
(LSEs) to reduce the emissions and network operating costs. Most present
approaches are restricted to 1) the scenarios involving with efficiently
scheduling the large number of appliances in real time, 2) the issues about
evaluating the contributions of individual residents towards participating
demand response (DR) program, and fairly distributing the rewards, and 3) the
concerns on performing cost-effective demand reduction request (DRR) for LSEs
with minimal rewards costs while not affecting their living comfortableness.
Therefore, this paper presents an optimal framework for residential load
aggregators (RLAs) which helps solve the problems mentioned above. Under this
framework, RLAs are able to realize the DRR for LSEs to generate optimal
control strategies over residential appliances quickly and efficiently. To
residents, the framework is designed with probabilistic model of
comfortableness, which minimizes the impact of DR program to their daily life.
To LSEs, the framework helps minimize the total reward costs of performing
DRRs. Moreover, the framework fairly and strategically distributes the
financial rewards to residents, which may stimulate the potential capability of
loads optimized and controlled by RLAs in demand side management. The proposed
framework has been validated on several numerical case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04448</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04448</id><created>2015-06-14</created><updated>2015-10-20</updated><authors><author><keyname>Wang</keyname><forenames>Yining</forenames></author><author><keyname>Tung</keyname><forenames>Hsiao-Yu</forenames></author><author><keyname>Smola</keyname><forenames>Alexander</forenames></author><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author></authors><title>Fast and Guaranteed Tensor Decomposition via Sketching</title><categories>stat.ML cs.LG</categories><comments>29 pages. Appeared in Proceedings of Advances in Neural Information
  Processing Systems (NIPS), held at Montreal, Canada in 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in
statistical learning of latent variable models and in data mining. In this
paper, we propose fast and randomized tensor CP decomposition algorithms based
on sketching. We build on the idea of count sketches, but introduce many novel
ideas which are unique to tensors. We develop novel methods for randomized
computation of tensor contractions via FFTs, without explicitly forming the
tensors. Such tensor contractions are encountered in decomposition methods such
as tensor power iterations and alternating least squares. We also design novel
colliding hashes for symmetric tensors to further save time in computing the
sketches. We then combine these sketching ideas with existing whitening and
tensor power iterative techniques to obtain the fastest algorithm on both
sparse and dense tensors. The quality of approximation under our method does
not depend on properties such as sparsity, uniformity of elements, etc. We
apply the method for topic modeling and obtain competitive results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04449</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04449</id><created>2015-06-14</created><authors><author><keyname>Chen</keyname><forenames>Wenlin</forenames></author><author><keyname>Wilson</keyname><forenames>James T.</forenames></author><author><keyname>Tyree</keyname><forenames>Stephen</forenames></author><author><keyname>Weinberger</keyname><forenames>Kilian Q.</forenames></author><author><keyname>Chen</keyname><forenames>Yixin</forenames></author></authors><title>Compressing Convolutional Neural Networks</title><categories>cs.LG cs.CV cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks (CNN) are increasingly used in many areas of
computer vision. They are particularly attractive because of their ability to
&quot;absorb&quot; great quantities of labeled data through millions of parameters.
However, as model sizes increase, so do the storage and memory requirements of
the classifiers. We present a novel network architecture, Frequency-Sensitive
Hashed Nets (FreshNets), which exploits inherent redundancy in both
convolutional layers and fully-connected layers of a deep learning model,
leading to dramatic savings in memory and storage consumption. Based on the key
observation that the weights of learned convolutional filters are typically
smooth and low-frequency, we first convert filter weights to the frequency
domain with a discrete cosine transform (DCT) and use a low-cost hash function
to randomly group frequency parameters into hash buckets. All parameters
assigned the same hash bucket share a single value learned with standard
back-propagation. To further reduce model size we allocate fewer hash buckets
to high-frequency components, which are generally less important. We evaluate
FreshNets on eight data sets, and show that it leads to drastically better
compressed performance than several relevant baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04458</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04458</id><created>2015-06-14</created><authors><author><keyname>Shimizu</keyname><forenames>Kensuke</forenames></author><author><keyname>Makino</keyname><forenames>Shoji</forenames></author><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author></authors><title>Inter-stimulus Interval Study for the Tactile Point-pressure
  Brain-computer Interface</title><categories>q-bio.NC cs.HC</categories><comments>4 pages, 5 figures, accepted for EMBC 2015, IEEE copyright</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The paper presents a study of an inter-stimulus interval (ISI) influence on a
tactile point-pressure stimulus-based brain-computer interface's (tpBCI)
classification accuracy. A novel tactile pressure generating tpBCI stimulator
is also discussed, which is based on a three-by-three pins' matrix prototype.
The six pin-linear patterns are presented to the user's palm during the online
tpBCI experiments in an oddball style paradigm allowing for &quot;the aha-responses&quot;
elucidation, within the event related potential (ERP). A subsequent
classification accuracies' comparison is discussed based on two ISI settings in
an online tpBCI application. A research hypothesis of classification
accuracies' non-significant differences with various ISIs is confirmed based on
the two settings of 120 ms and 300 ms, as well as with various numbers of ERP
response averaging scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04461</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04461</id><created>2015-06-14</created><updated>2015-06-22</updated><authors><author><keyname>Aminaka</keyname><forenames>Daiki</forenames></author><author><keyname>Makino</keyname><forenames>Shoji</forenames></author><author><keyname>Rutkowski</keyname><forenames>Tomasz M.</forenames></author></authors><title>Chromatic and High-frequency cVEP-based BCI Paradigm</title><categories>q-bio.NC cs.HC</categories><comments>4 pages, 4 figures, accepted for EMBC 2015, IEEE copyright</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present results of an approach to a code-modulated visual evoked potential
(cVEP) based brain-computer interface (BCI) paradigm using four high-frequency
flashing stimuli. To generate higher frequency stimulation compared to the
state-of-the-art cVEP-based BCIs, we propose to use the light-emitting diodes
(LEDs) driven from a small micro-controller board hardware generator designed
by our team. The high-frequency and green-blue chromatic flashing stimuli are
used in the study in order to minimize a danger of a photosensitive epilepsy
(PSE). We compare the the green-blue chromatic cVEP-based BCI accuracies with
the conventional white-black flicker based interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04462</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04462</id><created>2015-06-14</created><authors><author><keyname>Adelman</keyname><forenames>Ross</forenames></author><author><keyname>Gumerov</keyname><forenames>Nail A.</forenames></author><author><keyname>Duraiswami</keyname><forenames>Ramani</forenames></author></authors><title>Accurate computation of Galerkin double surface integrals in the 3-D
  boundary element method</title><categories>physics.comp-ph cs.MS cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many boundary element integral equation kernels are based on the Green's
functions of the Laplace and Helmholtz equations in three dimensions. These
include, for example, the Laplace, Helmholtz, elasticity, Stokes, and Maxwell's
equations. Integral equation formulations lead to more compact, but dense
linear systems. These dense systems are often solved iteratively via Krylov
subspace methods, which may be accelerated via the fast multipole method. There
are advantages to Galerkin formulations for such integral equations, as they
treat problems associated with kernel singularity, and lead to symmetric and
better conditioned matrices. However, the Galerkin method requires each entry
in the system matrix to be created via the computation of a double surface
integral over one or more pairs of triangles. There are a number of
semi-analytical methods to treat these integrals, which all have some issues,
and are discussed in this paper. We present novel methods to compute all the
integrals that arise in Galerkin formulations involving kernels based on the
Laplace and Helmholtz Green's functions to any specified accuracy. Integrals
involving completely geometrically separated triangles are non-singular and are
computed using a technique based on spherical harmonics and multipole
expansions and translations, which results in the integration of polynomial
functions over the triangles. Integrals involving cases where the triangles
have common vertices, edges, or are coincident are treated via scaling and
symmetry arguments, combined with automatic recursive geometric decomposition
of the integrals. Example results are presented, and the developed software is
available as open source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04463</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04463</id><created>2015-06-14</created><authors><author><keyname>Kestyn</keyname><forenames>James</forenames></author><author><keyname>Polizzi</keyname><forenames>Eric</forenames></author><author><keyname>Tang</keyname><forenames>Ping Tak Peter</forenames></author></authors><title>FEAST Eigensolver for non-Hermitian Problems</title><categories>math.NA cs.MS cs.NA physics.comp-ph</categories><comments>22 pages, 8 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A detailed new upgrade of the FEAST eigensolver targeting non-Hermitian
eigenvalue problems is presented and thoroughly discussed. It aims at
broadening the class of eigenproblems that can be addressed within the
framework of the FEAST algorithm. The algorithm is ideally suited for computing
selected interior eigenvalues and their associated right/left bi-orthogonal
eigenvectors,located within a subset of the complex plane. It combines subspace
iteration with efficient contour integration techniques that approximate the
left and right spectral projectors. We discuss the various algorithmic choices
that have been made to improve the stability and usability of the new
non-Hermitian eigensolver. The latter retains the convergence property and
multi-level parallelism of Hermitian FEAST, making it a valuable new software
tool for the scientific community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04472</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04472</id><created>2015-06-14</created><updated>2015-08-24</updated><authors><author><keyname>Pour</keyname><forenames>Elham Sagheb Hossein</forenames></author></authors><title>A Survey of Multithreading Image Analysis</title><categories>cs.CV</categories><comments>6 Pages, 1 Figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital image analysis has made a big advance in many areas of enterprise
applications including medicine, industry, and entertainment by assisting the
extraction of semantic information from digital images. However, its large
computational complexity has been a trouble to most real-time developments.
While image analysis in general has been studied for a log period in computer
science community, the use of multithreading strategy as the most efficient
improving computational capacity technique has been limited so far. In this
survey an attempt is made to explain the current knowledge and so far
progresses in incorporating image analysis with multithreading approaches. The
present work also provides insights and tendencies for the possible future
enhancement of multithreading image analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04474</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04474</id><created>2015-06-15</created><updated>2016-03-07</updated><authors><author><keyname>Hasegawa</keyname><forenames>Shun</forenames></author><author><keyname>Itoh</keyname><forenames>Toshiya</forenames></author></authors><title>Optimal Online Algorithms for the Multi-Objective Time Series Search
  Problem</title><categories>cs.DS</categories><comments>15 pages</comments><acm-class>F.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tiedemann, et al. [Proc. of WALCOM, LNCS 8973, 2015, pp.210-221] defined
multi-objective online problems (as an online version of multi-objective
optimization problems) and the competitive analysis for multi-objective online
problems and showed that (1) with respect to the worst component competitive
analysis, the online algorithm RPP-HIGH is best possible for the
multi-objective time series search~problem; (2) with respect to the arithmetic
mean component competitive analysis, the online algorithm RPP-MULT is best
possible for the bi-objective time series search problem; (3) with respect to
the geometric mean component competitive analysis, the online algorithm
RPP-MULT is best possible for the bi-objective time series search problem. In
this paper, we first point out that the definitions and frameworks of the
competitive analysis due to Tiedemann, et al. do not necessarily capture the
efficiency of online algorithms for multi-objective online problems and provide
modified definitions of the competitive analysis for multi-objective online
problems. Then under the modified framework, we present a simple online
algorithm Balanced Price Policy BPP_{k} for the multi-objective (k-objective)
time series search problem, and show that the algorithm BPP_{k} is best
possible with respect to any measure of the competitive analysis (defined by a
monotone continuous function f). Under the modified framework, we derive exact
values of the competitive ratio for the multi-objective time series search
problem with respect to the worst component competitive analysis, the
arithmetic mean component competitive analysis, and the geometric mean
component competitive analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04477</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04477</id><created>2015-06-15</created><authors><author><keyname>Lee</keyname><forenames>Sang-Woo</forenames></author><author><keyname>Heo</keyname><forenames>Min-Oh</forenames></author><author><keyname>Kim</keyname><forenames>Jiwon</forenames></author><author><keyname>Kim</keyname><forenames>Jeonghee</forenames></author><author><keyname>Zhang</keyname><forenames>Byoung-Tak</forenames></author></authors><title>Dual Memory Architectures for Fast Deep Learning of Stream Data via an
  Online-Incremental-Transfer Strategy</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The online learning of deep neural networks is an interesting problem of
machine learning because, for example, major IT companies want to manage the
information of the massive data uploaded on the web daily, and this technology
can contribute to the next generation of lifelong learning. We aim to train
deep models from new data that consists of new classes, distributions, and
tasks at minimal computational cost, which we call online deep learning.
Unfortunately, deep neural network learning through classical online and
incremental methods does not work well in both theory and practice. In this
paper, we introduce dual memory architectures for online incremental deep
learning. The proposed architecture consists of deep representation learners
and fast learnable shallow kernel networks, both of which synergize to track
the information of new data. During the training phase, we use various online,
incremental ensemble, and transfer learning techniques in order to achieve
lower error of the architecture. On the MNIST, CIFAR-10, and ImageNet image
recognition tasks, the proposed dual memory architectures performs much better
than the classical online and incremental ensemble algorithm, and their
accuracies are similar to that of the batch learner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04480</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04480</id><created>2015-06-15</created><updated>2015-10-29</updated><authors><author><keyname>Zhao</keyname><forenames>Mingbi</forenames></author></authors><title>A Clustering Based Approach for Realistic and Efficient Data-Driven
  Crowd Simulation</title><categories>cs.GR</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  equation 6</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a data-driven approach to generate realistic
steering behaviors for virtual crowds in crowd simulation. We take advantage of
both rule-based models and data-driven models by applying the interaction
patterns discovered from crowd videos. Unlike existing example-based models in
which current states are matched to states extracted from crowd videos
directly, our approach adopts a hierarchical mechanism to generate the steering
behaviors of agents. First, each agent is classified into one of the
interaction patterns that are automatically discovered from crowd video before
simulation. Then the most matched action is selected from the associated
interaction pattern to generate the steering behaviors of the agent. By doing
so, agents can avoid performing a simple state matching as in the traditional
example-based approaches, and can perform a wider variety of steering behaviors
as well as mimic the cognitive process of pedestrians. Simulation results on
scenarios with different crowd densities and main motion directions demonstrate
that our approach performs better than two state-of-the-art simulation models,
in terms of prediction accuracy. Besides, our approach is efficient enough to
run at interactive rates in real time simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04485</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04485</id><created>2015-06-15</created><updated>2015-08-31</updated><authors><author><keyname>Kochergin</keyname><forenames>V. V.</forenames><affiliation>Lomonosov Moscow State University, Faculty of Mechanics and Mathematics, Bogoliubov Institute for Theoretical Problems of Microphysics</affiliation></author><author><keyname>Mikhailovich</keyname><forenames>A. V.</forenames><affiliation>National Research University Higher School of Economics</affiliation></author></authors><title>Some Extensions of the Inversion Complexity of Boolean Functions</title><categories>cs.DM math.LO</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The minimum number of NOT gates in a Boolean circuit computing a Boolean
function is called the inversion complexity of the function. In 1957, A. A.
Markov determined the inversion complexity of every Boolean function and proved
that $\lceil\log_{2}(d(f)+1)\rceil$ NOT gates are necessary and sufficient to
compute any Boolean function $f$ (where $d(f)$ is maximum number of value
changes from 1 to 0 over all increasing chains of tuples of variables values).
In this paper we consider Boolean circuits over an arbitrary basis that
consists of all monotone functions (with zero weight) and finite nonempty set
of non-monotone functions (with unit weight). It is shown that the minimal
sufficient for a realization of the Boolean function $f$ number of non-monotone
gates is equal to $\lceil\log_{2}(d(f)+1)\rceil - O(1)$. Similar extends of
another classical result of A. A. Markov for the inversion complexity of system
of Boolean functions has been obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04486</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04486</id><created>2015-06-15</created><authors><author><keyname>Al-Okaily</keyname><forenames>Anas</forenames></author></authors><title>Error Tree: A Tree Structure for Hamming &amp; Edit Distances &amp; Wildcards
  Matching</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Error Tree is a novel tree structure that is mainly oriented to solve the
approximate pattern matching problems, Hamming and edit distances, as well as
the wildcards matching problem. The input is a text of length $n$ over a fixed
alphabet of length $\Sigma$, a pattern of length $m$, and $k$. The output is to
find all positions that have $\leq$ $k$ Hamming distance, edit distance, or
wildcards matching with $P$. The algorithm proposes for Hamming distance and
wildcards matching a tree structure that needs $O(n\frac{log_\Sigma
^{k}n}{k!})$ words and takes $O(\frac {m^k}{k!} + occ$)($O(m + \frac
{log_\Sigma ^kn}{k!} + occ$) in the average case) of query time for any
online/offline pattern, where $occ$ is the number of outputs. As well, a tree
structure of $O(2^{k}n\frac{log_\Sigma ^{k}n}{k!})$ words and $O(\frac
{m^k}{k!} + 3^{k}occ$)($O(m + \frac {log_\Sigma ^kn}{k!} + 3^{k}occ$) in the
average case) query time for edit distance for any online/offline pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04488</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04488</id><created>2015-06-15</created><authors><author><keyname>Mou</keyname><forenames>Lili</forenames></author><author><keyname>Li</keyname><forenames>Ge</forenames></author><author><keyname>Xu</keyname><forenames>Yan</forenames></author><author><keyname>Zhang</keyname><forenames>Lu</forenames></author><author><keyname>Jin</keyname><forenames>Zhi</forenames></author></authors><title>Distilling Word Embeddings: An Encoding Approach</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distilling knowledge from a well-trained cumbersome network to a small one
has become a new research topic recently, as lightweight neural networks with
high performance are particularly in need in various resource-restricted
systems. This paper addresses the problem of distilling embeddings for NLP
tasks. We propose an encoding approach to distill task-specific knowledge from
high-dimensional embeddings, which can retain high performance and reduce model
complexity to a large extent. Experimental results show our method is better
than directly training neural networks with small embeddings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04496</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04496</id><created>2015-06-15</created><authors><author><keyname>Weinzierl</keyname><forenames>Tobias</forenames></author></authors><title>The Peano software - parallel, automaton-based, dynamically adaptive
  grid traversals</title><categories>cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the third generation of the PDE framework Peano. It incorporates
a mesh storage and traversal scheme for dynamically adaptive multiscale
Cartesian grids. Starting from a formalism of the software design at hands of
two types of interacting automata, one automaton for the multiscale grid
traversal and the remaining automata for the application-specific code
fragments, we discuss two variants of data storage offered by the framework:
heaps and streams. Their usage is sketched at hands of a multigrid algorithm
and a shallow water code and we present some traversal reordering techniques.
They lead into concurrent and distributed mesh traversals. All reordering
results from tree transformations which are hidden from the
application-specific code fragments, but make the grid traversals anticipate
the interplay of mesh regularity, data decomposition, parallel facilities, and
data and task dependencies. The latter are prescribed by the applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04497</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04497</id><created>2015-06-15</created><updated>2016-02-09</updated><authors><author><keyname>Werner</keyname><forenames>Ivan</forenames></author></authors><title>Lower bounds for the dynamically defined measures</title><categories>math.DS cs.IT math-ph math.IT math.MP</categories><comments>Slightly polished version</comments><msc-class>28A99, 37A60, 37A05, 82C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamically defined measure (DDM) $\Phi$ arising from a finite measure
$\phi_0$ on an initial $\sigma$-algebra on a set $X$ and an invertible map
acting on the latter is considered. Several lower bounds for it are obtained
under the condition that there exists an invariant measure $\Lambda$ such that
$\Lambda\ll\phi_0$.
  First, a dynamically defined relative entropy measure
$\bar{\mathcal{K}}(\Lambda|\phi_0)$ is introduced. It is shown that it is a
signed measure on the generated $\sigma$-algebra, which allows to obtain a
lower bound for the DDM through
\[\Phi(Q)\geq\Lambda(Q)\min\left\{e^{-\frac{1}{\Lambda(Q)}\bar{\mathcal{K}}(\Lambda|\phi_0)(Q)},e\right\}\]
for all measurable $Q$ with $\Lambda(Q)&gt;0$.
  Then DDMs arising from the Hellinger integral,
$\mathcal{H}_\alpha(\Lambda,\phi_0)$, $\alpha\in[0,1]$, are constructed, which
provide lower bounds for $\Phi$ through
\[\Phi(Q)^\alpha\Lambda(Q)^{1-\alpha}\geq\mathcal{H}_{1-\alpha}\left(\Lambda,\phi_0\right)(Q)\]
for all measurable $Q$ and $\alpha\in[0,1]$.
  Next, a parameter dependent relative entropy measure
$\bar{\mathcal{K}}_\alpha(\Lambda|\phi_0)\geq\bar{\mathcal{K}}(\Lambda|\phi_0)$,
is introduced, which gives lower bounds through
\[\mathcal{H}_{1-\alpha}\left(\Lambda,\phi_0\right)(Q)\geq\Lambda(Q)e^{-\frac{\alpha}{\Lambda(Q)}\bar{\mathcal{K}}_{1-\alpha}(\Lambda|\phi_0)(Q)}\]
for all measurable $Q$ with $\Lambda(Q)&gt;0$ and $0&lt;\alpha&lt; \min\{1,
e\Lambda(Q)/\Phi(Q)\}$. If $\Lambda$ is ergodic, then
$\bar{\mathcal{K}}_\alpha(\Lambda|\phi_0)(X)&lt;\infty$ is equivalent to
$\Lambda\ll\Phi$ and to the essential boundedness of $d\Lambda/d\phi_0$ with
respect to $\Lambda$.
  Finally, it is shown that the function
$(0,1)\owns\alpha\longmapsto\mathcal{H}_\alpha(\Lambda,\phi_0)(Q)$ is
continuous and right differentiable for all measurable $Q$, which is either
strictly positive or zero everywhere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04498</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04498</id><created>2015-06-15</created><authors><author><keyname>Egi</keyname><forenames>Satoshi</forenames></author></authors><title>Egison: Non-Linear Pattern-Matching against Non-Free Data Types</title><categories>cs.PL</categories><comments>9 pages. arXiv admin note: text overlap with arXiv:1407.0729</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the Egison programming language whose feature is strong
pattern-matching facility against not only algebraic data types but also
non-free data types whose data have multiple ways of representation such as
sets and graphs. Our language supports multiple occurrences of the same
variables in a pattern, multiple results of pattern-matching, polymorphism of
pattern-constructors and loop-patterns, patterns that contain &quot;and-so-forth&quot;
whose repeat count can be changed by the parameter. This paper proposes the way
to design expressions that have all these features and demonstrates how these
features are useful to express programs concise. Egison has already implemented
in Haskell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04499</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04499</id><created>2015-06-15</created><authors><author><keyname>H&#xfc;bschle-Schneider</keyname><forenames>Lorenz</forenames></author><author><keyname>Raman</keyname><forenames>Rajeev</forenames></author></authors><title>Tree Compression with Top Trees Revisited</title><categories>cs.DS</categories><comments>SEA 2015</comments><acm-class>F.2.2; E.2; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit tree compression with top trees (Bille et al, ICALP'13) and
present several improvements to the compressor and its analysis. By
significantly reducing the amount of information stored and guiding the
compression step using a RePair-inspired heuristic, we obtain a fast compressor
achieving good compression ratios, addressing an open problem posed by Bille et
al. We show how, with relatively small overhead, the compressed file can be
converted into an in-memory representation that supports basic navigation
operations in worst-case logarithmic time without decompression. We also show a
much improved worst-case bound on the size of the output of top-tree
compression (answering an open question posed in a talk on this algorithm by
Weimann in 2012).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04500</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04500</id><created>2015-06-15</created><updated>2015-08-20</updated><authors><author><keyname>Soelistio</keyname><forenames>Yustinus Eko</forenames></author><author><keyname>Postma</keyname><forenames>Eric</forenames></author><author><keyname>Maes</keyname><forenames>Alfons</forenames></author></authors><title>Circle-based Eye Center Localization (CECL)</title><categories>cs.CV</categories><comments>Published and presented at The 14th IAPR International Conference on
  Machine Vision Applications, 2015. http://www.mva-org.jp/mva2015/</comments><doi>10.1109/MVA.2015.7153202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an improved eye center localization method based on the Hough
transform, called Circle-based Eye Center Localization (CECL) that is simple,
robust, and achieves accuracy on a par with typically more complex
state-of-the-art methods. The CECL method relies on color and shape cues that
distinguish the iris from other facial structures. The accuracy of the CECL
method is demonstrated through a comparison with 15 state-of-the-art eye center
localization methods against five error thresholds, as reported in the
literature. The CECL method achieved an accuracy of 80.8% to 99.4% and ranked
first for 2 of the 5 thresholds. It is concluded that the CECL method offers an
attractive alternative to existing methods for automatic eye center
localization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04502</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04502</id><created>2015-06-15</created><authors><author><keyname>Tachago</keyname><forenames>Fabrice P.</forenames></author><author><keyname>Ekodeck</keyname><forenames>Stephane G. R.</forenames></author><author><keyname>Ndoundam</keyname><forenames>Rene</forenames></author></authors><title>Steganography and Broadcasting</title><categories>cs.CR</categories><comments>10 pages</comments><msc-class>94A60</msc-class><acm-class>D.4.6</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Informally, steganography is the process of exchanging a secret message
between two communicating entities so that an eavesdropper may not know that a
message has been sent. After a review of some steganographic systems, we found
that these systems have some defects. First, there are situations in which some
concealment algorithms do not properly hide a secret message. Second, to
conceal one bit of a secret message, some ask at least five documents and make
at least two sampling operations, thus increasing their run-times. Considering
the different ways to communicate with the receiver, we propose two
steganographic systems adapted to the email communication whose algorithms are
deterministic. To hide one bit of a secret message, our steganographic systems
need only one document and performs one sampling operation and therefore
significantly reduces the run-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04505</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04505</id><created>2015-06-15</created><updated>2015-07-29</updated><authors><author><keyname>Esfandiari</keyname><forenames>Hossein</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>Applications of Uniform Sampling: Densest Subgraph and Beyond</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently [Bhattacharya et al., STOC 2015] provide the first non-trivial
algorithm for the densest subgraph problem in the streaming model with
additions and deletions to its edges, i.e., for dynamic graph streams. They
present a $(0.5-\epsilon)$-approximation algorithm using $\tilde{O}(n)$ space,
where factors of $\epsilon$ and $\log(n)$ are suppressed in the $\tilde{O}$
notation. However, the update time of this algorithm is large. To remedy this,
they also provide a $(0.25-\epsilon)$-approximation algorithm using
$\tilde{O}(n)$ space with update time $\tilde{O}(1)$.
  In this paper we improve the algorithms by Bhattacharya et al. by providing a
$(1-\epsilon)$-approximation algorithm using $\tilde{O}(n)$ space. Our
algorithm is conceptually simple - it samples $\tilde{O}(n)$ edges uniformly at
random, and finds the densest subgraph on the sampled graph. We also show how
to perform this sampling with update time $\tilde{O}(1)$. In addition to this,
we show that given oracle access to the edge set, we can implement our
algorithm in time $\tilde{O}(n)$ on a graph in the standard RAM model. To the
best of our knowledge this is the fastest $(0.5-\epsilon)$-approximation
algorithm for the densest subgraph problem in the RAM model given such oracle
access.
  Further, we extend our results to a general class of graph optimization
problems that we call heavy subgraph problems. This class contains many
interesting problems such as densest subgraph, directed densest subgraph,
densest bipartite subgraph, $d$-cut and $d$-heavy connected component. Our
result, by characterizing heavy subgraph problems, partially addresses open
problem 13 at the IITK Workshop on Algorithms for Data Streams in 2006
regarding the effects of subsampling in this context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04506</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04506</id><created>2015-06-15</created><authors><author><keyname>Sch&#xfc;lke</keyname><forenames>Christophe</forenames></author><author><keyname>Ricci-Tersenghi</keyname><forenames>Federico</forenames></author></authors><title>Multiple phases in modularity-based community detection</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 7 figures</comments><journal-ref>Phys. Rev. E 92, 042804 (2015)</journal-ref><doi>10.1103/PhysRevE.92.042804</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting communities in a network, based only on the adjacency matrix, is a
problem of interest to several scientific disciplines. Recently, Zhang and
Moore have introduced an algorithm in [P. Zhang and C. Moore, Proceedings of
the National Academy of Sciences 111, 18144 (2014)], called mod-bp, that avoids
overfitting the data by optimizing a weighted average of modularity (a popular
goodness-of-fit measure in community detection) and entropy (i.e. number of
configurations with a given modularity). The adjustment of the relative weight,
the &quot;temperature&quot; of the model, is crucial for getting a correct result from
mod-bp. In this work we study the many phase transitions that mod-bp may
undergo by changing the two parameters of the algorithm: the temperature $T$
and the maximum number of groups $q$. We introduce a new set of order
parameters that allow to determine the actual number of groups $\hat{q}$, and
we observe on both synthetic and real networks the existence of phases with any
$\hat{q} \in \{1,q\}$, which were unknown before. We discuss how to interpret
the results of mod-bp and how to make the optimal choice for the problem of
detecting significant communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04512</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04512</id><created>2015-06-15</created><authors><author><keyname>Ferretti</keyname><forenames>Stefano</forenames></author></authors><title>Self-Healing Protocols for Connectivity Maintenance in Unstructured
  Overlays</title><categories>cs.DC</categories><comments>The paper has been accepted to the journal Peer-to-Peer Networking
  and Applications. The final publication is available at Springer via
  http://dx.doi.org/10.1007/s12083-015-0384-5</comments><doi>10.1007/s12083-015-0384-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discuss on the use of self-organizing protocols to improve
the reliability of dynamic Peer-to-Peer (P2P) overlay networks. Two similar
approaches are studied, which are based on local knowledge of the nodes' 2nd
neighborhood. The first scheme is a simple protocol requiring interactions
among nodes and their direct neighbors. The second scheme adds a check on the
Edge Clustering Coefficient (ECC), a local measure that allows determining
edges connecting different clusters in the network. The performed simulation
assessment evaluates these protocols over uniform networks, clustered networks
and scale-free networks. Different failure modes are considered. Results
demonstrate the effectiveness of the proposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04513</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04513</id><created>2015-06-15</created><authors><author><keyname>Telgarsky</keyname><forenames>Matus</forenames></author><author><keyname>Dud&#xed;k</keyname><forenames>Miroslav</forenames></author><author><keyname>Schapire</keyname><forenames>Robert</forenames></author></authors><title>Convex Risk Minimization and Conditional Probability Estimation</title><categories>cs.LG stat.ML</categories><comments>To appear, COLT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proves, in very general settings, that convex risk minimization is
a procedure to select a unique conditional probability model determined by the
classification problem. Unlike most previous work, we give results that are
general enough to include cases in which no minimum exists, as occurs
typically, for instance, with standard boosting algorithms. Concretely, we
first show that any sequence of predictors minimizing convex risk over the
source distribution will converge to this unique model when the class of
predictors is linear (but potentially of infinite dimension). Secondly, we show
the same result holds for \emph{empirical} risk minimization whenever this
class of predictors is finite dimensional, where the essential technical
contribution is a norm-free generalization bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04518</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04518</id><created>2015-06-15</created><authors><author><keyname>Knoll</keyname><forenames>Christian</forenames></author><author><keyname>Geiger</keyname><forenames>Bernhard C.</forenames></author><author><keyname>Kubin</keyname><forenames>Gernot</forenames></author></authors><title>The Muculants -- a Higher-Order Statistical Approach</title><categories>math.PR cs.IT math.IT</categories><comments>8 pages</comments><msc-class>60E10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An alternative parametric description for discrete random variables, called
muculants, is proposed. Contrary to cumulants, muculants are based on the
Fourier series expansion rather than on the Taylor series expansion of the
logarithm of the characteristic function. Utilizing results from cepstral
theory, elementary properties of muculants are derived. A connection between
muculants and cumulants is developed, and the muculants of selected discrete
random variables are presented. Specifically, it is shown that the Poisson
distribution is the only discrete distribution where only the first two
muculants are non-zero, thus being the muculant-counterpart of the simple
cumulant structure of Gaussian distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04525</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04525</id><created>2015-06-15</created><updated>2015-09-14</updated><authors><author><keyname>Dai</keyname><forenames>Wanyang</forenames></author></authors><title>Unified Systems of FB-SPDEs/FB-SDEs with Jumps/Skew Reflections and
  Stochastic Differential Games</title><categories>math.PR cs.GT math-ph math.MP math.OC math.ST stat.TH</categories><comments>58 pages, 6 figures, invited talks and plenary talks at a number of
  conferences and workshops</comments><msc-class>60H15, 60H10, 91A15, 91A23, 60K25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study four systems and their interactions. First, we formulate a unified
system of coupled forward-backward stochastic partial differential equations
(FB-SPDEs) with Levy jumps, whose drift, diffusion, and jump coefficients may
involve partial differential operators. A solution to the FB-SPDEs is defined
by a 4-tuple general dimensional random vector-field process evolving in time
together with position parameters over a domain (e.g., a hyperbox or a
manifold). Under an infinite sequence of generalized local linear growth and
Lipschitz conditions, the well-posedness of an adapted 4-tuple strong solution
is proved over a suitably constructed topological space. Second, we consider a
unified system of FB-SDEs, a special form of the FB-SPDEs, however, with skew
boundary reflections. Under randomized linear growth and Lipschitz conditions
together with a general completely-S condition on reflections, we prove the
well-posedness of an adapted 6-tuple weak solution with boundary regulators to
the FB-SDEs by the Skorohod problem and an oscillation inequality.
Particularly, if the spectral radii in some sense for reflection matrices are
strictly less than the unity, an adapted 6-tuple strong solution is concerned.
Third, we formulate a stochastic differential game (SDG) with general number of
players based on the FB-SDEs. By a solution to the FB-SPDEs, we get a solution
to the FB-SDEs under a given control rule and then obtain a Pareto optimal Nash
equilibrium policy process to the SDG. Fourth, we study the applications of the
FB-SPDEs/FB-SDEs in queueing systems and quantum statistics while we use them
to motivate the SDG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04541</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04541</id><created>2015-06-15</created><authors><author><keyname>Deka</keyname><forenames>Deepjyoti</forenames></author><author><keyname>Baldick</keyname><forenames>Ross</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Optimal Data Attacks on Power Grids: Leveraging Detection &amp; Measurement
  Jamming</title><categories>cs.CR math.OC</categories><comments>8 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Meter measurements in the power grid are susceptible to manipulation by
adversaries, that can lead to errors in state estimation. This paper presents a
general framework to study attacks on state estimation by adversaries capable
of injecting bad-data into measurements and further, of jamming their
reception. Through these two techniques, a novel `detectable jamming' attack is
designed that changes the state estimation despite failing bad-data detection
checks. Compared to commonly studied `hidden' data attacks, these attacks have
lower costs and a wider feasible operating region. It is shown that the entire
domain of jamming costs can be divided into two regions, with distinct
graph-cut based formulations for the design of the optimal attack. The most
significant insight arising from this result is that the adversarial capability
to jam measurements changes the optimal 'detectable jamming' attack design only
if the jamming cost is less than half the cost of bad-data injection. A
polynomial time approximate algorithm for attack vector construction is
developed and its efficacy in attack design is demonstrated through simulations
on IEEE test systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04544</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04544</id><created>2015-06-15</created><updated>2015-10-21</updated><authors><author><keyname>Battiston</keyname><forenames>Federico</forenames></author><author><keyname>Cairoli</keyname><forenames>Andrea</forenames></author><author><keyname>Nicosia</keyname><forenames>Vincenzo</forenames></author><author><keyname>Baule</keyname><forenames>Adrian</forenames></author><author><keyname>Latora</keyname><forenames>Vito</forenames></author></authors><title>Interplay between consensus and coherence in a model of interacting
  opinions</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI nlin.AO</categories><comments>8 pages, 4 figures, accepted for publication in Physica D</comments><doi>10.1016/j.physd.2015.10.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The formation of agents' opinions in a social system is the result of an
intricate equilibrium among several driving forces. On the one hand, the social
pressure exerted by peers favours the emergence of local consensus. On the
other hand, the concurrent participation of agents to discussions on different
topics induces each agent to develop a coherent set of opinions across all the
topics in which he is active. Moreover, the pervasive action of external
stimuli, such as mass media, pulls the entire population towards a specific
configuration of opinions on different topics. Here we propose a model in which
agents with interrelated opinions, interacting on several layers representing
different topics, tend to spread their own ideas to their neighbourhood, strive
to maintain internal coherence, due to the fact that each agent identifies
meaningful relationships among its opinions on the different topics, and are at
the same time subject to external fields, resembling the pressure of mass
media. We show that the presence of heterogeneity in the internal coupling
assigned by agents to their different opinions allows to obtain states with
mixed levels of consensus, still ensuring that all the agents attain a coherent
set of opinions. Furthermore, we show that all the observed features of the
model are preserved in the presence of thermal noise up to a critical
temperature, after which global consensus is no longer attainable. This
suggests the relevance of our results for real social systems, where noise is
inevitably present in the form of information uncertainty and
misunderstandings. The model also demonstrates how mass media can be
effectively used to favour the propagation of a chosen set of opinions, thus
polarising the consensus of an entire population.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04549</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04549</id><created>2015-06-15</created><authors><author><keyname>Horsch</keyname><forenames>Moritz</forenames></author><author><keyname>H&#xfc;lsing</keyname><forenames>Andreas</forenames></author><author><keyname>Buchmann</keyname><forenames>Johannes</forenames></author></authors><title>PALPAS - PAsswordLess PAssword Synchronization</title><categories>cs.CR</categories><comments>An extended abstract of this work appears in the proceedings of ARES
  2015</comments><acm-class>K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tools that synchronize passwords over several user devices typically store
the encrypted passwords in a central online database. For encryption, a
low-entropy, password-based key is used. Such a database may be subject to
unauthorized access which can lead to the disclosure of all passwords by an
offline brute-force attack. In this paper, we present PALPAS, a secure and
user-friendly tool that synchronizes passwords between user devices without
storing information about them centrally. The idea of PALPAS is to generate a
password from a high entropy secret shared by all devices and a random salt
value for each service. Only the salt values are stored on a server but not the
secret. The salt enables the user devices to generate the same password but is
statistically independent of the password. In order for PALPAS to generate
passwords according to different password policies, we also present a mechanism
that automatically retrieves and processes the password requirements of
services. PALPAS users need to only memorize a single password and the setup of
PALPAS on a further device demands only a one-time transfer of few static data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04557</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04557</id><created>2015-06-15</created><updated>2016-03-07</updated><authors><author><keyname>Du</keyname><forenames>Chao</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Bo</forenames></author></authors><title>Learning Deep Generative Models with Doubly Stochastic MCMC</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present doubly stochastic gradient MCMC, a simple and generic method for
(approximate) Bayesian inference of deep generative models (DGMs) in a
collapsed continuous parameter space. At each MCMC sampling step, the algorithm
randomly draws a mini-batch of data samples to estimate the gradient of
log-posterior and further estimates the intractable expectation over hidden
variables via a neural adaptive importance sampler, where the proposal
distribution is parameterized by a deep neural network and learnt jointly. We
demonstrate the effectiveness on learning various DGMs in a wide range of
tasks, including density estimation, data generation and missing data
imputation. Our method outperforms many state-of-the-art competitors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04558</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04558</id><created>2015-06-15</created><authors><author><keyname>Dotterrer</keyname><forenames>Dominic</forenames></author><author><keyname>Kaufman</keyname><forenames>Tali</forenames></author><author><keyname>Wagner</keyname><forenames>Uli</forenames></author></authors><title>On Expansion and Topological Overlap</title><categories>math.GT cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a detailed and easily accessible proof of Gromov's
\emph{Topological Overlap Theorem}: Let $X$ is a finite simplicial complex or,
more generally, a finite polyhedral cell complex of dimension $\dim X=d$.
Informally, the theorem states that if $X$ has sufficiently strong
\emph{higher-dimensional expansion properties} (which generalize edge expansion
of graphs and are terms of cellular cochains of $X$) then $X$ has the following
\emph{topological overlap property}: for every continuous map $X \rightarrow
R^d$ there exists a point $p\in R^d$ whose preimage meets a positive fraction
$\mu&gt;0$ of the $d$-cells of $X$. More generally, the conclusion holds if $R^d$
is replaced by any $d$-dimensional piecewise-linear (PL) manifold $M$, with a
constant $\mu$ that depends only on $d$ and on the expansion properties of $X$,
but not on $M$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04559</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04559</id><created>2015-06-15</created><authors><author><keyname>Crochemore</keyname><forenames>Maxime</forenames></author><author><keyname>Iliopoulos</keyname><forenames>Costas S.</forenames></author><author><keyname>Kundu</keyname><forenames>Ritu</forenames></author><author><keyname>Mohamed</keyname><forenames>Manal</forenames></author><author><keyname>Vayani</keyname><forenames>Fatima</forenames></author></authors><title>Linear Algorithm for Conservative Degenerate Pattern Matching</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A degenerate symbol x* over an alphabet A is a non-empty subset of A, and a
sequence of such symbols is a degenerate string. A degenerate string is said to
be conservative if its number of non-solid symbols is upper-bounded by a fixed
positive constant k. We consider here the matching problem of conservative
degenerate strings and present the first linear-time algorithm that can find,
for given degenerate strings P* and T* of total length n containing k non-solid
symbols in total, the occurrences of P* in T* in O(nk) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04566</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04566</id><created>2015-06-15</created><authors><author><keyname>Hoeltgen</keyname><forenames>Laurent</forenames></author><author><keyname>Mainberger</keyname><forenames>Markus</forenames></author><author><keyname>Hoffmann</keyname><forenames>Sebastian</forenames></author><author><keyname>Weickert</keyname><forenames>Joachim</forenames></author><author><keyname>Tang</keyname><forenames>Ching Hoo</forenames></author><author><keyname>Setzer</keyname><forenames>Simon</forenames></author><author><keyname>Johannsen</keyname><forenames>Daniel</forenames></author><author><keyname>Neumann</keyname><forenames>Frank</forenames></author><author><keyname>Doerr</keyname><forenames>Benjamin</forenames></author></authors><title>Optimising Spatial and Tonal Data for PDE-based Inpainting</title><categories>cs.CV math.OC</categories><msc-class>94A08, 65Nxx, 65Kxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some recent methods for lossy signal and image compression store only a few
selected pixels and fill in the missing structures by inpainting with a partial
differential equation (PDE). Suitable operators include the Laplacian, the
biharmonic operator, and edge-enhancing anisotropic diffusion (EED). The
quality of such approaches depends substantially on the selection of the data
that is kept. Optimising this data in the domain and codomain gives rise to
challenging mathematical problems that shall be addressed in our work.
  In the 1D case, we prove results that provide insights into the difficulty of
this problem, and we give evidence that a splitting into spatial and tonal
(i.e. function value) optimisation does hardly deteriorate the results. In the
2D setting, we present generic algorithms that achieve a high reconstruction
quality even if the specified data is very sparse. To optimise the spatial
data, we use a probabilistic sparsification, followed by a nonlocal pixel
exchange that avoids getting trapped in bad local optima. After this spatial
optimisation we perform a tonal optimisation that modifies the function values
in order to reduce the global reconstruction error. For homogeneous diffusion
inpainting, this comes down to a least squares problem for which we prove that
it has a unique solution. We demonstrate that it can be found efficiently with
a gradient descent approach that is accelerated with fast explicit diffusion
(FED) cycles. Our framework allows to specify the desired density of the
inpainting mask a priori. Moreover, is more generic than other data
optimisation approaches for the sparse inpainting problem, since it can also be
extended to nonlinear inpainting operators such as EED. This is exploited to
achieve reconstructions with state-of-the-art quality.
  We also give an extensive literature survey on PDE-based image compression
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04571</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04571</id><created>2015-06-15</created><authors><author><keyname>Dugu&#xe9;</keyname><forenames>Nicolas</forenames><affiliation>LIFO</affiliation></author><author><keyname>Labatut</keyname><forenames>Vincent</forenames><affiliation>LIA</affiliation></author><author><keyname>Perez</keyname><forenames>Anthony</forenames><affiliation>LIFO</affiliation></author></authors><title>A community role approach to assess social capitalists visibility in the
  Twitter network</title><categories>cs.SI physics.soc-ph</categories><comments>arXiv admin note: substantial text overlap with arXiv:1406.6611</comments><proxy>ccsd</proxy><journal-ref>Social Network Analysis and Mining, Springer, 2015, 5, pp. 26</journal-ref><doi>10.1007/s13278-015-0266-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of Twitter, social capitalists are specific users trying to
increase their number of followers and interactions by any means. These users
are not healthy for the service, because they are either spammers or real users
flawing the notions of influence and visibility. Studying their behavior and
understanding their position in Twit-ter is thus of important interest. It is
also necessary to analyze how these methods effectively affect user visibility.
Based on a recently proposed method allowing to identify social capitalists, we
tackle both points by studying how they are organized, and how their links
spread across the Twitter follower-followee network. To that aim, we consider
their position in the network w.r.t. its community structure. We use the
concept of community role of a node, which describes its position in a network
depending on its connectiv-ity at the community level. However, the topological
measures originally defined to characterize these roles consider only certain
aspects of the community-related connectivity, and rely on a set of empirically
fixed thresholds. We first show the limitations of these measures, before
extending and generalizing them. Moreover, we use an unsupervised approach to
identify the roles, in order to provide more flexibility relatively to the
studied system. We then apply our method to the case of social capitalists and
show they are highly visible on Twitter, due to the specific roles they hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04573</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04573</id><created>2015-06-15</created><updated>2015-09-21</updated><authors><author><keyname>Germain</keyname><forenames>Pascal</forenames><affiliation>LHC</affiliation></author><author><keyname>Habrard</keyname><forenames>Amaury</forenames><affiliation>LHC</affiliation></author><author><keyname>Laviolette</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>LHC</affiliation></author><author><keyname>Morvant</keyname><forenames>Emilie</forenames><affiliation>LHC</affiliation></author></authors><title>A New PAC-Bayesian Perspective on Domain Adaptation</title><categories>stat.ML cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the issue of domain adaptation: we want to adapt a model from a
source distribution to a target one. We focus on models expressed as a majority
vote. Our main contribution is a novel theoretical analysis of the target risk
that is formulated as an upper bound expressing a trade-off between only two
terms: (i) the voters' joint errors on the source distribution, and (ii) the
voters' disagreement on the target one; both easily estimable from samples.
This new study is more precise than other analyses that usually rely on three
terms (including a hardly controllable term). Moreover, we derive a
PAC-Bayesian generalization bound, and specialize the result to linear
classifiers to propose a learning algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04579</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04579</id><created>2015-06-15</created><updated>2015-11-19</updated><authors><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Rabinovich</keyname><forenames>Andrew</forenames></author><author><keyname>Berg</keyname><forenames>Alexander C.</forenames></author></authors><title>ParseNet: Looking Wider to See Better</title><categories>cs.CV</categories><comments>ICLR 2016 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a technique for adding global context to deep convolutional
networks for semantic segmentation. The approach is simple, using the average
feature for a layer to augment the features at each location. In addition, we
study several idiosyncrasies of training, significantly increasing the
performance of baseline networks (e.g. from FCN). When we add our proposed
global feature, and a technique for learning normalization parameters, accuracy
increases consistently even over our improved versions of the baselines. Our
proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow
and PASCAL-Context with small additional computational cost over baselines, and
near current state-of-the-art performance on PASCAL VOC 2012 semantic
segmentation with a simple approach. Code is available at
https://github.com/weiliu89/caffe/tree/fcn .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04583</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04583</id><created>2015-06-15</created><updated>2015-06-16</updated><authors><author><keyname>Deghel</keyname><forenames>Matha</forenames></author><author><keyname>Ba&#x15f;tu&#x11f;</keyname><forenames>Ejder</forenames></author><author><keyname>Assaad</keyname><forenames>Mohamad</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>On the Benefits of Edge Caching for MIMO Interference Alignment</title><categories>cs.IT math.IT</categories><comments>20 pages, 5 figures. A shorter version is to be presented at 16th
  IEEE International Workshop on Signal Processing Advances in Wireless
  Communications (SPAWC'2015), Stockholm, Sweden</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this contribution, we jointly investigate the benefits of caching and
interference alignment (IA) in multiple-input multiple-output (MIMO)
interference channel under limited backhaul capacity. In particular, total
average transmission rate is derived as a function of various system parameters
such as backhaul link capacity, cache size, number of active
transmitter-receiver pairs as well as the quantization bits for channel state
information (CSI). Given the fact that base stations are equipped both with
caching and IA capabilities and have knowledge of content popularity profile,
we then characterize an operational regime where the caching is beneficial.
Subsequently, we find the optimal number of transmitter-receiver pairs that
maximizes the total average transmission rate. When the popularity profile of
requested contents falls into the operational regime, it turns out that caching
substantially improves the throughput as it mitigates the backhaul usage and
allows IA methods to take benefit of such limited backhaul.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04584</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04584</id><created>2015-06-15</created><authors><author><keyname>Yang</keyname><forenames>Zhihai</forenames></author><author><keyname>Xu</keyname><forenames>Lin</forenames></author><author><keyname>Cai</keyname><forenames>Zhongmin</forenames></author></authors><title>Re-scale AdaBoost for Attack Detection in Collaborative Filtering
  Recommender Systems</title><categories>cs.IR cs.CR cs.LG</categories><comments>30 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative filtering recommender systems (CFRSs) are the key components of
successful e-commerce systems. Actually, CFRSs are highly vulnerable to attacks
since its openness. However, since attack size is far smaller than that of
genuine users, conventional supervised learning based detection methods could
be too &quot;dull&quot; to handle such imbalanced classification. In this paper, we
improve detection performance from following two aspects. First, we extract
well-designed features from user profiles based on the statistical properties
of the diverse attack models, making hard classification task becomes easier to
perform. Then, refer to the general idea of re-scale Boosting (RBoosting) and
AdaBoost, we apply a variant of AdaBoost, called the re-scale AdaBoost
(RAdaBoost) as our detection method based on extracted features. RAdaBoost is
comparable to the optimal Boosting-type algorithm and can effectively improve
the performance in some hard scenarios. Finally, a series of experiments on the
MovieLens-100K data set are conducted to demonstrate the outperformance of
RAdaBoost comparing with some classical techniques such as SVM, kNN and
AdaBoost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04606</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04606</id><created>2015-06-15</created><authors><author><keyname>Rodrigues</keyname><forenames>Jose</forenames></author><author><keyname>Traina</keyname><forenames>Agma</forenames></author><author><keyname>Faloutsos</keyname><forenames>Christos</forenames></author><author><keyname>Traina</keyname><forenames>Caetano</forenames></author></authors><title>SuperGraph Visualization</title><categories>cs.SI physics.soc-ph</categories><comments>9 pages, appears in Proceedings of the IEEE International Symposium
  on Multimedia, 2006 as SuperGraph Visualization In: 8th IEEE International
  Symposium on Multimedia 227-234 IEEE Press</comments><journal-ref>8th IEEE International Symposium on Multimedia 227-234 IEEE Press
  (2006)</journal-ref><doi>10.1109/ISM.2006.143</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a large social or computer network, how can we visualize it, find
patterns, outliers, communities? Although several graph visualization tools
exist, they cannot handle large graphs with hundred thousand nodes and possibly
million edges. Such graphs bring two challenges: interactive visualization
demands prohibitive processing power and, even if we could interactively update
the visualization, the user would be overwhelmed by the excessive number of
graphical items. To cope with this problem, we propose a formal innovation on
the use of graph hierarchies that leads to GMine system. GMine promotes
scalability using a hierarchy of graph partitions, promotes concomitant
presentation for the graph hierarchy and for the original graph, and extends
analytical possibilities with the integration of the graph partitions in an
interactive environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04608</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04608</id><created>2015-06-15</created><authors><author><keyname>Nazir</keyname><forenames>Javairia</forenames></author><author><keyname>Sirshar</keyname><forenames>Mehreen</forenames></author></authors><title>Flow Segmentation in Dense Crowds</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A framework is proposed in this paper that is used to segment flow of dense
crowds. The flow field that is generated by the movement in the crowd is
treated just like an aperiodic dynamic system. On this flow field a grid of
particles is put over for particle advection by the use of a numerical
integration scheme. Then flow maps are generated which associates the initial
position of the particles with final position. The gradient of the flow maps
gives the amount of divergence of the neighboring particles. For forward
integration and analysis forward Finite time Lyapunov Exponent is calculated
and backward Finite time Lyapunov Exponent is also calculated it gives the
Lagrangian coherent structures of the flow in crowd. Lagrangian Coherent
Structures basically divides the flow in crowd into regions and these regions
have different dynamics. These regions are then used to get the boundary in the
different flow segments by using water shed algorithm. The experiment is
conducted on the crowd dataset of UCF (University of central Florida).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04615</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04615</id><created>2015-06-15</created><authors><author><keyname>Thramboulidis</keyname><forenames>Kleanthis</forenames></author></authors><title>Service-Oriented Architecture in Industrial Automation Systems - The
  case of IEC 61499: A Review</title><categories>cs.SE</categories><comments>5 pages, 1 Figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper by W. Dai et al. (IEEE Trans. On Industrial Informatics, vol.
11, no. 3, pp. 771-781, June 2015), a formal model is described for the
application of SOA in the distributed automation domain in order to achieve
flexible automation systems. A service-based execution environment architecture
based on the IEC 61499 Function Block model is proposed and a case study is
used to demonstrate dynamic reconfiguration. In this letter, a review of the
literature related to the use of SOA in Industrial Automation Systems is given
to set up a context for the discussion of the proposed in the above paper SOA
IEC61499 formal model. The presented, in the above paper, formal model and
execution environment architecture are commented towards a better understanding
of the potentials for the exploitation of the SOA paradigm in the industrial
automation domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04631</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04631</id><created>2015-06-15</created><updated>2015-10-24</updated><authors><author><keyname>Gorban</keyname><forenames>Alexander N.</forenames></author><author><keyname>Tyukin</keyname><forenames>Ivan Yu.</forenames></author><author><keyname>Prokhorov</keyname><forenames>Danil V.</forenames></author><author><keyname>Sofeikov</keyname><forenames>Konstantin I.</forenames></author></authors><title>Approximation with Random Bases: Pro et Contra</title><categories>cs.NA cs.DM</categories><comments>arXiv admin note: text overlap with arXiv:0905.0677</comments><msc-class>41A45, 41A45, 90C59, 92B20, 68W20</msc-class><doi>10.1016/j.ins.2015.09.021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we discuss the problem of selecting suitable approximators from
families of parameterized elementary functions that are known to be dense in a
Hilbert space of functions. We consider and analyze published procedures, both
randomized and deterministic, for selecting elements from these families that
have been shown to ensure the rate of convergence in $L_2$ norm of order
$O(1/N)$, where $N$ is the number of elements. We show that both randomized and
deterministic procedures are successful if additional information about the
families of functions to be approximated is provided. In the absence of such
additional information one may observe exponential growth of the number of
terms needed to approximate the function and/or extreme sensitivity of the
outcome of the approximation to parameters. Implications of our analysis for
applications of neural networks in modeling and control are illustrated with
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04641</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04641</id><created>2015-06-15</created><authors><author><keyname>Mamino</keyname><forenames>Marcello</forenames></author></authors><title>Strategy Recovery for Stochastic Mean Payoff Games</title><categories>cs.GT</categories><comments>6 pages</comments><msc-class>91A15, 68Q25</msc-class><acm-class>F.2.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that to find optimal positional strategies for stochastic mean
payoff games when the value of every state of the game is known, in general, is
as hard as solving such games tout court. This answers a question posed by
Daniel Andersson and Peter Bro Miltersen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04642</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04642</id><created>2015-06-15</created><authors><author><keyname>Yordzhev</keyname><forenames>Krasimir</forenames></author></authors><title>Semi-canonical binary matrices</title><categories>math.CO cs.DM cs.DS</categories><comments>Sixth International Scientific Conference - FMNS2015, South-West
  University, Blagoevgrad, Bulgaria, 113 - 124</comments><msc-class>05B20, 68N15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we define the concepts of semi-canonical and canonical binary
matrix. Strictly mathematical, we prove the correctness of these definitions.
We describe and we implement an algorithm for finding all semi-canonical binary
matrices taking into account the number of 1 in each of them. This problem
relates to the combinatorial problem of finding all pairs of disjoint
S-permutation matrices. In the described algorithm, the bit-wise operations are
substantially used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04643</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04643</id><created>2015-06-15</created><updated>2015-06-23</updated><authors><author><keyname>Sundaram</keyname><forenames>R. M.</forenames></author><author><keyname>Jalihal</keyname><forenames>Devendra</forenames></author><author><keyname>Ramaiyan</keyname><forenames>Venkatesh</forenames></author></authors><title>Asynchronous Communication over a Fading Channel and Additive Noise</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In \cite{Chandar2008}, Chandar et al studied a problem of sequential frame
synchronization for a frame transmitted randomly and uniformly among $A$ slots.
For a discrete memory-less channel (DMC), they showed that the frame length $N$
must scale as $e^{\alpha(Q)N}&gt;A$ for the frame detection error to go to zero
asymptotically with $A$. $\alpha(Q)$ is the synchronization threshold and $Q$
is channel transition probability. We study the sequential frame
synchronisation problem for a fading channel and additive noise and seek to
characterise the effect of fading. For a discrete ON-OFF fading channel (with
ON probability $p$) and additive noise (with channel transition probabilities
$Q_n$), we characterise the synchronisation threshold of the composite channel
$\alpha(Q)$ and show that $\alpha(Q)\leq p\,\alpha(Q_n)$. We then characterize
the synchronization threshold for Rayleigh fading and AWGN channel as a
function of channel parameters. The asynchronous framework permits a trade-off
between sync frame length, $N$, and channel, $Q$, to support asynchronism. This
allows us to characterize the synchronization threshold with sync frame energy
instead of sync frame length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04644</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04644</id><created>2015-06-08</created><authors><author><keyname>Mansour</keyname><forenames>Mohammad M.</forenames></author><author><keyname>Jalloul</keyname><forenames>Louay M. A.</forenames></author></authors><title>Optimized Configurable Architectures for Scalable Soft-Input Soft-Output
  MIMO Detectors with 256-QAM</title><categories>cs.IT math.IT</categories><doi>10.1109/TSP.2015.2446441</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an optimized low-complexity and high-throughput
multiple-input multiple-output (MIMO) signal detector core for detecting
spatially-multiplexed data streams. The core architecture supports various
layer configurations up to 4, while achieving near-optimal performance, as well
as configurable modulation constellations up to 256-QAM on each layer. The core
is capable of operating as a soft-input soft-output log-likelihood ratio (LLR)
MIMO detector which can be used in the context of iterative detection and
decoding. High area-efficiency is achieved via algorithmic and architectural
optimizations performed at two levels. First, distance computations and slicing
operations for an optimal 2-layer maximum a posteriori (MAP) MIMO detector are
optimized to eliminate the use of multipliers and reduce the overhead of
slicing in the presence of soft-input LLRs. We show that distances can be
easily computed using elementary addition operations, while optimal slicing is
done via efficient comparisons with soft decision boundaries, resulting in a
simple feed-forward pipelined architecture. Second, to support more layers, an
efficient channel decomposition scheme is presented that reduces the detection
of multiple layers into multiple 2-layer detection subproblems, which map onto
the 2-layer core with a slight modification using a distance accumulation stage
and a post-LLR processing stage. Various architectures are accordingly
developed to achieve a desired detection throughput and run-time
reconfigurability by time-multiplexing of one or more component cores. The
proposed core is applied as well to design an optimal multi-user MIMO detector
for LTE. The core occupies an area of 1.58MGE and achieves a throughput of 733
Mbps for 256-QAM when synthesized in 90 nm CMOS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04650</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04650</id><created>2015-06-15</created><authors><author><keyname>Reddy</keyname><forenames>B. Prashanth</forenames></author></authors><title>Coding Side-Information for Implementing Cubic Transformation</title><categories>cs.CR</categories><comments>12 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the implementation of the cubic public-key
transformation, a public-key cryptographic scheme that requires sending of
additional side-information. A coding scheme for the side-information, based on
the residue number system, is presented. In the conventional one-to-one
encryption mapping also, such coding can be used to send additional like that
of watermarking, which could be used to detect man-in-the-middle attacks or
used for authentication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04651</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04651</id><created>2015-06-09</created><updated>2015-11-18</updated><authors><author><keyname>Descombes</keyname><forenames>St&#xe9;phane</forenames><affiliation>NACHOS</affiliation></author><author><keyname>Duarte</keyname><forenames>Max</forenames><affiliation>LBNL</affiliation></author><author><keyname>Dumont</keyname><forenames>Thierry</forenames><affiliation>ICJ</affiliation></author><author><keyname>Guillet</keyname><forenames>Thomas</forenames><affiliation>ICJ</affiliation></author><author><keyname>Louvet</keyname><forenames>Violaine</forenames><affiliation>ICJ</affiliation></author><author><keyname>Massot</keyname><forenames>Marc</forenames><affiliation>EM2C, FR3487</affiliation></author></authors><title>Task-based adaptive multiresolution for time-space multi-scale
  reaction-diffusion systems on multi-core architectures</title><categories>cs.NA cs.DC math.AP math.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reaction-diffusion systems involving a large number of unknowns and a wide
spectrum of scales in space and time model various complex phenomena across
different disciplines such as combustion science, plasma physics, or biomedical
engineering. The numerical solution of these strongly multi-scale systems of
partial differential equations entails specific challenges due to the
potentially large stiffness stemming from the broad range of temporal scales in
the nonlinear source term or from the presence of steep spatial gradients at
the localized reaction fronts, thus hindering high-fidelity solutions at
reasonable computational cost. A new generation of techniques featuring
adaptation in space and time as well as error control has been introduced
recently, yielding accurate solutions to such complex models while considering
the entire spectrum of scales. Based on operator splitting, finite volume
adaptive multiresolution, and high order time integrators with specific
stability properties for each operator, these methods yield a high
computational efficiency for stiff reaction-diffusion problems and were
specifically designed to carry out large multidimensional simulations on
standard architectures such as powerful workstations. While demonstrating the
potential of such techniques the data structures of the original
implementation,based on trees of pointers, provided limited opportunities for
computational efficiency optimizations and posed challenges in terms of
parallel programming and load balancing. The present contribution proposes a
new implementation of the whole set of numerical methods including Radau5 and
Rock4, relying on fully different data structures together with the use of a
specific library, TBB, for shared-memory, task-based parallelism with work
stealing. The performance of our implementation is assessed in a series of
test-cases of increasing difficulty in two and three dimensions on multi-core
and many-core architectures, demonstrating high scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04654</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04654</id><created>2015-06-15</created><updated>2015-09-16</updated><authors><author><keyname>Marin</keyname><forenames>Dmitrii</forenames></author><author><keyname>Boykov</keyname><forenames>Yuri</forenames></author><author><keyname>Zhong</keyname><forenames>Yuchen</forenames></author></authors><title>Thin Structure Estimation with Curvature Regularization</title><categories>cs.CV</categories><comments>D. Marin, Y. Zhong, M. Drangova, Y. Boykov. Thin Structure Estimation
  with Curvature Regularization. International Conference on Computer Vision
  (ICCV), Santiago, Chili, December 2015, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications in vision require estimation of thin structures such as
boundary edges, surfaces, roads, blood vessels, neurons, etc. Unlike most
previous approaches, we simultaneously detect and delineate thin structures
with sub-pixel localization and real-valued orientation estimation. This is an
ill-posed problem that requires regularization. We propose an objective
function combining detection likelihoods with a prior minimizing curvature of
the center-lines or surfaces. Unlike simple block-coordinate descent, we
develop a novel algorithm that is able to perform joint optimization of
location and detection variables more effectively. Our lower bound optimization
algorithm applies to quadratic or absolute curvature. The proposed early vision
framework is sufficiently general and it can be used in many higher-level
applications. We illustrate the advantage of our approach on a range of 2D and
3D examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04655</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04655</id><created>2015-06-15</created><authors><author><keyname>Zhong</keyname><forenames>Yang</forenames></author><author><keyname>Li</keyname><forenames>Haibo</forenames></author></authors><title>Leveraging the Power of Gabor Phase for Face Identification: A Block
  Matching Approach</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Different from face verification, face identification is much more demanding.
To reach comparable performance, an identifier needs to be roughly N times
better than a verifier. To expect a breakthrough in face identification, we
need a fresh look at the fundamental building blocks of face recognition. In
this paper we focus on the selection of a suitable signal representation and
better matching strategy for face identification. We demonstrate how Gabor
phase could be leveraged to improve the performance of face identification by
using the Block Matching method. Compared to the existing approaches, the
proposed method features much lower algorithmic complexity: face images are
only filtered by a single-scale Gabor filter pair and the matching is performed
between any pairs of face images at hand without involving any training
process. Benchmark evaluations show that the proposed approach is totally
comparable to and even better than state-of-the-art algorithms, which are
typically based on more features extracted from a large set of Gabor faces
and/or rely on heavy training processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04657</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04657</id><created>2015-06-15</created><authors><author><keyname>Becker</keyname><forenames>Nico</forenames></author><author><keyname>Fidler</keyname><forenames>Markus</forenames></author></authors><title>A Non-stationary Service Curve Model for Performance Analysis of
  Transient Phases</title><categories>cs.NI cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steady-state solutions for a variety of relevant queueing systems are known
today, e.g., from queueing theory, effective bandwidths, and network calculus.
The behavior during transient phases, on the other hand, is understood to a
much lesser extent as its analysis poses significant challenges. Considering
the majority of short-lived flows, transient effects that have diverse causes,
such as TCP slow start, sleep scheduling in wireless networks, or signalling in
cellular networks, are, however, predominant. This paper contributes a general
model of regenerative service processes to characterize the transient behavior
of systems. The model leads to a notion of non-stationary service curves that
can be conveniently integrated into the framework of the stochastic network
calculus. We derive respective models of sleep scheduling and show the
significant impact of transient phases on backlogs and delays. We also consider
measurement methods that estimate the service of an unknown system from
observations of selected probe traffic. We find that the prevailing rate
scanning method does not recover the service during transient phases well. This
limitation is fundamental as it is explained by the non-convexity of
non-stationary service curves. A second key difficulty is proven to be due to
the super-additivity of network service processes. We devise a novel two-phase
probing technique that first determines a minimal pattern of probe traffic.
This probe is used to obtain an accurate estimate of the unknown transient
service.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04681</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04681</id><created>2015-06-15</created><updated>2015-07-31</updated><authors><author><keyname>Su</keyname><forenames>Lili</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author></authors><title>Byzantine Multi-Agent Optimization: Part I</title><categories>cs.DC math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study Byzantine fault-tolerant distributed optimization of a sum of convex
(cost) functions with real-valued scalar input/ouput. In particular, the goal
is to optimize a global cost function $\frac{1}{|\mathcal{N}|}\sum_{i\in
\mathcal{N}} h_i(x)$, where $\mathcal{N}$ is the set of non-faulty agents, and
$h_i(x)$ is agent $i$'s local cost function, which is initially known only to
agent $i$. In general, when some of the agents may be Byzantine faulty, the
above goal is unachievable, because the identity of the faulty agents is not
necessarily known to the non-faulty agents, and the faulty agents may behave
arbitrarily. Since the above global cost function cannot be optimized exactly
in presence of Byzantine agents, we define a weaker version of the problem.
  The goal for the weaker problem is to generate an output that is an optimum
of a function formed as a convex combination of local cost functions of the
non-faulty agents. More precisely, for some choice of weights $\alpha_i$ for
$i\in \mathcal{N}$ such that $\alpha_i\geq 0$ and $\sum_{i\in
\mathcal{N}}\alpha_i=1$, the output must be an optimum of the cost function
$\sum_{i\in \mathcal{N}} \alpha_ih_i(x)$. Ideally, we would like
$\alpha_i=\frac{1}{|\mathcal{N}|}$ for all $i\in \mathcal{N}$ -- however, this
cannot be guaranteed due to the presence of faulty agents. In fact, we show
that the maximum achievable number of nonzero weights ($\alpha_i$'s) is
$|\mathcal{N}|-f$, where $f$ is the upper bound on the number of Byzantine
agents. In addition, we present algorithms that ensure that at least
$|\mathcal{N}|-f$ agents have weights that are bounded away from 0. We also
propose a low-complexity suboptimal algorithm, which ensures that at least
$\lceil \frac{n}{2}\rceil-\phi$ agents have weights that are bounded away from
0, where $n$ is the total number of agents, and $\phi$ ($\phi\le f$) is the
actual number of Byzantine agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04693</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04693</id><created>2015-06-15</created><authors><author><keyname>Orman</keyname><forenames>G&#xfc;nce</forenames><affiliation>BIT Lab</affiliation></author><author><keyname>Labatut</keyname><forenames>Vincent</forenames><affiliation>LIA</affiliation></author><author><keyname>Plantevit</keyname><forenames>Marc</forenames><affiliation>LIRIS</affiliation></author><author><keyname>Boulicaut</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>LIRIS</affiliation></author></authors><title>Interpreting communities based on the evolution of a dynamic attributed
  network</title><categories>cs.SI physics.soc-ph</categories><proxy>ccsd</proxy><journal-ref>Social Network Analysis and Mining Journal (SNAM), 2015, 5, pp.20.
  \&amp;lt;http://link.springer.com/article/10.1007%2Fs13278-015-0262-4\&amp;gt;.
  \&amp;lt;10.1007/s13278-015-0262-4\&amp;gt;</journal-ref><doi>10.1007/s13278-015-0262-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many methods have been proposed to detect communities, not only in plain, but
also in attributed, directed or even dynamic complex networks. From the
modeling point of view, to be of some utility, the community structure must be
characterized relatively to the properties of the studied system. However, most
of the existing works focus on the detection of communities, and only very few
try to tackle this interpretation problem. Moreover, the existing approaches
are limited either by the type of data they handle, or by the nature of the
results they output. In this work, we see the interpretation of communities as
a problem independent from the detection process, consisting in identifying the
most characteristic features of communities. We give a formal definition of
this problem and propose a method to solve it. To this aim, we first define a
sequence-based representation of networks, combining temporal information,
community structure, topological measures, and nodal attributes. We then
describe how to identify the most emerging sequential patterns of this dataset,
and use them to characterize the communities. We study the performance of our
method on artificially generated dynamic attributed networks. We also
empirically validate our framework on real-world systems: a DBLP network of
scientific collaborations, and a LastFM network of social and musical
interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04701</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04701</id><created>2015-06-15</created><updated>2015-06-22</updated><authors><author><keyname>Wang</keyname><forenames>Mingming</forenames></author></authors><title>Multi-path Convolutional Neural Networks for Complex Image
  Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Networks demonstrate high performance on ImageNet
Large-Scale Visual Recognition Challenges contest. Nevertheless, the published
results only show the overall performance for all image classes. There is no
further analysis why certain images get worse results and how they could be
improved. In this paper, we provide deep performance analysis based on
different types of images and point out the weaknesses of convolutional neural
networks through experiment. We design a novel multiple paths convolutional
neural network, which feeds different versions of images into separated paths
to learn more comprehensive features. This model has better presentation for
image than the traditional single path model. We acquire better classification
results on complex validation set on both top 1 and top 5 scores than the best
ILSVRC 2013 classification model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04704</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04704</id><created>2015-06-15</created><authors><author><keyname>Sekara</keyname><forenames>Vedran</forenames></author><author><keyname>Stopczynski</keyname><forenames>Arkadiusz</forenames></author><author><keyname>Lehmann</keyname><forenames>Sune</forenames></author></authors><title>The fundamental structures of dynamic social networks</title><categories>physics.soc-ph cs.SI</categories><comments>10 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networks provide a powerful mathematical framework for analyzing the
structure and dynamics of complex systems (1-3). The study of group behavior
has deep roots in the social science literature (4,5) and community detection
is a central part of modern network science. Network communities have been
found to be highly overlapping and organized in a hierarchical structure (6-9).
Recent technological advances have provided a toolset for measuring the
detailed social dynamics at scale (10,11). In spite of great progress, a
quantitative description of the complex temporal behavior of social groups-with
dynamics spanning from minute-by-minute changes to patterns expressed on the
timescale of years-is still absent. Here we uncover a class of fundamental
structures embedded within highly dynamic social networks. On the shortest
time-scale, we find that social gatherings are fluid, with members coming and
going, but organized via a stable core of individuals. We show that cores
represent social contexts (9), with recurring meetings across weeks and months,
each with varying degrees of regularity. In this sense, cores provide a
vocabulary which quantifies the patterns of social life. The simplification is
so powerful that participation in social contexts is predictable with high
precision. Our results offer new perspectives for modeling and understanding of
processes in social systems, with applications including epidemiology, social
contagion, urban planning, digital economy, and quantitative sociology
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04714</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04714</id><created>2015-06-15</created><authors><author><keyname>Jayaraman</keyname><forenames>Dinesh</forenames></author><author><keyname>Grauman</keyname><forenames>Kristen</forenames></author></authors><title>Slow and Steady Feature Analysis: Higher Order Temporal Coherence in
  Video</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learned image representations constitute the current state-of-the-art for
visual recognition, yet they notoriously require large amounts of human-labeled
data to learn effectively. Unlabeled video data has the potential to reduce
this cost, if learning algorithms can exploit the frames' temporal coherence as
a weak---but free---form of supervision. Existing methods perform &quot;slow&quot;
feature analysis, encouraging the image representations of temporally close
frames to exhibit only small differences. While this standard approach captures
the fact that high-level visual signals change slowly over time, it fails to
capture how the visual content changes. We propose to generalize slow feature
analysis to &quot;steady&quot; feature analysis. The key idea is to impose a prior that
higher order derivatives in the learned feature space must be small. To this
end, we train a convolutional neural network with a regularizer that minimizes
a contrastive loss on tuples of sequential frames from unlabeled video.
Focusing on the case of triplets of frames, the proposed method encourages that
feature changes over time should be smooth, i.e., similar to the most recent
changes. Using five diverse image and video datasets, including unlabeled
YouTube and KITTI videos, we demonstrate our method's impact on object
recognition, scene classification, and action recognition tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04719</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04719</id><created>2015-06-15</created><updated>2015-10-26</updated><authors><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author><author><keyname>Balodis</keyname><forenames>Kaspars</forenames></author><author><keyname>Belovs</keyname><forenames>Aleksandrs</forenames></author><author><keyname>Lee</keyname><forenames>Troy</forenames></author><author><keyname>Santha</keyname><forenames>Miklos</forenames></author><author><keyname>Smotrovs</keyname><forenames>Juris</forenames></author></authors><title>Separations in Query Complexity Based on Pointer Functions</title><categories>cs.CC quant-ph</categories><comments>25 pages, 6 figures. Version 3 improves separation between Q_E and
  R_0 and updates references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1986, Saks and Wigderson conjectured that the largest separation between
deterministic and zero-error randomized query complexity for a total boolean
function is given by the function $f$ on $n=2^k$ bits defined by a complete
binary tree of NAND gates of depth $k$, which achieves $R_0(f) =
O(D(f)^{0.7537\ldots})$. We show this is false by giving an example of a total
boolean function $f$ on $n$ bits whose deterministic query complexity is
$\Omega(n/\log(n))$ while its zero-error randomized query complexity is $\tilde
O(\sqrt{n})$. We further show that the quantum query complexity of the same
function is $\tilde O(n^{1/4})$, giving the first example of a total function
with a super-quadratic gap between its quantum and deterministic query
complexities.
  We also construct a total boolean function $g$ on $n$ variables that has
zero-error randomized query complexity $\Omega(n/\log(n))$ and bounded-error
randomized query complexity $R(g) = \tilde O(\sqrt{n})$. This is the first
super-linear separation between these two complexity measures. The exact
quantum query complexity of the same function is $Q_E(g) = \tilde O(\sqrt{n})$.
  These two functions show that the relations $D(f) = O(R_1(f)^2)$ and $R_0(f)
= \tilde O(R(f)^2)$ are optimal, up to poly-logarithmic factors. Further
variations of these functions give additional separations between other query
complexity measures: a cubic separation between $Q$ and $R_0$, a $3/2$-power
separation between $Q_E$ and $R$, and a 4th power separation between
approximate degree and bounded-error randomized query complexity.
  All of these examples are variants of a function recently introduced by
\goos, Pitassi, and Watson which they used to separate the unambiguous
1-certificate complexity from deterministic query complexity and to resolve the
famous Clique versus Independent Set problem in communication complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04720</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04720</id><created>2015-06-15</created><authors><author><keyname>Nie</keyname><forenames>Siqi</forenames></author><author><keyname>Ji</keyname><forenames>Qiang</forenames></author></authors><title>Latent Regression Bayesian Network for Data Representation</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep directed generative models have attracted much attention recently due to
their expressive representation power and the ability of ancestral sampling.
One major difficulty of learning directed models with many latent variables is
the intractable inference. To address this problem, most existing algorithms
make assumptions to render the latent variables independent of each other,
either by designing specific priors, or by approximating the true posterior
using a factorized distribution. We believe the correlations among latent
variables are crucial for faithful data representation. Driven by this idea, we
propose an inference method based on the conditional pseudo-likelihood that
preserves the dependencies among the latent variables. For learning, we propose
to employ the hard Expectation Maximization (EM) algorithm, which avoids the
intractability of the traditional EM by max-out instead of sum-out to compute
the data likelihood. Qualitative and quantitative evaluations of our model
against state of the art deep models on benchmark datasets demonstrate the
effectiveness of the proposed algorithm in data representation and
reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04721</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04721</id><created>2015-06-15</created><authors><author><keyname>Wang</keyname><forenames>Qiaosong</forenames></author><author><keyname>Lin</keyname><forenames>Haiting</forenames></author><author><keyname>Ma</keyname><forenames>Yi</forenames></author><author><keyname>Kang</keyname><forenames>Sing Bing</forenames></author><author><keyname>Yu</keyname><forenames>Jingyi</forenames></author></authors><title>Automatic Layer Separation using Light Field Imaging</title><categories>cs.CV</categories><comments>9 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We propose a novel approach that jointly removes reflection or translucent
layer from a scene and estimates scene depth. The input data are captured via
light field imaging. The problem is couched as minimizing the rank of the
transmitted scene layer via Robust Principle Component Analysis (RPCA). We also
impose regularization based on piecewise smoothness, gradient sparsity, and
layer independence to simultaneously recover 3D geometry of the transmitted
layer. Experimental results on synthetic and real data show that our technique
is robust and reliable, and can handle a broad range of layer separation
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04722</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04722</id><created>2015-06-15</created><authors><author><keyname>Fitzsimmons</keyname><forenames>Zack</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author></authors><title>Complexity of Manipulative Actions When Voting with Ties</title><categories>cs.GT cs.CC cs.MA</categories><comments>A version of this paper will appear in ADT-2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the computational study of election problems has assumed that each
voter's preferences are, or should be extended to, a total order. However in
practice voters may have preferences with ties. We study the complexity of
manipulative actions on elections where voters can have ties, extending the
definitions of the election systems (when necessary) to handle voters with
ties. We show that for natural election systems allowing ties can both increase
and decrease the complexity of manipulation and bribery, and we state a general
result on the effect of voters with ties on the complexity of control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04723</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04723</id><created>2015-06-15</created><updated>2015-07-29</updated><authors><author><keyname>Liu</keyname><forenames>Ming-Yu</forenames></author><author><keyname>Lin</keyname><forenames>Shuoxin</forenames></author><author><keyname>Ramalingam</keyname><forenames>Srikumar</forenames></author><author><keyname>Tuzel</keyname><forenames>Oncel</forenames></author></authors><title>Layered Interpretation of Street View Images</title><categories>cs.CV</categories><comments>The paper will be presented in the 2015 Robotics: Science and Systems
  Conference (RSS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a layered street view model to encode both depth and semantic
information on street view images for autonomous driving. Recently, stixels,
stix-mantics, and tiered scene labeling methods have been proposed to model
street view images. We propose a 4-layer street view model, a compact
representation over the recently proposed stix-mantics model. Our layers encode
semantic classes like ground, pedestrians, vehicles, buildings, and sky in
addition to the depths. The only input to our algorithm is a pair of stereo
images. We use a deep neural network to extract the appearance features for
semantic classes. We use a simple and an efficient inference algorithm to
jointly estimate both semantic classes and layered depth values. Our method
outperforms other competing approaches in Daimler urban scene segmentation
dataset. Our algorithm is massively parallelizable, allowing a GPU
implementation with a processing speed about 9 fps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04729</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04729</id><created>2015-06-15</created><updated>2015-06-18</updated><authors><author><keyname>Shaghaghian</keyname><forenames>Shohreh</forenames></author><author><keyname>Coates</keyname><forenames>Mark</forenames></author></authors><title>Optimal Forwarding in Opportunistic Delay Tolerant Networks with Meeting
  Rate Estimations</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data transfer in opportunistic Delay Tolerant Networks (DTNs) must rely on
unscheduled sporadic meetings between nodes. The main challenge in these
networks is to develop a mechanism based on which nodes can learn to make
nearly optimal forwarding decision rules despite having no a-priori knowledge
of the network topology. The forwarding mechanism should ideally result in a
high delivery probability, low average latency and efficient usage of the
network resources. In this paper, we propose both centralized and decentralized
single-copy message forwarding algorithms that, under relatively strong
assumptions about the networks behaviour, minimize the expected latencies from
any node in the network to a particular destination. After proving the
optimality of our proposed algorithms, we develop a decentralized algorithm
that involves a recursive maximum likelihood procedure to estimate the meeting
rates. We confirm the improvement that our proposed algorithms make in the
system performance through numerical simulations on datasets from synthetic and
real-world opportunistic networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04737</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04737</id><created>2015-06-15</created><updated>2015-08-07</updated><authors><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author></authors><title>A transverse Hamiltonian variational technique for open quantum
  stochastic systems and its application to coherent quantum control</title><categories>quant-ph cs.SY math-ph math.MP math.OC</categories><comments>12 pages, 1 figure. A brief version of this paper will appear in the
  proceedings of the IEEE Multi-Conference on Systems and Control, 21-23
  September 2015, Sydney, Australia</comments><msc-class>81Q93, 93E20, 81S25, 49J40, 47J20, 49K30, 37K05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with variational methods for nonlinear open quantum
systems with Markovian dynamics governed by Hudson-Parthasarathy quantum
stochastic differential equations. The latter are driven by quantum Wiener
processes of the external boson fields and are specified by the system
Hamiltonian and system-field coupling operators. We consider the system
response to perturbations of these energy operators and introduce a transverse
Hamiltonian which encodes the propagation of the perturbations through the
unitary system-field evolution. This provides a tool for the infinitesimal
perturbation analysis and development of optimality conditions for coherent
quantum control problems. We apply the transverse Hamiltonian variational
technique to a mean square optimal coherent quantum filtering problem for a
measurement-free cascade connection of quantum systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04744</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04744</id><created>2015-06-15</created><authors><author><keyname>Niculae</keyname><forenames>Vlad</forenames></author><author><keyname>Kumar</keyname><forenames>Srijan</forenames></author><author><keyname>Boyd-Graber</keyname><forenames>Jordan</forenames></author><author><keyname>Danescu-Niculescu-Mizil</keyname><forenames>Cristian</forenames></author></authors><title>Linguistic Harbingers of Betrayal: A Case Study on an Online Strategy
  Game</title><categories>cs.CL cs.AI cs.SI physics.soc-ph stat.ML</categories><comments>To appear at ACL 2015. 10pp, 4 fig. Data and other info available at
  http://vene.ro/betrayal/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interpersonal relations are fickle, with close friendships often dissolving
into enmity. In this work, we explore linguistic cues that presage such
transitions by studying dyadic interactions in an online strategy game where
players form alliances and break those alliances through betrayal. We
characterize friendships that are unlikely to last and examine temporal
patterns that foretell betrayal.
  We reveal that subtle signs of imminent betrayal are encoded in the
conversational patterns of the dyad, even if the victim is not aware of the
relationship's fate. In particular, we find that lasting friendships exhibit a
form of balance that manifests itself through language. In contrast, sudden
changes in the balance of certain conversational attributes---such as positive
sentiment, politeness, or focus on future planning---signal impending betrayal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04757</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04757</id><created>2015-06-15</created><authors><author><keyname>McAuley</keyname><forenames>Julian</forenames></author><author><keyname>Targett</keyname><forenames>Christopher</forenames></author><author><keyname>Shi</keyname><forenames>Qinfeng</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>Image-based Recommendations on Styles and Substitutes</title><categories>cs.CV cs.IR</categories><comments>11 pages, 10 figures, SIGIR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans inevitably develop a sense of the relationships between objects, some
of which are based on their appearance. Some pairs of objects might be seen as
being alternatives to each other (such as two pairs of jeans), while others may
be seen as being complementary (such as a pair of jeans and a matching shirt).
This information guides many of the choices that people make, from buying
clothes to their interactions with each other. We seek here to model this human
sense of the relationships between objects based on their appearance. Our
approach is not based on fine-grained modeling of user annotations but rather
on capturing the largest dataset possible and developing a scalable method for
uncovering human notions of the visual relationships within. We cast this as a
network inference problem defined on graphs of related images, and provide a
large-scale dataset for the training and evaluation of the same. The system we
develop is capable of recommending which clothes and accessories will go well
together (and which will not), amongst a host of other applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04767</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04767</id><created>2015-06-15</created><authors><author><keyname>Quinn</keyname><forenames>Christopher J.</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author><author><keyname>Kiyavash</keyname><forenames>Negar</forenames></author></authors><title>Bounded Degree Approximations of Stochastic Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose algorithms to approximate directed information graphs. Directed
information graphs are probabilistic graphical models that depict causal
dependencies between stochastic processes in a network. The proposed algorithms
identify optimal and near-optimal approximations in terms of Kullback-Leibler
divergence. The user-chosen sparsity trades off the quality of the
approximation against visual conciseness and computational tractability. One
class of approximations contains graphs with specified in-degrees. Another
class additionally requires that the graph is connected. For both classes, we
propose algorithms to identify the optimal approximations and also near-optimal
approximations, using a novel relaxation of submodularity. We also propose
algorithms to identify the r-best approximations among these classes, enabling
robust decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04773</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04773</id><created>2015-05-27</created><authors><author><keyname>Coffrin</keyname><forenames>Carleton</forenames></author><author><keyname>Hijazi</keyname><forenames>Hassan L.</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author></authors><title>DistFlow Extensions for AC Transmission Systems</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convex relaxations of the power flow equations and, in particular, the
Semi-Definite Programming (SDP), Second-Order Cone (SOC), and Convex DistFlow
(CDF) relaxations, have attracted significant interest in recent years. Thus
far, studies of the CDF model and its connection to the other relaxations have
been limited to power distribution systems, which omit several parameters
necessary for modeling transmission systems. To increase the applicability of
the CDF relaxation, this paper develops an extended CDF model that is suitable
for transmission systems by incorporating bus shunts, line charging, and
transformers. Additionally, a theoretical result shows that the established
equivalence of the SOC and CDF models for distribution systems also holds in
this transmission system extension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04775</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04775</id><created>2015-06-15</created><updated>2015-08-10</updated><authors><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author></authors><title>A stochastic density matrix approach to approximation of probability
  distributions and its application to nonlinear systems</title><categories>math.PR cs.SY math.AP math.NA math.ST stat.TH</categories><comments>12 pages, 3 figures. A brief version of this paper will appear in the
  proceedings of the IEEE Multi-Conference on Systems and Control, 21-23
  September 2015, Sydney, Australia</comments><msc-class>62E17, 93E03, 60J60, 35Q84, 42B37, 42C05, 65K10, 93B40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper outlines an approach to the approximation of probability density
functions by quadratic forms of weighted orthonormal basis functions with
positive semi-definite Hermitian matrices of unit trace. Such matrices are
called stochastic density matrices in order to reflect an analogy with the
quantum mechanical density matrices. The SDM approximation of a PDF satisfies
the normalization condition and is nonnegative everywhere in contrast to the
truncated Gram-Charlier and Edgeworth expansions. For bases with an algebraic
structure, such as the Hermite polynomial and Fourier bases, the SDM
approximation can be chosen so as to satisfy given moment specifications and
can be optimized using a quadratic proximity criterion. We apply the SDM
approach to the Fokker-Planck-Kolmogorov PDF dynamics of Markov diffusion
processes governed by nonlinear stochastic differential equations. This leads
to an ordinary differential equation for the SDM dynamics of the approximating
PDF. As an example, we consider the Smoluchowski SDE on a multidimensional
torus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04776</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04776</id><created>2015-06-15</created><authors><author><keyname>Heaton</keyname><forenames>Jeff</forenames></author></authors><title>Encog: Library of Interchangeable Machine Learning Models for Java and
  C#</title><categories>cs.MS cs.LG</categories><msc-class>68T01</msc-class><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the Encog library for Java and C#, a scalable,
adaptable, multiplatform machine learning framework that was 1st released in
2008. Encog allows a variety of machine learning models to be applied to
datasets using regression, classification, and clustering. Various supported
machine learning models can be used interchangeably with minimal recoding.
Encog uses efficient multithreaded code to reduce training time by exploiting
modern multicore processors. The current version of Encog can be downloaded
from http://www.encog.org.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04782</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04782</id><created>2015-06-15</created><updated>2015-06-18</updated><authors><author><keyname>Hanawal</keyname><forenames>Manjesh Kumar</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author><author><keyname>Valko</keyname><forenames>Michal</forenames></author><author><keyname>Munos</keyname><forenames>R\' emi</forenames></author></authors><title>Cheap Bandits</title><categories>cs.LG</categories><comments>To be presented at ICML 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider stochastic sequential learning problems where the learner can
observe the \textit{average reward of several actions}. Such a setting is
interesting in many applications involving monitoring and surveillance, where
the set of the actions to observe represent some (geographical) area. The
importance of this setting is that in these applications, it is actually
\textit{cheaper} to observe average reward of a group of actions rather than
the reward of a single action. We show that when the reward is \textit{smooth}
over a given graph representing the neighboring actions, we can maximize the
cumulative reward of learning while \textit{minimizing the sensing cost}. In
this paper we propose CheapUCB, an algorithm that matches the regret guarantees
of the known algorithms for this setting and at the same time guarantees a
linear cost again over them. As a by-product of our analysis, we establish a
$\Omega(\sqrt{dT})$ lower bound on the cumulative regret of spectral bandits
for a class of graphs with effective dimension $d$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04787</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04787</id><created>2015-06-15</created><updated>2015-09-19</updated><authors><author><keyname>Ogunmolu</keyname><forenames>Olalekan</forenames></author><author><keyname>Gu</keyname><forenames>Xuejun</forenames></author><author><keyname>Jiang</keyname><forenames>Steve</forenames></author><author><keyname>Gans</keyname><forenames>Nicholas</forenames></author></authors><title>A Real-Time Soft Robotic Patient Positioning System for Maskless
  Head-and-Neck Cancer Radiotherapy: An Initial Investigation</title><categories>cs.RO</categories><comments>IEEE Conference on Automation Science and Engineering, Gothenburg,
  Sweden. 2015. Life Sciences and Health Care; Mechatronics; Emerging Topics in
  Automation</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We present an initial examination of a novel approach to accurately position
a patient during head and neck intensity modulated radiotherapy (IMRT).
Position-based visual-servoing of a radio-transparent soft robot is used to
control the flexion/extension cranial motion of a manikin head. A Kinect RGB-D
camera is used to measure head position and the error between the sensed and
desired position is used to control a pneumatic system which regulates pressure
within an inflatable air bladder (IAB). Results show that the system is capable
of controlling head motion to within 2mm with respect to a reference
trajectory. This establishes proof-of-concept that using multiple IABs and
actuators can improve cancer treatment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04799</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04799</id><created>2015-06-15</created><authors><author><keyname>Taylor</keyname><forenames>Joshua Adam</forenames></author><author><keyname>Dhople</keyname><forenames>Sairaj V.</forenames></author><author><keyname>Callaway</keyname><forenames>Duncan S.</forenames></author></authors><title>Power Systems Without Fuel</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The finiteness of fossil fuels implies that future electric power systems may
predominantly source energy from fuel-free renewable resources like wind and
solar. Evidently, these power systems without fuel will be environmentally
benign, sustainable, and subject to milder failure scenarios. Many of these
advantages were projected decades ago with the definition of the soft energy
path, which describes a future where all energy is provided by numerous small,
simple, and diverse renewable sources. Here we provide a thorough investigation
of power systems without fuel from technical and economic standpoints. The
paper is organized by timescale and covers issues like the irrelevance of unit
commitment in networks without large, fuel-based generators, the dubiousness of
nodal pricing without fuel costs, and the need for new system-level models and
control methods for semiconductor-based energy-conversion interfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04802</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04802</id><created>2015-06-15</created><updated>2015-10-22</updated><authors><author><keyname>Li</keyname><forenames>Yao</forenames></author><author><keyname>Hu</keyname><forenames>Lili</forenames></author></authors><title>A fast exact simulation method for a class of Markov jump processes</title><categories>math.NA cs.MS</categories><comments>The reviewers' comments are addressed in the new version</comments><msc-class>60J22, 65C05, 65C40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method of the stochastic simulation algorithm (SSA), named the
Hashing-Leaping method (HLM), for exact simulations of a class of Markov jump
processes, is presented in this paper. The HLM has a conditional constant
computational cost per event, which is independent of the number of exponential
clocks in the Markov process. The main idea of the HLM is to repeatedly
implement a hash-table-like bucket sort algorithm for all times of occurrence
covered by a time step with length $\tau$. This paper serves as an introduction
to this new SSA method. We introduce the method, demonstrate its
implementation, analyze its properties, and compare its performance with three
other commonly used SSA methods in four examples. Our performance tests and CPU
operation statistics show certain advantage of the HLM for large scale
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04803</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04803</id><created>2015-06-15</created><authors><author><keyname>Rahimi</keyname><forenames>Afshin</forenames></author><author><keyname>Vu</keyname><forenames>Duy</forenames></author><author><keyname>Cohn</keyname><forenames>Trevor</forenames></author><author><keyname>Baldwin</keyname><forenames>Timothy</forenames></author></authors><title>Exploiting Text and Network Context for Geolocation of Social Media
  Users</title><categories>cs.CL cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on automatically geolocating social media users has conventionally
been based on the text content of posts from a given user or the social network
of the user, with very little crossover between the two, and no bench-marking
of the two approaches over compara- ble datasets. We bring the two threads of
research together in first proposing a text-based method based on adaptive
grids, followed by a hybrid network- and text-based method. Evaluating over
three Twitter datasets, we show that the empirical difference between text- and
network-based methods is not great, and that hybridisation of the two is
superior to the component methods, especially in contexts where the user graph
is not well connected. We achieve state-of-the-art results on all three
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04806</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04806</id><created>2015-06-15</created><updated>2015-08-12</updated><authors><author><keyname>Shankar</keyname><forenames>Sukrit</forenames></author><author><keyname>Dragotti</keyname><forenames>Pier Luigi</forenames></author></authors><title>A Novel Semantics and Feature Preserving Perspective for Content Aware
  Image Retargeting</title><categories>cs.GR</categories><comments>74 Pages, 46 Figures, Masters Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is an increasing requirement for efficient image retargeting techniques
to adapt the content to various forms of digital media. With rapid growth of
mobile communications and dynamic web page layouts, one often needs to resize
the media content to adapt to the desired display sizes. For various layouts of
web pages and typically small sizes of handheld portable devices, the
importance in the original image content gets obfuscated after resizing it with
the approach of uniform scaling. Thus, there occurs a need for resizing the
images in a content aware manner which can automatically discard irrelevant
information from the image and present the salient features with more
magnitude. There have been proposed some image retargeting techniques keeping
in mind the content awareness of the input image. However, these techniques
fail to prove globally effective for various kinds of images and desired sizes.
The major problem is the inefficiency of these algorithms to process these
images with minimal visual distortion while also retaining the meaning conveyed
from the image. In this dissertation, we present a novel perspective for
content aware image retargeting, which is well implementable in real time. We
introduce a novel method of analysing semantic information within the input
image while also maintaining the important and visually significant features.
We present the various nuances of our algorithm mathematically and logically,
and show that the results prove better than the state-of-the-art techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04810</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04810</id><created>2015-06-15</created><authors><author><keyname>Boyali</keyname><forenames>Ali</forenames></author><author><keyname>Hashimoto</keyname><forenames>Naohisa</forenames></author><author><keyname>Matsumoto</keyname><forenames>Osamu</forenames></author></authors><title>Paradigm Shift in Continuous Signal Pattern Classification: Mobile Ride
  Assistance System for two-wheeled Mobility Robots</title><categories>cs.RO</categories><comments>This paper introduce a training approach for continuous signal
  pattern classification and its application to braking state classification of
  a mobility robots. In our previous journal article, we didn't employ a
  training method thus this paper is an improvement of a previously published
  journal article</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study we describe the development of a ride assistance application
which can be implemented on the widespread smart phones and tablet. The ride
assistance application has a signal processing and pattern classification
module which yield almost 100% recognition accuracy for real-time signal
pattern classification. We introduce a novel framework to build a training
dictionary with an overwhelming discriminating capacity which eliminates the
need of human intervention spotting the pattern on the training samples. We
verify the recognition accuracy of the proposed methodologies by providing the
results of another study in which the hand posture and gestures are tracked and
recognized for steering a robotic wheelchair.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04812</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04812</id><created>2015-06-15</created><authors><author><keyname>Treust</keyname><forenames>Ma&#xeb;l Le</forenames></author></authors><title>Empirical Coordination with Two-Sided State Information and Correlated
  Source and State</title><categories>cs.IT math.IT</categories><comments>5 figures, 5 pages, presented at IEEE International Symposium on
  Information Theory (ISIT) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The coordination of autonomous agents is a critical issue for decentralized
communication networks. Instead of transmitting information, the agents
interact in a coordinated manner in order to optimize a general objective
function. A target joint probability distribution is achievable if there exists
a code such that the sequences of symbols are jointly typical. The empirical
coordination is strongly related to the joint source-channel coding with
two-sided state information and correlated source and state. This problem is
also connected to state communication and is open for non-causal encoder and
decoder. We characterize the optimal solutions for perfect channel, for
lossless decoding, for independent source and channel, for causal encoding and
for causal decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04814</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04814</id><created>2015-06-15</created><authors><author><keyname>Treust</keyname><forenames>Ma&#xeb;l Le</forenames></author></authors><title>Empirical Coordination with Channel Feedback and Strictly Causal or
  Causal Encoding</title><categories>cs.IT math.IT</categories><comments>5 pages, 6 figures, presented at IEEE International Symposium on
  Information Theory (ISIT) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multi-terminal networks, feedback increases the capacity region and helps
communication devices to coordinate. In this article, we deepen the
relationship between coordination and feedback by considering a point-to-point
scenario with an information source and a noisy channel. Empirical coordination
is achievable if the encoder and the decoder can implement sequences of symbols
that are jointly typical for a target probability distribution. We investigate
the impact of feedback when the encoder has strictly causal or causal
observation of the source symbols. For both cases, we characterize the optimal
information constraints and we show that feedback improves coordination
possibilities. Surprisingly, feedback also reduces the number of auxiliary
random variables and simplifies the information constraints. For empirical
coordination with strictly causal encoding and feedback, the information
constraint does not involve auxiliary random variable anymore.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04815</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04815</id><created>2015-06-15</created><authors><author><keyname>Chavan</keyname><forenames>Amit</forenames></author><author><keyname>Huang</keyname><forenames>Silu</forenames></author><author><keyname>Deshpande</keyname><forenames>Amol</forenames></author><author><keyname>Elmore</keyname><forenames>Aaron</forenames></author><author><keyname>Madden</keyname><forenames>Samuel</forenames></author><author><keyname>Parameswaran</keyname><forenames>Aditya</forenames></author></authors><title>Towards a unified query language for provenance and versioning</title><categories>cs.DB</categories><comments>Theory and Practice of Provenance, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Organizations and teams collect and acquire data from various sources, such
as social interactions, financial transactions, sensor data, and genome
sequencers. Different teams in an organization as well as different data
scientists within a team are interested in extracting a variety of insights
which require combining and collaboratively analyzing datasets in diverse ways.
DataHub is a system that aims to provide robust version control and provenance
management for such a scenario. To be truly useful for collaborative data
science, one also needs the ability to specify queries and analysis tasks over
the versioning and the provenance information in a unified manner. In this
paper, we present an initial design of our query language, called VQuel, that
aims to support such unified querying over both types of information, as well
as the intermediate and final results of analyses. We also discuss some of the
key language design and implementation challenges moving forward.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04820</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04820</id><created>2015-06-15</created><updated>2015-10-30</updated><authors><author><keyname>Beygelzimer</keyname><forenames>Alina</forenames></author><author><keyname>Hazan</keyname><forenames>Elad</forenames></author><author><keyname>Kale</keyname><forenames>Satyen</forenames></author><author><keyname>Luo</keyname><forenames>Haipeng</forenames></author></authors><title>Online Gradient Boosting</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the theory of boosting for regression problems to the online
learning setting. Generalizing from the batch setting for boosting, the notion
of a weak learning algorithm is modeled as an online learning algorithm with
linear loss functions that competes with a base class of regression functions,
while a strong learning algorithm is an online learning algorithm with convex
loss functions that competes with a larger class of regression functions. Our
main result is an online gradient boosting algorithm which converts a weak
online learning algorithm into a strong one where the larger class of functions
is the linear span of the base class. We also give a simpler boosting algorithm
that converts a weak online learning algorithm into a strong one where the
larger class of functions is the convex hull of the base class, and prove its
optimality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04822</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04822</id><created>2015-06-15</created><authors><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Ge</keyname><forenames>Gennian</forenames></author></authors><title>Some Improvements on Locally Repairable Codes</title><categories>cs.IT math.IT</categories><comments>14 pages. arXiv admin note: text overlap with arXiv:1409.0952,
  arXiv:1311.3284 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The locally repairable codes (LRCs) were introduced to correct erasures
efficiently in distributed storage systems. LRCs are extensively studied
recently.
  In this paper, we first deal with the open case remained in \cite{q} and
derive an improved upper bound for the minimum distances of LRCs. We also give
an explicit construction for LRCs attaining this bound. Secondly, we consider
the constructions of LRCs with any locality and availability which have high
code rate and minimum distance as large as possible. We give a graphical model
for LRCs. By using the deep results from graph theory, we construct a family of
LRCs with any locality $r$ and availability $2$ with code rate
$\frac{r-1}{r+1}$ and optimal minimum distance $O(\log n)$ where $n$ is the
length of the code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04828</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04828</id><created>2015-06-16</created><updated>2015-10-05</updated><authors><author><keyname>Ananthapadmanabha</keyname><forenames>T. V.</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>A. G.</forenames></author><author><keyname>Sharma</keyname><forenames>Shubham</forenames></author></authors><title>Significance of the levels of spectral valleys with application to
  front/back distinction of vowel sounds</title><categories>cs.CL cs.SD</categories><comments>39 pages, 6 figures, submitted to JASA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An objective critical distance (OCD) has been defined as that spacing between
adjacent formants, when the level of the valley between them reaches the mean
spectral level. The measured OCD lies in the same range (viz., 3-3.5 bark) as
the critical distance determined by subjective experiments for similar
experimental conditions. The level of spectral valley serves a purpose similar
to that of the spacing between the formants with an added advantage that it can
be measured from the spectral envelope without an explicit knowledge of formant
frequencies. Based on the relative spacing of formant frequencies, the level of
the spectral valley, VI (between F1 and F2) is much higher than the level of
VII (spectral valley between F2 and F3) for back vowels and vice-versa for
front vowels. Classification of vowels into front/back distinction with the
difference (VI-VII) as an acoustic feature, tested using TIMIT, NTIMIT, Tamil
and Kannada language databases gives, on the average, an accuracy of about 95%,
which is comparable to the accuracy (90.6%) obtained using a neural network
classifier trained and tested using MFCC as the feature vector for TIMIT
database. The acoustic feature (VI-VII) has also been tested for its robustness
on the TIMIT database for additive white and babble noise and an accuracy of
about 95% has been obtained for SNRs down to 25 dB for both types of noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04830</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04830</id><created>2015-06-16</created><updated>2016-02-16</updated><authors><author><keyname>Nardelli</keyname><forenames>Pedro H. J.</forenames></author><author><keyname>Tom&#xe9;</keyname><forenames>Mauricio de Castro</forenames></author><author><keyname>Alves</keyname><forenames>Hirley</forenames></author><author><keyname>de Lima</keyname><forenames>Carlos H. M.</forenames></author><author><keyname>Latva-aho</keyname><forenames>Matti</forenames></author></authors><title>Maximizing the Link Throughput between Smart-meters and Aggregators as
  Secondary Users under Power and Outage Constraints</title><categories>cs.SY cs.IT math.IT</categories><doi>10.1016/j.adhoc.2015.11.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper assesses the communication link from smart meters to aggregators
as (unlicensed) secondary users that transmit their data over the (licensed)
primary uplink channel. The proposed scenario assumes: (i) meters' and
aggregators' positions are fixed so highly directional antennas are employed,
(ii) secondary users transmit with limited power in relation to the primary,
(iii) meters' transmissions are coordinated to avoid packet collisions, and
(iv) the secondary links' robustness is guaranteed by an outage constraint.
Under these assumptions, the interference caused by secondary users in both
primary (base-stations) and other secondary users can be neglected. As
unlicensed users, however, meter-aggregator links do experience interference
from the mobile users of the primary network, whose positions and traffic
activity are unknown. To cope with this uncertainty, we model the mobile users
spatial distribution as a Poisson point process. We then derive a closed-form
solution for the maximum achievable throughput with respect to a reference
secondary link subject to transmit power and outage constraints. Our numerical
results illustrate the effects of such constraints on the optimal throughput,
evincing that more frequent outage events improve the system performance in the
scenario under study. We also show that relatively high outage probabilities
have little effect on the reconstruction of the average power demand curve that
is transmitted from the smart-meter to the aggregator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04832</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04832</id><created>2015-06-16</created><updated>2016-01-12</updated><authors><author><keyname>Shinde</keyname><forenames>Shweta</forenames></author><author><keyname>Chua</keyname><forenames>Zheng Leong</forenames></author><author><keyname>Narayanan</keyname><forenames>Viswesh</forenames></author><author><keyname>Saxena</keyname><forenames>Prateek</forenames></author></authors><title>Preventing Your Faults From Telling Your Secrets: Defenses Against
  Pigeonhole Attacks</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New hardware primitives such as Intel SGX secure a user-level process in
presence of an untrusted or compromised OS. Such &quot;enclaved execution&quot; systems
are vulnerable to several side-channels, one of which is the page fault
channel. In this paper, we show that the page fault side-channel has sufficient
channel capacity to extract bits of encryption keys from commodity
implementations of cryptographic routines in OpenSSL and Libgcrypt --- leaking
27% on average and up to 100% of the secret bits in many case-studies. To
mitigate this, we propose a software-only defense that masks page fault
patterns by determinising the program's memory access behavior. We show that
such a technique can be built into a compiler, and implement it for a subset of
C which is sufficient to handle the cryptographic routines we study. This
defense when implemented generically can have significant overhead of up to
4000X, but with help of developer-assisted compiler optimizations, the overhead
reduces to at most 29.22% in our case studies. Finally, we discuss scope for
hardware-assisted defenses, and show one solution that can reduce overheads to
6.77% with support from hardware changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04834</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04834</id><created>2015-06-16</created><updated>2015-11-09</updated><authors><author><keyname>Bowman</keyname><forenames>Samuel R.</forenames></author><author><keyname>Manning</keyname><forenames>Christopher D.</forenames></author><author><keyname>Potts</keyname><forenames>Christopher</forenames></author></authors><title>Tree-structured composition in neural networks without tree-structured
  architectures</title><categories>cs.CL cs.LG</categories><comments>To appear in the proceedings of the 2015 NIPS Workshop on Cognitive
  Computation: Integrating Neural and Symbolic Approaches</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tree-structured neural networks encode a particular tree geometry for a
sentence in the network design. However, these models have at best only
slightly outperformed simpler sequence-based models. We hypothesize that neural
sequence models like LSTMs are in fact able to discover and implicitly use
recursive compositional structure, at least for tasks with clear cues to that
structure in the data. We demonstrate this possibility using an artificial data
task for which recursive compositional structure is crucial, and find an
LSTM-based sequence model can indeed learn to exploit the underlying tree
structure. However, its performance consistently lags behind that of tree
models, even on large training sets, suggesting that tree-structured models are
more effective at exploiting recursive structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04836</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04836</id><created>2015-06-16</created><authors><author><keyname>Kaushal</keyname><forenames>Hemani</forenames></author><author><keyname>Kaddoum</keyname><forenames>Georges</forenames></author></authors><title>Free Space Optical Communication: Challenges and Mitigation Techniques</title><categories>cs.IT math.IT</categories><comments>28 pages, 13 figures and 8 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, free space optical (FSO) communication has gained
significant importance owing to its unique features: large bandwidth, license
free spectrum, high data rate, easy and quick deployability, less power and low
mass requirement. FSO communication uses optical carrier in the near infrared
(IR) and visible band to establish either terrestrial links within the Earths
atmosphere or inter-satellite or deep space links or ground to satellite or
satellite to ground links. However, despite of great potential of FSO
communication, its performance is limited by the adverse effects (viz.,
absorption, scattering and turbulence) of the atmospheric channel. Out of these
three effects, the atmospheric turbulence is a major challenge that may lead to
serious degradation in the bit error rate (BER) performance of the system and
make the communication link infeasible. This paper presents a comprehensive
survey on various challenges faced by FSO communication system for both
terrestrial and space links. It will provide details of various performance
mitigation techniques in order to have high link availability and reliability
of FSO system. The first part of the paper will focus on various types of
impairments that poses a serious challenge to the performance of FSO system for
both terrestrial and space links. The latter part of the paper will provide the
reader with an exhaustive review of various techniques used in FSO system both
at physical layer as well as at the upper layers (transport, network or link
layer) to combat the adverse effects of the atmosphere. Further, this survey
uniquely offers the current literature on FSO coding and modulation schemes
using various channel models and detection techniques. It also presents a
recently developed technique in FSO system using orbital angular momentum to
combat the effect of atmospheric turbulence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04838</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04838</id><created>2015-06-16</created><authors><author><keyname>Allen-Zhu</keyname><forenames>Zeyuan</forenames></author><author><keyname>Liao</keyname><forenames>Zhenyu</forenames></author><author><keyname>Orecchia</keyname><forenames>Lorenzo</forenames></author></authors><title>Spectral Sparsification and Regret Minimization Beyond Matrix
  Multiplicative Updates</title><categories>cs.LG cs.DS math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide a novel construction of the linear-sized spectral
sparsifiers of Batson, Spielman and Srivastava [BSS14]. While previous
constructions required $\Omega(n^4)$ running time [BSS14, Zou12], our
sparsification routine can be implemented in almost-quadratic running time
$O(n^{2+\varepsilon})$.
  The fundamental conceptual novelty of our work is the leveraging of a strong
connection between sparsification and a regret minimization problem over
density matrices. This connection was known to provide an interpretation of the
randomized sparsifiers of Spielman and Srivastava [SS11] via the application of
matrix multiplicative weight updates (MWU) [CHS11, Vis14]. In this paper, we
explain how matrix MWU naturally arises as an instance of the
Follow-the-Regularized-Leader framework and generalize this approach to yield a
larger class of updates. This new class allows us to accelerate the
construction of linear-sized spectral sparsifiers, and give novel insights on
the motivation behind Batson, Spielman and Srivastava [BSS14].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04843</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04843</id><created>2015-06-16</created><authors><author><keyname>Dasgupta</keyname><forenames>Anirban</forenames></author><author><keyname>Routray</keyname><forenames>Aurobinda</forenames></author><author><keyname>Mallavollu</keyname><forenames>Sai Nataraj</forenames></author></authors><title>Detection and Estimation of Iris Centre</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detection of iris center is an active area of research in the field of
computer vision and human-machine interaction systems. The major issues
involved in the detection of iris involves glint on the corneal region,
occlusion of the iris by eye-lids, occlusions due to eye gaze, high speed of
processing etc. This paper presents an algorithm for detecting and estimating
the iris center thereby addressing some of these issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04849</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04849</id><created>2015-06-16</created><authors><author><keyname>Jindal</keyname><forenames>Vandana</forenames></author><author><keyname>Verma</keyname><forenames>A. K.</forenames></author><author><keyname>Bawa</keyname><forenames>Seema</forenames></author></authors><title>Green WSN- Optimization of Energy Use Through Reduction in Communication
  Workload</title><categories>cs.NI</categories><comments>11 pages, 6 Figures, 3 Tables</comments><journal-ref>IJFCST ISSN: 1839-7662 May 2015, Volume 5, Number 3 pg 57-69</journal-ref><doi>10.5121/ijfcst.2015.5305</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications of Wireless Sensor Networks (WSNs) are growing day by day due to
the ease of deployment, reduction in costs to affordable levels and versatility
of these networks. Besides developing advanced micro fabrication technologies
means are being devised to reduce energy consumption to bring the network setup
and operational costs down. With increasing applications amount of energy
consumed in these networks is enormous. Even a small savings in energy
consumption will result in huge benefits in energy consumption globally. Bulk
of the energy is consumed in communication activity of these networks. It is
our endeavor to optimize this activity to make these networks energy efficient
and thereby reducing their impact on the overall environment in line with the
principle Go Green.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04855</identifier>
 <datestamp>2015-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04855</id><created>2015-06-16</created><updated>2015-07-23</updated><authors><author><keyname>Kot&#x142;owski</keyname><forenames>Wojciech</forenames></author><author><keyname>Warmuth</keyname><forenames>Manfred K.</forenames></author></authors><title>PCA with Gaussian perturbations</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of machine learning deals with vector parameters. Ideally we would like
to take higher order information into account and make use of matrix or even
tensor parameters. However the resulting algorithms are usually inefficient.
Here we address on-line learning with matrix parameters. It is often easy to
obtain online algorithm with good generalization performance if you
eigendecompose the current parameter matrix in each trial (at a cost of
$O(n^3)$ per trial). Ideally we want to avoid the decompositions and spend
$O(n^2)$ per trial, i.e. linear time in the size of the matrix data. There is a
core trade-off between the running time and the generalization performance,
here measured by the regret of the on-line algorithm (total gain of the best
off-line predictor minus the total gain of the on-line algorithm). We focus on
the key matrix problem of rank $k$ Principal Component Analysis in
$\mathbb{R}^n$ where $k \ll n$. There are $O(n^3)$ algorithms that achieve the
optimum regret but require eigendecompositions. We develop a simple algorithm
that needs $O(kn^2)$ per trial whose regret is off by a small factor of
$O(n^{1/4})$. The algorithm is based on the Follow the Perturbed Leader
paradigm. It replaces full eigendecompositions at each trial by the problem
finding $k$ principal components of the current covariance matrix that is
perturbed by Gaussian noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04857</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04857</id><created>2015-06-16</created><authors><author><keyname>Kwon</keyname><forenames>Keehang</forenames></author></authors><title>Mutually Exclusive Modules in Logic Programming</title><categories>cs.PL</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logic programming has traditionally lacked devices for expressing mutually
exclusive modules. We address this limitation by adopting choice-conjunctive
modules of the form $D_0 \&amp; D_1$ where $D_0, D_1$ are a conjunction of Horn
clauses and $\&amp;$ is a linear logic connective. Solving a goal $G$ using $D_0 \&amp;
D_1$ -- $exec(D_0 \&amp; D_1,G)$ -- has the following operational semantics:
$choose$ a successful one between $exec(D_0,G)$ and $exec(D_1,G)$. In other
words, if $D_0$ is chosen in the course of solving $G$, then $D_1$ will be
discarded and vice versa. Hence, the class of choice-conjunctive modules can
capture the notion of mutually exclusive modules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04861</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04861</id><created>2015-06-16</created><authors><author><keyname>Filtser</keyname><forenames>Omrit</forenames></author><author><keyname>Katz</keyname><forenames>Matthew J.</forenames></author></authors><title>The Discrete Fr\'echet Gap</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the discrete Fr\'echet gap and its variants as an alternative
measure of similarity between polygonal curves. We believe that for some
applications the new measure (and its variants) may better reflect our
intuitive notion of similarity than the discrete Fr\'echet distance (and its
variants), since the latter measure is indifferent to (matched) pairs of points
that are relatively close to each other. Referring to the frogs analogy by
which the discrete Fr\'echet distance is often described, the discrete
Fr\'echet gap is the minimum difference between the longest and shortest
positions of the leash needed for the frogs to traverse their point sequences.
  We present an optimization scheme, which is suitable for any monotone
function defined for pairs of distances such as the gap and ratio functions. We
apply this scheme to two variants of the discrete Fr\'echet gap, namely, the
one-sided discrete Fr\'echet gap with shortcuts and the weak discrete Fr\'echet
gap, to obtain $O(n^2 \log^2 n)$-time algorithms for computing them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04862</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04862</id><created>2015-06-16</created><updated>2015-08-17</updated><authors><author><keyname>Rubinchik</keyname><forenames>Mikhail</forenames></author><author><keyname>Shur</keyname><forenames>Arseny M.</forenames></author></authors><title>EERTREE: An Efficient Data Structure for Processing Palindromes in
  Strings</title><categories>cs.DS cs.FL</categories><comments>21 pages, 2 figures. Accepted to IWOCA 2015</comments><msc-class>68P05, 68W32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new linear-size data structure which provides a fast access to
all palindromic substrings of a string or a set of strings. This structure
inherits some ideas from the construction of both the suffix trie and suffix
tree. Using this structure, we present simple and efficient solutions for a
number of problems involving palindromes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04863</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04863</id><created>2015-06-16</created><authors><author><keyname>Passmore</keyname><forenames>Grant Olney</forenames></author></authors><title>Decidability of Univariate Real Algebra with Predicates for Rational and
  Integer Powers</title><categories>math.LO cs.LO</categories><comments>To appear in CADE-25: 25th International Conference on Automated
  Deduction, 2015. Proceedings to be published by Springer-Verlag</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove decidability of univariate real algebra extended with predicates for
rational and integer powers, i.e., $(x^n \in \mathbb{Q})$ and $(x^n \in
\mathbb{Z})$. Our decision procedure combines computation over real algebraic
cells with the rational root theorem and witness construction via algebraic
number density arguments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04867</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04867</id><created>2015-06-16</created><authors><author><keyname>Dawar</keyname><forenames>Siddharth</forenames></author><author><keyname>Goyal</keyname><forenames>Vikram</forenames></author><author><keyname>Bera</keyname><forenames>Debajyoti</forenames></author></authors><title>Efficient Reverse k Nearest Neighbor evaluation for hierarchical index</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Reverse Nearest Neighbor&quot; query finds applications in decision support
systems, profile-based marketing, emergency services etc. In this paper, we
point out a few flaws in the branch and bound algorithms proposed earlier for
computing monochromatic RkNN queries over data points stored in hierarchical
index. We give suitable counter examples to validate our claims and propose a
correct algorithm for the corresponding problem. We show that our algorithm is
correct by identifying necessary conditions behind correctness of algorithms
for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04869</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04869</id><created>2015-06-16</created><authors><author><keyname>Chang</keyname><forenames>Shuhua</forenames></author><author><keyname>Wang</keyname><forenames>Xinyu</forenames></author><author><keyname>Shananin</keyname><forenames>Alexander</forenames></author></authors><title>Modeling and Computation of Mean Field Equilibria in Producers' Game
  with Emission Permits Trading</title><categories>q-fin.EC cs.SY math.OC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we present a mean field game to model the production behaviors
of a very large number of producers, whose carbon emissions are regulated by
government. Especially, an emission permits trading scheme is considered in our
model, in which each enterprise can trade its own permits flexibly. By means of
the mean field equilibrium, we obtain a Hamilton-Jacobi-Bellman (HJB) equation
coupled with a Kolmogorov equation, which are satisfied by the adjoint state
and the density of producers (agents), respectively. Then, we propose a
so-called fitted finite volume method to solve the HJB equation and the
Kolmogorov equation. The efficiency and the usefulness of this method are
illustrated by the numerical experiments. Under different conditions, the
equilibrium states as well as the effects of the emission permits price are
examined, which demonstrates that the emission permits trading scheme
influences the producers' behaviors, that is, more populations would like to
choose a lower rather than a higher emission level when the emission permits
are expensive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04871</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04871</id><created>2015-06-16</created><updated>2016-02-10</updated><authors><author><keyname>Bozzano</keyname><forenames>Marco</forenames><affiliation>Fondazione Bruno Kessler</affiliation></author><author><keyname>Cimatti</keyname><forenames>Alessandro</forenames><affiliation>Fondazione Bruno Kessler</affiliation></author><author><keyname>Gario</keyname><forenames>Marco</forenames><affiliation>Fondazione Bruno Kessler</affiliation></author><author><keyname>Tonetta</keyname><forenames>Stefano</forenames><affiliation>Fondazione Bruno Kessler</affiliation></author></authors><title>Formal Design of Asynchronous Fault Detection and Identification
  Components using Temporal Epistemic Logic</title><categories>cs.LO</categories><comments>33 pages, 20 figures</comments><proxy>LMCS</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous critical systems, such as satellites and space rovers, must be
able to detect the occurrence of faults in order to ensure correct operation.
This task is carried out by Fault Detection and Identification (FDI)
components, that are embedded in those systems and are in charge of detecting
faults in an automated and timely manner by reading data from sensors and
triggering predefined alarms. The design of effective FDI components is an
extremely hard problem, also due to the lack of a complete theoretical
foundation, and of precise specification and validation techniques. In this
paper, we present the first formal approach to the design of FDI components for
discrete event systems, both in a synchronous and asynchronous setting. We
propose a logical language for the specification of FDI requirements that
accounts for a wide class of practical cases, and includes novel aspects such
as maximality and trace-diagnosability. The language is equipped with a clear
semantics based on temporal epistemic logic, and is proved to enjoy suitable
properties. We discuss how to validate the requirements and how to verify that
a given FDI component satisfies them. We propose an algorithm for the synthesis
of correct-by-construction FDI components, and report on the applicability of
the design approach on an industrial case-study coming from aerospace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04878</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04878</id><created>2015-06-16</created><updated>2015-07-08</updated><authors><author><keyname>Stewart</keyname><forenames>Russell</forenames></author><author><keyname>Andriluka</keyname><forenames>Mykhaylo</forenames></author></authors><title>End-to-end people detection in crowded scenes</title><categories>cs.CV</categories><comments>9 pages, 7 figures. Submitted to NIPS 2015. Supplementary material
  video: http://www.youtube.com/watch?v=QeWl0h3kQ24</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current people detectors operate either by scanning an image in a sliding
window fashion or by classifying a discrete set of proposals. We propose a
model that is based on decoding an image into a set of people detections. Our
system takes an image as input and directly outputs a set of distinct detection
hypotheses. Because we generate predictions jointly, common post-processing
steps such as non-maximum suppression are unnecessary. We use a recurrent LSTM
layer for sequence generation and train our model end-to-end with a new loss
function that operates on sets of detections. We demonstrate the effectiveness
of our approach on the challenging task of detecting people in crowded scenes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04879</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04879</id><created>2015-06-16</created><updated>2015-09-16</updated><authors><author><keyname>Astefanoaei</keyname><forenames>Lacramioara</forenames><affiliation>UJF, Verimag</affiliation></author><author><keyname>Rayana</keyname><forenames>Souha Ben</forenames><affiliation>UJF, Verimag</affiliation></author><author><keyname>Bensalem</keyname><forenames>Saddek</forenames><affiliation>UJF, Verimag</affiliation></author><author><keyname>Bozga</keyname><forenames>Marius</forenames><affiliation>CNRS, Verimag</affiliation></author><author><keyname>Combaz</keyname><forenames>Jacques</forenames><affiliation>CNRS, Verimag</affiliation></author></authors><title>Compositional Verification for Timed Systems Based on Automatic
  Invariant Generation</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:15) 2015</journal-ref><doi>10.2168/LMCS-11(3:15)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method for compositional verification to address the state space
explosion problem inherent to model-checking timed systems with a large number
of components. The main challenge is to obtain pertinent global timing
constraints from the timings in the components alone. To this end, we make use
of auxiliary clocks to automatically generate new invariants which capture the
constraints induced by the synchronisations between components. The method has
been implemented in the RTD-Finder tool and successfully experimented on
several benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04882</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04882</id><created>2015-06-16</created><authors><author><keyname>Goldberg</keyname><forenames>Paul W.</forenames></author></authors><title>The Complexity of the Path-following Solutions of Two-dimensional
  Sperner/Brouwer Functions</title><categories>cs.CC</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are a number of results saying that for certain &quot;path-following&quot;
algorithms that solve PPAD-complete problems, the solution obtained by the
algorithm is PSPACE-complete to compute. We conjecture that these results are
special cases of a much more general principle, that all such algorithms
compute PSPACE-complete solutions. Such a general result might shed new light
on the complexity class PPAD.
  In this paper we present a new PSPACE-completeness result for an interesting
challenge instance for this conjecture. Chen and Deng~\cite{CD} showed that it
is PPAD-complete to find a trichromatic triangle in a concisely-represented
Sperner triangulation. The proof of Sperner's lemma --- that such a solution
always exists --- identifies one solution in particular, that is found via a
natural &quot;path-following&quot; approach. Here we show that it is PSPACE-complete to
compute this specific solution, together with a similar result for the
computation of the path-following solution of two-dimensional discrete Brouwer
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04885</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04885</id><created>2015-06-16</created><updated>2015-12-21</updated><authors><author><keyname>Asarin</keyname><forenames>Eugene</forenames><affiliation>LIAFA</affiliation></author><author><keyname>Cervelle</keyname><forenames>Julien</forenames><affiliation>LACL</affiliation></author><author><keyname>Degorre</keyname><forenames>Aldric</forenames><affiliation>LIAFA</affiliation></author><author><keyname>Dima</keyname><forenames>Catalin</forenames><affiliation>LACL</affiliation></author><author><keyname>Horn</keyname><forenames>Florian</forenames><affiliation>LIAFA</affiliation></author><author><keyname>Kozyakin</keyname><forenames>Victor</forenames><affiliation>IITP</affiliation></author></authors><title>Entropy Games and Matrix Multiplication Games</title><categories>cs.GT cs.IT math.IT</categories><comments>Accepted to STACS 2016</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two intimately related new classes of games are introduced and studied:
entropy games (EGs) and matrix multiplication games (MMGs). An EG is played on
a finite arena by two-and-a-half players: Despot, Tribune and the
non-deterministic People. Despot wants to make the set of possible People's
behaviors as small as possible, while Tribune wants to make it as large as
possible.An MMG is played by two players that alternately write matrices from
some predefined finite sets. One wants to maximize the growth rate of the
product, and the other to minimize it. We show that in general MMGs are
undecidable in quite a strong sense.On the positive side, EGs correspond to a
subclass of MMGs, and we prove that such MMGs and EGs are determined, and that
the optimal strategies are simple. The complexity of solving such games is in
NP\&amp;coNP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04886</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04886</id><created>2015-06-16</created><authors><author><keyname>Xu</keyname><forenames>Guangkui</forenames></author><author><keyname>Cao</keyname><forenames>Xiwang</forenames></author><author><keyname>Xu</keyname><forenames>Shanding</forenames></author></authors><title>Several new classes of Boolean functions with few Walsh transform values</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, several new classes of Boolean functions with few Walsh
transform values, including bent, semi-bent and five-valued functions, are
obtained by adding the product of two or three linear functions to some known
bent functions.Numerical results show that the proposed class contains cubic
bent functions that are affinely inequivalent to all known quadratic ones.
Meanwhile, we determine the distribution of the Walsh spectrum of five-valued
functions constructed in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04891</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04891</id><created>2015-06-16</created><authors><author><keyname>Bagnall</keyname><forenames>Douglas</forenames></author></authors><title>Author Identification using Multi-headed Recurrent Neural Networks</title><categories>cs.CL cs.LG cs.NE</categories><comments>8 pages, 3 figures Notebook for the PAN@CLEF Author Identification
  challange</comments><msc-class>68T50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent neural networks (RNNs) are very good at modelling the flow of text,
but typically need to be trained on a far larger corpus than is available for
the PAN 2015 Author Identification task. This paper describes a novel approach
where the output layer of a character-level RNN language model is split into
several independent predictive sub-models, each representing an author, while
the recurrent layer is shared by all. This allows the recurrent layer to model
the language as a whole without over-fitting, while the outputs select aspects
of the underlying model that reflect their author's style. The method proves
competitive, ranking first in two of the four languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04894</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04894</id><created>2015-06-16</created><authors><author><keyname>Jamali</keyname><forenames>Vahid</forenames></author><author><keyname>Michalopoulos</keyname><forenames>Diomidis S.</forenames></author><author><keyname>Uysal</keyname><forenames>Murat</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Resource Allocation for Mixed RF and Hybrid RF/FSO Relaying</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a mixed RF and hybrid RF/FSO system where several
mobile users transmit their data over an RF link to a relay node (e.g. a small
cell base station) and the relay forwards the information to a destination
(e.g. a macro cell base station) over a hybrid RF/FSO backhaul link. The relay
and the destination employ multiple antennas for transmission and reception
over the RF links while each mobile user has a single antenna. The RF links are
full-duplex with respect to the FSO link and half-duplex with respect to each
other, i.e., either the user-relay RF link or the relay-destination RF link is
active. For this communication setup, we derive the optimal resource allocation
policy for sharing the RF bandwidth resource between the RF links. Our
numerical results show the effectiveness of the proposed communication
architecture and resource allocation policy, and their superiority compared to
existing schemes which employ only one type of backhaul link.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04896</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04896</id><created>2015-06-16</created><updated>2015-10-26</updated><authors><author><keyname>Grabowski</keyname><forenames>Szymon</forenames></author><author><keyname>Raniszewski</keyname><forenames>Marcin</forenames></author><author><keyname>Deorowicz</keyname><forenames>Sebastian</forenames></author></authors><title>FM-index for dummies</title><categories>cs.DS</categories><msc-class>68W32</msc-class><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The FM-index is a celebrated compressed data structure for full-text pattern
searching. After the first wave of interest in its theoretical developments, we
can observe a surge of interest in practical FM-index variants in the last few
years. These enhancements are often related to a bit-vector representation,
augmented with an efficient rank-handling data structure. In this work, we
propose a new, cache-friendly, implementation of the rank primitive and
advocate for a very simple architecture of the FM-index, which trades
compression ratio for speed. Experimental results show that our variants are
2--3 times faster than the fastest known ones, for the price of using typically
1.5--5 times more space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04897</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04897</id><created>2015-06-16</created><authors><author><keyname>Rosa</keyname><forenames>Rudolf</forenames></author></authors><title>Parsing Natural Language Sentences by Semi-supervised Methods</title><categories>cs.CL</categories><comments>Dissertation interim report. Overlap with papers accepted to ACL 2015
  and Depling 2015, and a paper under review at IWPT 2015</comments><report-no>3039210042125978224</report-no><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present our work on semi-supervised parsing of natural language sentences,
focusing on multi-source crosslingual transfer of delexicalized dependency
parsers. We first evaluate the influence of treebank annotation styles on
parsing performance, focusing on adposition attachment style. Then, we present
KLcpos3, an empirical language similarity measure, designed and tuned for
source parser weighting in multi-source delexicalized parser transfer. And
finally, we introduce a novel resource combination method, based on
interpolation of trained parser models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04902</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04902</id><created>2015-06-16</created><authors><author><keyname>Hassan</keyname><forenames>Ahmed Hassan M.</forenames></author><author><keyname>Zayid</keyname><forenames>Elrasheed Ismail M.</forenames></author><author><keyname>Awad</keyname><forenames>Mohammed Altayeb</forenames></author><author><keyname>Mohammed</keyname><forenames>Ahmed Salah</forenames></author><author><keyname>Hassan</keyname><forenames>Samreen Tarig</forenames></author></authors><title>Performance Evaluation Of Qos In Wimax Network</title><categories>cs.NI cs.IT math.IT</categories><doi>10.5121/caij.2015.2202</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  OPNET Modeler is used to simulate the architecture and to calculate the
performance criteria (i.e. throughput, delay and data dropped) that slightly
concerned in network estimation. It is concluded that our models shorten the
time quite a bit for obtaining the performance measures of an end-to-end delay
as well as throughput can be used as an effective tool for this purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04904</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04904</id><created>2015-06-16</created><authors><author><keyname>Schuler</keyname><forenames>Jonas</forenames></author><author><keyname>Sabzevari</keyname><forenames>Reza</forenames></author><author><keyname>Scaramuzza</keyname><forenames>Davide</forenames></author></authors><title>LightPanel: Active Mobile Platform for Dense 3D Modelling</title><categories>cs.RO cs.MM</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a novel platform for dense 3D modelling. This
platform is an active image acquisition setup assisted with a set of light
sources and a distance sensor. The hardware setup is designed for being mounted
on a mobile robot which is remotely driven to create accurate dense 3D models
from out-of-reach objects. For this reason, the object is actively illuminated
by the imaging setup and Photometric Stereo is used to recover the dense 3D
model. The proposed image acquisition setup, called LightPanel, is described
from design to calibration and discusses the practical challenges of using
Photometric Stereo under uncontrolled lighting conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04908</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04908</id><created>2015-06-16</created><authors><author><keyname>Roulet</keyname><forenames>Vincent</forenames></author><author><keyname>Fogel</keyname><forenames>Fajwel</forenames></author><author><keyname>d'Aspremont</keyname><forenames>Alexandre</forenames></author><author><keyname>Bach</keyname><forenames>Francis</forenames></author></authors><title>Supervised Clustering in the Data Cube</title><categories>cs.LG</categories><comments>Submitted to NIPS</comments><msc-class>68T05, 91C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a supervised clustering problem seeking to cluster either features,
tasks or sample points using losses extracted from supervised learning
problems. We formulate a unified optimization problem handling these three
settings and derive algorithms whose core iteration complexity is concentrated
in a k-means clustering step, which can be approximated efficiently. We test
our methods on both artificial and realistic data sets extracted from movie
reviews and 20NewsGroup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04910</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04910</id><created>2015-06-16</created><authors><author><keyname>Sezgin</keyname><forenames>Ali</forenames></author></authors><title>Sequential Consistency and Concurrent Data Structures</title><categories>cs.DC cs.DS</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linearizability, the de facto correctness condition for concurrent data
structure implementations, despite its intuitive appeal is known to lead to
poor scalability. This disadvantage has led researchers to design scalable data
structures satisfying consistency conditions weaker than linearizability.
Despite this recent trend, sequential consistency as a strictly weaker
consistency condition than linearizability has received no interest.
  In this paper, we investigate the applicability of sequential consistency as
an alternative correctness criterion for concurrent data structure
implementations. Our first finding formally justifies the reluctance in moving
towards sequentially consistent data structures: Implementations in which each
thread modifies only its thread-local variables are sequentially consistent for
various standard data structures such as pools, queues and stacks. We also show
that for almost all data structures, and the data structures we consider in
this paper, it is possible to have sequentially consistent behaviors in which a
designated thread does not synchronize at all. As a potential remedy, we define
a hierarchy of quantitatively strengthened variants of sequential consistency
such that the stronger the variant the more synchronization it enforces which
at the limit is equal to that enforced by linearizability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04911</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04911</id><created>2015-06-16</created><updated>2015-06-25</updated><authors><author><keyname>Mobilia</keyname><forenames>Mauro</forenames></author></authors><title>Nonlinear $q$-voter model with inflexible zealots</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI nlin.AO q-bio.PE</categories><comments>11 pages, 10 figures. For Physical Review E (accepted, final version)</comments><journal-ref>Phys. Rev. E 92, 012803 (2015)</journal-ref><doi>10.1103/PhysRevE.92.012803</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the dynamics of the nonlinear $q$-voter model with inflexible
zealots in a finite well-mixed population. In this system, each individual
supports one of two parties and is either a susceptible voter or an inflexible
zealot. At each time step, a susceptible adopts the opinion of a neighbor if
this belongs to a group of $q\geq 2$ neighbors all in the same state, whereas
inflexible zealots never change their opinion. In the presence of zealots of
both parties the model is characterized by a fluctuating stationary state and,
below a zealotry density threshold, the distribution of opinions is bimodal.
After a characteristic time, most susceptibles become supporters of the party
having more zealots and the opinion distribution is asymmetric. When the number
of zealots of both parties is the same, the opinion distribution is symmetric
and, in the long run, susceptibles endlessly swing from the state where they
all support one party to the opposite state. Above the zealotry density
threshold, when there is an unequal number of zealots of each type, the
probability distribution is single-peaked and non-Gaussian. These properties
are investigated analytically and with stochastic simulations. We also study
the mean time to reach a consensus when zealots support only one party.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04912</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04912</id><created>2015-06-16</created><authors><author><keyname>Abolghasemi</keyname><forenames>Vahid</forenames></author><author><keyname>Shen</keyname><forenames>Hao</forenames></author><author><keyname>Shen</keyname><forenames>Yaochun</forenames></author><author><keyname>Gan</keyname><forenames>Lu</forenames></author></authors><title>Subsampled terahertz data reconstruction based on spatio-temporal
  dictionary learning</title><categories>cs.CV</categories><doi>10.1016/j.dsp.2015.04.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of terahertz pulsed imaging and reconstruction is
addressed. It is assumed that an incomplete (subsampled) three dimensional THz
data set has been acquired and the aim is to recover all missing samples. A
sparsity-inducing approach is proposed for this purpose. First, a simple
interpolation is applied to incomplete noisy data. Then, we propose a
spatio-temporal dictionary learning method to obtain an appropriate sparse
representation of data based on a joint sparse recovery algorithm. Then, using
the sparse coefficients and the learned dictionary, the 3D data is effectively
denoised by minimizing a simple cost function. We consider two types of
terahertz data to evaluate the performance of the proposed approach; THz data
acquired for a model sample with clear layered structures (e.g., a T-shape
plastic sheet buried in a polythene pellet), and pharmaceutical tablet data
(with low spatial resolution). The achieved signal-to-noise-ratio for
reconstruction of T-shape data, from only 5% observation was 19 dB. Moreover,
the accuracies of obtained thickness and depth measurements for pharmaceutical
tablet data after reconstruction from 10% observation were 98.8%, and 99.9%,
respectively. These results, along with chemical mapping analysis, presented at
the end of this paper, confirm the accuracy of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04917</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04917</id><created>2015-06-16</created><updated>2015-12-22</updated><authors><author><keyname>Crochemore</keyname><forenames>Maxime</forenames></author><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author><author><keyname>Merca&#x15f;</keyname><forenames>Robert</forenames></author><author><keyname>Pissis</keyname><forenames>Solon P.</forenames></author></authors><title>Linear-Time Sequence Comparison Using Minimal Absent Words &amp;
  Applications</title><categories>cs.DS cs.FL</categories><comments>Accepted to LATIN 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sequence comparison is a prerequisite to virtually all comparative genomic
analyses. It is often realized by sequence alignment techniques, which are
computationally expensive. This has led to increased research into
alignment-free techniques, which are based on measures referring to the
composition of sequences in terms of their constituent patterns. These
measures, such as $q$-gram distance, are usually computed in time linear with
respect to the length of the sequences. In this article, we focus on the
complementary idea: how two sequences can be efficiently compared based on
information that does not occur in the sequences. A word is an {\em absent
word} of some sequence if it does not occur in the sequence. An absent word is
{\em minimal} if all its proper factors occur in the sequence. Here we present
the first linear-time and linear-space algorithm to compare two sequences by
considering {\em all} their minimal absent words. In the process, we present
results of combinatorial interest, and also extend the proposed techniques to
compare circular sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04923</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04923</id><created>2015-06-16</created><authors><author><keyname>Ray</keyname><forenames>Kumar Sankar</forenames></author><author><keyname>Mondal</keyname><forenames>Mandrita</forenames></author></authors><title>Logical Inference by DNA Strand Algebra</title><categories>q-bio.BM cs.ET</categories><comments>18 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the concept of DNA strand displacement and DNA strand algebra we
have developed a method for logical inference which is not based on silicon
based computing. Essentially, it is a paradigm shift from silicon to carbon. In
this paper we have considered the inference mechanism, viz. modus ponens, to
draw conclusion from any observed fact. Thus, the present approach to logical
inference based on DNA strand algebra is basically an attempt to develop expert
system design in the domain of DNA computing. We have illustrated our
methodology with respect to worked out example. Our methodology is very
flexible for implementation of different expert system applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04924</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04924</id><created>2015-06-16</created><updated>2015-06-17</updated><authors><author><keyname>Hong</keyname><forenames>Seunghoon</forenames></author><author><keyname>Noh</keyname><forenames>Hyeonwoo</forenames></author><author><keyname>Han</keyname><forenames>Bohyung</forenames></author></authors><title>Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation</title><categories>cs.CV</categories><comments>Added a link to the project page for more comprehensive illustration
  of results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel deep neural network architecture for semi-supervised
semantic segmentation using heterogeneous annotations. Contrary to existing
approaches posing semantic segmentation as a single task of region-based
classification, our algorithm decouples classification and segmentation, and
learns a separate network for each task. In this architecture, labels
associated with an image are identified by classification network, and binary
segmentation is subsequently performed for each identified label in
segmentation network. The decoupled architecture enables us to learn
classification and segmentation networks separately based on the training data
with image-level and pixel-wise class labels, respectively. It facilitates to
reduce search space for segmentation effectively by exploiting class-specific
activation maps obtained from bridging layers. Our algorithm shows outstanding
performance compared to other semi-supervised approaches even with much less
training images with strong annotations in PASCAL VOC dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04929</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04929</id><created>2015-06-16</created><authors><author><keyname>Wa&#x142;&#x119;ga</keyname><forenames>Przemys&#x142;aw Andrzej</forenames></author><author><keyname>Bhatt</keyname><forenames>Mehul</forenames></author><author><keyname>Schultz</keyname><forenames>Carl</forenames></author></authors><title>ASPMT(QS): Non-Monotonic Spatial Reasoning with Answer Set Programming
  Modulo Theories</title><categories>cs.AI cs.LO</categories><comments>pages 13. accepted for publication at: LPNMR 2015 - Logic Programming
  and Nonmonotonic Reasoning, 13th International Conference, LPNMR 2015, LNAI
  Vol. 9345., Lexington, September 27-30, 2015. Proceedings., (editors:
  Francesco Calimeri, Giovambattista Ianni, Miroslaw Truszczynski)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The systematic modelling of \emph{dynamic spatial systems} [9] is a key
requirement in a wide range of application areas such as comonsense cognitive
robotics, computer-aided architecture design, dynamic geographic information
systems. We present ASPMT(QS), a novel approach and fully-implemented prototype
for non-monotonic spatial reasoning ---a crucial requirement within dynamic
spatial systems-- based on Answer Set Programming Modulo Theories (ASPMT).
ASPMT(QS) consists of a (qualitative) spatial representation module (QS) and a
method for turning tight ASPMT instances into Sat Modulo Theories (SMT)
instances in order to compute stable models by means of SMT solvers. We
formalise and implement concepts of default spatial reasoning and spatial frame
axioms using choice formulas. Spatial reasoning is performed by encoding
spatial relations as systems of polynomial constraints, and solving via SMT
with the theory of real nonlinear arithmetic. We empirically evaluate ASPMT(QS)
in comparison with other prominent contemporary spatial reasoning systems. Our
results show that ASPMT(QS) is the only existing system that is capable of
reasoning about indirect spatial effects (i.e. addressing the ramification
problem), and integrating geometric and qualitative spatial information within
a non-monotonic spatial reasoning context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04931</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04931</id><created>2015-06-16</created><authors><author><keyname>K</keyname><forenames>Anjan</forenames></author><author><keyname>K</keyname><forenames>Srinath N</forenames></author><author><keyname>Abraham</keyname><forenames>Jibi</forenames></author></authors><title>Entropy Based Detection And Behavioral Analysis Of Hybrid Covert
  Channeling Secured Communication</title><categories>cs.CR</categories><comments>15 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Covert channels is a vital setup in the analysing the strength of security in
a network.Covert Channel is illegitimate channelling over the secured channel
and establishes a malicious conversation.The trapdoor set in such channels
proliferates making covert channel sophisticated to detect their presence in
network firewall.This is due to the intricate covert scheme that enables to
build robust covert channel over the network.From an attacker's perspective
this will ameliorate by placing multiple such trapdoors in different protocols
in the rudimentary protocol stack. This leads to a unique scenario of Hybrid
Covert Channel, where different covert channel trapdoors exist at the same
instance of time in same layer of protocol stack. For detection agents to
detect such event is complicated due to lack of knowledge over the different
covert schemes. To improve the knowledge of the detection engine to detect the
hybrid covert channel scenario it is required to explore all possible
clandestine mediums used in the formation of such channels. This can be
explored by different schemes available and their entropy impact on hybrid
covert channel. The environment can be composed of resources and subject under
at-tack and subject which have initiated the attack (attacker). The paper sets
itself an objective to understand the different covert schemes and the attack
scenario (modelling) and possibilities of covert mediums along with metric for
detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04933</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04933</id><created>2015-06-16</created><updated>2015-06-22</updated><authors><author><keyname>Sekeh</keyname><forenames>Salimeh Yasaei</forenames></author><author><keyname>Polpo</keyname><forenames>Adriano</forenames></author></authors><title>On relative weighted entropies with central moments weight functions</title><categories>cs.IT math.IT math.PR</categories><comments>22 pages, 4figures</comments><msc-class>60A10, 60B05, 60C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following [1], the aim of this paper is to analyze the relative weighted
entropy involving the central moments weight functions. We compare the standard
relative entropy with the weighted case in two particular forms of Gaussian
distributions. As an application, the weighted deviance information criterion
is proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04935</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04935</id><created>2015-06-16</created><authors><author><keyname>Gu&#xe9;rit</keyname><forenames>St&#xe9;phanie</forenames></author><author><keyname>Jacques</keyname><forenames>Laurent</forenames></author><author><keyname>Macq</keyname><forenames>Beno&#xee;t</forenames></author><author><keyname>Lee</keyname><forenames>John A.</forenames></author></authors><title>Post-Reconstruction Deconvolution of PET Images by Total Generalized
  Variation Regularization</title><categories>cs.CV math.OC</categories><comments>First published in the Proceedings of the 23rd European Signal
  Processing Conference (EUSIPCO-2015) in 2015, published by EURASIP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improving the quality of positron emission tomography (PET) images, affected
by low resolution and high level of noise, is a challenging task in nuclear
medicine and radiotherapy. This work proposes a restoration method, achieved
after tomographic reconstruction of the images and targeting clinical
situations where raw data are often not accessible. Based on inverse problem
methods, our contribution introduces the recently developed total generalized
variation (TGV) norm to regularize PET image deconvolution. Moreover, we
stabilize this procedure with additional image constraints such as positivity
and photometry invariance. A criterion for updating and adjusting automatically
the regularization parameter in case of Poisson noise is also presented.
Experiments are conducted on both synthetic data and real patient images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04940</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04940</id><created>2015-06-16</created><authors><author><keyname>Ma</keyname><forenames>Xi</forenames></author><author><keyname>Wang</keyname><forenames>Xiaoxi</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author><author><keyname>Zhang</keyname><forenames>Zhiyong</forenames></author></authors><title>Recognize Foreign Low-Frequency Words with Similar Pairs</title><categories>cs.CL</categories><report-no>1064</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-frequency words place a major challenge for automatic speech recognition
(ASR). The probabilities of these words, which are often important name
entities, are generally under-estimated by the language model (LM) due to their
limited occurrences in the training data. Recently, we proposed a word-pair
approach to deal with the problem, which borrows information of frequent words
to enhance the probabilities of low-frequency words. This paper presents an
extension to the word-pair method by involving multiple `predicting words' to
produce better estimation for low-frequency words. We also employ this approach
to deal with out-of-language words in the task of multi-lingual speech
recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04945</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04945</id><created>2015-06-16</created><authors><author><keyname>Schultz</keyname><forenames>Carl</forenames></author><author><keyname>Bhatt</keyname><forenames>Mehul</forenames></author></authors><title>Spatial Symmetry Driven Pruning Strategies for Efficient Declarative
  Spatial Reasoning</title><categories>cs.AI cs.LO cs.PL</categories><comments>22 pages. Accepted for publication at: COSIT 2015 - Conference on
  Spatial Information Theory XII (COSIT), Santa Fe, New Mexico, USA ,October
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Declarative spatial reasoning denotes the ability to (declaratively) specify
and solve real-world problems related to geometric and qualitative spatial
representation and reasoning within standard knowledge representation and
reasoning (KR) based methods (e.g., logic programming and derivatives). One
approach for encoding the semantics of spatial relations within a declarative
programming framework is by systems of polynomial constraints. However, solving
such constraints is computationally intractable in general (i.e. the theory of
real-closed fields).
  We present a new algorithm, implemented within the declarative spatial
reasoning system CLP(QS), that drastically improves the performance of deciding
the consistency of spatial constraint graphs over conventional polynomial
encodings. We develop pruning strategies founded on spatial symmetries that
form equivalence classes (based on affine transformations) at the qualitative
spatial level. Moreover, pruning strategies are themselves formalised as
knowledge about the properties of space and spatial symmetries. We evaluate our
algorithm using a range of benchmarks in the class of contact problems, and
proofs in mereology and geometry. The empirical results show that CLP(QS) with
knowledge-based spatial pruning outperforms conventional polynomial encodings
by orders of magnitude, and can thus be applied to problems that are otherwise
unsolvable in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04954</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04954</id><created>2015-06-08</created><authors><author><keyname>Soltani</keyname><forenames>Sara</forenames></author><author><keyname>Kilmer</keyname><forenames>Misha E.</forenames></author><author><keyname>Hansen</keyname><forenames>Per Christian</forenames></author></authors><title>A Tensor-Based Dictionary Learning Approach to Tomographic Image
  Reconstruction</title><categories>cs.CV cs.NA math.NA</categories><comments>29 pages</comments><msc-class>15A69, 65F22, 65K10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider tomographic reconstruction using priors in the form of a
dictionary learned from training images. The reconstruction has two stages:
first we construct a tensor dictionary prior from our training data, and then
we pose the reconstruction problem in terms of recovering the expansion
coefficients in that dictionary. Our approach differs from past approaches in
that a) we use a third-order tensor representation for our images and b) we
recast the reconstruction problem using the tensor formulation. The dictionary
learning problem is presented as a non-negative tensor factorization problem
with sparsity constraints. The reconstruction problem is formulated in a convex
optimization framework by looking for a solution with a sparse representation
in the tensor dictionary. Numerical results show that our tensor formulation
leads to very sparse representations of both the training images and the
reconstructions due to the ability of representing repeated features compactly
in the dictionary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04956</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04956</id><created>2015-06-16</created><authors><author><keyname>Davis</keyname><forenames>Ernest</forenames></author><author><keyname>Marcus</keyname><forenames>Gary</forenames></author></authors><title>The Scope and Limits of Simulation in Cognitive Models</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been proposed that human physical reasoning consists largely of
running &quot;physics engines in the head&quot; in which the future trajectory of the
physical system under consideration is computed precisely using accurate
scientific theories. In such models, uncertainty and incomplete knowledge is
dealt with by sampling probabilistically over the space of possible
trajectories (&quot;Monte Carlo simulation&quot;). We argue that such simulation-based
models are too weak, in that there are many important aspects of human physical
reasoning that cannot be carried out this way, or can only be carried out very
inefficiently; and too strong, in that humans make large systematic errors that
the models cannot account for. We conclude that simulation-based reasoning
makes up at most a small part of a larger system that encompasses a wide range
of additional cognitive processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04971</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04971</id><created>2015-06-16</created><authors><author><keyname>Phan</keyname><forenames>Anh-Huy</forenames></author><author><keyname>Tichavsky</keyname><forenames>Petr</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>Tensor Deflation for CANDECOMP/PARAFAC. Part 3: Rank Splitting</title><categories>cs.NA math.OC</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  CANDECOMP/PARAFAC (CPD) approximates multiway data by sum of rank-1 tensors.
Our recent study has presented a method to rank-1 tensor deflation, i.e.
sequential extraction of the rank-1 components. In this paper, we extend the
method to block deflation problem. When at least two factor matrices have full
column rank, one can extract two rank-1 tensors simultaneously, and rank of the
data tensor is reduced by 2. For decomposition of order-3 tensors of size R x R
x R and rank-R, the block deflation has a complexity of O(R^3) per iteration
which is lower than the cost O(R^4) of the ALS algorithm for the overall CPD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04972</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04972</id><created>2015-06-16</created><authors><author><keyname>Yang</keyname><forenames>Yang</forenames></author><author><keyname>Pesavento</keyname><forenames>Marius</forenames></author></authors><title>A Novel Iterative Convex Approximation Method</title><categories>math.OC cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel iterative convex approximation algorithm to
efficiently compute stationary points for a large class of possibly nonconvex
optimization problems. The stationary points are obtained by solving a sequence
of successively refined approximate problems, each of which is much easier to
solve than the original problem. To achieve convergence, the approximate
problem only needs to exhibit a weak form of convexity, namely,
pseudo-convexity. We show that the proposed framework not only includes as
special cases a number of existing methods, for example, the gradient method
and the Jacobi algorithm, but also leads to new algorithms which enjoy easier
implementation and faster convergence speed. We also propose a novel line
search method for nondifferentiable optimization problems, which is carried out
over a properly constructed differentiable function with the benefit of a
simplified implementation as compared to state-of-the-art line search
techniques that directly operate on the original nondifferentiable objective
function. The advantages of the proposed algorithm are shown, both
theoretically and numerically, by several example applications, namely, MIMO
broadcast channel capacity computation and LASSO in sparse signal recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04986</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04986</id><created>2015-06-16</created><authors><author><keyname>Ni</keyname><forenames>Eric C.</forenames></author><author><keyname>Ciocan</keyname><forenames>Dragos F.</forenames></author><author><keyname>Henderson</keyname><forenames>Shane G.</forenames></author><author><keyname>Hunter</keyname><forenames>Susan R.</forenames></author></authors><title>Efficient Ranking and Selection in Parallel Computing Environments</title><categories>cs.DC math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of ranking and selection (R&amp;S) procedures is to identify the best
stochastic system from among a finite set of competing alternatives. Such
procedures require constructing estimates of each system's performance, which
can be obtained simultaneously by running multiple independent replications on
a parallel computing platform. However, nontrivial statistical and
implementation issues arise when designing R&amp;S procedures for a parallel
computing environment. Thus we propose several design principles for parallel
R&amp;S procedures that preserve statistical validity and maximize core
utilization, especially when large numbers of alternatives or cores are
involved. These principles are followed closely by our parallel Good Selection
Procedure (GSP), which, under the assumption of normally distributed output,
(i) guarantees to select a system in the indifference zone with high
probability, (ii) runs efficiently on up to 1,024 parallel cores, and (iii) in
an example uses smaller sample sizes compared to existing parallel procedures,
particularly for large problems (over $10^6$ alternatives). In our
computational study we discuss two methods for implementing GSP on parallel
computers, namely the Message-Passing Interface (MPI) and Hadoop MapReduce and
show that the latter provides good protection against core failures at the
expense of a significant drop in utilization due to periodic unavoidable
synchronization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04998</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04998</id><created>2015-06-16</created><authors><author><keyname>Part</keyname><forenames>Fedor</forenames></author><author><keyname>Luo</keyname><forenames>Zhaohui</forenames></author></authors><title>Semi-simplicial Types in Logic-enriched Homotopy Type Theory</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of defining Semi-Simplicial Types (SSTs) in Homotopy Type Theory
(HoTT) has been recognized as important during the Year of Univalent
Foundations at the Institute of Advanced Study. According to the interpretation
of HoTT in Quillen model categories, SSTs are type-theoretic versions of Reedy
fibrant semi-simplicial objects in a model category and simplicial and
semi-simplicial objects play a crucial role in many constructions in homotopy
theory and higher category theory. Attempts to define SSTs in HoTT lead to some
difficulties such as the need of infinitary assumptions which are beyond HoTT
with only non-strict equality types.
  Voevodsky proposed a definition of SSTs in Homotopy Type System (HTS), an
extension of HoTT with non-fibrant types, including an extensional strict
equality type. However, HTS does not have the desirable computational
properties such as decidability of type checking and strong normalization. In
this paper, we study a logic-enriched homotopy type theory, an alternative
extension of HoTT with equational logic based on the idea of logic-enriched
type theories. In contrast to Voevodskys HTS, all types in our system are
fibrant and it can be implemented in existing proof assistants. We show how
SSTs can be defined in our system and outline an implementation in the proof
assistant Plastic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.04999</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.04999</id><created>2015-06-16</created><updated>2015-06-29</updated><authors><author><keyname>Rosati</keyname><forenames>Matteo</forenames></author><author><keyname>Giovannetti</keyname><forenames>Vittorio</forenames></author></authors><title>Achieving the Holevo bound via a bisection decoding protocol</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>33 pages and 2 figures. Introduction expanded with new references and
  typos corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new decoding protocol to realize transmission of classical
information through a quantum channel at asymptotically maximum capacity,
achieving the Holevo bound and thus the optimal communication rate. At variance
with previous proposals, our scheme recover the message bit by bit, making use
of a series &quot;yes-no&quot; measurements, organized in bisection fashion, thus
determining which codeword was sent in logN steps, N being the number of
codewords.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05001</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05001</id><created>2015-06-16</created><authors><author><keyname>Presti</keyname><forenames>Liliana Lo</forenames></author><author><keyname>La Cascia</keyname><forenames>Marco</forenames></author></authors><title>Using Hankel Matrices for Dynamics-based Facial Emotion Recognition and
  Pain Detection</title><categories>cs.CV cs.AI cs.RO</categories><comments>in IEEE Proceedings of Workshop on Analysis and Modeling of Face and
  Gestures (CVPRW 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new approach to model the temporal dynamics of a
sequence of facial expressions. To this purpose, a sequence of Face Image
Descriptors (FID) is regarded as the output of a Linear Time Invariant (LTI)
system. The temporal dynamics of such sequence of descriptors are represented
by means of a Hankel matrix. The paper presents different strategies to compute
dynamics-based representation of a sequence of FID, and reports classification
accuracy values of the proposed representations within different standard
classification frameworks. The representations have been validated in two very
challenging application domains: emotion recognition and pain detection.
Experiments on two publicly available benchmarks and comparison with
state-of-the-art approaches demonstrate that the dynamics-based FID
representation attains competitive performance when off-the-shelf
classification tools are adopted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05011</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05011</id><created>2015-06-16</created><updated>2016-03-01</updated><authors><author><keyname>Karaletsos</keyname><forenames>Theofanis</forenames></author><author><keyname>Belongie</keyname><forenames>Serge</forenames></author><author><keyname>R&#xe4;tsch</keyname><forenames>Gunnar</forenames></author></authors><title>Bayesian representation learning with oracle constraints</title><categories>stat.ML cs.CV cs.LG</categories><comments>16 pages, publishes in ICLR 16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representation learning systems typically rely on massive amounts of labeled
data in order to be trained to high accuracy. Recently, high-dimensional
parametric models like neural networks have succeeded in building rich
representations using either compressive, reconstructive or supervised
criteria. However, the semantic structure inherent in observations is
oftentimes lost in the process. Human perception excels at understanding
semantics but cannot always be expressed in terms of labels. Thus,
\emph{oracles} or \emph{human-in-the-loop systems}, for example crowdsourcing,
are often employed to generate similarity constraints using an implicit
similarity function encoded in human perception. In this work we propose to
combine \emph{generative unsupervised feature learning} with a
\emph{probabilistic treatment of oracle information like triplets} in order to
transfer implicit privileged oracle knowledge into explicit nonlinear Bayesian
latent factor models of the observations. We use a fast variational algorithm
to learn the joint model and demonstrate applicability to a well-known image
dataset. We show how implicit triplet information can provide rich information
to learn representations that outperform previous metric learning approaches as
well as generative models without this side-information in a variety of
predictive tasks. In addition, we illustrate that the proposed approach
compartmentalizes the latent spaces semantically which allows interpretation of
the latent variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05012</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05012</id><created>2015-06-16</created><authors><author><keyname>Jamdar</keyname><forenames>Adit</forenames></author><author><keyname>Abraham</keyname><forenames>Jessica</forenames></author><author><keyname>Khanna</keyname><forenames>Karishma</forenames></author><author><keyname>Dubey</keyname><forenames>Rahul</forenames></author></authors><title>Emotion Analysis of Songs Based on Lyrical and Audio Features</title><categories>cs.CL cs.AI cs.SD</categories><comments>16 pages, 2 figures, 6 tables, 5 equations in International journal
  of Artificial Intelligence &amp; Applications (IJAIA)</comments><doi>10.5121/ijaia.2015.6304</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a method is proposed to detect the emotion of a song based on
its lyrical and audio features. Lyrical features are generated by segmentation
of lyrics during the process of data extraction. ANEW and WordNet knowledge is
then incorporated to compute Valence and Arousal values. In addition to this,
linguistic association rules are applied to ensure that the issue of ambiguity
is properly addressed. Audio features are used to supplement the lyrical ones
and include attributes like energy, tempo, and danceability. These features are
extracted from The Echo Nest, a widely used music intelligence platform.
Construction of training and test sets is done on the basis of social tags
extracted from the last.fm website. The classification is done by applying
feature weighting and stepwise threshold reduction on the k-Nearest Neighbors
algorithm to provide fuzziness in the classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05017</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05017</id><created>2015-06-16</created><updated>2015-06-25</updated><authors><author><keyname>Basaras</keyname><forenames>Pavlos</forenames></author><author><keyname>Maglaras</keyname><forenames>Leandros</forenames></author><author><keyname>Katsaros</keyname><forenames>Dimitrios</forenames></author><author><keyname>Janicke</keyname><forenames>Helge</forenames></author></authors><title>A Robust Eco-Routing Protocol Against Malicious Data in Vehicular
  Networks</title><categories>cs.CR</categories><comments>8 pages, 8 figures</comments><msc-class>68U20</msc-class><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular networks have a diverse range of applications that vary from
safety, to traffic management and comfort. Vehicular communications (VC) can
assist in the ecorouting of vehicles in order to reduce the overall mileage and
CO2 emissions by the exchange of data among vehicle-entities. However, the
trustworthiness of these data is crucial as false information can heavily
affect the performance of applications. Hence, the devising of mechanisms that
reassure the integrity of the exchanged data is of utmost importance. In this
article we investigate how tweaked information originating from malicious nodes
can affect the performance of a real time eco routing mechanism that uses DSRC
communications, namely ErouVe. We also develop and evaluate defense mechanisms
that exploit vehicular communications in order to filter out tweaked data. We
prove that our proposed mechanisms can restore the performance of the ErouVe to
near its optimal operation and can be used as a basis for protecting other
similar traffic management systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05018</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05018</id><created>2015-06-16</created><authors><author><keyname>Buzmakov</keyname><forenames>Aleksey</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Sergei O.</forenames></author><author><keyname>Napoli</keyname><forenames>Amedeo</forenames></author></authors><title>Revisiting Pattern Structure Projections</title><categories>cs.DM</categories><comments>16 pages, 4 figures, ICFCA-2015 conference</comments><journal-ref>Formal Concept Analysis. Springer International Publishing, 2015.
  200-215</journal-ref><doi>10.1007/978-3-319-19545-2_13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formal concept analysis (FCA) is a well-founded method for data analysis and
has many applications in data mining. Pattern structures is an extension of FCA
for dealing with complex data such as sequences or graphs. However the
computational complexity of computing with pattern structures is high and
projections of pattern structures were introduced for simplifying computation.
In this paper we introduce o-projections of pattern structures, a
generalization of projections which defines a wider class of projections
preserving the properties of the original approach. Moreover, we show that
o-projections form a semilattice and we discuss the correspondence between
o-projections and the representation contexts of o-projected pattern
structures.
  KEYWORDS: formal concept analysis, pattern structures, representation
contexts, projections
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05025</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05025</id><created>2015-06-16</created><updated>2015-11-04</updated><authors><author><keyname>Gogioso</keyname><forenames>Stefano</forenames><affiliation>Quantum Group, University of Oxford</affiliation></author></authors><title>A Bestiary of Sets and Relations</title><categories>quant-ph cs.LO math.CT math.QA</categories><comments>In Proceedings QPL 2015, arXiv:1511.01181</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 195, 2015, pp. 208-227</journal-ref><doi>10.4204/EPTCS.195.16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building on established literature and recent developments in the
graph-theoretic characterisation of its CPM category, we provide a treatment of
pure state and mixed state quantum mechanics in the category fRel of finite
sets and relations. On the way, we highlight the wealth of exotic beasts that
hide amongst the extensive operational and structural similarities that the
theory shares with more traditional arenas of categorical quantum mechanics,
such as the category fdHilb. We conclude our journey by proving that fRel is
local, but not without some unexpected twists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05028</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05028</id><created>2015-06-16</created><updated>2015-11-04</updated><authors><author><keyname>Heunen</keyname><forenames>Chris</forenames><affiliation>University of Oxford</affiliation></author><author><keyname>Tull</keyname><forenames>Sean</forenames><affiliation>University of Oxford</affiliation></author></authors><title>Categories of relations as models of quantum theory</title><categories>math.CT cs.LO quant-ph</categories><comments>In Proceedings QPL 2015, arXiv:1511.01181</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 195, 2015, pp. 247-261</journal-ref><doi>10.4204/EPTCS.195.18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Categories of relations over a regular category form a family of models of
quantum theory. Using regular logic, many properties of relations over sets
lift to these models, including the correspondence between Frobenius structures
and internal groupoids. Over compact Hausdorff spaces, this lifting gives
continuous symmetric encryption. Over a regular Mal'cev category, this
correspondence gives a characterization of categories of completely positive
maps, enabling the formulation of quantum features. These models are closer to
Hilbert spaces than relations over sets in several respects: Heisenberg
uncertainty, impossibility of broadcasting, and behavedness of rank one
morphisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05032</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05032</id><created>2015-06-16</created><updated>2015-10-23</updated><authors><author><keyname>Vu</keyname><forenames>Tiep Huu</forenames></author><author><keyname>Mousavi</keyname><forenames>Hojjat Seyed</forenames></author><author><keyname>Monga</keyname><forenames>Vishal</forenames></author><author><keyname>Rao</keyname><forenames>Arvind UK</forenames></author><author><keyname>Rao</keyname><forenames>Ganesh</forenames></author></authors><title>Histopathological Image Classification using Discriminative
  Feature-oriented Dictionary Learning</title><categories>cs.CV</categories><comments>Accepted version to Transaction on Medical Imaging, 13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In histopathological image analysis, feature extraction for classification is
a challenging task due to the diversity of histology features suitable for each
problem as well as presence of rich geometrical structures. In this paper, we
propose an automatic feature discovery framework via learning class-specific
dictionaries and present a low-complexity method for classification and disease
grading in histopathology. Essentially, our Discriminative Feature-oriented
Dictionary Learning (DFDL) method learns class-specific dictionaries such that
under a sparsity constraint, the learned dictionaries allow representing a new
image sample parsimoniously via the dictionary corresponding to the class
identity of the sample. At the same time, the dictionary is designed to be
poorly capable of representing samples from other classes. Experiments on three
challenging real-world image databases: 1) histopathological images of
intraductal breast lesions, 2) mammalian kidney, lung and spleen images
provided by the Animal Diagnostics Lab (ADL) at Pennsylvania State University,
and 3) brain tumor images from The Cancer Genome Atlas (TCGA) database, reveal
the merits of our proposal over state-of-the-art alternatives. {Moreover, we
demonstrate that DFDL exhibits a more graceful decay in classification accuracy
against the number of training images which is highly desirable in practice
where generous training is often not available
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05036</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05036</id><created>2015-06-16</created><authors><author><keyname>Yankelevsky</keyname><forenames>Yael</forenames></author><author><keyname>Shvartz</keyname><forenames>Ishai</forenames></author><author><keyname>Avraham</keyname><forenames>Tamar</forenames></author><author><keyname>Bruckstein</keyname><forenames>Alfred M.</forenames></author></authors><title>Depth Perception in Autostereograms: 1/f-Noise is Best</title><categories>cs.CV</categories><doi>10.1364/JOSAA.33.000149</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An autostereogram is a single image that encodes depth information that pops
out when looking at it. The trick is achieved by replicating a vertical strip
that sets a basic two-dimensional pattern with disparity shifts that encode a
three-dimensional scene. It is of interest to explore the dependency between
the ease of perceiving depth in autostereograms and the choice of the basic
pattern used for generating them. In this work we confirm a theory proposed by
Bruckstein et al. to explain the process of autostereographic depth perception,
providing a measure for the ease of &quot;locking into&quot; the depth profile, based on
the spectral properties of the basic pattern used. We report the results of
three sets of psychophysical experiments using autostereograms generated from
two-dimensional random noise patterns having power spectra of the form
$1/f^\beta$. The experiments were designed to test the ability of human
subjects to identify smooth, low resolution surfaces, as well as detail, in the
form of higher resolution objects in the depth profile, and to determine limits
in identifying small objects as a function of their size. In accordance with
the theory, we discover a significant advantage of the $1/f$ noise pattern
(pink noise) for fast depth lock-in and fine detail detection, showing that
such patterns are optimal choices for autostereogram design. Validating the
theoretical model predictions strengthens its underlying assumptions, and
contributes to a better understanding of the visual system's binocular
disparity mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05043</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05043</id><created>2015-06-16</created><authors><author><keyname>Avanzini</keyname><forenames>Martin</forenames></author><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames></author><author><keyname>Moser</keyname><forenames>Georg</forenames></author></authors><title>Analysing the Complexity of Functional Programs: Higher-Order Meets
  First-Order (Long Version)</title><categories>cs.LO cs.CC</categories><comments>Long version of paper presented at ICFP 2015</comments><acm-class>F.3.2</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We show how the complexity of higher-order functional programs can be
analysed automatically by applying program transformations to a
defunctionalized versions of them, and feeding the result to existing tools for
the complexity analysis of first-order term rewrite systems. This is done while
carefully analysing complexity preservation and reflection of the employed
transformations such that the complexity of the obtained term rewrite system
reflects on the complexity of the initial program. Further, we describe
suitable strategies for the application of the studied transformations and
provide ample experimental data for assessing the viability of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05055</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05055</id><created>2015-06-16</created><authors><author><keyname>Jiang</keyname><forenames>Jiuchuan</forenames></author><author><keyname>Jaeger</keyname><forenames>Manfred</forenames></author></authors><title>Numeric Input Relations for Relational Learning with Applications to
  Community Structure Analysis</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most work in the area of statistical relational learning (SRL) is focussed on
discrete data, even though a few approaches for hybrid SRL models have been
proposed that combine numerical and discrete variables. In this paper we
distinguish numerical random variables for which a probability distribution is
defined by the model from numerical input variables that are only used for
conditioning the distribution of discrete response variables. We show how
numerical input relations can very easily be used in the Relational Bayesian
Network framework, and that existing inference and learning methods need only
minor adjustments to be applied in this generalized setting. The resulting
framework provides natural relational extensions of classical probabilistic
models for categorical data. We demonstrate the usefulness of RBN models with
numeric input relations by several examples.
  In particular, we use the augmented RBN framework to define probabilistic
models for multi-relational (social) networks in which the probability of a
link between two nodes depends on numeric latent feature vectors associated
with the nodes. A generic learning procedure can be used to obtain a
maximum-likelihood fit of model parameters and latent feature values for a
variety of models that can be expressed in the high-level RBN representation.
Specifically, we propose a model that allows us to interpret learned latent
feature values as community centrality degrees by which we can identify nodes
that are central for one community, that are hubs between communities, or that
are isolated nodes. In a multi-relational setting, the model also provides a
characterization of how different relations are associated with each community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05064</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05064</id><created>2015-06-16</created><authors><author><keyname>Klav&#xed;k</keyname><forenames>Pavel</forenames></author><author><keyname>Zeman</keyname><forenames>Peter</forenames></author></authors><title>Automorphism Groups of Comparability Graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comparability graphs are graphs which have transitive orientations. The
dimension of a poset is the least number of linear orders whose intersection
gives this poset. The dimension ${\rm dim}(X)$ of a comparability graph $X$ is
the dimension of any transitive orientation of X, and by $k$-DIM we denote the
class of comparability graphs $X$ with ${\rm dim}(X) \le k$. It is known that
the complements of comparability graphs are exactly function graphs and
permutation graphs equal 2-DIM.
  In this paper, we characterize the automorphism groups of permutation graphs
similarly to Jordan's characterization for trees (1869). For permutation
graphs, there is an extra operation, so there are some extra groups not
realized by trees. For $k \ge 4$, we show that every finite group can be
realized as the automorphism group of some graph in $k$-DIM, and testing graph
isomorphism for $k$-DIM is GI-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05068</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05068</id><created>2015-06-13</created><authors><author><keyname>Fujita</keyname><forenames>Kazuhisa</forenames></author></authors><title>Extract an essential skeleton of a character as a graph from a character
  image</title><categories>cs.CV</categories><journal-ref>International Journal of Computer Science Issues 10, 5, 35-39,
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to make a graph representing an essential skeleton of a
character from an image that includes a machine printed or a handwritten
character using the growing neural gas (GNG) method and the relative
neighborhood graph (RNG) algorithm. The visual system in our brain can
recognize printed characters and handwritten characters easily, robustly, and
precisely. How can our brains robustly recognize characters? In the visual
processing in our brain, essential features of an object will be used for
recognition. The essential features are crosses, corners, junctions and so on.
These features may be useful for character recognition by a computer. However,
extraction of the features is difficult. If the skeleton of a character is
represented as a graph, the features can be more easily extracted. To extract
the skeleton of a character as a graph from a character image, we used the GNG
method and the RNG algorithm. We achieved to extract skeleton graphs from
images including distorted, noisy, and handwritten characters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05069</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05069</id><created>2015-06-15</created><authors><author><keyname>Abbas</keyname><forenames>Hosny A.</forenames></author><author><keyname>Mohamed</keyname><forenames>Ahmed M.</forenames></author></authors><title>Review on the Design of Web Based SCADA Systems Based on OPC DA Protocol</title><categories>cs.OH</categories><comments>review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most familiar SCADA (supervisory control and data acquisition)
application protocols now is OPC protocol. This interface is supported by
almost all SCADA, visualization, and process control systems. There are many
research efforts tried to design and implement an approach to access an OPC DA
server through the Internet. To achieve this goal they used diverse of modern
IT technologies like XML, Web services, Java and AJAX. In this paper, we
present a complete classification of the different approaches introduced in the
literature. A comparative study is also introduced. Finally we study the
feasibility of the realization of these approaches based on the real time
constraints imposed by the nature of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05070</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05070</id><created>2015-06-15</created><updated>2015-07-22</updated><authors><author><keyname>Chaki</keyname><forenames>Soumi</forenames></author></authors><title>Reservoir Characterization: A Machine Learning Approach</title><categories>cs.CE cs.LG</categories><comments>Supervisors: Prof. Aurobinda Routray and Prof. William K. Mohanty</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reservoir Characterization (RC) can be defined as the act of building a
reservoir model that incorporates all the characteristics of the reservoir that
are pertinent to its ability to store hydrocarbons and also to produce them.It
is a difficult problem due to non-linear and heterogeneous subsurface
properties and associated with a number of complex tasks such as data fusion,
data mining, formulation of the knowledge base, and handling of the
uncertainty.This present work describes the development of algorithms to obtain
the functional relationships between predictor seismic attributes and target
lithological properties. Seismic attributes are available over a study area
with lower vertical resolution. Conversely, well logs and lithological
properties are available only at specific well locations in a study area with
high vertical resolution.Sand fraction, which represents per unit sand volume
within the rock, has a balanced distribution between zero to unity.The thesis
addresses the issues of handling the information content mismatch between
predictor and target variables and proposes regularization of target property
prior to building a prediction model.In this thesis, two Artificial Neural
Network (ANN) based frameworks are proposed to model sand fraction from
multiple seismic attributes without and with well tops information
respectively. The performances of the frameworks are quantified in terms of
Correlation Coefficient, Root Mean Square Error, Absolute Error Mean, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05071</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05071</id><created>2015-06-15</created><authors><author><keyname>Sireesha</keyname><forenames>C.</forenames></author><author><keyname>Jyostna</keyname><forenames>G.</forenames></author><author><keyname>Varan</keyname><forenames>P. Raghu</forenames></author><author><keyname>Eswari</keyname><forenames>P. R. L.</forenames></author></authors><title>PROP - Patronage of PHP Web Applications</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PHP is one of the most commonly used languages to develop web sites because
of its simplicity, easy to learn and it can be easily embedded with any of the
databases. A web developer with his basic knowledge developing an application
without practising secure guidelines, improper validation of user inputs leads
to various source code vulnerabilities. Logical flaws while designing,
implementing and hosting the web application causes work flow deviation
attacks. In this paper, we are analyzing the complete behaviour of a web
application through static and dynamic analysis methodologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05072</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05072</id><created>2015-06-16</created><updated>2015-09-10</updated><authors><author><keyname>Gualdron</keyname><forenames>Hugo</forenames></author><author><keyname>Cordeiro</keyname><forenames>Robson</forenames></author><author><keyname>Rodrigues</keyname><forenames>Jose</forenames></author></authors><title>StructMatrix: large-scale visualization of graphs by means of structure
  detection and dense matrices</title><categories>cs.SI physics.soc-ph</categories><comments>To appear: 8 pages, paper to be published at the Fifth IEEE ICDM
  Workshop on Data Mining in Networks, 2015 as Hugo Gualdron, Robson Cordeiro,
  Jose Rodrigues (2015) StructMatrix: Large-scale visualization of graphs by
  means of structure detection and dense matrices In: The Fifth IEEE ICDM
  Workshop on Data Mining in Networks 1--8, IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a large-scale graph with millions of nodes and edges, how to reveal
macro patterns of interest, like cliques, bi-partite cores, stars, and chains?
Furthermore, how to visualize such patterns altogether getting insights from
the graph to support wise decision-making? Although there are many algorithmic
and visual techniques to analyze graphs, none of the existing approaches is
able to present the structural information of graphs at large-scale. Hence,
this paper describes StructMatrix, a methodology aimed at high-scalable visual
inspection of graph structures with the goal of revealing macro patterns of
interest. StructMatrix combines algorithmic structure detection and adjacency
matrix visualization to present cardinality, distribution, and relationship
features of the structures found in a given graph. We performed experiments in
real, large-scale graphs with up to one million nodes and millions of edges.
StructMatrix revealed that graphs of high relevance (e.g., Web, Wikipedia and
DBLP) have characterizations that reflect the nature of their corresponding
domains; our findings have not been seen in the literature so far. We expect
that our technique will bring deeper insights into large graph mining,
leveraging their use for decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05073</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05073</id><created>2015-06-15</created><authors><author><keyname>Anbalagan</keyname><forenames>Dorai Ashok Shanmugavel</forenames></author></authors><title>Secure Shell (SSH): Public Key Authentication over Hypertext Transfer
  Protocol (HTTP)</title><categories>cs.CR cs.NI</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secure Shell (SSH) protocol requires all implementations to support public
key authentication method (&quot;publickey&quot;) for authentication purposes, so web
applications which provide a SSH client over the web browser need to support
&quot;publickey&quot;. However, restrictions in Hypertext Transfer Protocol (HTTP), such
as same origin policy, and limited access to local resources, make it difficult
to perform such authentications. In this document, a system to perform
&quot;publickey&quot; authentication over HTTP is provided. It is ensured that no
compromise is made that would pose a security risk to SSH protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05078</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05078</id><created>2015-06-16</created><updated>2015-09-16</updated><authors><author><keyname>Kalil</keyname><forenames>Mohamad</forenames></author><author><keyname>Shami</keyname><forenames>Abdallah</forenames></author><author><keyname>Al-Dweik</keyname><forenames>Arafat</forenames></author></authors><title>Wireless Resources Virtualization for Cloud Radio Access Networks
  (C-RAN)</title><categories>cs.NI</categories><comments>This paper has been withdrawn by the author due to a simulation bug
  that affected some results</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides solutions for virtualizing C-RANs wireless resources and
sharing them between multiple mobile network operators (MNOs). The proposed
solutions dynamically allocate wireless resources to users who subscribe to
MNOs across the network. In addition, the proposed solutions maintain a high
level of isolation between different MNOs, provide efficient and fair resource
utilization, enable different scheduling polices, and manage intercell
interference (ICI). An optimal solution is formulated as a combinatorial
problem, which is computationally expensive. Consequently, two low-complexity
suboptimal solutions with comparable performance are provided. The optimal and
suboptimal solutions are compared in terms of complexity and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05079</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05079</id><created>2015-06-16</created><updated>2015-07-08</updated><authors><author><keyname>Uzna&#x144;ski</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>All Permutations Supersequence is coNP-complete</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that deciding whether a given input word contains as subsequence
every possible permutation of integers $\{1,2,\ldots,n\}$ is coNP-complete. The
coNP-completeness holds even when given the guarantee that the input word
contains as subsequences all of length $n-1$ sequences over the same set of
integers. We also show NP-completeness of a related problem of Partially
Non-crossing Perfect Matching in Bipartite Graphs, i.e. to find a perfect
matching in an ordered bipartite graph where edges of the matching incident to
selected vertices (even only from one side) are non-crossing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05082</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05082</id><created>2015-06-10</created><updated>2015-06-29</updated><authors><author><keyname>Casas</keyname><forenames>Noe</forenames></author></authors><title>A review of landmark articles in the field of co-evolutionary computing</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coevolution is a powerful tool in evolutionary computing that mitigates some
of its endemic problems, namely stagnation in local optima and lack of
convergence in high dimensionality problems. Since its inception in 1990, there
are multiple articles that have contributed greatly to the development and
improvement of the coevolutionary techniques. In this report we review some of
those landmark articles dwelving in the techniques they propose and how they
fit to conform robust evolutionary algorithms
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05085</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05085</id><created>2015-06-16</created><updated>2016-01-19</updated><authors><author><keyname>Pei</keyname><forenames>Wenjie</forenames></author><author><keyname>Dibeklio&#x11f;lu</keyname><forenames>Hamdi</forenames></author><author><keyname>Tax</keyname><forenames>David M. J.</forenames></author><author><keyname>van der Maaten</keyname><forenames>Laurens</forenames></author></authors><title>Time Series Classification using the Hidden-Unit Logistic Model</title><categories>cs.LG cs.CV</categories><comments>17 pages, 4 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new model for time series classification, called the hidden-unit
logistic model, that uses binary stochastic hidden units to model latent
structure in the data. The hidden units are connected in a chain structure that
models temporal dependencies in the data. Compared to the prior models for time
series classification such as the hidden conditional random field, our model
can model very complex decision boundaries because the number of latent states
grows exponentially with the number of hidden units. We demonstrate the strong
performance of our model in experiments on a variety of (computer vision)
tasks, including handwritten character recognition, speech recognition, facial
expression, and action recognition. We also present a state-of-the-art system
for facial action unit detection based on the hidden-unit logistic model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05093</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05093</id><created>2015-06-16</created><updated>2015-07-12</updated><authors><author><keyname>Ghaffari</keyname><forenames>Mohsen</forenames></author></authors><title>An Improved Distributed Algorithm for Maximal Independent Set</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Maximal Independent Set (MIS) problem is one of the basics in the study
of locality in distributed graph algorithms. This paper presents an extremely
simple randomized algorithm providing a near-optimal local complexity for this
problem, which incidentally, when combined with some recent techniques, also
leads to a near-optimal global complexity.
  Classical algorithms of Luby [STOC'85] and Alon, Babai and Itai [JALG'86]
provide the global complexity guarantee that, with high probability, all nodes
terminate after $O(\log n)$ rounds. In contrast, our initial focus is on the
local complexity, and our main contribution is to provide a very simple
algorithm guaranteeing that each particular node $v$ terminates after $O(\log
\mathsf{deg}(v)+\log 1/\epsilon)$ rounds, with probability at least
$1-\epsilon$. The guarantee holds even if the randomness outside $2$-hops
neighborhood of $v$ is determined adversarially. This degree-dependency is
optimal, due to a lower bound of Kuhn, Moscibroda, and Wattenhofer [PODC'04].
  Interestingly, this local complexity smoothly transitions to a global
complexity: by adding techniques of Barenboim, Elkin, Pettie, and Schneider
[FOCS'12, arXiv: 1202.1983v3], we get a randomized MIS algorithm with a high
probability global complexity of $O(\log \Delta) + 2^{O(\sqrt{\log \log n})}$,
where $\Delta$ denotes the maximum degree. This improves over the $O(\log^2
\Delta) + 2^{O(\sqrt{\log \log n})}$ result of Barenboim et al., and gets close
to the $\Omega(\min\{\log \Delta, \sqrt{\log n}\})$ lower bound of Kuhn et al.
  Corollaries include improved algorithms for MIS in graphs of upper-bounded
arboricity, or lower-bounded girth, for Ruling Sets, for MIS in the Local
Computation Algorithms (LCA) model, and a faster distributed algorithm for the
Lov\'asz Local Lemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05101</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05101</id><created>2015-06-15</created><authors><author><keyname>Kashyap</keyname><forenames>Hirak</forenames></author><author><keyname>Ahmed</keyname><forenames>Hasin Afzal</forenames></author><author><keyname>Hoque</keyname><forenames>Nazrul</forenames></author><author><keyname>Roy</keyname><forenames>Swarup</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Dhruba Kumar</forenames></author></authors><title>Big Data Analytics in Bioinformatics: A Machine Learning Perspective</title><categories>cs.CE cs.LG</categories><comments>20 pages survey paper on Big data analytics in Bioinformatics</comments><acm-class>A.1; C.1.4; H.2.8.a; H.2.8.b</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bioinformatics research is characterized by voluminous and incremental
datasets and complex data analytics methods. The machine learning methods used
in bioinformatics are iterative and parallel. These methods can be scaled to
handle big data using the distributed and parallel computing technologies.
  Usually big data tools perform computation in batch-mode and are not
optimized for iterative processing and high data dependency among operations.
In the recent years, parallel, incremental, and multi-view machine learning
algorithms have been proposed. Similarly, graph-based architectures and
in-memory big data tools have been developed to minimize I/O cost and optimize
iterative processing.
  However, there lack standard big data architectures and tools for many
important bioinformatics problems, such as fast construction of co-expression
and regulatory networks and salient module identification, detection of
complexes over growing protein-protein interaction data, fast analysis of
massive DNA, RNA, and protein sequence data, and fast querying on incremental
and heterogeneous disease networks. This paper addresses the issues and
challenges posed by several big data problems in bioinformatics, and gives an
overview of the state of the art and the future research opportunities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05127</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05127</id><created>2015-06-16</created><updated>2015-12-27</updated><authors><author><keyname>Neumann</keyname><forenames>Eike</forenames><affiliation>Technische Universit&#xe4;t Darmstadt</affiliation></author></authors><title>Computational Problems in Metric Fixed Point Theory and their Weihrauch
  Degrees</title><categories>math.LO cs.LO</categories><comments>44 pages</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (4:20) 2015</journal-ref><doi>10.2168/LMCS-11(4:20)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational difficulty of the problem of finding fixed points
of nonexpansive mappings in uniformly convex Banach spaces. We show that the
fixed point sets of computable nonexpansive self-maps of a nonempty, computably
weakly closed, convex and bounded subset of a computable real Hilbert space are
precisely the nonempty, co-r.e. weakly closed, convex subsets of the domain. A
uniform version of this result allows us to determine the Weihrauch degree of
the Browder-Goehde-Kirk theorem in computable real Hilbert space: it is
equivalent to a closed choice principle, which receives as input a closed,
convex and bounded set via negative information in the weak topology and
outputs a point in the set, represented in the strong topology. While in finite
dimensional uniformly convex Banach spaces, computable nonexpansive mappings
always have computable fixed points, on the unit ball in infinite-dimensional
separable Hilbert space the Browder-Goehde-Kirk theorem becomes
Weihrauch-equivalent to the limit operator, and on the Hilbert cube it is
equivalent to Weak Koenig's Lemma. In particular, computable nonexpansive
mappings may not have any computable fixed points in infinite dimension. We
also study the computational difficulty of the problem of finding rates of
convergence for a large class of fixed point iterations, which generalise both
Halpern- and Mann-iterations, and prove that the problem of finding rates of
convergence already on the unit interval is equivalent to the limit operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05143</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05143</id><created>2015-06-16</created><updated>2015-06-17</updated><authors><author><keyname>Viteri-Mera</keyname><forenames>Carlos A.</forenames></author><author><keyname>Teixeira</keyname><forenames>Fernando L.</forenames></author></authors><title>Interference-Nulling Time-Reversal Beamforming for mm-Wave Massive MIMO
  in Multi-User Frequency-Selective Indoor Channels</title><categories>cs.IT math.IT</categories><comments>25 pages, 9 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave (mm-wave) and massive MIMO have been proposed for next
generation wireless systems. However, there are many open problems for the
implementation of those technologies. In particular, beamforming is necessary
in mm-wave systems in order to counter high propagation losses. However,
conventional beamsteering is not always appropriate in rich scattering
multipath channels with frequency selective fading, such as those found in
indoor environments. In this context, time-reversal (TR) is considered a
promising beamforming technique for such mm-wave massive MIMO systems. In this
paper, we analyze a baseband TR beamforming system for mm-wave multi-user
massive MIMO. We verify that, as the number of antennas increases, TR yields
good equalization and interference mitigation properties, but inter-user
interference (IUI) remains a main impairment. Thus, we propose a novel
technique called interference-nulling TR (INTR) to minimize IUI. We evaluate
numerically the performance of INTR and compare it with conventional TR and
equalized TR beamforming. We use a 60 GHz MIMO channel model with spatial
correlation based on the IEEE 802.11ad SISO NLoS model. We demonstrate that
INTR outperforms conventional TR with respect to average BER per user and
achievable sum rate under diverse conditions, providing both diversity and
multiplexing gains simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05148</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05148</id><created>2015-06-16</created><authors><author><keyname>Georgiou</keyname><forenames>Harris V.</forenames></author></authors><title>Elements of Game Theory - Part I: Foundations, acts and mechanisms</title><categories>cs.GT</categories><comments>37 pages, 3 figures, 9 tables, 51 references. arXiv admin note: text
  overlap with arXiv:1502.02191</comments><report-no>HG/GT.0615b.01v1</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, a gentle introduction to Game Theory is presented in the form
of basic concepts and examples. Minimax and Nash's theorem are introduced as
the formal definitions for optimal strategies and equilibria in zero-sum and
nonzero-sum games. Several elements of cooperative gaming, coalitions, voting
ensembles, voting power and collective efficiency are described in brief.
Analytical (matrix) and extended (tree-graph) forms of game representation is
illustrated as the basic tools for identifying optimal strategies and
&quot;solutions&quot; in games of any kind. Next, a typology of four standard nonzero-sum
games is investigated, analyzing the Nash equilibria and the optimal strategies
in each case. Signaling, stance and third-party intermediates are described as
very important properties when analyzing strategic moves, while credibility and
reputation is described as crucial factors when signaling promises or threats.
Utility is introduced as a generalization of typical cost/gain functions and it
is used to explain the incentives of irrational players under the scope of
&quot;rational irrationality&quot;. Finally, a brief reference is presented for several
other more advanced concepts of gaming, including emergence of cooperation,
evolutionary stable strategies, two-level games, metagames, hypergames and the
Harsanyi transformation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05154</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05154</id><created>2015-06-16</created><authors><author><keyname>Batista</keyname><forenames>Andre Filipe de Moraes</forenames></author><author><keyname>Marietto</keyname><forenames>Maria das Gra&#xe7;as Bruno</forenames></author></authors><title>SNA-based reasoning for multiagent team composition</title><categories>cs.MA cs.AI cs.SI</categories><comments>10 pages</comments><journal-ref>International Journal of Artificial Intelligence &amp; Applications
  (IJAIA) Vol. 6, No. 3, May 2015</journal-ref><doi>10.5121/ijaia.2015.6305</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The social network analysis (SNA), branch of complex systems can be used in
the construction of multiagent systems. This paper proposes a study of how
social network analysis can assist in modeling multiagent systems, while
addressing similarities and differences between the two theories. We built a
prototype of multi-agent systems for resolution of tasks through the formation
of teams of agents that are formed on the basis of the social network
established between agents. Agents make use of performance indicators to assess
when should change their social network to maximize the participation in teams
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05157</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05157</id><created>2015-06-16</created><updated>2016-02-27</updated><authors><author><keyname>Schreiber</keyname><forenames>Martin</forenames></author><author><keyname>Peddle</keyname><forenames>Adam</forenames></author><author><keyname>Haut</keyname><forenames>Terry</forenames></author><author><keyname>Wingate</keyname><forenames>Beth</forenames></author></authors><title>A Decentralized Parallelization-in-Time Approach with Parareal</title><categories>cs.DC cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With steadily increasing parallelism for high-performance architectures,
simulations requiring a good strong scalability are prone to be limited in
scalability with standard spatial-decomposition strategies at a certain amount
of parallel processors. This can be a show-stopper if the simulation results
have to be computed with wallclock time restrictions (e.g.\,for weather
forecasts) or as fast as possible (e.g. for urgent computing). Here, the
time-dimension is the only one left for parallelization and we focus on
Parareal as one particular parallelization-in-time method.
  We discuss a software approach for making Parareal parallelization
transparent for application developers, hence allowing fast prototyping for
Parareal. Further, we introduce a decentralized Parareal which results in
autonomous simulation instances which only require communicating with the
previous and next simulation instances, hence with strong locality for
communication. This concept is evaluated by a prototypical solver for the
rotational shallow-water equations which we use as a representative black-box
solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05158</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05158</id><created>2015-06-16</created><authors><author><keyname>Arnold</keyname><forenames>Taylor</forenames></author></authors><title>An Entropy Maximizing Geohash for Distributed Spatiotemporal Database
  Indexing</title><categories>cs.DB cs.DC</categories><comments>12 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a modification of the standard geohash algorithm based on maximum
entropy encoding in which the data volume is approximately constant for a given
hash prefix length. Distributed spatiotemporal databases, which typically
require interleaving spatial and temporal elements into a single key, reap
large benefits from a balanced geohash by creating a consistent ratio between
spatial and temporal precision even across areas of varying data density. This
property is also useful for indexing purely spatial datasets, where the load
distribution of large range scans is an important aspect of query performance.
We apply our algorithm to data generated proportional to population as given by
census block population counts provided from the US Census Bureau.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05163</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05163</id><created>2015-06-16</created><authors><author><keyname>Henaff</keyname><forenames>Mikael</forenames></author><author><keyname>Bruna</keyname><forenames>Joan</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Deep Convolutional Networks on Graph-Structured Data</title><categories>cs.LG cs.CV cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Learning's recent successes have mostly relied on Convolutional
Networks, which exploit fundamental statistical properties of images, sounds
and video data: the local stationarity and multi-scale compositional structure,
that allows expressing long range interactions in terms of shorter, localized
interactions. However, there exist other important examples, such as text
documents or bioinformatic data, that may lack some or all of these strong
statistical regularities.
  In this paper we consider the general question of how to construct deep
architectures with small learning complexity on general non-Euclidean domains,
which are typically unknown and need to be estimated from the data. In
particular, we develop an extension of Spectral Networks which incorporates a
Graph Estimation procedure, that we test on large-scale classification
problems, matching or improving over Dropout Networks with far less parameters
to estimate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05164</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05164</id><created>2015-06-16</created><authors><author><keyname>Zhang</keyname><forenames>Jiawei</forenames></author><author><keyname>Shao</keyname><forenames>Weixiang</forenames></author><author><keyname>Wang</keyname><forenames>Senzhang</forenames></author><author><keyname>Kong</keyname><forenames>Xiangnan</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author></authors><title>Partial Network Alignment with Anchor Meta Path and Truncated Generic
  Stable Matching</title><categories>cs.SI</categories><comments>12 pages, 7 figures, short version is the invited paper of IEEE IRI
  '15</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To enjoy more social network services, users nowadays are usually involved in
multiple online social networks simultaneously. The shared users between
different networks are called anchor users, while the remaining unshared users
are named as non-anchor users. Connections between accounts of anchor users in
different networks are defined as anchor links and networks partially aligned
by anchor links can be represented as partially aligned networks. In this
paper, we want to predict anchor links between partially aligned social
networks, which is formally defined as the partial network alignment problem.
The partial network alignment problem is very difficult to solve because of the
following two challenges: (1) the lack of general features for anchor links,
and (2) the &quot;one-to-one$_\le$&quot; (one to at most one) constraint on anchor links.
To address these two challenges, a new method PNA (Partial Network Aligner) is
proposed in this paper. PNA (1) extracts a set of explicit anchor adjacency
features and latent topological features for anchor links based on the anchor
meta path concept and tensor decomposition techniques, and (2) utilizes the
generic stable matching to identify the non-anchor users to prune the redundant
anchor links attached to them. Extensive experiments conducted on two
real-world partially aligned social networks demonstrate that PNA can solve the
partial network alignment problem very well and outperform all the other
comparison methods with significant advantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05171</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05171</id><created>2015-06-16</created><updated>2016-01-21</updated><authors><author><keyname>Gibson</keyname><forenames>Travis E.</forenames></author><author><keyname>Bashan</keyname><forenames>Amir</forenames></author><author><keyname>Cao</keyname><forenames>Hong-Tai</forenames></author><author><keyname>Weiss</keyname><forenames>Scott T.</forenames></author><author><keyname>Liu</keyname><forenames>Yang-Yu</forenames></author></authors><title>On the Origins and Control of Community Types in the Human Microbiome</title><categories>q-bio.QM cs.SY</categories><comments>Main Text, Figures, Methods, Supplementary Figures, and Supplementary
  Text</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microbiome-based stratification of healthy individuals into compositional
categories, referred to as &quot;community types&quot;, holds promise for drastically
improving personalized medicine. Despite this potential, the existence of
community types and the degree of their distinctness have been highly debated.
Here we adopted a dynamic systems approach and found that heterogeneity in the
interspecific interactions or the presence of strongly interacting species is
sufficient to explain community types, independent of the topology of the
underlying ecological network. By controlling the presence or absence of these
strongly interacting species we can steer the microbial ecosystem to any
desired community type. This open-loop control strategy still holds even when
the community types are not distinct but appear as dense regions within a
continuous gradient. This finding can be used to develop viable therapeutic
strategies for shifting the microbial composition to a healthy configuration
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05172</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05172</id><created>2015-06-16</created><authors><author><keyname>Kelley</keyname><forenames>Jaimie</forenames></author><author><keyname>Stewart</keyname><forenames>Christopher</forenames></author><author><keyname>Morris</keyname><forenames>Nathaniel</forenames></author><author><keyname>Tiwari</keyname><forenames>Devesh</forenames></author><author><keyname>He</keyname><forenames>Yuxiong</forenames></author><author><keyname>Elnikety</keyname><forenames>Sameh</forenames></author></authors><title>Measuring and Managing Answer Quality for Online Data-Intensive Services</title><categories>cs.DC</categories><comments>Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online data-intensive services parallelize query execution across distributed
software components. Interactive response time is a priority, so online query
executions return answers without waiting for slow running components to
finish. However, data from these slow components could lead to better answers.
We propose Ubora, an approach to measure the effect of slow running components
on the quality of answers. Ubora randomly samples online queries and executes
them twice. The first execution elides data from slow components and provides
fast online answers; the second execution waits for all components to complete.
Ubora uses memoization to speed up mature executions by replaying network
messages exchanged between components. Our systems-level implementation works
for a wide range of platforms, including Hadoop/Yarn, Apache Lucene, the
EasyRec Recommendation Engine, and the OpenEphyra question answering system.
Ubora computes answer quality much faster than competing approaches that do not
use memoization. With Ubora, we show that answer quality can and should be used
to guide online admission control. Our adaptive controller processed 37% more
queries than a competing controller guided by the rate of timeouts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05173</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05173</id><created>2015-06-16</created><updated>2015-12-05</updated><authors><author><keyname>Paul</keyname><forenames>Saurabh</forenames></author><author><keyname>Drineas</keyname><forenames>Petros</forenames></author></authors><title>Feature Selection for Ridge Regression with Provable Guarantees</title><categories>stat.ML cs.IT cs.LG math.IT</categories><comments>To appear in Neural Computation. A shorter version of this paper
  appeared at ECML-PKDD 2014 under the title &quot;Deterministic Feature Selection
  for Regularized Least Squares Classification.&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce single-set spectral sparsification as a deterministic sampling
based feature selection technique for regularized least squares classification,
which is the classification analogue to ridge regression. The method is
unsupervised and gives worst-case guarantees of the generalization power of the
classification function after feature selection with respect to the
classification function obtained using all features. We also introduce
leverage-score sampling as an unsupervised randomized feature selection method
for ridge regression. We provide risk bounds for both single-set spectral
sparsification and leverage-score sampling on ridge regression in the fixed
design setting and show that the risk in the sampled space is comparable to the
risk in the full-feature space. We perform experiments on synthetic and
real-world datasets, namely a subset of TechTC-300 datasets, to support our
theory. Experimental results indicate that the proposed methods perform better
than the existing feature selection methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05185</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05185</id><created>2015-06-16</created><authors><author><keyname>Roguski</keyname><forenames>&#x141;ukasz</forenames></author><author><keyname>Ribeca</keyname><forenames>Paolo</forenames></author></authors><title>CARGO: Effective format-free compressed storage of genomic information</title><categories>q-bio.GN cs.CE cs.DS</categories><comments>13 (Main) + 31 (Supplementary) + 88 (Manual) pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent super-exponential growth in the amount of sequencing data
generated worldwide has put techniques for compressed storage into the focus.
Most available solutions, however, are strictly tied to specific bioinformatics
formats, sometimes inheriting from them suboptimal design choices; this hinders
flexible and effective data sharing. Here we present CARGO (Compressed
ARchiving for GenOmics), a high-level framework to automatically generate
software systems optimized for the compressed storage of arbitrary types of
large genomic data collections. Straightforward applications of our approach to
FASTQ and SAM archives require a few lines of code, produce solutions that
match and sometimes outperform specialized format-tailored compressors, and
scale well to multi-TB datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05187</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05187</id><created>2015-06-16</created><authors><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Li</keyname><forenames>Yijun</forenames></author><author><keyname>Chen</keyname><forenames>Xiaogang</forenames></author><author><keyname>Yang</keyname><forenames>Jie</forenames></author><author><keyname>Wu</keyname><forenames>Qiang</forenames></author><author><keyname>Yu</keyname><forenames>Jingyi</forenames></author></authors><title>Robust High Quality Image Guided Depth Upsampling</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Time-of-Flight (ToF) depth sensing camera is able to obtain depth maps at a
high frame rate. However, its low resolution and sensitivity to the noise are
always a concern. A popular solution is upsampling the obtained noisy low
resolution depth map with the guidance of the companion high resolution color
image. However, due to the constrains in the existing upsampling models, the
high resolution depth map obtained in such way may suffer from either texture
copy artifacts or blur of depth discontinuity. In this paper, a novel
optimization framework is proposed with the brand new data term and smoothness
term. The comprehensive experiments using both synthetic data and real data
show that the proposed method well tackles the problem of texture copy
artifacts and blur of depth discontinuity. It also demonstrates sufficient
robustness to the noise. Moreover, a data driven scheme is proposed to
adaptively estimate the parameter in the upsampling optimization framework. The
encouraging performance is maintained even in the case of large upsampling e.g.
$8\times$ and $16\times$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05193</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05193</id><created>2015-06-16</created><authors><author><keyname>Barari</keyname><forenames>Soubhik</forenames></author></authors><title>Analyzing Latent Topics in Student Confessions Communities on Facebook</title><categories>cs.SI cs.CY</categories><comments>5 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, confessions pages have grown popular on social media sites
such as Facebook and Twitter, particularly within college communities. Such
pages allow users to anonymously submit confessions related to collegiate
experience that are subsequently publicly broadcasted. Because of the anonymous
nature of disclosure, we believe that confessions pages are novel data sources
from which to discover trends and issues in a collegiate community. Aggregating
a dataset of more than 20,000 posts from one such page, we analyze natural
language characteristics of the originating community with LDA, pointwise
mutual information and sentiment analysis. Using a Markov topic model, we
examine the latent topics in our corpus and find that loneliness is a highly
regular pattern. Our findings on student confession communities support
contemporary sociological theories contextualizing student loneliness in the
framework of social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05196</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05196</id><created>2015-06-16</created><authors><author><keyname>Khan</keyname><forenames>Salman H.</forenames></author><author><keyname>Hayat</keyname><forenames>Munawar</forenames></author><author><keyname>Bennamoun</keyname><forenames>Mohammed</forenames></author><author><keyname>Togneri</keyname><forenames>Roberto</forenames></author><author><keyname>Sohel</keyname><forenames>Ferdous</forenames></author></authors><title>A Discriminative Representation of Convolutional Features for Indoor
  Scene Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Indoor scene recognition is a multi-faceted and challenging problem due to
the diverse intra-class variations and the confusing inter-class similarities.
This paper presents a novel approach which exploits rich mid-level
convolutional features to categorize indoor scenes. Traditionally used
convolutional features preserve the global spatial structure, which is a
desirable property for general object recognition. However, we argue that this
structuredness is not much helpful when we have large variations in scene
layouts, e.g., in indoor scenes. We propose to transform the structured
convolutional activations to another highly discriminative feature space. The
representation in the transformed space not only incorporates the
discriminative aspects of the target dataset, but it also encodes the features
in terms of the general object categories that are present in indoor scenes. To
this end, we introduce a new large-scale dataset of 1300 object categories
which are commonly present in indoor scenes. Our proposed approach achieves a
significant performance boost over previous state of the art approaches on five
major scene classification datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05197</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05197</id><created>2015-06-17</created><updated>2016-02-05</updated><authors><author><keyname>Li</keyname><forenames>Chang</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Success Probability and Area Spectral Efficiency in Multiuser MIMO
  HetNets</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a general and closed-form result for the success probability in
downlink multiple-antenna (MIMO) heterogeneous cellular networks (HetNets),
utilizing a novel Toeplitz matrix representation. This main result, which is
equivalently the signal-to-interference ratio (SIR) distribution, includes
multiuser MIMO, single-user MIMO and per-tier biasing for $K$ different tiers
of randomly placed base stations (BSs), assuming zero-forcing precoding and
perfect channel state information. The large SIR limit of this result admits a
simple closed form that is accurate at moderate SIRs, e.g., above 5 dB. These
results reveal that the SIR-invariance property of SISO HetNets does not hold
for MIMO HetNets; instead the success probability may decrease as the network
density increases. We prove that the maximum success probability is achieved by
activating only one tier of BSs, while the maximum area spectral efficiency
(ASE) is achieved by activating all the BSs. This reveals a unique tradeoff
between the ASE and link reliability in multiuser MIMO HetNets. To achieve the
maximum ASE while guaranteeing a certain link reliability, we develop efficient
algorithms to find the optimal BS densities. It is shown that as the link
reliability requirement increases, more BSs and more tiers should be
deactivated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05198</identifier>
 <datestamp>2015-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05198</id><created>2015-06-17</created><updated>2015-07-28</updated><authors><author><keyname>Liang</keyname><forenames>Jia Hui</forenames></author><author><keyname>Ganesh</keyname><forenames>Vijay</forenames></author><author><keyname>Raman</keyname><forenames>Venkatesh</forenames></author><author><keyname>Czarnecki</keyname><forenames>Krzysztof</forenames></author></authors><title>SAT-based Analysis of Large Real-world Feature Models is Easy</title><categories>cs.SE cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern conflict-driven clause-learning (CDCL) Boolean SAT solvers provide
efficient automatic analysis of real-world feature models (FM) of systems
ranging from cars to operating systems. It is well-known that solver-based
analysis of real-world FMs scale very well even though SAT instances obtained
from such FMs are large, and the corresponding analysis problems are known to
be NP-complete. To better understand why SAT solvers are so effective, we
systematically studied many syntactic and semantic characteristics of a
representative set of large real-world FMs. We discovered that a key reason why
large real-world FMs are easy-to-analyze is that the vast majority of the
variables in these models are unrestricted, i.e., the models are satisfiable
for both true and false assignments to such variables under the current partial
assignment. Given this discovery and our understanding of CDCL SAT solvers, we
show that solvers can easily find satisfying assignments for such models
without too many backtracks relative to the model size, explaining why solvers
scale so well. Further analysis showed that the presence of unrestricted
variables in these real-world models can be attributed to their high-degree of
variability. Additionally, we experimented with a series of well-known
non-backtracking simplifications that are particularly effective in solving
FMs. The remaining variables/clauses after simplifications, called the core,
are so few that they are easily solved even with backtracking, further
strengthening our conclusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05202</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05202</id><created>2015-06-17</created><updated>2015-11-12</updated><authors><author><keyname>Coffrin</keyname><forenames>Carleton</forenames></author><author><keyname>Hijazi</keyname><forenames>Hassan L.</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author></authors><title>Network Flow and Copper Plate Relaxations for AC Transmission Systems</title><categories>math.OC cs.SY</categories><comments>This article includes power flow background information similar to
  that in arXiv:1506.04773 and builds on the three bus example from
  arXiv:1502.07847</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonlinear convex relaxations of the power flow equations and, in particular,
the Semi-Definite Programming (SDP), Convex Quadratic (QC), and Second-Order
Cone (SOC) relaxations, have attracted significant interest in recent years.
Thus far, little attention has been given to simpler linear relaxations of the
power flow equations, which may bring significant performance gains at the cost
of model accuracy. To fill the gap, this paper develops two intuitive linear
relaxations of the power flow equations, one based on classic network flow
models (NF) and another inspired by copper plate approximations (CP).
Theoretical results show that the proposed NF model is a relaxation of the
established nonlinear SOC model and the CP model is a relaxation of the NF
model. Consequently, considering the linear NF and CP relaxations alongside the
established nonlinear relaxations (SDP, QC, SOC) provides a rich variety of
tradeoffs between the relaxation accuracy and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05203</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05203</id><created>2015-06-17</created><authors><author><keyname>Han</keyname><forenames>Myoungji</forenames></author><author><keyname>Kang</keyname><forenames>Munseong</forenames></author><author><keyname>Cho</keyname><forenames>Sukhyeun</forenames></author><author><keyname>Gu</keyname><forenames>Geonmo</forenames></author><author><keyname>Sim</keyname><forenames>Jeong Seop</forenames></author><author><keyname>Park</keyname><forenames>Kunsoo</forenames></author></authors><title>Fast Multiple Order-Preserving Matching Algorithms</title><categories>cs.DS</categories><comments>15 pages, 8 figures, submitted to IWOCA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a text $T$ and a pattern $P$, the order-preserving matching problem is
to find all substrings in $T$ which have the same relative orders as $P$.
Order-preserving matching has been an active research area since it was
introduced by Kubica et al. \cite{kubica2013linear} and Kim et al.
\cite{kim2014order}. In this paper we present two algorithms for the multiple
order-preserving matching problem, one of which runs in sublinear time on
average and the other in linear time on average. Both algorithms run much
faster than the previous algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05212</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05212</id><created>2015-06-17</created><authors><author><keyname>Roy</keyname><forenames>Subhrajit</forenames></author><author><keyname>San</keyname><forenames>Phyo Phyo</forenames></author><author><keyname>Hussain</keyname><forenames>Shaista</forenames></author><author><keyname>Wei</keyname><forenames>Lee Wang</forenames></author><author><keyname>Basu</keyname><forenames>Arindam</forenames></author></authors><title>Learning Spike time codes through Morphological Learning with Binary
  Synapses</title><categories>cs.NE</categories><comments>6 pages, 4 figures, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a neuron with nonlinear dendrites (NNLD) and binary synapses
that is able to learn temporal features of spike input patterns is considered.
Since binary synapses are considered, learning happens through formation and
elimination of connections between the inputs and the dendritic branches to
modify the structure or &quot;morphology&quot; of the NNLD. A morphological learning
algorithm inspired by the 'Tempotron', i.e., a recently proposed temporal
learning algorithm-is presented in this work. Unlike 'Tempotron', the proposed
learning rule uses a technique to automatically adapt the NNLD threshold during
training. Experimental results indicate that our NNLD with 1-bit synapses can
obtain similar accuracy as a traditional Tempotron with 4-bit synapses in
classifying single spike random latency and pair-wise synchrony patterns.
Hence, the proposed method is better suited for robust hardware implementation
in the presence of statistical variations. We also present results of applying
this rule to real life spike classification problems from the field of tactile
sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05217</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05217</id><created>2015-06-17</created><authors><author><keyname>Junaid</keyname><forenames>Mohsin</forenames></author><author><keyname>Liu</keyname><forenames>Donggang</forenames></author><author><keyname>Kung</keyname><forenames>David</forenames></author></authors><title>Dexteroid: Detecting Malicious Behaviors in Android Apps Using
  Reverse-Engineered Life Cycle Models</title><categories>cs.CR</categories><comments>36 pages, 9 figures, 9 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of Android malware has increased greatly during the last few
years. Static analysis is widely used in detecting such malware by analyzing
the code without execution. However, the effectiveness of current tools depends
on the app model as well as the malware detection algorithm that analyzes the
app model. If the model and/or the algorithm is inadequate, then sophisticated
attacks that are triggered by a specific sequence of events will not be
detected.
  This paper presents the Dexteroid framework, which is based on
reverse-engineered life cycle models that accurately capture the behaviors of
Android components. Furthermore, Dexteroid systematically derives event
sequences from the models, and uses them to detect attacks launched by specific
ordering of events. A prototype implementation of Dexteroid has been used to
conduct a series of experiments, which show that the proposed framework is
effective and efficient in terms of precision, recall, and execution time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05222</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05222</id><created>2015-06-17</created><authors><author><keyname>Di Renzo</keyname><forenames>Marco</forenames></author></authors><title>Stochastic Geometry Modeling and Performance Evaluation of mmWave
  Cellular Communications</title><categories>cs.IT math.IT</categories><comments>Presented at 2015 IEEE International Conference on Communications
  (ICC), London, UK (June 2015). arXiv admin note: substantial text overlap
  with arXiv:1410.3577</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new mathematical framework to the analysis of millimeter
wave cellular networks is introduced. Its peculiarity lies in considering
realistic path-loss and blockage models, which are derived from experimental
data recently reported in the literature. The path-loss model accounts for
different distributions for line-of-sight and non-line-of-sight propagation
conditions and the blockage model includes an outage state that provides a
better representation of the outage possibilities of millimeter wave
communications. By modeling the locations of the base stations as points of a
Poisson point process and by relying upon a noise-limited approximation for
typical millimeter wave network deployments, exact integral expressions for
computing the coverage probability and the average rate are obtained. With the
aid of Monte Carlo simulations, the noise-limited approximation is shown to be
sufficiently accurate for typical network densities. Furthermore, it is shown
that sufficiently dense millimeter wave cellular networks are capable of
outperforming micro wave cellular networks, both in terms of coverage
probability and average rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05226</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05226</id><created>2015-06-17</created><authors><author><keyname>Duan</keyname><forenames>Ruifeng</forenames></author><author><keyname>Zheng</keyname><forenames>Zhong</forenames></author><author><keyname>J&#xe4;ntti</keyname><forenames>Riku</forenames></author><author><keyname>H&#xe4;m&#xe4;l&#xe4;inen</keyname><forenames>Jyri</forenames></author></authors><title>Asymptotic Analysis of Multi-Antenna Cognitive Radio Systems Using
  Extreme Value Theory</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a spectrum-sharing cognitive radio system with antenna selection
applied at the secondary transmitter (ST). Based on the extreme value theory,
we deduce a simple and accurate expression for the asymptotic distribution of
the signal to interference plus noise ratio at the secondary receiver. Using
this result, the asymptotic mean capacity and the outage capacity for the
secondary user (SU) are derived. The obtained asymptotic capacities approach
the exact results as the number of transmit antennas $N$ increases. Results
indicate that the rate of the SU scales as $\log(N)$ when the transmit power of
the ST is limited by the maximum allowable interference level, while rate
scales as $\log(\log(N))$ if ST is limited by the maximum transmit power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05230</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05230</id><created>2015-06-17</created><authors><author><keyname>Faruqui</keyname><forenames>Manaal</forenames></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author></authors><title>Non-distributional Word Vector Representations</title><categories>cs.CL</categories><comments>Proceedings of ACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-driven representation learning for words is a technique of central
importance in NLP. While indisputably useful as a source of features in
downstream tasks, such vectors tend to consist of uninterpretable components
whose relationship to the categories of traditional lexical semantic theories
is tenuous at best. We present a method for constructing interpretable word
vectors from hand-crafted linguistic resources like WordNet, FrameNet etc.
These vectors are binary (i.e, contain only 0 and 1) and are 99.9% sparse. We
analyze their performance on state-of-the-art evaluation methods for
distributional models of word vectors and find they are competitive to standard
distributional approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05231</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05231</id><created>2015-06-17</created><updated>2016-02-25</updated><authors><author><keyname>Geiger</keyname><forenames>Bernhard C.</forenames></author></authors><title>The Fractality of Polar and Reed-Muller Codes</title><categories>cs.IT math.IT</categories><comments>9 pages, one figure</comments><doi>10.3929/ethz-a-010602015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generator matrices of polar codes and Reed-Muller codes are obtained by
selecting rows from the Kronecker product of a lower-triangular binary square
matrix. For polar codes, the selection is based on the Bhattacharyya parameter
of the row, which is closely related to the error probability of the
corresponding input bit under sequential decoding. For Reed-Muller codes, the
selection is based on the Hamming weight of the row. This work investigates the
properties of the index sets pointing to those rows in the infinite blocklength
limit. In particular, the Lebesgue measure, the Hausdorff dimension, and the
self-similarity of these sets will be discussed. It is shown that these index
sets have several properties that are common to fractals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05232</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05232</id><created>2015-06-17</created><updated>2015-11-28</updated><authors><author><keyname>Sun</keyname><forenames>Shizhao</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Liwei</forenames></author><author><keyname>Liu</keyname><forenames>Xiaoguang</forenames></author><author><keyname>Liu</keyname><forenames>Tie-Yan</forenames></author></authors><title>On the Depth of Deep Neural Networks: A Theoretical View</title><categories>cs.LG</categories><comments>AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People believe that depth plays an important role in success of deep neural
networks (DNN). However, this belief lacks solid theoretical justifications as
far as we know. We investigate role of depth from perspective of margin bound.
In margin bound, expected error is upper bounded by empirical margin error plus
Rademacher Average (RA) based capacity term. First, we derive an upper bound
for RA of DNN, and show that it increases with increasing depth. This indicates
negative impact of depth on test performance. Second, we show that deeper
networks tend to have larger representation power (measured by Betti numbers
based complexity) than shallower networks in multi-class setting, and thus can
lead to smaller empirical margin error. This implies positive impact of depth.
The combination of these two results shows that for DNN with restricted number
of hidden units, increasing depth is not always good since there is a tradeoff
between positive and negative impacts. These results inspire us to seek
alternative ways to achieve positive impact of depth, e.g., imposing
margin-based penalty terms to cross entropy loss so as to reduce empirical
margin error without increasing depth. Our experiments show that in this way,
we achieve significantly better test performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05235</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05235</id><created>2015-06-17</created><authors><author><keyname>Abbas</keyname><forenames>Hosny A.</forenames></author><author><keyname>Shaheen</keyname><forenames>Samir I.</forenames></author><author><keyname>Amin</keyname><forenames>Mohammed H.</forenames></author></authors><title>On the Adoption of Multi-Agent Systems for the Development of Industrial
  Control Networks</title><categories>cs.SY cs.MA</categories><comments>9 pages; ICAS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-Agent Systems (MAS) are adopted and tested with many complex and
critical industrial applications, which are required to be adaptive, scalable,
context-aware, and include real-time constraints. Industrial Control Networks
(ICN) are examples of these applications. An ICN is considered a system that
contains a variety of interconnected industrial equipments, such as physical
control processes, control systems, computers, and communication networks. It
is built to supervise and control industrial processes. This paper presents a
development case study on building a multi-layered agent-based ICN in which
agents cooperate to provide an effective supervision and control of a set of
control processes, basically controlled by a set of legacy control systems with
limited computing capabilities. The proposed ICN is designed to add an
intelligent layer on top of legacy control systems to compensate their limited
capabilities using a cost-effective agent-based approach, and also to provide
global synchronization and safety plans. It is tested and evaluated within a
simulation environment. The main conclusion of this research is that agents and
MAS can provide an effective, flexible, and cost-effective solution to handle
the emerged limitations of legacy control systems if they are properly
integrated with these systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05247</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05247</id><created>2015-06-17</created><updated>2015-06-19</updated><authors><author><keyname>Yang</keyname><forenames>Zhihai</forenames></author></authors><title>Defending Grey Attacks by Exploiting Wavelet Analysis in Collaborative
  Filtering Recommender Systems</title><categories>cs.IR cs.CR</categories><comments>16 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Shilling&quot; attacks or &quot;profile injection&quot; attacks have always major
challenges in collaborative filtering recommender systems (CFRSs). Many efforts
have been devoted to improve collaborative filtering techniques which can
eliminate the &quot;shilling&quot; attacks. However, most of them focused on detecting
push attack or nuke attack which is rated with the highest score or lowest
score on the target items. Few pay attention to grey attack when a target item
is rated with a lower or higher score than the average score, which shows a
more hidden rating behavior than push or nuke attack. In this paper, we present
a novel detection method to make recommender systems resistant to such attacks.
To characterize grey ratings, we exploit rating deviation of item to
discriminate between grey attack profiles and genuine profiles. In addition, we
also employ novelty and popularity of item to construct rating series. Since it
is difficult to discriminate between the rating series of attacker and genuine
users, we incorporate into discrete wavelet transform (DWT) to amplify these
differences based on the rating series of rating deviation, novelty and
popularity, respectively. Finally, we respectively extract features from rating
series of rating deviation-based, novelty-based and popularity-based by using
amplitude domain analysis method and combine all clustered results as our
detection results. We conduct a list of experiments on both the Book-Crossing
and HetRec-2011 datasets in diverse attack models. Experimental results were
included to validate the effectiveness of our approach in comparison with the
benchmarked methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05254</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05254</id><created>2015-06-17</created><updated>2016-01-05</updated><authors><author><keyname>Schulman</keyname><forenames>John</forenames></author><author><keyname>Heess</keyname><forenames>Nicolas</forenames></author><author><keyname>Weber</keyname><forenames>Theophane</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author></authors><title>Gradient Estimation Using Stochastic Computation Graphs</title><categories>cs.LG</categories><comments>Advances in Neural Information Processing Systems 28 (NIPS 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a variety of problems originating in supervised, unsupervised, and
reinforcement learning, the loss function is defined by an expectation over a
collection of random variables, which might be part of a probabilistic model or
the external world. Estimating the gradient of this loss function, using
samples, lies at the core of gradient-based learning algorithms for these
problems. We introduce the formalism of stochastic computation
graphs---directed acyclic graphs that include both deterministic functions and
conditional probability distributions---and describe how to easily and
automatically derive an unbiased estimator of the loss function's gradient. The
resulting algorithm for computing the gradient estimator is a simple
modification of the standard backpropagation algorithm. The generic scheme we
propose unifies estimators derived in variety of prior work, along with
variance-reduction techniques therein. It could assist researchers in
developing intricate models involving a combination of stochastic and
deterministic operations, enabling, for example, attention, memory, and control
actions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05255</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05255</id><created>2015-06-17</created><authors><author><keyname>Karowski</keyname><forenames>Niels</forenames></author><author><keyname>Miller</keyname><forenames>Konstantin</forenames></author></authors><title>Optimized Asynchronous Passive Multi-Channel Discovery of Beacon-Enabled
  Networks</title><categories>cs.NI</categories><report-no>TKN-15-002</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neighbor discovery is a fundamental task for wireless networks deployment. It
is essential for setup and maintenance of networks and is typically a
precondition for further communication. In this work we focus on passive
discovery of networks operating in multi-channel environments, performed by
listening for periodically transmitted beaconing messages. It is well-known
that performance of such discovery approaches strongly depends on the structure
of the adopted Beacon Interval (BI) set, that is, set of intervals between
individual beaconing messages. However, although imposing constraints on this
set has the potential to make the discovery process more efficient, there is
demand for high-performance discovery strategies for BI sets that are as
general as possible. They would allow to cover a broad range of wireless
technologies and deployment scenarios, and enable network operators to select
BI's that are best suited for the targeted application and/or device
characteristics. In the present work, we introduce a family of novel
low-complexity discovery algorithms that minimize both the Expected Mean
Discovery Time (EMDT) and the makespan, for a quite general family of BI sets.
Notably, this family of BI sets completely includes BI's supported by IEEE
802.15.4 and a large part of BI's supported by IEEE 802.11. Furthermore, we
present another novel discovery algorithm, based on an Integer Linear Program
(ILP), that minimizes EMDT for arbitrary BI sets. In addition to analytically
proving optimality, we numerically evaluate the proposed algorithms using
different families of BI sets and compare their performance with the passive
scan of IEEE 802.15.4 w.r.t. various performance metrics, such as makespan,
EMDT, energy usage, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05257</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05257</id><created>2015-06-17</created><authors><author><keyname>Mankowitz</keyname><forenames>Daniel J.</forenames></author><author><keyname>Rivlin</keyname><forenames>Ehud</forenames></author></authors><title>CFORB: Circular FREAK-ORB Visual Odometry</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel Visual Odometry algorithm entitled Circular FREAK-ORB
(CFORB). This algorithm detects features using the well-known ORB algorithm
[12] and computes feature descriptors using the FREAK algorithm [14]. CFORB is
invariant to both rotation and scale changes, and is suitable for use in
environments with uneven terrain. Two visual geometric constraints have been
utilized in order to remove invalid feature descriptor matches. These
constraints have not previously been utilized in a Visual Odometry algorithm. A
variation to circular matching [16] has also been implemented. This allows
features to be matched between images without having to be dependent upon the
epipolar constraint. This algorithm has been run on the KITTI benchmark dataset
and achieves a competitive average translational error of $3.73 \%$ and average
rotational error of $0.0107 deg/m$. CFORB has also been run in an indoor
environment and achieved an average translational error of $3.70 \%$. After
running CFORB in a highly textured environment with an approximately uniform
feature spread across the images, the algorithm achieves an average
translational error of $2.4 \%$ and an average rotational error of $0.009
deg/m$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05261</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05261</id><created>2015-06-17</created><authors><author><keyname>Wang</keyname><forenames>Shiqiang</forenames></author><author><keyname>Urgaonkar</keyname><forenames>Rahul</forenames></author><author><keyname>Zafer</keyname><forenames>Murtaza</forenames></author><author><keyname>He</keyname><forenames>Ting</forenames></author><author><keyname>Chan</keyname><forenames>Kevin</forenames></author><author><keyname>Leung</keyname><forenames>Kin K.</forenames></author></authors><title>Dynamic Service Migration in Mobile Edge-Clouds</title><categories>cs.DC cs.NI math.OC</categories><comments>in Proc. of IFIP Networking 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the dynamic service migration problem in mobile edge-clouds that
host cloud-based services at the network edge. This offers the benefits of
reduction in network overhead and latency but requires service migrations as
user locations change over time. It is challenging to make these decisions in
an optimal manner because of the uncertainty in node mobility as well as
possible non-linearity of the migration and transmission costs. In this paper,
we formulate a sequential decision making problem for service migration using
the framework of Markov Decision Process (MDP). Our formulation captures
general cost models and provides a mathematical framework to design optimal
service migration policies. In order to overcome the complexity associated with
computing the optimal policy, we approximate the underlying state space by the
distance between the user and service locations. We show that the resulting MDP
is exact for uniform one-dimensional mobility while it provides a close
approximation for uniform two-dimensional mobility with a constant additive
error term. We also propose a new algorithm and a numerical technique for
computing the optimal solution which is significantly faster in computation
than traditional methods based on value or policy iteration. We illustrate the
effectiveness of our approach by simulation using real-world mobility traces of
taxis in San Francisco.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05267</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05267</id><created>2015-06-17</created><authors><author><keyname>Tanaskovic</keyname><forenames>Marko</forenames></author><author><keyname>Fagiano</keyname><forenames>Lorenzo</forenames></author><author><keyname>Novara</keyname><forenames>Carlo</forenames></author><author><keyname>Morari</keyname><forenames>Manfred</forenames></author></authors><title>On-line direct data driven controller design approach with automatic
  update for some of the tuning parameters</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manuscript contains technical details of recent results developed by the
authors on the algorithm for direct design of controllers for nonlinear systems
from data that has the ability to to automatically modify some of the tuning
parameters in order to increase control performance over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05268</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05268</id><created>2015-06-17</created><authors><author><keyname>Wu</keyname><forenames>Zhenzhou</forenames></author><author><keyname>Takaki</keyname><forenames>Shinji</forenames></author><author><keyname>Yamagishi</keyname><forenames>Junichi</forenames></author></authors><title>Deep Denoising Auto-encoder for Statistical Speech Synthesis</title><categories>cs.SD cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a deep denoising auto-encoder technique to extract better
acoustic features for speech synthesis. The technique allows us to
automatically extract low-dimensional features from high dimensional spectral
features in a non-linear, data-driven, unsupervised way. We compared the new
stochastic feature extractor with conventional mel-cepstral analysis in
analysis-by-synthesis and text-to-speech experiments. Our results confirm that
the proposed method increases the quality of synthetic speech in both
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05270</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05270</id><created>2015-06-17</created><authors><author><keyname>Biboudis</keyname><forenames>Aggelos</forenames></author><author><keyname>Fourtounis</keyname><forenames>George</forenames></author><author><keyname>Smaragdakis</keyname><forenames>Yannis</forenames></author></authors><title>jUCM: Universal Class Morphing (position paper)</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend prior work on class-morphing to provide a more expressive
pattern-based compile-time reflection language. Our MorphJ language offers a
disciplined form of metaprogramming that produces types by statically iterating
over and pattern-matching on fields and methods of other types. We expand such
capabilities with &quot;universal morphing&quot;, which also allows pattern-matching over
types (e.g., all classes nested in another, all supertypes of a class) while
maintaining modular type safety for our meta-programs. We present informal
examples of the functionality and discuss a design for adding universal
morphing to Java.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05272</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05272</id><created>2015-06-17</created><authors><author><keyname>Groen</keyname><forenames>Derek</forenames></author><author><keyname>Guo</keyname><forenames>Xiaohu</forenames></author><author><keyname>Grogan</keyname><forenames>James A.</forenames></author><author><keyname>Schiller</keyname><forenames>Ulf D.</forenames></author><author><keyname>Osborne</keyname><forenames>James M.</forenames></author></authors><title>Software development practices in academia: a case study comparison</title><categories>cs.SE physics.comp-ph</categories><comments>22 pages, 3 figures, 1 table, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Academic software development practices often differ from those of commercial
development settings, yet only limited research has been conducted on assessing
software development practises in academia. Here we present a case study of
software development practices in four open-source scientific codes over a
period of nine years, characterizing the evolution of their respective
development teams, their scientific productivity, and the adoption (or
discontinuation) of specific software engineering practises as the team size
changes. We show that the transient nature of the development team results in
the adoption of different development strategies. We relate measures of
publication output to accumulated numbers of developers and find that for the
projects considered the time-scale for returns on expended development effort
is approximately three years. We discuss the implications of our findings for
evaluating the performance of research software development, and in general any
computationally oriented scientific project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05274</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05274</id><created>2015-06-17</created><updated>2015-12-22</updated><authors><author><keyname>Rodol&#xe0;</keyname><forenames>Emanuele</forenames></author><author><keyname>Cosmo</keyname><forenames>Luca</forenames></author><author><keyname>Bronstein</keyname><forenames>Michael M.</forenames></author><author><keyname>Torsello</keyname><forenames>Andrea</forenames></author><author><keyname>Cremers</keyname><forenames>Daniel</forenames></author></authors><title>Partial Functional Correspondence</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a method for computing partial functional
correspondence between non-rigid shapes. We use perturbation analysis to show
how removal of shape parts changes the Laplace-Beltrami eigenfunctions, and
exploit it as a prior on the spectral representation of the correspondence.
Corresponding parts are optimization variables in our problem and are used to
weight the functional correspondence; we are looking for the largest and most
regular (in the Mumford-Shah sense) parts that minimize correspondence
distortion. We show that our approach can cope with very challenging
correspondence settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05279</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05279</id><created>2015-06-17</created><updated>2015-08-03</updated><authors><author><keyname>Czerwinski</keyname><forenames>Wojciech</forenames></author><author><keyname>Gogacz</keyname><forenames>Tomasz</forenames></author><author><keyname>Kopczynski</keyname><forenames>Eryk</forenames></author></authors><title>Non-dominating sequences of vectors using only resets and increments</title><categories>cs.DM</categories><doi>10.3233/FI-2015-1247</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a variant of Dickson lemma, where each entry of a vector can be
reseted or incremented by 1 in respect to the previous one. We give an example
of non dominating sequence of length $2^{2^{\theta (n)}}$. It perfectly match
the previously known upperbound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05282</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05282</id><created>2015-06-17</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author></authors><title>Why Bother With Syntax?</title><categories>cs.AI cs.LO</categories><comments>To appear in &quot;Rohit Parikh on Logic, Language and Society&quot; (C.
  Baskent, L. Moss, and R. Ramanjum, editors)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short note discusses the role of syntax vs. semantics and the interplay
between logic, philosophy, and language in computer science and game theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05285</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05285</id><created>2015-06-17</created><authors><author><keyname>Rauzy</keyname><forenames>Pablo</forenames></author><author><keyname>Guilley</keyname><forenames>Sylvain</forenames></author><author><keyname>Najm</keyname><forenames>Zakaria</forenames></author></authors><title>Formally Proved Security of Assembly Code Against Power Analysis: A Case
  Study on Balanced Logic</title><categories>cs.CR</categories><comments>Journal of Cryptographic Engineering, Springer, 2015</comments><proxy>ccsd</proxy><doi>10.1007/s13389-015-0105-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In his keynote speech at CHES 2004, Kocher advocated that side-channel
attacks were an illustration that formal cryptography was not as secure as it
was believed because some assumptions (e.g., no auxiliary information is
available during the computation) were not modeled. This failure is caused by
formal methods' focus on models rather than implementations. In this paper we
present formal methods and tools for designing protected code and proving its
security against power analysis. These formal methods avoid the discrepancy
between the model and the implementation by working on the latter rather than
on a high-level model. Indeed, our methods allow us (a) to automatically insert
a power balancing countermeasure directly at the assembly level, and to prove
the correctness of the induced code transformation; and (b) to prove that the
obtained code is balanced with regard to a reasonable leakage model. We also
show how to characterize the hardware to use the resources which maximize the
relevancy of the model. The tools implementing our methods are then
demonstrated in a case study on an 8-bit AVR smartcard for which we generate a
provably protected present implementation that reveals to be at least 250 times
more resistant to CPA attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05297</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05297</id><created>2015-06-17</created><updated>2016-02-03</updated><authors><author><keyname>Boskos</keyname><forenames>Dimitris</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author></authors><title>Decentralized Abstractions For Multi-Agent Systems Under Coupled
  Constraints</title><categories>cs.SY</categories><comments>33 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this report is to define abstractions for multi-agent systems
with feedback interconnection in their dynamics. In the proposed decentralized
framework, we specify a finite or countable transition system for each agent
which only takes into account the discrete positions of its neighbors. The
dynamics of the considered systems consist of two components. An appropriate
feedback law which guarantees that certain system and network requirements are
fulfilled and induces coupled constraints, and additional free inputs which we
exploit in order to accomplish high level tasks. In this work, we provide
sufficient conditions on the space and time discretization for the abstraction
of the system's behaviour which ensure that we can extract a well posed and
hence meaningful transition system. Furthermore, these conditions include
design parameters whose tuning provides the possibility for multiple
transitions, and hence, enable the construction of transition systems with
motion planning capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05312</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05312</id><created>2015-06-17</created><authors><author><keyname>Waliszko</keyname><forenames>Jaros&#x142;aw</forenames></author></authors><title>Knowledge representation and processing methods in Semantic Web</title><categories>cs.SE cs.LO</categories><comments>Master's thesis at AGH UST, Krakow 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal is to take a closer look at progress of knowledge engineering in the
field of Semantic Web. Along with theory of Knowledge Representation (KR) and
knowledge processing methods such as Description Logic (DL), reasoning
mechanisms and ontology modelling languages (OWL, RDF, RDFS), the thesis shows
the practical usage of the mentioned approaches in building systems driven by
ontologies. A working prototype of ontology-driven application, written in
Java, has been developed within the scope of this thesis. The system main
assumption is an attempt to integrate database and ontology approach, for
storing and inferring desired information about domain of traffic dangers. For
the needs of the system, domain model of traffic danger concept has been also
designed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05313</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05313</id><created>2015-06-17</created><updated>2015-06-22</updated><authors><author><keyname>Benerjee</keyname><forenames>Krishna Gopal</forenames></author><author><keyname>Gupta</keyname><forenames>Manish K.</forenames></author></authors><title>On Dress Codes with Flowers</title><categories>cs.IT math.IT</categories><comments>7 pages, updated draft, submitted to conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fractional Repetition (FR) codes are well known class of Distributed
Replication-based Simple Storage (Dress) codes for the Distributed Storage
Systems (DSSs). In such systems, the replicas of data packets encoded by
Maximum Distance Separable (MDS) code, are stored on distributed nodes. Most of
the available constructions for the FR codes are based on combinatorial designs
and Graph theory. In this work, we propose an elegant sequence based approach
for the construction of the FR code. In particular, we propose a beautiful
class of codes known as Flower codes and study its basic properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05323</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05323</id><created>2015-06-17</created><authors><author><keyname>Imam</keyname><forenames>Neena</forenames></author><author><keyname>Brim</keyname><forenames>Michael</forenames></author><author><keyname>Oral</keyname><forenames>Sarp</forenames></author></authors><title>Proceedings of the 2015 International Workshop on the Lustre Ecosystem:
  Challenges and Opportunities</title><categories>cs.DC</categories><comments>International Workshop on the Lustre Ecosystem: Challenges and
  Opportunities, March 2015, Annapolis MD</comments><proxy>Michael Brim</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Lustre parallel file system has been widely adopted by high-performance
computing (HPC) centers as an effective system for managing large-scale storage
resources. Lustre achieves unprecedented aggregate performance by parallelizing
I/O over file system clients and storage targets at extreme scales. Today, 7
out of 10 fastest supercomputers in the world use Lustre for high-performance
storage. To date, Lustre development has focused on improving the performance
and scalability of large-scale scientific workloads. In particular, large-scale
checkpoint storage and retrieval, which is characterized by bursty I/O from
coordinated parallel clients, has been the primary driver of Lustre development
over the last decade. With the advent of extreme scale computing and Big Data
computing, many HPC centers are seeing increased user interest in running
diverse workloads that place new demands on Lustre. In March 2015, the
International Workshop on the Lustre Ecosystem: Challenges and Opportunities
was held in Annapolis, Maryland at the Historic Inns of Annapolis Governor
Calvert House. This workshop series is intended to help explore improvements in
the performance and flexibility of Lustre for supporting diverse application
workloads. The 2015 workshop was the inaugural edition, and the goal was to
initiate a discussion on the open challenges associated with enhancing Lustre
for diverse applications, the technological advances necessary, and the
associated impacts to the Lustre ecosystem. The workshop program featured a day
of tutorials and a day of technical paper presentations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05324</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05324</id><created>2015-06-17</created><authors><author><keyname>Determe</keyname><forenames>J. F.</forenames></author><author><keyname>Louveaux</keyname><forenames>J.</forenames></author><author><keyname>Jacques</keyname><forenames>L.</forenames></author><author><keyname>Horlin</keyname><forenames>F.</forenames></author></authors><title>Simultaneous Orthogonal Matching Pursuit With Noise Stabilization:
  Theoretical Analysis</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the joint support recovery of similar sparse vectors on
the basis of a limited number of noisy linear measurements, i.e., in a multiple
measurement vector (MMV) model. The additive noise signals on each measurement
vector are assumed to be Gaussian and to exhibit different variances. The
simultaneous orthogonal matching pursuit (SOMP) algorithm is generalized to
weight the impact of each measurement vector on the choice of the atoms to be
picked according to their noise levels. The new algorithm is referred to as
SOMP-NS where NS stands for noise stabilization. To begin with, a theoretical
framework to analyze the performance of the proposed algorithm is developed.
This framework is then used to build conservative lower bounds on the
probability of partial or full joint support recovery. Numerical simulations
show that the proposed algorithm outperforms SOMP and that the theoretical
lower bound provides a great insight into how SOMP-NS behaves when the
weighting strategy is modified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05348</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05348</id><created>2015-06-16</created><updated>2015-09-18</updated><authors><author><keyname>Marshall</keyname><forenames>J. S.</forenames></author><author><keyname>Thomson</keyname><forenames>M. A.</forenames></author></authors><title>The Pandora Software Development Kit for Pattern Recognition</title><categories>physics.data-an cs.DC hep-ex physics.ins-det</categories><comments>Accepted by European Physical Journal C, 4 September 2015</comments><doi>10.1140/epjc/s10052-015-3659-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of automated solutions to pattern recognition problems is
important in many areas of scientific research and human endeavour. This paper
describes the implementation of the Pandora Software Development Kit, which
aids the process of designing, implementing and running pattern recognition
algorithms. The Pandora Application Programming Interfaces ensure simple
specification of the building-blocks defining a pattern recognition problem.
The logic required to solve the problem is implemented in algorithms. The
algorithms request operations to create or modify data structures and the
operations are performed by the Pandora framework. This design promotes an
approach using many decoupled algorithms, each addressing specific topologies.
Details of algorithms addressing two pattern recognition problems in High
Energy Physics are presented: reconstruction of events at a high-energy e+e-
linear collider and reconstruction of cosmic ray or neutrino events in a liquid
argon time projection chamber.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05367</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05367</id><created>2015-06-17</created><authors><author><keyname>Marzi</keyname><forenames>Zhinus</forenames></author><author><keyname>Ramasamy</keyname><forenames>Dinesh</forenames></author><author><keyname>Madhow</keyname><forenames>Upamanyu</forenames></author></authors><title>Compressive channel estimation and tracking for large arrays in mm wave
  picocells</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and investigate a compressive architecture for estimation and
tracking of sparse spatial channels in millimeter (mm) wave picocellular
networks. The base stations are equipped with antenna arrays with a large
number of elements (which can fit within compact form factors because of the
small carrier wavelength) and employ radio frequency (RF) beamforming, so that
standard least squares adaptation techniques (which require access to
individual antenna elements) are not applicable. We focus on the downlink, and
show that &quot;compressive beacons,&quot; transmitted using pseudorandom phase settings
at the base station array, and compressively processed using pseudorandom phase
settings at the mobile array, provide information sufficient for accurate
estimation of the two-dimensional (2D) spatial frequencies associated with the
directions of departure of the dominant rays from the base station, and the
associated complex gains. This compressive approach is compatible with coarse
phase-only control, and is based on a near-optimal sequential algorithm for
frequency estimation which can exploit the geometric continuity of the channel
across successive beaconing intervals to reduce the overhead to less than 1%
even for very large (32 x 32) arrays. Compressive beaconing is essentially
omnidirectional, and hence does not enjoy the SNR and spatial reuse benefits of
beamforming obtained during data transmission. We therefore discuss system
level design considerations for ensuring that the beacon SNR is sufficient for
accurate channel estimation, and that inter-cell beacon interference is
controlled by an appropriate reuse scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05374</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05374</id><created>2015-06-17</created><updated>2015-06-22</updated><authors><author><keyname>Wen</keyname><forenames>Fei</forenames></author><author><keyname>Yang</keyname><forenames>Yuan</forenames></author><author><keyname>Liu</keyname><forenames>Peilin</forenames></author><author><keyname>Ying</keyname><forenames>Rendong</forenames></author><author><keyname>Liu</keyname><forenames>Yipeng</forenames></author></authors><title>Efficient $\ell_q$ Minimization Algorithms for Compressive Sensing Based
  on Proximity Operator</title><categories>cs.IT math.IT</categories><comments>18 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers solving the unconstrained $\ell_q$-norm ($0\leq q&lt;1$)
regularized least squares ($\ell_q$-LS) problem for recovering sparse signals
in compressive sensing. We propose two highly efficient first-order algorithms
via incorporating the proximity operator for nonconvex $\ell_q$-norm functions
into the fast iterative shrinkage/thresholding (FISTA) and the alternative
direction method of multipliers (ADMM) frameworks, respectively. Furthermore,
in solving the nonconvex $\ell_q$-LS problem, a sequential minimization
strategy is adopted in the new algorithms to gain better global convergence
performance. Unlike most existing $\ell_q$-minimization algorithms, the new
algorithms solve the $\ell_q$-minimization problem without smoothing
(approximating) the $\ell_q$-norm. Meanwhile, the new algorithms scale well for
large-scale problems, as often encountered in image processing. We show that
the proposed algorithms are the fastest methods in solving the nonconvex
$\ell_q$-minimization problem, while offering competent performance in
recovering sparse signals and compressible images compared with several
state-of-the-art algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05375</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05375</id><created>2015-06-17</created><authors><author><keyname>Ausloos</keyname><forenames>Marcel</forenames></author></authors><title>Coherent measures of the impact of co-authors in peer review journals
  and in proceedings publications</title><categories>physics.soc-ph cs.DL nlin.AO</categories><comments>22 pages; 2 Tables; 6 Figures; 38 references; prepared for Physica A</comments><journal-ref>Physica A (2015) 568-578</journal-ref><doi>10.1016/j.physa.2015.06.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the coauthor effect in different types of publications,
usually not equally respected in measuring research impact. {\it A priori}
unexpected relationships are found between the total coauthor core value,
$m_a$, of a leading investigator (LI), and the related values for their
publications in either peer review journals ($j$) or in proceedings ($p$). A
surprisingly linear relationship is found: $ m_a^{(j)} + 0.4\;m_a^{(p)} =
m_a^{(jp)} $. Furthermore, another relationship is found concerning the measure
of the total number of citations, $A_a$, i.e. the surface of the citation
size-rank histogram up to $m_a$. Another linear relationship exists :
$A_a^{(j)} + 1.36\; A_a^{(p)} = A_a^{(jp)} $. These empirical findings
coefficients (0.4 and 1.36) are supported by considerations based on an
empirical power law found between the number of joint publications of an author
and the rank of a coauthor. Moreover, a simple power law relationship is found
between $m_a$ and the number ($r_M$) of coauthors of a LI: $m_a\simeq
r_M^{\mu}$; the power law exponent $\mu$ depends on the type ($j$ or $p$) of
publications. These simple relations, at this time limited to publications in
physics, imply that coauthors are a &quot;more positive measure&quot; of a principal
investigator role, in both types of scientific outputs, than the Hirsch index
could indicate. Therefore, to scorn upon co-authors in publications, in
particular in proceedings, is incorrect. On the contrary, the findings suggest
an immediate test of coherence of scientific authorship in scientific policy
processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05382</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05382</id><created>2015-06-17</created><updated>2016-01-29</updated><authors><author><keyname>Lash</keyname><forenames>Michael T.</forenames></author><author><keyname>Zhao</keyname><forenames>Kang</forenames></author></authors><title>Early Predictions of Movie Success: the Who, What, and When of
  Profitability</title><categories>cs.AI cs.SI</categories><msc-class>68U35</msc-class><acm-class>H.4.2; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a decision support system to aid movie investment
decisions at the early stage of movie productions. The system predicts the
success of a movie based on its profitability by leveraging historical data
from various sources. Using social network analysis and text mining techniques,
the system automatically extracts several groups of features, including &quot;who&quot;
are on the cast, &quot;what&quot; a movie is about, &quot;when&quot; a movie will be released, as
well as &quot;hybrid&quot; features that match &quot;who&quot; with &quot;what&quot;, and &quot;when&quot; with &quot;what&quot;.
Experiment results with movies during an 11-year period showed that the system
outperforms benchmark methods by a large margin in predicting movie
profitability. Novel features we proposed also made great contributions to the
prediction. In addition to designing a decision support system with practical
utilities, our analysis of key factors for movie profitability may also have
implications for theoretical research on team performance and the success of
creative work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05393</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05393</id><created>2015-06-17</created><authors><author><keyname>Wang</keyname><forenames>Ze</forenames></author></authors><title>MRF-ZOOM: A Fast Dictionary Searching Algorithm for Magnetic Resonance
  Fingerprinting</title><categories>cs.DS cs.CV</categories><comments>7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Magnetic resonance fingerprinting (MRF) is a new technique for simultaneously
quantifying multiple MR parameters using one temporally resolved MR scan. But
its brute-force dictionary generating and searching (DGS) process causes a huge
disk space demand and computational burden, prohibiting it from a practical
multiple slice high-definition imaging. The purpose of this paper was to
provide a fast and space efficient DGS algorithm for MRF. Based on an empirical
analysis of properties of the distance function of the acquired MRF signal and
the pre-defined MRF dictionary entries, we proposed a parameter separable MRF
DGS method, which breaks the multiplicative computation complexity into an
additive one and enabling a resolution scalable multi-resolution DGS process,
which was dubbed as MRF ZOOM. The evaluation results showed that MRF ZOOM was
hundreds or thousands of times faster than the original brute-force DGS method.
The acceleration was even higher when considering the time difference for
generating the dictionary. Using a high precision quantification, MRF can find
the right parameter values for a 64x64 imaging slice in 117 secs. Our data also
showed that spatial constraints can be used to further speed up MRF ZOOM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05399</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05399</id><created>2015-06-17</created><authors><author><keyname>Vrettos</keyname><forenames>Evangelos</forenames></author><author><keyname>Oldewurtel</keyname><forenames>Frauke</forenames></author><author><keyname>Andersson</keyname><forenames>G&#xf6;ran</forenames></author></authors><title>Robust Energy-Constrained Frequency Reserves from Aggregations of
  Commercial Buildings</title><categories>cs.SY math.OC</categories><comments>29 pages, 8 figures, 3 tables, submitted to the IEEE Transactions on
  Power Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been shown that the heating, ventilation, and air conditioning (HVAC)
systems of commercial buildings can offer ancillary services to power systems
without loss of comfort. In this paper, we propose a new control framework for
reliable scheduling and provision of frequency reserves by aggregations of
commercial buildings. The framework incorporates energy-constrained frequency
signals, which are adopted by several transmission system operators for loads
and storage devices. We use a hierarchical approach with three levels: (i)
reserve capacities are allocated among buildings (e.g., on a daily basis) using
techniques from robust optimization, (ii) a robust model predictive controller
optimizes the HVAC system consumption typically every 30 minutes, and (iii) a
feedback controller adjusts the consumption to provide reserves in real time.
We demonstrate how the framework can be used to estimate the reserve capacities
in simulations with typical Swiss office buildings and different reserve
product characteristics. Our results show that an aggregation of approximately
100 buildings suffices to meet the 5 MW minimum bid size of the Swiss reserve
market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05402</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05402</id><created>2015-06-17</created><authors><author><keyname>Atanassova</keyname><forenames>Iana</forenames></author><author><keyname>Bertin</keyname><forenames>Marc</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author></authors><title>Editorial for the First Workshop on Mining Scientific Papers:
  Computational Linguistics and Bibliometrics</title><categories>cs.CL cs.DL cs.IR</categories><comments>4 pages, Workshop on Mining Scientific Papers: Computational
  Linguistics and Bibliometrics at ISSI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The workshop &quot;Mining Scientific Papers: Computational Linguistics and
Bibliometrics&quot; (CLBib 2015), co-located with the 15th International Society of
Scientometrics and Informetrics Conference (ISSI 2015), brought together
researchers in Bibliometrics and Computational Linguistics in order to study
the ways Bibliometrics can benefit from large-scale text analytics and sense
mining of scientific papers, thus exploring the interdisciplinarity of
Bibliometrics and Natural Language Processing (NLP). The goals of the workshop
were to answer questions like: How can we enhance author network analysis and
Bibliometrics using data obtained by text analytics? What insights can NLP
provide on the structure of scientific writing, on citation networks, and on
in-text citation analysis? This workshop is the first step to foster the
reflection on the interdisciplinarity and the benefits that the two disciplines
Bibliometrics and Natural Language Processing can drive from it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05404</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05404</id><created>2015-06-17</created><authors><author><keyname>Ujum</keyname><forenames>Ephrance Abu</forenames></author><author><keyname>Prathap</keyname><forenames>Gangan</forenames></author><author><keyname>Ratnavelu</keyname><forenames>Kuru</forenames></author></authors><title>Best of both worlds? Simultaneous evaluation of researchers and their
  works</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>16 pages, 2 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores a dual score system that simultaneously evaluates the
relative importance of researchers and their works. It is a modification of the
CITEX algorithm recently described in Pal and Ruj (2015). Using available
publication data for $m$ author keywords (as a proxy for researchers) and $n$
papers it is possible to construct a $m \times n$ author-paper feature matrix.
This is further combined with citation data to construct a HITS-like algorithm
that iteratively satisfies two criteria: first, \emph{a good author is cited by
good authors}, and second, \emph{a good paper is cited by good authors}.
Following Pal and Ruj, the resulting algorithm produces an author eigenscore
and a paper eigenscore. The algorithm is tested on 213,530 citable publications
listed under Thomson ISI's &quot;\emph{Information Science \&amp; Library Science}&quot; JCR
category from 1980--2012.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05410</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05410</id><created>2015-06-15</created><authors><author><keyname>Frey</keyname><forenames>Seth</forenames></author><author><keyname>Goldstone</keyname><forenames>Robert L.</forenames></author></authors><title>Flocking in the depths of strategic iterated reasoning</title><categories>physics.soc-ph cs.SI</categories><comments>149 pages, incl. supplement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Must it be the case that increasingly deep levels of strategic iterated
reasoning by humans give increasingly close approximations of normative
economic rationality? And if not, what might they do instead? We argue that
human higher-level reasoning processes may support non-equilibrium,
non-convergent &quot;flocking&quot; behavior. Flocking in the physical world is the
sustained convergence of both the positions and velocities of the members of a
group. We make the metaphor to flocking in our experiments by introducing
decision environments in which participants' choices and reasoning processes
function as their positions and velocities, respectively. With this definition,
we demonstrate flocking in multiple group experiments over three unrelated
economic games. The first game is the classic Beauty Pageant, the second is
called the Mod Game, and we introduce the Runway Game. Though they three bear
no formal resemblance to each other, subjects play them the same: as eliciting
iterated reasoning in a way that causes individuals to converge to a &quot;normed&quot;
depth of iterated reasoning. Our analyses and modeling suggest that flocking
emerges from the way that players integrate feedback to adjust their depths of
reasoning each round --- an unexpected interaction between &quot;higher&quot;- and
&quot;lower&quot;-level reasoning processes. We offer new opportunities for cognitive
science in behavioral game theory by speculating on the role that mental models
play in helping participants represent these games as games of iterated
reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05413</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05413</id><created>2015-06-15</created><authors><author><keyname>Douglass</keyname><forenames>Rex W.</forenames></author></authors><title>Understanding Civil War Violence through Military Intelligence: Mining
  Civilian Targeting Records from the Vietnam War</title><categories>cs.CY cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Military intelligence is underutilized in the study of civil war violence.
Declassified records are hard to acquire and difficult to explore with the
standard econometrics toolbox. I investigate a contemporary government database
of civilians targeted during the Vietnam War. The data are detailed, with up to
45 attributes recorded for 73,712 individual civilian suspects. I employ an
unsupervised machine learning approach of cleaning, variable selection,
dimensionality reduction, and clustering. I find support for a simplifying
typology of civilian targeting that distinguishes different kinds of suspects
and different kinds targeting methods. The typology is robust, successfully
clustering both government actors and rebel departments into groups that mirror
their known functions. The exercise highlights methods for dealing with high
dimensional found conflict data. It also illustrates how aggregating measures
of political violence masks a complex underlying empirical data generating
process as well as a complex institutional reporting process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05421</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05421</id><created>2015-05-28</created><authors><author><keyname>Ch'ng</keyname><forenames>Eugene</forenames></author><author><keyname>Gaffney</keyname><forenames>Vince</forenames></author><author><keyname>Hakvoort</keyname><forenames>Gido</forenames></author></authors><title>Stigmergy in Comparative Settlement Choice and Palaeoenvironment
  Simulation</title><categories>physics.soc-ph cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decisions on settlement location in the face of climate change and coastal
inundation may have resulted in success, survival or even catastrophic failure
for early settlers in many parts of the world. In this study we investigate
various questions related to how individuals respond to a palaeoenvironmental
simulation, on an interactive tabletop device where participants have the
opportunity to build a settlement on a coastal landscape, balancing safety and
access to resources, including sea and terrestrial foodstuffs, whilst taking
into consideration the threat of rising sea levels. The results of the study
were analysed to consider whether decisions on settlement were predicated to be
near to locations where previous structures were located, stigmergically, and
whether later settler choice would fare better, and score higher, as time
progressed. The proximity of settlements was investigated and the reasons for
clustering were considered. The interactive simulation was exhibited to
thousands of visitors at the 2012 Royal Society Summer Science Exhibition at
the Europes Lost World exhibit. 347 participants contributed to the simulation,
providing a sufficiently large sample of data for analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05424</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05424</id><created>2015-06-17</created><authors><author><keyname>Miranda</keyname><forenames>Conrado Silva</forenames></author><author><keyname>Von Zuben</keyname><forenames>Fernando Jos&#xe9;</forenames></author></authors><title>Hybrid Algorithm for Multi-Objective Optimization by Greedy Hypervolume
  Maximization</title><categories>cs.NE cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a high-performance hybrid algorithm, called Hybrid
Hypervolume Maximization Algorithm (H2MA), for multi-objective optimization
that alternates between exploring the decision space and exploiting the already
obtained non-dominated solutions. The proposal is centered on maximizing the
hypervolume indicator, thus converting the multi-objective problem into a
single-objective one. The exploitation employs gradient-based methods, but
considering a single candidate efficient solution at a time, to overcome
limitations associated with population-based approaches and also to allow an
easy control of the number of solutions provided. There is an interchange
between two steps. The first step is a deterministic local exploration, endowed
with an automatic procedure to detect stagnation. When stagnation is detected,
the search is switched to a second step characterized by a stochastic global
exploration using an evolutionary algorithm. Using five ZDT benchmarks with 30
variables, the performance of the new algorithm is compared to state-of-the-art
algorithms for multi-objective optimization, more specifically NSGA-II, SPEA2,
and SMS-EMOA. The solutions found by the H2MA guide to higher hypervolume and
smaller distance to the true Pareto frontier with significantly less function
evaluations, even when the gradient is estimated numerically. Furthermore,
although only continuous decision spaces have been considered here, discrete
decision spaces could also have been treated, replacing gradient-based search
by hill-climbing. Finally, a thorough explanation is provided to support the
expressive gain in performance that was achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05427</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05427</id><created>2015-06-17</created><authors><author><keyname>Giulioni</keyname><forenames>Massimiliano</forenames></author><author><keyname>Corradi</keyname><forenames>Federico</forenames></author><author><keyname>Dante</keyname><forenames>Vittorio</forenames></author><author><keyname>del Giudice</keyname><forenames>Paolo</forenames></author></authors><title>Real time unsupervised learning of visual stimuli in neuromorphic VLSI
  systems</title><categories>cs.NE q-bio.NC</categories><comments>submitted to Scientific Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neuromorphic chips embody computational principles operating in the nervous
system, into microelectronic devices. In this domain it is important to
identify computational primitives that theory and experiments suggest as
generic and reusable cognitive elements. One such element is provided by
attractor dynamics in recurrent networks. Point attractors are equilibrium
states of the dynamics (up to fluctuations), determined by the synaptic
structure of the network; a `basin' of attraction comprises all initial states
leading to a given attractor upon relaxation, hence making attractor dynamics
suitable to implement robust associative memory. The initial network state is
dictated by the stimulus, and relaxation to the attractor state implements the
retrieval of the corresponding memorized prototypical pattern. In a previous
work we demonstrated that a neuromorphic recurrent network of spiking neurons
and suitably chosen, fixed synapses supports attractor dynamics. Here we focus
on learning: activating on-chip synaptic plasticity and using a theory-driven
strategy for choosing network parameters, we show that autonomous learning,
following repeated presentation of simple visual stimuli, shapes a synaptic
connectivity supporting stimulus-selective attractors. Associative memory
develops on chip as the result of the coupled stimulus-driven neural activity
and ensuing synaptic dynamics, with no artificial separation between learning
and retrieval phases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05439</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05439</id><created>2015-06-17</created><updated>2015-12-29</updated><authors><author><keyname>Frogner</keyname><forenames>Charlie</forenames></author><author><keyname>Zhang</keyname><forenames>Chiyuan</forenames></author><author><keyname>Mobahi</keyname><forenames>Hossein</forenames></author><author><keyname>Araya-Polo</keyname><forenames>Mauricio</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author></authors><title>Learning with a Wasserstein Loss</title><categories>cs.LG cs.CV stat.ML</categories><comments>NIPS 2015; v3 updates Algorithm 1 and Equations 6, 8</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning to predict multi-label outputs is challenging, but in many problems
there is a natural metric on the outputs that can be used to improve
predictions. In this paper we develop a loss function for multi-label learning,
based on the Wasserstein distance. The Wasserstein distance provides a natural
notion of dissimilarity for probability measures. Although optimizing with
respect to the exact Wasserstein distance is costly, recent work has described
a regularized approximation that is efficiently computed. We describe an
efficient learning algorithm based on this regularization, as well as a novel
extension of the Wasserstein distance from probability measures to unnormalized
measures. We also describe a statistical learning bound for the loss. The
Wasserstein loss can encourage smoothness of the predictions with respect to a
chosen metric on the output space. We demonstrate this property on a real-data
tag prediction problem, using the Yahoo Flickr Creative Commons dataset,
outperforming a baseline that doesn't use the metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05442</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05442</id><created>2015-06-17</created><authors><author><keyname>Ross</keyname><forenames>James A.</forenames></author><author><keyname>Richie</keyname><forenames>David A.</forenames></author><author><keyname>Park</keyname><forenames>Song J.</forenames></author><author><keyname>Shires</keyname><forenames>Dale R.</forenames></author></authors><title>Parallel Programming Model for the Epiphany Many-Core Coprocessor Using
  Threaded MPI</title><categories>cs.DC</categories><comments>7 pages, 6 figures, presented at ISCA'15, Third ACM International
  Workshop on Manycore Embedded Systems</comments><acm-class>C.1.4; D.1.3</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The Adapteva Epiphany many-core architecture comprises a 2D tiled mesh
Network-on-Chip (NoC) of low-power RISC cores with minimal uncore
functionality. It offers high computational energy efficiency for both integer
and floating point calculations as well as parallel scalability. Yet despite
the interesting architectural features, a compelling programming model has not
been presented to date. This paper demonstrates an efficient parallel
programming model for the Epiphany architecture based on the Message Passing
Interface (MPI) standard. Using MPI exploits the similarities between the
Epiphany architecture and a conventional parallel distributed cluster of serial
cores. Our approach enables MPI codes to execute on the RISC array processor
with little modification and achieve high performance. We report benchmark
results for the threaded MPI implementation of four algorithms (dense
matrix-matrix multiplication, N-body particle interaction, a five-point 2D
stencil update, and 2D FFT) and highlight the importance of fast inter-core
communication for the architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05443</identifier>
 <datestamp>2015-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05443</id><created>2015-06-17</created><authors><author><keyname>Souza</keyname><forenames>Andre Abrantes D. P.</forenames></author><author><keyname>Netto</keyname><forenames>Marco A. S.</forenames></author></authors><title>Using Application Data for SLA-aware Auto-scaling in Cloud Environments</title><categories>cs.DC</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the establishment of cloud computing as the environment of choice for
most modern applications, auto-scaling is an economic matter of great
importance. For applications like stream computing that process ever changing
amounts of data, modifying the number and configuration of resources to meet
performance requirements becomes essential. Current solutions on auto-scaling
are mostly rule-based using infrastructure level metrics such as
CPU/memory/network utilization, and system level metrics such as throughput and
response time. In this paper, we introduce a study on how effective
auto-scaling can be using data generated by the application itself. To make
this assessment, two algorithms are proposed that use a priori knowledge of the
data stream and use sentiment analysis from soccer-related tweets, triggering
auto-scaling operations according to rapid changes in the public sentiment
about the soccer players that happens just before big bursts of messages. Our
application-based auto-scaling was able to reduce the number of SLA violations
by up to 95% and reduce resource requirements by up to 33%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05474</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05474</id><created>2015-06-17</created><authors><author><keyname>De</keyname><forenames>Abir</forenames></author><author><keyname>Valera</keyname><forenames>Isabel</forenames></author><author><keyname>Ganguly</keyname><forenames>Niloy</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Sourangshu</forenames></author><author><keyname>Rodriguez</keyname><forenames>Manuel Gomez</forenames></author></authors><title>Modeling Opinion Dynamics in Diffusion Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media and social networking sites have become a global pinboard for
exposition and discussion of news, topics, and ideas, where social media users
increasingly form their opinion about a particular topic by learning
information about it from her peers. In this context, whenever a user posts a
message about a topic, we observe a noisy estimate of her current opinion about
it but the influence the user may have on other users' opinions is hidden. In
this paper, we introduce a probabilistic modeling framework of opinion
dynamics, which allows the underlying opinion of a user to be modulated by
those expressed by her neighbors over time. We then identify a set of
conditions under which users' opinions converge to a steady state, find a
linear relation between the initial opinions and the opinions in the steady
state, and develop an efficient estimation method to fit the parameters of the
model from historical fine-grained opinion and information diffusion event
data. Experiments on data gathered from Twitter, Reddit and Amazon show that
our model provides a good fit to the data and more accurate predictions than
alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05481</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05481</id><created>2015-06-17</created><authors><author><keyname>Dobrowolski</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>Swing-twist decomposition in Clifford algebra</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The swing-twist decomposition is a standard routine in motion planning for
humanoid limbs. In this paper the decomposition formulas are derived and
discussed in terms of Clifford algebra. With the decomposition one can express
an arbitrary spinor as a product of a twist-free spinor and a swing-free spinor
(or vice-versa) in 3-dimensional Euclidean space. It is shown that in the
derived decomposition formula the twist factor is a generalized projection of a
spinor onto a vector in Clifford algebra. As a practical application of the
introduced theory an optimized decomposition algorithm is proposed. It
favourably compares to existing swing-twist decomposition implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05485</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05485</id><created>2015-06-17</created><updated>2015-06-18</updated><authors><author><keyname>Lee</keyname><forenames>Kooktae</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Raktim</forenames></author></authors><title>On the Convergence Analysis of Asynchronous Distributed Quadratic
  Programming via Dual Decomposition</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the convergence as well as the rate of convergence
of asynchronous distributed quadratic programming (QP) with dual decomposition
technique. In general, distributed optimization requires synchronization of
data at each iteration step due to the interdependency of data. This
synchronization latency may incur a large amount of waiting time caused by an
idle process during computation. We aim to attack this synchronization penalty
in distributed QP problems by implementing asynchronous update of dual
variable. The price to pay for adopting asynchronous computing algorithms is
unpredictability of the solution, resulting in a tradeoff between speedup and
accuracy. Thus, the convergence to an optimal solution is not guaranteed owing
to the stochastic behavior of asynchrony. In this paper, we employ the switched
system framework as an analysis tool to investigate the convergence of
asynchronous distributed QP. This switched system will facilitate analysis on
asynchronous distributed QP with dual decomposition, providing necessary and
sufficient conditions for the mean square convergence. Also, we provide an
analytic expression for the rate of convergence through the switched system,
which enables performance analysis of asynchronous algorithms as compared with
synchronous case. To verify the validity of the proposed methods, numerical
examples are presented with an implementation of asynchronous parallel QP using
OpenMP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05490</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05490</id><created>2015-06-17</created><authors><author><keyname>Martin</keyname><forenames>Travis</forenames></author><author><keyname>Ball</keyname><forenames>Brian</forenames></author><author><keyname>Newman</keyname><forenames>M. E. J.</forenames></author></authors><title>Structural inference for uncertain networks</title><categories>cs.SI cond-mat.stat-mech physics.soc-ph</categories><comments>12 pages, 4 figures</comments><journal-ref>Phys. Rev. E 93, 012306 (2016)</journal-ref><doi>10.1103/PhysRevE.93.012306</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the study of networked systems such as biological, technological, and
social networks the available data are often uncertain. Rather than knowing the
structure of a network exactly, we know the connections between nodes only with
a certain probability. In this paper we develop methods for the analysis of
such uncertain data, focusing particularly on the problem of community
detection. We give a principled maximum-likelihood method for inferring
community structure and demonstrate how the results can be used to make
improved estimates of the true structure of the network. Using
computer-generated benchmark networks we demonstrate that our methods are able
to reconstruct known communities more accurately than previous approaches based
on data thresholding. We also give an example application to the detection of
communities in a protein-protein interaction network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05497</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05497</id><created>2015-06-17</created><authors><author><keyname>Miller</keyname><forenames>Christopher W.</forenames></author><author><keyname>Yang</keyname><forenames>Insoon</forenames></author></authors><title>Optimal Dynamic Contracts for a Large-Scale Principal-Agent Hierarchy: A
  Concavity-Preserving Approach</title><categories>math.OC cs.SY q-fin.EC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a continuous-time contract whereby a top-level player can
incentivize a hierarchy of players below him to act in his best interest
despite only observing the output of his direct subordinate. This paper extends
Sannikov's approach from a situation of asymmetric information between a
principal and an agent to one of hierarchical information between several
players. We develop an iterative algorithm for constructing an incentive
compatible contract and define the correct notion of concavity which must be
preserved during iteration. We identify conditions under which a dynamic
programming construction of an optimal dynamic contract can be reduced to only
a one-dimensional state space and one-dimensional control set, independent of
the size of the hierarchy. In this sense, our results contribute to the
applicability of dynamic programming on dynamic contracts for a large-scale
principal-agent hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05499</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05499</id><created>2015-06-17</created><authors><author><keyname>Stefanescu</keyname><forenames>Gheorghe</forenames></author></authors><title>Self-assembling interactive modules: A research programme</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a research programme for getting structural
characterisations for 2-dimensional languages generated by self-assembling
tiles. This is part of a larger programme on getting a formal foundation of
parallel, interactive, distributed systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05505</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05505</id><created>2015-06-17</created><authors><author><keyname>Barba</keyname><forenames>Luis</forenames></author><author><keyname>Duque</keyname><forenames>Frank</forenames></author><author><keyname>Fabila-Monroy</keyname><forenames>Ruy</forenames></author><author><keyname>Hidalgo-Toscano</keyname><forenames>Carlos</forenames></author></authors><title>Drawing the Horton Set in an Integer Grid of Minimum Size</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1978 Erd\H os asked if every sufficiently large set of points in general
position in the plane contains the vertices of a convex $k$-gon, with the
additional property that no other point of the set lies in its interior.
Shortly after, Horton provided a construction---which is now called the Horton
set---with no such $7$-gon. In this paper we show that the Horton set of $n$
points can be realized with integer coordinates of absolute value at most
$\frac{1}{2} n^{\frac{1}{2} \log (n/2)}$. We also show that any set of points
with integer coordinates combinatorially equivalent (with the same order type)
to the Horton set, contains a point with a coordinate of absolute value at
least $c \cdot n^{\frac{1}{24}\log (n/2)}$, where $c$ is a positive constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05506</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05506</id><created>2015-06-17</created><updated>2015-08-06</updated><authors><author><keyname>Maruyama</keyname><forenames>Yuzo</forenames></author><author><keyname>Tone</keyname><forenames>Ryoko</forenames></author><author><keyname>Asami</keyname><forenames>Yasushi</forenames></author></authors><title>Noise Addition for Individual Records to Preserve Privacy and
  Statistical Characteristics: Case Study of Real Estate Transaction Data</title><categories>stat.ME cs.CR</categories><comments>16 pages; minor change in title; revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method of perturbing a major variable by adding noise such
that results of regression analysis are unaffected. The extent of the
perturbation can be controlled using a single parameter, which eases an actual
perturbation application. On the basis of results of a numerical experiment, we
recommend an appropriate value of the parameter that can achieve both
sufficient perturbation to mask original values and sufficient coherence
between perturbed and original data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05514</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05514</id><created>2015-06-17</created><authors><author><keyname>Sandouk</keyname><forenames>Ubai</forenames></author><author><keyname>Chen</keyname><forenames>Ke</forenames></author></authors><title>Learning Contextualized Semantics from Co-occurring Terms via a Siamese
  Architecture</title><categories>cs.IR cs.CL cs.LG</categories><report-no>2015-06-18</report-no><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the biggest challenges in Multimedia information retrieval and
understanding is to bridge the semantic gap by properly modeling concept
semantics in context. The presence of out of vocabulary (OOV) concepts
exacerbates this difficulty. To address the semantic gap issues, we formulate a
problem on learning contextualized semantics from descriptive terms and propose
a novel Siamese architecture to model the contextualized semantics from
descriptive terms. By means of pattern aggregation and probabilistic topic
models, our Siamese architecture captures contextualized semantics from the
co-occurring descriptive terms via unsupervised learning, which leads to a
concept embedding space of the terms in context. Furthermore, the co-occurring
OOV concepts can be easily represented in the learnt concept embedding space.
The main properties of the concept embedding space are demonstrated via
visualization. Using various settings in semantic priming, we have carried out
a thorough evaluation by comparing our approach to a number of state-of-the-art
methods on six annotation corpora in different domains, i.e., MagTag5K, CAL500
and Million Song Dataset in the music domain as well as Corel5K, LabelMe and
SUNDatabase in the image domain. Experimental results on semantic priming
suggest that our approach outperforms those state-of-the-art methods
considerably in various aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05525</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05525</id><created>2015-06-17</created><updated>2015-06-23</updated><authors><author><keyname>Ma</keyname><forenames>Jingxue</forenames></author><author><keyname>Zhang</keyname><forenames>Tao</forenames></author><author><keyname>Feng</keyname><forenames>Tao</forenames></author><author><keyname>Ge</keyname><forenames>Gennian</forenames></author></authors><title>New results on permutation polynomials over finite fields</title><categories>cs.IT math.IT math.NT</categories><comments>21 pages. We will change the title of our paper later since it is the
  same with others</comments><msc-class>11T06, 11T55, 05A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Permutation polynomials over finite fields constitute an active research area
and have applications in many areas of science and engineering. In this paper,
four classes of monomial complete permutation polynomials and one class of
trinomial complete permutation polynomials are presented, one of which confirms
a conjecture proposed by Wu et al. (Sci. China Math., to appear. Doi:
10.1007/s11425-014-4964-2). Furthermore, we give two classes of trinomial
permutation polynomials, and make some progress on a conjecture about the
differential uniformity of power permutation polynomials proposed by Blondeau
et al. (Int. J. Inf. Coding Theory, 2010, 1, pp. 149-170).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05527</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05527</id><created>2015-06-17</created><authors><author><keyname>Martini</keyname><forenames>Ben</forenames></author><author><keyname>Do</keyname><forenames>Quang</forenames></author><author><keyname>Choo</keyname><forenames>Kim-Kwang Raymond</forenames></author></authors><title>Conceptual evidence collection and analysis methodology for Android
  devices</title><categories>cs.CY</categories><comments>in Cloud Security Ecosystem (Syngress, an Imprint of Elsevier), 2015</comments><doi>10.1016/B978-0-12-801595-7.00014-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Android devices continue to grow in popularity and capability meaning the
need for a forensically sound evidence collection methodology for these devices
also increases. This chapter proposes a methodology for evidence collection and
analysis for Android devices that is, as far as practical, device agnostic.
Android devices may contain a significant amount of evidential data that could
be essential to a forensic practitioner in their investigations. However, the
retrieval of this data requires that the practitioner understand and utilize
techniques to analyze information collected from the device. The major
contribution of this research is an in-depth evidence collection and analysis
methodology for forensic practitioners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05529</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05529</id><created>2015-06-17</created><authors><author><keyname>Zhang</keyname><forenames>Jiawei</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author></authors><title>Mutual Community Detection across Multiple Partially Aligned Social
  Networks</title><categories>cs.SI physics.soc-ph</categories><comments>12 pages, 5 figures, this paper is the extended version of &quot;MCD:
  Mutual Clustering across Multiple Social Networks&quot; accepted by IEEE Big Data
  Congress Visionary Track' 15</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection in online social networks has been a hot research topic
in recent years. Meanwhile, to enjoy more social network services, users
nowadays are usually involved in multiple online social networks
simultaneously, some of which can share common information and structures.
Networks that involve some common users are named as multiple &quot;partially
aligned networks&quot;. In this paper, we want to detect communities of multiple
partially aligned networks simultaneously, which is formally defined as the
&quot;Mutual Clustering&quot; problem. The &quot;Mutual Clustering&quot; problem is very
challenging as it has two important issues to address: (1) how to preserve the
network characteristics in mutual community detection? and (2) how to utilize
the information in other aligned networks to refine and disambiguate the
community structures of the shared users? To solve these two challenges, a
novel community detection method, MCD (Mutual Community Detector), is proposed
in this paper. MCD can detect social community structures of users in multiple
partially aligned networks at the same time with full considerations of (1)
characteristics of each network, and (2) information of the shared users across
aligned networks. Extensive experiments conducted on two real-world partially
aligned heterogeneous social networks demonstrate that MCD can solve the
&quot;Mutual Clustering&quot; problem very well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05532</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05532</id><created>2015-06-17</created><updated>2015-08-14</updated><authors><author><keyname>Hayat</keyname><forenames>Munawar</forenames></author><author><keyname>Khan</keyname><forenames>Salman H.</forenames></author><author><keyname>Bennamoun</keyname><forenames>Mohammed</forenames></author><author><keyname>An</keyname><forenames>Senjian</forenames></author></authors><title>A Spatial Layout and Scale Invariant Feature Representation for Indoor
  Scene Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike standard object classification, where the image to be classified
contains one or multiple instances of the same object, indoor scene
classification is quite different since the image consists of multiple distinct
objects. Further, these objects can be of varying sizes and are present across
numerous spatial locations in different layouts. For automatic indoor scene
categorization, large scale spatial layout deformations and scale variations
are therefore two major challenges and the design of rich feature descriptors
which are robust to these challenges is still an open problem. This paper
introduces a new learnable feature descriptor called &quot;spatial layout and scale
invariant convolutional activations&quot; to deal with these challenges. For this
purpose, a new Convolutional Neural Network architecture is designed which
incorporates a novel 'Spatially Unstructured' layer to introduce robustness
against spatial layout deformations. To achieve scale invariance, we present a
pyramidal image representation. For feasible training of the proposed network
for images of indoor scenes, the paper proposes a new methodology which
efficiently adapts a trained network model (on a large scale data) for our task
with only a limited amount of available training data. Compared with existing
state of the art, the proposed approach achieves a relative performance
improvement of 3.2%, 3.8%, 7.0%, 11.9% and 2.1% on MIT-67, Scene-15, Sports-8,
Graz-02 and NYU datasets respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05533</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05533</id><created>2015-06-17</created><authors><author><keyname>Martini</keyname><forenames>Ben</forenames></author><author><keyname>Do</keyname><forenames>Quang</forenames></author><author><keyname>Choo</keyname><forenames>Kim-Kwang Raymond</forenames></author></authors><title>Mobile Cloud Forensics: An Analysis of Seven Popular Android Apps</title><categories>cs.CY cs.CR</categories><comments>Book Chapter in Cloud Security Ecosystem (Syngress, an Imprint of
  Elsevier), 2015</comments><doi>10.1016/B978-0-12-801595-7.00015-X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the evidence collection and analysis methodology for Android devices
proposed by Martini, Do and Choo, we examined and analyzed seven popular
Android cloud-based apps. Firstly, we analyzed each app in order to see what
information could be obtained from their private app storage and SD card
directories. We collated the information and used it to aid our investigation
of each app database files and AccountManager data. To complete our
understanding of the forensic artefacts stored by apps we analyzed, we
performed further analysis on the apps to determine if the user authentication
credentials could be collected for each app based on the information gained in
the initial analysis stages. The contributions of this research include a
detailed description of artefacts, which are of general forensic interest, for
each app analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05541</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05541</id><created>2015-06-17</created><authors><author><keyname>Sun</keyname><forenames>Yi</forenames></author><author><keyname>Yin</keyname><forenames>Xiaoqi</forenames></author><author><keyname>Wang</keyname><forenames>Nanshu</forenames></author><author><keyname>Jiang</keyname><forenames>Junchen</forenames></author><author><keyname>Sekar</keyname><forenames>Vyas</forenames></author><author><keyname>Jin</keyname><forenames>Yun</forenames></author><author><keyname>Sinopoli</keyname><forenames>Bruno</forenames></author></authors><title>Analyzing TCP Throughput Stability and Predictability with Implications
  for Adaptive Video Streaming</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work suggests that TCP throughput stability and predictability within
a video viewing session can inform the design of better video bitrate
adaptation algorithms. Despite a rich tradition of Internet measurement,
however, our understanding of throughput stability and predictability is quite
limited. To bridge this gap, we present a measurement study of throughput
stability using a large-scale dataset from a video service provider. Drawing on
this analysis, we propose a simple-but-effective prediction mechanism based on
a hidden Markov model and demonstrate that it outperforms other approaches. We
also show the practical implications in improving the user experience of
adaptive video streaming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05543</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05543</id><created>2015-06-17</created><authors><author><keyname>Rana</keyname><forenames>Rajib</forenames></author><author><keyname>Hume</keyname><forenames>Margee</forenames></author><author><keyname>Reilly</keyname><forenames>John</forenames></author><author><keyname>Soar</keyname><forenames>Jeffrey</forenames></author></authors><title>wHealth - Transforming Telehealth Services</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A worldwide increase in proportions of older people in the population poses
the challenge of managing their increasing healthcare needs within limited
resources. To achieve this many countries are interested in adopting telehealth
technology. Several shortcomings of state-of-the-art telehealth technology
constrain widespread adoption of telehealth services. We present an
ensemble-sensing framework - wHealth (short form of wireless health) for
effective delivery of telehealth services. It extracts personal health
information using sensors embedded in everyday devices and allows effective and
seamless communication between patients and clinicians. Due to the
non-stigmatizing design, ease of maintenance, simplistic interaction and
seamless intervention, our wHealth platform has the potential to enable
widespread adoption of telehealth services for managing elderly healthcare. We
discuss the key barriers and potential solutions to make the wHealth platform a
reality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05549</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05549</id><created>2015-06-18</created><authors><author><keyname>Li</keyname><forenames>Dapeng</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Guvenc</keyname><forenames>Ismail</forenames></author><author><keyname>Mehbodniya</keyname><forenames>Abolfazl</forenames></author><author><keyname>Adachi</keyname><forenames>Fumiyuki</forenames></author></authors><title>Decentralized Energy Allocation for Wireless Networks with Renewable
  Energy Powered Base Stations</title><categories>cs.NI cs.GT cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a green wireless communication system in which base stations
are powered by renewable energy sources is considered. This system consists of
a capacity-constrained renewable power supplier (RPS) and a base station (BS)
that faces a predictable random connection demand from mobile user equipments
(UEs). In this model, the BS powered via a combination of a renewable power
source and the conventional electric grid, seeks to specify the renewable power
inventory policy, i.e., the power storage level. On the other hand, the RPS
must strategically choose the energy amount that is supplied to the BS. An
M/M/1 make-to-stock queuing model is proposed to investigate the decentralized
decisions when the two parties optimize their individual costs in a
noncooperative manner. The problem is formulated as a noncooperative game whose
Nash equilibrium (NE) strategies are characterized in order to identify the
causes of inefficiency in the decentralized operation. A set of simple linear
contracts are introduced to coordinate the system so as to achieve an optimal
system performance. The proposed approach is then extended to a setting with
one monopolistic RPS and N BSs that are privately informed of their optimal
energy inventory levels. In this scenario, we show that the widely-used
proportional allocation mechanism is no longer socially optimal. In order to
make the BSs truthfully report their energy demand, an incentive compatible
(IC) mechanism is proposed for our model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05557</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05557</id><created>2015-06-18</created><updated>2015-11-04</updated><authors><author><keyname>Garg</keyname><forenames>Dhanesh</forenames></author><author><keyname>Kumar</keyname><forenames>Staish</forenames></author></authors><title>Exponential Quantum Tsallis Havrda Charvat Entropy of Type Alpha</title><categories>cs.IT math-ph math.IT math.MP</categories><comments>10 pages, no figure</comments><report-no>1284489</report-no><msc-class>94A15, 94A24, 26D15, 46N50, 46L30, 47L90</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entropy is a key measure in studies related to information theory and its
many applications. Campbell of the first time recognized that exponential of
Shannons entropy is just the size of the sample space when the distribution is
uniform. In this paper, we introduce a quantity which is called exponential
Tsallis Havrda Charvat entropy and discuss its some properties. Further, we
gave the application of exponential Tsallis Havrda Charvat entropy in quantum
information theory which is called exponential quantum Tsallis Havrda Charvat
entropy with its some major properties such as non-negative, concavity and
continuity. It is found that projective measurement will not decrease the
quantum entropy of a quantum state and at the end of the paper gave an upper
bound on the quantum exponential entropy in terms of ensembles of pure state.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05558</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05558</id><created>2015-06-18</created><authors><author><keyname>Pai</keyname><forenames>Srikanth B.</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>A Singleton Bound for Generalized Ferrers Diagram Rank Metric Codes</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we will employ the technique used in the proof of classical
Singleton bound to derive upper bounds for rank metric codes and Ferrers
diagram rank metric codes. These upper bounds yield the rank distance Singleton
bound and an upper bound presented by Etzion and Silberstein respectively. Also
we introduce generalized Ferrers diagram rank metric code which is a Ferrers
diagram rank metric code where the underlying rank metric code is not
necessarily linear. A new Singleton bound for generalized Ferrers diagram rank
metric code is obtained using our technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05561</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05561</id><created>2015-06-18</created><authors><author><keyname>Moot</keyname><forenames>Richard</forenames><affiliation>LaBRI</affiliation></author></authors><title>Comparing and evaluating extended Lambek calculi</title><categories>cs.CL cs.LO</categories><comments>Empirical advances in categorial grammars, Aug 2015, Barcelona,
  Spain. 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lambeks Syntactic Calculus, commonly referred to as the Lambek calculus, was
innovative in many ways, notably as a precursor of linear logic. But it also
showed that we could treat our grammatical framework as a logic (as opposed to
a logical theory). However, though it was successful in giving at least a basic
treatment of many linguistic phenomena, it was also clear that a slightly more
expressive logical calculus was needed for many other cases. Therefore, many
extensions and variants of the Lambek calculus have been proposed, since the
eighties and up until the present day. As a result, there is now a large class
of calculi, each with its own empirical successes and theoretical results, but
also each with its own logical primitives. This raises the question: how do we
compare and evaluate these different logical formalisms? To answer this
question, I present two unifying frameworks for these extended Lambek calculi.
Both are proof net calculi with graph contraction criteria. The first calculus
is a very general system: you specify the structure of your sequents and it
gives you the connectives and contractions which correspond to it. The calculus
can be extended with structural rules, which translate directly into graph
rewrite rules. The second calculus is first-order (multiplicative
intuitionistic) linear logic, which turns out to have several other,
independently proposed extensions of the Lambek calculus as fragments. I will
illustrate the use of each calculus in building bridges between analyses
proposed in different frameworks, in highlighting differences and in helping to
identify problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05573</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05573</id><created>2015-06-18</created><authors><author><keyname>Sanlaville</keyname><forenames>Kevin</forenames><affiliation>LTCI</affiliation></author><author><keyname>Assayag</keyname><forenames>G&#xe9;rard</forenames><affiliation>LTCI</affiliation></author><author><keyname>Bevilacqua</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>LTCI</affiliation></author><author><keyname>Pelachaud</keyname><forenames>Catherine</forenames><affiliation>LTCI</affiliation></author></authors><title>Emergence of synchrony in an Adaptive Interaction Model</title><categories>cs.HC cs.AI</categories><comments>Intelligent Virtual Agents 2015 Doctoral Consortium, Aug 2015, Delft,
  Netherlands. IVA Doctoral Consortium, 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a Human-Computer Interaction context, we aim to elaborate an adaptive and
generic interaction model in two different use cases: Embodied Conversational
Agents and Creative Musical Agents for musical improvisation. To reach this
goal, we'll try to use the concepts of adaptation and synchronization to
enhance the interactive abilities of our agents and guide the development of
our interaction model, and will try to make synchrony emerge from non-verbal
dimensions of interaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05574</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05574</id><created>2015-06-18</created><authors><author><keyname>Helmigh</keyname><forenames>Jonathan</forenames></author></authors><title>Information Diffusion issues</title><categories>cs.SI</categories><comments>7 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this report there will be a discussion for Information Diffusion. There
will be discussions on what information diffusion is, its key characteristics
and on several other aspects of these kinds of networks. This report will focus
on peer to peer models in information diffusion. There will be discussions on
epidemic model, OSN and other details related to information diffusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05579</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05579</id><created>2015-06-18</created><authors><author><keyname>Gong</keyname><forenames>Qingyuan</forenames></author><author><keyname>Wang</keyname><forenames>Jiaqi</forenames></author><author><keyname>Wang</keyname><forenames>Yan</forenames></author><author><keyname>Wei</keyname><forenames>Dongsheng</forenames></author><author><keyname>Wang</keyname><forenames>Jin</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author></authors><title>Topology-Aware Node Selection for Data Regeneration in Heterogeneous
  Distributed Storage Systems</title><categories>cs.DC</categories><comments>14pages, 7 pages, 4 algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed storage systems introduce redundancy to protect data from node
failures. After a storage node fails, the lost data should be regenerated at a
replacement storage node as soon as possible to maintain the same level of
redundancy. Minimizing such a regeneration time is critical to the reliability
of distributed storage systems. Existing work commits to reduce the
regeneration time by either minimizing the regenerating traffic, or adjusting
the regenerating traffic patterns, whereas nodes participating data
regeneration are generally assumed to be given beforehand. However, such
regeneration time also depends heavily on the selection of the participating
nodes. Selecting different participating nodes actually involve different data
links between the nodes. Real-world distributed storage systems usually exhibit
heterogeneous link capacities. It is possible to further reduce the
regeneration time via exploiting such link capacity differences and avoiding
the link bottlenecks. In this paper, we consider the minimization of the
regeneration time by selecting the participating nodes in heterogeneous
networks. We analyze the regeneration time and propose node selection
algorithms for overlay networks and real-world topologies. Considering that the
flexible amount of data blocks from each provider may deeply influence the
regeneration time, several techniques are designed to enhance our schemes in
overlay networks. Experimental results show that our node selection schemes can
significantly reduce the regeneration time for each topology, especially in
practical networks with heterogeneous link capacities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05595</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05595</id><created>2015-06-18</created><authors><author><keyname>G</keyname><forenames>David Gonz&#xe1;lez</forenames></author><author><keyname>H&#xe4;m&#xe4;l&#xe4;inen</keyname><forenames>Jyri</forenames></author><author><keyname>Yanikomeroglu</keyname><forenames>Halim</forenames></author><author><keyname>Garc&#xed;a-Lozano</keyname><forenames>Mario</forenames></author><author><keyname>Senarath</keyname><forenames>Gamini</forenames></author></authors><title>A Novel Multiobjective Cell Switch-Off Method with Low Complexity for
  Realistic Cellular Deployments</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cell Switch-Off (CSO) is recognized as a promising approach to reduce the
energy consumption in next-generation cellular networks. However, CSO poses
serious challenges not only from the resource allocation perspective but also
from the implementation point of view. Indeed, CSO represents a difficult
optimization problem due to its NP-complete nature. Moreover, there are a
number of important practical limitations in the implementation of CSO schemes
such as the need for minimizing the real-time complexity and the number of
transitions as well as handovers. This article introduces a novel approach to
CSO based on multiobjective optimization that makes use of the statistical
description of the service demand (known by operators). In addition, downlink
and uplink coverage criteria are included and a comparative analysis between
different models to characterize intercell interference is also presented to
shed light on their impact on CSO. The method distinguishes itself from the
other proposals in two ways: 1) The number of cell switch on/off transitions as
well as handovers are minimized, and 2) the computationally-heavy part of the
algorithm is executed offline, which makes its implementation feasible. The
results show that the proposed scheme achieves substantial energy savings in
small cell deployments where traffic is highly unbalanced, with neither
compromising the Quality-of-Service (QoS) nor requiring heavy real-time
processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05600</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05600</id><created>2015-06-18</created><authors><author><keyname>Rahmadi</keyname><forenames>Ridho</forenames></author><author><keyname>Groot</keyname><forenames>Perry</forenames></author><author><keyname>Heins</keyname><forenames>Marianne</forenames></author><author><keyname>Knoop</keyname><forenames>Hans</forenames></author><author><keyname>Heskes</keyname><forenames>Tom</forenames></author><author><keyname>consortium</keyname><forenames>The OPTIMISTIC</forenames></author></authors><title>Causality on Cross-Sectional Data: Stable Specification Search in
  Constrained Structural Equation Modeling</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Causal modeling has long been an attractive topic for many researchers and in
recent decades there has seen a surge in theoretical development and discovery
algorithms. Generally discovery algorithms can be divided into two approaches:
constraint-based and score-based. A disadvantage of currently existing
constraint-based and score-based approaches, however, is the inherent
instability in structure estimation. With finite samples small changes in the
data can lead to completely different optimal structures. The present work
introduces a new score-based causal discovery algorithm that is robust for
finite samples based on recent advances in stability selection using
subsampling and selection algorithms. Structure search is performed over
Structural Equation Models. Our approach uses exploratory search but allows
incorporation of prior background knowledge to constrain the search space. We
show that our approach produces accurate structure estimates on one simulated
data set and two real-world data sets for Chronic Fatigue Syndrome and
Attention Deficit Hyperactivity Disorder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05603</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05603</id><created>2015-06-18</created><authors><author><keyname>Rodol&#xe0;</keyname><forenames>Emanuele</forenames></author><author><keyname>Moeller</keyname><forenames>Michael</forenames></author><author><keyname>Cremers</keyname><forenames>Daniel</forenames></author></authors><title>Point-wise Map Recovery and Refinement from Functional Correspondence</title><categories>cs.CV cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since their introduction in the shape analysis community, functional maps
have met with considerable success due to their ability to compactly represent
dense correspondences between deformable shapes, with applications ranging from
shape matching and image segmentation, to exploration of large shape
collections. Despite the numerous advantages of such representation, however,
the problem of converting a given functional map back to a point-to-point map
has received a surprisingly limited interest. In this paper we analyze the
general problem of point-wise map recovery from arbitrary functional maps. In
doing so, we rule out many of the assumptions required by the currently
established approach -- most notably, the limiting requirement of the input
shapes being nearly-isometric. We devise an efficient recovery process based on
a simple probabilistic model. Experiments confirm that this approach achieves
remarkable accuracy improvements in very challenging cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05605</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05605</id><created>2015-06-18</created><authors><author><keyname>Barras</keyname><forenames>Bruno</forenames><affiliation>SPECFUN</affiliation></author><author><keyname>Tankink</keyname><forenames>Carst</forenames><affiliation>SPECFUN</affiliation></author><author><keyname>Tassi</keyname><forenames>Enrico</forenames><affiliation>MARELLE</affiliation></author></authors><title>Asynchronous processing of Coq documents: from the kernel up to the user
  interface</title><categories>cs.LO cs.MS</categories><comments>in Proceedings of ITP, Aug 2015, Nanjing, China</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work described in this paper improves the reactivity of the Coq system by
completely redesigning the way it processes a formal document. By subdividing
such work into independent tasks the system can give precedence to the ones of
immediate interest for the user and postpones the others. On the user side, a
modern interface based on the PIDE middleware aggregates and present in a
consistent way the output of the prover. Finally postponed tasks are processed
exploiting modern, parallel, hardware to offer better scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05607</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05607</id><created>2015-06-18</created><updated>2015-08-27</updated><authors><author><keyname>Cattaruzza</keyname><forenames>Dario</forenames></author><author><keyname>Abate</keyname><forenames>Alessandro</forenames></author><author><keyname>Schrammel</keyname><forenames>Peter</forenames></author><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author></authors><title>Unbounded-Time Analysis of Guarded LTI Systems with Inputs by Abstract
  Acceleration (extended version)</title><categories>cs.SY</categories><comments>extended version of paper published in SAS'15</comments><acm-class>F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear Time Invariant (LTI) systems are ubiquitous in control applications.
Unbounded-time reachability analysis that can cope with industrial-scale models
with thousands of variables is needed. To tackle this problem, we use abstract
acceleration, a method for unbounded-time polyhedral reachability analysis for
linear systems. Existing variants of the method are restricted to closed
systems, i.e., dynamical models without inputs or non-determinism. In this
paper, we present an extension of abstract acceleration to linear loops with
inputs, which correspond to discrete-time LTI control systems under guard
conditions. The new method relies on a relaxation of the solution of the linear
dynamical equation that leads to a precise over-approximation of the set of
reachable states, which are evaluated using support functions. In order to
increase scalability, we use floating-point computations and ensure soundness
by interval arithmetic. Our experiments show that performance increases by
several orders of magnitude over alternative approaches in the literature. In
turn, this tremendous gain allows us to improve on precision by computing more
expensive abstractions. We outperform state-of-the-art tools for unbounded-time
analysis of LTI system with inputs in speed as well as in precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05608</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05608</id><created>2015-06-18</created><authors><author><keyname>Monostori</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author><author><keyname>Valckenaers</keyname><forenames>Paul</forenames></author><author><keyname>Dolgui</keyname><forenames>Alexandre</forenames></author><author><keyname>Panetto</keyname><forenames>Herv&#xe9;</forenames></author><author><keyname>Brdys</keyname><forenames>Mietek</forenames></author><author><keyname>Cs&#xe1;ji</keyname><forenames>Bal&#xe1;zs Csan&#xe1;d</forenames></author></authors><title>Cooperative Control in Production and Logistics</title><categories>cs.SY cs.MA math.OC</categories><comments>Status Report prepared by the IFAC Coordinating Committee on
  Manufacturing and Logistics Systems</comments><journal-ref>Annual Reviews in Control, Volume 39, 2015, Pages 12-29</journal-ref><doi>10.1016/j.arcontrol.2015.03.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical applications of control engineering and information and
communication technology (ICT) in production and logistics are often done in a
rigid, centralized and hierarchical way. These inflexible approaches are
typically not able to cope with the complexities of the manufacturing
environment, such as the instabilities, uncertainties and abrupt changes caused
by internal and external disturbances, or a large number and variety of
interacting, interdependent elements. A paradigm shift, e.g., novel organizing
principles and methods, is needed for supporting the interoperability of
dynamic alliances of agile and networked systems. Several solution proposals
argue that the future of manufacturing and logistics lies in network-like,
dynamic, open and reconfigurable systems of cooperative autonomous entities.
  The paper overviews various distributed approaches and technologies of
control engineering and ICT that can support the realization of cooperative
structures from the resource level to the level of networked enterprises.
Standard results as well as recent advances from control theory, through
cooperative game theory, distributed machine learning to holonic systems,
cooperative enterprise modelling, system integration, and autonomous logistics
processes are surveyed. A special emphasis is put on the theoretical
developments and industrial applications of Robustly Feasible Model Predictive
Control (RFMPC). Two case studies are also discussed: i) a holonic, PROSA-based
approach to generate short-term forecasts for an additive manufacturing system
by means of a delegate multi-agent system (D-MAS); and ii) an application of
distributed RFMPC to a drinking water distribution system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05620</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05620</id><created>2015-06-18</created><authors><author><keyname>van Bevern</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Komusiewicz</keyname><forenames>Christian</forenames></author><author><keyname>Sorge</keyname><forenames>Manuel</forenames></author></authors><title>Approximation algorithms for mixed, windy, and capacitated arc routing
  problems</title><categories>cs.DS cs.DM</categories><comments>16 pages (3 in appendix), 3 figures</comments><msc-class>90B06</msc-class><acm-class>F.2.2; G.1.6; G.2.1; G.2.2; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present approximation algorithms for the directed Rural Postman problem
(DRPP), the mixed, windy Rural Postman problem (MWRPP), and the mixed, windy
Capacitated Arc Routing problem (MWCARP). We show that
$\alpha(n)$-approximations for $n$-vertex metric asymmetric TSP
($\triangle$-ATSP) yield $(\alpha(C) + 1)$-approximations for DRPP,
$(\alpha(C)+3)$-approximations for MWRPP, and $O(\alpha(C+1))$-approximations
for MWCARP, where $C$ is the number of weakly connected components induced by
positive-demand edges and arcs. Combining this with a result from the
literature, we obtain $O(\log C/\log \log C)$-approximations for each of the
considered problems, which improves to polynomial-time constant-factor
approximations if $C \in O(\log n)$. Moreover, since $\alpha$-approximations
for DRPP yield $\alpha$-approximations for $\triangle$-ATSP, our result shows
that $\triangle$-ATSP and MWCARP are approximation-equivalent up to small
factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05628</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05628</id><created>2015-06-18</created><authors><author><keyname>Ren</keyname><forenames>Yongli</forenames></author><author><keyname>Tomko</keyname><forenames>Martin</forenames></author><author><keyname>Salim</keyname><forenames>Flora</forenames></author><author><keyname>Ong</keyname><forenames>Kevin</forenames></author><author><keyname>Sanderson</keyname><forenames>Mark</forenames></author></authors><title>Analyzing Web Behavior in Indoor Retail Spaces</title><categories>cs.IR cs.SI</categories><msc-class>68U35</msc-class><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze 18 million rows of Wi-Fi access logs collected over a one year
period from over 120,000 anonymized users at an inner-city shopping mall. The
anonymized dataset gathered from an opt-in system provides users' approximate
physical location, as well as Web browsing and some search history. Such data
provides a unique opportunity to analyze the interaction between people's
behavior in physical retail spaces and their Web behavior, serving as a proxy
to their information needs. We find: (1) the use of Wi-Fi network maps the
opening hours of the mall; (2) there is a weekly periodicity in users' visits
to the mall; (3) around 60% of registered Wi-Fi users actively browse the Web
and around 10% of them use Wi-Fi for accessing Web search engines; (4) people
are likely to spend a relatively constant amount of time browsing the Web while
their visiting duration may vary; (5) people tend to visit similar mall
locations and Web content during their repeated visits to the mall; (6) the
physical spatial context has a small but significant influence on the Web
content that indoor users browse; (7) accompanying users tend to access
resources from the same Web domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05632</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05632</id><created>2015-06-18</created><authors><author><keyname>Sweeney</keyname><forenames>Latanya</forenames></author><author><keyname>Crosas</keyname><forenames>Merce</forenames></author></authors><title>An Open Science Platform for the Next Generation of Data</title><categories>cs.CY cs.DL</categories><comments>32 pages, 8 figures</comments><acm-class>H.3.1; H.3.2; H.3.3; H.3.5; H.3.6; H.3.7; H.2.7; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Imagine an online work environment where researchers have direct and
immediate access to myriad data sources and tools and data management
resources, useful throughout the research lifecycle. This is our vision for the
next generation of the Dataverse Network: an Open Science Platform (OSP). For
the first time, researchers would be able to seamlessly access and create
primary and derived data from a variety of sources: prior research results,
public data sets, harvested online data, physical instruments, private data
collections, and even data from other standalone repositories. Researchers
could recruit research participants and conduct research directly on the OSP,
if desired, using readily available tools. Researchers could create private or
shared workspaces to house data, access tools, and computation and could
publish data directly on the platform or publish elsewhere with persistent,
data citations on the OSP. This manuscript describes the details of an Open
Science Platform and its construction. Having an Open Science Platform will
especially impact the rate of new scientific discoveries and make scientific
findings more credible and accountable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05636</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05636</id><created>2015-06-18</created><updated>2015-12-12</updated><authors><author><keyname>Zhao</keyname><forenames>Shiyu</forenames></author><author><keyname>Zelazo</keyname><forenames>Daniel</forenames></author></authors><title>Translational and Scaling Formation Maneuver Control via a Bearing-Based
  Approach</title><categories>cs.SY</categories><comments>Accepted by IEEE Transactions on Control of Network Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies distributed maneuver control of multi-agent formations in
arbitrary dimensions. The objective is to control the translation and scale of
the formation while maintaining the desired formation pattern. Unlike
conventional approaches where the target formation is defined by relative
positions or distances, we propose a novel bearing-based approach where the
target formation is defined by inter-neighbor bearings. Since the bearings are
invariant to the translation and scale of the formation, the bearing-based
approach provides a simple solution to the problem of translational and scaling
formation maneuver control. Linear formation control laws for double-integrator
dynamics are proposed and the global formation stability is analyzed. This
paper also studies bearing-based formation control in the presence of practical
problems including input disturbances, acceleration saturation, and collision
avoidance. The theoretical results are illustrated with numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05644</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05644</id><created>2015-06-18</created><authors><author><keyname>Caruso</keyname><forenames>Xavier</forenames><affiliation>IRMAR</affiliation></author><author><keyname>Roe</keyname><forenames>David</forenames><affiliation>IRMAR</affiliation></author><author><keyname>Vaccon</keyname><forenames>Tristan</forenames><affiliation>IRMAR</affiliation></author></authors><title>p-Adic Stability In Linear Algebra</title><categories>math.NT cs.SC</categories><comments>ISSAC 2015, Jul 2015, Bath, United Kingdom. 2015</comments><proxy>ccsd</proxy><doi>10.1145/2755996.2756655</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the differential precision methods developed previously by the same
authors, we study the p-adic stability of standard operations on matrices and
vector spaces. We demonstrate that lattice-based methods surpass naive methods
in many applications, such as matrix multiplication and sums and intersections
of subspaces. We also analyze determinants , characteristic polynomials and LU
factorization using these differential methods. We supplement our observations
with numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05645</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05645</id><created>2015-06-18</created><authors><author><keyname>Bostan</keyname><forenames>Alin</forenames><affiliation>SPECFUN</affiliation></author><author><keyname>Caruso</keyname><forenames>Xavier</forenames><affiliation>IRMAR</affiliation></author><author><keyname>Schost</keyname><forenames>&#xc9;ric</forenames></author></authors><title>A Fast Algorithm for Computing the p-Curvature</title><categories>cs.SC</categories><comments>ISSAC 2015, Jul 2015, Bath, United Kingdom</comments><proxy>ccsd</proxy><doi>10.1145/2755996.2756674</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design an algorithm for computing the $p$-curvature of a differential
system in positive characteristic $p$. For a system of dimension $r$ with
coefficients of degree at most $d$, its complexity is $\softO (p d r^\omega)$
operations in the ground field (where $\omega$ denotes the exponent of matrix
multiplication), whereas the size of the output is about $p d r^2$. Our
algorithm is then quasi-optimal assuming that matrix multiplication is
(\emph{i.e.} $\omega = 2$). The main theoretical input we are using is the
existence of a well-suited ring of series with divided powers for which an
analogue of the Cauchy--Lipschitz Theorem holds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05659</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05659</id><created>2015-06-18</created><updated>2016-02-16</updated><authors><author><keyname>Arava</keyname><forenames>Radhika</forenames></author></authors><title>An Efficient homophilic model and Algorithms for Community Detection
  using Nash Dynamics</title><categories>cs.SI physics.soc-ph</categories><comments>The paper is not well-written. I would like to update the paper after
  it is published, so that it will be more useful to the community</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of community detection is important as it helps in understanding
the spread of information in a social network. All real complex networks have
an inbuilt structure which captures and characterizes the network dynamics
between its nodes. Linkages are more likely to form between similar nodes,
leading to the formation of some community structure which characterizes the
network dynamic. The more friends they have in common, the more the influence
that each person can exercise on the other.
  We propose a disjoint community detection algorithm, $\textit{NashDisjoint}$
that detects disjoint communities in any given network. We evaluate the
algorithm $\textit{NashDisjoint}$ on the standard LFR benchmarks, and we find
that our algorithm works at least as good as that of the state of the art
algorithms for the mixing factors less than 0.55 in all the cases. We propose
an overlapping community detection algorithm $\textit{NashOverlap}$ to detect
the overlapping communities in any given network. We evaluate the algorithm
$\textit{NashOverlap}$ on the standard LFR benchmarks and we find that our
algorithm works far better than the state of the art algorithms in around 152
different scenarios, generated by varying the number of nodes, mixing factor
and overlapping membership.
  We run our algorithm $\textit{NashOverlap}$ on DBLP dataset to detect the
large collaboration groups and found very interesting results. Also, these
results of our algorithm on DBLP collaboration network are compared with the
results of the $\textit{COPRA}$ algorithm and $\textit{OSLOM}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05661</identifier>
 <datestamp>2015-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05661</id><created>2015-06-18</created><updated>2015-08-12</updated><authors><author><keyname>Palla</keyname><forenames>Gergely</forenames></author><author><keyname>Tib&#xe9;ly</keyname><forenames>Gergely</forenames></author><author><keyname>Mones</keyname><forenames>Enys</forenames></author><author><keyname>Pollner</keyname><forenames>P&#xe9;ter</forenames></author><author><keyname>Vicsek</keyname><forenames>Tam&#xe1;s</forenames></author></authors><title>Hierarchical networks of scientific journals</title><categories>physics.soc-ph cs.DL stat.AP</categories><journal-ref>Palgrave Communications 1, 15016 (2015)</journal-ref><doi>10.1057/palcomms.2015.16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific journals are the repositories of the gradually accumulating
knowledge of mankind about the world surrounding us. Just as our knowledge is
organised into classes ranging from major disciplines, subjects and fields to
increasingly specific topics, journals can also be categorised into groups
using various metrics. In addition to the set of topics characteristic for a
journal, they can also be ranked regarding their relevance from the point of
overall influence. One widespread measure is impact factor, but in the present
paper we intend to reconstruct a much more detailed description by studying the
hierarchical relations between the journals based on citation data. We use a
measure related to the notion of m-reaching centrality and find a network which
shows the level of influence of a journal from the point of the direction and
efficiency with which information spreads through the network. We can also
obtain an alternative network using a suitably modified nested hierarchy
extraction method applied to the same data. The results are weakly
methodology-dependent and reveal non-trivial relations among journals. The two
alternative hierarchies show large similarity with some striking differences,
providing together a complex picture of the intricate relations between
scientific journals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05662</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05662</id><created>2015-06-18</created><updated>2015-09-16</updated><authors><author><keyname>Bonnabel</keyname><forenames>Silv&#xe8;re</forenames></author><author><keyname>Barrau</keyname><forenames>Axel</forenames></author></authors><title>An intrinsic Cram\'er-Rao bound on Lie groups</title><categories>cs.SY</categories><comments>To appear in the conference Geometric Sciences of Information GSI15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In his 2005 paper, S.T. Smith proposed an intrinsic Cram\'er-Rao bound on the
variance of estimators of a parameter defined on a Riemannian manifold. In the
present technical note, we consider the special case where the parameter lives
in a Lie group. In this case, by choosing, e.g., the right invariant metric,
parallel transport becomes very simple, which allows a more straightforward and
natural derivation of the bound in terms of Lie bracket, albeit for a slightly
different definition of the estimation error. For bi-invariant metrics, the Lie
group exponential map we use to define the estimation error, and the Riemannian
exponential map used by S.T. Smith coincide, and we prove in this case that
both results are identical indeed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05671</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05671</id><created>2015-06-18</created><updated>2015-06-29</updated><authors><author><keyname>Brain</keyname><forenames>Martin</forenames></author><author><keyname>Joshi</keyname><forenames>Saurabh</forenames></author><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author><author><keyname>Schrammel</keyname><forenames>Peter</forenames></author></authors><title>Safety Verification and Refutation by k-invariants and k-induction
  (extended version)</title><categories>cs.LO cs.SE</categories><comments>extended version of paper published at SAS'15</comments><acm-class>F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most software verification tools can be classified into one of a number of
established families, each of which has their own focus and strengths. For
example, concrete counterexample generation in model checking, invariant
inference in abstract interpretation and completeness via annotation for
deductive verification. This creates a significant and fundamental usability
problem as users may have to learn and use one technique to find potential
problems but then need an entirely different one to show that they have been
fixed. This paper presents a single, unified algorithm kIkI, which strictly
generalises abstract interpretation, bounded model checking and k-induction.
This not only combines the strengths of these techniques but allows them to
interact and reinforce each other, giving a `single-tool' approach to
verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05672</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05672</id><created>2015-06-18</created><authors><author><keyname>Dulisch</keyname><forenames>Nadine</forenames></author><author><keyname>Kempf</keyname><forenames>Andreas Oskar</forenames></author><author><keyname>Schaer</keyname><forenames>Philipp</forenames></author></authors><title>Query Expansion for Survey Question Retrieval in the Social Sciences</title><categories>cs.DL cs.IR</categories><comments>to appear in Proceedings of 19th International Conference on Theory
  and Practice of Digital Libraries 2015 (TPDL 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, the importance of research data and the need to archive and
to share it in the scientific community have increased enormously. This
introduces a whole new set of challenges for digital libraries. In the social
sciences typical research data sets consist of surveys and questionnaires. In
this paper we focus on the use case of social science survey question reuse and
on mechanisms to support users in the query formulation for data sets. We
describe and evaluate thesaurus- and co-occurrence-based approaches for query
expansion to improve retrieval quality in digital libraries and research data
archives. The challenge here is to translate the information need and the
underlying sociological phenomena into proper queries. As we can show retrieval
quality can be improved by adding related terms to the queries. In a direct
comparison automatically expanded queries using extracted co-occurring terms
can provide better results than queries manually reformulated by a domain
expert and better results than a keyword-based BM25 baseline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05673</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05673</id><created>2015-06-18</created><authors><author><keyname>Bl&#xe4;sius</keyname><forenames>Thomas</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>A New Perspective on Clustered Planarity as a Combinatorial Embedding
  Problem</title><categories>cs.DS cs.DM math.CO</categories><comments>17 pages, 2 figures</comments><acm-class>G.2.1; G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The clustered planarity problem (c-planarity) asks whether a hierarchically
clustered graph admits a planar drawing such that the clusters can be nicely
represented by regions. We introduce the cd-tree data structure and give a new
characterization of c-planarity. It leads to efficient algorithms for
c-planarity testing in the following cases. (i) Every cluster and every
co-cluster (complement of a cluster) has at most two connected components. (ii)
Every cluster has at most five outgoing edges.
  Moreover, the cd-tree reveals interesting connections between c-planarity and
planarity with constraints on the order of edges around vertices. On one hand,
this gives rise to a bunch of new open problems related to c-planarity, on the
other hand it provides a new perspective on previous results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05676</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05676</id><created>2015-06-17</created><authors><author><keyname>Marsik</keyname><forenames>Jiri</forenames><affiliation>SEMAGRAMME</affiliation></author><author><keyname>Amblard</keyname><forenames>Maxime</forenames><affiliation>SEMAGRAMME</affiliation></author></authors><title>Pragmatic Side Effects</title><categories>cs.CL</categories><comments>Redrawing Pragmasemantic Borders, Mar 2015, Groningen, Netherlands.
  https://sites.google.com/site/redraw2015/</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the quest to give a formal compositional semantics to natural languages,
semanticists have started turning their attention to phenomena that have been
also considered as parts of pragmatics (e.g., discourse anaphora and
presupposition projection). To account for these phenomena, the very kinds of
meanings assigned to words and phrases are often revisited. To be more
specific, in the prevalent paradigm of modeling natural language denotations
using the simply-typed lambda calculus (higher-order logic) this means
revisiting the types of denotations assigned to individual parts of speech.
However, the lambda calculus also serves as a fundamental theory of
computation, and in the study of computation, similar type shifts have been
employed to give a meaning to side effects. Side effects in programming
languages correspond to actions that go beyond the lexical scope of an
expression (a thrown exception might propagate throughout a program, a variable
modified at one point might later be read at an another) or even beyond the
scope of the program itself (a program might interact with the outside world by
e.g., printing documents, making sounds, operating robotic limbs...).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05677</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05677</id><created>2015-06-18</created><authors><author><keyname>Bern&#xe1;th</keyname><forenames>Attila</forenames></author><author><keyname>Pap</keyname><forenames>Gyula</forenames></author></authors><title>Blocking optimal arborescences</title><categories>math.CO cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of covering minimum cost common bases of two matroids is
NP-complete, even if the two matroids coincide, and the costs are all equal to
1. In this paper we show that the following special case is solvable in
polynomial time: given a digraph $D=(V,A)$ with a designated root node $r\in V$
and arc-costs $c:A\to \mathbb{R}$, find a minimum cardinality subset $H$ of the
arc set $A$ such that $H$ intersects every minimum $c$-cost $r$-arborescence.
By an $r$-arborescence we mean a spanning arborescence of root $r$. The
algorithm we give solves a weighted version as well, in which a nonnegative
weight function $w:A\to \mathbb{R}_+$ (unrelated to $c$) is also given, and we
want to find a subset $H$ of the arc set such that $H$ intersects every minimum
$c$-cost $r$-arborescence, and $w(H)=\sum_{a\in H}w(a)$ is minimum. The running
time of the algorithm is $O(n^3T(n,m))$, where $n$ and $m$ denote the number of
nodes and arcs of the input digraph, and $T(n,m)$ is the time needed for a
minimum $s-t$ cut computation in this digraph. A polyhedral description is not
given, and seems rather challenging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05690</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05690</id><created>2015-06-18</created><authors><author><keyname>Silva</keyname><forenames>Filipi N.</forenames></author><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author><author><keyname>Bardosova</keyname><forenames>Maria</forenames></author><author><keyname>Oliveira</keyname><forenames>Osvaldo N.</forenames><suffix>Jr.</suffix></author><author><keyname>Costa</keyname><forenames>Luciano da F.</forenames></author></authors><title>Using network science and text analytics to produce surveys in a
  scientific topic</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of science to understand its own structure is becoming popular, but
understanding the organization of knowledge areas is still limited because some
patterns are only discoverable with proper computational treatment of
large-scale datasets. In this paper, we introduce a network-based methodology
combined with text analytics to construct the taxonomy of science fields. The
methodology is illustrated with application to two topics: complex networks
(CN) and photonic crystals (PC). We built citation networks using data from the
Web of Science and used a community detection algorithm for partitioning to
obtain science maps of the fields considered. We also created an importance
index for text analytics in order to obtain keywords that define the
communities. A dendrogram of the relatedness among the subtopics was also
obtained. Among the interesting patterns that emerged from the analysis, we
highlight the identification of two well-defined communities in PC area, which
is consistent with the known existence of two distinct communities of
researchers in the area: telecommunication engineers and physicists. With the
methodology, it was also possible to assess the interdisciplinary and time
evolution of subtopics defined by the keywords. The automatic tools described
here are potentially useful not only to provide an overview of scientific areas
but also to assist scientists in performing systematic research on a specific
topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05692</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05692</id><created>2015-06-18</created><authors><author><keyname>Gasse</keyname><forenames>Maxime</forenames><affiliation>DM2L</affiliation></author><author><keyname>Aussem</keyname><forenames>Alex</forenames><affiliation>DM2L</affiliation></author><author><keyname>Elghazel</keyname><forenames>Haytham</forenames><affiliation>DM2L</affiliation></author></authors><title>A hybrid algorithm for Bayesian network structure learning with
  application to multi-label learning</title><categories>stat.ML cs.AI cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1101.5184 by other authors</comments><proxy>ccsd</proxy><journal-ref>Expert Systems with Applications, Elsevier, 2014, 41 (15),
  pp.6755-6772</journal-ref><doi>10.1016/j.eswa.2014.04.032</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel hybrid algorithm for Bayesian network structure learning,
called H2PC. It first reconstructs the skeleton of a Bayesian network and then
performs a Bayesian-scoring greedy hill-climbing search to orient the edges.
The algorithm is based on divide-and-conquer constraint-based subroutines to
learn the local structure around a target variable. We conduct two series of
experimental comparisons of H2PC against Max-Min Hill-Climbing (MMHC), which is
currently the most powerful state-of-the-art algorithm for Bayesian network
structure learning. First, we use eight well-known Bayesian network benchmarks
with various data sizes to assess the quality of the learned structure returned
by the algorithms. Our extensive experiments show that H2PC outperforms MMHC in
terms of goodness of fit to new data and quality of the network structure with
respect to the true dependence structure of the data. Second, we investigate
H2PC's ability to solve the multi-label learning problem. We provide
theoretical results to characterize and identify graphically the so-called
minimal label powersets that appear as irreducible factors in the joint
distribution under the faithfulness condition. The multi-label learning problem
is then decomposed into a series of multi-class classification problems, where
each multi-class variable encodes a label powerset. H2PC is shown to compare
favorably to MMHC in terms of global classification accuracy over ten
multi-label data sets covering different application domains. Overall, our
experiments support the conclusions that local structural learning with H2PC in
the form of local neighborhood induction is a theoretically well-motivated and
empirically effective learning framework that is well suited to multi-label
learning. The source code (in R) of H2PC as well as all data sets used for the
empirical tests are publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05693</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05693</id><created>2015-06-18</created><authors><author><keyname>Smail</keyname><forenames>Omar</forenames><affiliation>UR1, ATNET</affiliation></author><author><keyname>Cousin</keyname><forenames>Bernard</forenames><affiliation>UR1, ATNET</affiliation></author><author><keyname>Mekkakia</keyname><forenames>Zoulikha</forenames></author><author><keyname>Mekki</keyname><forenames>Rachida</forenames></author></authors><title>A multipath energy-conserving routing protocol for wireless ad hoc
  networks lifetime improvement</title><categories>cs.NI</categories><proxy>ccsd</proxy><journal-ref>EURASIP Journal on Wireless Communications and Networking,
  SpringerOpen, 2014, 2014 (139), pp. 1 - 22</journal-ref><doi>10.1186/1687-1499-2014-139</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ad hoc networks are wireless mobile networks that can operate without
infrastructure and without centralized network management. Traditional
techniques of routing are not well adapted. Indeed, their lack of reactivity
with respect to the variability of network changes makes them difficult to use.
Moreover, conserving energy is a critical concern in the design of routing
protocols for ad hoc networks, because most mobile nodes operate with limited
battery capacity, and the energy depletion of a node affects not only the node
itself but also the overall network lifetime. In all proposed single-path
routing schemes a new path-discovery process is required once a path failure is
detected, and this process causes delay and wastage of node resources. A
multipath routing scheme is an alternative to maximize the network lifetime. In
this paper, we propose an energy-efficient multipath routing protocol, called
AOMR-LM (Ad hoc On-demand Multipath Routing with Lifetime Maximization), which
preserves the residual energy of nodes and balances the consumed energy to
increase the network lifetime. To achieve this goal, we used the residual
energy of nodes for calculating the node energy level. The multipath selection
mechanism uses this energy level to classify the paths. Two parameters are
analyzed: the energy threshold beta and the coefficient alpha. These parameters
are required to classify the nodes and to ensure the preservation of node
energy. Our protocol improves the performance of mobile ad hoc networks by
prolonging the lifetime of the network. This novel protocol has been compared
with other protocols: AOMDV and ZD-AOMDV. The protocol performance has been
evaluated in terms of network lifetime, energy consumption, and end-to-end
delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05702</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05702</id><created>2015-06-18</created><updated>2015-07-28</updated><authors><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author></authors><title>Comparing the writing style of real and artificial papers</title><categories>cs.CL</categories><comments>To appear in Scientometrics (2015)</comments><journal-ref>Scientometrics 105 (3), (2015) pp. 1763-1779</journal-ref><doi>10.1007/s11192-015-1637-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have witnessed the increase of competition in science. While
promoting the quality of research in many cases, an intense competition among
scientists can also trigger unethical scientific behaviors. To increase the
total number of published papers, some authors even resort to software tools
that are able to produce grammatical, but meaningless scientific manuscripts.
Because automatically generated papers can be misunderstood as real papers, it
becomes of paramount importance to develop means to identify these scientific
frauds. In this paper, I devise a methodology to distinguish real manuscripts
from those generated with SCIGen, an automatic paper generator. Upon modeling
texts as complex networks (CN), it was possible to discriminate real from fake
papers with at least 89\% of accuracy. A systematic analysis of features
relevance revealed that the accessibility and betweenness were useful in
particular cases, even though the relevance depended upon the dataset. The
successful application of the methods described here show, as a proof of
principle, that network features can be used to identify scientific gibberish
papers. In addition, the CN-based approach can be combined in a straightforward
fashion with traditional statistical language processing methods to improve the
performance in identifying artificially generated papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05703</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05703</id><created>2015-06-18</created><authors><author><keyname>Lebret</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Collobert</keyname><forenames>Ronan</forenames></author></authors><title>&quot;The Sum of Its Parts&quot;: Joint Learning of Word and Phrase
  Representations with Autoencoders</title><categories>cs.CL</categories><comments>Deep Learning Workshop, ICML 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been a lot of effort to represent words in continuous
vector spaces. Those representations have been shown to capture both semantic
and syntactic information about words. However, distributed representations of
phrases remain a challenge. We introduce a novel model that jointly learns word
vector representations and their summation. Word representations are learnt
using the word co-occurrence statistical information. To embed sequences of
words (i.e. phrases) with different sizes into a common semantic space, we
propose to average word vector representations. In contrast with previous
methods which reported a posteriori some compositionality aspects by simple
summation, we simultaneously train words to sum, while keeping the maximum
information from the original vectors. We evaluate the quality of the word
representations on several classical word evaluation tasks, and we introduce a
novel task to evaluate the quality of the phrase representations. While our
distributed representations compete with other methods of learning word
representations on word evaluations, we show that they give better performance
on the phrase evaluation. Such representations of phrases could be interesting
for many tasks in natural language processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05713</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05713</id><created>2015-06-18</created><authors><author><keyname>Ji</keyname><forenames>Zhijian</forenames></author><author><keyname>Chen</keyname><forenames>Tongwen</forenames></author><author><keyname>Yu</keyname><forenames>Haisheng</forenames></author></authors><title>Destructive nodes in multi-agent controllability</title><categories>cs.SY</categories><comments>27 pages</comments><acm-class>I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, several necessary and sufficient graphical conditions are
derived for the controllability of multi-agent systems by taking advantage of
the proposed concept of controllability destructive nodes. A key step of
arriving at this result is the establishment of a relationship between topology
structures of the controllability destructive nodes and a specific eigenvector
of the Laplacian matrix. The results on double, triple and quadruple
controllability destructive nodes constitute a novel approach to study the
controllability. In particular, the approach is applied to the graph consisting
of five nodes to get a complete graphical characterization of controllability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05715</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05715</id><created>2015-06-18</created><authors><author><keyname>Bl&#xe4;sius</keyname><forenames>Thomas</forenames></author><author><keyname>Karrer</keyname><forenames>Annette</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>Simultaneous Embedding: Edge Orderings, Relative Positions, Cutvertices</title><categories>cs.DS cs.DM math.CO</categories><comments>64 pages, 20 figures</comments><acm-class>G.2.1; G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simultaneous embedding (with fixed edges) of two graphs $G^1$ and $G^2$
with common graph $G=G^1 \cap G^2$ is a pair of planar drawings of $G^1$ and
$G^2$ that coincide on $G$. It is an open question whether there is a
polynomial-time algorithm that decides whether two graphs admit a simultaneous
embedding (problem SEFE).
  In this paper, we present two results. First, a set of three linear-time
preprocessing algorithms that remove certain substructures from a given SEFE
instance, producing a set of equivalent SEFE instances without such
substructures. The structures we can remove are (1) cutvertices of the union
graph $G^\cup = G^1 \cup G^2$, (2) most separating pairs of $G^\cup$, and (3)
connected components of $G$ that are biconnected but not a cycle.
  Second, we give an $O(n^3)$-time algorithm solving SEFE for instances with
the following restriction. Let $u$ be a pole of a P-node $\mu$ in the SPQR-tree
of a block of $G^1$ or $G^2$. Then at most three virtual edges of $\mu$ may
contain common edges incident to $u$. All algorithms extend to the sunflower
case, i.e., to the case of more than three graphs pairwise intersecting in the
same common graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05719</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05719</id><created>2015-06-18</created><updated>2015-06-24</updated><authors><author><keyname>Fraser</keyname><forenames>Dallas J.</forenames></author><author><keyname>Hamel</keyname><forenames>Ang&#xe8;le M.</forenames></author><author><keyname>Ho&#xe0;ng</keyname><forenames>Ch&#xed;nh T.</forenames></author></authors><title>A Coloring Algorithm for $4K_1$-free line graphs</title><categories>math.CO cs.DM</categories><comments>15 pages; updated a definition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $L$ be a set of graphs. $Free$($L$) is the set of graphs that do not
contain any graph in $L$ as an induced subgraph. It is known that if $L$ is a
set of four-vertex graphs, then the complexity of the coloring problem for
$Free$($L$) is known with three exceptions: $L $= {claw, $4K_1$}, $L$ = {claw,
$4K_1$, co-diamond}, and $L$ = {$C_4$, $4K_1$}. In this paper, we study the
coloring problem for $Free$(claw, $4K_1$). We solve the coloring problem for a
subclass of $Free$(claw, $4K_1$) which contains the class of $4K_1$-free line
graphs. Our result implies the chromatic index of a graph with no matching of
size four can be computed in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05721</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05721</id><created>2015-06-18</created><authors><author><keyname>Chen</keyname><forenames>Lin</forenames></author><author><keyname>Megow</keyname><forenames>Nicole</forenames></author><author><keyname>Schewior</keyname><forenames>Kevin</forenames></author></authors><title>An O(m^2 log m)-Competitive Algorithm for Online Machine Minimization</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the online machine minimization problem in which jobs with hard
deadlines arrive online over time at their release dates. The task is to
determine a feasible schedule on a minimum number of machines. Our main result
is a general O(m^2 log m)-competitive algorithm for the preemptive online
problem, where m is the optimal number of machines used in an offline solution.
This is the first improvement on an O(log (p_max/p_min))-competitive algorithm
by Phillips et al. (STOC 1997), which was to date the best known algorithm even
when m=2. Our algorithm is O(1)-competitive for any m that is bounded by a
constant. To develop the algorithm, we investigate two complementary special
cases of the problem, namely, laminar instances and agreeable instances, for
which we provide an O(log m)-competitive and an O(1)-competitive algorithm,
respectively. Our O(1)-competitive algorithm for agreeable instances actually
produces a non-preemptive schedule, which is of its own interest as there
exists a strong lower bound of n, the number of jobs, for the general
non-preemptive online machine minimization problem by Saha (FSTTCS 2013), which
even holds for laminar instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05728</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05728</id><created>2015-06-18</created><authors><author><keyname>Colange</keyname><forenames>Maximilien</forenames></author><author><keyname>Racordon</keyname><forenames>Dimitri</forenames></author><author><keyname>Buchs</keyname><forenames>Didier</forenames></author></authors><title>A CEGAR-like Approach for Cost LTL Bounds</title><categories>cs.LO</categories><comments>17 pages</comments><acm-class>F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Qualitative formal verification, that seeks boolean answers about the
behavior of a system, is often insufficient for practical purposes. Observing
quantitative information is of interest, e.g. for the proper calibration of a
battery or a real-time scheduler. Historically, the focus has been on
quantities in a continuous domain, but recent years showed a renewed interest
for discrete quantitative domains.
  Cost Linear Temporal Logic (CLTL) is a quantitative extension of classical
LTL. It integrates into a nice theory developed in the past few years that
extends the qualitative setting, with counterparts in terms of logics, automata
and algebraic structure. We propose a practical usage of this logics for
model-checking purposes. A CLTL formula defines a function from infinite words
to integers. Finding the bounds of such a function over a given set of words
can be seen as an extension of LTL universal and existential model-checking. We
propose a CEGAR-like algorithm to find these bounds by relying on classical LTL
model-checking, and use B\&quot;{u}chi automata with counters to implement it. This
method constitutes a first step towards the practical use of such a discrete
quantitative logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05751</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05751</id><created>2015-06-18</created><authors><author><keyname>Denton</keyname><forenames>Emily</forenames></author><author><keyname>Chintala</keyname><forenames>Soumith</forenames></author><author><keyname>Szlam</keyname><forenames>Arthur</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author></authors><title>Deep Generative Image Models using a Laplacian Pyramid of Adversarial
  Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a generative parametric model capable of producing
high quality samples of natural images. Our approach uses a cascade of
convolutional networks within a Laplacian pyramid framework to generate images
in a coarse-to-fine fashion. At each level of the pyramid, a separate
generative convnet model is trained using the Generative Adversarial Nets (GAN)
approach (Goodfellow et al.). Samples drawn from our model are of significantly
higher quality than alternate approaches. In a quantitative assessment by human
evaluators, our CIFAR10 samples were mistaken for real images around 40% of the
time, compared to 10% for samples drawn from a GAN baseline model. We also show
samples from models trained on the higher resolution images of the LSUN scene
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05752</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05752</id><created>2015-06-18</created><updated>2015-06-23</updated><authors><author><keyname>Yang</keyname><forenames>Zhihai</forenames></author></authors><title>Detecting Abnormal Profiles in Collaborative Filtering Recommender
  Systems</title><categories>cs.IR</categories><comments>13 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:1506.04584, arXiv:1506.05247</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Personalization collaborative filtering recommender systems (CFRSs) are the
crucial components of popular e-commerce services. In practice, CFRSs are also
particularly vulnerable to &quot;shilling&quot; attacks or &quot;profile injection&quot; attacks
due to their openness. The attackers can carefully inject chosen attack
profiles into CFRSs in order to bias the recommendation results to their
benefits. To reduce this risk, various detection techniques have been proposed
to detect such attacks, which use diverse features extracted from user
profiles. However, relying on limited features to improve the detection
performance is difficult seemingly, since the existing features can not fully
characterize the attack profiles and genuine profiles. In this paper, we
propose a novel detection method to make recommender systems resistant to the
&quot;shilling&quot; attacks or &quot;profile injection&quot; attacks. The existing features can be
briefly summarized as two aspects including rating behavior based and item
distribution based. We firstly formulate the problem as finding a mapping model
between rating behavior and item distribution by exploiting the least-squares
approximate solution. Based on the trained model, we design a detector by
employing a regressor to detect such attacks. Extensive experiments on both the
MovieLens-100K and MovieLens-ml-latest-small datasets examine the effectiveness
of our proposed detection method. Experimental results were included to
validate the outperformance of our approach in comparison with benchmarked
method including KNN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05754</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05754</id><created>2015-06-18</created><authors><author><keyname>Silva</keyname><forenames>Luciana</forenames></author><author><keyname>Felix</keyname><forenames>Daniel</forenames></author><author><keyname>Valente</keyname><forenames>Marco Tulio</forenames></author><author><keyname>Maia</keyname><forenames>Marcelo</forenames></author></authors><title>ModularityCheck: A Tool for Assessing Modularity using Co-Change
  Clusters</title><categories>cs.SE</categories><journal-ref>V Brazilian Conference on Software: Theory and Practice (Tools
  Track), p. 1-8, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is widely accepted that traditional modular structures suffer from the
dominant decomposition problem. Therefore, to improve current modularity views,
it is important to investigate the impact of design decisions concerning
modularity in other dimensions, as the evolutionary view. In this paper, we
propose the ModularityCheck tool to assess package modularity using co-change
clusters, which are sets of classes that usually changed together in the past.
Our tool extracts information from version control platforms and issue reports,
retrieves co-change clusters, generates metrics related to co-change clusters,
and provides visualizations for assessing modularity. We also provide a case
study to evaluate the tool. http://youtu.be/7eBYa2dfIS8
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05770</identifier>
 <datestamp>2015-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05770</id><created>2015-06-18</created><authors><author><keyname>Carvalho</keyname><forenames>Joao</forenames></author><author><keyname>Pequito</keyname><forenames>Sergio</forenames></author><author><keyname>Aguiar</keyname><forenames>A. Pedro</forenames></author><author><keyname>Kar</keyname><forenames>Soummya</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>Distributed Verification of Structural Controllability for Linear
  Time-Invariant Systems</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the development and deployment of large-scale dynamical systems,
often composed of geographically distributed smaller subsystems, we address the
problem of verifying their controllability in a distributed manner. In this
work we study controllability in the structural system theoretic sense,
structural controllability. In other words, instead of focusing on a specific
numerical system realization, we provide guarantees for equivalence classes of
linear time-invariant systems on the basis of their structural sparsity
patterns, i.e., location of zero/nonzero entries in the plant matrices. To this
end, we first propose several necessary and/or sufficient conditions to ensure
structural controllability of the overall system, on the basis of the
structural patterns of the subsystems and their interconnections. The proposed
verification criteria are shown to be efficiently implementable (i.e., with
polynomial time complexity in the number of the state variables and inputs) in
two important subclasses of interconnected dynamical systems: similar (i.e.,
every subsystem has the same structure), and serial (i.e., every subsystem
outputs to at most one other subsystem). Secondly, we provide a distributed
algorithm to verify structural controllability for interconnected dynamical
systems. The proposed distributed algorithm is efficient and implementable at
the subsystem level; the algorithm is iterative, based on communication among
(physically) interconnected subsystems, and requires only local model and
interconnection knowledge at each subsystem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05783</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05783</id><created>2015-06-18</created><updated>2015-11-25</updated><authors><author><keyname>Pontecorvi</keyname><forenames>Matteo</forenames></author><author><keyname>Ramachandran</keyname><forenames>Vijaya</forenames></author></authors><title>A Faster Algorithm for Fully Dynamic Betweenness Centrality</title><categories>cs.DS</categories><comments>The current revision (v3) includes minor changes in the Introduction.
  There is no change to the main result. A brief summary of this paper will
  appear in Proc. ISAAC 2015, in a paper by the authors entitled &quot;Fully Dynamic
  Betweenness Centrality''. arXiv admin note: text overlap with arXiv:1412.3852</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new fully dynamic algorithm for maintaining betweenness
centrality (BC) of vertices in a directed graph $G=(V,E)$ with positive edge
weights. BC is a widely used parameter in the analysis of large complex
networks. We achieve an amortized $O((\nu^*)^2 \log^2 n)$ time per update,
where $n = |V| $ and $\nu^*$ bounds the number of distinct edges that lie on
shortest paths through any single vertex. This result improves on the amortized
bound for fully dynamic BC in [Pontecorvi-Ramachandran2015] by a logarithmic
factor. Our algorithm uses new data structures and techniques that are
extensions of the method in the fully dynamic algorithm in Thorup [Thorup2004]
for APSP in graphs with unique shortest paths. For graphs with $\nu^* = O(n)$,
our algorithm matches the fully dynamic APSP bound in [Thorup2004], which holds
for graphs with $\nu^* = n-1$, since it assumes unique shortest paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05790</identifier>
 <datestamp>2015-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05790</id><created>2015-06-18</created><updated>2015-11-10</updated><authors><author><keyname>Balsubramani</keyname><forenames>Akshay</forenames></author><author><keyname>Freund</keyname><forenames>Yoav</forenames></author></authors><title>Scalable Semi-Supervised Aggregation of Classifiers</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present and empirically evaluate an efficient algorithm that learns to
aggregate the predictions of an ensemble of binary classifiers. The algorithm
uses the structure of the ensemble predictions on unlabeled data to yield
significant performance improvements. It does this without making assumptions
on the structure or origin of the ensemble, without parameters, and as scalably
as linear learning. We empirically demonstrate these performance gains with
random forests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05819</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05819</id><created>2015-06-18</created><authors><author><keyname>Yadgar</keyname><forenames>Ron</forenames></author><author><keyname>Cohen</keyname><forenames>Asaf</forenames></author><author><keyname>Gurewitz</keyname><forenames>Omer</forenames></author></authors><title>Asymptotic Analysis for Reliable Data Dissemination in Shared loss
  Multicast Trees</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The completion time for the dissemination (or alternatively, aggregation) of
information from all nodes in a network plays a critical role in the design and
analysis of communication systems, especially in real time applications for
which delay is critical. In this work, we analyse the completion time of data
dissemination in a shared loss (i.e., unreliable links) multicast tree, at the
limit of large number of nodes. Specifically, analytic expressions for upper
and lower bounds on the expected completion time are provided, and, in
particular, it is shown that both these bounds scale as $\alpha \log n$. For
example, on a full binary tree with $n$ end users, and packet loss probability
of $0.1$, we bound the expected completion time for disseminating one packet
from below by $1.41 \log_2 n+o \left( \log n \right)$ and from above by $1.78
\log_2 n+o \left( \log n \right)$.
  Clearly, the completion time is determined by the last end user who receives
the message, that is, a maximum over all arrival times. Hence, Extreme Value
Theory (EVT) is an appropriate tool to explore this problem. However, since
arrival times are correlated, non-stationary, and furthermore, time slots are
discrete, a thorough study of EVT for Non-Stationary Integer Valued (NSIV)
sequences is required. To the best of our knowledge, such processes were not
studied before in the framework of EVT. Consequently, we derive the asymptotic
distribution of the maxima of NSIV sequences satisfying certain conditions, and
give EVT results which are applicable also beyond the scope of this work. These
result are then used to derive tight bounds on the completion time. Finally,
the results are validated by extensive simulations and numerical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05846</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05846</id><created>2015-06-18</created><authors><author><keyname>Emmons</keyname><forenames>John</forenames></author><author><keyname>Johnson</keyname><forenames>Steven</forenames></author><author><keyname>Urness</keyname><forenames>Timothy</forenames></author><author><keyname>Kilpatrick</keyname><forenames>Adina</forenames></author></authors><title>Automated Assignment of Backbone NMR Data using Artificial Intelligence</title><categories>cs.AI q-bio.BM</categories><comments>Midwest Instruction and Computing Symposium (MICS 2013); 5 pages
  including figures. John Emmons, Steven Johnson, Timothy Urness, and Adina
  Kilpatrick. &quot;Automated Assignment Of Backbone NMR Data using Artificial
  Intelligence&quot;. La Crosse, Wisconsin, April 2013. Midwest Instruction and
  Computing Symposium (MICS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nuclear magnetic resonance (NMR) spectroscopy is a powerful method for the
investigation of three-dimensional structures of biological molecules such as
proteins. Determining a protein structure is essential for understanding its
function and alterations in function which lead to disease. One of the major
challenges of the post-genomic era is to obtain structural and functional
information on the many unknown proteins encoded by thousands of newly
identified genes. The goal of this research is to design an algorithm capable
of automating the analysis of backbone protein NMR data by implementing AI
strategies such as greedy and A* search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05849</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05849</id><created>2015-06-18</created><authors><author><keyname>Wu</keyname><forenames>Xundong</forenames></author></authors><title>An Iterative Convolutional Neural Network Algorithm Improves Electron
  Microscopy Image Segmentation</title><categories>cs.NE cs.LG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  To build the connectomics map of the brain, we developed a new algorithm that
can automatically refine the Membrane Detection Probability Maps (MDPM)
generated to perform automatic segmentation of electron microscopy (EM) images.
To achieve this, we executed supervised training of a convolutional neural
network to recover the removed center pixel label of patches sampled from a
MDPM. MDPM can be generated from other machine learning based algorithms
recognizing whether a pixel in an image corresponds to the cell membrane. By
iteratively applying this network over MDPM for multiple rounds, we were able
to significantly improve membrane segmentation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05851</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05851</id><created>2015-06-18</created><authors><author><keyname>Xu</keyname><forenames>Jian</forenames></author><author><keyname>Lee</keyname><forenames>Kuang-chih</forenames></author><author><keyname>Li</keyname><forenames>Wentong</forenames></author><author><keyname>Qi</keyname><forenames>Hang</forenames></author><author><keyname>Lu</keyname><forenames>Quan</forenames></author></authors><title>Smart Pacing for Effective Online Ad Campaign Optimization</title><categories>cs.AI cs.GT</categories><comments>KDD'15, August 10-13, 2015, Sydney, NSW, Australia</comments><doi>10.1145/2783258.2788615</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In targeted online advertising, advertisers look for maximizing campaign
performance under delivery constraint within budget schedule. Most of the
advertisers typically prefer to impose the delivery constraint to spend budget
smoothly over the time in order to reach a wider range of audiences and have a
sustainable impact. Since lots of impressions are traded through public
auctions for online advertising today, the liquidity makes price elasticity and
bid landscape between demand and supply change quite dynamically. Therefore, it
is challenging to perform smooth pacing control and maximize campaign
performance simultaneously. In this paper, we propose a smart pacing approach
in which the delivery pace of each campaign is learned from both offline and
online data to achieve smooth delivery and optimal performance goals. The
implementation of the proposed approach in a real DSP system is also presented.
Experimental evaluations on both real online ad campaigns and offline
simulations show that our approach can effectively improve campaign performance
and achieve delivery goals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05855</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05855</id><created>2015-06-18</created><updated>2016-01-08</updated><authors><author><keyname>LaMont</keyname><forenames>Colin H.</forenames></author><author><keyname>Wiggins</keyname><forenames>Paul A.</forenames></author></authors><title>The Frequentist Information Criterion (FIC): The unification of
  information-based and frequentist inference</title><categories>stat.ML cs.LG physics.data-an</categories><comments>7 Pages, 3 figures, &amp; Appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The failure of the information-based Akaike Information Criterion (AIC) in
the context of singular models can be rectified by the definition of a
Frequentist Information Criterion (FIC). FIC applies a frequentist
approximation to the computation of the model complexity, which can be
estimated analytically in many contexts. Like AIC, FIC can be understood as an
unbiased estimator of the model predictive performance and is therefore
identical to AIC for regular models in the large-observation-number limit
($N\rightarrow \infty$) . In the presence of unidentifiable parameters, the
complexity exhibits a more general, non-AIC-like scaling ($\gg N^0$). For
instance, both BIC-like ($\propto\log N$) and Hannan-Quinn-like ($\propto \log
\log N$) scaling with observation number $N$ are observed. Unlike the Bayesian
model selection approach, FIC is free from {\it ad hoc} prior probability
distributions and appears to be widely applicable to model selection problems.
Finally we demonstrate that FIC (information-based inference) is equivalent to
frequentist inference for an important class of models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05857</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05857</id><created>2015-06-18</created><authors><author><keyname>Mohamed</keyname><forenames>Ehab Mahmoud</forenames></author><author><keyname>Kusano</keyname><forenames>Hideyuki</forenames></author><author><keyname>Sakaguchi</keyname><forenames>Kei</forenames></author><author><keyname>Sampei</keyname><forenames>Seiichi</forenames></author></authors><title>WiFi Assisted Multi-WiGig AP Coordination for Future Multi-Gbps WLANs</title><categories>cs.NI</categories><comments>6 pages, 8 Figures, IEEE International Symposium on Personal Indoor
  and Mobile Radio Communications (PIMRC) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Gigabit (WiGig) access points (APs) using 60 GHz unlicensed
frequency band are considered as key enablers for future Gbps wireless local
area networks (WLANs). Exhaustive search analog beamforming (BF) is mainly used
with WiGig transmissions to overcome channel propagation loss and accomplish
high rate data transmissions. Due to its short range transmission with high
susceptibility to path blocking, a multiple number of WiGig APs should be
installed to fully cover a typical target environment. Therefore, coordination
among the installed APs is highly needed for enabling WiGig concurrent
transmissions while overcoming packet collisions and reducing interference,
which highly increases the total throughput of WiGig WLANs. In this paper, we
propose a comprehensive architecture for coordinated WiGig WLANs. The proposed
WiGig WLAN is based on a tight coordination between the 5 GHz (WiFi) and the 60
GHz (WiGig) unlicensed frequency bands. By which, the wide coverage WiFi band
is used to do the signaling required for organizing WiGig concurrent data
transmissions using control/user (C/U) plane splitting. To reduce interference
to existing WiGig data links while doing BF, a novel location based BF
mechanism is also proposed based on WiFi fingerprinting. The proposed
coordinated WiGig WLAN highly outperforms conventional un-coordinated one in
terms of total throughput, average packet delay and packet dropping rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05858</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05858</id><created>2015-06-18</created><authors><author><keyname>Mohamed</keyname><forenames>Ehab Mahmoud</forenames></author><author><keyname>Sakaguchi</keyname><forenames>Kei</forenames></author><author><keyname>Sampei</keyname><forenames>Seiichi</forenames></author></authors><title>Delayed Offloading using Cloud Cooperated Millimeter Wave Gates</title><categories>cs.NI</categories><comments>5 pages, 7 Figures, IEEE Intentional Symposium on Personal Indoor and
  Mobile Radio Communications (PIMRC) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing wireless cellular networks capacity is one of the major challenges
for the coming years, especially if we consider the annual doubling of mobile
user traffic. Towards that and thanks to the fact that a significant amount of
mobile data is indeed delay tolerable, in this paper, we suggest embedding the
delayed offloading of some user traffic to be a part of future wireless
cellular networks. To accomplish this, user delayed files will be offloaded
using ultra-high speed Millimeter Wave (Mm-W) Gates. The Mm-W Gate, which will
be distributed inside the Macro basestation (BS) area, consists of number of
Mm-W Access Points (APs) controlled by a local coordinator installed inside the
Gate. To effectively manage the delayed offloading mechanism, utilizing the
concept of User/Control (U/C) data splitting, the Gates coordinators and the
Macro BS are connected to the Cloud Radio Access Network (C-RAN) through
optical fiber links. Also, files offloading organizer software is used by the
User Equipment (UE). A novel weighted proportional fairness (WPF) user
scheduling algorithm is proposed to maximize the Gate Offloading Efficiency
(GOFE) with maintaining long term fairness among the different mobility users
pass through the Gate. If the Gate is properly designed and the files delay
deadlines are properly set; near 100% GOFE with average reduction of 99.7% in
UE energy consumption can be obtained, in time the user just passes through the
Gate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05860</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05860</id><created>2015-06-18</created><authors><author><keyname>Han</keyname><forenames>Shaobo</forenames></author><author><keyname>Liao</keyname><forenames>Xuejun</forenames></author><author><keyname>Dunson</keyname><forenames>David B.</forenames></author><author><keyname>Carin</keyname><forenames>Lawrence</forenames></author></authors><title>Variational Gaussian Copula Inference</title><categories>stat.ML cs.LG stat.CO</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We utilize copulas to constitute a unified framework for constructing and
optimizing variational proposals. The optimal variational posterior under
Sklar's representation is found by minimizing the KL divergence to the true
posterior. For models with continuous and non-Gaussian hidden variables, we
propose a Gaussian copula family that preserves multivariate posterior
dependence. We further employ a class of nonparametric transformations based on
Bernstein polynomials that provide ample flexibility in characterizing the
univariate marginal posteriors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05865</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05865</id><created>2015-06-18</created><updated>2016-02-19</updated><authors><author><keyname>Hu</keyname><forenames>Baotian</forenames></author><author><keyname>Chen</keyname><forenames>Qingcai</forenames></author><author><keyname>Zhu</keyname><forenames>Fangze</forenames></author></authors><title>LCSTS: A Large Scale Chinese Short Text Summarization Dataset</title><categories>cs.CL cs.IR cs.LG</categories><comments>Recently, we received feedbacks from Yuya Taguchi from NAIST in Japan
  and Qian Chen from USTC of China, that the results in the EMNLP2015 version
  seem to be underrated. So we carefully checked our results and find out that
  we made a mistake while using the standard ROUGE. Then we re-evaluate all
  methods in the paper and get corrected results listed in Table 2 of this
  version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic text summarization is widely regarded as the highly difficult
problem, partially because of the lack of large text summarization data set.
Due to the great challenge of constructing the large scale summaries for full
text, in this paper, we introduce a large corpus of Chinese short text
summarization dataset constructed from the Chinese microblogging website Sina
Weibo, which is released to the public
{http://icrc.hitsz.edu.cn/Article/show/139.html}. This corpus consists of over
2 million real Chinese short texts with short summaries given by the author of
each text. We also manually tagged the relevance of 10,666 short summaries with
their corresponding short texts. Based on the corpus, we introduce recurrent
neural network for the summary generation and achieve promising results, which
not only shows the usefulness of the proposed corpus for short text
summarization research, but also provides a baseline for further research on
this topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05869</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05869</id><created>2015-06-18</created><updated>2015-07-21</updated><authors><author><keyname>Vinyals</keyname><forenames>Oriol</forenames></author><author><keyname>Le</keyname><forenames>Quoc</forenames></author></authors><title>A Neural Conversational Model</title><categories>cs.CL</categories><comments>ICML Deep Learning Workshop 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conversational modeling is an important task in natural language
understanding and machine intelligence. Although previous approaches exist,
they are often restricted to specific domains (e.g., booking an airline ticket)
and require hand-crafted rules. In this paper, we present a simple approach for
this task which uses the recently proposed sequence to sequence framework. Our
model converses by predicting the next sentence given the previous sentence or
sentences in a conversation. The strength of our model is that it can be
trained end-to-end and thus requires much fewer hand-crafted rules. We find
that this straightforward model can generate simple conversations given a large
conversational training dataset. Our preliminary results suggest that, despite
optimizing the wrong objective function, the model is able to converse well. It
is able extract knowledge from both a domain specific dataset, and from a
large, noisy, and general domain dataset of movie subtitles. On a
domain-specific IT helpdesk dataset, the model can find a solution to a
technical problem via conversations. On a noisy open-domain movie transcript
dataset, the model can perform simple forms of common sense reasoning. As
expected, we also find that the lack of consistency is a common failure mode of
our model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05870</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05870</id><created>2015-06-18</created><authors><author><keyname>Chen</keyname><forenames>Kuan-Wen</forenames></author><author><keyname>Wang</keyname><forenames>Chun-Hsin</forenames></author><author><keyname>Wei</keyname><forenames>Xiao</forenames></author><author><keyname>Liang</keyname><forenames>Qiao</forenames></author><author><keyname>Yang</keyname><forenames>Ming-Hsuan</forenames></author><author><keyname>Chen</keyname><forenames>Chu-Song</forenames></author><author><keyname>Hung</keyname><forenames>Yi-Ping</forenames></author></authors><title>To Know Where We Are: Vision-Based Positioning in Outdoor Environments</title><categories>cs.CV</categories><comments>11 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Augmented reality (AR) displays become more and more popular recently,
because of its high intuitiveness for humans and high-quality head-mounted
display have rapidly developed. To achieve such displays with augmented
information, highly accurate image registration or ego-positioning are
required, but little attention have been paid for out-door environments. This
paper presents a method for ego-positioning in outdoor environments with low
cost monocular cameras. To reduce the computational and memory requirements as
well as the communication overheads, we formulate the model compression
algorithm as a weighted k-cover problem for better preserving model structures.
Specifically for real-world vision-based positioning applications, we consider
the issues with large scene change and propose a model update algorithm to
tackle these problems. A long- term positioning dataset with more than one
month, 106 sessions, and 14,275 images is constructed. Based on both local and
up-to-date models constructed in our approach, extensive experimental results
show that high positioning accuracy (mean ~ 30.9cm, stdev. ~ 15.4cm) can be
achieved, which outperforms existing vision-based algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05872</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05872</id><created>2015-06-18</created><authors><author><keyname>Kong</keyname><forenames>Chen</forenames></author><author><keyname>Lucey</keyname><forenames>Simon</forenames></author></authors><title>On the Uniqueness of Group Sparse Coding</title><categories>cs.IT math.IT</categories><comments>3 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this technical document we present a proof on the uniqueness of group
sparse coding through the block ACS theorem. Leveraging the original ACS
theorem of Hillar and Sommer for sparse coding, we demonstrate a similar
uniqueness property holds for the task of group sparse coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05885</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05885</id><created>2015-06-19</created><authors><author><keyname>Djaballah</keyname><forenames>A</forenames><affiliation>ENSTA ParisTech U2IS/SFL</affiliation></author><author><keyname>Chapoutot</keyname><forenames>Alexandre</forenames><affiliation>ENSTA ParisTech U2IS/SFL</affiliation></author><author><keyname>Kieffer</keyname><forenames>Michel</forenames><affiliation>LMeASI</affiliation></author><author><keyname>Bouissou</keyname><forenames>O</forenames><affiliation>LMeASI</affiliation></author></authors><title>Construction of Parametric Barrier Functions for Dynamical Systems using
  Interval Analysis</title><categories>math.DS cs.SY</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, barrier certificates have been introduced to prove the safety of
continuous or hybrid dynamical systems. A barrier certificate needs to exhibit
some barrier function, which partitions the state space in two subsets: the
safe subset in which the state can be proved to remain and the complementary
subset containing some unsafe region. This approach does not require any
reachability analysis, but needs the computation of a valid barrier function,
which is difficult when considering general nonlinear systems and barriers.
This paper presents a new approach for the construction of barrier functions
for nonlinear dynamical systems. The proposed technique searches for the
parameters of a parametric barrier function using interval analysis. Complex
dynamics can be considered without needing any relaxation of the constraints to
be satisfied by the barrier function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05887</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05887</id><created>2015-06-19</created><authors><author><keyname>Bernot</keyname><forenames>Gilles</forenames></author><author><keyname>Comet</keyname><forenames>Jean-Paul</forenames></author><author><keyname>Khalis</keyname><forenames>Zohra</forenames></author><author><keyname>Richard</keyname><forenames>Adrien</forenames></author><author><keyname>Roux</keyname><forenames>Olivier</forenames></author></authors><title>A Genetically Modified Hoare Logic</title><categories>cs.CE cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important problem when modeling gene networks lies in the identification
of parameters, even if we consider a purely discrete framework as the one of
Ren\'e Thomas. Here we are interested in the exhaustive search of all parameter
values that are consistent with observed behaviors of the gene network. We
present in this article a new approach based on Hoare Logic and on a weakest
precondition calculus to generate constraints on possible parameter values.
Observed behaviors play the role of &quot;programs&quot; for the classical Hoare logic,
and computed weakest preconditions represent the sets of all compatible
parameterizations expressed as constraints on parameters. Finally we give a
proof of correctness of our Hoare logic for gene networks as well as a proof of
completeness based on the computation of the weakest precondition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05889</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05889</id><created>2015-06-19</created><authors><author><keyname>Davenport</keyname><forenames>Mark A.</forenames></author><author><keyname>Massimino</keyname><forenames>Andrew K.</forenames></author><author><keyname>Needell</keyname><forenames>Deanna</forenames></author><author><keyname>Woolf</keyname><forenames>Tina</forenames></author></authors><title>Constrained adaptive sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that we wish to estimate a vector $\mathbf{x} \in \mathbb{C}^n$ from
a small number of noisy linear measurements of the form $\mathbf{y} = \mathbf{A
x} + \mathbf{z}$, where $\mathbf{z}$ represents measurement noise. When the
vector $\mathbf{x}$ is sparse, meaning that it has only $s$ nonzeros with $s
\ll n$, one can obtain a significantly more accurate estimate of $\mathbf{x}$
by adaptively selecting the rows of $\mathbf{A}$ based on the previous
measurements provided that the signal-to-noise ratio (SNR) is sufficiently
large. In this paper we consider the case where we wish to realize the
potential of adaptivity but where the rows of $\mathbf{A}$ are subject to
physical constraints. In particular, we examine the case where the rows of
$\mathbf{A}$ are constrained to belong to a finite set of allowable measurement
vectors. We demonstrate both the limitations and advantages of adaptive sensing
in this constrained setting. We prove that for certain measurement ensembles,
the benefits offered by adaptive designs fall far short of the improvements
that are possible in the unconstrained adaptive setting. On the other hand, we
also provide both theoretical and empirical evidence that in some scenarios
adaptivity does still result in substantial improvements even in the
constrained setting. To illustrate these potential gains, we propose practical
algorithms for constrained adaptive sensing by exploiting connections to the
theory of optimal experimental design and show that these algorithms exhibit
promising performance in some representative applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05893</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05893</id><created>2015-06-19</created><authors><author><keyname>Bundala</keyname><forenames>Daniel</forenames></author><author><keyname>Seshia</keyname><forenames>Sanjit A.</forenames></author></authors><title>On Systematic Testing for Execution-Time Analysis</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a program and a time deadline, does the program finish before the
deadline when executed on a given platform? With the requirement to produce a
test case when such a violation can occur, we refer to this problem as the
worst-case execution-time testing (WCETT) problem.
  In this paper, we present an approach for solving the WCETT problem for
loop-free programs by timing the execution of a program on a small number of
carefully calculated inputs. We then create a sequence of integer linear
programs the solutions of which encode the best timing model consistent with
the measurements. By solving the programs we can find the worst-case input as
well as estimate execution time of any other input. Our solution is more
accurate than previous approaches and, unlikely previous work, by increasing
the number of measurements we can produce WCETT bounds up to any desired
accuracy.
  Timing of a program depends on the properties of the platform it executes on.
We further show how our approach can be used to quantify the timing
repeatability of the underlying platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05896</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05896</id><created>2015-06-19</created><authors><author><keyname>Lin</keyname><forenames>Jun</forenames></author><author><keyname>Yan</keyname><forenames>Zhiyuan</forenames></author></authors><title>A hybrid partial sum computation unit architecture for list decoders of
  polar codes</title><categories>cs.IT math.IT</categories><comments>5 pages, presented at the 2015 IEEE International Conference on
  Acoustics, Speech, and Signal Processing (ICASSP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the successive cancelation (SC) algorithm works well for very long
polar codes, its error performance for shorter polar codes is much worse.
Several SC based list decoding algorithms have been proposed to improve the
error performances of both long and short polar codes. A significant step of SC
based list decoding algorithms is the updating of partial sums for all decoding
paths. In this paper, we first proposed a lazy copy partial sum computation
algorithm for SC based list decoding algorithms. Instead of copying partial
sums directly, our lazy copy algorithm copies indices of partial sums. Based on
our lazy copy algorithm, we propose a hybrid partial sum computation unit
architecture, which employs both registers and memories so that the overall
area efficiency is improved. Compared with a recent partial sum computation
unit for list decoders, when the list size $L=4$, our partial sum computation
unit achieves an area saving of 23\% and 63\% for block length $2^{13}$ and
$2^{15}$, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05897</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05897</id><created>2015-06-19</created><authors><author><keyname>Henrion</keyname><forenames>Didier</forenames><affiliation>LAAS-MAC</affiliation></author><author><keyname>Naldi</keyname><forenames>Simone</forenames><affiliation>LAAS</affiliation></author><author><keyname>Din</keyname><forenames>Mohab Safey El</forenames><affiliation>PolSys, LIP6</affiliation></author></authors><title>Real root finding for low rank linear matrices</title><categories>cs.SC math.AG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding low rank $m\times m$ matrices in a real affine
subspace of dimension n has many applications in information and systems
theory, where low rank is synonymous of structure and parcimony. We design a
symbolic computation algorithm to solve this problem efficiently, exactly and
rigorously: the input are the rational coefficients of the matrices spanning
the affine subspace as well as the expected maximum rank, and the output is a
rational parametrization encoding a finite set of points that intersects each
connected component of the low rank real algebraic set. The complexity of our
algorithm is studied thoroughly. It is essentially polynomial in
binomial(n+m(m--r),n) where r is the expected maximum rank; it improves on the
state-of-the-art in the field. Moreover, computer experiments show the
practical efficiency of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05900</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05900</id><created>2015-06-19</created><authors><author><keyname>Ashtiani</keyname><forenames>Hassan</forenames></author><author><keyname>Ben-David</keyname><forenames>Shai</forenames></author></authors><title>Representation Learning for Clustering: A Statistical Framework</title><categories>stat.ML cs.LG</categories><comments>To be published in Proceedings of UAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of communicating domain knowledge from a user to the
designer of a clustering algorithm. We propose a protocol in which the user
provides a clustering of a relatively small random sample of a data set. The
algorithm designer then uses that sample to come up with a data representation
under which $k$-means clustering results in a clustering (of the full data set)
that is aligned with the user's clustering. We provide a formal statistical
model for analyzing the sample complexity of learning a clustering
representation with this paradigm. We then introduce a notion of capacity of a
class of possible representations, in the spirit of the VC-dimension, showing
that classes of representations that have finite such dimension can be
successfully learned with sample size error bounds, and end our discussion with
an analysis of that dimension for classes of representations induced by linear
embeddings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05903</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05903</id><created>2015-06-19</created><updated>2015-07-20</updated><authors><author><keyname>Cossu</keyname><forenames>Jean-Val&#xe8;re</forenames><affiliation>LIA</affiliation></author><author><keyname>Dugu&#xe9;</keyname><forenames>Nicolas</forenames><affiliation>LIFO</affiliation></author><author><keyname>Labatut</keyname><forenames>Vincent</forenames><affiliation>LIA</affiliation></author></authors><title>Detecting Real-World Influence Through Twitter</title><categories>cs.SI</categories><comments>2nd European Network Intelligence Conference (ENIC), Sep 2015,
  Karlskrona, Sweden</comments><proxy>ccsd</proxy><doi>10.1109/ENIC.2015.20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the issue of detecting the real-life influence
of people based on their Twitter account. We propose an overview of common
Twitter features used to characterize such accounts and their activity, and
show that these are inefficient in this context. In particular, retweets and
followers numbers, and Klout score are not relevant to our analysis. We thus
propose several Machine Learning approaches based on Natural Language
Processing and Social Network Analysis to label Twitter users as Influencers or
not. We also rank them according to a predicted influence level. Our proposals
are evaluated over the CLEF RepLab 2014 dataset, and outmatch state-of-the-art
ranking methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05905</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05905</id><created>2015-06-19</created><authors><author><keyname>Daskin</keyname><forenames>Anmer</forenames></author></authors><title>Quantum IsoRank: Efficient Alignment of Multiple PPI Networks</title><categories>cs.CE q-bio.MN quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comparative analyses of protein-protein interaction networks play important
roles in the understanding of biological processes. The growing enormity of
available data on the networks becomes a computational challenge for the
conventional alignment algorithms. Quantum algorithms generally provide
efficiency over their classical counterparts in solving various problems. One
of such algorithms is the quantum phase estimation algorithm which generates
the principal eigenvector of a stochastic matrix with probability one. Using
this property, in this article, we describe a quantum computing approach for
the alignment of protein-protein interaction networks by following the
classical algorithm IsoRank which uses the principal eigenvector of the
stochastic matrix representing the Kronecker product of the normalized
adjacency matrices of networks for the pairwise alignment. We also present a
measurement scheme to efficiently procure the alignment from the output state
of the phase estimation algorithm where the eigenvector is encoded as the
amplitudes of this state. Furthermore, since the stochastic matrices are
generally not Hermitian, we discuss how to approximate such matrices and
generate quantum circuits. Finally we discuss the complexity of the quantum
approach and show that it is exponentially more efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05908</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05908</id><created>2015-06-19</created><authors><author><keyname>Piech</keyname><forenames>Chris</forenames></author><author><keyname>Spencer</keyname><forenames>Jonathan</forenames></author><author><keyname>Huang</keyname><forenames>Jonathan</forenames></author><author><keyname>Ganguli</keyname><forenames>Surya</forenames></author><author><keyname>Sahami</keyname><forenames>Mehran</forenames></author><author><keyname>Guibas</keyname><forenames>Leonidas</forenames></author><author><keyname>Sohl-Dickstein</keyname><forenames>Jascha</forenames></author></authors><title>Deep Knowledge Tracing</title><categories>cs.AI cs.CY cs.LG</categories><acm-class>K.3.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Knowledge tracing---where a machine models the knowledge of a student as they
interact with coursework---is a well established problem in computer supported
education. Though effectively modeling student knowledge would have high
educational impact, the task has many inherent challenges. In this paper we
explore the utility of using Recurrent Neural Networks (RNNs) to model student
learning. The RNN family of models have important advantages over previous
methods in that they do not require the explicit encoding of human domain
knowledge, and can capture more complex representations of student knowledge.
Using neural networks results in substantial improvements in prediction
performance on a range of knowledge tracing datasets. Moreover the learned
model can be used for intelligent curriculum design and allows straightforward
interpretation and discovery of structure in student tasks. These results
suggest a promising new line of research for knowledge tracing and an exemplary
application task for RNNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05909</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05909</id><created>2015-06-19</created><authors><author><keyname>Bortolussi</keyname><forenames>Luca</forenames></author><author><keyname>Lanciani</keyname><forenames>Roberta</forenames></author></authors><title>Fluid Model Checking of Timed Properties</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of verifying timed properties of Markovian models of
large populations of interacting agents, modelled as finite state automata. In
particular, we focus on time-bounded properties of (random) individual agents
specified by Deterministic Timed Automata (DTA) endowed with a single clock.
Exploiting ideas from fluid approximation, we estimate the satisfaction
probability of the DTA properties by reducing it to the computation of the
transient probability of a subclass of Time-Inhomogeneous Markov Renewal
Processes with exponentially and deterministically-timed transitions, and a
small state space. For this subclass of models, we show how to derive a set of
Delay Differential Equations (DDE), whose numerical solution provides a fast
and accurate estimate of the satisfaction probability. In the paper, we also
prove the asymptotic convergence of the approach, and exemplify the method on a
simple epidemic spreading model. Finally, we also show how to construct a
system of DDEs to efficiently approximate the average number of agents that
satisfy the DTA specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05913</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05913</id><created>2015-06-19</created><authors><author><keyname>Doerr</keyname><forenames>Benjamin</forenames></author><author><keyname>Doerr</keyname><forenames>Carola</forenames></author><author><keyname>K&#xf6;tzing</keyname><forenames>Timo</forenames></author></authors><title>Solving Problems with Unknown Solution Length at (Almost) No Extra Cost</title><categories>cs.NE</categories><comments>This is a preliminary version of a paper that is to appear at the
  Genetic and Evolutionary Computation Conference (GECCO 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most research in the theory of evolutionary computation assumes that the
problem at hand has a fixed problem size. This assumption does not always apply
to real-world optimization challenges, where the length of an optimal solution
may be unknown a priori.
  Following up on previous work of Cathabard, Lehre, and Yao [FOGA 2011] we
analyze variants of the (1+1) evolutionary algorithm for problems with unknown
solution length. For their setting, in which the solution length is sampled
from a geometric distribution, we provide mutation rates that yield an expected
optimization time that is of the same order as that of the (1+1) EA knowing the
solution length.
  We then show that almost the same run times can be achieved even if \emph{no}
a priori information on the solution length is available.
  Finally, we provide mutation rates suitable for settings in which neither the
solution length nor the positions of the relevant bits are known. Again we
obtain almost optimal run times for the \textsc{OneMax} and
\textsc{LeadingOnes} test functions, thus solving an open problem from
Cathabard et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05920</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05920</id><created>2015-06-19</created><authors><author><keyname>Kato</keyname><forenames>Tsuyoshi</forenames></author><author><keyname>Relator</keyname><forenames>Raissa</forenames></author><author><keyname>Ngouv</keyname><forenames>Hayliang</forenames></author><author><keyname>Hirohashi</keyname><forenames>Yoshihiro</forenames></author><author><keyname>Kakimoto</keyname><forenames>Tetsuhiro</forenames></author><author><keyname>Okada</keyname><forenames>Kinya</forenames></author></authors><title>New Descriptor for Glomerulus Detection in Kidney Microscopy Image</title><categories>cs.CV</categories><journal-ref>BMC Bioinformatics, 16:316, 2015</journal-ref><doi>10.1186/s12859-015-0739-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Glomerulus detection is a key step in histopathological evaluation of
microscopy images of kidneys. However, the task of automatic detection of
glomeruli poses challenges due to the disparity in sizes and shapes of
glomeruli in renal sections. Moreover, extensive variations of their
intensities due to heterogeneity in immunohistochemistry staining are also
encountered. Despite being widely recognized as a powerful descriptor for
general object detection, the rectangular histogram of oriented gradients
(Rectangular HOG) suffers from many false positives due to the aforementioned
difficulties in the context of glomerulus detection.
  A new descriptor referred to as Segmental HOG is developed to perform a
comprehensive detection of hundreds of glomeruli in images of whole kidney
sections. The new descriptor possesses flexible blocks that can be adaptively
fitted to input images to acquire robustness to deformations of glomeruli.
Moreover, the novel segmentation technique employed herewith generates high
quality segmentation outputs and the algorithm is assured to converge to an
optimal solution. Consequently, experiments using real world image data reveal
that Segmental HOG achieves significant improvements in detection performance
compared to Rectangular HOG.
  The proposed descriptor and method for glomeruli detection present promising
results and is expected to be useful in pathological evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05925</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05925</id><created>2015-06-19</created><updated>2015-11-30</updated><authors><author><keyname>Lee</keyname><forenames>Seunghyun</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Cognitive Wireless Powered Network: Spectrum Sharing Models and
  Throughput Maximization</title><categories>cs.IT math.IT</categories><comments>This is the longer version of a paper to appear in IEEE Transactions
  on Cognitive Communications and Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent advance in radio-frequency (RF) wireless energy transfer (WET) has
motivated the study of wireless powered communication network (WPCN), in which
distributed wireless devices are powered via dedicated WET by the hybrid
access-point (H-AP) in the downlink (DL) for uplink (UL) wireless information
transmission (WIT). In this paper, by exploiting the cognitive radio (CR)
technique, we study a new type of CR enabled secondary WPCN, called cognitive
WPCN, under spectrum sharing with the primary wireless communication system. In
particular, we consider a cognitive WPCN, consisting of one single H-AP with
constant power supply and distributed users, shares the same spectrum for its
DL WET and UL WIT with an existing primary communication link, where the WPCN's
WET/WIT and the primary link's WIT may interfere with each other. Under this
new setup, we propose two coexisting models for spectrum sharing of the two
systems, namely underlay and overlay based cognitive WPCNs, depending on
different types of knowledge on the primary user transmission available at the
cognitive WPCN. For each model, we maximize the sum-throughput of the cognitive
WPCN by optimizing its transmission under different constraints applied to
protect the primary user transmission. Analysis and simulation results are
provided to compare the sum-throughput of the cognitive WPCN versus the
achievable rate of the primary user in two coexisting models. It is shown that
the overlay based cognitive WPCN outperforms the underlay based counterpart,
thanks to its fully cooperative WET/WIT design with the primary WIT, while it
also requires higher complexity for implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05929</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05929</id><created>2015-06-19</created><authors><author><keyname>van Noord</keyname><forenames>Nanne</forenames></author><author><keyname>Postma</keyname><forenames>Eric</forenames></author></authors><title>Exploring the influence of scale on artist attribution</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous work has shown that the artist of an artwork can be identified by
use of computational methods that analyse digital images. However, the
digitised artworks are often investigated at a coarse scale discarding many of
the important details that may define an artist's style. In recent years high
resolution images of artworks have become available, which, combined with
increased processing power and new computational techniques, allow us to
analyse digital images of artworks at a very fine scale. In this work we train
and evaluate a Convolutional Neural Network (CNN) on the task of artist
attribution using artwork images of varying resolutions. To this end, we
combine two existing methods to enable the application of high resolution
images to CNNs. By comparing the attribution performances obtained at different
scales, we find that in most cases finer scales are beneficial to the
attribution performance, whereas for a minority of the artists, coarser scales
appear to be preferable. We conclude that artist attribution would benefit from
a multi-scale CNN approach which vastly expands the possibilities for
computational art forensics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05934</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05934</id><created>2015-06-19</created><authors><author><keyname>Lienart</keyname><forenames>Thibaut</forenames></author><author><keyname>Teh</keyname><forenames>Yee Whye</forenames></author><author><keyname>Doucet</keyname><forenames>Arnaud</forenames></author></authors><title>Expectation Particle Belief Propagation</title><categories>stat.CO cs.AI stat.ML</categories><comments>submitted to NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an original particle-based implementation of the Loopy Belief
Propagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a
continuous state space. The algorithm constructs adaptively efficient proposal
distributions approximating the local beliefs at each note of the MRF. This is
achieved by considering proposal distributions in the exponential family whose
parameters are updated iterately in an Expectation Propagation (EP) framework.
The proposed particle scheme provides consistent estimation of the LBP
marginals as the number of particles increases. We demonstrate that it provides
more accurate results than the Particle Belief Propagation (PBP) algorithm of
Ihler and McAllester (2009) at a fraction of the computational cost and is
additionally more robust empirically. The computational complexity of our
algorithm at each iteration is quadratic in the number of particles. We also
propose an accelerated implementation with sub-quadratic computational
complexity which still provides consistent estimates of the loopy BP marginal
distributions and performs almost as well as the original procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05937</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05937</id><created>2015-06-19</created><authors><author><keyname>Doerr</keyname><forenames>Benjamin</forenames></author><author><keyname>Doerr</keyname><forenames>Carola</forenames></author></authors><title>A Tight Runtime Analysis of the $(1+(\lambda, \lambda))$ Genetic
  Algorithm on OneMax</title><categories>cs.NE</categories><comments>This is a preliminary version of a paper that is to appear at Genetic
  and Evolutionary Computation Conference (GECCO 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding how crossover works is still one of the big challenges in
evolutionary computation research, and making our understanding precise and
proven by mathematical means might be an even bigger one. As one of few
examples where crossover provably is useful, the $(1+(\lambda, \lambda))$
Genetic Algorithm (GA) was proposed recently in [Doerr, Doerr, Ebel: TCS 2015].
Using the fitness level method, the expected optimization time on general
OneMax functions was analyzed and a $O(\max\{n\log(n)/\lambda, \lambda n\})$
bound was proven for any offspring population size $\lambda \in [1..n]$.
  We improve this work in several ways, leading to sharper bounds and a better
understanding of how the use of crossover speeds up the runtime in this
algorithm. We first improve the upper bound on the runtime to
$O(\max\{n\log(n)/\lambda, n\lambda \log\log(\lambda)/\log(\lambda)\})$. This
improvement is made possible from observing that in the parallel generation of
$\lambda$ offspring via crossover (but not mutation), the best of these often
is better than the expected value, and hence several fitness levels can be
gained in one iteration.
  We then present the first lower bound for this problem. It matches our upper
bound for all values of $\lambda$. This allows to determine the asymptotically
optimal value for the population size. It is $\lambda =
\Theta(\sqrt{\log(n)\log\log(n)/\log\log\log(n)})$, which gives an optimization
time of $\Theta(n \sqrt{\log(n)\log\log\log(n)/\log\log(n)})$. Hence the
improved runtime analysis gives a better runtime guarantee along with a better
suggestion for the parameter $\lambda$.
  We finally give a tail bound for the upper tail of the runtime distribution,
which shows that the actual runtime exceeds our runtime guarantee by a factor
of $(1+\delta)$ with probability $O((n/\lambda^2)^{-\delta})$ only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05941</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05941</id><created>2015-06-19</created><updated>2015-12-16</updated><authors><author><keyname>Davoli</keyname><forenames>Luca</forenames></author><author><keyname>Veltri</keyname><forenames>Luca</forenames></author><author><keyname>Ventre</keyname><forenames>Pier Luigi</forenames></author><author><keyname>Siracusano</keyname><forenames>Giuseppe</forenames></author><author><keyname>Salsano</keyname><forenames>Stefano</forenames></author></authors><title>Traffic Engineering with Segment Routing: SDN-based Architectural Design
  and Open Source Implementation</title><categories>cs.NI</categories><comments>Extended version of poster paper accepted for EWSDN 2015 (version v4
  - December 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traffic Engineering (TE) in IP carrier networks is one of the functions that
can benefit from the Software Defined Networking paradigm. By logically
centralizing the control of the network, it is possible to &quot;program&quot; per-flow
routing based on TE goals. Traditional per-flow routing requires a direct
interaction between the SDN controller and each node that is involved in the
traffic paths. Depending on the granularity and on the temporal properties of
the flows, this can lead to scalability issues for the amount of routing state
that needs to be maintained in core network nodes and for the required
configuration traffic. On the other hand, Segment Routing (SR) is an emerging
approach to routing that may simplify the route enforcement delegating all the
configuration and per-flow state at the border of the network. In this work we
propose an architecture that integrates the SDN paradigm with SR-based TE, for
which we have provided an open source reference implementation. We have
designed and implemented a simple TE/SR heuristic for flow allocation and we
show and discuss experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05942</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05942</id><created>2015-06-19</created><updated>2015-12-17</updated><authors><author><keyname>Wang</keyname><forenames>Xuehui</forenames></author><author><keyname>Suo</keyname><forenames>Jinli</forenames></author><author><keyname>Yu</keyname><forenames>Jingyi</forenames></author><author><keyname>Zhang</keyname><forenames>Yongdong</forenames></author><author><keyname>Dai</keyname><forenames>Qionghai</forenames></author></authors><title>Scene-adaptive Coded Apertures Imaging</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to bad motivation
  proof and poor experiment performance</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coded aperture imaging systems have recently shown great success in
recovering scene depth and extending the depth-of-field. The ideal pattern,
however, would have to serve two conflicting purposes: 1) be broadband to
ensure robust deconvolution and 2) has sufficient zero-crossings for a high
depth discrepancy. This paper presents a simple but effective scene-adaptive
coded aperture solution to bridge this gap. We observe that the geometric
structures in a natural scene often exhibit only a few edge directions, and the
successive frames are closely correlated. Therefore we adopt a spatial
partitioning and temporal propagation scheme. In each frame, we address one
principal direction by applying depth-discriminative codes along it and
broadband codes along its orthogonal direction. Since within a frame only the
regions with edge direction corresponding to its aperture code behaves well, we
utilize the close among-frame correlation to propagate the high quality single
frame results temporally to obtain high performance over the whole image
lattice. To physically implement this scheme, we use a Liquid Crystal on
Silicon (LCoS) microdisplay that permits fast changing pattern codes. Firstly,
we capture the scene with a pinhole and analyze the scene content to determine
primary edge orientations. Secondly, we sequentially apply the proposed coding
scheme with these orientations in the following frames. Experiments on both
synthetic and real scenes show that our technique is able to combine advantages
of the state-of-the-art patterns for recovering better quality depth map and
all-focus images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05944</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05944</id><created>2015-06-19</created><authors><author><keyname>Xiang</keyname><forenames>Chong</forenames></author><author><keyname>Yang</keyname><forenames>Li</forenames></author></authors><title>Indistinguishability and semantic security for quantum encryption scheme</title><categories>cs.CR quant-ph</categories><comments>14 pages, no figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the definition of security for encryption scheme in quantum
context. We systematically define the indistinguishability and semantic
security for quantum public-key and private-key encryption schemes, and for
computational security, physical security and information-theoretic security.
Based on our definition, we present a necessary and sufficient condition that
leads to information-theoretic indistinguishability for quantum encryption
scheme. The equivalence between the indistinguishability and semantic security
of quantum encryption scheme is also proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05947</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05947</id><created>2015-06-19</created><authors><author><keyname>Sikorsky</keyname><forenames>Maksym</forenames></author><author><keyname>Soroka</keyname><forenames>Anton</forenames></author><author><keyname>Mosiychuk</keyname><forenames>Vitaliy</forenames></author><author><keyname>Sharpan</keyname><forenames>Oleg</forenames></author></authors><title>Comparison of methods of automatic blood pressure measurement in the
  same device</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comparison of tacho-oscillographic and oscillometric methods for human blood
pressure measurement with cuff occlusion in the same device at the same time is
described. For this purpose the measurement system and software for signal
processing that realize these methods in the same device was developed.
Experiment shows that oscillometric method with photoplethysmographic (PPG)
sensors does not require empiric criteria to search systolic and diastolic
blood pressure parameters due to possibility to apply correlation analysis.
Perspectives of applying oscillometric method with PPG sensors in automatic
blood pressure monitors is showed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05950</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05950</id><created>2015-06-19</created><authors><author><keyname>Pahikkala</keyname><forenames>Tapio</forenames></author><author><keyname>Viljanen</keyname><forenames>Markus</forenames></author><author><keyname>Airola</keyname><forenames>Antti</forenames></author><author><keyname>Waegeman</keyname><forenames>Willem</forenames></author></authors><title>Spectral Analysis of Symmetric and Anti-Symmetric Pairwise Kernels</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning regression functions from pairwise data
when there exists prior knowledge that the relation to be learned is symmetric
or anti-symmetric. Such prior knowledge is commonly enforced by symmetrizing or
anti-symmetrizing pairwise kernel functions. Through spectral analysis, we show
that these transformations reduce the kernel's effective dimension. Further, we
provide an analysis of the approximation properties of the resulting kernels,
and bound the regularization bias of the kernels in terms of the corresponding
bias of the original kernel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05963</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05963</id><created>2015-06-19</created><authors><author><keyname>Kaniovski</keyname><forenames>Serguei</forenames></author><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author></authors><title>Representation-Compatible Power Indices</title><categories>cs.GT</categories><comments>28 pages, 1 figure, and 11 tables</comments><msc-class>91A12, 91A80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies power indices based on average representations of a
weighted game. If restricted to account for the lack of power of dummy voters,
average representations become coherent measures of voting power, with power
distributions being proportional to the distribution of weights in the average
representation. This makes these indices representation-compatible, a property
not fulfilled by classical power indices. Average representations can be
tailored to reveal the equivalence classes of voters defined by the Isbell
desirability relation, which leads to a pair of new power indices that ascribes
equal power to all members of an equivalence class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05969</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05969</id><created>2015-06-19</created><authors><author><keyname>Diallo</keyname><forenames>Papa Fary</forenames><affiliation>WIMMICS</affiliation></author><author><keyname>Corby</keyname><forenames>Olivier</forenames><affiliation>WIMMICS</affiliation></author><author><keyname>Mirbel</keyname><forenames>Isabelle</forenames><affiliation>WIMMICS</affiliation></author><author><keyname>Lo</keyname><forenames>Moussa</forenames></author><author><keyname>Ndiaye</keyname><forenames>Seydina M.</forenames></author></authors><title>HuTO: an Human Time Ontology for Semantic Web Applications</title><categories>cs.AI</categories><comments>in French. Ing{\'e}nierie des Connaissances 2015, Jul 2015, Rennes,
  France. Association Fran\c{c}aise pour Intelligence Artificielle (AFIA)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The temporal phenomena have many facets that are studied by different
communities. In Semantic Web, large heterogeneous data are handled and
produced. These data often have informal, semi-formal or formal temporal
information which must be interpreted by software agents. In this paper we
present Human Time Ontology (HuTO) an RDFS ontology to annotate and represent
temporal data. A major contribution of HuTO is the modeling of non-convex
intervals giving the ability to write queries for this kind of interval. HuTO
also incorporates normalization and reasoning rules to explicit certain
information. HuTO also proposes an approach which associates a temporal
dimension to the knowledge base content. This facilitates information retrieval
by considering or not the temporal aspect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05977</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05977</id><created>2015-06-19</created><authors><author><keyname>Conte</keyname><forenames>Alessio</forenames></author><author><keyname>Grossi</keyname><forenames>Roberto</forenames></author><author><keyname>Marino</keyname><forenames>Andrea</forenames></author><author><keyname>Rizzi</keyname><forenames>Romeo</forenames></author></authors><title>Enumerating Cyclic Orientations of a Graph</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acyclic and cyclic orientations of an undirected graph have been widely
studied for their importance: an orientation is acyclic if it assigns a
direction to each edge so as to obtain a directed acyclic graph (DAG) with the
same vertex set; it is cyclic otherwise. As far as we know, only the
enumeration of acyclic orientations has been addressed in the literature. In
this paper, we pose the problem of efficiently enumerating all the
\emph{cyclic} orientations of an undirected connected graph with $n$ vertices
and $m$ edges, observing that it cannot be solved using algorithmic techniques
previously employed for enumerating acyclic orientations.We show that the
problem is of independent interest from both combinatorial and algorithmic
points of view, and that each cyclic orientation can be listed with
$\tilde{O}(m)$ delay time. Space usage is $O(m)$ with an additional setup cost
of $O(n^2)$ time before the enumeration begins, or $O(mn)$ with a setup cost of
$\tilde{O}(m)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05980</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05980</id><created>2015-06-19</created><updated>2015-07-21</updated><authors><author><keyname>Loruenser</keyname><forenames>Thomas</forenames></author><author><keyname>Rodriguez</keyname><forenames>Charles Bastos</forenames></author><author><keyname>Demirel</keyname><forenames>Denise</forenames></author><author><keyname>Fischer-Huebner</keyname><forenames>Simone</forenames></author><author><keyname>Gross</keyname><forenames>Thomas</forenames></author><author><keyname>Langer</keyname><forenames>Thomas</forenames></author><author><keyname>Noes</keyname><forenames>Mathieu des</forenames></author><author><keyname>Poehls</keyname><forenames>Henrich C.</forenames></author><author><keyname>Rozenberg</keyname><forenames>Boris</forenames></author><author><keyname>Slamanig</keyname><forenames>Daniel</forenames></author></authors><title>Towards a New Paradigm for Privacy and Security in Cloud Services</title><categories>cs.CR</categories><comments>To appear in 4th Cyber Security and Privacy EU Forum, CSP Forum 2015,
  Revised Selected Papers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The market for cloud computing can be considered as the major growth area in
ICT. However, big companies and public authorities are reluctant to entrust
their most sensitive data to external parties for storage and processing. The
reason for their hesitation is clear: There exist no satisfactory approaches to
adequately protect the data during its lifetime in the cloud. The EU Project
Prismacloud (Horizon 2020 programme; duration 2/2015-7/2018) addresses these
challenges and yields a portfolio of novel technologies to build security
enabled cloud services, guaranteeing the required security with the strongest
notion possible, namely by means of cryptography. We present a new approach
towards a next generation of security and privacy enabled services to be
deployed in only partially trusted cloud infrastructures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05981</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05981</id><created>2015-06-19</created><authors><author><keyname>Backhouse</keyname><forenames>Roland</forenames></author><author><keyname>Ferreira</keyname><forenames>Jo&#xe3;o F.</forenames></author></authors><title>On Euclid's Algorithm and Elementary Number Theory</title><categories>cs.DS cs.DM math.CO</categories><journal-ref>Sci. Comput. Program. 76 (3) (2011) 160-180</journal-ref><doi>10.1016/j.scico.2010.05.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms can be used to prove and to discover new theorems. This paper
shows how algorithmic skills in general, and the notion of invariance in
particular, can be used to derive many results from Euclid's algorithm. We
illustrate how to use the algorithm as a verification interface (i.e., how to
verify theorems) and as a construction interface (i.e., how to investigate and
derive new theorems).
  The theorems that we verify are well-known and most of them are included in
standard number theory books. The new results concern distributivity properties
of the greatest common divisor and a new algorithm for efficiently enumerating
the positive rationals in two different ways. One way is known and is due to
Moshe Newman. The second is new and corresponds to a deforestation of the
Stern-Brocot tree of rationals. We show that both enumerations stem from the
same simple algorithm. In this way, we construct a Stern-Brocot enumeration
algorithm with the same time and space complexity as Newman's algorithm. A
short review of the original papers by Stern and Brocot is also included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05985</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05985</id><created>2015-06-19</created><authors><author><keyname>Bresson</keyname><forenames>Xavier</forenames></author><author><keyname>Laurent</keyname><forenames>Thomas</forenames></author><author><keyname>von Brecht</keyname><forenames>James</forenames></author></authors><title>Enhanced Lasso Recovery on Graph</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work aims at recovering signals that are sparse on graphs. Compressed
sensing offers techniques for signal recovery from a few linear measurements
and graph Fourier analysis provides a signal representation on graph. In this
paper, we leverage these two frameworks to introduce a new Lasso recovery
algorithm on graphs. More precisely, we present a non-convex, non-smooth
algorithm that outperforms the standard convex Lasso technique. We carry out
numerical experiments on three benchmark graph datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.05996</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.05996</id><created>2015-06-19</created><authors><author><keyname>Remacle</keyname><forenames>J. -F.</forenames></author><author><keyname>Gandham</keyname><forenames>R.</forenames></author><author><keyname>Warburton</keyname><forenames>T.</forenames></author></authors><title>GPU accelerated spectral finite elements on all-hex meshes</title><categories>cs.CE cs.DC cs.NA</categories><comments>23 pages, 7 figures</comments><msc-class>65Y05, 65Y10, 65Y20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a spectral element finite element scheme that efficiently
solves elliptic problems on unstructured hexahedral meshes. The discrete
equations are solved using a matrix-free preconditioned conjugate gradient
algorithm. An additive Schwartz two-scale preconditioner is employed that
allows h-independence convergence. An extensible multi-threading programming
API is used as a common kernel language that allows runtime selection of
different computing devices (GPU and CPU) and different threading interfaces
(CUDA, OpenCL and OpenMP). Performance tests demonstrate that problems with
over 50 million degrees of freedom can be solved in a few seconds on an
off-the-shelf GPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06001</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06001</id><created>2015-06-19</created><authors><author><keyname>Devernay</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Beardsley</keyname><forenames>Paul</forenames><affiliation>DRZ</affiliation></author></authors><title>Stereoscopic Cinema</title><categories>cs.CV</categories><comments>Published as Ronfard, R\'emi and Taubin, Gabriel. Image and Geometry
  Processing for 3-D Cinematography, 5, Springer Berlin Heidelberg, pp.11-51,
  2010, Geometry and Computing, 978-3-642-12392-4</comments><proxy>ccsd</proxy><doi>10.1007/978-3-642-12392-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stereoscopic cinema has seen a surge of activity in recent years, and for the
first time all of the major Hollywood studios released 3-D movies in 2009. This
is happening alongside the adoption of 3-D technology for sports broadcasting,
and the arrival of 3-D TVs for the home. Two previous attempts to introduce 3-D
cinema in the 1950s and the 1980s failed because the contemporary technology
was immature and resulted in viewer discomfort. But current technologies --
such as accurately-adjustable 3-D camera rigs with onboard computers to
automatically inform a camera operator of inappropriate stereoscopic shots,
digital processing for post-shooting rectification of the 3-D imagery, digital
projectors for accurate positioning of the two stereo projections on the cinema
screen, and polarized silver screens to reduce cross-talk between the viewers
left- and right-eyes -- mean that the viewer experience is at a much higher
level of quality than in the past. Even so, creation of stereoscopic cinema is
an open, active research area, and there are many challenges from acquisition
to post-production to automatic adaptation for different-sized display. This
chapter describes the current state-of-the-art in stereoscopic cinema, and
directions of future work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06004</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06004</id><created>2015-06-18</created><authors><author><keyname>Plotkin</keyname><forenames>Boris</forenames></author><author><keyname>Plotkin</keyname><forenames>Tatjana</forenames></author></authors><title>Automata and automata mappings of semigroups</title><categories>cs.FL math.RA</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is devoted to two types of algebraic models of automata. The usual
(first type) model leads to the developed decomposition theory (Krohn-Rhodes
theory). We introduce another type of automata model and study how these
automata are related to cascade connections of automata of the first type. The
introduced automata play a significant role in group theory and, hopefully, in
the theory of formal languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06006</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06006</id><created>2015-06-19</created><authors><author><keyname>Kruthiventi</keyname><forenames>Srinivas S. S.</forenames></author><author><keyname>Babu</keyname><forenames>R. Venkatesh</forenames></author></authors><title>Crowd Flow Segmentation in Compressed Domain using CRF</title><categories>cs.CV</categories><comments>In IEEE International Conference on Image Processing (ICIP), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowd flow segmentation is an important step in many video surveillance
tasks. In this work, we propose an algorithm for segmenting flows in H.264
compressed videos in a completely unsupervised manner. Our algorithm works on
motion vectors which can be obtained by partially decoding the compressed video
without extracting any additional features. Our approach is based on modelling
the motion vector field as a Conditional Random Field (CRF) and obtaining
oriented motion segments by finding the optimal labelling which minimises the
global energy of CRF. These oriented motion segments are recursively merged
based on gradient across their boundaries to obtain the final flow segments.
This work in compressed domain can be easily extended to pixel domain by
substituting motion vectors with motion based features like optical flow. The
proposed algorithm is experimentally evaluated on a standard crowd flow dataset
and its superior performance in both accuracy and computational time are
demonstrated through quantitative results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06011</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06011</id><created>2015-06-19</created><authors><author><keyname>Fayolle</keyname><forenames>Guy</forenames></author><author><keyname>Muhlethaler</keyname><forenames>Paul</forenames></author></authors><title>A Markovian Analysis of IEEE 802.11 Broadcast Transmission Networks with
  Buffering</title><categories>cs.PF math.PR</categories><msc-class>Primary 60J10, secondary 30D05, 30E99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to analyze the so-called back-off technique of
the IEEE 802.11 protocol in broadcast mode with waiting queues. In contrast to
existing models, packets arriving when a station (or node) is in back-off state
are not discarded, but are stored in a buffer of infinite capacity. As in
previous studies, the key point of our analysis hinges on the assumption that
the time on the channel is viewed as a random succession of transmission slots
(whose duration corresponds to the length of a packet) and mini-slots during
which the back-o? of the station is decremented. These events occur
independently, with given probabilities. The state of a node is represented by
a two-dimensional Markov chain in discrete-time, formed by the back-off counter
and the number of packets at the station. Two models are proposed both of which
are shown to cope reasonably well with the physical principles of the protocol.
The stabillity (ergodicity) conditions are obtained and interpreted in terms of
maximum throughput. Several approximations related to these models are also
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06017</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06017</id><created>2015-06-18</created><authors><author><keyname>Plotkin</keyname><forenames>Boris</forenames></author><author><keyname>Plotkin</keyname><forenames>Tatjana</forenames></author></authors><title>Decompositions and complexity of linear automata</title><categories>math.RA cs.FL</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Krohn-Rhodes complexity theory for pure (without linearity) automata is
well-known. This theory uses an operation of wreath product as a decomposition
tool. The main goal of the paper is to introduce the notion of complexity of
linear automata. This notion is ultimately related with decompositions of
linear automata. The study of these decompositions is the second objective of
the paper. In order to define complexity for linear automata, we have to use
three operations, namely, triangular product of linear automata, wreath product
of pure automata and wreath product of a linear automaton with a pure one which
returns a linear automaton. We define the complexity of a linear automaton as
the minimal number of operations in the decompositions of the automaton into
indecomposable components (atoms). This theory relies on the following
parallelism between wreath and triangular products: both of them are terminal
objects in the categories of cascade connections of automata. The wreath
product is the terminal object in the Krohn-Rhodes theory for pure automata,
while the triangular product provides the terminal object for the cascade
connections of linear automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06021</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06021</id><created>2015-06-19</created><authors><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Yang</keyname><forenames>Zeyao</forenames></author></authors><title>Measuring Emotional Contagion in Social Media</title><categories>cs.SI cs.LG physics.soc-ph</categories><comments>10 pages, 5 figures</comments><doi>10.1371/journal.pone.0142390</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media are used as main discussion channels by millions of individuals
every day. The content individuals produce in daily social-media-based
micro-communications, and the emotions therein expressed, may impact the
emotional states of others. A recent experiment performed on Facebook
hypothesized that emotions spread online, even in absence of non-verbal cues
typical of in-person interactions, and that individuals are more likely to
adopt positive or negative emotions if these are over-expressed in their social
network. Experiments of this type, however, raise ethical concerns, as they
require massive-scale content manipulation with unknown consequences for the
individuals therein involved. Here, we study the dynamics of emotional
contagion using Twitter. Rather than manipulating content, we devise a null
model that discounts some confounding factors (including the effect of
emotional contagion). We measure the emotional valence of content the users are
exposed to before posting their own tweets. We determine that on average a
negative post follows an over-exposure to 4.34% more negative content than
baseline, while positive posts occur after an average over-exposure to 4.50%
more positive contents. We highlight the presence of a linear relationship
between the average emotional valence of the stimuli users are exposed to, and
that of the responses they produce. We also identify two different classes of
individuals: highly and scarcely susceptible to emotional contagion. Highly
susceptible users are significantly less inclined to adopt negative emotions
than the scarcely susceptible ones, but equally likely to adopt positive
emotions. In general, the likelihood of adopting positive emotions is much
greater than that of negative emotions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06024</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06024</id><created>2015-06-19</created><authors><author><keyname>Droste</keyname><forenames>Manfred</forenames></author><author><keyname>Perevoshchikov</keyname><forenames>Vitaly</forenames></author></authors><title>Multi-weighted Automata and MSO Logic</title><categories>cs.LO cs.FL</categories><comments>The final version appeared in the Proceedings of the 8th
  International Computer Science Symposium in Russia (CSR 2013)</comments><doi>10.1007/978-3-642-38536-0_36</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weighted automata are non-deterministic automata where the transitions are
equipped with weights. They can model quantitative aspects of systems like
costs or energy consumption. The value of a run can be computed, for example,
as the maximum, limit average, or discounted sum of transition weights. In
multi-weighted automata, transitions carry several weights and can model, for
example, the ratio between rewards and costs, or the efficiency of use of a
primary resource under some upper bound constraint on a secondary resource.
Here, we introduce a general model for multi-weighted automata as well as a
multiweighted MSO logic. In our main results, we show that this multi-weighted
MSO logic and multi-weighted automata are expressively equivalent both for
finite and infinite words. The translation process is effective, leading to
decidability results for our multi-weighted MSO logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06033</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06033</id><created>2015-06-19</created><authors><author><keyname>Simeonovski</keyname><forenames>Milivoj</forenames></author><author><keyname>Bendun</keyname><forenames>Fabian</forenames></author><author><keyname>Asghar</keyname><forenames>Muhammad Rizwan</forenames></author><author><keyname>Backes</keyname><forenames>Michael</forenames></author><author><keyname>Marnau</keyname><forenames>Ninja</forenames></author><author><keyname>Druschel</keyname><forenames>Peter</forenames></author></authors><title>Oblivion: Mitigating Privacy Leaks by Controlling the Discoverability of
  Online Information</title><categories>cs.CR cs.CY cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search engines are the prevalently used tools to collect information about
individuals on the Internet. Search results typically comprise a variety of
sources that contain personal information -- either intentionally released by
the person herself, or unintentionally leaked or published by third parties,
often with detrimental effects on the individual's privacy. To grant
individuals the ability to regain control over their disseminated personal
information, the European Court of Justice recently ruled that EU citizens have
a right to be forgotten in the sense that indexing systems, must offer them
technical means to request removal of links from search results that point to
sources violating their data protection rights. As of now, these technical
means consist of a web form that requires a user to manually identify all
relevant links upfront and to insert them into the web form, followed by a
manual evaluation by employees of the indexing system to assess if the request
is eligible and lawful.
  We propose a universal framework Oblivion to support the automation of the
right to be forgotten in a scalable, provable and privacy-preserving manner.
First, Oblivion enables a user to automatically find and tag her disseminated
personal information using natural language processing and image recognition
techniques and file a request in a privacy-preserving manner. Second, Oblivion
provides indexing systems with an automated and provable eligibility mechanism,
asserting that the author of a request is indeed affected by an online
resource. The automated ligibility proof ensures censorship-resistance so that
only legitimately affected individuals can request the removal of corresponding
links from search results. We have conducted comprehensive evaluations, showing
that Oblivion is capable of handling 278 removal requests per second, and is
hence suitable for large-scale deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06038</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06038</id><created>2015-06-19</created><authors><author><keyname>Droste</keyname><forenames>Manfred</forenames></author><author><keyname>Perevoshchikov</keyname><forenames>Vitaly</forenames></author></authors><title>A Nivat Theorem for Weighted Timed Automata and Weighted Relative
  Distance Logic</title><categories>cs.FL</categories><comments>The final version appeared in the Proceedings of the 41st
  International Colloquium on Automata, Languages, and Programming (ICALP 2014)</comments><doi>10.1007/978-3-662-43951-7_15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weighted timed automata (WTA) model quantitative aspects of real-time systems
like continuous consumption of memory, power or financial resources. They
accept quantitative timed languages where every timed word is mapped to a
value, e.g., a real number. In this paper, we prove a Nivat theorem for WTA
which states that recognizable quantitative timed languages are exactly those
which can be obtained from recognizable boolean timed languages with the help
of several simple operations. We also introduce a weighted extension of
relative distance logic developed by Wilke, and we show that our weighted
relative distance logic and WTA are equally expressive. The proof of this
result can be derived from our Nivat theorem and Wilke's theorem for relative
distance logic. Since the proof of our Nivat theorem is constructive, the
translation process from logic to automata and vice versa is also constructive.
This leads to decidability results for weighted relative distance logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06039</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06039</id><created>2015-06-19</created><authors><author><keyname>Dubbs</keyname><forenames>Alexander</forenames></author><author><keyname>Guevara</keyname><forenames>James</forenames></author><author><keyname>Peterka</keyname><forenames>Darcy S.</forenames></author><author><keyname>Yuste</keyname><forenames>Rafael</forenames></author></authors><title>moco: Fast Motion Correction for Calcium Imaging</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motion correction is the first in a pipeline of algorithms to analyze calcium
imaging videos and extract biologically relevant information, for example the
network structure of the neurons therein. Fast motion correction would be
especially critical for closed-loop activity triggered stimulation experiments,
where accurate detection and targeting of specific cells in necessary. Our
algorithm uses a Fourier-transform approach, and its efficiency derives from a
combination of judicious downsampling and the accelerated computation of many
$L_2$ norms using dynamic programming and two-dimensional, fft-accelerated
convolutions. Its accuracy is comparable to that of established community-used
algorithms, and it is more stable to large translational motions. It is
programmed in Java and is compatible with ImageJ.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06040</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06040</id><created>2015-06-19</created><authors><author><keyname>Karahan</keyname><forenames>Esin</forenames></author><author><keyname>Rojas-Lopez</keyname><forenames>Pedro A.</forenames></author><author><keyname>Bringas-Vega</keyname><forenames>Maria L.</forenames></author><author><keyname>Valdes-Hernandez</keyname><forenames>Pedro A.</forenames></author><author><keyname>Valdes-Sosa</keyname><forenames>Pedro A.</forenames></author></authors><title>Tensor Analysis and Fusion of Multimodal Brain Images</title><categories>stat.ME cs.NA stat.AP stat.ML</categories><comments>23 pages, 15 figures, submitted to Proceedings of the IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current high-throughput data acquisition technologies probe dynamical systems
with different imaging modalities, generating massive data sets at different
spatial and temporal resolutions posing challenging problems in multimodal data
fusion. A case in point is the attempt to parse out the brain structures and
networks that underpin human cognitive processes by analysis of different
neuroimaging modalities (functional MRI, EEG, NIRS etc.). We emphasize that the
multimodal, multi-scale nature of neuroimaging data is well reflected by a
multi-way (tensor) structure where the underlying processes can be summarized
by a relatively small number of components or &quot;atoms&quot;. We introduce
Markov-Penrose diagrams - an integration of Bayesian DAG and tensor network
notation in order to analyze these models. These diagrams not only clarify
matrix and tensor EEG and fMRI time/frequency analysis and inverse problems,
but also help understand multimodal fusion via Multiway Partial Least Squares
and Coupled Matrix-Tensor Factorization. We show here, for the first time, that
Granger causal analysis of brain networks is a tensor regression problem, thus
allowing the atomic decomposition of brain networks. Analysis of EEG and fMRI
recordings shows the potential of the methods and suggests their use in other
scientific domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06046</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06046</id><created>2015-04-16</created><authors><author><keyname>Yadav</keyname><forenames>Poonam</forenames></author></authors><title>Face Prediction Model for an Automatic Age-invariant Face Recognition
  System</title><categories>cs.CV cs.NE</categories><comments>3 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Automated face recognition and identification softwares are becoming part of
our daily life; it finds its abode not only with Facebook's auto photo tagging,
Apple's iPhoto, Google's Picasa, Microsoft's Kinect, but also in Homeland
Security Department's dedicated biometric face detection systems. Most of these
automatic face identification systems fail where the effects of aging come into
the picture. Little work exists in the literature on the subject of face
prediction that accounts for aging, which is a vital part of the computer face
recognition systems. In recent years, individual face components' (e.g. eyes,
nose, mouth) features based matching algorithms have emerged, but these
approaches are still not efficient. Therefore, in this work we describe a Face
Prediction Model (FPM), which predicts human face aging or growth related image
variation using Principle Component Analysis (PCA) and Artificial Neural
Network (ANN) learning techniques. The FPM captures the facial changes, which
occur with human aging and predicts the facial image with a few years of gap
with an acceptable accuracy of face matching from 76 to 86%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06053</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06053</id><created>2015-05-20</created><authors><author><keyname>Janssen</keyname><forenames>Jeannette</forenames></author><author><keyname>Pralat</keyname><forenames>Pawel</forenames></author><author><keyname>Wilson</keyname><forenames>Rory</forenames></author></authors><title>Non-Uniform Distribution of Nodes in the Spatial Preferential Attachment
  Model</title><categories>cs.SI math.CO math.PR physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spatial preferential attachment (SPA) is a model for complex networks. In
the SPA model, nodes are embedded in a metric space, and each node has a sphere
of influence whose size increases if the node gains an in-link, and otherwise
decreases with time. In this paper, we study the behaviour of the SPA model
when the distribution of the nodes is non-uniform. Specifically, the space is
divided into dense and sparse regions, where it is assumed that the dense
regions correspond to coherent communities. We prove precise theoretical
results regarding the degree of a node, the number of common neighbours, and
the average out-degree in a region. Moreover, we show how these theoretically
derived results about the graph properties of the model can be used to
formulate a reliable estimator for the distance between certain pairs of nodes,
and to estimate the density of the region containing a given node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06055</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06055</id><created>2015-06-19</created><authors><author><keyname>Huang</keyname><forenames>Tianyao</forenames></author><author><keyname>Zhao</keyname><forenames>Tong</forenames></author></authors><title>Low PMEPR OFDM radar waveform design using the iterative least squares
  algorithm</title><categories>stat.AP cs.IT math.IT</categories><comments>6 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter considers waveform design of orthogonal frequency division
multiplexing (OFDM) signal for radar applications, and aims at mitigating the
envelope fluctuation in OFDM. A novel method is proposed to reduce the
peak-to-mean envelope power ratio (PMEPR), which is commonly used to evaluate
the fluctuation. The proposed method is based on the tone reservation approach,
in which some bits or subcarriers of OFDM are allocated for decreasing PMEPR.
We introduce the coefficient of variation of envelopes (CVE) as the cost
function for waveform optimization, and develop an iterative least squares
algorithm. Minimizing CVE leads to distinct PMEPR reduction, and it is
guaranteed that the cost function monotonically decreases by applying the
iterative algorithm. Simulations demonstrate that the envelope is significantly
smoothed by the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06057</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06057</id><created>2015-06-18</created><authors><author><keyname>Aladova</keyname><forenames>Elena</forenames></author><author><keyname>Gvaramia</keyname><forenames>Aleko</forenames></author><author><keyname>Plotkin</keyname><forenames>Boris</forenames></author><author><keyname>Plotkin</keyname><forenames>Tatjana</forenames></author></authors><title>Multi-sorted logic, models and logical geometry</title><categories>cs.LO</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\Theta$ be a variety of algebras, $(H, \Psi, f)$ be a model, where $H$
is an algebra from $\Theta$, $\Psi$ is a set of relation symbols $\varphi$, $f$
is an interpretation of all $\varphi$ in $H$. Let $X^0$ be an infinite set of
variables, $\Gamma$ be a collection of all finite subsets in $X^0$ (collection
of sorts), $\widetilde\Phi$ be the multi-sorted algebra of formulas. These data
define a knowledge base $KB(H,\Psi, f)$. In the paper the notion of isomorphism
of knowledge bases is considered. We give sufficient conditions which provide
isomorphism of knowledge bases. We also study the problem of necessary and
sufficient conditions for isomorphism of two knowledge bases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06066</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06066</id><created>2015-06-19</created><authors><author><keyname>Govindasamy</keyname><forenames>Siddhartan</forenames></author></authors><title>Uplink Performance of Large Optimum-Combining Antenna Arrays in
  Power-Controlled Cellular Networks</title><categories>cs.IT math.IT</categories><comments>To appear in ICC 2015. Caption for Figure 2 has been corrected in
  this version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The uplink of interference-limited cellular networks with base stations that
have large numbers of antennas and use linear Minimum-Mean-Square Error (MMSE)
processing with power control is analyzed. Simple approximations, which are
exact in an asymptotic sense, are provided for the spectral efficiencies
(b/s/Hz) of links in these systems. It is also found that when the number of
base-station antennas is moderately large, and the number of mobiles in the
entire network is large, correlations between the transmit powers of mobiles
within a given cell do not significantly influence the spectral efficiency of
the system. As a result, mobiles can perform simple power control (e.g.
fractional power control) that does not depend on other users in the network,
reducing system complexity and improving the analytical tractability of such
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06068</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06068</id><created>2015-06-19</created><authors><author><keyname>Qiu</keyname><forenames>Teng</forenames></author><author><keyname>Li</keyname><forenames>Yongjie</forenames></author></authors><title>A general framework for the IT-based clustering methods</title><categories>cs.CV cs.LG stat.ML</categories><comments>17 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Previously, we proposed a physically inspired rule to organize the data
points in a sparse yet effective structure, called the in-tree (IT) graph,
which is able to capture a wide class of underlying cluster structures in the
datasets, especially for the density-based datasets. Although there are some
redundant edges or lines between clusters requiring to be removed by computer,
this IT graph has a big advantage compared with the k-nearest-neighborhood
(k-NN) or the minimal spanning tree (MST) graph, in that the redundant edges in
the IT graph are much more distinguishable and thus can be easily determined by
several methods previously proposed by us.
  In this paper, we propose a general framework to re-construct the IT graph,
based on an initial neighborhood graph, such as the k-NN or MST, etc, and the
corresponding graph distances. For this general framework, our previous way of
constructing the IT graph turns out to be a special case of it. This general
framework 1) can make the IT graph capture a wider class of underlying cluster
structures in the datasets, especially for the manifolds, and 2) should be more
effective to cluster the sparse or graph-based datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06072</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06072</id><created>2015-06-19</created><authors><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Yang</keyname><forenames>Zeyao</forenames></author></authors><title>Quantifying the Effect of Sentiment on Information Diffusion in Social
  Media</title><categories>cs.SI cs.LG physics.soc-ph</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media have become the main vehicle of information production and
consumption online. Millions of users every day log on their Facebook or
Twitter accounts to get updates and news, read about their topics of interest,
and become exposed to new opportunities and interactions. Although recent
studies suggest that the contents users produce will affect the emotions of
their readers, we still lack a rigorous understanding of the role and effects
of contents sentiment on the dynamics of information diffusion. This work aims
at quantifying the effect of sentiment on information diffusion, to understand:
(i) whether positive conversations spread faster and/or broader than negative
ones (or vice-versa); (ii) what kind of emotions are more typical of popular
conversations on social media; and, (iii) what type of sentiment is expressed
in conversations characterized by different temporal dynamics. Our findings
show that, at the level of contents, negative messages spread faster than
positive ones, but positive ones reach larger audiences, suggesting that people
are more inclined to share and favorite positive contents, the so-called
positive bias. As for the entire conversations, we highlight how different
temporal dynamics exhibit different sentiment patterns: for example, positive
sentiment builds up for highly-anticipated events, while unexpected events are
mainly characterized by negative sentiment. Our contribution is a milestone to
understand how the emotions expressed in short texts affect their spreading in
online social ecosystems, and may help to craft effective policies and
strategies for content generation and diffusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06075</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06075</id><created>2015-06-19</created><authors><author><keyname>Dvijotham</keyname><forenames>Krishnamurthy</forenames></author><author><keyname>Vuffray</keyname><forenames>Marc</forenames></author><author><keyname>Misra</keyname><forenames>Sidhant</forenames></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author></authors><title>Natural Gas Flow Solutions with Guarantees: A Monotone Operator Theory
  Approach</title><categories>cs.SY</categories><comments>Submitted to CDC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider balanced flows in a natural gas transmission network and discuss
computationally hard problems such as establishing if solution of the
underlying nonlinear gas flow equations exists, if it is unique, and finding
the solution. Particular topologies, e.g. trees, are known to be easy to solve
based on a variational description of the gas flow equations, but these
approaches do not generalize. In this paper, we show that the gas flow problem
can be solved efficiently using the tools of monotone operator theory, provided
that we look for solution within certain monotonicity domains. We characterize
a family of monotonicity domains, described in terms of Linear Matrix
Inequalities (LMI) in the state variables, each containing at most one
solution. We also develop an efficient algorithm to choose a particular
monotonicity domain, for which the LMI based condition simplifies to a bound on
the flows. Performance of the technique is illustrated on exemplary gas
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06079</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06079</id><created>2015-06-19</created><authors><author><keyname>Ducoat</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Oggier</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames></author></authors><title>On skew polynomial codes and lattices from quotients of cyclic division
  algebras</title><categories>cs.IT math.IT math.RA</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a variation of Construction A of lattices from linear codes
defined using the quotient $\Lambda/\mathfrak p\Lambda$ of some order $\Lambda$
inside a cyclic division $F$-algebra, for $\mathfrak p$ a prime ideal of a
number field $F$. To obtain codes over this quotient, we first give an
isomorphism between $\Lambda/\mathfrak p\Lambda$ and a ring of skew
polynomials. We then discuss definitions and basic properties of skew
polynomial codes, which are needed for Construction A, but also explore further
properties of the dual of such codes. We conclude by providing an application
to space-time coding, which is the original motivation to consider cyclic
division $F$-algebras as a starting point for this variation of Construction A.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06081</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06081</id><created>2015-06-19</created><updated>2015-10-08</updated><authors><author><keyname>Zheng</keyname><forenames>Qinqing</forenames></author><author><keyname>Lafferty</keyname><forenames>John</forenames></author></authors><title>A Convergent Gradient Descent Algorithm for Rank Minimization and
  Semidefinite Programming from Random Linear Measurements</title><categories>stat.ML cs.LG</categories><comments>Sample complexity updated. Accepted to NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple, scalable, and fast gradient descent algorithm to
optimize a nonconvex objective for the rank minimization problem and a closely
related family of semidefinite programs. With $O(r^3 \kappa^2 n \log n)$ random
measurements of a positive semidefinite $n \times n$ matrix of rank $r$ and
condition number $\kappa$, our method is guaranteed to converge linearly to the
global optimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06086</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06086</id><created>2015-06-19</created><authors><author><keyname>Silva</keyname><forenames>Danilo</forenames></author><author><keyname>Terra</keyname><forenames>Ricardo</forenames></author><author><keyname>Valente</keyname><forenames>Marco Tulio</forenames></author></authors><title>JExtract: An Eclipse Plug-in for Recommending Automated Extract Method
  Refactorings</title><categories>cs.SE</categories><comments>V Brazilian Conference on Software: Theory and Practice (Tools
  Track), p. 1-8, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although Extract Method is a key refactoring for improving program
comprehension, refactoring tools for such purpose are often underused. To
address this shortcoming, we present JExtract, a recommendation system based on
structural similarity that identifies Extract Method refactoring opportunities
that are directly automated by IDE-based refactoring tools. Our evaluation
suggests that JExtract is far more effective (w.r.t. recall and precision) to
identify misplaced code in methods than JDeodorant, a state-of-the-art tool
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06096</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06096</id><created>2015-06-19</created><authors><author><keyname>Thanou</keyname><forenames>Dorina</forenames></author><author><keyname>Chou</keyname><forenames>Philip A.</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Graph-based compression of dynamic 3D point cloud sequences</title><categories>cs.CV cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of compression of 3D point cloud sequences
that are characterized by moving 3D positions and color attributes. As
temporally successive point cloud frames are similar, motion estimation is key
to effective compression of these sequences. It however remains a challenging
problem as the point cloud frames have varying numbers of points without
explicit correspondence information. We represent the time-varying geometry of
these sequences with a set of graphs, and consider 3D positions and color
attributes of the points clouds as signals on the vertices of the graphs. We
then cast motion estimation as a feature matching problem between successive
graphs. The motion is estimated on a sparse set of representative vertices
using new spectral graph wavelet descriptors. A dense motion field is
eventually interpolated by solving a graph-based regularization problem. The
estimated motion is finally used for removing the temporal redundancy in the
predictive coding of the 3D positions and the color characteristics of the
point cloud sequences. Experimental results demonstrate that our method is able
to accurately estimate the motion between consecutive frames. Moreover, motion
estimation is shown to bring significant improvement in terms of the overall
compression performance of the sequence. To the best of our knowledge, this is
the first paper that exploits both the spatial correlation inside each frame
(through the graph) and the temporal correlation between the frames (through
the motion estimation) to compress the color and the geometry of 3D point cloud
sequences in an efficient way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06098</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06098</id><created>2015-06-19</created><authors><author><keyname>Rodrigues</keyname><forenames>J.</forenames></author><author><keyname>Gazziro</keyname><forenames>M.</forenames></author><author><keyname>Goncalves</keyname><forenames>N.</forenames></author><author><keyname>Neto</keyname><forenames>O.</forenames></author><author><keyname>Fernandes</keyname><forenames>Y.</forenames></author><author><keyname>Gimenes</keyname><forenames>A.</forenames></author><author><keyname>Alegre</keyname><forenames>C.</forenames></author><author><keyname>Assis</keyname><forenames>R.</forenames></author></authors><title>The 12 prophets dataset</title><categories>cs.GR</categories><comments>Full dataset online at http://aleijadinho3d.icmc.usp.br/data.html</comments><report-no>University of Sao Paulo, Technical Report ICMC-USP 400, 2014</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The &quot;Ajeijadinho 3D&quot; project is an initiative supported by the University of
S\~ao Paulo (Museum of Science and Dean of Culture and Extension), which
involves the 3D digitization of art works of Brazilian sculptor Antonio
Francisco Lisboa, better known as Aleijadinho. The project made use of advanced
acquisition and processing of 3D meshes for preservation and dissemination of
the cultural heritage. The dissemination occurs through a Web portal, so that
the population has the opportunity to meet the art works in detail using 3D
visualization and interaction. The portal address is
http://www.aleijadinho3d.icmc.usp.br. The 3D acquisitions were conducted over a
week at the end of July 2013 in the cities of Ouro Preto, MG, Brazil and
Congonhas do Campo, MG, Brazil. The scanning was done with a special equipment
supplied by company Leica Geosystems, which allowed the work to take place at
distances between 10 and 30 meters, defining a non-invasive procedure,
simplified logistics, and without the need for preparation or isolation of the
sites. In Ouro Preto, we digitized the churches of Francisco of Assis, Our Lady
of Carmo, and Our Lady of Mercy; in Congonhas do Campo we scanned the entire
Sanctuary of Bom Jesus de Matosinhos and his 12 prophets. Once scanned, the art
works went through a long process of preparation, which required careful
handling of meshes done by experts from the University of S\~ao Paulo in
partnership with company Imprimate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06099</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06099</id><created>2015-06-19</created><updated>2015-11-07</updated><authors><author><keyname>Mudunuru</keyname><forenames>M. K.</forenames></author><author><keyname>Nakshatrala</keyname><forenames>K. B.</forenames></author></authors><title>On enforcing maximum principles and achieving element-wise species
  balance for advection-diffusion-reaction equations under the finite element
  method</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a robust computational framework for advective-diffusive-reactive
systems that satisfies maximum principles, the non-negative constraint, and
element-wise species balance property. The proposed methodology is valid on
general computational grids, can handle heterogeneous anisotropic media, and
provides accurate numerical solutions even for very high P\'eclet numbers. The
significant contribution of this paper is to incorporate advection (which makes
the spatial part of the differential operator non-self-adjoint) into the
non-negative computational framework, and overcome numerical challenges
associated with advection. We employ low-order mixed finite element
formulations based on least-squares formalism, and enforce explicit constraints
on the discrete problem to meet the desired properties. The resulting
constrained discrete problem belongs to convex quadratic programming for which
a unique solution exists. Maximum principles and the non-negative constraint
give rise to bound constraints while element-wise species balance gives rise to
equality constraints. The resulting convex quadratic programming problems are
solved using an interior-point algorithm. Several numerical results pertaining
to advection-dominated problems are presented to illustrate the robustness,
convergence, and the overall performance of the proposed computational
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06100</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06100</id><created>2015-06-19</created><authors><author><keyname>Bouchard</keyname><forenames>Guillaume</forenames></author><author><keyname>Lakshminarayanan</keyname><forenames>Balaji</forenames></author></authors><title>Approximate Inference with the Variational Holder Bound</title><categories>stat.ML cs.LG math.FA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the Variational Holder (VH) bound as an alternative to
Variational Bayes (VB) for approximate Bayesian inference. Unlike VB which
typically involves maximization of a non-convex lower bound with respect to the
variational parameters, the VH bound involves minimization of a convex upper
bound to the intractable integral with respect to the variational parameters.
Minimization of the VH bound is a convex optimization problem; hence the VH
method can be applied using off-the-shelf convex optimization algorithms and
the approximation error of the VH bound can also be analyzed using tools from
convex optimization literature. We present experiments on the task of
integrating a truncated multivariate Gaussian distribution and compare our
method to VB, EP and a state-of-the-art numerical integration method for this
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06102</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06102</id><created>2015-06-19</created><authors><author><keyname>Bauman</keyname><forenames>Paul T.</forenames></author><author><keyname>Stogner</keyname><forenames>Roy H.</forenames></author></authors><title>GRINS: A Multiphysics Framework Based on the libMesh Finite Element
  Library</title><categories>cs.MS cs.CE</categories><comments>Submitted to SISC CSE Special Issue</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The progression of scientific computing resources has enabled the numerical
approximation of mathematical models describing complex physical phenomena. A
significant portion of researcher time is typically dedicated to the
development of software to compute the numerical solutions. This work describes
a flexible C++ software framework, built on the libMesh finite element library,
designed to alleviate developer burden and provide easy access to modern
computational algorithms, including quantity-of-interest-driven parallel
adaptive mesh refinement on unstructured grids and adjoint-based sensitivities.
Other software environments are highlighted and the current work motivated; in
particular, the present work is an attempt to balance software infrastructure
and user flexibility. The applicable class of problems and design of the
software components is discussed in detail. Several examples demonstrate the
effectiveness of the design, including applications that incorporate
uncertainty. Current and planned developments are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06107</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06107</id><created>2015-06-19</created><authors><author><keyname>Mikl&#xf3;s</keyname><forenames>Istv&#xe1;n</forenames></author><author><keyname>Smith</keyname><forenames>Heather</forenames></author></authors><title>The computational complexity of calculating partition functions of
  optimal medians with Hamming distance</title><categories>cs.CC cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single Cut-or-Join is, computationally, the simplest mathematical model of
genome rearrangement. Given a collection of species, we fix a tree which
represents their ancestral histories, labeling the leaves with the observed
genomes. To assess the likelihood of an assignment of ancestors, we use the
parsimony criterion. For each most parsimonious labeling of the ancestors, we
relate each pair of vertices along a common edge with an Single Cut-or-Join
scenario to create a most parsimonious SCJ scenario.
  Here, we determine that the problem of counting the number of most
parsimonious SCJ scenarios is \#P-complete when the ancestral tree is a star or
a binary tree. Further, we extend the result for star trees to analogous
questions, weighting each most parsimonious labeling in various ways in the
summation over all most parsimonious labelings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06112</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06112</id><created>2015-06-19</created><updated>2016-01-11</updated><authors><author><keyname>Rudd</keyname><forenames>Ethan M.</forenames></author><author><keyname>Jain</keyname><forenames>Lalit P.</forenames></author><author><keyname>Scheirer</keyname><forenames>Walter J.</forenames></author><author><keyname>Boult</keyname><forenames>Terrance E.</forenames></author></authors><title>The Extreme Value Machine</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a novel characterization of the max-margin distribution
in input space. The use of the statistical Extreme Value Theory (EVT) is
introduced for modeling margin distances, allowing us to derive a scalable
non-linear model called the Extreme Value Machine (EVM). Without the need for a
kernel, the EVM leverages a margin model to estimate the probability of sample
inclusion in each class. The EVM selects a near-optimal subset of the training
vectors to optimize the gain in terms of points covered versus parameters used.
We show that the problem reduces to the NP-hard Set Cover problem which has a
provable polynomial time approximation. The resulting machine has comparable
closed set accuracy (i.e., when all testing classes are known at training time)
to optimized RBF SVMs and exhibits far superior performance in open set
recognition (i.e., when unknown classes exist at testing time). In open set
recognition performance, the EVM is more accurate and more scalable than the
state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06114</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06114</id><created>2015-06-19</created><authors><author><keyname>Mukherjee</keyname><forenames>Pritam</forenames></author><author><keyname>Xie</keyname><forenames>Jianwei</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>Secure Degrees of Freedom of One-hop Wireless Networks with No
  Eavesdropper CSIT</title><categories>cs.IT cs.CR math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, June 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider three channel models: the wiretap channel with $M$ helpers, the
$K$-user multiple access wiretap channel, and the $K$-user interference channel
with an external eavesdropper, when no eavesdropper's channel state information
(CSI) is available at the transmitters. In each case, we establish the optimal
sum secure degrees of freedom (s.d.o.f.) by providing achievable schemes and
matching converses. We show that the unavailability of the eavesdropper's CSIT
does not reduce the s.d.o.f. of the wiretap channel with helpers. However,
there is loss in s.d.o.f. for both the multiple access wiretap channel and the
interference channel with an external eavesdropper. In particular, we show that
in the absence of eavesdropper's CSIT, the $K$-user multiple access wiretap
channel reduces to a wiretap channel with $(K-1)$ helpers from a sum s.d.o.f.
perspective, and the optimal sum s.d.o.f. reduces from
$\frac{K(K-1)}{K(K-1)+1}$ to $\frac{K-1}{K}$. For the interference channel with
an external eavesdropper, the optimal sum s.d.o.f. decreases from
$\frac{K(K-1)}{2K-1}$ to $\frac{K-1}{2}$ in the absence of the eavesdropper's
CSIT. Our results show that the lack of eavesdropper's CSIT does not have a
significant impact on the optimal s.d.o.f. for any of the three channel models,
especially when the number of users is large. This implies that physical layer
security can be made robust to the unavailability of eavesdropper CSIT at high
signal to noise ratio (SNR) regimes by careful modification of the achievable
schemes as demonstrated in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06129</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06129</id><created>2015-06-18</created><authors><author><keyname>Wiggins</keyname><forenames>Paul A.</forenames></author></authors><title>A simple application of FIC to model selection</title><categories>physics.data-an cs.LG stat.ML</categories><comments>7 Pages, 1 figure, &amp; Appendix. arXiv admin note: text overlap with
  arXiv:1506.05855</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have recently proposed a new information-based approach to model
selection, the Frequentist Information Criterion (FIC), that reconciles
information-based and frequentist inference. The purpose of this current paper
is to provide a simple example of the application of this criterion and a
demonstration of the natural emergence of model complexities with both AIC-like
($N^0$) and BIC-like ($\log N$) scaling with observation number $N$. The
application developed is deliberately simplified to make the analysis
analytically tractable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06138</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06138</id><created>2015-06-19</created><authors><author><keyname>Marzen</keyname><forenames>Sarah E.</forenames></author><author><keyname>DeDeo</keyname><forenames>Simon</forenames></author></authors><title>The evolution of lossy compression</title><categories>q-bio.NC cs.IT math.IT nlin.AO physics.soc-ph q-bio.PE</categories><comments>14 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In complex environments, there are costs to both ignorance and perception. An
organism needs to track fitness-relevant information about its world, but the
more information it tracks, the more resources it must devote to memory and
processing. Rate-distortion theory shows that, when errors are allowed,
remarkably efficient internal representations can be found by
biologically-plausible hill-climbing mechanisms. We identify two regimes: a
high-fidelity regime where perceptual costs scale logarithmically with
environmental complexity, and a low-fidelity regime where perceptual costs are,
remarkably, independent of the environment. When environmental complexity is
rising, Darwinian evolution should drive organisms to the threshold between the
high- and low-fidelity regimes. Organisms that code efficiently will find
themselves able to make, just barely, the most subtle distinctions in their
environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06149</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06149</id><created>2015-06-19</created><authors><author><keyname>Morcillo</keyname><forenames>Jes&#xfa;s Mu&#xf1;oz</forenames></author><author><keyname>Czurda</keyname><forenames>Klemens</forenames></author><author><keyname>Trotha</keyname><forenames>Caroline Y. Robertson-von</forenames></author></authors><title>Typologies of the Popular Science Web Video</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The creation of popular science web videos on the Internet has increased in
recent years. The diversity of formats, genres, and producers makes it
difficult to formulate a universal definition of science web videos since not
every producer considers him- or herself to be a science communicator in an
institutional sense, and professionalism and success on video platforms no
longer depend exclusively on technical excellence or production costs.
Entertainment, content quality, and authenticity have become the keys to
community building and success. The democratization of science video production
allows a new variety of genres, styles, and forms. This article provides a
first overview of the typologies and characteristics of popular science web
videos. To avoid a misleading identification of science web videos with
institutionally produced videos, we steer clear of the term science
communication video, since many of the actual producers are not even familiar
with the academic discussion on science communication, and since the subject
matter does not depend on political or educational strategies. A content
analysis of 200 videos from 100 online video channels was conducted. Several
factors such as narrative strategies, video editing techniques, and design
tendencies with regard to cinematography, the number of shots, the kind of
montage used, and even the spread use of sound design and special FX point to
an increasing professionalism among science communicators independent of
institutional or personal commitments: in general, it can be said that supposed
amateurs are creating the visual language of science video communication. This
study represents an important step in understanding the essence of current
popular science web videos and provides an evidence-based definition as a
helpful cornerstone for further studies on science communication within this
kind of new media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06154</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06154</id><created>2015-06-19</created><authors><author><keyname>Cloud</keyname><forenames>Jason</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Network Coding Over SATCOM: Lessons Learned</title><categories>cs.NI</categories><comments>Accepted to WiSATS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Satellite networks provide unique challenges that can restrict users' quality
of service. For example, high packet erasure rates and large latencies can
cause significant disruptions to applications such as video streaming or
voice-over-IP. Network coding is one promising technique that has been shown to
help improve performance, especially in these environments. However,
implementing any form of network code can be challenging. This paper will use
an example of a generation-based network code and a sliding-window network code
to help highlight the benefits and drawbacks of using one over the other.
In-order packet delivery delay, as well as network efficiency, will be used as
metrics to help differentiate between the two approaches. Furthermore, lessoned
learned during the course of our research will be provided in an attempt to
help the reader understand when and where network coding provides its benefits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06155</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06155</id><created>2015-06-19</created><updated>2015-06-24</updated><authors><author><keyname>Norouzi</keyname><forenames>Mohammad</forenames></author><author><keyname>Collins</keyname><forenames>Maxwell D.</forenames></author><author><keyname>Fleet</keyname><forenames>David J.</forenames></author><author><keyname>Kohli</keyname><forenames>Pushmeet</forenames></author></authors><title>CO2 Forest: Improved Random Forest by Continuous Optimization of Oblique
  Splits</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel algorithm for optimizing multivariate linear threshold
functions as split functions of decision trees to create improved Random Forest
classifiers. Standard tree induction methods resort to sampling and exhaustive
search to find good univariate split functions. In contrast, our method
computes a linear combination of the features at each node, and optimizes the
parameters of the linear combination (oblique) split functions by adopting a
variant of latent variable SVM formulation. We develop a convex-concave upper
bound on the classification loss for a one-level decision tree, and optimize
the bound by stochastic gradient descent at each internal node of the tree.
Forests of up to 1000 Continuously Optimized Oblique (CO2) decision trees are
created, which significantly outperform Random Forest with univariate splits
and previous techniques for constructing oblique trees. Experimental results
are reported on multi-class classification benchmarks and on Labeled Faces in
the Wild (LFW) dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06157</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06157</id><created>2015-06-19</created><authors><author><keyname>Puleo</keyname><forenames>Gregory J.</forenames></author></authors><title>Complexity of a Disjoint Matching Problem on Bipartite Graphs</title><categories>cs.DS cs.DM math.CO</categories><comments>6 pages, 1 figure</comments><msc-class>05C70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following question: given an $(X,Y)$-bigraph $G$ and a set $S
\subset X$, does $G$ contain two disjoint matchings $M_1$ and $M_2$ such that
$M_1$ saturates $X$ and $M_2$ saturates $S$? When $|S|\geq |X|-1$, this
question is solvable by finding an appropriate factor of the graph. In
contrast, we show that when $S$ is allowed to be an arbitrary subset of $X$,
the problem is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06158</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06158</id><created>2015-06-19</created><authors><author><keyname>Weiss</keyname><forenames>David</forenames></author><author><keyname>Alberti</keyname><forenames>Chris</forenames></author><author><keyname>Collins</keyname><forenames>Michael</forenames></author><author><keyname>Petrov</keyname><forenames>Slav</forenames></author></authors><title>Structured Training for Neural Network Transition-Based Parsing</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present structured perceptron training for neural network transition-based
dependency parsing. We learn the neural network representation using a gold
corpus augmented by a large number of automatically parsed sentences. Given
this fixed network representation, we learn a final layer using the structured
perceptron with beam-search decoding. On the Penn Treebank, our parser reaches
94.26% unlabeled and 92.41% labeled attachment accuracy, which to our knowledge
is the best accuracy on Stanford Dependencies to date. We also provide in-depth
ablative analysis to determine which aspects of our model provide the largest
gains in accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06162</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06162</id><created>2015-06-19</created><authors><author><keyname>Borgs</keyname><forenames>Christian</forenames></author><author><keyname>Chayes</keyname><forenames>Jennifer T.</forenames></author><author><keyname>Smith</keyname><forenames>Adam</forenames></author></authors><title>Private Graphon Estimation for Sparse Graphs</title><categories>math.ST cs.CR cs.DS stat.TH</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design algorithms for fitting a high-dimensional statistical model to a
large, sparse network without revealing sensitive information of individual
members. Given a sparse input graph $G$, our algorithms output a
node-differentially-private nonparametric block model approximation. By
node-differentially-private, we mean that our output hides the insertion or
removal of a vertex and all its adjacent edges. If $G$ is an instance of the
network obtained from a generative nonparametric model defined in terms of a
graphon $W$, our model guarantees consistency, in the sense that as the number
of vertices tends to infinity, the output of our algorithm converges to $W$ in
an appropriate version of the $L_2$ norm. In particular, this means we can
estimate the sizes of all multi-way cuts in $G$.
  Our results hold as long as $W$ is bounded, the average degree of $G$ grows
at least like the log of the number of vertices, and the number of blocks goes
to infinity at an appropriate rate. We give explicit error bounds in terms of
the parameters of the model; in several settings, our bounds improve on or
match known nonprivate results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06163</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06163</id><created>2015-06-19</created><updated>2015-09-18</updated><authors><author><keyname>Chandu</keyname><forenames>Drona Pratap</forenames></author></authors><title>Big Step Greedy Algorithm for Maximum Coverage Problem</title><categories>cs.DS</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a greedy heuristic named as Big step greedy heuristic and
investigates the application of Big step greedy heuristic for maximum
k-coverage problem. Greedy algorithms construct the solution in multiple steps,
the classical greedy algorithm for maximum k-coverage problem, in each step
selects one set that contains the greatest number of uncovered elements. The
Big step greedy heuristic, in each step selects p (1 &lt;= p &lt;= k) sets such that
the union of selected p sets contains the greatest number of uncovered elements
by evaluating all possible p-combinations of given sets. When p=k Big step
greedy algorithm behaves like exact algorithm that computes optimal solution by
evaluating all possible k-combinations of given sets. When p=1 it behaves like
the classical greedy algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06165</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06165</id><created>2015-06-19</created><updated>2015-09-16</updated><authors><author><keyname>Chatzieleftheriou</keyname><forenames>George</forenames><affiliation>Aristotle University of Thessaloniki, Greece</affiliation></author><author><keyname>Bonakdarpour</keyname><forenames>Borzoo</forenames><affiliation>University of Waterloo, Canada</affiliation></author><author><keyname>Katsaros</keyname><forenames>Panagiotis</forenames><affiliation>Aristotle University of Thessaloniki, Greece</affiliation></author><author><keyname>Smolka</keyname><forenames>Scott A.</forenames><affiliation>Stony Brook University, NY, USA</affiliation></author></authors><title>Abstract Model Repair</title><categories>cs.LO</categories><comments>43 pages</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:11) 2015</journal-ref><doi>10.2168/LMCS-11(3:11)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a Kripke structure M and CTL formula $\varphi$, where M does not
satisfy $\varphi$, the problem of Model Repair is to obtain a new model M' such
that M' satisfies $\varphi$. Moreover, the changes made to M to derive M'
should be minimum with respect to all such M'. As in model checking, state
explosion can make it virtually impossible to carry out model repair on models
with infinite or even large state spaces. In this paper, we present a framework
for model repair that uses abstraction refinement to tackle state explosion.
Our framework aims to repair Kripke Structure models based on a Kripke Modal
Transition System abstraction and a 3-valued semantics for CTL. We introduce an
abstract-model-repair algorithm for which we prove soundness and
semi-completeness, and we study its complexity class. Moreover, a prototype
implementation is presented to illustrate the practical utility of
abstract-model-repair on an Automatic Door Opener system model and a model of
the Andrew File System 1 protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06166</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06166</id><created>2015-06-19</created><authors><author><keyname>Fu</keyname><forenames>Peng</forenames></author><author><keyname>Komendantskaya</keyname><forenames>Ekaterina</forenames></author></authors><title>A Type-Theoretic Approach to Structural Resolution</title><categories>cs.LO</categories><comments>LOPSTR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structural resolution (or S-resolution) is a newly proposed alternative to
SLD-resolution that allows a systematic separation of derivations into
term-matching and unification steps. Productive logic programs are those for
which term-matching reduction on any query must terminate. For productive
programs with coinductive meaning, finite term-rewriting reductions can be seen
as measures of observation in an infinite derivation. Ability of handling
corecursion in a productive way is an attractive computational feature of
S-resolution.
  In this paper, we make first steps towards a better conceptual understanding
of operational properties of S-resolution as compared to SLD-resolution. To
this aim, we propose a type system for the analysis of both SLD-resolution and
S-resolution.
  We formulate S-resolution and SLD-resolution as reduction systems, and show
their soundness relative to the type system. One of the central methods of this
paper is realizability transformation, which makes logic programs productive
and non-overlapping. We show that S-resolution and SLD-resolution are only
equivalent for programs with these two properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06179</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06179</id><created>2015-06-19</created><authors><author><keyname>Ghasemian</keyname><forenames>Amir</forenames></author><author><keyname>Zhang</keyname><forenames>Pan</forenames></author><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>Peel</keyname><forenames>Leto</forenames></author></authors><title>Detectability thresholds and optimal algorithms for community structure
  in dynamic networks</title><categories>stat.ML cond-mat.dis-nn cs.LG cs.SI physics.data-an</categories><comments>9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the fundamental limits on learning latent community structure in
dynamic networks. Specifically, we study dynamic stochastic block models where
nodes change their community membership over time, but where edges are
generated independently at each time step. In this setting (which is a special
case of several existing models), we are able to derive the detectability
threshold exactly, as a function of the rate of change and the strength of the
communities. Below this threshold, we claim that no algorithm can identify the
communities better than chance. We then give two algorithms that are optimal in
the sense that they succeed all the way down to this limit. The first uses
belief propagation (BP), which gives asymptotically optimal accuracy, and the
second is a fast spectral clustering algorithm, based on linearizing the BP
equations. We verify our analytic and algorithmic results via numerical
simulation, and close with a brief discussion of extensions and open questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06185</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06185</id><created>2015-06-19</created><authors><author><keyname>Huber</keyname><forenames>Markus</forenames></author><author><keyname>Gmeiner</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>R&#xfc;de</keyname><forenames>Ulrich</forenames></author><author><keyname>Wohlmuth</keyname><forenames>Barbara</forenames></author></authors><title>Resilience for Multigrid Software at the Extreme Scale</title><categories>cs.MS cs.NA</categories><msc-class>65N55, 65Y05, 68Q85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fault tolerant algorithms for the numerical approximation of elliptic partial
differential equations on modern supercomputers play a more and more important
role in the future design of exa-scale enabled iterative solvers. Here, we
combine domain partitioning with highly scalable geometric multigrid schemes to
obtain fast and fault-robust solvers in three dimensions. The recovery strategy
is based on a hierarchical hybrid concept where the values on lower dimensional
primitives such as faces are stored redundantly and thus can be recovered
easily in case of a failure. The lost volume unknowns in the faulty region are
re-computed approximately with multigrid cycles by solving a local Dirichlet
problem on the faulty subdomain. Different strategies are compared and
evaluated with respect to performance, computational cost, and speed up.
Especially effective are strategies in which the local recovery in the faulty
region is executed in parallel with global solves and when the local recovery
is additionally accelerated. This results in an asynchronous multigrid
iteration that can fully compensate faults. Excellent parallel performance on a
current peta-scale system is demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06193</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06193</id><created>2015-06-19</created><updated>2015-07-21</updated><authors><author><keyname>Wang</keyname><forenames>Xinhua</forenames></author><author><keyname>Chen</keyname><forenames>Zengqiang</forenames></author><author><keyname>Yuan</keyname><forenames>Zhuzhi</forenames></author></authors><title>Modeling and control of an agile tail-sitter aircraft</title><categories>cs.SY</categories><doi>10.1016/j.jfranklin.2015.09.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a model of an agile tail-sitter aircraft, which can
operate as a helicopter as well as capable of transition to fixed-wing flight.
Aerodynamics of the co-axial counter-rotating propellers with quad rotors are
analysed under the condition that the co-axial is operated at equal rotor
torque (power). A finite-time convergent observer based on Lyapunov function is
presented to estimate the unknown nonlinear terms in co-axial counter-rotating
propellers, the uncertainties and external disturbances during mode transition.
Furthermore, a simple controller based on the finite-time convergent observer
and quaternion method is designed to implement mode transition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06194</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06194</id><created>2015-06-19</created><authors><author><keyname>Knepley</keyname><forenames>Matthew G.</forenames></author><author><keyname>Lange</keyname><forenames>Michael</forenames></author><author><keyname>Gorman</keyname><forenames>Gerard J.</forenames></author></authors><title>Unstructured Overlapping Mesh Distribution in Parallel</title><categories>cs.MS cs.DC math.NA</categories><comments>14 pages, 6 figures, submitted to TOMS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple mathematical framework and API for parallel mesh and data
distribution, load balancing, and overlap generation. It relies on viewing the
mesh as a Hasse diagram, abstracting away information such as cell shape,
dimension, and coordinates. The high level of abstraction makes our interface
both concise and powerful, as the same algorithm applies to any representable
mesh, such as hybrid meshes, meshes embedded in higher dimension, and
overlapped meshes in parallel. We present evidence, both theoretical and
experimental, that the algorithms are scalable and efficient. A working
implementation can be found in the latest release of the PETSc libraries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06199</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06199</id><created>2015-06-19</created><authors><author><keyname>Banerjee</keyname><forenames>Taposh</forenames></author><author><keyname>Firouzi</keyname><forenames>Hamed</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Non-parametric Quickest Change Detection for Large Scale Random Matrices</title><categories>math.ST cs.IT math.IT stat.ME stat.TH</categories><comments>Proc. of ISIT, Hong Kong, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of quickest detection of a change in the distribution of a
$n\times p$ random matrix based on a sequence of observations having a single
unknown change point is considered. The forms of the pre- and post-change
distributions of the rows of the matrices are assumed to belong to the family
of elliptically contoured densities with sparse dispersion matrices but are
otherwise unknown. We propose a non-parametric stopping rule that is based on a
novel summary statistic related to k-nearest neighbor correlation between
columns of each observed random matrix. In the large scale regime of
$p\rightarrow \infty$ and $n$ fixed we show that, among all functions of the
proposed summary statistic, the proposed stopping rule is asymptotically
optimal under a minimax quickest change detection (QCD) model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06204</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06204</id><created>2015-06-20</created><updated>2015-09-01</updated><authors><author><keyname>Pinheiro</keyname><forenames>Pedro O.</forenames></author><author><keyname>Collobert</keyname><forenames>Ronan</forenames></author><author><keyname>Dollar</keyname><forenames>Piotr</forenames></author></authors><title>Learning to Segment Object Candidates</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent object detection systems rely on two critical steps: (1) a set of
object proposals is predicted as efficiently as possible, and (2) this set of
candidate proposals is then passed to an object classifier. Such approaches
have been shown they can be fast, while achieving the state of the art in
detection performance. In this paper, we propose a new way to generate object
proposals, introducing an approach based on a discriminative convolutional
network. Our model is trained jointly with two objectives: given an image
patch, the first part of the system outputs a class-agnostic segmentation mask,
while the second part of the system outputs the likelihood of the patch being
centered on a full object. At test time, the model is efficiently applied on
the whole test image and generates a set of segmentation masks, each of them
being assigned with a corresponding object likelihood score. We show that our
model yields significant improvements over state-of-the-art object proposal
algorithms. In particular, compared to previous approaches, our model obtains
substantially higher object recall using fewer proposals. We also show that our
model is able to generalize to unseen categories it has not seen during
training. Unlike all previous approaches for generating object masks, we do not
rely on edges, superpixels, or any other form of low-level segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06205</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06205</id><created>2015-06-20</created><authors><author><keyname>Torres-Moreno</keyname><forenames>Juan-Manuel</forenames></author></authors><title>Trivergence of Probability Distributions, at glance</title><categories>cs.IT math.IT</categories><comments>10 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce the intuitive notion of trivergence of probability
distributions (TPD). This notion allow us to calculate the similarity among
triplets of objects. For this computation, we can use the well known measures
of probability divergences like Kullback-Leibler and Jensen-Shannon. Divergence
measures may be used in Information Retrieval tasks as Automatic Text
Summarization, Text Classification, among many others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06213</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06213</id><created>2015-06-20</created><authors><author><keyname>Ali</keyname><forenames>Abdelmohsen</forenames></author><author><keyname>Hamouda</keyname><forenames>Walaa</forenames></author></authors><title>Spectrum Monitoring Using Energy Ratio Algorithm For OFDM-Based
  Cognitive Radio Networks</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a spectrum monitoring algorithm for Orthogonal Frequency
Division Multiplexing (OFDM) based cognitive radios by which the primary user
reappearance can be detected during the secondary user transmission. The
proposed technique reduces the frequency with which spectrum sensing must be
performed and greatly decreases the elapsed time between the start of a primary
transmission and its detection by the secondary network. This is done by
sensing the change in signal strength over a number of reserved OFDM
sub-carriers so that the reappearance of the primary user is quickly detected.
Moreover, the OFDM impairments such as power leakage, Narrow Band Interference
(NBI), and Inter-Carrier Interference (ICI) are investigated and their impact
on the proposed technique is studied. Both analysis and simulation show that
the \emph{energy ratio} algorithm can effectively and accurately detect the
appearance of the primary user. Furthermore, our method achieves high immunity
to frequency-selective fading channels for both single and multiple receive
antenna systems, with a complexity that is approximately twice that of a
conventional energy detector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06215</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06215</id><created>2015-06-20</created><authors><author><keyname>Naveen</keyname><forenames>K. P.</forenames></author><author><keyname>Kumar</keyname><forenames>Anurag</forenames></author><author><keyname>Altman</keyname><forenames>Eitan</forenames></author></authors><title>Competitive Selection of Ephemeral Relays in Wireless Networks</title><categories>cs.NI cs.GT cs.SY</categories><comments>25 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a setting in which two nodes (referred to as forwarders) compete
to choose a relay node from a set of relays, as they ephemerally become
available (e.g., wake up from a sleep state). Each relay, when it arrives,
offers a (possibly different) &quot;reward&quot; to each forwarder. Each forwarder's
objective is to minimize a combination of the delay incurred in choosing a
relay and the reward offered by the chosen relay. As an example, we develop the
reward structure for the specific problem of geographical forwarding over a
network of sleep-wake cycling relays.
  We study two variants of the generic relay selection problem, namely, the
completely observable (CO) case where, when a relay arrives, both forwarders
get to observe both rewards, and the partially observable (PO) case where each
forwarder can only observe its own reward. Formulating the problem as a two
person stochastic game, we characterize solution in terms of Nash Equilibrium
Policy Pairs (NEPPs). For the CO case we provide a general structure of the
NEPPs. For the PO case we prove that there exists an NEPP within the class of
threshold policy pairs.
  We then consider the particular application of geographical forwarding of
packets in a shared network of sleep-wake cycling wireless relays. For this
problem, for a particular reward structure, using realistic parameter values
corresponding to TelosB wireless mote, we numerically compare the performance
(in terms of cost to both forwarders) of the various NEPPs and draw the
following key insight: even for moderate separation between the two forwarders,
the performance of the various NEPPs is close to the performance of a simple
strategy where each forwarder behaves as if the other forwarder is not present.
We also conduct simulation experiments to study the end-to-end performance of
the simple forwarding policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06216</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06216</id><created>2015-06-20</created><authors><author><keyname>Ali</keyname><forenames>Abdelmohsen</forenames></author><author><keyname>Hamouda</keyname><forenames>Walaa</forenames></author><author><keyname>Uysal</keyname><forenames>Murat</forenames></author></authors><title>Next Generation M2M Cellular Networks: Challenges and Practical
  Considerations</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we present the major challenges of future machine-to-machine
(M2M) cellular networks such as spectrum scarcity problem, support for
low-power, low-cost, and numerous number of devices. As being an integral part
of the future Internet-of-Things (IoT), the true vision of M2M communications
cannot be reached with conventional solutions that are typically cost
inefficient. Cognitive radio concept has emerged to significantly tackle the
spectrum under-utilization or scarcity problem. Heterogeneous network model is
another alternative to relax the number of covered users. To this extent, we
present a complete fundamental understanding and engineering knowledge of
cognitive radios, heterogeneous network model, and power and cost challenges in
the context of future M2M cellular networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06221</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06221</id><created>2015-06-20</created><updated>2016-01-04</updated><authors><author><keyname>Magalingam</keyname><forenames>Pritheega</forenames></author><author><keyname>Davis</keyname><forenames>Stephen</forenames></author><author><keyname>Rao</keyname><forenames>Asha</forenames></author></authors><title>Ranking the Importance Level of Intermediaries to a Criminal using a
  Reliance Measure</title><categories>cs.SI physics.soc-ph</categories><comments>Paper version 3.0</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research on finding important intermediate nodes in a network
suspected to contain criminal activity is highly dependent on network
centrality values. Betweenness centrality, for example, is widely used to rank
the nodes that act as brokers in the shortest paths connecting all source and
all the end nodes in a network. However both the shortest path node betweenness
and the linearly scaled betweenness can only show rankings for all the nodes in
a network. In this paper we explore the mathematical concept of pair-dependency
on intermediate nodes, adapting the concept to criminal relationships and
introducing a new source-intermediate reliance measure. To illustrate our
measure, we apply it to rank the nodes in the Enron email dataset and the
Noordin Top Terrorist networks. We compare the reliance ranking with Google
PageRank, Markov centrality as well as betweenness centrality and show that a
criminal investigation using the reliance measure, will lead to a different
prioritisation in terms of possible people to investigate. While the ranking
for the Noordin Top terrorist network nodes yields more extreme differences
than for the Enron email transaction network, in the latter the reliance values
for the set of finance managers immediately identified another employee
convicted of money laundering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06256</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06256</id><created>2015-06-20</created><authors><author><keyname>Fursin</keyname><forenames>Grigori</forenames></author><author><keyname>Memon</keyname><forenames>Abdul</forenames></author><author><keyname>Guillon</keyname><forenames>Christophe</forenames></author><author><keyname>Lokhmotov</keyname><forenames>Anton</forenames></author></authors><title>Collective Mind, Part II: Towards Performance- and Cost-Aware Software
  Engineering as a Natural Science</title><categories>cs.SE cs.LG cs.PF</categories><comments>Presented at the 18th International Workshop on Compilers for
  Parallel Computing (CPC'15), London, UK</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, engineers have to develop software often without even knowing which
hardware it will eventually run on in numerous mobile phones, tablets,
desktops, laptops, data centers, supercomputers and cloud services.
Unfortunately, optimizing compilers are not keeping pace with ever increasing
complexity of computer systems anymore and may produce severely underperforming
executable codes while wasting expensive resources and energy.
  We present our practical and collaborative solution to this problem via
light-weight wrappers around any software piece when more than one
implementation or optimization choice available. These wrappers are connected
with a public Collective Mind autotuning infrastructure and repository of
knowledge (c-mind.org/repo) to continuously monitor various important
characteristics of these pieces (computational species) across numerous
existing hardware configurations together with randomly selected optimizations.
Similar to natural sciences, we can now continuously track winning solutions
(optimizations for a given hardware) that minimize all costs of a computation
(execution time, energy spent, code size, failures, memory and storage
footprint, optimization time, faults, contentions, inaccuracy and so on) of a
given species on a Pareto frontier along with any unexpected behavior. The
community can then collaboratively classify solutions, prune redundant ones,
and correlate them with various features of software, its inputs (data sets)
and used hardware either manually or using powerful predictive analytics
techniques. Our approach can then help create a large, realistic, diverse,
representative, and continuously evolving benchmark with related optimization
knowledge while gradually covering all possible software and hardware to be
able to predict best optimizations and improve compilers and hardware depending
on usage scenarios and requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06261</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06261</id><created>2015-06-20</created><authors><author><keyname>Vallabhan</keyname><forenames>M.</forenames></author><author><keyname>Seshadhri</keyname><forenames>S.</forenames></author><author><keyname>Ashok</keyname><forenames>S.</forenames></author><author><keyname>Ramaswmay</keyname><forenames>S.</forenames></author><author><keyname>Ayyagari</keyname><forenames>R.</forenames></author></authors><title>An analytical framework for analysis and design of networked control
  systems with random delays and packet losses</title><categories>cs.SY cs.NI</categories><comments>Proceedings of the 25th IEEE Canadian Conference on Electrical and
  Computer Engineering (CCECE)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delays and data losses are undesirable from a control system perspective as
they tend to adversely affect performance Networked Control Systems (NCSs) are
a class of control systems wherein control components exchange information
using a shared communication channel. Delays and packet losses in the
communication channels are usually random, thereby making the analysis and
design of control loops more complex. The usual assumptions in classical
control theory, such as delay free sensing and synchronous actuation, assume
lesser significance when it comes to NCSs. Hence, this necessitates a
reformulation/relook into the existing models used for NCS control loop
analysis and design. In this paper, we study and present the reformulations
required for NCSs to include random delays and packet loss in the channel. This
paper thereby gives a complete overview of what has been accomplished thus far
in NCS research and puts forth a unified framework for analyzing a host of
problems that can be captured as NCSs subjected to random delays and packet
losses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06265</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06265</id><created>2015-06-20</created><authors><author><keyname>Chattopadhyay</keyname><forenames>Amit</forenames></author><author><keyname>Vegter</keyname><forenames>Gert</forenames></author><author><keyname>Yap</keyname><forenames>Chee K.</forenames></author></authors><title>Certified Computation of planar Morse-Smale Complexes</title><categories>cs.CG</categories><comments>Under Review in Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Morse-Smale complex is an important tool for global topological analysis
in various problems of computational geometry and topology. Algorithms for
Morse-Smale complexes have been presented in case of piecewise linear
manifolds. However, previous research in this field is incomplete in the case
of smooth functions. In the current paper we address the following question:
Given an arbitrarily complex Morse-Smale system on a planar domain, is it
possible to compute its certified (topologically correct) Morse-Smale complex?
Towards this, we develop an algorithm using interval arithmetic to compute
certified critical points and separatrices forming the Morse-Smale complexes of
smooth functions on bounded planar domain. Our algorithm can also compute
geometrically close Morse-Smale complexes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06270</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06270</id><created>2015-06-20</created><authors><author><keyname>Cooper</keyname><forenames>S. Barry</forenames></author></authors><title>The Machine as Data: A Computational View of Emergence and Definability</title><categories>math.LO cs.GL</categories><comments>35 pages, to appear in journal 'Synthese'</comments><msc-class>03D99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Turing's (1936) paper on computable numbers has played its role in
underpinning different perspectives on the world of information. On the one
hand, it encourages a digital ontology, with a perceived flatness of
computational structure comprehensively hosting causality at the physical level
and beyond. On the other (the main point of Turing's paper), it can give an
insight into the way in which higher order information arises and leads to loss
of computational control - while demonstrating how the control can be
re-established, in special circumstances, via suitable type reductions. We
examine the classical computational framework more closely than is usual,
drawing out lessons for the wider application of information-theoretical
approaches to characterizing the real world. The problem which arises across a
range of contexts is the characterizing of the balance of power between the
complexity of informational structure (with emergence, chaos, randomness and
'big data' prominently on the scene) and the means available (simulation,
codes, statistical sampling, human intuition, semantic constructs) to bring
this information back into the computational fold. We proceed via appropriate
mathematical modelling to a more coherent view of the computational structure
of information, relevant to a wide spectrum of areas of investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06271</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06271</id><created>2015-06-20</created><authors><author><keyname>Cao</keyname><forenames>Ruohan</forenames></author><author><keyname>Gao</keyname><forenames>Hui</forenames></author><author><keyname>Lv</keyname><forenames>Tiejun</forenames></author><author><keyname>Yang</keyname><forenames>Shaoshi</forenames></author><author><keyname>Huang</keyname><forenames>Shanguo</forenames></author></authors><title>Phase Rotation Aided Relay Selection in Two-Way Decode-and-Forward Relay
  Networks</title><categories>cs.IT math.IT</categories><comments>14 pages, 8 figures, 4 appendices, accepted to publish on IEEE
  Transactions on Vehicular Technology, June 2015</comments><doi>10.1109/TVT.2015.2442622</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a relay selection scheme that aims to improve the
end-to-end symbol error rate (SER) performance of a two-way relay network
(TWRN). The TWRN consists of two single-antenna sources and multiple relays
employing decode-and-forward (DF) protocol. It is shown that the SER
performance is determined by the minimum decision distance (DD) observed in the
TWRN. However, the minimum DD is likely to be made arbitrarily small by channel
fading. To tackle this problem, a phase rotation (PR) aided relay selection
(RS) scheme is proposed to enlarge the minium DD, which in turn improves the
SER performance. The proposed PR based scheme rotates the phases of the
transmitted symbols of one source and of the selected relay according to the
channel state information, aiming for increasing all DDs to be above a desired
bound. The lower bound is further optimized by using a MaxMin-RS criterion
associated with the channel gains. It is demonstrated that the PR aided
MaxMin-RS approach achieves full diversity gain and an improved array gain.
Furthermore, compared with the existing DF based schemes, the proposed scheme
allows more flexible relay antenna configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06272</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06272</id><created>2015-06-20</created><authors><author><keyname>Jin</keyname><forenames>Junqi</forenames></author><author><keyname>Fu</keyname><forenames>Kun</forenames></author><author><keyname>Cui</keyname><forenames>Runpeng</forenames></author><author><keyname>Sha</keyname><forenames>Fei</forenames></author><author><keyname>Zhang</keyname><forenames>Changshui</forenames></author></authors><title>Aligning where to see and what to tell: image caption with region-based
  attention and scene factorization</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent progress on automatic generation of image captions has shown that it
is possible to describe the most salient information conveyed by images with
accurate and meaningful sentences. In this paper, we propose an image caption
system that exploits the parallel structures between images and sentences. In
our model, the process of generating the next word, given the previously
generated ones, is aligned with the visual perception experience where the
attention shifting among the visual regions imposes a thread of visual
ordering. This alignment characterizes the flow of &quot;abstract meaning&quot;, encoding
what is semantically shared by both the visual scene and the text description.
Our system also makes another novel modeling contribution by introducing
scene-specific contexts that capture higher-level semantic information encoded
in an image. The contexts adapt language models for word generation to specific
scene types. We benchmark our system and contrast to published results on
several popular datasets. We show that using either region-based attention or
scene-specific contexts improves systems without those components. Furthermore,
combining these two modeling ingredients attains the state-of-the-art
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06273</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06273</id><created>2015-06-20</created><authors><author><keyname>Ma</keyname><forenames>Chuiwen</forenames></author><author><keyname>Shi</keyname><forenames>Liang</forenames></author><author><keyname>Huang</keyname><forenames>Hanlu</forenames></author><author><keyname>Yan</keyname><forenames>Mengyuan</forenames></author></authors><title>3D Reconstruction from Full-view Fisheye Camera</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, we proposed a 3D reconstruction method for the full-view
fisheye camera. The camera we used is Ricoh Theta, which captures spherical
images and has a wide field of view (FOV). The conventional stereo apporach
based on perspective camera model cannot be directly applied and instead we
used a spherical camera model to depict the relation between 3D point and its
corresponding observation in the image. We implemented a system that can
reconstruct the 3D scene using captures from two or more cameras. A GUI is also
created to allow users to control the view perspective and obtain a better
intuition of how the scene is rebuilt. Experiments showed that our
reconstruction results well preserved the structure of the scene in the real
world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06274</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06274</id><created>2015-06-20</created><authors><author><keyname>Ma</keyname><forenames>Chuiwen</forenames></author><author><keyname>Su</keyname><forenames>Hao</forenames></author><author><keyname>Shi</keyname><forenames>Liang</forenames></author></authors><title>Pose Estimation Based on 3D Models</title><categories>cs.CV cs.LG cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we proposed a pose estimation system based on rendered image
training set, which predicts the pose of objects in real image, with knowledge
of object category and tight bounding box. We developed a patch-based
multi-class classification algorithm, and an iterative approach to improve the
accuracy. We achieved state-of-the-art performance on pose estimation task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06275</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06275</id><created>2015-06-20</created><updated>2015-06-25</updated><authors><author><keyname>Siek</keyname><forenames>Konrad</forenames></author><author><keyname>Wojciechowski</keyname><forenames>Pawe&#x142; T.</forenames></author></authors><title>Last-use Opacity: A Strong Safety Property for Transactional Memory with
  Early Release Support</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transactional Memory (TM) safety properties like opacity, TMS, VWC, etc.
regulate what values can be read by transactions to maintain consistency and to
prevent executing dangerous operations like accessing illegal memory addresses
or entering infinite loops. However, the existing preclude using the early
release mechanism as a technique for optimizing TM, by forbidding or imposing
impractical restrictions on reading from live transactions. This paper analyzes
these restrictions and introduces last-use opacity, a new strong safety
property that is nevertheless applicable to systems with early release. We also
empirically show performance gained from using early release in a last-use
opaque TM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06278</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06278</id><created>2015-06-20</created><updated>2015-10-07</updated><authors><author><keyname>Fotouhi</keyname><forenames>Babak</forenames></author><author><keyname>Momeni</keyname><forenames>Naghmeh</forenames></author></authors><title>Growing Multiplex Networks with Arbitrary Number of Layers</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><journal-ref>Phys. Rev. E 92, 062812 (2015)</journal-ref><doi>10.1103/PhysRevE.92.062812</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the problem of growing multiplex networks. Currently,
the results on the joint degree distribution of growing multiplex networks
present in the literature pertain to the case of two layers, and are confined
to the special case of homogeneous growth, and are limited to the state state
(that is, the limit of infinite size). In the present paper, we obtain
closed-form solutions for the joint degree distribution of heterogeneously
growing multiplex networks with arbitrary number of layers in the steady state.
Heterogeneous growth means that each incoming node establishes different
numbers of links in different layers. We consider both uniform and preferential
growth. We then extend the analysis of the uniform growth mechanism to
arbitrary times. We obtain a closed-form solution for the time-dependent joint
degree distribution of a growing multiplex network with arbitrary initial
conditions. Throughout, theoretical findings are corroborated with Monte Carlo
simulations. The results shed light on the effects of the initial network on
the transient dynamics of growing multiplex networks, and takes a step towards
characterizing the temporal variations of the connectivity of growing multiplex
networks, as well as predicting their future structural properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06279</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06279</id><created>2015-06-20</created><authors><author><keyname>Jones</keyname><forenames>Shawn M.</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>Avoiding Spoilers in Fan Wikis of Episodic Fiction</title><categories>cs.DL</categories><comments>18 pages, 31 figures, 3 tables, 2 algorithms</comments><acm-class>H.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A variety of fan-based wikis about episodic fiction (e.g., television shows,
novels, movies) exist on the World Wide Web. These wikis provide a wealth of
information about complex stories, but if readers are behind in their viewing
they run the risk of encountering &quot;spoilers&quot; -- information that gives away key
plot points before the intended time of the show's writers. Enterprising
readers might browse the wiki in a web archive so as to view the page prior to
a specific episode date and thereby avoid spoilers. Unfortunately, due to how
web archives choose the &quot;best&quot; page, it is still possible to see spoilers
(especially in sparse archives).
  In this paper we discuss how to use Memento to avoid spoilers. Memento uses
TimeGates to determine which best archived page to give back to the user,
currently using a minimum distance heuristic. We quantify how this heuristic is
inadequate for avoiding spoilers, analyzing data collected from fan wikis and
the Internet Archive. We create an algorithm for calculating the probability of
encountering a spoiler in a given wiki article. We conduct an experiment with
16 wiki sites for popular television shows. We find that 38% of those pages are
unavailable in the Internet Archive. We find that when accessing fan wiki pages
in the Internet Archive there is as much as a 66% chance of encountering a
spoiler. Using sample access logs from the Internet Archive, we find that 19%
of actual requests to the Wayback Machine for wikia.com pages ended in
spoilers. We suggest the use of a different minimum distance heuristic,
minpast, for wikis, using the desired datetime as an upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06284</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06284</id><created>2015-06-20</created><authors><author><keyname>Kolpakov</keyname><forenames>Roman</forenames></author><author><keyname>Posypkin</keyname><forenames>Mikhail</forenames></author></authors><title>Upper bound on the number of steps for solving the subset sum problem by
  the Branch-and-Bound method</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of one of the particular cases of the
knapsack problem: the subset sum problem. For solving this problem we consider
one of the basic variants of the Branch-and-Bound method in which any
sub-problem is decomposed along the free variable with the maximal weight. By
the complexity of solving a problem by the Branch-and-Bound method we mean the
number of steps required for solving the problem by this method. In the paper
we obtain upper bounds on the complexity of solving the subset sum problem by
the Branch-and-Bound method. These bounds can be easily computed from the input
data of the problem. So these bounds can be used for the the preliminary
estimation of the computational resources required for solving the subset sum
problem by the Branch-and-Bound method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06289</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06289</id><created>2015-06-20</created><updated>2015-10-27</updated><authors><author><keyname>Tsakiris</keyname><forenames>Manolis C.</forenames></author><author><keyname>Vidal</keyname><forenames>Rene</forenames></author></authors><title>Filtrated Algebraic Subspace Clustering</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace clustering is the problem of clustering data that lie close to a
union of linear subspaces. In the abstract form of the problem, where no noise
or other corruptions are present, the data are assumed to lie in general
position inside the algebraic variety of a union of subspaces, and the
objective is to decompose the variety into its constituent subspaces. Prior
algebraic-geometric approaches to this problem require the subspaces to be of
equal dimension, or the number of subspaces to be known. Subspaces of arbitrary
dimensions can still be recovered in closed form, in terms of all homogeneous
polynomials of degree $m$ that vanish on their union, when an upper bound m on
the number of the subspaces is given. In this paper, we propose an alternative,
provably correct, algorithm for addressing a union of at most $m$
arbitrary-dimensional subspaces, based on the idea of descending filtrations of
subspace arrangements. Our algorithm uses the gradient of a vanishing
polynomial at a point in the variety to find a hyperplane containing the
subspace S passing through that point. By intersecting the variety with this
hyperplane, we obtain a subvariety that contains S, and recursively applying
the procedure until no non-trivial vanishing polynomial exists, our algorithm
eventually identifies S. By repeating this procedure for other points, our
algorithm eventually identifies all the subspaces by returning a basis for
their orthogonal complement. Finally, we develop a variant of the abstract
algorithm, suitable for computations with noisy data. We show by experiments on
synthetic and real data that the proposed algorithm outperforms
state-of-the-art methods on several occasions, thus demonstrating the merit of
the idea of filtrations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06294</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06294</id><created>2015-06-20</created><authors><author><keyname>Tong</keyname><forenames>Guangmo</forenames></author><author><keyname>Wu</keyname><forenames>Weili</forenames></author><author><keyname>Tang</keyname><forenames>Shaojie</forenames></author><author><keyname>Du</keyname><forenames>Ding-Zhu</forenames></author></authors><title>Adaptive Influence Maximization in Dynamic Social Networks</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the purpose of propagating information and ideas through a social
network, a seeding strategy aims to find a small set of seed users that are
able to maximize the spread of the influence, which is termed as influence
maximization problem. Despite a large number of works have studied this
problem, the existing seeding strategies are limited to the static social
networks. In fact, due to the high speed data transmission and the large
population of participants, the diffusion processes in real-world social
networks have many aspects of uncertainness. Unfortunately, as shown in the
experiments, in such cases the state-of-art seeding strategies are pessimistic
as they fails to trace the dynamic changes in a social network. In this paper,
we study the strategies selecting seed users in an adaptive manner. We first
formally model the Dynamic Independent Cascade model and introduce the concept
of adaptive seeding strategy. Then based on the proposed model, we show that a
simple greedy adaptive seeding strategy finds an effective solution with a
provable performance guarantee. Besides the greedy algorithm an efficient
heuristic algorithm is provided in order to meet practical requirements.
Extensive experiments have been performed on both the real-world networks and
synthetic power-law networks. The results herein demonstrate the superiority of
the adaptive seeding strategies to other standard methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06296</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06296</id><created>2015-06-20</created><authors><author><keyname>Chun</keyname><forenames>Young Jin</forenames></author><author><keyname>Hasna</keyname><forenames>Mazen Omar</forenames></author><author><keyname>Ghrayeb</keyname><forenames>Ali</forenames></author><author><keyname>Di Renzo</keyname><forenames>Marco</forenames></author></authors><title>On Modeling Heterogeneous Wireless Networks Using Non-Poisson Point
  Processes</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to IEEE Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future wireless networks are required to support 1000 times higher data rate,
than the current LTE standard. In order to meet the ever increasing demand, it
is inevitable that, future wireless networks will have to develop seamless
interconnection between multiple technologies. A manifestation of this idea is
the collaboration among different types of network tiers such as macro and
small cells, leading to the so-called heterogeneous networks (HetNets).
Researchers have used stochastic geometry to analyze such networks and
understand their real potential. Unsurprisingly, it has been revealed that
interference has a detrimental effect on performance, especially if not modeled
properly. Interference can be correlated in space and/or time, which has been
overlooked in the past. For instance, it is normally assumed that the nodes are
located completely independent of each other and follow a homogeneous Poisson
point process (PPP), which is not necessarily true in real networks since the
node locations are spatially dependent. In addition, the interference
correlation created by correlated stochastic processes has mostly been ignored.
To this end, we take a different approach in modeling the interference where we
use non-PPP, as well as we study the impact of spatial and temporal correlation
on the performance of HetNets. To illustrate the impact of correlation on
performance, we consider three case studies from real-life scenarios.
Specifically, we use massive multiple-input multiple-output (MIMO) to
understand the impact of spatial correlation; we use the random medium access
protocol to examine the temporal correlation; and we use cooperative relay
networks to illustrate the spatial-temporal correlation. We present several
numerical examples through which we demonstrate the impact of various
correlation types on the performance of HetNets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06299</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06299</id><created>2015-06-20</created><authors><author><keyname>Andreychenko</keyname><forenames>Alexander</forenames></author><author><keyname>Magnin</keyname><forenames>Morgan</forenames></author><author><keyname>Inoue</keyname><forenames>Katsumi</forenames></author></authors><title>Modeling of Resilience Properties in Oscillatory Biological Systems
  using Parametric Time Petri Nets, Supplementary Information</title><categories>cs.LO</categories><comments>8 pages, 2 figures, supplementary information</comments><msc-class>68Q60, 68Q45, 97M60</msc-class><acm-class>D.2.2; D.2.4; F.4.1; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated verification of living organism models allows us to gain previously
unknown knowledge about underlying biological processes. In this paper, we show
the benefits to use parametric time Petri nets in order to analyze precisely
the dynamic behavior of biological oscillatory systems. In particular, we focus
on the resilience properties of such systems. This notion is crucial to
understand the behavior of biological systems (e.g. the mammalian circadian
rhythm) that are reactive and adaptive enough to endorse major changes in their
environment (e.g. jet-lags, day-night alternating work-time). We formalize
these properties through parametric TCTL and demonstrate how changes of the
environmental conditions can be tackled to guarantee the resilience of living
organisms. In particular, we are able to discuss the influence of various
perturbations, e.g. artificial jet-lag or components knock-out, with regard to
quantitative delays. This analysis is crucial when it comes to model
elicitation for dynamic biological systems. We demonstrate the applicability of
this technique using a simplified model of circadian clock.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06302</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06302</id><created>2015-06-20</created><authors><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Lee</keyname><forenames>Euiwoong</forenames></author></authors><title>Inapproximability of $H$-Transversal/Packing</title><categories>cs.CC cs.DS</categories><comments>31 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph $G = (V_G, E_G)$ and a fixed &quot;pattern&quot; graph $H =
(V_H, E_H)$ with $k$ vertices, we consider the $H$-Transversal and $H$-Packing
problems. The former asks to find the smallest $S \subseteq V_G$ such that the
subgraph induced by $V_G \setminus S$ does not have $H$ as a subgraph, and the
latter asks to find the maximum number of pairwise disjoint $k$-subsets $S_1,
..., S_m \subseteq V_G$ such that the subgraph induced by each $S_i$ has $H$ as
a subgraph.
  We prove that if $H$ is 2-connected, $H$-Transversal and $H$-Packing are
almost as hard to approximate as general $k$-Hypergraph Vertex Cover and
$k$-Set Packing, so it is NP-hard to approximate them within a factor of
$\Omega (k)$ and $\widetilde \Omega (k)$ respectively. We also show that there
is a 1-connected $H$ where $H$-Transversal admits an $O(\log k)$-approximation
algorithm, so that the connectivity requirement cannot be relaxed from 2 to 1.
For a special case of $H$-Transversal where $H$ is a (family of) cycles, we
mention the implication of our result to the related Feedback Vertex Set
problem, and give a different hardness proof for directed graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06305</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06305</id><created>2015-06-20</created><authors><author><keyname>Garcia-Bernardo</keyname><forenames>J.</forenames></author><author><keyname>Qi</keyname><forenames>H.</forenames></author><author><keyname>Shultz</keyname><forenames>J. M.</forenames></author><author><keyname>Cohen</keyname><forenames>A. M.</forenames></author><author><keyname>Johnson</keyname><forenames>N. F.</forenames></author><author><keyname>Dodds</keyname><forenames>P. S.</forenames></author></authors><title>Social media affects the timing, location, and severity of school
  shootings</title><categories>physics.soc-ph cs.SI</categories><comments>Main text: 6 pages, 3 figures; Supplementary: 4 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past two decades, school shootings within the United States have
repeatedly devastated communities and shaken public opinion. Many of these
attacks appear to be `lone wolf' ones driven by specific individual
motivations, and the identification of precursor signals and hence actionable
policy measures would thus seem highly unlikely. Here, we take a system wide
view and investigate the timing of school attacks and the dynamical feedback
with social media. We identify a trend divergence in which college attacks have
continued to accelerate over the last 25 years while those carried out on K-12
schools have slowed down. We establish the copycat effect in school shootings
and uncover a statistical association between social media chatter and the
probability of an attack in the following days. While hinting at causality,
this relationship may also help mitigate the frequency and intensity of future
attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06312</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06312</id><created>2015-06-20</created><authors><author><keyname>Huang</keyname><forenames>Junfei</forenames></author><author><keyname>Shou</keyname><forenames>Guochu</forenames></author></authors><title>A QoS Guarantee Strategy for Multimedia Conferencing based on Bayesian
  Networks</title><categories>cs.MM cs.NI</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service Oriented Architecture (SOA) is commonly employed in the design and
implementation of web service systems. The key technology to enable media
communications in the context of SOA is the Service Oriented Communication. To
exploit the advantage of SOA, we design and implement a web-based multimedia
conferencing system that provides users with a hybrid orchestration of web and
communication services. As the current SOA lacks effective QoS guarantee
solutions for multimedia services, the user satisfaction is greatly challenged
with QoS violations, e.g., low video PSNR (Peak Signal-to-Noise Ratio) and long
playback delay. Motivated by addressing the critical problem, we firstly employ
the Business Process Execution Language (BPEL) service engine for the hybrid
services orchestration and execution. Secondly, we propose a novel
context-aware approach to quantify and leverage the causal relationships
between QoS metrics and available contexts based on Bayesian networks (CABIN).
This approach includes three phases: (1) information discretization, (2) causal
relationship profiling, and (3) optimal context tuning. We implement CABIN in a
real-life multimedia conferencing system and compare its performance with
existing delay and throughput oriented schemes. Experimental results show that
CABIN outperforms the competing approaches in improving the video quality in
terms of PSNR. It also provides a one-stop shop controls both the web and
communication services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06318</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06318</id><created>2015-06-21</created><authors><author><keyname>Chen</keyname><forenames>Shang-Tse</forenames></author><author><keyname>Balcan</keyname><forenames>Maria-Florina</forenames></author><author><keyname>Chau</keyname><forenames>Duen Horng</forenames></author></authors><title>Communication Efficient Distributed Agnostic Boosting</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning from distributed data in the agnostic
setting, i.e., in the presence of arbitrary forms of noise. Our main
contribution is a general distributed boosting-based procedure for learning an
arbitrary concept space, that is simultaneously noise tolerant, communication
efficient, and computationally efficient. This improves significantly over
prior works that were either communication efficient only in noise-free
scenarios or computationally prohibitive. Empirical results on large synthetic
and real-world datasets demonstrate the effectiveness and scalability of the
proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06325</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06325</id><created>2015-06-21</created><updated>2015-06-23</updated><authors><author><keyname>Klein</keyname><forenames>Saleet</forenames></author><author><keyname>Levi</keyname><forenames>Amit</forenames></author><author><keyname>Safra</keyname><forenames>Muli</forenames></author><author><keyname>Shikhelman</keyname><forenames>Clara</forenames></author><author><keyname>Spinka</keyname><forenames>Yinon</forenames></author></authors><title>On the Converse of Talagrand's Influence Inequality</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1994, Talagrand showed a generalization of the celebrated KKL theorem. In
this work, we prove that the converse of this generalization also holds.
Namely, for any sequence of numbers $0&lt;a_1,a_2,\ldots,a_n\le 1$ such that
$\sum_{j=1}^n a_j/(1-\log a_j)\ge C$ for some constant $C&gt;0$, it is possible to
find a roughly balanced Boolean function $f$ such that $\textrm{Inf}_j[f] &lt;
a_j$ for every $1 \le j \le n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06332</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06332</id><created>2015-06-21</created><authors><author><keyname>Semaev</keyname><forenames>Igor</forenames></author></authors><title>Experimental Study of DIGIPASS GO3 and the Security of Authentication</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the analysis of $6$-digit one-time passwords(OTP) generated by
DIGIPASS GO3 we were able to reconstruct the synchronisation system of the
token, the OTP generating algorithm and the verification protocol in details
essential for an attack. The OTPs are more predictable than expected. A forgery
attack is described. We argue the attack success probability is $8^{-5}$. That
is much higher than $10^{-6}$ which may be expected if all the digits are
independent and uniformly distributed. Under natural assumptions even in a
relatively small bank or company with $10^4$ customers the number of
compromised accounts during a year may be more than $100$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06343</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06343</id><created>2015-06-21</created><updated>2015-06-26</updated><authors><author><keyname>Li</keyname><forenames>Yao</forenames></author><author><keyname>Liu</keyname><forenames>Lingqiao</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>Mining Mid-level Visual Patterns with Deep CNN Activations</title><categories>cs.CV</categories><comments>15 pages. fixed some typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of mid-level visual element discovery is to find clusters of
image patches that are both representative and discriminative. Here we study
this problem from the prospective of pattern mining while relying on the
recently popularized Convolutional Neural Networks (CNNs). We observe that a
fully-connected CNN activation extracted from an image patch typically
possesses two appealing properties that enable its seamless integration with
pattern mining techniques. The marriage between CNN activations and association
rule mining, a well-known pattern mining technique in the literature, leads to
fast and effective discovery of representative and discriminative patterns from
a huge number of image patches. When we retrieve and visualize image patches
with the same pattern, surprisingly, they are not only visually similar but
also semantically consistent, and thus give rise to a mid-level visual element
in our work. Given the patterns and retrieved mid-level visual elements, we
propose two methods to generate image feature representations for each. The
first method is to use the patterns as codewords in a dictionary, similar to
the Bag-of-Visual-Words model, we compute a Bag-of-Patterns representation. The
second one relies on the retrieved mid-level visual elements to construct a
Bag-of-Elements representation. We evaluate the two encoding methods on scene
and object classification tasks, and demonstrate that our approach outperforms
or matches recent works using CNN activations for these tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06345</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06345</id><created>2015-06-21</created><authors><author><keyname>Barg</keyname><forenames>Alexander</forenames></author><author><keyname>Mazumdar</keyname><forenames>Arya</forenames></author><author><keyname>Wang</keyname><forenames>Rongrong</forenames></author></authors><title>Restricted isometry property of random subdictionaries</title><categories>cs.IT math.IT</categories><comments>To appear in the IEEE Transactions on Information Theory, 2015. A
  detailed draft which is a predecessor of this paper appears as
  arXiv:1303.1847</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study statistical restricted isometry, a property closely related to
sparse signal recovery, of deterministic sensing matrices of size $m \times N$.
A matrix is said to have a statistical restricted isometry property (StRIP) of
order $k$ if most submatrices with $k$ columns define a near-isometric map of
${\mathbb R}^k$ into ${\mathbb R}^m$. As our main result, we establish
sufficient conditions for the StRIP property of a matrix in terms of the mutual
coherence and mean square coherence. We show that for many existing
deterministic families of sampling matrices, $m=O(k)$ rows suffice for
$k$-StRIP, which is an improvement over the known estimates of either $m =
\Theta(k \log N)$ or $m = \Theta(k\log k)$. We also give examples of matrix
families that are shown to have the StRIP property using our sufficient
conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06346</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06346</id><created>2015-06-21</created><updated>2015-10-04</updated><authors><author><keyname>Boissonnat</keyname><forenames>Jean-Daniel</forenames></author><author><keyname>Dyer</keyname><forenames>Ramsay</forenames></author><author><keyname>Ghosh</keyname><forenames>Arijit</forenames></author></authors><title>An elementary approach to tangent space variation on Riemannian
  submanifolds</title><categories>cs.CG math.DG</categories><comments>8 pages</comments><msc-class>68W05</msc-class><acm-class>I.3.5</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We give asymptotically tight estimates of tangent space variation on
Riemannian submanifolds of Euclidean space with respect to the local feature
size of the submanifolds. We show that the result follows directly from
structural properties of local feature size of the Riemannian submanifold and
some elementary Euclidean geometry. We also show that using the tangent
variation result one can prove a new structural property of local feature size
function. This structural property is a generalization of a result of Giesen
and Wagner [GW04, Lem. 7].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06357</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06357</id><created>2015-06-21</created><authors><author><keyname>Elyengui</keyname><forenames>Saida</forenames></author><author><keyname>Bouhouchi</keyname><forenames>Riadh</forenames></author><author><keyname>Ezzedine</keyname><forenames>Tahar</forenames></author></authors><title>LOADng Routing Protocol Evaluation for Bidirectional Data flow in AMI
  Mesh Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research work denotes a novel evaluation of LOADng the routing protocol
for Wireless Sensor Network. The LOADng protocol implementation is a part of
ITU-T G.9903 recommendation based on the framework of the LLN On-demand Ad hoc
Distance-vector Routing Protocol - Next Generation (LOADng) proposed by IETF
specified by the IETF Internet-Draft draft-clausen-lln-loadng-11 and currently
still in its design phase. LOADng is a reactive on demand distance vector
routing protocol derived from AODV the Ad hoc On-demand distance vector
protocol proposed by IETF. This work was motivated by the need for a novel
protocol implementation for smart metering applications providing better
performance and less complexity than RPL the Routing Protocol for Low power and
lossy networks and adapted to (LLNs) requirements and constraints. Our
implementation was successfully integrated into the communication layer of
Contiki OS the Wireless Sensor Network operating system and evaluated through
extensive simulations for AMI Mesh Networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06366</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06366</id><created>2015-06-21</created><authors><author><keyname>Chen</keyname><forenames>He-Wen</forenames></author><author><keyname>Wang</keyname><forenames>Zih-Ci</forenames></author><author><keyname>Kuo</keyname><forenames>Shu-Yu</forenames></author><author><keyname>Chou</keyname><forenames>Yao-Hsin</forenames></author></authors><title>A Novel Method for Stock Forecasting based on Fuzzy Time Series Combined
  with the Longest Common/Repeated Sub-sequence</title><categories>cs.CE cs.AI cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stock price forecasting is an important issue for investors since extreme
accuracy in forecasting can bring about high profits. Fuzzy Time Series (FTS)
and Longest Common/Repeated Sub-sequence (LCS/LRS) are two important issues for
forecasting prices. However, to the best of our knowledge, there are no
significant studies using LCS/LRS to predict stock prices. It is impossible
that prices stay exactly the same as historic prices. Therefore, this paper
proposes a state-of-the-art method which combines FTS and LCS/LRS to predict
stock prices. This method is based on the principle that history will repeat
itself. It uses different interval lengths in FTS to fuzzify the prices, and
LCS/LRS to look for the same pattern in the historical prices to predict future
stock prices. In the experiment, we examine various intervals of fuzzy time
sets in order to achieve high prediction accuracy. The proposed method
outperforms traditional methods in terms of prediction accuracy and,
furthermore, it is easy to implement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06369</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06369</id><created>2015-06-21</created><authors><author><keyname>Candr&#xe1;kov&#xe1;</keyname><forenames>Barbora</forenames></author><author><keyname>Luko&#x165;ka</keyname><forenames>Robert</forenames></author></authors><title>Cubic TSP - a 1.3-approximation</title><categories>cs.DM math.OC</categories><comments>15 pages</comments><msc-class>90C27, 90B10, 90C10, 90C59</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that every simple bridgeless cubic graph with n &gt;= 8 vertices has a
travelling salesman tour of length at most 1.3n - 2, which can be constructed
in polynomial time. The algorithm provides the best polynomial time
approximation of TSP on simple cubic graphs known at present.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06372</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06372</id><created>2015-06-21</created><authors><author><keyname>Jafari</keyname><forenames>Amir H.</forenames></author><author><keyname>Lopez-Perez</keyname><forenames>David</forenames></author><author><keyname>Ding</keyname><forenames>Ming</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author></authors><title>Study on Scheduling Techniques for Ultra Dense Small Cell Networks</title><categories>cs.IT math.IT</categories><comments>6 pages, 7 figures, Accepted to IEEE VTC-Fall 2015 Boston</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most promising approach to enhance network capacity for the next
generation of wireless cellular networks (5G) is densification, which benefits
from the extensive spatial reuse of the spectrum and the reduced distance
between transmitters and receivers. In this paper, we examine the performance
of different schedulers in ultra dense small cell deployments. Due to the
stronger line of sight (LOS) at low inter-site distances (ISDs), we discuss
that the Rician fading channel model is more suitable to study network
performance than the Rayleigh one, and model the Rician K factor as a function
of distance between the user equipment (UE) and its serving base station (BS).
We also construct a cross-correlation shadowing model that takes into account
the ISD, and finally investigate potential multi-user diversity gains in ultra
dense small cell deployments by comparing the performances of proportional fair
(PF) and round robin (RR) schedulers. Our study shows that as network becomes
denser, the LOS component starts to dominate the path loss model which
significantly increases the interference. Simulation results also show that
multi-user diversity is considerably reduced at low ISDs, and thus the PF
scheduling gain over the RR one is small, around 10% in terms of cell
throughput. As a result, the RR scheduling may be preferred for dense small
cell deployments due to its simplicity. Despite both the interference
aggravation as well as the multi-user diversity loss, network densification is
still worth it from a capacity view point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06378</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06378</id><created>2015-06-21</created><updated>2015-06-24</updated><authors><author><keyname>Smolka</keyname><forenames>Steffen</forenames></author><author><keyname>Eliopoulos</keyname><forenames>Spiridon</forenames></author><author><keyname>Foster</keyname><forenames>Nate</forenames></author><author><keyname>Guha</keyname><forenames>Arjun</forenames></author></authors><title>A Fast Compiler for NetKAT</title><categories>cs.PL</categories><acm-class>D.3.4</acm-class><doi>10.1145/2784731.2784761</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-level programming languages play a key role in a growing number of
networking platforms, streamlining application development and enabling precise
formal reasoning about network behavior. Unfortunately, current compilers only
handle &quot;local&quot; programs that specify behavior in terms of hop-by-hop forwarding
behavior, or modest extensions such as simple paths. To encode richer &quot;global&quot;
behaviors, programmers must add extra state -- something that is tricky to get
right and makes programs harder to write and maintain. Making matters worse,
existing compilers can take tens of minutes to generate the forwarding state
for the network, even on relatively small inputs. This forces programmers to
waste time working around performance issues or even revert to using
hardware-level APIs.
  This paper presents a new compiler for the NetKAT language that handles rich
features including regular paths and virtual networks, and yet is several
orders of magnitude faster than previous compilers. The compiler uses symbolic
automata to calculate the extra state needed to implement &quot;global&quot; programs,
and an intermediate representation based on binary decision diagrams to
dramatically improve performance. We describe the design and implementation of
three essential compiler stages: from virtual programs (which specify behavior
in terms of virtual topologies) to global programs (which specify network-wide
behavior in terms of physical topologies), from global programs to local
programs (which specify behavior in terms of single-switch behavior), and from
local programs to hardware-level forwarding tables. We present results from
experiments on real-world benchmarks that quantify performance in terms of
compilation time and forwarding table size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06390</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06390</id><created>2015-06-21</created><authors><author><keyname>Ahmad</keyname><forenames>Syed Amaar</forenames></author></authors><title>A Waterfilling Algorithm for Multiple Access Point Connectivity with
  Constrained Backhaul Network</title><categories>cs.IT math.IT</categories><comments>Accepted for IEEE Wireless Communication Letters June, 2015.
  Keywords: Optimization, Resource Allocation, HetNets, Feedback control, Small
  Cells</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 4G/5G paradigm offers User Equipment (UE) simultaneous connectivity to a
plurality of wireless Access Points (APs). We consider a UE communicating with
its destination through multiple uplinks operating on orthogonal wireless
channels with unequal bandwidth. The APs are connected via a tree-like backhaul
network with destination at the root and non-ideal link capacities. We develop
a power allocation scheme that achieves a near-optimal rate without explicit
knowledge of the backhaul network topology at transmitter side. The proposed
algorithm waterfills a dynamic subset of uplinks using a low-overhead backhaul
load feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06394</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06394</id><created>2015-06-21</created><authors><author><keyname>Adam</keyname><forenames>Elie M.</forenames></author><author><keyname>Dahleh</keyname><forenames>Munther A.</forenames></author><author><keyname>Ozdaglar</keyname><forenames>Asuman</forenames></author></authors><title>Towards an Algebra for Cascade Effects</title><categories>cs.DM cs.SI math.CO</categories><comments>All comments are welcome. 11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new class of (dynamical) systems that inherently capture
cascading effects (viewed as consequential effects) and are naturally amenable
to combinations. We develop an axiomatic general theory around those systems,
and guide the endeavor towards an understanding of cascading failure. The
theory evolves as an interplay of lattices and fixed points, and its results
may be instantiated to commonly studied models of cascade effects.
  We characterize the systems through their fixed points, and equip them with
two operators. We uncover properties of the operators, and express global
systems through combinations of local systems. We enhance the theory with a
notion of failure, and understand the class of shocks inducing a system to
failure. We develop a notion of mu-rank to capture the energy of a system, and
understand the minimal amount of effort required to fail a system, termed
resilience. We deduce a dual notion of fragility and show that the combination
of systems sets a limit on the amount of fragility inherited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06395</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06395</id><created>2015-06-21</created><updated>2016-02-03</updated><authors><author><keyname>Danila</keyname><forenames>Bogdan</forenames></author></authors><title>Comprehensive spectral approach for community structure analysis on
  complex networks</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><comments>11 pages, 10 figures, final version</comments><journal-ref>Phys. Rev. E 93, 022301 (2016)</journal-ref><doi>10.1103/PhysRevE.93.022301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple but efficient spectral approach for analyzing the community
structure of complex networks is introduced. It works the same way for all
types of networks, by spectrally splitting the adjacency matrix into a
&quot;unipartite&quot; and a &quot;multipartite&quot; component. These two matrices reveal the
structure of the network from different perspectives and can be analyzed at
different levels of detail. Their entries, or the entries of their lower-rank
approximations, provide measures of the affinity or antagonism between the
nodes that highlight the communities and the &quot;gateway&quot; links that connect them
together. An algorithm is then proposed to achieve the automatic assignment of
the nodes to communities based on the information provided by either matrix.
This algorithm naturally generates overlapping communities but can also be
tuned to eliminate the overlaps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06399</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06399</id><created>2015-06-21</created><updated>2015-06-24</updated><authors><author><keyname>Mukhopadhyay</keyname><forenames>Sagnik</forenames></author><author><keyname>Sanyal</keyname><forenames>Swagato</forenames></author></authors><title>Towards Better Separation between Deterministic and Randomized Query
  Complexity</title><categories>cs.CC</categories><comments>Reference added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there exists a Boolean function $F$ which observes the following
separations among deterministic query complexity $(D(F))$, randomized zero
error query complexity $(R_0(F))$ and randomized one-sided error query
complexity $(R_1(F))$: $R_1(F) = \widetilde{O}(\sqrt{D(F)})$ and
$R_0(F)=\widetilde{O}(D(F))^{3/4}$. This refutes the conjecture made by Saks
and Wigderson that for any Boolean function $f$,
$R_0(f)=\Omega({D(f)})^{0.753..}$. This also shows widest separation between
$R_1(f)$ and $D(f)$ for any Boolean function. The function $F$ was defined by
G{\&quot;{o}}{\&quot;{o}}s, Pitassi and Watson who studied it for showing a separation
between deterministic decision tree complexity and unambiguous
non-deterministic decision tree complexity. Independently of us, Ambainis et al
proved that different variants of the function $F$ certify optimal (quadratic)
separation between $D(f)$ and $R_0(f)$, and polynomial separation between
$R_0(f)$ and $R_1(f)$. Viewed as separation results, our results are subsumed
by those of Ambainis et al. However, while the functions considerd in the work
of Ambainis et al are different variants of $F$, we work with the original
function $F$ itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06404</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06404</id><created>2015-06-21</created><authors><author><keyname>Boes</keyname><forenames>Olivier</forenames></author><author><keyname>Fischer</keyname><forenames>Mareike</forenames></author><author><keyname>Kelk</keyname><forenames>Steven</forenames></author></authors><title>A linear bound on the number of states in optimal convex characters for
  maximum parsimony distance</title><categories>q-bio.PE cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two phylogenetic trees on the same set of taxa X, the maximum parsimony
distance d_MP is defined as the maximum, ranging over all characters c on X, of
the absolute difference in parsimony score induced by c on the two trees. In
this note we prove that for binary trees there exists a character achieving
this maximum that is convex on one of the trees (i.e. the parsimony score
induced on that tree is equal to the number of states in the character minus 1)
and such that the number of states in the character is at most 7d_MP - 5. This
is the first non-trivial bound on the number of states required by optimal
characters, convex or otherwise. The result potentially has algorithmic
significance because, unlike general characters, convex characters with a
bounded number of states can be enumerated in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06418</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06418</id><created>2015-06-21</created><authors><author><keyname>Hoffmann</keyname><forenames>Raphael</forenames></author><author><keyname>Zettlemoyer</keyname><forenames>Luke</forenames></author><author><keyname>Weld</keyname><forenames>Daniel S.</forenames></author></authors><title>Extreme Extraction: Only One Hour per Relation</title><categories>cs.CL cs.AI cs.IR</categories><acm-class>H.2.8, H.3.1, I.2.7, I.5.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information Extraction (IE) aims to automatically generate a large knowledge
base from natural language text, but progress remains slow. Supervised learning
requires copious human annotation, while unsupervised and weakly supervised
approaches do not deliver competitive accuracy. As a result, most fielded
applications of IE, as well as the leading TAC-KBP systems, rely on significant
amounts of manual engineering. Even &quot;Extreme&quot; methods, such as those reported
in Freedman et al. 2011, require about 10 hours of expert labor per relation.
  This paper shows how to reduce that effort by an order of magnitude. We
present a novel system, InstaRead, that streamlines authoring with an ensemble
of methods: 1) encoding extraction rules in an expressive and compositional
representation, 2) guiding the user to promising rules based on corpus
statistics and mined resources, and 3) introducing a new interactive
development cycle that provides immediate feedback --- even on large datasets.
Experiments show that experts can create quality extractors in under an hour
and even NLP novices can author good extractors. These extractors equal or
outperform ones obtained by comparably supervised and state-of-the-art
distantly supervised approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06419</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06419</id><created>2015-06-21</created><updated>2015-06-22</updated><authors><author><keyname>Norman</keyname><forenames>Gethin</forenames></author><author><keyname>Parker</keyname><forenames>David</forenames></author><author><keyname>Zou</keyname><forenames>Xueyi</forenames></author></authors><title>Verification and Control of Partially Observable Probabilistic Real-Time
  Systems</title><categories>cs.LO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose automated techniques for the verification and control of
probabilistic real-time systems that are only partially observable. To formally
model such systems, we define an extension of probabilistic timed automata in
which local states are partially visible to an observer or controller. We give
a probabilistic temporal logic that can express a range of quantitative
properties of these models, relating to the probability of an event's
occurrence or the expected value of a reward measure. We then propose
techniques to either verify that such a property holds or to synthesise a
controller for the model which makes it true. Our approach is based on an
integer discretisation of the model's dense-time behaviour and a grid-based
abstraction of the uncountable belief space induced by partial observability.
The latter is necessarily approximate since the underlying problem is
undecidable, however we show how both lower and upper bounds on numerical
results can be generated. We illustrate the effectiveness of the approach by
implementing it in the PRISM model checker and applying it to several case
studies, from the domains of computer security and task scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06426</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06426</id><created>2015-06-21</created><authors><author><keyname>Staecker</keyname><forenames>P. Christopher</forenames></author></authors><title>A Borsuk-Ulam theorem for digital images</title><categories>math.GN cs.GR</categories><comments>14 pages</comments><msc-class>52C07, 68R01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Borsuk-Ulam theorem states that a continuous function $f:S^n \to \R^n$
has a point $x\in S^n$ with $f(x)=f(-x)$. We give an analogue of this theorem
for digital images, which are modeled as discrete spaces of adjacent pixels
equipped with $\Z^n$-valued functions.
  In particular, for a concrete two-dimensional rectangular digital image whose
pixels all have an assigned &quot;brightness&quot; function, we prove that there must
exist a pair of opposite boundary points whose brightnesses are approximately
equal. This theorem applies generally to any integer-valued function on an
abstract simple graph.
  We also discuss generalizations to digital images of dimension 3 and higher.
We give some partial results for higher dimensional images, and show a counter
example which demonstrates that the full results obtained in lower dimensions
cannot hold generally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06438</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06438</id><created>2015-06-21</created><updated>2015-10-02</updated><authors><author><keyname>De Sa</keyname><forenames>Christopher</forenames></author><author><keyname>Zhang</keyname><forenames>Ce</forenames></author><author><keyname>Olukotun</keyname><forenames>Kunle</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author></authors><title>Taming the Wild: A Unified Analysis of Hogwild!-Style Algorithms</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of
machine learning problems. Researchers and industry have developed several
techniques to optimize SGD's runtime performance, including asynchronous
execution and reduced precision. Our main result is a martingale-based analysis
that enables us to capture the rich noise models that may arise from such
techniques. Specifically, we use our new analysis in three ways: (1) we derive
convergence rates for the convex case (Hogwild!) with relaxed assumptions on
the sparsity of the problem; (2) we analyze asynchronous SGD algorithms for
non-convex matrix problems including matrix completion; and (3) we design and
analyze an asynchronous SGD algorithm, called Buckwild!, that uses
lower-precision arithmetic. We show experimentally that our algorithms run
efficiently for a variety of problems on modern hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06440</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06440</id><created>2015-06-21</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>The Jordan-Brouwer theorem for graphs</title><categories>cs.DM cs.CG math.GT</categories><comments>26 pages, 1 figure</comments><msc-class>05C15, 57M15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a discrete Jordan-Brouwer-Schoenflies separation theorem telling
that a (d-1)-sphere H embedded in a d-sphere G defines two different connected
graphs A,B in G such a way that the intersection of A and B is H and the union
is G and such that the complementary graphs A,B are both d-balls. The graph
theoretic definitions are due to Evako: the unit sphere of a vertex x of a
graph G=(V,E) is the graph generated by {y | (x,y) in E} Inductively, a finite
simple graph is called contractible if there is a vertex x such that both its
unit sphere S(x) as well as the graph generated by V-{x} are contractible.
Inductively, still following Evako, a d-sphere is a finite simple graph for
which every unit sphere is a (d-1)-sphere and such that removing a single
vertex renders the graph contractible. A d-ball B is a contractible graph for
which each unit sphere S(x) is either a (d-1)-sphere in which case x is called
an interior point, or S(x) is a (d-1)-ball in which case x is called a boundary
point and such that the set of boundary point vertices generates a
(d-1)-sphere. These inductive definitions are based on the assumption that the
empty graph is the unique (-1)-sphere and that the one-point graph K_1 is the
unique 0-ball and that K_1 is contractible. The theorem needs the following
notion of embedding: a sphere H is embedded in a graph G if it is a sub-graph
of G and if any intersection with any finite set of mutually adjacent unit
spheres is a sphere. A knot of co-dimension k in G is a (d-k)-sphere H embedded
in a d-sphere G.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06442</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06442</id><created>2015-06-21</created><updated>2016-01-07</updated><authors><author><keyname>Meng</keyname><forenames>Fandong</forenames></author><author><keyname>Lu</keyname><forenames>Zhengdong</forenames></author><author><keyname>Tu</keyname><forenames>Zhaopeng</forenames></author><author><keyname>Li</keyname><forenames>Hang</forenames></author><author><keyname>Liu</keyname><forenames>Qun</forenames></author></authors><title>A Deep Memory-based Architecture for Sequence-to-Sequence Learning</title><categories>cs.CL cs.LG cs.NE</categories><comments>13 pages, Under review as a conference paper at ICLR 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose DEEPMEMORY, a novel deep architecture for sequence-to-sequence
learning, which performs the task through a series of nonlinear transformations
from the representation of the input sequence (e.g., a Chinese sentence) to the
final output sequence (e.g., translation to English). Inspired by the recently
proposed Neural Turing Machine (Graves et al., 2014), we store the intermediate
representations in stacked layers of memories, and use read-write operations on
the memories to realize the nonlinear transformations between the
representations. The types of transformations are designed in advance but the
parameters are learned from data. Through layer-by-layer transformations,
DEEPMEMORY can model complicated relations between sequences necessary for
applications such as machine translation between distant languages. The
architecture can be trained with normal back-propagation on sequenceto-sequence
data, and the learning can be easily scaled up to a large corpus. DEEPMEMORY is
broad enough to subsume the state-of-the-art neural translation model in
(Bahdanau et al., 2015) as its special case, while significantly improving upon
the model with its deeper architecture. Remarkably, DEEPMEMORY, being purely
neural network-based, can achieve performance comparable to the traditional
phrase-based machine translation system Moses with a small vocabulary and a
modest parameter size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06444</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06444</id><created>2015-06-21</created><authors><author><keyname>Bhattiprolu</keyname><forenames>Vijay V. S. P.</forenames></author><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Lee</keyname><forenames>Euiwoong</forenames></author></authors><title>Approximate Hypergraph Coloring under Low-discrepancy and Related
  Promises</title><categories>cs.DS cs.CC</categories><comments>Approx 2015</comments><acm-class>G.2.1; G.2.2; F.2.2; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A hypergraph is said to be $\chi$-colorable if its vertices can be colored
with $\chi$ colors so that no hyperedge is monochromatic. $2$-colorability is a
fundamental property (called Property B) of hypergraphs and is extensively
studied in combinatorics. Algorithmically, however, given a $2$-colorable
$k$-uniform hypergraph, it is NP-hard to find a $2$-coloring miscoloring fewer
than a fraction $2^{-k+1}$ of hyperedges (which is achieved by a random
$2$-coloring), and the best algorithms to color the hypergraph properly require
$\approx n^{1-1/k}$ colors, approaching the trivial bound of $n$ as $k$
increases.
  In this work, we study the complexity of approximate hypergraph coloring, for
both the maximization (finding a $2$-coloring with fewest miscolored edges) and
minimization (finding a proper coloring using fewest number of colors)
versions, when the input hypergraph is promised to have the following stronger
properties than $2$-colorability:
  (A) Low-discrepancy: If the hypergraph has discrepancy $\ell \ll \sqrt{k}$,
we give an algorithm to color the it with $\approx n^{O(\ell^2/k)}$ colors.
However, for the maximization version, we prove NP-hardness of finding a
$2$-coloring miscoloring a smaller than $2^{-O(k)}$ (resp. $k^{-O(k)}$)
fraction of the hyperedges when $\ell = O(\log k)$ (resp. $\ell=2$). Assuming
the UGC, we improve the latter hardness factor to $2^{-O(k)}$ for almost
discrepancy-$1$ hypergraphs.
  (B) Rainbow colorability: If the hypergraph has a $(k-\ell)$-coloring such
that each hyperedge is polychromatic with all these colors, we give a
$2$-coloring algorithm that miscolors at most $k^{-\Omega(k)}$ of the
hyperedges when $\ell \ll \sqrt{k}$, and complement this with a matching UG
hardness result showing that when $\ell =\sqrt{k}$, it is hard to even beat the
$2^{-k+1}$ bound achieved by a random coloring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06448</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06448</id><created>2015-06-21</created><authors><author><keyname>Roth</keyname><forenames>Holger R.</forenames></author><author><keyname>Lu</keyname><forenames>Le</forenames></author><author><keyname>Farag</keyname><forenames>Amal</forenames></author><author><keyname>Shin</keyname><forenames>Hoo-Chang</forenames></author><author><keyname>Liu</keyname><forenames>Jiamin</forenames></author><author><keyname>Turkbey</keyname><forenames>Evrim</forenames></author><author><keyname>Summers</keyname><forenames>Ronald M.</forenames></author></authors><title>DeepOrgan: Multi-level Deep Convolutional Networks for Automated
  Pancreas Segmentation</title><categories>cs.CV</categories><comments>To be presented at MICCAI 2015 - 18th International Conference on
  Medical Computing and Computer Assisted Interventions, Munich, Germany</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Automatic organ segmentation is an important yet challenging problem for
medical image analysis. The pancreas is an abdominal organ with very high
anatomical variability. This inhibits previous segmentation methods from
achieving high accuracies, especially compared to other organs such as the
liver, heart or kidneys. In this paper, we present a probabilistic bottom-up
approach for pancreas segmentation in abdominal computed tomography (CT) scans,
using multi-level deep convolutional networks (ConvNets). We propose and
evaluate several variations of deep ConvNets in the context of hierarchical,
coarse-to-fine classification on image patches and regions, i.e. superpixels.
We first present a dense labeling of local image patches via
$P{-}\mathrm{ConvNet}$ and nearest neighbor fusion. Then we describe a regional
ConvNet ($R_1{-}\mathrm{ConvNet}$) that samples a set of bounding boxes around
each image superpixel at different scales of contexts in a &quot;zoom-out&quot; fashion.
Our ConvNets learn to assign class probabilities for each superpixel region of
being pancreas. Last, we study a stacked $R_2{-}\mathrm{ConvNet}$ leveraging
the joint space of CT intensities and the $P{-}\mathrm{ConvNet}$ dense
probability maps. Both 3D Gaussian smoothing and 2D conditional random fields
are exploited as structured predictions for post-processing. We evaluate on CT
images of 82 patients in 4-fold cross-validation. We achieve a Dice Similarity
Coefficient of 83.6$\pm$6.3% in training and 71.8$\pm$10.7% in testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06456</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06456</id><created>2015-06-21</created><authors><author><keyname>Szegedy</keyname><forenames>Mario</forenames></author></authors><title>An $O(n^{0.4732})$ upper bound on the complexity of the GKS
  communication game</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an $5\cdot n^{\log_{30}5}$ upper bund on the complexity of the
communication game introduced by G. Gilmer, M. Kouck\'y and M. Saks \cite{saks}
to study the Sensitivity Conjecture \cite{linial}, improving on their
$\sqrt{999\over 1000}\sqrt{n}$ bound. We also determine the exact complexity of
the game up to $n\le 9$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06472</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06472</id><created>2015-06-22</created><authors><author><keyname>Baldi</keyname><forenames>Pierre</forenames></author><author><keyname>Sadowski</keyname><forenames>Peter</forenames></author></authors><title>The Ebb and Flow of Deep Learning: a Theory of Local Learning</title><categories>cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a physical neural system, where storage and processing are intimately
intertwined, the rules for adjusting the synaptic weights can only depend on
variables that are available locally, such as the activity of the pre- and
post-synaptic neurons, resulting in local learning rules. A systematic
framework for studying the space of local learning rules must first define the
nature of the local variables, and then the functional form that ties them
together into each learning rule. We consider polynomial local learning rules
and analyze their behavior and capabilities in both linear and non-linear
networks. As a byproduct, this framework enables also the discovery of new
learning rules as well as important relationships between learning rules and
group symmetries. Stacking local learning rules in deep feedforward networks
leads to deep local learning. While deep local learning can learn interesting
representations, it cannot learn complex input-output functions, even when
targets are available for the top layer. Learning complex input-output
functions requires local deep learning where target information is propagated
to the deep layers through a backward channel. The nature of the propagated
information about the targets, and the backward channel through which this
information is propagated, partition the space of learning algorithms. For any
learning algorithm, the capacity of the backward channel can be defined as the
number of bits provided about the gradient per weight, divided by the number of
required operations per weight. We estimate the capacity associated with
several learning algorithms and show that backpropagation outperforms them and
achieves the maximum possible capacity. The theory clarifies the concept of
Hebbian learning, what is learnable by Hebbian learning, and explains the
sparsity of the space of learning rules discovered so far.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06476</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06476</id><created>2015-06-22</created><authors><author><keyname>Teh</keyname><forenames>Wen Chean</forenames></author></authors><title>Parikh matrices and Parikh Rewriting Systems</title><categories>cs.FL math.CO</categories><comments>15 pages, preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the introduction of the Parikh matrix mapping, its injectivity problem
is on top of the list of open problems in this topic. In 2010 Salomaa provided
a solution for the ternary alphabet in terms of a Thue system with an
additional feature called counter. This paper proposes the notion of a Parikh
rewriting system as a generalization and systematization of Salomaa's result.
It will be shown that every Parikh rewriting system induces a Thue system
without counters that serves as a feasible solution to the injectivity problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06479</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06479</id><created>2015-06-22</created><updated>2016-01-27</updated><authors><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Cai</keyname><forenames>Ning</forenames></author><author><keyname>N&#xf6;tzel</keyname><forenames>Janis</forenames></author></authors><title>The Classical-Quantum Channel with Random State Parameters Known to the
  Sender</title><categories>quant-ph cs.IT math.IT</categories><comments>26 pages, no figures. Many typos corrected. More details added to the
  proofs. Added proof for the direct part of capacity theorem for causal
  encoding</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an analog of the well-known Gel'fand Pinsker Channel which uses
quantum states for the transmission of the data. We consider the case where
both the sender's inputs to the channel and the channel states are to be taken
from a finite set (cq-channel with state information at the sender). We
distinguish between causal and non-causal channel state information at the
sender. The receiver remains ignorant, throughout. We give a single-letter
description of the capacity in the first case. In the second case we present
two different regularized expressions for the capacity. It is an astonishing
and unexpected result of our work that a simple change from causal to
non-causal channel state information at the encoder causes the complexity of a
numerical computation of the capacity formula to change from trivial to
seemingly difficult. Still, even the non-single letter formula allows one to
draw nontrivial conclusions, for example regarding continuity of the capacity
with respect to changes in the system parameters. The direct parts of both
coding theorems are based on a special class of POVMs which are derived from
orthogonal projections onto certain representations of the symmetric groups.
This approach supports a reasoning that is inspired by the classical method of
types. In combination with the non-commutative union bound these POVMs yield an
elegant method of proof for the direct part of the coding theorem in the first
case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06484</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06484</id><created>2015-06-22</created><authors><author><keyname>Michelusi</keyname><forenames>Nicolo</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>Cross-layer estimation and control for Cognitive Radio: Exploiting
  Sparse Network Dynamics</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to IEEE Transactions on Cognitive Communications and
  Networking (invited)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a cross-layer framework to jointly optimize spectrum sensing
and scheduling in resource constrained agile wireless networks is presented. A
network of secondary users (SUs) accesses portions of the spectrum left unused
by a network of licensed primary users (PUs). A central controller (CC)
schedules the traffic of the SUs, based on distributed compressed measurements
collected by the SUs. Sensing and scheduling are jointly controlled to maximize
the SU throughput, with constraints on PU throughput degradation and SU cost.
The sparsity in the spectrum dynamics is exploited: leveraging a prior spectrum
occupancy estimate, the CC needs to estimate only a residual uncertainty vector
via sparse recovery techniques. The high complexity entailed by the POMDP
formulation is reduced by a low-dimensional belief representation via
minimization of the Kullback-Leibler divergence. It is proved that the
optimization of sensing and scheduling can be decoupled. A partially myopic
scheduling strategy is proposed for which structural properties can be proved
showing that the myopic scheme allocates SU traffic to likely idle spectral
bands. Simulation results show that this framework balances optimally the
resources between spectrum sensing and data transmission. This framework
defines sensing-scheduling schemes most informative for network control,
yielding energy efficient resource utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06488</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06488</id><created>2015-06-22</created><updated>2015-11-06</updated><authors><author><keyname>Klav&#xed;k</keyname><forenames>Pavel</forenames></author><author><keyname>Nedela</keyname><forenames>Roman</forenames></author><author><keyname>Zeman</keyname><forenames>Peter</forenames></author></authors><title>Constructive Approach to Automorphism Groups of Planar Graphs</title><categories>math.CO cs.DM math.GR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1503.06556</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By Frucht's Theorem, every abstract finite group is isomorphic to the
automorphism group of some graph. In 1975, Babai characterized which of these
abstract groups can be realized as the automorphism groups of planar graphs. In
this paper, we give a more detailed and understandable description of these
groups. We describe stabilizers of vertices in connected planar graphs as the
class of groups closed under the direct product and semidirect products with
symmetric, dihedral and cyclic groups. The automorphism group of a connected
planar graph is obtained as semidirect product of a direct product of these
stabilizers with a spherical group. Our approach is based on the decomposition
to 3-connected components and gives a quadratic-time algorithm for computing
the automorphism group of a planar graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06490</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06490</id><created>2015-06-22</created><authors><author><keyname>Zhou</keyname><forenames>Xiaoqiang</forenames></author><author><keyname>Hu</keyname><forenames>Baotian</forenames></author><author><keyname>Chen</keyname><forenames>Qingcai</forenames></author><author><keyname>Tang</keyname><forenames>Buzhou</forenames></author><author><keyname>Wang</keyname><forenames>Xiaolong</forenames></author></authors><title>Answer Sequence Learning with Neural Networks for Answer Selection in
  Community Question Answering</title><categories>cs.CL cs.IR cs.LG</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the answer selection problem in community question answering
(CQA) is regarded as an answer sequence labeling task, and a novel approach is
proposed based on the recurrent architecture for this problem. Our approach
applies convolution neural networks (CNNs) to learning the joint representation
of question-answer pair firstly, and then uses the joint representation as
input of the long short-term memory (LSTM) to learn the answer sequence of a
question for labeling the matching quality of each answer. Experiments
conducted on the SemEval 2015 CQA dataset shows the effectiveness of our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06492</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06492</id><created>2015-06-22</created><authors><author><keyname>Jeandel</keyname><forenames>Emmanuel</forenames><affiliation>CARTE</affiliation></author><author><keyname>Rao</keyname><forenames>Michael</forenames><affiliation>LIP</affiliation></author></authors><title>An aperiodic set of 11 Wang tiles</title><categories>cs.DM cs.FL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new aperiodic tile set containing 11 Wang tiles on 4 colors is presented.
This tile set is minimal in the sense that no Wang set with less than 11 tiles
is aperiodic, and no Wang set with less than 4 colors is aperiodic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06497</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06497</id><created>2015-06-22</created><authors><author><keyname>Lhote</keyname><forenames>Nathan</forenames></author></authors><title>Towards an algebraic characterization of rational word functions</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In formal language theory, several different models characterize regular
languages, such as finite automata, congruences of finite index, or monadic
second-order logic (MSO). Moreover, several fragments of MSO have effective
characterizations based on algebraic properties. When we consider transducers
instead of automata, such characterizations are much more challenging, because
many of the properties of regular languages do not generalize to regular word
functions.
  In this paper we consider word functions that are definable by one-way
transducers (rational functions). We show that the canonical bimachine of
Reutenauer and Sch\&quot;utzenberger preserves certain algebraic properties of
rational functions, similar to the case of word languages. In particular, we
give an effective characterization of functions that can be defined by an
aperiodic one-way transducer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06501</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06501</id><created>2015-06-22</created><authors><author><keyname>Lombardi</keyname><forenames>Damiano</forenames></author><author><keyname>Pant</keyname><forenames>Sanjay</forenames></author></authors><title>A non-parametric k-nearest neighbour entropy estimator</title><categories>cs.IT math.IT math.NA</categories><journal-ref>Phys. Rev. E 93, 013310 (2016)</journal-ref><doi>10.1103/PhysRevE.93.013310</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A non-parametric k-nearest neighbour based entropy estimator is proposed. It
improves on the classical Kozachenko-Leonenko estimator by considering
non-uniform probability densities in the region of k-nearest neighbours around
each sample point. It aims at improving the classical estimators in three
situations: first, when the dimensionality of the random variable is large;
second, when near-functional relationships leading to high correlation between
components of the random variable are present; and third, when the marginal
variances of random variable components vary significantly with respect to each
other. Heuristics on the error of the proposed and classical estimators are
presented. Finally, the proposed estimator is tested for a variety of
distributions in successively increasing dimensions and in the presence of a
near-functional relationship. Its performance is compared with a classical
estimator and shown to be a significant improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06516</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06516</id><created>2015-06-22</created><authors><author><keyname>Cui</keyname><forenames>Wei</forenames></author><author><keyname>Pu</keyname><forenames>Cunlai</forenames></author><author><keyname>Xu</keyname><forenames>Zhongqi</forenames></author></authors><title>Bounded link prediction for very large networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluation of link prediction methods is a hard task in very large complex
networks because of the inhibitive computational cost. By setting a lower bound
of the number of common neighbors (CN), we propose a new framework to
efficiently and precisely evaluate the performances of CN-based similarity
indices in link prediction for very large heterogeneous networks. Specifically,
we propose a fast algorithm based on the parallel computing scheme to obtain
all the node pairs with CN values larger than the lower bound. Furthermore, we
propose a new measurement, called self-predictability, to quantify the
performance of the CN-based similarity indices in link prediction, which on the
other side can indicate the link predictability of a network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06534</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06534</id><created>2015-06-22</created><updated>2015-10-14</updated><authors><author><keyname>Balkir</keyname><forenames>Esma</forenames></author><author><keyname>Sadrzadeh</keyname><forenames>Mehrnoosh</forenames></author><author><keyname>Coecke</keyname><forenames>Bob</forenames></author></authors><title>Distributional Sentence Entailment Using Density Matrices</title><categories>cs.CL cs.IT cs.LO math.CT math.IT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Categorical compositional distributional model of Coecke et al. (2010)
suggests a way to combine grammatical composition of the formal, type logical
models with the corpus based, empirical word representations of distributional
semantics. This paper contributes to the project by expanding the model to also
capture entailment relations. This is achieved by extending the representations
of words from points in meaning space to density operators, which are
probability distributions on the subspaces of the space. A symmetric measure of
similarity and an asymmetric measure of entailment is defined, where lexical
entailment is measured using von Neumann entropy, the quantum variant of
Kullback-Leibler divergence. Lexical entailment, combined with the composition
map on word representations, provides a method to obtain entailment relations
on the level of sentences. Truth theoretic and corpus-based examples are
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06537</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06537</id><created>2015-06-22</created><authors><author><keyname>Abbes</keyname><forenames>Samy</forenames></author></authors><title>Synchronization of Bernoulli sequences on shared letters</title><categories>cs.FL cs.LO</categories><comments>28 pages, 15 references, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synchronizing sequences of letters sharing letters are known to produce a
trace in the concurrency theoretic sense, i.e., a labeled partially ordered
set. We study probabilistic aspects by considering the synchronization of
Bernoulli sequences of letters, under the light of Bernoulli measures recently
introduced for trace monoids.
  We introduce two algorithms that produce random traces by synchronizing
random sequences in a distributed way. The traces are distributed according to
a Bernoulli scheme. The Basic Synchronization Algorithm (BSA) usually produces
finite traces, except in some particular cases where it produces infinite
traces. The Full Synchronization Algorithm (FSA), which builds on the BSA,
always produces infinite traces.
  We thoroughly study some specific examples: the dimer model, either with a
line shape or with a ring shape. The algorithms BSA and FSA are shown to be
very powerful for these examples, since they can be used to simulate any
Bernoulli scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06540</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06540</id><created>2015-06-22</created><authors><author><keyname>Takhanov</keyname><forenames>Rustem</forenames></author></authors><title>Hybrid (V)CSPs and algebraic reductions</title><categories>cs.CC</categories><comments>22 pages. arXiv admin note: text overlap with arXiv:1504.07067</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constraint Satisfaction Problem (CSP) can be stated as computing a
homomorphism $\mbox{$\bR \rightarrow \bGamma$}$ between two relational
structures, e.g.\ between two directed graphs.
  Recently, the {\em hybrid} setting, where both sides are restricted
simultaneously, attracted some attention. It assumes that the right side
structure $\bGamma$ is fixed and $\bR$ belongs to a class of relational
structures $\mathcal{H}$ (called a {\em structural restriction}) that is,
additionally, {\em closed under inverse homomorphisms}. The key tool that
connects hybrid CSPs with fixed-template CSPs is a construction called a
&quot;lifted language,&quot; namely a multi-sorted language $\bGamma_{\bR}$ that can be
constructed from an input $\bR$. The tractability of a language $\bGamma_{\bR}$
for any input $\bR\in\mathcal{H}$ is a necessary condition for tractability of
the hybrid problem.
  First we investigate the case when the last property is not only necessary,
but also is sufficient. It turns out that in the latter case, if
Bulatov-Jeavons-Krokhin characterization of tractable constraint languages is
correct, a structural restriction $\mathcal{H}$ is tractable if and only if it
consists of structures that can be homomorphically mapped to some fixed finite
relational structure $\bGamma'$ (that depends only on $\bGamma$).
  In the second part we generalize the construction of $\bGamma'$ and introduce
a finite structure $\bGamma^{\mathfrak{B}}$, indexed by some set of finite
algebras $\mathfrak{B}$. We prove that under some natural conditions on
$\mathfrak{B}$, $\textsc{CSP}(\bGamma)$ is polynomial-time Turing reducible to
$\textsc{CSP}(\bGamma^{\mathfrak{B}})$ and some polymorphisms of $\bGamma$ have
analogs in $\pol(\bGamma^{\mathfrak{B}})$. This construction introduce a new
set of algorithms for fixed-template CSPs and we suggest it as a tool to
approach Feder-Vardi dichotomy conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06558</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06558</id><created>2015-06-22</created><authors><author><keyname>Liu</keyname><forenames>Tang</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author><author><keyname>Chung</keyname><forenames>Sae-Young</forenames></author></authors><title>On the DoF region of the two-user Interference Channel with an
  Instantaneous Relay</title><categories>cs.IT math.IT</categories><comments>Presented in ISIT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the Degrees of Freedom (DoF) of the two-user multi-antenna
Gaussian interference channel with an {\em instantaneous relay}, or relay
without delay, where the relay transmitted signal in channel use $t$ can depend
on all received signals up to and including that at channel use $t$. It is
assumed that the two transmitters and the two receivers have $M$ antennas,
while the relay receives through $N$ antennas and transmits through $L$
antennas. An achievable DoF region is derived, for all possible values of
$(M,N,L)$, based on a memoryless linear transmission strategy at the relay that
aims to {\it neutralize} as much interference as possible at the receivers. The
proposed scheme is shown to attain the largest sum DoF among all memoryless
linear transmission strategies at the relay and to actually be optimal for
certain values of $(M,N,L)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06564</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06564</id><created>2015-06-22</created><updated>2015-08-23</updated><authors><author><keyname>Dabrowski</keyname><forenames>Konrad K.</forenames></author><author><keyname>Dross</keyname><forenames>Francois</forenames></author><author><keyname>Johnson</keyname><forenames>Matthew</forenames></author><author><keyname>Paulusma</keyname><forenames>Daniel</forenames></author></authors><title>Filling the Complexity Gaps for Colouring Planar and Bounded Degree
  Graphs</title><categories>cs.DS cs.CC cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a natural restriction of the List Colouring problem: $k$-Regular
List Colouring, which corresponds to the List Colouring problem where every
list has size exactly $k$. We give a complete classification of the complexity
of $k$-Regular List Colouring restricted to planar graphs, planar bipartite
graphs, planar triangle-free graphs and to planar graphs with no $4$-cycles and
no $5$-cycles. We also give a complete classification of the complexity of this
problem and a number of related colouring problems for graphs with bounded
maximum degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06573</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06573</id><created>2015-06-22</created><authors><author><keyname>Balsubramani</keyname><forenames>Akshay</forenames></author></authors><title>PAC-Bayes Iterated Logarithm Bounds for Martingale Mixtures</title><categories>cs.LG math.PR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give tight concentration bounds for mixtures of martingales that are
simultaneously uniform over (a) mixture distributions, in a PAC-Bayes sense;
and (b) all finite times. These bounds are proved in terms of the martingale
variance, extending classical Bernstein inequalities, and sharpening and
simplifying prior work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06575</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06575</id><created>2015-06-22</created><authors><author><keyname>Ko</keyname><forenames>Seung-Woo</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Lyun</forenames></author></authors><title>Impact of Node Speed on Throughput of Energy-Constrained Mobile Networks
  with Wireless Power Transfer</title><categories>cs.IT cs.NI math.IT</categories><comments>single column, double spacing, 26 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wireless charging station (WCS) transfers energy wirelessly to mobile nodes
within its charging region. This paper investigates the impact of node speed on
the throughput of WCS overlaid mobile networks when packet transmissions are
constrained by the energy status of each node. The energy provision for each
node depends on its moving speed. A slow-moving node outside the charging
region is unable to receive energy from WCSs for a long time, while one inside
the charging region consistently recharges the battery. Reflecting on these
phenomena, we design a two-dimensional Markov chain, where the states
respectively represent the remaining energy and the distance to the nearest
WCS. Solving this enables the following theoretic insights. Firstly, the
throughput is a non-decreasing function of node speed. With faster node speed,
the throughput converges to that of the independent and identically distributed
(i.i.d.) mobility model where nodes can move anywhere without being restricted
by their previous positions. Secondly, if the battery capacity of each node is
infinite, the throughput is equivalent to that of the i.i.d. mobility model
regardless of node speed. Finally, the throughput scaling is calculated as
Theta(min(1, m/n)c^{\min(1,m/n))), where n and m represent the number of nodes
and WCSs, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06579</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06579</id><created>2015-06-22</created><authors><author><keyname>Yosinski</keyname><forenames>Jason</forenames></author><author><keyname>Clune</keyname><forenames>Jeff</forenames></author><author><keyname>Nguyen</keyname><forenames>Anh</forenames></author><author><keyname>Fuchs</keyname><forenames>Thomas</forenames></author><author><keyname>Lipson</keyname><forenames>Hod</forenames></author></authors><title>Understanding Neural Networks Through Deep Visualization</title><categories>cs.CV cs.LG cs.NE</categories><comments>12 pages. To appear at ICML Deep Learning Workshop 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recent years have produced great advances in training large, deep neural
networks (DNNs), including notable successes in training convolutional neural
networks (convnets) to recognize natural images. However, our understanding of
how these models work, especially what computations they perform at
intermediate layers, has lagged behind. Progress in the field will be further
accelerated by the development of better tools for visualizing and interpreting
neural nets. We introduce two such tools here. The first is a tool that
visualizes the activations produced on each layer of a trained convnet as it
processes an image or video (e.g. a live webcam stream). We have found that
looking at live activations that change in response to user input helps build
valuable intuitions about how convnets work. The second tool enables
visualizing features at each layer of a DNN via regularized optimization in
image space. Because previous versions of this idea produced less recognizable
images, here we introduce several new regularization methods that combine to
produce qualitatively clearer, more interpretable visualizations. Both tools
are open source and work on a pre-trained convnet with minimal setup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06580</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06580</id><created>2015-06-22</created><authors><author><keyname>Goldfarb</keyname><forenames>Doron</forenames></author><author><keyname>Merkl</keyname><forenames>Dieter</forenames></author><author><keyname>Schich</keyname><forenames>Maximilian</forenames></author></authors><title>Quantifying Cultural Histories via Person Networks in Wikipedia</title><categories>cs.SI physics.soc-ph</categories><comments>14 pages, 11 figures, Presented as conference poster at NetSci 2015</comments><acm-class>H.3.4; K.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At least since Priestley's 1765 Chart of Biography, large numbers of
individual person records have been used to illustrate aggregate patterns of
cultural history. Wikidata, the structured database sister of Wikipedia,
currently contains about 2.7 million explicit person records, across all
language versions of the encyclopedia. These individuals, notable according to
Wikipedia editing criteria, are connected via millions of hyperlinks between
their respective Wikipedia articles. This situation provides us with the chance
to go beyond the illustration of an idiosyncratic subset of individuals, as in
the case of Priestly. In this work we summarize the overlap of nationalities
and occupations, based on their co-occurrence in Wikidata individuals. We
construct networks of co-occurring nationalities and occupations, provide
insights into their respective community structure, and apply the results to
select and color chronologically structured subsets of a large network of
individuals, connected by Wikipedia hyperlinks. While the imagined communities
of nationality are much more discrete in terms of co-occurrence than
occupations, our quantifications reveal the existing overlap of nationality as
much less clear-cut than in case of occupational domains. Our work contributes
to a growing body of research using biographies of notable persons to analyze
cultural processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06628</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06628</id><created>2015-06-22</created><updated>2015-06-22</updated><authors><author><keyname>Wei</keyname><forenames>Yunchao</forenames></author><author><keyname>Zhao</keyname><forenames>Yao</forenames></author><author><keyname>Zhu</keyname><forenames>Zhenfeng</forenames></author><author><keyname>Wei</keyname><forenames>Shikui</forenames></author><author><keyname>Xiao</keyname><forenames>Yanhui</forenames></author><author><keyname>Feng</keyname><forenames>Jiashi</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Modality-dependent Cross-media Retrieval</title><categories>cs.CV cs.IR cs.LG</categories><comments>in ACM Transactions on Intelligent Systems and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the cross-media retrieval between images and
text, i.e., using image to search text (I2T) and using text to search images
(T2I). Existing cross-media retrieval methods usually learn one couple of
projections, by which the original features of images and text can be projected
into a common latent space to measure the content similarity. However, using
the same projections for the two different retrieval tasks (I2T and T2I) may
lead to a tradeoff between their respective performances, rather than their
best performances. Different from previous works, we propose a
modality-dependent cross-media retrieval (MDCR) model, where two couples of
projections are learned for different cross-media retrieval tasks instead of
one couple of projections. Specifically, by jointly optimizing the correlation
between images and text and the linear regression from one modal space (image
or text) to the semantic space, two couples of mappings are learned to project
images and text from their original feature spaces into two common latent
subspaces (one for I2T and the other for T2I). Extensive experiments show the
superiority of the proposed MDCR compared with other methods. In particular,
based the 4,096 dimensional convolutional neural network (CNN) visual feature
and 100 dimensional LDA textual feature, the mAP of the proposed method
achieves 41.5\%, which is a new state-of-the-art performance on the Wikipedia
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06633</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06633</id><created>2015-06-22</created><authors><author><keyname>Skorski</keyname><forenames>Maciej</forenames></author></authors><title>A New Approximate Min-Max Theorem with Applications in Cryptography</title><categories>cs.CR cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel proof technique that can be applied to attack a broad
class of problems in computational complexity, when switching the order of
universal and existential quantifiers is helpful. Our approach combines the
standard min-max theorem and convex approximation techniques, offering
quantitative improvements over the standard way of using min-max theorems as
well as more concise and elegant proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06636</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06636</id><created>2015-06-22</created><updated>2015-06-23</updated><authors><author><keyname>Kerautret</keyname><forenames>Bertrand</forenames><affiliation>LORIA</affiliation></author><author><keyname>Kr&#xe4;henb&#xfc;hl</keyname><forenames>Adrien</forenames><affiliation>LORIA</affiliation></author><author><keyname>Debled-Rennesson</keyname><forenames>Isabelle</forenames><affiliation>LORIA</affiliation></author><author><keyname>Lachaud</keyname><forenames>Jacques-Olivier</forenames></author></authors><title>3D Geometric Analysis of Tubular Objects based on Surface Normal
  Accumulation</title><categories>cs.GR</categories><comments>in 18th International Conference on Image Analysis and Processing,
  Sep 2015, Genova, Italy. 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a simple and efficient method for the reconstruction and
extraction of geometric parameters from 3D tubular objects. Our method
constructs an image that accumulates surface normal information, then peaks
within this image are located by tracking. Finally, the positions of these are
optimized to lie precisely on the tubular shape centerline. This method is very
versatile, and is able to process various input data types like full or partial
mesh acquired from 3D laser scans, 3D height map or discrete volumetric images.
The proposed algorithm is simple to implement, contains few parameters and can
be computed in linear time with respect to the number of surface faces. Since
the extracted tube centerline is accurate, we are able to decompose the tube
into rectilinear parts and torus-like parts. This is done with a new linear
time 3D torus detection algorithm, which follows the same principle of a
previous work on 2D arc circle recognition. Detailed experiments show the
versatility, accuracy and robustness of our new method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06642</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06642</id><created>2015-06-22</created><authors><author><keyname>Carofiglio</keyname><forenames>Giovanna</forenames></author><author><keyname>Mekinda</keyname><forenames>Leonce</forenames></author><author><keyname>Muscariello</keyname><forenames>Luca</forenames></author></authors><title>LAC: Introducing Latency-Aware Caching in Information-Centric Networks -
  Technical Report</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latency-minimization is recognized as one of the pillars of 5G network
architecture design. Information-Centric Networking (ICN) appears a promising
candidate technology for building an agile communication model that reduces
latency through in-network caching. However, no proposal has developed so far
latency-aware cache management mechanisms for ICN. In the paper, we investigate
the role of latency awareness on data delivery performance in ICN and introduce
LAC, a new simple, yet very effective, Latency-Aware Cache management policy.
The designed mechanism leverages in a distributed fashion local latency
observations to decide whether to store an object in a network cache. The
farther the object, latency-wise, the more favorable the caching decision. By
means of simulations, show that LAC outperforms state of the art proposals and
results in a reduction of the content mean delivery time and standard deviation
by up to 50%, along with a very fast convergence to these figures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06646</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06646</id><created>2015-06-22</created><authors><author><keyname>Taniguchi</keyname><forenames>Tadahiro</forenames></author><author><keyname>Nakashima</keyname><forenames>Ryo</forenames></author><author><keyname>Nagasaka</keyname><forenames>Shogo</forenames></author></authors><title>Nonparametic Bayesian Double Articulation Analyzer for Direct Language
  Acquisition from Continuous Speech Signals</title><categories>cs.AI cs.CL cs.LG stat.ML</categories><comments>14 pages, 7 figures, Draft submitted to IEEE Transactions on
  Autonomous Mental Development (TAMD)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human infants can discover words directly from unsegmented speech signals
without any explicitly labeled data. In this paper, we develop a novel machine
learning method called nonparametric Bayesian double articulation analyzer
(NPB-DAA) that can directly acquire language and acoustic models from observed
continuous speech signals. For this purpose, we propose an integrative
generative model that combines a language model and an acoustic model into a
single generative model called the &quot;hierarchical Dirichlet process hidden
language model&quot; (HDP-HLM). The HDP-HLM is obtained by extending the
hierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed by
Johnson et al. An inference procedure for the HDP-HLM is derived using the
blocked Gibbs sampler originally proposed for the HDP-HSMM. This procedure
enables the simultaneous and direct inference of language and acoustic models
from continuous speech signals. Based on the HDP-HLM and its inference
procedure, we developed a novel double articulation analyzer. By assuming
HDP-HLM as a generative model of observed time series data, and by inferring
latent variables of the model, the method can analyze latent double
articulation structure, i.e., hierarchically organized latent words and
phonemes, of the data in an unsupervised manner. The novel unsupervised double
articulation analyzer is called NPB-DAA.
  The NPB-DAA can automatically estimate double articulation structure embedded
in speech signals. We also carried out two evaluation experiments using
synthetic data and actual human continuous speech signals representing Japanese
vowel sequences. In the word acquisition and phoneme categorization tasks, the
NPB-DAA outperformed a conventional double articulation analyzer (DAA) and
baseline automatic speech recognition system whose acoustic model was trained
in a supervised manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06648</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06648</id><created>2015-06-22</created><authors><author><keyname>Chawla</keyname><forenames>Chetan</forenames></author><author><keyname>Chana</keyname><forenames>Inderveer</forenames></author></authors><title>Strategy-proof Pricing Approach for Cloud Market</title><categories>cs.DC</categories><comments>Includes 2 Figures, 2 Tables and 4 Pages. Presented in International
  Conference on Communication, Information and Computing Technology (ICCICT-15)
  held in Amritsar on 12-13 May, 2015</comments><msc-class>97P70</msc-class><acm-class>J.7</acm-class><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In this paper, we design and develop a pricing model applicable to strategy
proof pricing. To provide an economic stability towards its consumers. The
economic model we use is Vickrey-Clarke-Groves (VCG). By this each service
provider has to provide a true cost of its services in the cloud market. For
the selection of suitable service for the consumer we adopt a dynamic
programing based algorithm and VCG is used to calculate the payment. Strategy
proof pricing offers a unique cloud pricing service that takes the complexity
out of traditional pricing and enables cloud providers to price accurately,
consistently and competitively
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06650</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06650</id><created>2015-06-22</created><authors><author><keyname>Shah</keyname><forenames>Syed A. W.</forenames></author><author><keyname>Abed-Meraim</keyname><forenames>Karim</forenames></author><author><keyname>Al-Naffouri</keyname><forenames>Tareq Y.</forenames></author></authors><title>Multi-Modulus Algorithms Using Hyperbolic and Givens Rotations for MIMO
  Deconvolution</title><categories>cs.IT math.IT stat.ML</categories><comments>12 pages, 6 figures, submitted to IEEE Trans. Signal Process</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of blind multiple-input multiple-output
deconvolution of a communication system. Two new iterative blind source
separation (BSS) algorithms are presented, based on the minimization of
multi-modulus criterion. Further, we show that the design of algorithm in the
complex domain is quite complicated, so a special structure of real filtering
matrix is suggested and maintained throughout the design. Then, a first
multi-modulus algorithm based on data whitening and Givens rotations is
proposed. An improved version of the latter is introduced for small sample
sizes by combining Hyperbolic (Shear) with Givens rotations to compensate for
the ill whitening that occurs in this case. Proposed methods are finally
compared with several BSS algorithms in terms of signal-to-interference and
noise ratio, symbol error rate and convergence rate. Simulation results show
that the proposed methods outperform the contemporary BSS algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06659</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06659</id><created>2015-06-22</created><authors><author><keyname>Naseem</keyname><forenames>Nayyab</forenames></author><author><keyname>Sirshar</keyname><forenames>Mehreen</forenames></author></authors><title>Target Tracking In Real Time Surveillance Cameras and Videos</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security concerns has been kept on increasing, so it is important for
everyone to keep their property safe from thefts and destruction. So the need
for surveillance techniques are also increasing. The system has been developed
to detect the motion in a video. A system has been developed for real time
applications by using the techniques of background subtraction and frame
differencing. In this system, motion is detected from the webcam or from the
real time video. Background subtraction and frames differencing method has been
used to detect the moving target. In background subtraction method, current
frame is subtracted from the referenced frame and then the threshold is
applied. If the difference is greater than the threshold then it is considered
as the pixel from the moving object, otherwise it is considered as background
pixel. Similarly, two frames difference method takes difference between two
continuous frames. Then that resultant difference frame is thresholded and the
amount of difference pixels is calculated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06661</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06661</id><created>2015-06-22</created><authors><author><keyname>Lago</keyname><forenames>Ugo Dal</forenames></author><author><keyname>Rioli</keyname><forenames>Alessandro</forenames></author></authors><title>Applicative Bisimulation and Quantum $\lambda$-Calculi (Long Version)</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applicative bisimulation is a coinductive technique to check program
equivalence in higher-order functional languages. It is known to be sound, and
sometimes complete, with respect to context equivalence. In this paper we show
that applicative bisimulation also works when the underlying language of
programs takes the form of a linear $\lambda$-calculus extended with features
such as probabilistic binary choice, but also quantum data, the latter being a
setting in which linearity plays a role. The main results are proofs of
soundness for the obtained notions of bisimilarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06668</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06668</id><created>2015-06-22</created><authors><author><keyname>Ochiai</keyname><forenames>Yoichi</forenames></author><author><keyname>Kumagai</keyname><forenames>Kota</forenames></author><author><keyname>Hoshi</keyname><forenames>Takayuki</forenames></author><author><keyname>Rekimoto</keyname><forenames>Jun</forenames></author><author><keyname>Hasegawa</keyname><forenames>Satoshi</forenames></author><author><keyname>Hayasaki</keyname><forenames>Yoshio</forenames></author></authors><title>Fairy Lights in Femtoseconds: Aerial and Volumetric Graphics Rendered by
  Focused Femtosecond Laser Combined with Computational Holographic Fields</title><categories>cs.GR cs.HC physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method of rendering aerial and volumetric graphics using
femtosecond lasers. A high-intensity laser excites a physical matter to emit
light at an arbitrary 3D position. Popular applications can then be explored
especially since plasma induced by a femtosecond laser is safer than that
generated by a nanosecond laser. There are two methods of rendering graphics
with a femtosecond laser in air: Producing holograms using spatial light
modulation technology, and scanning of a laser beam by a galvano mirror. The
holograms and workspace of the system proposed here occupy a volume of up to 1
cm^3; however, this size is scalable depending on the optical devices and their
setup. This paper provides details of the principles, system setup, and
experimental evaluation, and discussions on scalability, design space, and
applications of this system. We tested two laser sources: an adjustable (30-100
fs) laser which projects up to 1,000 pulses per second at energy up to 7 mJ per
pulse, and a 269-fs laser which projects up to 200,000 pulses per second at an
energy up to 50 uJ per pulse. We confirmed that the spatiotemporal resolution
of volumetric displays, implemented with these laser sources, is 4,000 and
200,000 dots per second. Although we focus on laser-induced plasma in air, the
discussion presented here is also applicable to other rendering principles such
as fluorescence and microbubble in solid/liquid materials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06671</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06671</id><created>2015-06-22</created><authors><author><keyname>Elenberg</keyname><forenames>Ethan R.</forenames></author><author><keyname>Shanmugam</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Borokhovich</keyname><forenames>Michael</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author></authors><title>Beyond Triangles: A Distributed Framework for Estimating 3-profiles of
  Large Graphs</title><categories>cs.SI cs.DC cs.DS cs.IT math.IT</categories><comments>To appear in part at KDD'15</comments><acm-class>G.2.2; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of approximating the $3$-profile of a large graph.
$3$-profiles are generalizations of triangle counts that specify the number of
times a small graph appears as an induced subgraph of a large graph. Our
algorithm uses the novel concept of $3$-profile sparsifiers: sparse graphs that
can be used to approximate the full $3$-profile counts for a given large graph.
Further, we study the problem of estimating local and ego $3$-profiles, two
graph quantities that characterize the local neighborhood of each vertex of a
graph.
  Our algorithm is distributed and operates as a vertex program over the
GraphLab PowerGraph framework. We introduce the concept of edge pivoting which
allows us to collect $2$-hop information without maintaining an explicit
$2$-hop neighborhood list at each vertex. This enables the computation of all
the local $3$-profiles in parallel with minimal communication.
  We test out implementation in several experiments scaling up to $640$ cores
on Amazon EC2. We find that our algorithm can estimate the $3$-profile of a
graph in approximately the same time as triangle counting. For the harder
problem of ego $3$-profiles, we introduce an algorithm that can estimate
profiles of hundreds of thousands of vertices in parallel, in the timescale of
minutes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06681</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06681</id><created>2015-06-22</created><authors><author><keyname>Sugathan</keyname><forenames>Sherin</forenames></author><author><keyname>Scaria</keyname><forenames>Reshma</forenames></author><author><keyname>James</keyname><forenames>Alex Pappachen</forenames></author></authors><title>Adaptive Digital Scan Variable Pixels</title><categories>cs.CV</categories><comments>4th International Conference on Advances in Computing, Communications
  and Informatics, August, 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The square and rectangular shape of the pixels in the digital images for
sensing and display purposes introduces several inaccuracies in the
representation of digital images. The major disadvantage of square pixel shapes
is the inability to accurately capture and display the details in the objects
having variable orientations to edges, shapes and regions. This effect can be
observed by the inaccurate representation of diagonal edges in low resolution
square pixel images. This paper explores a less investigated idea of using
variable shaped pixels for improving visual quality of image scans without
increasing the square pixel resolution. The proposed adaptive filtering
technique reports an improvement in image PSNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06684</identifier>
 <datestamp>2015-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06684</id><created>2015-06-22</created><updated>2015-08-27</updated><authors><author><keyname>Inggs</keyname><forenames>Gordon</forenames></author><author><keyname>Thomas</keyname><forenames>David B.</forenames></author><author><keyname>Constantinides</keyname><forenames>George</forenames></author><author><keyname>Luk</keyname><forenames>Wayne</forenames></author></authors><title>Seeing Shapes in Clouds: On the Performance-Cost trade-off for
  Heterogeneous Infrastructure-as-a-Service</title><categories>cs.DC</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/10</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the near future FPGAs will be available by the hour, however this new
Infrastructure as a Service (IaaS) usage mode presents both an opportunity and
a challenge: The opportunity is that programmers can potentially trade
resources for performance on a much larger scale, for much shorter periods of
time than before. The challenge is in finding and traversing the trade-off for
heterogeneous IaaS that guarantees increased resources result in the greatest
possible increased performance. Such a trade-off is Pareto optimal. The Pareto
optimal trade-off for clusters of heterogeneous resources can be found by
solving multiple, multi-objective optimisation problems, resulting in an
optimal allocation of tasks to the available platforms. Solving these
optimisation programs can be done using simple heuristic approaches or formal
Mixed Integer Linear Programming (MILP) techniques. When pricing 128 financial
options using a Monte Carlo algorithm upon a heterogeneous cluster of Multicore
CPU, GPU and FPGA platforms, the MILP approach produces a trade-off that is up
to 110% faster than a heuristic approach, and over 50% cheaper. These results
suggest that high quality performance-resource trade-offs of heterogeneous IaaS
are best realised through a formal optimisation approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06691</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06691</id><created>2015-06-22</created><authors><author><keyname>Krestinskaya</keyname><forenames>Olga</forenames></author><author><keyname>Fedorova</keyname><forenames>Irina</forenames></author><author><keyname>James</keyname><forenames>Alex Pappachen</forenames></author></authors><title>Memristor Load Current Mirror Circuit</title><categories>cs.ET</categories><comments>4th International Conference on Advances in Computing, Communications
  and Informaticst, IEEE, August, 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Simple current mirrors with semiconductor resistive loads suffer from large
on-chip area, leakage currents and thermal effects. In this paper, we report
the feasibility of using memristive loads as a replacement of semiconductor
resistors in simplistic current mirror configuration. We report power, area and
total harmonic distribution, and report the corner conditions on resistance
tolerances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06704</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06704</id><created>2015-04-25</created><authors><author><keyname>Bakhrushin</keyname><forenames>Vladimir</forenames></author></authors><title>Software realization of the complex spectra analysis algorithm in R</title><categories>cs.MS physics.data-an</categories><journal-ref>Bakhrushin, V. (2015) &quot;Software realization of the complex spectra
  analysis algorithm in R&quot; [&quot;Programmnaya realizatsiya algoritma analiza
  slozhnyih spektrov na yazyike R&quot;], Sistemni tehnologiyi, No 2 (97), pp. 3 - 7</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software realization of the complex spectra decomposition on unknown number
of similarcomponents is proposed.The algorithm is based on non-linear
minimizing the sum of squared residuals of the spectrum model. For the adequacy
checking the complex of criteria is used.It tests the model residuals
correspondence with the normal distribution, equality to zero of their mean
value and autocorrelation. Also the closeness of residuals and experimental
data variances is checked.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06707</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06707</id><created>2015-06-22</created><updated>2015-06-28</updated><authors><author><keyname>Chamroukhi</keyname><forenames>Faicel</forenames></author></authors><title>Non-Normal Mixtures of Experts</title><categories>stat.ME cs.LG stat.ML</categories><comments>61 pages</comments><msc-class>62-XX, 62H30, 62H12, 62-07, 62Fxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixture of Experts (MoE) is a popular framework for modeling heterogeneity in
data for regression, classification and clustering. For continuous data which
we consider here in the context of regression and cluster analysis, MoE usually
use normal experts, that is, expert components following the Gaussian
distribution. However, for a set of data containing a group or groups of
observations with asymmetric behavior, heavy tails or atypical observations,
the use of normal experts may be unsuitable and can unduly affect the fit of
the MoE model. In this paper, we introduce new non-normal mixture of experts
(NNMoE) which can deal with these issues regarding possibly skewed,
heavy-tailed data and with outliers. The proposed models are the skew-normal
MoE and the robust $t$ MoE and skew $t$ MoE, respectively named SNMoE, TMoE and
STMoE. We develop dedicated expectation-maximization (EM) and expectation
conditional maximization (ECM) algorithms to estimate the parameters of the
proposed models by monotonically maximizing the observed data log-likelihood.
We describe how the presented models can be used in prediction and in
model-based clustering of regression data. Numerical experiments carried out on
simulated data show the effectiveness and the robustness of the proposed models
in terms modeling non-linear regression functions as well as in model-based
clustering. Then, to show their usefulness for practical applications, the
proposed models are applied to the real-world data of tone perception for
musical data analysis, and the one of temperature anomalies for the analysis of
climate change data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06714</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06714</id><created>2015-06-22</created><authors><author><keyname>Sordoni</keyname><forenames>Alessandro</forenames></author><author><keyname>Galley</keyname><forenames>Michel</forenames></author><author><keyname>Auli</keyname><forenames>Michael</forenames></author><author><keyname>Brockett</keyname><forenames>Chris</forenames></author><author><keyname>Ji</keyname><forenames>Yangfeng</forenames></author><author><keyname>Mitchell</keyname><forenames>Margaret</forenames></author><author><keyname>Nie</keyname><forenames>Jian-Yun</forenames></author><author><keyname>Gao</keyname><forenames>Jianfeng</forenames></author><author><keyname>Dolan</keyname><forenames>Bill</forenames></author></authors><title>A Neural Network Approach to Context-Sensitive Generation of
  Conversational Responses</title><categories>cs.CL cs.AI cs.LG cs.NE</categories><comments>A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji, M. Mitchell,
  J.-Y. Nie, J. Gao, B. Dolan. 2015. A Neural Network Approach to
  Context-Sensitive Generation of Conversational Responses. In Proc. of
  NAACL-HLT. Pages 196-205</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel response generation system that can be trained end to end
on large quantities of unstructured Twitter conversations. A neural network
architecture is used to address sparsity issues that arise when integrating
contextual information into classic statistical models, allowing the system to
take into account previous dialog utterances. Our dynamic-context generative
models show consistent gains over both context-sensitive and
non-context-sensitive Machine Translation and Information Retrieval baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06715</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06715</id><created>2015-06-22</created><authors><author><keyname>Mirrokni</keyname><forenames>Vahab</forenames></author><author><keyname>Zadimoghaddam</keyname><forenames>Morteza</forenames></author></authors><title>Randomized Composable Core-sets for Distributed Submodular Maximization</title><categories>cs.DS cs.DC</categories><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An effective technique for solving optimization problems over massive data
sets is to partition the data into smaller pieces, solve the problem on each
piece and compute a representative solution from it, and finally obtain a
solution inside the union of the representative solutions for all pieces. This
technique can be captured via the concept of {\em composable core-sets}, and
has been recently applied to solve diversity maximization problems as well as
several clustering problems. However, for coverage and submodular maximization
problems, impossibility bounds are known for this technique \cite{IMMM14}. In
this paper, we focus on efficient construction of a randomized variant of
composable core-sets where the above idea is applied on a {\em random
clustering} of the data. We employ this technique for the coverage, monotone
and non-monotone submodular maximization problems. Our results significantly
improve upon the hardness results for non-randomized core-sets, and imply
improved results for submodular maximization in a distributed and streaming
settings.
  In summary, we show that a simple greedy algorithm results in a
$1/3$-approximate randomized composable core-set for submodular maximization
under a cardinality constraint. This is in contrast to a known $O({\log k\over
\sqrt{k}})$ impossibility result for (non-randomized) composable core-set. Our
result also extends to non-monotone submodular functions, and leads to the
first 2-round MapReduce-based constant-factor approximation algorithm with
$O(n)$ total communication complexity for either monotone or non-monotone
functions. Finally, using an improved analysis technique and a new algorithm
$\mathsf{PseudoGreedy}$, we present an improved $0.545$-approximation algorithm
for monotone submodular maximization, which is in turn the first
MapReduce-based algorithm beating factor $1/2$ in a constant number of rounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06724</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06724</id><created>2015-06-22</created><authors><author><keyname>Zhu</keyname><forenames>Yukun</forenames></author><author><keyname>Kiros</keyname><forenames>Ryan</forenames></author><author><keyname>Zemel</keyname><forenames>Richard</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author><author><keyname>Urtasun</keyname><forenames>Raquel</forenames></author><author><keyname>Torralba</keyname><forenames>Antonio</forenames></author><author><keyname>Fidler</keyname><forenames>Sanja</forenames></author></authors><title>Aligning Books and Movies: Towards Story-like Visual Explanations by
  Watching Movies and Reading Books</title><categories>cs.CV cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Books are a rich source of both fine-grained information, how a character, an
object or a scene looks like, as well as high-level semantics, what someone is
thinking, feeling and how these states evolve through a story. This paper aims
to align books to their movie releases in order to provide rich descriptive
explanations for visual content that go semantically far beyond the captions
available in current datasets. To align movies and books we exploit a neural
sentence embedding that is trained in an unsupervised way from a large corpus
of books, as well as a video-text neural embedding for computing similarities
between movie clips and sentences in the book. We propose a context-aware CNN
to combine information from multiple sources. We demonstrate good quantitative
performance for movie/book alignment and show several qualitative examples that
showcase the diversity of tasks our model can be used for.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06726</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06726</id><created>2015-06-22</created><authors><author><keyname>Kiros</keyname><forenames>Ryan</forenames></author><author><keyname>Zhu</keyname><forenames>Yukun</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author><author><keyname>Zemel</keyname><forenames>Richard S.</forenames></author><author><keyname>Torralba</keyname><forenames>Antonio</forenames></author><author><keyname>Urtasun</keyname><forenames>Raquel</forenames></author><author><keyname>Fidler</keyname><forenames>Sanja</forenames></author></authors><title>Skip-Thought Vectors</title><categories>cs.CL cs.LG</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an approach for unsupervised learning of a generic, distributed
sentence encoder. Using the continuity of text from books, we train an
encoder-decoder model that tries to reconstruct the surrounding sentences of an
encoded passage. Sentences that share semantic and syntactic properties are
thus mapped to similar vector representations. We next introduce a simple
vocabulary expansion method to encode words that were not seen as part of
training, allowing us to expand our vocabulary to a million words. After
training our model, we extract and evaluate our vectors with linear models on 8
tasks: semantic relatedness, paraphrase detection, image-sentence ranking,
question-type classification and 4 benchmark sentiment and subjectivity
datasets. The end result is an off-the-shelf encoder that can produce highly
generic sentence representations that are robust and perform well in practice.
We will make our encoder publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06735</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06735</id><created>2015-06-19</created><authors><author><keyname>Singh</keyname><forenames>Tejinder</forenames></author></authors><title>Hybrid Memristor-CMOS (MeMOS) based Logic Gates and Adder Circuits</title><categories>cs.ET</categories><comments>10 pages, 13 figures, 5 tables, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Practical memristor came into picture just few years back and instantly
became the topic of interest for researchers and scientists. Memristor is the
fourth basic two-terminal passive circuit element apart from well known
resistor, capacitor and inductor. Recently, memristor based architectures has
been proposed by many researchers. In this paper, we have designed a hybrid
Memristor-CMOS (MeMOS) logic based adder circuit that can be used in numerous
logic computational architectures. We have also analyzed the transient response
of logic gates designed using MeMOS logic circuits. MeMOS use CMOS 180 nm
process with memristor to compute boolean logic operations. Various parameters
including speed, ares, delay and power dissipation are computed and compared
with standard CMOS 180 nm logic design. The proposed logic shows better area
utilization and excellent results from existing CMOS logic circuits at standard
1.8 V operating voltage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06737</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06737</id><created>2015-06-22</created><authors><author><keyname>Florescu</keyname><forenames>Laura</forenames></author><author><keyname>Perkins</keyname><forenames>Will</forenames></author></authors><title>Spectral Thresholds in the Bipartite Stochastic Block Model</title><categories>math.PR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a bipartite stochastic block model on vertex sets $V_1$ and $V_2$
of size $n_1$ and $n_2$ respectively, with planted partitions in each, and ask
at what densities can spectral algorithms recover the partition of the smaller
vertex set. The model was recently used by Feldman et al. to give a unified
algorithm for random planted hypergraph partitioning and planted random k-SAT.
  When $n_2 \gg n_1$, multiple thresholds emerge. We show that the singular
vectors of the rectangular adjacency matrix exhibit a localization /
delocalization phase transition at edge density $p = \tilde \Theta(n_1^{-2/3}
n_2^{-1/3})$, giving recovery above the threshold and no recovery below.
Nevertheless, we propose a simple spectral algorithm, Diagonal Deletion SVD,
which recovers the partition at density $p = \tilde \Theta(n_1^{-1/2}
n_2^{-1/2})$.
  Finally, we locate a sharp threshold for detection of the partition, in the
sense of the results of Mossel, Neeman, Sly and Massouli\'e for the stochastic
block model. This gives the best known bounds for efficient recovery densities
in planted k-SAT and hypergraph partitioning as well as showing a barrier to
further improvement via the reduction to the bipartite block model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06745</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06745</id><created>2015-06-22</created><authors><author><keyname>Nachmanson</keyname><forenames>Lev</forenames></author><author><keyname>Prutkin</keyname><forenames>Roman</forenames></author><author><keyname>Lee</keyname><forenames>Bongshin</forenames></author><author><keyname>Riche</keyname><forenames>Nathalie Henry</forenames></author><author><keyname>Holroyd</keyname><forenames>Alexander E.</forenames></author><author><keyname>Chen</keyname><forenames>Xiaoji</forenames></author></authors><title>GraphMaps: Browsing Large Graphs as Interactive Maps</title><categories>cs.GR cs.DM cs.MS cs.SI</categories><comments>submitted to GD 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms for laying out large graphs have seen significant progress in the
past decade. However, browsing large graphs remains a challenge. Rendering
thousands of graphical elements at once often results in a cluttered image, and
navigating these elements naively can cause disorientation. To address this
challenge we propose a method called GraphMaps, mimicking the browsing
experience of online geographic maps.
  GraphMaps creates a sequence of layers, where each layer refines the previous
one. During graph browsing, GraphMaps chooses the layer corresponding to the
zoom level, and renders only those entities of the layer that intersect the
current viewport. The result is that, regardless of the graph size, the number
of entities rendered at each view does not exceed a predefined threshold, yet
all graph elements can be explored by the standard zoom and pan operations.
  GraphMaps preprocesses a graph in such a way that during browsing, the
geometry of the entities is stable, and the viewer is responsive. Our case
studies indicate that GraphMaps is useful in gaining an overview of a large
graph, and also in exploring a graph on a finer level of detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06784</identifier>
 <datestamp>2015-08-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06784</id><created>2015-06-22</created><updated>2015-08-06</updated><authors><author><keyname>Trautman</keyname><forenames>Pete</forenames></author></authors><title>Assistive Planning in Complex, Dynamic Environments: a Probabilistic
  Approach</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the probabilistic foundations of shared control in complex dynamic
environments. In order to do this, we formulate shared control as a random
process and describe the joint distribution that governs its behavior. For
tractability, we model the relationships between the operator, autonomy, and
crowd as an undirected graphical model. Further, we introduce an interaction
function between the operator and the robot, that we call &quot;agreeability&quot;; in
combination with the methods developed in~\cite{trautman-ijrr-2015}, we extend
a cooperative collision avoidance autonomy to shared control. We therefore
quantify the notion of simultaneously optimizing over agreeability (between the
operator and autonomy), and safety and efficiency in crowded environments. We
show that for a particular form of interaction function between the autonomy
and the operator, linear blending is recovered exactly. Additionally, to
recover linear blending, unimodal restrictions must be placed on the models
describing the operator and the autonomy. In turn, these restrictions raise
questions about the flexibility and applicability of the linear blending
framework. Additionally, we present an extension of linear blending called
&quot;operator biased linear trajectory blending&quot; (which formalizes some recent
approaches in linear blending such as~\cite{dragan-ijrr-2013}) and show that
not only is this also a restrictive special case of our probabilistic approach,
but more importantly, is statistically unsound, and thus, mathematically,
unsuitable for implementation. Instead, we suggest a statistically principled
approach that guarantees data is used in a consistent manner, and show how this
alternative approach converges to the full probabilistic framework. We conclude
by proving that, in general, linear blending is suboptimal with respect to the
joint metric of agreeability, safety, and efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06791</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06791</id><created>2015-06-22</created><authors><author><keyname>Goldsteen</keyname><forenames>Abigail</forenames></author><author><keyname>Grandison</keyname><forenames>Tyrone</forenames></author><author><keyname>Just</keyname><forenames>Mike</forenames></author><author><keyname>Koved</keyname><forenames>Larry</forenames></author><author><keyname>Malcolm</keyname><forenames>Rohan</forenames></author><author><keyname>Thorpe</keyname><forenames>Sean</forenames></author></authors><title>Proceedings of the Ninth Workshop on Web 2.0 Security and Privacy (W2SP)
  2015</title><categories>cs.CR</categories><comments>10 papers in the workshop proceedings (from 33 submissions: 30%
  acceptance rate)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the Ninth Workshop on Web 2.0 Security and Privacy
(W2SP) 2015, held in San Jose, CA, USA, on May 21, 2015. The workshop was held
as part of the IEEE Computer Society Security and Privacy Workshops, in
conjunction with the IEEE Symposium on Security and Privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06793</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06793</id><created>2015-06-22</created><authors><author><keyname>Alatabbi</keyname><forenames>Ali</forenames></author><author><keyname>Islam</keyname><forenames>A. S. Sohidull</forenames></author><author><keyname>Rahman</keyname><forenames>M. Sohel</forenames></author><author><keyname>Simpson</keyname><forenames>Jamie</forenames></author><author><keyname>Smyth</keyname><forenames>W. F.</forenames></author></authors><title>Enhanced Covers of Regular &amp; Indeterminate Strings using Prefix Tables</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A \itbf{cover} of a string $x = x[1..n]$ is a proper substring $u$ of $x$
such that $x$ can be constructed from possibly overlapping instances of $u$. A
recent paper \cite{FIKPPST13} relaxes this definition --- an \itbf{enhanced
cover} $u$ of $x$ is a border of $x$ (that is, a proper prefix that is also a
suffix) that covers a {\it maximum} number of positions in $x$ (not necessarily
all) --- and proposes efficient algorithms for the computation of enhanced
covers. These algorithms depend on the prior computation of the \itbf{border
array} $\beta[1..n]$, where $\beta[i]$ is the length of the longest border of
$x[1..i]$, $1 \le i \le n$. In this paper, we first show how to compute
enhanced covers using instead the \itbf{prefix table}: an array $\pi[1..n]$
such that $\pi[i]$ is the length of the longest substring of $x$ beginning at
position $i$ that matches a prefix of $x$. Unlike the border array, the prefix
table is robust: its properties hold also for \itbf{indeterminate strings} ---
that is, strings defined on {\it subsets} of the alphabet $\Sigma$ rather than
individual elements of $\Sigma$. Thus, our algorithms, in addition to being
faster in practice and more space-efficient than those of \cite{FIKPPST13},
allow us to easily extend the computation of enhanced covers to indeterminate
strings. Both for regular and indeterminate strings, our algorithms execute in
expected linear time. Along the way we establish an important theoretical
result: that the expected maximum length of any border of any prefix of a
regular string $x$ is approximately 1.64 for binary alphabets, less for larger
ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06796</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06796</id><created>2015-06-22</created><updated>2015-10-19</updated><authors><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author><author><keyname>Helbing</keyname><forenames>Dirk</forenames></author></authors><title>When slower is faster</title><categories>nlin.AO cond-mat.dis-nn cs.NE physics.soc-ph q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The slower is faster (SIF) effect occurs when a system performs worse as its
components try to do better. Thus, a moderate individual efficiency actually
leads to a better systemic performance. The SIF effect takes place in a variety
of phenomena. We review studies and examples of the SIF effect in pedestrian
dynamics, vehicle traffic, traffic light control, logistics, public transport,
social dynamics, ecological systems, and adaptation. Drawing on these examples,
we generalize common features of the SIF effect and suggest possible future
lines of research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06801</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06801</id><created>2015-06-22</created><authors><author><keyname>Cheng</keyname><forenames>Hao-Chung</forenames></author><author><keyname>Hsieh</keyname><forenames>Min-Hsiu</forenames></author></authors><title>New Characterizations of Matrix $\Phi$-Entropies, Poincar\'e and Sobolev
  Inequalities and an Upper Bound to Holevo Quantity</title><categories>quant-ph cs.IT math-ph math.IT math.MP math.PR</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive new characterizations of the matrix $\Phi$-entropies introduced in
[Electron.~J.~Probab., 19(20): 1--30, 2014}]. These characterizations help to
better understand the properties of matrix $\Phi$-entropies, and are a powerful
tool for establishing matrix concentration inequalities for matrix-valued
functions of independent random variables. In particular, we use the
subadditivity property to prove a Poincar\'e inequality for the matrix
$\Phi$-entropies. We also provide a new proof for the matrix Efron-Stein
inequality. Furthermore, we derive logarithmic Sobolev inequalities for
matrix-valued functions defined on Boolean hypercubes and with Gaussian
distributions. Our proof relies on the powerful matrix Bonami-Beckner
inequality. Finally, the Holevo quantity in quantum information theory is
closely related to the matrix $\Phi$-entropies. This allows us to upper bound
the Holevo quantity of a classical-quantum ensemble that undergoes a special
Markov evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06813</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06813</id><created>2015-06-22</created><updated>2016-02-16</updated><authors><author><keyname>Shahrokhi</keyname><forenames>Farhad</forenames></author></authors><title>Bounds for the Clique Cover Width of Factors of the Apex Graph of the
  Planar Grid</title><categories>cs.DM math.CO</categories><journal-ref>Congressus Numerantium 224 (2015), 213-220</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The {\it clique cover width} of $G$, denoted by $ccw(G)$, is the minimum
value of the bandwidth of all graphs that are obtained by contracting the
cliques in a clique cover of $G$ into a single vertex. For $i=1,2,...,d,$ let
$G_i$ be a graph with $V(G_i)=V$, and let $G$ be a graph with $V(G)=V$ and
$E(G)=\cap_{i=1}^d(G_i)$, then we write $G=\cap_{i=1}^dG_i$ and call each
$G_i,i=1,2,...,d$ a factor of $G$. We are interested in the case where $G_1$ is
chordal, and $ccw(G_i),i=2,3...,d$ for each factor $G_i$ is &quot;small&quot;. Here we
show a negative result. Specifically, let ${\hat G}(k,n)$ be the graph obtained
by joining a set of $k$ apex vertices of degree $n^2$ to all vertices of an
$n\times n$ grid, and then adding some possible edges among these $k$ vertices.
We prove that if ${\hat G}(k,n)=\cap_{i=1}^dG_i$, with $G_1$ being chordal,
then, $max_{2\le i\le d}\{ccw(G_i)\}\ge {n^{1\over d-1}\over 2.{(2c)}^{1\over
{d-1}}}$, where $c$ is a constant. Furthermore, for $d=2$, we construct a
chordal graph $G_1$ and a graph $G_2$ with $ccw(G_2)\le {n\over 2}+k$ so that
${\hat G}(k,n)=G_1\cap G_2$. Finally, let ${\hat G}$ be the clique sum graph of
${\hat G}(k_i, n_i), i=1,2,...t$, where the underlying grid is $n_i\times n_i$
and the sum is taken at apex vertices. Then, we show ${\hat G}=G_1\cap G_2$,
where, $G_1$ is chordal and $ccw(G_2)\le \sum_{i=1}^t(n_i+k_i)$. The
implications and applications of the results are discussed, including
addressing a recent question of David Wood.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06817</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06817</id><created>2015-06-22</created><updated>2015-08-16</updated><authors><author><keyname>Gelashvili</keyname><forenames>Rati</forenames></author></authors><title>On the Optimal Space Complexity of Consensus for Anonymous Processes</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The optimal space complexity of consensus in shared memory is a decades-old
open problem. For a system of $n$ processes, no algorithm is known that uses a
sublinear number of registers. However, the best known lower bound due to Fich,
Herlihy, and Shavit requires $\Omega(\sqrt{n})$ registers.
  The special symmetric case of the problem where processes are anonymous (run
the same algorithm) has also attracted attention. Even in this case, the best
lower and upper bounds are still $\Omega(\sqrt{n})$ and $O(n)$. Moreover, Fich,
Herlihy, and Shavit first proved their lower bound for anonymous processes, and
then extended it to the general case. As such, resolving the anonymous case
might be a significant step towards understanding and solving the general
problem.
  In this work, we show that in a system of anonymous processes, any consensus
algorithm satisfying nondeterministic solo termination has to use $\Omega(n)$
read-write registers in some execution. This implies an $\Omega(n)$ lower bound
on the space complexity of deterministic obstruction-free and randomized
wait-free consensus, matching the upper bound and closing the symmetric case of
the open problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06822</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06822</id><created>2015-06-22</created><authors><author><keyname>Zarei</keyname><forenames>Shahram</forenames></author><author><keyname>Gerstacker</keyname><forenames>Wolfgang</forenames></author><author><keyname>Aulin</keyname><forenames>Jocelyn</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>I/Q Imbalance Aware Widely-Linear Receiver for Uplink Multi-Cell Massive
  MIMO Systems</title><categories>cs.IT math.IT</categories><comments>40 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In-phase/quadrature-phase (I/Q) imbalance is one of the most important
hardware impairments in communication systems. It arises in the analogue parts
of direct conversion radio frequency (RF) transceivers and can cause severe
performance losses. In this paper, I/Q imbalance (IQI) aware widely-linear (WL)
channel estimation and data detection schemes for uplink multi-cell massive
multiple-input multiple-output (MIMO) systems are proposed. The resulting
receiver is a WL extension of the minimum mean square error (MMSE) receiver and
jointly mitigates multi-user interference and IQI by processing the real and
the imaginary parts of the received signal separately. The IQI arising at both
the base station (BS) and the user terminals (UTs) is then taken into account.
The considered channel state information (CSI) acquisition model includes the
effects of both estimation errors and pilot contamination, which is caused by
the reuse of the same training sequences in neighboring cells. We apply results
from random matrix theory to derive analytical expressions for the achievable
sum rates of the proposed IQI aware and conventional IQI unaware receivers. Our
simulation results show that the performance of the proposed IQI aware WLMMSE
receiver in a system with IQI is close to that of the MMSE receiver in an ideal
system without IQI. Moreover, our results for the sum rate of the IQI unaware
MMSE receiver reveal that the performance loss due to IQI can be large and, if
left unattended, does not vanish for large numbers of BS antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06825</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06825</id><created>2015-06-22</created><authors><author><keyname>Flynn</keyname><forenames>John</forenames></author><author><keyname>Neulander</keyname><forenames>Ivan</forenames></author><author><keyname>Philbin</keyname><forenames>James</forenames></author><author><keyname>Snavely</keyname><forenames>Noah</forenames></author></authors><title>DeepStereo: Learning to Predict New Views from the World's Imagery</title><categories>cs.CV</categories><comments>Video showing additional results available at
  http://youtu.be/cizgVZ8rjKA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep networks have recently enjoyed enormous success when applied to
recognition and classification problems in computer vision, but their use in
graphics problems has been limited. In this work, we present a novel deep
architecture that performs new view synthesis directly from pixels, trained
from a large number of posed image sets. In contrast to traditional approaches
which consist of multiple complex stages of processing, each of which require
careful tuning and can fail in unexpected ways, our system is trained
end-to-end. The pixels from neighboring views of a scene are presented to the
network which then directly produces the pixels of the unseen view. The
benefits of our approach include generality (we only require posed image sets
and can easily apply our method to different domains), and high quality results
on traditionally difficult scenes. We believe this is due to the end-to-end
nature of our system which is able to plausibly generate pixels according to
color, depth, and texture priors learnt automatically from the training data.
To verify our method we show that it can convincingly reproduce known test
views from nearby imagery. Additionally we show images rendered from novel
viewpoints. To our knowledge, our work is the first to apply deep learning to
the problem of new view synthesis from sets of real-world, natural imagery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06830</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06830</id><created>2015-06-22</created><updated>2015-10-16</updated><authors><author><keyname>Zhou</keyname><forenames>Zhengchun</forenames></author><author><keyname>Li</keyname><forenames>Nian</forenames></author><author><keyname>Fan</keyname><forenames>Cuiling</forenames></author><author><keyname>Helleseth</keyname><forenames>Tor</forenames></author></authors><title>Linear Codes with Two or Three Weights From Quadratic Bent Functions</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1503.06512 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear codes with few weights have applications in secrete sharing,
authentication codes, association schemes, and strongly regular graphs. In this
paper, several classes of $p$-ary linear codes with two or three weights are
constructed from quadratic Bent functions over the finite field $\gf_p$, where
$p$ is an odd prime. They include some earlier linear codes as special cases.
The weight distributions of these linear codes are also determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06832</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06832</id><created>2015-06-22</created><authors><author><keyname>Davletcharova</keyname><forenames>Assel</forenames></author><author><keyname>Sugathan</keyname><forenames>Sherin</forenames></author><author><keyname>Abraham</keyname><forenames>Bibia</forenames></author><author><keyname>James</keyname><forenames>Alex Pappachen</forenames></author></authors><title>Detection and Analysis of Emotion From Speech Signals</title><categories>cs.SD cs.CL cs.HC</categories><comments>2nd International Symposium on Computer Vision and the Internet,
  2015; to appear in Procedia Computer Science Journal, Elsevier, 2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recognizing emotion from speech has become one the active research themes in
speech processing and in applications based on human-computer interaction. This
paper conducts an experimental study on recognizing emotions from human speech.
The emotions considered for the experiments include neutral, anger, joy and
sadness. The distinuishability of emotional features in speech were studied
first followed by emotion classification performed on a custom dataset. The
classification was performed for different classifiers. One of the main feature
attribute considered in the prepared dataset was the peak-to-peak distance
obtained from the graphical representation of the speech signals. After
performing the classification tests on a dataset formed from 30 different
subjects, it was found that for getting better accuracy, one should consider
the data collected from one person rather than considering the data from a
group of people.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06833</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06833</id><created>2015-06-22</created><updated>2015-08-19</updated><authors><author><keyname>Ferraro</keyname><forenames>Francis</forenames><affiliation>Kenneth</affiliation></author><author><keyname>Mostafazadeh</keyname><forenames>Nasrin</forenames><affiliation>Kenneth</affiliation></author><author><keyname>Ting-Hao</keyname><affiliation>Kenneth</affiliation></author><author><keyname>Huang</keyname></author><author><keyname>Vanderwende</keyname><forenames>Lucy</forenames></author><author><keyname>Devlin</keyname><forenames>Jacob</forenames></author><author><keyname>Galley</keyname><forenames>Michel</forenames></author><author><keyname>Mitchell</keyname><forenames>Margaret</forenames></author></authors><title>A Survey of Current Datasets for Vision and Language Research</title><categories>cs.CL cs.AI cs.CV cs.GL</categories><comments>To appear in EMNLP 2015, short proceedings. Dataset analysis and
  discussion expanded, including an initial examination into reporting bias for
  one of them. F.F. and N.M. contributed equally to this work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrating vision and language has long been a dream in work on artificial
intelligence (AI). In the past two years, we have witnessed an explosion of
work that brings together vision and language from images to videos and beyond.
The available corpora have played a crucial role in advancing this area of
research. In this paper, we propose a set of quality metrics for evaluating and
analyzing the vision &amp; language datasets and categorize them accordingly. Our
analyses show that the most recent datasets have been using more complex
language and more abstract concepts, however, there are different strengths and
weaknesses in each.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06840</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06840</id><created>2015-06-22</created><updated>2016-01-24</updated><authors><author><keyname>Reddi</keyname><forenames>Sashank J.</forenames></author><author><keyname>Hefny</keyname><forenames>Ahmed</forenames></author><author><keyname>Sra</keyname><forenames>Suvrit</forenames></author><author><keyname>P&#xf3;czos</keyname><forenames>Barnab&#xe1;s</forenames></author><author><keyname>Smola</keyname><forenames>Alex</forenames></author></authors><title>On Variance Reduction in Stochastic Gradient Descent and its
  Asynchronous Variants</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study optimization algorithms based on variance reduction for stochastic
gradient descent (SGD). Remarkable recent progress has been made in this
direction through development of algorithms like SAG, SVRG, SAGA. These
algorithms have been shown to outperform SGD, both theoretically and
empirically. However, asynchronous versions of these algorithms---a crucial
requirement for modern large-scale applications---have not been studied. We
bridge this gap by presenting a unifying framework for many variance reduction
techniques. Subsequently, we propose an asynchronous algorithm grounded in our
framework, and prove its fast convergence. An important consequence of our
general approach is that it yields asynchronous versions of variance reduction
algorithms such as SVRG and SAGA as a byproduct. Our method achieves near
linear speedup in sparse settings common to machine learning. We demonstrate
the empirical performance of our method through a concrete realization of
asynchronous SVRG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06848</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06848</id><created>2015-06-22</created><authors><author><keyname>Poursoltan</keyname><forenames>Shayan</forenames></author><author><keyname>Neumann</keyname><forenames>FranK</forenames></author></authors><title>A Feature-Based Analysis on the Impact of Set of Constraints for
  e-Constrained Differential Evolution</title><categories>cs.NE</categories><comments>17 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different types of evolutionary algorithms have been developed for
constrained continuous optimization. We carry out a feature-based analysis of
evolved constrained continuous optimization instances to understand the
characteristics of constraints that make problems hard for evolutionary
algorithm. In our study, we examine how various sets of constraints can
influence the behaviour of e-Constrained Differential Evolution. Investigating
the evolved instances, we obtain knowledge of what type of constraints and
their features make a problem difficult for the examined algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06850</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06850</id><created>2015-06-22</created><authors><author><keyname>Liu</keyname><forenames>Yuan</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>Information and Energy Cooperation in OFDM Relaying: Protocols and
  Optimization</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE TVT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrating power transfer into wireless communications for supporting
simultaneous wireless information and power transfer (SWIPT) is a promising
technique in energy-constrained wireless networks. While most existing work on
SWIPT focuses on capacity-energy characterization, the benefits of cooperative
transmission for SWIPT are much less investigated. In this paper, we consider
SWIPT in an orthogonal frequency-division multiplexing (OFDM) relaying system,
where a source node transfers information and a fraction of power
simultaneously to a relay node, and the relay node uses the harvested power
from the source node to forward the source information to the destination. To
support the simultaneous information and energy cooperation, we first propose a
transmission protocol assuming that the direct link between the source and
destination does not exist, namely power splitting (PS) relaying protocol,
where the relay node splits the received signal power in the first hop into two
separate parts, one for information decoding and the other for energy
harvesting. Then, we consider the case that the direct link between the source
and destination is available, and the transmission mode adaptation (TMA)
protocol is proposed, where the transmission can be completed by cooperative
mode and direct mode simultaneously (over different subcarriers). In direct
mode, when the source transmits signal to the destination, the destination
receives the signal as information and the relay node concurrently receives the
signal for energy harvesting. Joint resource allocation problems are formulated
to maximize the system throughput. By using the Lagrangian dual method, we
develop efficient algorithms to solve the nonconvex optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06855</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06855</id><created>2015-06-23</created><authors><author><keyname>Alhashim</keyname><forenames>Ibraheem</forenames></author></authors><title>Modeling and Correspondence of Topologically Complex 3D Shapes</title><categories>cs.GR</categories><report-no>SFU-CMPT TR 2015-55-2</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3D shape creation and modeling remains a challenging task especially for
novice users. Many methods in the field of computer graphics have been proposed
to automate the often repetitive and precise operations needed during the
modeling of detailed shapes. This report surveys different approaches of shape
modeling and correspondence especially for shapes exhibiting topological
complexity. We focus on methods designed to help generate or process shapes
with large number of interconnected components often found in man-made shapes.
We first discuss a variety of modeling techniques, that leverage existing
shapes, in easy to use creative modeling systems. We then discuss possible
correspondence strategies for topologically different shapes as it is a
requirement for such systems. Finally, we look at different shape
representations and tools that facilitate the modification of shape topology
and we focus on those particularly useful in free-form 3D modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06857</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06857</id><created>2015-06-23</created><updated>2015-06-26</updated><authors><author><keyname>Askalidis</keyname><forenames>Georgios</forenames></author></authors><title>The Impact of Large Scale Promotions on the Sales and Ratings of Mobile
  Apps: Evidence from Apple's App Store</title><categories>cs.CY cs.GT cs.SI</categories><comments>21 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile apps is a highly competitive market and promotions is a way for apps
to gain word-of-mouth. Our work tries to quantify the benefits and risks of
various promotions and suggest ways to design them in ways that amplify the
benefits and mitigate the risks. We study four promotions offered on Apple's
App Store that vary in scale, price discount and redemption procedure. We find
that each of these characteristics has a unique effect on the sales and
ratings. Promotions that are digital (i.e. very easy to redeem) and full price
discounted are the ones that attract the largest audiences alongside with the
largest disturbances on the ratings, whereas making the redemption of a coupon
slightly harder or offering a non-full price discount will cause a lesser
increase on sales but can help filter out potentially sub-optimal user
experiences and the reviews that come with it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06863</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06863</id><created>2015-06-23</created><updated>2015-06-23</updated><authors><author><keyname>Galley</keyname><forenames>Michel</forenames></author><author><keyname>Brockett</keyname><forenames>Chris</forenames></author><author><keyname>Sordoni</keyname><forenames>Alessandro</forenames></author><author><keyname>Ji</keyname><forenames>Yangfeng</forenames></author><author><keyname>Auli</keyname><forenames>Michael</forenames></author><author><keyname>Quirk</keyname><forenames>Chris</forenames></author><author><keyname>Mitchell</keyname><forenames>Margaret</forenames></author><author><keyname>Gao</keyname><forenames>Jianfeng</forenames></author><author><keyname>Dolan</keyname><forenames>Bill</forenames></author></authors><title>deltaBLEU: A Discriminative Metric for Generation Tasks with
  Intrinsically Diverse Targets</title><categories>cs.CL</categories><comments>6 pages, to appear at ACL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Discriminative BLEU (deltaBLEU), a novel metric for intrinsic
evaluation of generated text in tasks that admit a diverse range of possible
outputs. Reference strings are scored for quality by human raters on a scale of
[-1, +1] to weight multi-reference BLEU. In tasks involving generation of
conversational responses, deltaBLEU correlates reasonably with human judgments
and outperforms sentence-level and IBM BLEU in terms of both Spearman's rho and
Kendall's tau.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06868</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06868</id><created>2015-06-23</created><authors><author><keyname>Zhou</keyname><forenames>Luping</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Liu</keyname><forenames>Lingqiao</forenames></author><author><keyname>Ogunbona</keyname><forenames>Philip</forenames></author><author><keyname>Shen</keyname><forenames>Dinggang</forenames></author></authors><title>Learning Discriminative Bayesian Networks from High-dimensional
  Continuous Neuroimaging Data</title><categories>cs.CV cs.LG</categories><comments>16 pages and 5 figures for the article (excluding appendix)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to its causal semantics, Bayesian networks (BN) have been widely employed
to discover the underlying data relationship in exploratory studies, such as
brain research. Despite its success in modeling the probability distribution of
variables, BN is naturally a generative model, which is not necessarily
discriminative. This may cause the ignorance of subtle but critical network
changes that are of investigation values across populations. In this paper, we
propose to improve the discriminative power of BN models for continuous
variables from two different perspectives. This brings two general
discriminative learning frameworks for Gaussian Bayesian networks (GBN). In the
first framework, we employ Fisher kernel to bridge the generative models of GBN
and the discriminative classifiers of SVMs, and convert the GBN parameter
learning to Fisher kernel learning via minimizing a generalization error bound
of SVMs. In the second framework, we employ the max-margin criterion and build
it directly upon GBN models to explicitly optimize the classification
performance of the GBNs. The advantages and disadvantages of the two frameworks
are discussed and experimentally compared. Both of them demonstrate strong
power in learning discriminative parameters of GBNs for neuroimaging based
brain network analysis, as well as maintaining reasonable representation
capacity. The contributions of this paper also include a new Directed Acyclic
Graph (DAG) constraint with theoretical guarantee to ensure the graph validity
of GBN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06869</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06869</id><created>2015-06-23</created><updated>2016-01-05</updated><authors><author><keyname>Chong</keyname><forenames>C. T.</forenames><affiliation>National University of Singapore</affiliation></author><author><keyname>Hoi</keyname><forenames>Gordon</forenames><affiliation>National University of Singapore</affiliation></author><author><keyname>Stephan</keyname><forenames>Frank</forenames><affiliation>National University of Singapore</affiliation></author><author><keyname>Turetsky</keyname><forenames>Daniel</forenames><affiliation>Kurt Goedel Research Center</affiliation></author></authors><title>Partial functions and domination</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:16) 2015</journal-ref><doi>10.2168/LMCS-11(3:16)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current work introduces the notion of pdominant sets and studies their
recursion-theoretic properties. Here a set A is called pdominant iff there is a
partial A-recursive function {\psi} such that for every partial recursive
function {\phi} and almost every x in the domain of {\phi} there is a y in the
domain of {\psi} with y&lt;= x and {\psi}(y) &gt; {\phi}(x). While there is a full
{\pi}01-class of nonrecursive sets where no set is pdominant, there is no
{\pi}01-class containing only pdominant sets. No weakly 2-generic set is
pdominant while there are pdominant 1-generic sets below K. The halves of
Chaitin's {\Omega} are pdominant. No set which is low for Martin-L\&quot;of random
is pdominant. There is a low r.e. set which is pdominant and a high r.e. set
which is not pdominant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06876</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06876</id><created>2015-06-23</created><authors><author><keyname>Popov</keyname><forenames>Alexander</forenames></author><author><keyname>Zermas</keyname><forenames>Dimitrios</forenames></author><author><keyname>Papanikolopoulos</keyname><forenames>Nikolaos</forenames></author></authors><title>Autonomous 3D Reconstruction Using a MAV</title><categories>cs.CV</categories><comments>6 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An approach is proposed for high resolution 3D reconstruction of an object
using a Micro Air Vehicle (MAV). A system is described which autonomously
captures images and performs a dense 3D reconstruction via structure from
motion with no prior knowledge of the environment. Only the MAVs own sensors,
the front facing camera and the Inertial Measurement Unit (IMU) are utilized.
Precision agriculture is considered as an example application for the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06879</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06879</id><created>2015-06-23</created><authors><author><keyname>Hu</keyname><forenames>Zewen</forenames></author><author><keyname>Wu</keyname><forenames>Yishan</forenames></author></authors><title>A Probe into Causes of Non-citation Based on Survey Data</title><categories>cs.DL</categories><msc-class>62</msc-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Empirical analysis results about the possible causes leading to non-citation
may help increase the potential of researchers' work to be cited and editorial
staffs of journals to identify contributions with potential high quality. In
this study, we conduct a survey on the possible causes leading to citation or
non-citation based on a questionnaire. We then perform a statistical analysis
to identify the major causes leading to non-citation in combination with the
analysis on the data collected through the survey. Most respondents to our
questionnaire identified eight major causes that facilitate easy citation of
one's papers, such as research hotspots and novel topics of content, longer
intervals after publication, research topics similar to my work, high quality
of content, reasonable self-citation, highlighted title, prestigious authors,
academic tastes and interests similar to mine.They also pointed out that the
vast difference between their current and former research directions as the
primary reason for their previously uncited papers. They feel that text that
includes notes, comments, and letters to editors are rarely cited, and the same
is true for too short or too lengthy papers. In comparison, it is easier for
reviews, articles, or papers of intermediate length to be cited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06881</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06881</id><created>2015-06-23</created><authors><author><keyname>Arandjelovic</keyname><forenames>Ognjen</forenames></author></authors><title>Automatic vehicle tracking and recognition from aerial image sequences</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of automated vehicle tracking and
recognition from aerial image sequences. Motivated by its successes in the
existing literature focus on the use of linear appearance subspaces to describe
multi-view object appearance and highlight the challenges involved in their
application as a part of a practical system. A working solution which includes
steps for data extraction and normalization is described. In experiments on
real-world data the proposed methodology achieved promising results with a high
correct recognition rate and few, meaningful errors (type II errors whereby
genuinely similar targets are sometimes being confused with one another).
Directions for future research and possible improvements of the proposed method
are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06882</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06882</id><created>2015-06-23</created><authors><author><keyname>Alameda-Pineda</keyname><forenames>Xavier</forenames></author><author><keyname>Staiano</keyname><forenames>Jacopo</forenames></author><author><keyname>Subramanian</keyname><forenames>Ramanathan</forenames></author><author><keyname>Batrinca</keyname><forenames>Ligia</forenames></author><author><keyname>Ricci</keyname><forenames>Elisa</forenames></author><author><keyname>Lepri</keyname><forenames>Bruno</forenames></author><author><keyname>Lanz</keyname><forenames>Oswald</forenames></author><author><keyname>Sebe</keyname><forenames>Nicu</forenames></author></authors><title>SALSA: A Novel Dataset for Multimodal Group Behavior Analysis</title><categories>cs.CV</categories><comments>14 pages, 11 figures</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Studying free-standing conversational groups (FCGs) in unstructured social
settings (e.g., cocktail party ) is gratifying due to the wealth of information
available at the group (mining social networks) and individual (recognizing
native behavioral and personality traits) levels. However, analyzing social
scenes involving FCGs is also highly challenging due to the difficulty in
extracting behavioral cues such as target locations, their speaking activity
and head/body pose due to crowdedness and presence of extreme occlusions. To
this end, we propose SALSA, a novel dataset facilitating multimodal and
Synergetic sociAL Scene Analysis, and make two main contributions to research
on automated social interaction analysis: (1) SALSA records social interactions
among 18 participants in a natural, indoor environment for over 60 minutes,
under the poster presentation and cocktail party contexts presenting
difficulties in the form of low-resolution images, lighting variations,
numerous occlusions, reverberations and interfering sound sources; (2) To
alleviate these problems we facilitate multimodal analysis by recording the
social interplay using four static surveillance cameras and sociometric badges
worn by each participant, comprising the microphone, accelerometer, bluetooth
and infrared sensors. In addition to raw data, we also provide annotations
concerning individuals' personality as well as their position, head, body
orientation and F-formation information over the entire event duration. Through
extensive experiments with state-of-the-art approaches, we show (a) the
limitations of current methods and (b) how the recorded multiple cues
synergetically aid automatic analysis of social interactions. SALSA is
available at http://tev.fbk.eu/salsa.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06899</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06899</id><created>2015-06-23</created><authors><author><keyname>Marani</keyname><forenames>Roberto</forenames></author><author><keyname>Gelao</keyname><forenames>Gennaro</forenames></author><author><keyname>Perri</keyname><forenames>Anna Gina</forenames></author></authors><title>A review on memristor applications</title><categories>cond-mat.mtrl-sci cs.ET</categories><comments>12 pages, 17 figures in International Journal of Advances in
  Engineering and Technology (IJAET), june 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a review on the main applications of the fourth
fundamental circuit element, named &quot;memristor&quot;, which had been proposed for the
first time by Leon Chua and has recently been developed by a team at HP
Laboratories led by Stanley Williams. In particular, after a brief analysis of
memristor theory with a description of the first memristor, manufactured at HP
Laboratories, we present its main applications in the circuit design and
computer technology, together with future developments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06904</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06904</id><created>2015-06-23</created><authors><author><keyname>Jon</keyname><forenames>Kim Song</forenames></author><author><keyname>Gum</keyname><forenames>An Hae</forenames></author></authors><title>New Approach to translation of Isolated Units in English-Korean Machine
  Translation</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  It is the most effective way for quick translation of tremendous amount of
explosively increasing science and technique information material to develop a
practicable machine translation system and introduce it into translation
practice. This essay treats problems arising from translation of isolated units
on the basis of the practical materials and experiments obtained in the
development and introduction of English-Korean machine translation system. In
other words, this essay considers establishment of information for isolated
units and their Korean equivalents and word order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06905</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06905</id><created>2015-06-23</created><authors><author><keyname>Wan</keyname><forenames>Jiuqing</forenames></author><author><keyname>Xing</keyname><forenames>Menglin</forenames></author></authors><title>Person re-identification via efficient inference in fully connected CRF</title><categories>cs.CV</categories><comments>7 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of person re-identification problem,
i.e., retrieving instances from gallery which are generated by the same person
as the given probe image. This is very challenging because the person's
appearance usually undergoes significant variations due to changes in
illumination, camera angle and view, background clutter, and occlusion over the
camera network. In this paper, we assume that the matched gallery images should
not only be similar to the probe, but also be similar to each other, under
suitable metric. We express this assumption with a fully connected CRF model in
which each node corresponds to a gallery and every pair of nodes are connected
by an edge. A label variable is associated with each node to indicate whether
the corresponding image is from target person. We define unary potential for
each node using existing feature calculation and matching techniques, which
reflect the similarity between probe and gallery image, and define pairwise
potential for each edge in terms of a weighed combination of Gaussian kernels,
which encode appearance similarity between pair of gallery images. The specific
form of pairwise potential allows us to exploit an efficient inference
algorithm to calculate the marginal distribution of each label variable for
this dense connected CRF. We show the superiority of our method by applying it
to public datasets and comparing with the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06918</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06918</id><created>2015-06-23</created><authors><author><keyname>Kloos</keyname><forenames>Tobias</forenames></author><author><keyname>St&#xf6;ckler</keyname><forenames>Joachim</forenames></author><author><keyname>Gr&#xf6;chenig</keyname><forenames>Karlheinz</forenames></author></authors><title>Implementation of discretized Gabor frames and their duals</title><categories>math.NA cs.IT math.IT</categories><comments>16 pages, 4 figures</comments><msc-class>42C15, 42C40, 65D07, 65T99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The usefulness of Gabor frames depends on the easy computability of a
suitable dual window. This question is addressed under several aspects: several
versions of Schulz's iterative algorithm for the approximation of the canonical
dual window are analyzed for their numerical stability. For Gabor frames with
totally positive windows or with exponential B-splines a direct algorithm
yields a family of exact dual windows with compact support. It is shown that
these dual windows converge exponentially fast to the canonical dual window.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06924</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06924</id><created>2015-06-23</created><authors><author><keyname>Schweitzer</keyname><forenames>Frank</forenames></author><author><keyname>Nanumyan</keyname><forenames>Vahan</forenames></author><author><keyname>Tessone</keyname><forenames>Claudio J.</forenames></author><author><keyname>Xia</keyname><forenames>Xi</forenames></author></authors><title>How do OSS projects change in number and size? A large-scale analysis to
  test a model of project growth</title><categories>cs.SE cs.SI nlin.AO physics.soc-ph</categories><comments>22 pages, 10 figures</comments><journal-ref>Advs. Complex Syst. 17, 1550008 (2014)</journal-ref><doi>10.1142/S0219525915500083</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Established Open Source Software (OSS) projects can grow in size if new
developers join, but also the number of OSS projects can grow if developers
choose to found new projects. We discuss to what extent an established model
for firm growth can be applied to the dynamics of OSS projects. Our analysis is
based on a large-scale data set from SourceForge (SF) consisting of monthly
data for 10 years, for up to 360'000 OSS projects and up to 340'000 developers.
Over this time period, we find an exponential growth both in the number of
projects and developers, with a remarkable increase of single-developer
projects after 2009. We analyze the monthly entry and exit rates for both
projects and developers, the growth rate of established projects and the
monthly project size distribution. To derive a prediction for the latter, we
use modeling assumptions of how newly entering developers choose to either
found a new project or to join existing ones. Our model applies only to
collaborative projects that are deemed to grow in size by attracting new
developers. We verify, by a thorough statistical analysis, that the Yule-Simon
distribution is a valid candidate for the size distribution of collaborative
projects except for certain time periods where the modeling assumptions no
longer hold. We detect and empirically test the reason for this limitation,
i.e., the fact that an increasing number of established developers found
additional new projects after 2009.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06933</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06933</id><created>2015-06-23</created><authors><author><keyname>Studer</keyname><forenames>Thomas</forenames></author></authors><title>Justification logic enjoys the strong finite model property</title><categories>cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We observe that justification logic enjoys a form the strong finite model
property (sometimes also called small model property). Thus we obtain
decidability proofs for justification logic that do not rely on Post's theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06962</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06962</id><created>2015-06-23</created><authors><author><keyname>Latouche</keyname><forenames>Pierre</forenames><affiliation>SAMM</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>Graphs in machine learning: an introduction</title><categories>stat.ML cs.LG cs.SI physics.soc-ph</categories><proxy>ccsd</proxy><journal-ref>European Symposium on Artificial Neural Networks, Computational
  Intelligence and Machine Learning (ESANN), Apr 2015, Bruges, Belgium.
  pp.207-218, 2015, Proceedings of the 23-th European Symposium on Artificial
  Neural Networks, Computational Intelligence and Machine Learning (ESANN 2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs are commonly used to characterise interactions between objects of
interest. Because they are based on a straightforward formalism, they are used
in many scientific fields from computer science to historical sciences. In this
paper, we give an introduction to some methods relying on graphs for learning.
This includes both unsupervised and supervised methods. Unsupervised learning
algorithms usually aim at visualising graphs in latent spaces and/or clustering
the nodes. Both focus on extracting knowledge from graph topologies. While most
existing techniques are only applicable to static graphs, where edges do not
evolve through time, recent developments have shown that they could be extended
to deal with evolving networks. In a supervised context, one generally aims at
inferring labels or numerical values attached to nodes using both the graph
and, when they are available, node characteristics. Balancing the two sources
of information can be challenging, especially as they can disagree locally or
globally. In both contexts, supervised and un-supervised, data can be
relational (augmented with one or several global graphs) as described above, or
graph valued. In this latter case, each object of interest is given as a full
graph (possibly completed by other characteristics). In this context, natural
tasks include graph clustering (as in producing clusters of graphs rather than
clusters of nodes in a single graph), graph classification, etc. 1 Real
networks One of the first practical studies on graphs can be dated back to the
original work of Moreno [51] in the 30s. Since then, there has been a growing
interest in graph analysis associated with strong developments in the modelling
and the processing of these data. Graphs are now used in many scientific
fields. In Biology [54, 2, 7], for instance, metabolic networks can describe
pathways of biochemical reactions [41], while in social sciences networks are
used to represent relation ties between actors [66, 56, 36, 34]. Other examples
include powergrids [71] and the web [75]. Recently, networks have also been
considered in other areas such as geography [22] and history [59, 39]. In
machine learning, networks are seen as powerful tools to model problems in
order to extract information from data and for prediction purposes. This is the
object of this paper. For more complete surveys, we refer to [28, 62, 49, 45].
In this section, we introduce notations and highlight properties shared by most
real networks. In Section 2, we then consider methods aiming at extracting
information from a unique network. We will particularly focus on clustering
methods where the goal is to find clusters of vertices. Finally, in Section 3,
techniques that take a series of networks into account, where each network is
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06968</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06968</id><created>2015-06-23</created><authors><author><keyname>Rodrigues</keyname><forenames>Jose</forenames></author><author><keyname>Balan</keyname><forenames>Andre</forenames></author><author><keyname>Zaina</keyname><forenames>Luciana</forenames></author><author><keyname>Traina</keyname><forenames>Agma</forenames></author></authors><title>A Survey on Distributed Visualization Techniques over Clusters of
  Personal Computers</title><categories>cs.GR</categories><journal-ref>INFOCOMP Journal of Computer Science, 2009, ISSN: 1807-4545, vol
  8: 4. 79-90</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last years, Distributed Visualization over Personal Computer (PC)
clusters has become important for research and industrial communities. They
have made large-scale visualizations practical and more accessible. In this
work we survey Distributed Visualization techniques aiming at compiling last
decade's literature on the use of PC clusters as suitable alternatives to
high-end workstations. We review the topic by defining basic concepts,
enumerating system requirements and implementation challenges, and presenting
up-to-date methodologies. Our work fulfills the needs of newcomers and seasoned
professionals as an introductory compilation at the same time that it can help
experienced personnel by organizing ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06972</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06972</id><created>2015-06-23</created><authors><author><keyname>Barta</keyname><forenames>Gergo</forenames></author><author><keyname>Borbely</keyname><forenames>Gyula</forenames></author><author><keyname>Nagy</keyname><forenames>Gabor</forenames></author><author><keyname>Kazi</keyname><forenames>Sandor</forenames></author><author><keyname>Henk</keyname><forenames>Tamas</forenames></author></authors><title>GEFCOM 2014 - Probabilistic Electricity Price Forecasting</title><categories>stat.ML cs.CE cs.LG stat.AP</categories><comments>10 pages, 5 figures, KES-IDT 2015 conference. The final publication
  is available at Springer via http://dx.doi.org/10.1007/978-3-319-19857-6_7</comments><doi>10.1007/978-3-319-19857-6_7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy price forecasting is a relevant yet hard task in the field of
multi-step time series forecasting. In this paper we compare a well-known and
established method, ARMA with exogenous variables with a relatively new
technique Gradient Boosting Regression. The method was tested on data from
Global Energy Forecasting Competition 2014 with a year long rolling window
forecast. The results from the experiment reveal that a multi-model approach is
significantly better performing in terms of error metrics. Gradient Boosting
can deal with seasonality and auto-correlation out-of-the box and achieve lower
rate of normalized mean absolute error on real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06980</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06980</id><created>2015-06-23</created><updated>2015-11-22</updated><authors><author><keyname>Hardt</keyname><forenames>Moritz</forenames></author><author><keyname>Megiddo</keyname><forenames>Nimrod</forenames></author><author><keyname>Papadimitriou</keyname><forenames>Christos</forenames></author><author><keyname>Wootters</keyname><forenames>Mary</forenames></author></authors><title>Strategic Classification</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning relies on the assumption that unseen test instances of a
classification problem follow the same distribution as observed training data.
However, this principle can break down when machine learning is used to make
important decisions about the welfare (employment, education, health) of
strategic individuals. Knowing information about the classifier, such
individuals may manipulate their attributes in order to obtain a better
classification outcome. As a result of this behavior---often referred to as
gaming---the performance of the classifier may deteriorate sharply. Indeed,
gaming is a well-known obstacle for using machine learning methods in practice;
in financial policy-making, the problem is widely known as Goodhart's law. In
this paper, we formalize the problem, and pursue algorithms for learning
classifiers that are robust to gaming.
  We model classification as a sequential game between a player named &quot;Jury&quot;
and a player named &quot;Contestant.&quot; Jury designs a classifier, and Contestant
receives an input to the classifier, which he may change at some cost. Jury's
goal is to achieve high classification accuracy with respect to Contestant's
original input and some underlying target classification function. Contestant's
goal is to achieve a favorable classification outcome while taking into account
the cost of achieving it.
  For a natural class of cost functions, we obtain computationally efficient
learning algorithms which are near-optimal. Surprisingly, our algorithms are
efficient even on concept classes that are computationally hard to learn. For
general cost functions, designing an approximately optimal strategy-proof
classifier, for inverse-polynomial approximation, is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06981</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06981</id><created>2015-06-23</created><authors><author><keyname>Lenc</keyname><forenames>Karel</forenames></author><author><keyname>Vedaldi</keyname><forenames>Andrea</forenames></author></authors><title>R-CNN minus R</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep convolutional neural networks (CNNs) have had a major impact in most
areas of image understanding, including object category detection. In object
detection, methods such as R-CNN have obtained excellent results by integrating
CNNs with region proposal generation algorithms such as selective search. In
this paper, we investigate the role of proposal generation in CNN-based
detectors in order to determine whether it is a necessary modelling component,
carrying essential geometric information not contained in the CNN, or whether
it is merely a way of accelerating detection. We do so by designing and
evaluating a detector that uses a trivial region generation scheme, constant
for each image. Combined with SPP, this results in an excellent and fast
detector that does not require to process an image with algorithms other than
the CNN itself. We also streamline and simplify the training of CNN-based
detectors by integrating several learning steps in a single algorithm, as well
as by proposing a number of improvements that accelerate detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06983</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06983</id><created>2015-06-23</created><authors><author><keyname>Alatabbi</keyname><forenames>Ali</forenames></author><author><keyname>Daykin</keyname><forenames>Jacqueline W.</forenames></author><author><keyname>Rahman</keyname><forenames>M. Sohel</forenames></author></authors><title>Linear Algorithms for Computing the Lyndon Border Array and the Lyndon
  Suffix Array</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding repetitive structures and inherent
patterns in a given string $\s{s}$ of length $n$ over a finite totally ordered
alphabet. A border $\s{u}$ of a string $\s{s}$ is both a prefix and a suffix of
$\s{s}$ such that $\s{u} \not= \s{s}$. The computation of the border array of a
string $\s{s}$, namely the borders of each prefix of $\s{s}$, is strongly
related to the string matching problem: given a string $\s{w}$, find all of its
occurrences in $\s{s}$. A {\itshape Lyndon word} is a primitive word (i.e., it
is not a power of another word) which is minimal for the lexicographical order
of its conjugacy class (i.e., the set of words obtained by cyclic rotations of
the letters). In this paper we combine these concepts to introduce the
\emph{Lyndon Border Array} $\mathcal L \beta$ of $\s{s}$, whose $i$-th entry
$\mathcal L \beta(\s{s})[i]$ is the length of the longest border of $\s{s}[1
\dd i]$ which is also a Lyndon word. We propose linear-time and linear-space
algorithms for computing $\mathcal L \beta (\s{s})$. %in the case of both
binary and bounded alphabets. Further, we introduce the \emph{Lyndon Suffix
Array}, and by modifying the efficient suffix array technique of Ko and Aluru
\cite{KA03} outline a linear time and space algorithm for its construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06987</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06987</id><created>2015-06-23</created><authors><author><keyname>Hof</keyname><forenames>Hans-Joachim</forenames></author></authors><title>Towards Enhanced Usability of IT Security Mechanisms - How to Design
  Usable IT Security Mechanisms Using the Example of Email Encryption</title><categories>cs.CR cs.SE</categories><journal-ref>International Journal On Advances in Security, volume 6, number
  1&amp;2, pp. 78-87 ISSN 1942-2636, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, advanced security mechanisms exist to protect data, systems, and
networks. Most of these mechanisms are effective, and security experts can
handle them to achieve a sufficient level of security for any given system.
However, most of these systems have not been designed with focus on good
usability for the average end user. Today, the average end user often struggles
with understanding and using security mecha-nisms. Other security mechanisms
are simply annoying for end users. As the overall security of any system is
only as strong as the weakest link in this system, bad usability of IT security
mechanisms may result in operating errors, resulting in inse-cure systems.
Buying decisions of end users may be affected by the usability of security
mechanisms. Hence, software provid-ers may decide to better have no security
mechanism then one with a bad usability. Usability of IT security mechanisms is
one of the most underestimated properties of applications and sys-tems. Even IT
security itself is often only an afterthought. Hence, usability of security
mechanisms is often the after-thought of an afterthought. This paper presents
some guide-lines that should help software developers to improve end user
usability of security-related mechanisms, and analyzes com-mon applications
based on these guidelines. Based on these guidelines, the usability of email
encryption is analyzed and an email encryption solution with increased
usability is presented. The approach is based on an automated key and trust
man-agement. The compliance of the proposed email encryption solution with the
presented guidelines for usable security mechanisms is evaluated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06988</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06988</id><created>2015-06-23</created><authors><author><keyname>Adami</keyname><forenames>Christoph</forenames></author><author><keyname>LaBar</keyname><forenames>Thomas</forenames></author></authors><title>From Entropy to Information: Biased Typewriters and the Origin of Life</title><categories>q-bio.PE cs.IT math.IT nlin.AO q-bio.BM</categories><comments>19 pages, 8 figures, to appear in &quot;From Matter to Life: Information
  and Causality&quot;, S.I. Walker, P.C.W. Davies, and G. Ellis, eds, Cambridge
  University Press, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The origin of life can be understood mathematically to be the origin of
information that can replicate. The likelihood that entropy spontaneously
becomes information can be calculated from first principles, and depends
exponentially on the amount of information that is necessary for replication.
We do not know what the minimum amount of information for self-replication is
because it must depend on the local chemistry, but we can study how this
likelihood behaves in different known chemistries, and we can study ways in
which this likelihood can be enhanced. Here we present evidence from numerical
simulations (using the digital life chemistry &quot;Avida&quot;) that using a biased
probability distribution for the creation of monomers (the &quot;biased typewriter&quot;)
can exponentially increase the likelihood of spontaneous emergence of
information from entropy. We show that this likelihood may depend on the length
of the sequence that the information is embedded in, but in a non-trivial
manner: there may be an optimum sequence length that maximizes the likelihood.
We conclude that the likelihood of spontaneous emergence of self-replication is
much more malleable than previously thought, and that the biased probability
distributions of monomers that are the norm in biochemistry may significantly
enhance these likelihoods
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06990</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06990</id><created>2015-06-23</created><authors><author><keyname>Schmidtke</keyname><forenames>Alexander</forenames></author><author><keyname>Hof</keyname><forenames>Hans-Joachim</forenames></author></authors><title>Fighting Spam by Breaking the Econonmy of Advertisment by Unsolicited
  Emails</title><categories>cs.CR</categories><journal-ref>in SECURWARE 2013, The Seventh International Conference on
  Emerging Security Information, Systems and Technologies, pp. 96-101, ISBN
  978-61208-298-1, August 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsolicited email (spam) is still a problem for users of the email service.
Even though current email anti-spam solutions filter most spam emails, some
spam emails still are delivered to the inbox of users. A special class of spam
emails advertises websites, e.g., online dating sites or online pharmacies. The
success rate of this kind of advertisement is rather low, however, as sending
an email does only involve minimal costs, even a very low success rate results
in enough revenue such that this kind of advertisement pays off. The anti-spam
approach presented in this paper aims on increasing the costs for websites that
are advertised by spam emails and on lowering the revenues from spam. Costs can
be increased for a website by increasing traffic. Revenues can be decreased by
making the website slow responding, hence some business gets lost. To increase
costs and decreased revenues a decentralized peer-to-peer coordination
mechanism is used to have mail clients to agree on a start date and time for an
anti-spam campaign. During a campaign, all clients that received spam emails
advertings a website send an opt-out request to this website. A huge number of
opt-out requests results in increased traffic to this website and will likely
result in a slower responsibility of the website. The coordination mechanism
presented in this paper is based on a peer-to-peer mechanisms and a so-called
paranoid trust model to avoid manipulation by spammers. An implementation for
the Thunderbird email client exist. The anti-spam approach presented in this
paper breaks the economy of spam, hence makes advertisement by unsolicited
emails unattractive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.06996</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.06996</id><created>2015-06-23</created><authors><author><keyname>Falk</keyname><forenames>Rainer</forenames></author><author><keyname>Fries</keyname><forenames>Steffen</forenames></author><author><keyname>Hof</keyname><forenames>Hans-Joachim</forenames></author></authors><title>Secure Communication Using Electronic Identity Cards for Voice over IP
  Communication, Home Energy Management, and eMobility</title><categories>cs.CR</categories><journal-ref>International Journal On Advances in Security, Vol. 3, No. 3 &amp; 4,
  ISSN 1942-2636, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using communication services is a common part of everyday life in a personal
or business context. Communication services include Internet services like
voice services, chat service, and web 2.0 technologies (wikis, blogs, etc), but
other usage areas like home energy management and eMobility are will be
increasingly tackled. Such communication services typically authenticate
participants. For this identities of some kind are used to identify the
communication peer to the user of a service or to the service itself. Calling
line identification used in the Session Initiation Protocol (SIP) used for
Voice over IP (VoIP) is just one example. Authentication and identification of
eCar users for accounting during charging of the eCar is another example. Also,
further mechanisms rely on identities, e.g., whitelists defining allowed
communication peers. Trusted identities prevent identity spoofing, hence are a
basic building block for the protection of communication. However, providing
trusted identities in a practical way is still a difficult problem and too
often application specific identities are used, making identity handling a
hassle. Nowadays, many countries introduced electronic identity cards, e.g.,
the German &quot;Elektronischer Personalausweis&quot; (ePA). As many German citizens will
possess an ePA soon, it can be used as security token to provide trusted
identities. Especially new usage areas (like eMobility) should from the start
be based on the ubiquitous availability of trusted identities. This paper
describes how identity cards can be integrated within three domains: home
energy management, vehicle-2-grid communication, and SIP-based voice over IP
telephony. In all three domains, identity cards are used to reliably identify
users and authenticate participants. As an example for an electronic identity
card, this paper focuses on the German ePA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07000</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07000</id><created>2015-06-23</created><authors><author><keyname>Herbreteau</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>LaBRI</affiliation></author><author><keyname>Tran</keyname><forenames>Thanh-Tung</forenames><affiliation>LaBRI</affiliation></author></authors><title>Improving search order for reachability testing in timed automata</title><categories>cs.LO cs.FL</categories><proxy>ccsd</proxy><journal-ref>International Conference on Formal Modeling and Analysis of Timed
  Systems (FORMATS), Sep 2015, Madrid, Spain</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard algorithms for reachability analysis of timed automata are sensitive
to the order in which the transitions of the automata are taken. To tackle this
problem, we propose a ranking system and a waiting strategy. This paper
discusses the reason why the search order matters and shows how a ranking
system and a waiting strategy can be integrated into the standard reachability
algorithm to alleviate and prevent the problem respectively. Experiments show
that the combination of the two approaches gives optimal search order on
standard benchmarks except for one example. This suggests that it should be
used instead of the standard BFS algorithm for reachability analysis of timed
automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07002</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07002</id><created>2015-06-23</created><authors><author><keyname>Lancien</keyname><forenames>C&#xe9;cilia</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>Parallel repetition and concentration for (sub-)no-signalling games via
  a flexible constrained de Finetti reduction</title><categories>quant-ph cs.CC</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use a recently discovered constrained de Finetti reduction (aka
&quot;Post-Selection Lemma&quot;) to study the parallel repetition of multi-player
non-local games under no-signalling strategies. Since the technique allows us
to reduce general strategies to independent plays, we obtain parallel
repetition (corresponding to winning all rounds) in the same way as exponential
concentration of the probability to win a fraction larger than the value of the
game.
  Our proof technique leads us naturally to a relaxation of no-signalling (NS)
strategies, which we dub sub-no-signalling (SNOS). While for two players the
two concepts coincide, they differ for three or more players. Our results are
most complete and satisfying for arbitrary number of sub-no-signalling players,
where we get universal parallel repetition and concentration for any game,
while the no-signalling case is obtained as a corollary, but only for games
with &quot;full support&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07020</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07020</id><created>2015-06-23</created><authors><author><keyname>Mishra</keyname><forenames>Mayank</forenames></author><author><keyname>Bellur</keyname><forenames>Umesh</forenames></author></authors><title>De-Fragmenting the Cloud</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing VM placement schemes have measured their effectiveness solely by
looking either Physical Machine's resources(CPU, memory) or network resource.
However, real applications use all resource types to varying degrees. The
result of applying existing placement schemes to VMs running real applications
is a fragmented data center where resources along one dimension become unusable
even though they are available because of the unavailability of resources along
other dimensions. An example of this fragmentation is unusable CPU because of a
bottlenecked network link from the physical machine which has available CPU. To
date, evaluations of the efficacy of VM placement schemes has not recognized
this fragmentation and it's ill effects, let alone try to measure it and avoid
it. In this paper, we first define the notion of what we term &quot;relative
resource fragmentation&quot; and illustrate how it can be measured in a data center.
The metric we put forth for capturing the degree of fragmentation is
comprehensive and includes all key data center resource types. We then propose
a scheme of minimizing this fragmentation so as to maximize the availability of
existing set of data center resources. Results of empirical evaluations of our
placement scheme compared to existing network based placement schemes show a
reduction of fragmentation by as much as 15% and increase in number of
successfully placed applications by upto 20%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07031</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07031</id><created>2015-06-23</created><authors><author><keyname>Droste</keyname><forenames>Manfred</forenames></author><author><keyname>D&#xfc;ck</keyname><forenames>Stefan</forenames></author></authors><title>Weighted Automata and Logics for Infinite Nested Words</title><categories>cs.FL cs.LO</categories><comments>LATA 2014, 12 pages</comments><acm-class>F.1.1; F.4.1; F.4.3</acm-class><journal-ref>Proc. of Language and Automata Theory and Applications (LATA
  2014), LNCS 8370, pp. 323-334. Springer (2014)</journal-ref><doi>10.1007/978-3-319-04921-2_26</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nested words introduced by Alur and Madhusudan are used to capture structures
with both linear and hierarchical order, e.g. XML documents, without losing
valuable closure properties. Furthermore, Alur and Madhusudan introduced
automata and equivalent logics for both finite and infinite nested words, thus
extending B\&quot;uchi's theorem to nested words. Recently, average and discounted
computations of weights in quantitative systems found much interest. Here, we
will introduce and investigate weighted automata models and weighted MSO logics
for infinite nested words. As weight structures we consider valuation monoids
which incorporate average and discounted computations of weights as well as the
classical semirings. We show that under suitable assumptions, two resp. three
fragments of our weighted logics can be transformed into each other. Moreover,
we show that the logic fragments have the same expressive power as weighted
nested word automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07032</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07032</id><created>2015-06-23</created><updated>2016-02-16</updated><authors><author><keyname>Takaguchi</keyname><forenames>Taro</forenames></author><author><keyname>Yano</keyname><forenames>Yosuke</forenames></author><author><keyname>Yoshida</keyname><forenames>Yuichi</forenames></author></authors><title>Coverage centralities for temporal networks</title><categories>physics.soc-ph cs.SI</categories><comments>13 pages, 10 figures</comments><journal-ref>European Physical Journal B, 89, 35 (2016)</journal-ref><doi>10.1140/epjb/e2016-60498-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structure of real networked systems, such as social relationship, can be
modeled as temporal networks in which each edge appears only at the prescribed
time. Understanding the structure of temporal networks requires quantifying the
importance of a temporal vertex, which is a pair of vertex index and time. In
this paper, we define two centrality measures of a temporal vertex based on the
fastest temporal paths which use the temporal vertex. The definition is free
from parameters and robust against the change in time scale on which we focus.
In addition, we can efficiently compute these centrality values for all
temporal vertices. Using the two centrality measures, we reveal that
distributions of these centrality values of real-world temporal networks are
heterogeneous. For various datasets, we also demonstrate that a majority of the
highly central temporal vertices are located within a narrow time window around
a particular time. In other words, there is a bottleneck time at which most
information sent in the temporal network passes through a small number of
temporal vertices, which suggests an important role of these temporal vertices
in spreading phenomena.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07044</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07044</id><created>2015-06-23</created><updated>2015-07-21</updated><authors><author><keyname>Molkaraie</keyname><forenames>Mehdi</forenames></author><author><keyname>Gomez</keyname><forenames>Vicenc</forenames></author></authors><title>Efficient Monte Carlo Methods for the Potts Model at Low Temperature</title><categories>stat.CO cs.IT math.IT physics.comp-ph</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating the partition function of the
ferromagnetic $q$-state Potts model. We propose an importance sampling
algorithm in the dual of the normal factor graph representing the model. The
algorithm can efficiently compute an estimate of the partition function when
the coupling parameters of the model are strong (corresponding to models at low
temperature) or when the model contains a mixture of strong and weak couplings.
We show that, in this setting, the proposed algorithm significantly outperforms
the state of the art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07046</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07046</id><created>2015-06-23</created><updated>2015-07-08</updated><authors><author><keyname>Bronfman</keyname><forenames>Slava</forenames></author></authors><title>Two-Body Assignment Problem in the Context of the Israeli Medical
  Internship Match</title><categories>cs.GT</categories><comments>MSc thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis was submitted as partial fulfillment of the requirements for the
Masters Degree in the Department of Computer Science, Bar-Ilan University.
  The final step in getting an Israeli M.D. is performing a year-long
internship in one of the hospitals in Israel. Internships are decided upon by a
lottery, which is known as &quot;The Internship Lottery&quot;. In 2014 we redesigned the
lottery, replacing it with a more efficient one. The new method is based on
calculating a tentative lottery, in which each student has some probability of
getting to each hospital. Then a computer program &quot;trades&quot; between the
students, where trade is performed only if it is beneficial to both sides. This
trade creates surplus, which translates to more students getting one of their
top choices. The average student improved his place by $0.91$ seats. The new
method can improve the welfare of medical graduates, by giving them more
probability to get to one of their top choices. It can be applied in internship
markets in other countries as well.
  This thesis presents the market, the redesign process and the new mechanism
which is now in use. There are two main lessons that we have learned from this
market. The first is the &quot;Do No Harm&quot; principle, which states that (almost) all
participants should prefer the new mechanism to the old one. The second is that
new approaches need to be used when dealing with two-body problems in object
assignment. We focus on the second lesson, and study two-body problems in the
context of the assignment problem. We show that decomposing stochastic
assignment matrices to deterministic allocations is NP-hard in the presence of
couples, and present a polynomial time algorithm with the optimal worst case
guarantee. We also study the performance of our algorithm on real-world and on
simulated data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07054</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07054</id><created>2015-06-23</created><authors><author><keyname>Kim</keyname><forenames>Kee-Hoon</forenames></author><author><keyname>No</keyname><forenames>Jong-Seon</forenames></author><author><keyname>Shin</keyname><forenames>Dong-Joon</forenames></author></authors><title>On the Properties of Cubic Metric for OFDM Signals</title><categories>cs.IT math.IT</categories><msc-class>94A12</msc-class><doi>10.1109/LSP.2015.2502261</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a metric for amplitude fluctuation of orthogonal frequency division
multiplexing (OFDM) signal, cubic metric (CM) has received an increasing
attention because it is more closely related to the distortion induced by
nonlinear devices than the well-known peak-to-average power ratio (PAPR). In
this paper, the properties of CM of OFDM signal is investigated. First,
asymptotic distribution of CM is derived. Second, it is verified that 1.7 times
oversampling rate is good enough to capture the CM of continuous OFDM signals
in terms of mean square error, which is also practically meaningful because the
fast Fourier transform size is typically 1.7 times larger than the nominal
bandwidth in the long-term evolution (LTE) of cellular communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07055</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07055</id><created>2015-06-23</created><authors><author><keyname>Pohl</keyname><forenames>Christoph</forenames></author><author><keyname>Hof</keyname><forenames>Hans-Joachim</forenames></author></authors><title>The All-Seeing Eye: A Massive-Multi-Sensor Zero-Configuration Intrusion
  Detection System for Web Applications</title><categories>cs.CR</categories><comments>SECURWARE 2013 : The Seventh International Conference on Emerging
  Security Information, Systems and Technologies</comments><journal-ref>SECURWARE 2013 : The Seventh International Conference on Emerging
  Security Information, Systems and Technologies,2013,66-71</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Timing attacks are a challenge for current intrusion detection solutions.
Timing attacks are dangerous for web applications because they may leak
information about side channel vulnerabilities. This paper presents a
massive-multi-sensor zero-configuration Intrusion Detection System that is
especially good at detecting timing attacks. Unlike current solutions, the
proposed Intrusion Detection System uses a huge number of sensors for attack
detection. These sensors include sensors automatically inserted into web
application or into the frameworks used to build web applications. With this
approach the Intrusion Detection System is able to detect sophisticated attacks
like timing attacks or other brute-force attacks with increased accuracy. The
proposed massive-multi-sensor zero-configuration intrusion detection system
does not need specific knowledge about the system to protect, hence it offers
zero-configuration capability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07059</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07059</id><created>2015-06-23</created><authors><author><keyname>Kim</keyname><forenames>Kee-Hoon</forenames></author></authors><title>On the Shift Value Set of Cyclic Shifted Sequences for PAPR Reduction in
  OFDM Systems</title><categories>cs.IT math.IT</categories><msc-class>94A99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orthogonal frequency division multiplexing (OFDM) signals have high
peak-to-average power ratio (PAPR), which causes distortion when OFDM signal
passes through a nonlinear high power amplifier (HPA). A partial transmit
sequence (PTS) scheme is one of the typical PAPR reduction methods. A cyclic
shifted sequences (CSS) scheme is evolved from the PTS scheme to improve the
PAPR reduction performance, where OFDM signal subsequences are cyclically
shifted and combined to generate alternative OFDM signal sequences. The shift
value (SV) sets in the CSS scheme should be carefully selected because those
are closely related to the PAPR reduction performance of the CSS scheme. In
this letter, we propose some criteria to select the good SV sets and verify its
validness through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07062</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07062</id><created>2015-06-23</created><authors><author><keyname>Portegies</keyname><forenames>J. M.</forenames></author><author><keyname>Fick</keyname><forenames>R. H. J.</forenames></author><author><keyname>Sanguinetti</keyname><forenames>G. R.</forenames></author><author><keyname>Meesters</keyname><forenames>S. P. L.</forenames></author><author><keyname>Girard</keyname><forenames>G.</forenames></author><author><keyname>Duits</keyname><forenames>R.</forenames></author></authors><title>Improving Fiber Alignment in HARDI by Combining Contextual PDE Flow with
  Constrained Spherical Deconvolution</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two strategies to improve the quality of tractography results
computed from diffusion weighted magnetic resonance imaging (DW-MRI) data. Both
methods are based on the same PDE framework, defined in the coupled space of
positions and orientations, associated with a stochastic process describing the
enhancement of elongated structures while preserving crossing structures. In
the first method we use the enhancement PDE for contextual regularization of a
fiber orientation distribution (FOD) that is obtained on individual voxels from
high angular resolution diffusion imaging (HARDI) data via constrained
spherical deconvolution (CSD). Thereby we improve the FOD as input for
subsequent tractography. Secondly, we introduce the fiber to bundle coherence
(FBC), a measure for quantification of fiber alignment. The FBC is computed
from a tractography result using the same PDE framework and provides a
criterion for removing the spurious fibers. We validate the proposed
combination of CSD and enhancement on phantom data and on human data, acquired
with different scanning protocols. On the phantom data we find that PDE
enhancements improve both local metrics and global metrics of tractography
results, compared to CSD without enhancements. On the human data we show that
the enhancements allow for a better reconstruction of crossing fiber bundles
and they reduce the variability of the tractography output with respect to the
acquisition parameters. Finally, we show that both the enhancement of the FODs
and the use of the FBC measure on the tractography improve the stability with
respect to different stochastic realizations of probabilistic tractography.
This is shown in a clinical application: the reconstruction of the optic
radiation for epilepsy surgery planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07076</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07076</id><created>2015-06-23</created><updated>2015-08-16</updated><authors><author><keyname>Bernstein</keyname><forenames>Aaron</forenames></author><author><keyname>Stein</keyname><forenames>Cliff</forenames></author></authors><title>Fully Dynamic Matching in Bipartite Graphs</title><categories>cs.DS</categories><comments>Longer version of paper that appears in ICALP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximum cardinality matching in bipartite graphs is an important and
well-studied problem. The fully dynamic version, in which edges are inserted
and deleted over time has also been the subject of much attention. Existing
algorithms for dynamic matching (in general graphs) seem to fall into two
groups: there are fast (mostly randomized) algorithms that do not achieve a
better than 2-approximation, and there slow algorithms with $\O(\sqrt{m})$
update time that achieve a better-than-2 approximation. Thus the obvious
question is whether we can design an algorithm -- deterministic or randomized
-- that achieves a tradeoff between these two: a $o(\sqrt{m})$ approximation
and a better-than-2 approximation simultaneously. We answer this question in
the affirmative for bipartite graphs.
  Our main result is a fully dynamic algorithm that maintains a $3/2 + \eps$
approximation in worst-case update time $O(m^{1/4}\eps^{/2.5})$. We also give
stronger results for graphs whose arboricity is at most $\al$, achieving a $(1+
\eps)$ approximation in worst-case time $O(\al (\al + \log n))$ for constant
$\eps$. When the arboricity is constant, this bound is $O(\log n)$ and when the
arboricity is polylogarithmic the update time is also polylogarithmic.
  The most important technical developement is the use of an intermediate graph
we call an edge degree constrained subgraph (EDCS). This graph places
constraints on the sum of the degrees of the endpoints of each edge: upper
bounds for matched edges and lower bounds for unmatched edges. The main
technical content of our paper involves showing both how to maintain an EDCS
dynamically and that and EDCS always contains a sufficiently large matching. We
also make use of graph orientations to help bound the amount of work done
during each update.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07077</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07077</id><created>2015-06-23</created><updated>2015-08-31</updated><authors><author><keyname>Cascone</keyname><forenames>Carmelo</forenames></author><author><keyname>Pollini</keyname><forenames>Luca</forenames></author><author><keyname>Sanvito</keyname><forenames>Davide</forenames></author><author><keyname>Capone</keyname><forenames>Antonio</forenames></author></authors><title>Traffic Management Applications for Stateful SDN Data Plane</title><categories>cs.NI</categories><comments>6 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The successful OpenFlow approach to Software Defined Networking (SDN) allows
network programmability through a central controller able to orchestrate a set
of dumb switches. However, the simple match/action abstraction of OpenFlow
switches constrains the evolution of the forwarding rules to be fully managed
by the controller. This can be particularly limiting for a number of
applications that are affected by the delay of the slow control path, like
traffic management applications. Some recent proposals are pushing toward an
evolution of the OpenFlow abstraction to enable the evolution of forwarding
policies directly in the data plane based on state machines and local events.
In this paper, we present two traffic management applications that exploit a
stateful data plane and their prototype implementation based on OpenState, an
OpenFlow evolution that we recently proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07080</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07080</id><created>2015-06-23</created><authors><author><keyname>Man&#x10d;inska</keyname><forenames>Laura</forenames></author></authors><title>Maximally entangled states in pseudo-telepathy games</title><categories>quant-ph cs.CC math-ph math.MP</categories><comments>In addition to the published material this version contains results
  on self testing</comments><journal-ref>Computing with New Resources, vol. 8808 of LNCS, pages 200--207,
  2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A pseudo-telepathy game is a nonlocal game which can be won with probability
one using some finite-dimensional quantum strategy but not using a classical
one. Our central question is whether there exist two-party pseudo-telepathy
games which cannot be won with probability one using a maximally entangled
state. Towards answering this question, we develop conditions under which
maximally entangled states suffice. In particular, we show that maximally
entangled states suffice for weak projection games which we introduce as a
relaxation of projection games. Our results also imply that any
pseudo-telepathy weak projection game yields a device-independent certification
of a maximally entangled state. In particular, by establishing connections to
the setting of communication complexity, we exhibit a class of games $G_n$ for
testing maximally entangled states of local dimension $\Omega(n)$. We leave the
robustness of these self-tests as an open question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07087</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07087</id><created>2015-06-23</created><authors><author><keyname>Tirumalasetty</keyname><forenames>Sudhir</forenames></author><author><keyname>Jadda</keyname><forenames>Aruna</forenames></author><author><keyname>Edara</keyname><forenames>Sreenivasa Reddy</forenames></author></authors><title>An Enhanced Apriori Algorithm for Discovering Frequent Patterns with
  Optimal Number of Scans</title><categories>cs.DB</categories><comments>in International Journal of Computer Science Issues 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining is wide spreading its applications in several areas. There are
different tasks in mining which provides solutions for wide variety of problems
in order to discover knowledge. Among those tasks association mining plays a
pivotal role for identifying frequent patterns. Among the available association
mining algorithms Apriori algorithm is one of the most prevalent and dominant
algorithm which is used to discover frequent patterns. This algorithm is used
to discover frequent patterns from small to large databases. This paper points
toward the inadequacy of the tangible Apriori algorithm of wasting time for
scanning the whole transactional database for discovering association rules and
proposes an enhancement on Apriori algorithm to overcome this problem. This
enhancement is obtained by dropping the amount of time used in scanning the
transactional database by just limiting the number of transactions while
calculating the frequency of an item or item-pairs. This improved version of
Apriori algorithm optimizes the time used for scanning the whole transactional
database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07094</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07094</id><created>2015-06-23</created><updated>2016-03-07</updated><authors><author><keyname>Milk</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Rave</keyname><forenames>Stephan</forenames></author><author><keyname>Schindler</keyname><forenames>Felix</forenames></author></authors><title>pyMOR - Generic Algorithms and Interfaces for Model Order Reduction</title><categories>cs.MS</categories><msc-class>35-04, 35J20, 35L03, 65-04, 65N30, 65Y05, 68N01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reduced basis methods are projection-based model order reduction techniques
for reducing the computational complexity of solving parametrized partial
differential equation problems. In this work we discuss the design of pyMOR, a
freely available software library of model order reduction algorithms, in
particular reduced basis methods, implemented with the Python programming
language. As its main design feature, all reduction algorithms in pyMOR are
implemented generically via operations on well-defined vector array, operator
and discretization interface classes. This allows for an easy integration with
existing third-party high-performance partial differential equation solvers
without adding any model reduction specific code to these solvers. Besides an
in-depth discussion of pyMOR's design philosophy and architecture, we present
several benchmark results and numerical examples showing the feasibility of our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07097</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07097</id><created>2015-06-22</created><authors><author><keyname>Gajduk</keyname><forenames>Andrej</forenames></author><author><keyname>Zdraveski</keyname><forenames>Vladimir</forenames></author><author><keyname>Basnarkov</keyname><forenames>Lasko</forenames></author><author><keyname>Todorovski</keyname><forenames>Mirko</forenames></author><author><keyname>Kocarev</keyname><forenames>Ljupco</forenames></author></authors><title>A Strategy for Power System Stability Improvement via Controlled
  Charge/Discharge of Plug-in Electric Vehicles</title><categories>cs.SY</categories><comments>18 pages, 7 figures, subbmited for review in Elsevier's International
  Journal of Electrical Power &amp; Energy Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Plug-in electrical vehicles (PEV) are capable of both grid-to-vehicle (G2V)
and vehicle-to-grid (V2G) power transfer. The advantages of developing V2G
include an additional revenue stream for cleaner vehicles, increased stability
and reliability of the electric grid, lower electric system costs, and
eventually, inexpensive storage and backup for renewable electricity. Here we
show how smart control of PEVs can improve the stability of power grids using
only local frequency measurements. We evaluate the proposed control strategy on
the IEEE Case 3 and the IEEE New England power systems. The results show that
V2G leads to improved steady-state stability, larger region of stability,
reduced frequency and voltage fluctuations during transients and longer
critical clearing times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07116</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07116</id><created>2015-06-23</created><authors><author><keyname>Trugenberger</keyname><forenames>Carlo A.</forenames></author></authors><title>Scientific Discovery by Machine Intelligence: A New Avenue for Drug
  Research</title><categories>cs.AI cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The majority of big data is unstructured and of this majority the largest
chunk is text. While data mining techniques are well developed and standardized
for structured, numerical data, the realm of unstructured data is still largely
unexplored. The general focus lies on information extraction, which attempts to
retrieve known information from text. The Holy Grail, however is knowledge
discovery, where machines are expected to unearth entirely new facts and
relations that were not previously known by any human expert. Indeed,
understanding the meaning of text is often considered as one of the main
characteristics of human intelligence. The ultimate goal of semantic artificial
intelligence is to devise software that can understand the meaning of free
text, at least in the practical sense of providing new, actionable information
condensed out of a body of documents. As a stepping stone on the road to this
vision I will introduce a totally new approach to drug research, namely that of
identifying relevant information by employing a self-organizing semantic engine
to text mine large repositories of biomedical research papers, a technique
pioneered by Merck with the InfoCodex software. I will describe the methodology
and a first successful experiment for the discovery of new biomarkers and
phenotypes for diabetes and obesity on the basis of PubMed abstracts, public
clinical trials and Merck internal documents. The reported approach shows much
promise and has potential to impact fundamentally pharmaceutical research as a
way to shorten time-to-market of novel drugs, and for early recognition of dead
ends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07118</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07118</id><created>2015-06-23</created><authors><author><keyname>Afek</keyname><forenames>Yehuda</forenames></author><author><keyname>Kecher</keyname><forenames>Roman</forenames></author><author><keyname>Sulamy</keyname><forenames>Moshe</forenames></author></authors><title>Faster task allocation by idle ants</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We model and analyze the distributed task allocation problem, which is solved
by ant colonies on a daily basis. Ant colonies employ task allocation in which
ants are moved from one task to the other in order to meet changing demands
introduced by the environment, such as excess or shortage of food, dirtier or
cleaner nest, etc. The different tasks are: nursing (overseeing the hatching of
newbies), cleaning, patrolling (searching for new food sources), and foraging
(collecting and carrying the food to the nest). Ants solve this task allocation
efficiently in nature and we mimic their mechanism by presenting a distributed
algorithm that is a variant of the ants algorithm. We then analyze the
complexity of the resulting task allocation distributed algorithms, and show
under what conditions an efficient algorithm exists. In particular, we provide
an $\Omega(n)$ lower bound on the time complexity of task allocation when there
are no idle ants, and a contrasting upper bound of $O(\ln{n})$ when a constant
fraction of the ants are idle, where $n$ is the total number of ants in the
colony. Our analysis suggests a possible explanation of why ant colonies keep
part of the ants in a colony idle, not doing anything.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07136</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07136</id><created>2015-06-23</created><authors><author><keyname>Benninghoff</keyname><forenames>Heike</forenames></author><author><keyname>Garcke</keyname><forenames>Harald</forenames></author></authors><title>Segmentation of Three-dimensional Images with Parametric Active Surfaces
  and Topology Changes</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a novel parametric method for segmentation of
three-dimensional images. We consider a piecewise constant version of the
Mumford-Shah and the Chan-Vese functionals and perform a region-based
segmentation of 3D image data. An evolution law is derived from energy
minimization problems which push the surfaces to the boundaries of 3D objects
in the image. We propose a parametric scheme which describes the evolution of
parametric surfaces. An efficient finite element scheme is proposed for a
numerical approximation of the evolution equations. Since standard parametric
methods cannot handle topology changes automatically, an efficient method is
presented to detect, identify and perform changes in the topology of the
surfaces. One main focus of this paper are the algorithmic details to handle
topology changes like splitting and merging of surfaces and change of the genus
of a surface. Different artificial images are studied to demonstrate the
ability to detect the different types of topology changes. Finally, the
parametric method is applied to segmentation of medical 3D images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07139</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07139</id><created>2015-06-23</created><updated>2015-08-10</updated><authors><author><keyname>Smith</keyname><forenames>Jake A.</forenames></author><author><keyname>Uskov</keyname><forenames>Dmitry B.</forenames></author><author><keyname>Kaplan</keyname><forenames>Lev</forenames></author></authors><title>Optimal Encoding Capacity of a Linear Optical Quantum Channel</title><categories>quant-ph cs.IT math.IT</categories><comments>6 pages, 4 figures</comments><journal-ref>Phys. Rev. A 92, 022324 (2015)</journal-ref><doi>10.1103/PhysRevA.92.022324</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here, we study the capacity of a quantum channel, assuming linear optical
encoding, as a function of available photons and optical modes. First, we
observe that substantial improvement is made possible by not restricting
ourselves to a rail-encoded qubit basis. Then, we derive an analytic formula
for general channel capacity and show that this capacity is achieved without
requiring the use of entangling operations typically required for scalable
universal quantum computation, e.g. KLM measurement-assisted transformations.
As an example, we provide an explicit encoding scheme using the resources
required of standard dense coding using two dual-rail qubits (2 photons in 4
modes). In this case, our protocol encodes one additional bit of information.
Greater gains are expected for larger systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07152</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07152</id><created>2015-06-23</created><updated>2015-11-27</updated><authors><author><keyname>Vu</keyname><forenames>Thanh Long</forenames></author><author><keyname>Araifi</keyname><forenames>Surour Al</forenames></author><author><keyname>Elmoursi</keyname><forenames>Mohamed</forenames></author><author><keyname>Turitsyn</keyname><forenames>Konstantin</forenames></author></authors><title>Toward Simulation-free Estimation of Critical Clearing Time</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contingency screening for transient stability of large-scale, strongly
nonlinear, interconnected power systems is one of the most computationally
challenging parts of Dynamic Security Assessment and requires huge resources to
perform time-domain simulations-based assessment. To reduce computational cost
of time-domain simulations, direct energy methods have been extensively
developed. However, these methods, as well as other existing methods, still
rely on time-consuming numerical integration of the fault-on dynamics. This
task is computationally hard, since possibly thousands of contingencies need to
be scanned and thousands of accompanied fault-on dynamics simulations need to
be performed and stored on a regular basis. In this paper, we introduce a novel
framework to eliminate the need for fault-on dynamics simulations in
contingency screening. This simulation-free framework is based on bounding the
fault-on dynamics and extending the recently introduced Lyapunov Function
Family approach for transient stability analysis of structure-preserving model.
In turn, a lower bound of the critical clearing time (CCT) is obtained by
solving convex optimization problems without relying on any time-domain
simulations. A comprehensive analysis is carried out to validate this novel
technique on a number of IEEE test cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07158</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07158</id><created>2015-06-23</created><authors><author><keyname>Venugopal</keyname><forenames>Kiran</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Device-to-Device Millimeter Wave Communications: Interference, Coverage,
  Rate, and Finite Topologies</title><categories>cs.IT math.IT</categories><comments>31 pages, 17 figures, Submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging applications involving device-to-device communication among wearable
electronics require Gbps throughput, which can be achieved by utilizing
millimeter wave (mmWave) frequency bands. When many such communicating devices
are indoors in close proximity, like in a train car or airplane cabin,
interference can be a serious impairment. This paper uses stochastic geometry
to analyze the performance of mmWave networks with a finite number of
interferers in a finite network region. Prior work considered either lower
carrier frequencies with different antenna and channel assumptions, or a
network with an infinite spatial extent. In this paper, human users not only
carry potentially interfering devices, but also act to block interfering
signals. Using a sequence of assumptions, expressions for coverage and rate are
developed that capture the effects of key antenna characteristics like
directivity and gain, and are a function of the finite area and number of
users. The assumptions are validated through a combination of analysis and
simulation. The main conclusions are that mmWave frequencies can provide Gbps
throughput even with omni-directional transceiver antennas, and larger antenna
arrays give better system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07165</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07165</id><created>2015-06-23</created><authors><author><keyname>Sol&#xe9;-Ribalta</keyname><forenames>Albert</forenames></author><author><keyname>De Domenico</keyname><forenames>Manlio</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Sergio</forenames></author><author><keyname>Arenas</keyname><forenames>Alex</forenames></author></authors><title>Random walk centrality in interconnected multilayer networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>18 pages, 4 figures. arXiv admin note: substantial text overlap with
  arXiv:1311.2906</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-world complex systems exhibit multiple levels of relationships. In many
cases they require to be modeled as interconnected multilayer networks,
characterizing interactions of several types simultaneously. It is of crucial
importance in many fields, from economics to biology and from urban planning to
social sciences, to identify the most (or the less) influential nodes in a
network using centrality measures. However, defining the centrality of actors
in interconnected complex networks is not trivial. In this paper, we rely on
the tensorial formalism recently proposed to characterize and investigate this
kind of complex topologies, and extend two well known random walk centrality
measures, the random walk betweenness and closeness centrality, to
interconnected multilayer networks. For each of the measures we provide
analytical expressions that completely agree with numerically results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07167</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07167</id><created>2015-06-23</created><authors><author><keyname>Hof</keyname><forenames>Hans-Joachim</forenames></author></authors><title>User-Centric IT Security - How to Design Usable Security Mechanisms</title><categories>cs.CR</categories><comments>arXiv admin note: substantial text overlap with arXiv:1506.06987</comments><journal-ref>The Fifth International Conference on Advances in Human-oriented
  and Personalized Mechanisms, Technologies, and Services (CENTRIC 2012), pp.
  7-12, November 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, advanced security mechanisms exist to protect data, systems, and
networks. Most of these mechanisms are effective, and security experts can
handle them to achieve a sufficient level of security for any given system.
However, most of these systems have not been designed with focus on good
usability for the average end user. Today, the average end user often struggles
with understanding and using security mechanisms. Other security mechanisms are
simply annoying for end users. As the overall security of any system is only as
strong as the weakest link in this system, bad usability of IT security
mechanisms may result in operating errors, resulting in insecure systems.
Buying decisions of end users may be affected by the usability of security
mechanisms. Hence software providers may decide to better have no security
mechanism then one with a bad usability. Usability of IT security mechanisms is
one of the most underestimated properties of applications and systems. Even IT
security itself is often only an afterthought. Hence, usability of security
mechanisms is often the afterthought of an afterthought. Software developers
are missing guidelines on how to build security mechanisms with good usability
for end users. This paper presents some guidelines that should help software
developers to improve end user usability of security-related mechanisms, and
analyzes common applications based on these guidelines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07190</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07190</id><created>2015-06-23</created><authors><author><keyname>Mrk&#x161;i&#x107;</keyname><forenames>Nikola</forenames></author><author><keyname>S&#xe9;aghdha</keyname><forenames>Diarmuid &#xd3;</forenames></author><author><keyname>Thomson</keyname><forenames>Blaise</forenames></author><author><keyname>Ga&#x161;i&#x107;</keyname><forenames>Milica</forenames></author><author><keyname>Su</keyname><forenames>Pei-Hao</forenames></author><author><keyname>Vandyke</keyname><forenames>David</forenames></author><author><keyname>Wen</keyname><forenames>Tsung-Hsien</forenames></author><author><keyname>Young</keyname><forenames>Steve</forenames></author></authors><title>Multi-domain Dialog State Tracking using Recurrent Neural Networks</title><categories>cs.CL cs.LG</categories><comments>Accepted as a short paper in the 53rd Annual Meeting of the
  Association for Computational Linguistics (ACL 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dialog state tracking is a key component of many modern dialog systems, most
of which are designed with a single, well-defined domain in mind. This paper
shows that dialog data drawn from different dialog domains can be used to train
a general belief tracking model which can operate across all of these domains,
exhibiting superior performance to each of the domain-specific models. We
propose a training procedure which uses out-of-domain data to initialise belief
tracking models for entirely new domains. This procedure leads to improvements
in belief tracking performance regardless of the amount of in-domain data
available for training the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07191</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07191</id><created>2015-06-23</created><updated>2015-07-11</updated><authors><author><keyname>Dvijotham</keyname><forenames>Krishnamurthy</forenames></author><author><keyname>Turitsyn</keyname><forenames>Konstantin</forenames></author></authors><title>Construction of power flow feasibility sets</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new approach for construction of convex analytically simple
regions where the AC power flow equations are guaranteed to have a feasible
solutions. Construction of these regions is based on efficient semidefinite
programming techniques accelerated via sparsity exploiting algorithms.
Resulting regions have a simple geometric shape in the space of power
injections (polytope or ellipsoid) and can be efficiently used for assessment
of system security in the presence of uncertainty. Efficiency and tightness of
the approach is validated on a number of test networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07194</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07194</id><created>2015-06-23</created><updated>2015-12-27</updated><authors><author><keyname>Boccignone</keyname><forenames>Giuseppe</forenames></author></authors><title>Advanced statistical methods for eye movement analysis and modeling: a
  gentle introduction</title><categories>physics.data-an cs.CV q-bio.NC</categories><comments>Extended draft of Chapter to appear in &quot;An introduction to the
  scientific foundations of eye movement research and its applications&quot;</comments><acm-class>G.3; I.5; I.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this Chapter we show that by considering eye movements, and in particular,
the resulting sequence of gaze shifts, a stochastic process, a wide variety of
tools become available for analyses and modelling beyond conventional
statistical methods. Such tools encompass random walk analyses and more complex
techniques borrowed from the pattern recognition and machine learning fields.
  After a brief, though critical, probabilistic tour of current computational
models of eye movements and visual attention, we lay down the basis for gaze
shift pattern analysis. To this end, the concepts of Markov Processes, the
Wiener process and related random walks within the Gaussian framework of the
Central Limit Theorem will be introduced. Then, we will deliberately violate
fundamental assumptions of the Central Limit Theorem to elicit a larger
perspective, rooted in statistical physics, for analysing and modelling eye
movements in terms of anomalous, non-Gaussian, random walks and modern foraging
theory.
  Eventually, by resorting to machine learning techniques, we discuss how the
analyses of movement patterns can develop into the inference of hidden patterns
of the mind: inferring the observer's task, assessing cognitive impairments,
classifying expertise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07196</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07196</id><created>2015-06-23</created><authors><author><keyname>Tamo</keyname><forenames>Itzhak</forenames></author><author><keyname>Barg</keyname><forenames>Alexander</forenames></author><author><keyname>Frolov</keyname><forenames>Alexey</forenames></author></authors><title>Bounds on the Parameters of Locally Recoverable Codes</title><categories>cs.IT math.IT</categories><comments>Submitted for publication. A part of the results of this paper were
  presented at the 2014 IEEE International Symposium on Information Theory,
  July 2014, Honululu</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A locally recoverable code (LRC code) is a code over a finite alphabet such
that every symbol in the encoding is a function of a small number of other
symbols that form a recovering set. In this paper we derive new finite-length
and asymptotic bounds on the parameters of LRC codes. For LRC codes with a
single recovering set for every coordinate, we derive an asymptotic
Gilbert-Varshamov type bound for LRC codes and find the maximum attainable
relative distance of asymptotically good LRC codes. Similar results are
established for LRC codes with two disjoint recovering sets for every
coordinate. For the case of multiple recovering sets we derive a lower bound on
the parameters using expander graph arguments. Finally, we also derive
finite-length upper bounds on the rate and distance of LRC codes with multiple
recovering sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07198</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07198</id><created>2015-06-23</created><authors><author><keyname>Heindlmaier</keyname><forenames>Michael</forenames></author><author><keyname>Bidokhti</keyname><forenames>Shirin Saeedi</forenames></author></authors><title>Capacity Regions of Two-User Broadcast Erasure Channels with Feedback
  and Hidden Memory</title><categories>cs.IT math.IT</categories><comments>extended version of ISIT paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The two-receiver broadcast packet erasure channel with feedback and memory is
studied. Memory is modeled using a finite-state Markov chain representing a
channel state. The channel state is unknown at the transmitter, but
observations of this hidden Markov chain are available at the transmitter
through feedback. Matching outer and inner bounds are derived and the capacity
region is determined. The capacity region does not have a single-letter
characterization and is, in this sense, uncomputable. Approximations of the
capacity region are provided and two optimal coding algorithms are outlined.
The first algorithm is a probabilistic coding scheme that bases its decisions
on the past L feedback sequences. Its achievable rate-region approaches the
capacity region exponentially fast in L. The second algorithm is a
backpressure-like algorithm that performs optimally in the long run.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07204</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07204</id><created>2015-06-23</created><authors><author><keyname>Temprano</keyname><forenames>Oscar</forenames></author></authors><title>Complexity of a Tetris variant</title><categories>cs.CC</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In this paper we are going to solve an open problem about the game tetris. We
are going to give the first results in the complexity of a variant of offline
tetris introduced by Erik Demaine, Susan Hohenberger and David Liben Nowell in
their paper &quot;Tetris is hard, even to approximate&quot;. In this variant, that
follows a model of movements introduced by John Brzustowsky, we can move and
rotate a piece the number of times we want in the first row. But then, when we
left the piece fall, we cannot move it or rotate it anymore. We are going to
demonstrate that the problem of maximizing the number of cleared lines of this
variant on a particular game board, is NP-hard by reducing the 3-partition
problem to the problem of clearing the board in this variant of tetris
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07208</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07208</id><created>2015-06-23</created><authors><author><keyname>N&#xe1;dvorn&#xed;k</keyname><forenames>Ing. Ji&#x159;&#xed;</forenames></author></authors><title>Cross-matching Engine for Incremental Photometric Sky Survey</title><categories>cs.DB astro-ph.SR</categories><comments>57 pages, 36 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For light curve generation, a pre-planned photometry survey is needed
nowadays, where all of the exposure coordinates have to be given and don't
change during the survey. This thesis shows it is not required and we can
data-mine these light curves from astronomical data that was never meant for
this purpose. With this approach, we can recycle all of the photometric surveys
in the world and generate light curves of observed objects for them.
  This thesis is addressing mostly the catalog generation process, which is
needed for creating the light curves. In practice, it focuses on one of the
most important problems in astroinformatics which is clustering data volumes on
Big Data scale where most of the traditional techniques stagger. We consider a
wide variety of possible solutions from the view of performance, scalability,
distributability, etc. We defined criteria for time and memory complexity which
we evaluated for all of the tested solutions. Furthermore, we created quality
standards which we also take into account when evaluating the results.
  We are using relational databases as a starting point of our implementation
and compare them with the newest technologies potentially usable for solving
our problem. These are noSQL Array databases or transferring the heavy
computations of clustering towards supercomputers by using parallelism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07212</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07212</id><created>2015-06-23</created><authors><author><keyname>Frongillo</keyname><forenames>Rafael</forenames></author><author><keyname>Kash</keyname><forenames>Ian A.</forenames></author></authors><title>On Elicitation Complexity and Conditional Elicitation</title><categories>cs.LG math.OC math.ST q-fin.MF stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Elicitation is the study of statistics or properties which are computable via
empirical risk minimization. While several recent papers have approached the
general question of which properties are elicitable, we suggest that this is
the wrong question---all properties are elicitable by first eliciting the
entire distribution or data set, and thus the important question is how
elicitable. Specifically, what is the minimum number of regression parameters
needed to compute the property?
  Building on previous work, we introduce a new notion of elicitation
complexity and lay the foundations for a calculus of elicitation. We establish
several general results and techniques for proving upper and lower bounds on
elicitation complexity. These results provide tight bounds for eliciting the
Bayes risk of any loss, a large class of properties which includes spectral
risk measures and several new properties of interest. Finally, we extend our
calculus to conditionally elicitable properties, which are elicitable
conditioned on knowing the value of another property, giving a necessary
condition for the elicitability of both properties together.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07214</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07214</id><created>2015-06-23</created><authors><author><keyname>Borraz-Sanchez</keyname><forenames>Conrado</forenames></author><author><keyname>Bent</keyname><forenames>Russell</forenames></author><author><keyname>Backhaus</keyname><forenames>Scott</forenames></author><author><keyname>Hijazi</keyname><forenames>Hassan</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author></authors><title>Convex Relaxations for Gas Expansion Planning</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expansion of natural gas networks is a critical process involving substantial
capital expenditures with complex decision-support requirements. Given the
non-convex nature of gas transmission constraints, global optimality and
infeasibility guarantees can only be offered by global optimisation approaches.
Unfortunately, state-of-the-art global optimisation solvers are unable to scale
up to real-world size instances. In this study, we present a convex
mixed-integer second-order cone relaxation for the gas expansion planning
problem under steady-state conditions. The underlying model offers tight lower
bounds with high computational efficiency. In addition, the optimal solution of
the relaxation can often be used to derive high-quality solutions to the
original problem, leading to provably tight optimality gaps and, in some cases,
global optimal soluutions. The convex relaxation is based on a few key ideas,
including the introduction of flux direction variables, exact McCormick
relaxations, on/off constraints, and integer cuts. Numerical experiments are
conducted on the traditional Belgian gas network, as well as other real larger
networks. The results demonstrate both the accuracy and computational speed of
the relaxation and its ability to produce high-quality solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07216</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07216</id><created>2015-06-23</created><updated>2015-11-22</updated><authors><author><keyname>Braverman</keyname><forenames>Mark</forenames></author><author><keyname>Garg</keyname><forenames>Ankit</forenames></author><author><keyname>Ma</keyname><forenames>Tengyu</forenames></author><author><keyname>Nguyen</keyname><forenames>Huy L.</forenames></author><author><keyname>Woodruff</keyname><forenames>David P.</forenames></author></authors><title>Communication Lower Bounds for Statistical Estimation Problems via a
  Distributed Data Processing Inequality</title><categories>cs.LG cs.CC cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the tradeoff between the statistical error and communication cost of
distributed statistical estimation problems in high dimensions. In the
distributed sparse Gaussian mean estimation problem, each of the $m$ machines
receives $n$ data points from a $d$-dimensional Gaussian distribution with
unknown mean $\theta$ which is promised to be $k$-sparse. The machines
communicate by message passing and aim to estimate the mean $\theta$. We
provide a tight (up to logarithmic factors) tradeoff between the estimation
error and the number of bits communicated between the machines. This directly
leads to a lower bound for the distributed sparse linear regression problem: to
achieve the statistical minimax error, the total communication is at least
$\Omega(\min\{n,d\}m)$, where $n$ is the number of observations that each
machine receives and $d$ is the ambient dimension. We also give the first
optimal simultaneous protocol in the dense case for mean estimation.
  As our main technique, we prove a distributed data processing inequality, as
a generalization of usual data processing inequalities, which might be of
independent interest and useful for other problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07220</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07220</id><created>2015-06-23</created><authors><author><keyname>Peng</keyname><forenames>Yangtuo</forenames></author><author><keyname>Jiang</keyname><forenames>Hui</forenames></author></authors><title>Leverage Financial News to Predict Stock Price Movements Using Word
  Embeddings and Deep Neural Networks</title><categories>cs.CE cs.AI cs.CL</categories><comments>5 pages, 2 figures, technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Financial news contains useful information on public companies and the
market. In this paper we apply the popular word embedding methods and deep
neural networks to leverage financial news to predict stock price movements in
the market. Experimental results have shown that our proposed methods are
simple but very effective, which can significantly improve the stock prediction
accuracy on a standard financial database over the baseline system using only
the historical price information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07224</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07224</id><created>2015-06-23</created><authors><author><keyname>Guo</keyname><forenames>Jian</forenames></author><author><keyname>Gould</keyname><forenames>Stephen</forenames></author></authors><title>Deep CNN Ensemble with Data Augmentation for Object Detection</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We report on the methods used in our recent DeepEnsembleCoco submission to
the PASCAL VOC 2012 challenge, which achieves state-of-the-art performance on
the object detection task. Our method is a variant of the R-CNN model proposed
Girshick:CVPR14 with two key improvements to training and evaluation. First,
our method constructs an ensemble of deep CNN models with different
architectures that are complementary to each other. Second, we augment the
PASCAL VOC training set with images from the Microsoft COCO dataset to
significantly enlarge the amount training data. Importantly, we select a subset
of the Microsoft COCO images to be consistent with the PASCAL VOC task. Results
on the PASCAL VOC evaluation server show that our proposed method outperform
all previous methods on the PASCAL VOC 2012 detection task at time of
submission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07230</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07230</id><created>2015-06-23</created><authors><author><keyname>Han</keyname><forenames>Guangyue</forenames></author><author><keyname>Song</keyname><forenames>Jian</forenames></author></authors><title>Extensions of the I-MMSE Relation</title><categories>cs.IT math.IT math.PR</categories><comments>35 pages. arXiv admin note: text overlap with arXiv:1401.3527</comments><msc-class>94A15 (Primary), 60H10, 60F05 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unveiling a fundamental link between information theory and estimation
theory, the I-MMSE relation by Guo, Shamai and Verdu~\cite{gu05}, together with
its numerous extensions, has great theoretical significance and various
practical applications. On the other hand, its influences to date have been
restricted to channels without feedback or memory, due to the absence of its
extensions to such channels. In this paper, we propose extensions of the I-MMSE
relation to discrete-time and continuous-time Gaussian channels with feedback
and/or memory. Our approach is based on a very simple observation, which can be
applied to other scenarios, such as a simple and direct proof of the classical
de Bruijn's identity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07234</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07234</id><created>2015-06-24</created><authors><author><keyname>Yang</keyname><forenames>Yaoqing</forenames></author><author><keyname>Grover</keyname><forenames>Pulkit</forenames></author><author><keyname>Kar</keyname><forenames>Soummya</forenames></author></authors><title>Computing Linear Transformations with Unreliable Components</title><categories>cs.IT cs.CC math.IT</categories><comments>Submitted for publication. Initial Submission: Jun. 2015. 53 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing a binary linear transformation using
unreliable components when all circuit components are unreliable. Two noise
models of unreliable components are considered: probabilistic errors and
permanent errors. We introduce the &quot;ENCODED&quot; technique that ensures that the
error probability of the computation of the linear transformation is kept
bounded below a small constant independent of the size of the linear
transformation even when all logic gates in the computation are noisy. Further,
we show that the scheme requires fewer operations (in order sense) than its
&quot;uncoded&quot; counterpart. By deriving a lower bound, we show that in some cases,
the scheme is order-optimal. Using these results, we examine the gain in
energy-efficiency from use of &quot;voltage-scaling&quot; scheme where gate-energy is
reduced by lowering the supply voltage. We use a gate energy-reliability model
to show that tuning gate-energy appropriately at different stages of the
computation (&quot;dynamic&quot; voltage scaling), in conjunction with ENCODED, can lead
to order-sense energy-savings over the classical &quot;uncoded&quot; approach. Finally,
we also examine the problem of computing a linear transformation when noiseless
decoders can be used, providing upper and lower bounds to the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07236</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07236</id><created>2015-06-24</created><updated>2015-06-24</updated><authors><author><keyname>Tanaka</keyname><forenames>Kanji</forenames></author><author><keyname>Kondo</keyname><forenames>Eiji</forenames></author></authors><title>Incremental RANSAC for Online Relocation in Large Dynamic Environments</title><categories>cs.RO cs.CV</categories><comments>Offprint of ICRA2006 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicle relocation is the problem in which a mobile robot has to estimate the
self-position with respect to an a priori map of landmarks using the perception
and the motion measurements without using any knowledge of the initial
self-position. Recently, RANdom SAmple Consensus (RANSAC), a robust
multi-hypothesis estimator, has been successfully applied to offline relocation
in static environments. On the other hand, online relocation in dynamic
environments is still a difficult problem, for available computation time is
always limited, and for measurement include many outliers. To realize real time
algorithm for such an online process, we have developed an incremental version
of RANSAC algorithm by extending an efficient preemption RANSAC scheme. This
novel scheme named incremental RANSAC is able to find inlier hypotheses of
self-positions out of large number of outlier hypotheses contaminated by
outlier measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07240</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07240</id><created>2015-06-24</created><authors><author><keyname>Bhandari</keyname><forenames>Ayush</forenames></author><author><keyname>Zayed</keyname><forenames>Ahmed</forenames></author></authors><title>Convolution and Product Theorem for the Special Affine Fourier Transform</title><categories>cs.IT math-ph math.IT math.MP</categories><comments>8 Pages, 1 Figure, 1 Table. Technical Report</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The Special Affine Fourier Transform or the SAFT generalizes a number of well
known unitary transformations as well as signal processing and optics related
mathematical operations. Unlike the Fourier transform, the SAFT does not work
well with the standard convolution operation.
  Recently, Q. Xiang and K. Y. Qin introduced a new convolution operation that
is more suitable for the SAFT and by which the SAFT of the convolution of two
functions is the product of their SAFTs and a phase factor. However, their
convolution structure does not work well with the inverse transform in sofar as
the inverse transform of the product of two functions is not equal to the
convolution of the transforms. In this article we introduce a new convolution
operation that works well with both the SAFT and its inverse leading to an
analogue of the convolution and product formulas for the Fourier transform.
Furthermore, we introduce a second convolution operation that leads to the
elimination of the phase factor in the convolution formula obtained by Q. Xiang
and K. Y. Qin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07246</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07246</id><created>2015-06-24</created><authors><author><keyname>Biedl</keyname><forenames>Therese</forenames></author><author><keyname>Derka</keyname><forenames>Martin</forenames></author></authors><title>$1$-String $B_1$-VPG Representations of Planar Partial $3$-Trees and
  Some Subclasses</title><categories>cs.CG cs.DM cs.DS</categories><comments>To appear at the 27th Canadian Conference on Computational Geometry,
  Kingston, Ontario, August 10--12, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Planar partial $3$-trees are subgraphs of those planar graphs obtained by
repeatedly inserting a vertex of degree $3$ into a face. In this paper, we show
that planar partial $3$-trees have $1$-string $B_1$-VPG representations, i.e.,
representations where every vertex is represented by an orthogonal curve with
at most one bend, every two curves intersect at most once, and intersections of
curves correspond to edges in the graph. We also that some subclasses of planar
partial 3-trees have L-representations, i.e., a $B_1$-VPG representation where
every curve has the shape of an L.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07248</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07248</id><created>2015-06-24</created><authors><author><keyname>Daouya</keyname><forenames>La&#xef;che</forenames><affiliation>L'IFORCE</affiliation></author><author><keyname>Bouchemakh</keyname><forenames>Isma</forenames><affiliation>L'IFORCE</affiliation></author><author><keyname>Sopena</keyname><forenames>Eric</forenames><affiliation>LaBRI</affiliation></author></authors><title>Packing coloring of some undirected and oriented coronae graphs</title><categories>cs.DM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The packing chromatic number $\pcn(G)$ of a graph $G$ is the smallest integer
$k$ such that its set of vertices $V(G)$ can be partitioned into $k$ disjoint
subsets $V\_1$, \ldots, $V\_k$, in such a way that every two distinct vertices
in $V\_i$ are at distance greater than $i$ in $G$ for every $i$, $1\le i\le k$.
For a given integer $p \ge 1$, the generalized corona $G\odot pK\_1$ of a graph
$G$ is the graph obtained from $G$ by adding $p$ degree-one neighbors to every
vertex of $G$. In this paper, we determine the packing chromatic number of
generalized coronae of paths and cycles. Moreover, by considering digraphs and
the (weak) directed distance between vertices, we get a natural extension of
the notion of packing coloring to digraphs. We then determine the packing
chromatic number of orientations of generalized coronae of paths and cycles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07251</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07251</id><created>2015-06-24</created><authors><author><keyname>Vervier</keyname><forenames>K&#xe9;vin</forenames><affiliation>CBIO</affiliation></author><author><keyname>Mah&#xe9;</keyname><forenames>Pierre</forenames><affiliation>CBIO</affiliation></author><author><keyname>Veyrieras</keyname><forenames>Jean-Baptiste</forenames><affiliation>CBIO</affiliation></author><author><keyname>Vert</keyname><forenames>Jean-Philippe</forenames><affiliation>CBIO</affiliation></author></authors><title>Benchmark of structured machine learning methods for microbial
  identification from mass-spectrometry data</title><categories>stat.ML cs.LG q-bio.QM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microbial identification is a central issue in microbiology, in particular in
the fields of infectious diseases diagnosis and industrial quality control. The
concept of species is tightly linked to the concept of biological and clinical
classification where the proximity between species is generally measured in
terms of evolutionary distances and/or clinical phenotypes. Surprisingly, the
information provided by this well-known hierarchical structure is rarely used
by machine learning-based automatic microbial identification systems.
Structured machine learning methods were recently proposed for taking into
account the structure embedded in a hierarchy and using it as additional a
priori information, and could therefore allow to improve microbial
identification systems. We test and compare several state-of-the-art machine
learning methods for microbial identification on a new Matrix-Assisted Laser
Desorption/Ionization Time-of-Flight mass spectrometry (MALDI-TOF MS) dataset.
We include in the benchmark standard and structured methods, that leverage the
knowledge of the underlying hierarchical structure in the learning process. Our
results show that although some methods perform better than others, structured
methods do not consistently perform better than their &quot;flat&quot; counterparts. We
postulate that this is partly due to the fact that standard methods already
reach a high level of accuracy in this context, and that they mainly confuse
species close to each other in the tree, a case where using the known hierarchy
is not helpful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07254</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07254</id><created>2015-06-24</created><authors><author><keyname>Louche</keyname><forenames>Ugo</forenames></author><author><keyname>Ralaivola</keyname><forenames>Liva</forenames></author></authors><title>Unconfused ultraconservative multiclass algorithms</title><categories>cs.LG</categories><proxy>ccsd</proxy><report-no>MLJ-2015</report-no><journal-ref>Machine Learning, Springer Verlag (Germany), 2015, Machine
  learning, 99 (2), pp.351</journal-ref><doi>10.1007/s10994-015-5490-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We tackle the problem of learning linear classifiers from noisy datasets in a
multiclass setting. The two-class version of this problem was studied a few
years ago where the proposed approaches to combat the noise revolve around a
Per-ceptron learning scheme fed with peculiar examples computed through a
weighted average of points from the noisy training set. We propose to build
upon these approaches and we introduce a new algorithm called UMA (for
Unconfused Multiclass additive Algorithm) which may be seen as a generalization
to the multiclass setting of the previous approaches. In order to characterize
the noise we use the confusion matrix as a multiclass extension of the
classification noise studied in the aforemen-tioned literature. Theoretically
well-founded, UMA furthermore displays very good empirical noise robustness, as
evidenced by numerical simulations conducted on both synthetic and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07257</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07257</id><created>2015-06-24</created><authors><author><keyname>Gao</keyname><forenames>Jingyu</forenames></author><author><keyname>Yang</keyname><forenames>Jinfu</forenames></author><author><keyname>Wang</keyname><forenames>Guanghui</forenames></author><author><keyname>Li</keyname><forenames>Mingai</forenames></author></authors><title>A Novel Feature Extraction Method for Scene Recognition Based on
  Centered Convolutional Restricted Boltzmann Machines</title><categories>cs.CV</categories><comments>22 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scene recognition is an important research topic in computer vision, while
feature extraction is a key step of object recognition. Although classical
Restricted Boltzmann machines (RBM) can efficiently represent complicated data,
it is hard to handle large images due to its complexity in computation. In this
paper, a novel feature extraction method, named Centered Convolutional
Restricted Boltzmann Machines (CCRBM), is proposed for scene recognition. The
proposed model is an improved Convolutional Restricted Boltzmann Machines
(CRBM) by introducing centered factors in its learning strategy to reduce the
source of instabilities. First, the visible units of the network are redefined
using centered factors. Then, the hidden units are learned with a modified
energy function by utilizing a distribution function, and the visible units are
reconstructed using the learned hidden units. In order to achieve better
generative ability, the Centered Convolutional Deep Belief Networks (CCDBN) is
trained in a greedy layer-wise way. Finally, a softmax regression is
incorporated for scene recognition. Extensive experimental evaluations using
natural scenes, MIT-indoor scenes, and Caltech 101 datasets show that the
proposed approach performs better than other counterparts in terms of
stability, generalization, and discrimination. The CCDBN model is more suitable
for natural scene image recognition by virtue of convolutional property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07259</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07259</id><created>2015-06-24</created><authors><author><keyname>Wei</keyname><forenames>Lu</forenames></author><author><keyname>Pitaval</keyname><forenames>Renaud-Alexandre</forenames></author><author><keyname>Corander</keyname><forenames>Jukka</forenames></author><author><keyname>Tirkkonen</keyname><forenames>Olav</forenames></author></authors><title>From Random Matrix Theory to Coding Theory: Volume of a Metric Ball in
  Unitary Group</title><categories>cs.IT math-ph math.IT math.MP</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Volume estimates of metric balls in manifolds find diverse applications in
information and coding theory. In this paper, some new results for the volume
of a metric ball in unitary group are derived via various tools from random
matrix theory. The first result is an integral representation of the exact
volume, which involves a Toeplitz determinant of Bessel functions. The
connection to matrix-variate hypergeometric functions and Szeg\H{o}'s strong
limit theorem lead independently from the finite size formula to an asymptotic
one. The convergence of the limiting formula is exceptionally fast due to an
underlying mock-Gaussian behavior. The proposed volume estimate enables simple
but accurate analytical evaluation of coding-theoretic bounds of unitary codes.
In particular, the Gilbert-Varshamov lower bound and the Hamming upper bound on
cardinality as well as the resulting bounds on code rate and minimum distance
are derived. Moreover, bounds on the scaling law of code rate are found.
Lastly, a closed-form bound on diversity sum relevant to unitary space-time
codes is obtained, which was only computed numerically in literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07260</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07260</id><created>2015-06-24</created><authors><author><keyname>Bazgan</keyname><forenames>Cristina</forenames></author><author><keyname>Brankovic</keyname><forenames>Ljiljana</forenames></author><author><keyname>Casel</keyname><forenames>Katrin</forenames></author><author><keyname>Fernau</keyname><forenames>Henning</forenames></author><author><keyname>Jansen</keyname><forenames>Klaus</forenames></author><author><keyname>Lampis</keyname><forenames>Michael</forenames></author><author><keyname>Liedloff</keyname><forenames>Mathieu</forenames></author><author><keyname>Monnot</keyname><forenames>J&#xe9;r&#xf4;me</forenames></author><author><keyname>Paschos</keyname><forenames>Vangelis Th.</forenames></author></authors><title>Algorithmic Aspects of Upper Domination</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study combinatorial and algorithmic resp. complexity
questions of upper domination, i.e., the maximum cardinality of a minimal
dominating set in a graph. We give a full classification of the related
maximisation and minimisation problems, as well as the related parameterised
problems, on general graphs and on graphs of bounded degree, and we also study
planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07269</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07269</id><created>2015-06-24</created><updated>2015-09-09</updated><authors><author><keyname>Gil</keyname><forenames>David Leon</forenames></author></authors><title>Distribution of elliptic twins over fixed finite fields: Numerical
  results</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report numerical results, and describe plans for future experiments,
related to the number of prime-order curves and &quot;elliptic twin&quot; curves over the
primes P-224, P-256, and P-384 standardized by NIST for cryptographic
applications. Although these results are not sufficient to confirm the formula
of Shparlinski and Sutantyo 2014 over these fields, they strongly suggest (~99%
probability) that the NIST curve P-384 was not chosen from a uniform
distribution over prime-order curves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07271</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07271</id><created>2015-06-24</created><authors><author><keyname>Yang</keyname><forenames>Jinfu</forenames></author><author><keyname>Gao</keyname><forenames>Jingyu</forenames></author><author><keyname>Wang</keyname><forenames>Guanghui</forenames></author><author><keyname>Zhang</keyname><forenames>Shanshan</forenames></author></authors><title>Natural Scene Recognition Based on Superpixels and Deep Boltzmann
  Machines</title><categories>cs.CV</categories><comments>29 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Deep Boltzmann Machines (DBM) is a state-of-the-art unsupervised learning
model, which has been successfully applied to handwritten digit recognition
and, as well as object recognition. However, the DBM is limited in scene
recognition due to the fact that natural scene images are usually very large.
In this paper, an efficient scene recognition approach is proposed based on
superpixels and the DBMs. First, a simple linear iterative clustering (SLIC)
algorithm is employed to generate superpixels of input images, where each
superpixel is regarded as an input of a learning model. Then, a two-layer DBM
model is constructed by stacking two restricted Boltzmann machines (RBMs), and
a greedy layer-wise algorithm is applied to train the DBM model. Finally, a
softmax regression is utilized to categorize scene images. The proposed
technique can effectively reduce the computational complexity and enhance the
performance for large natural image recognition. The approach is verified and
evaluated by extensive experiments, including the fifteen-scene categories
dataset the UIUC eight-sports dataset, and the SIFT flow dataset, are used to
evaluate the proposed method. The experimental results show that the proposed
approach outperforms other state-of-the-art methods in terms of recognition
rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07272</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07272</id><created>2015-06-24</created><authors><author><keyname>Mascetti</keyname><forenames>Sergio</forenames></author><author><keyname>Picinali</keyname><forenames>Lorenzo</forenames></author><author><keyname>Gerino</keyname><forenames>Andrea</forenames></author><author><keyname>Ahmetovic</keyname><forenames>Dragan</forenames></author><author><keyname>Bernareggi</keyname><forenames>Cristian</forenames></author></authors><title>Sonification of guidance data during road crossing for people with
  visual impairments or blindness</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last years several solutions were proposed to support people with
visual impairments or blindness during road crossing. These solutions focus on
computer vision techniques for recognizing pedestrian crosswalks and computing
their relative position from the user. Instead, this contribution addresses a
different problem; the design of an auditory interface that can effectively
guide the user during road crossing. Two original auditory guiding modes based
on data sonification are presented and compared with a guiding mode based on
speech messages.
  Experimental evaluation shows that there is no guiding mode that is best
suited for all test subjects. The average time to align and cross is not
significantly different among the three guiding modes, and test subjects
distribute their preferences for the best guiding mode almost uniformly among
the three solutions. From the experiments it also emerges that higher effort is
necessary for decoding the sonified instructions if compared to the speech
instructions, and that test subjects require frequent `hints' (in the form of
speech messages). Despite this, more than 2/3 of test subjects prefer one of
the two guiding modes based on sonification. There are two main reasons for
this: firstly, with speech messages it is harder to hear the sound of the
environment, and secondly sonified messages convey information about the
&quot;quantity&quot; of the expected movement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07273</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07273</id><created>2015-06-24</created><authors><author><keyname>Mostafanasab</keyname><forenames>Hojjat</forenames></author><author><keyname>Karimi</keyname><forenames>Negin</forenames></author></authors><title>$(1-2u^2)$-constacyclic codes over
  $\mathbb{F}_p+u\mathbb{F}_p+u^2\mathbb{F}_p$</title><categories>cs.IT math.IT</categories><comments>8 pages</comments><msc-class>94B05, 94B15, 11T71, 13M99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathbb{F}_p$ be a finite field and $u$ be an indeterminate. This
article studies $(1-2u^2)$-constacyclic codes over the ring
$\mathbb{F}_p+u\mathbb{F}_p+u^2\mathbb{F}_p$, where $u^3=u$. We describe
generator polynomials of this kind of codes and investigate the structural
properties of these codes by a decomposition theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07275</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07275</id><created>2015-06-24</created><updated>2015-10-09</updated><authors><author><keyname>Blenk</keyname><forenames>Andreas</forenames></author><author><keyname>Basta</keyname><forenames>Arsany</forenames></author><author><keyname>Reisslein</keyname><forenames>Martin</forenames></author><author><keyname>Kellerer</keyname><forenames>Wolfgang</forenames></author></authors><title>Survey on Network Virtualization Hypervisors for Software Defined
  Networking</title><categories>cs.NI</categories><comments>IEEE Communications Surveys and Tutorials, in print, 2015</comments><doi>10.1109/COMST.2015.2489183</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software defined networking (SDN) has emerged as a promising paradigm for
making the control of communication networks flexible. SDN separates the data
packet forwarding plane, i.e., the data plane, from the control plane and
employs a central controller. Network virtualization allows the flexible
sharing of physical networking resources by multiple users (tenants). Each
tenant runs its own applications over its virtual network, i.e., its slice of
the actual physical network. The virtualization of SDN networks promises to
allow networks to leverage the combined benefits of SDN networking and network
virtualization and has therefore attracted significant research attention in
recent years. A critical component for virtualizing SDN networks is an SDN
hypervisor that abstracts the underlying physical SDN network into multiple
logically isolated virtual SDN networks (vSDNs), each with its own controller.
We comprehensively survey hypervisors for SDN networks in this article. We
categorize the SDN hypervisors according to their architecture into centralized
and distributed hypervisors. We furthermore sub-classify the hypervisors
according to their execution platform into hypervisors running exclusively on
general-purpose compute platforms, or on a combination of general-purpose
compute platforms with general- or special-purpose network elements. We
exhaustively compare the network attribute abstraction and isolation features
of the existing SDN hypervisors. As part of the future research agenda, we
outline the development of a performance evaluation framework for SDN
hypervisors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07285</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07285</id><created>2015-06-24</created><updated>2016-03-05</updated><authors><author><keyname>Kumar</keyname><forenames>Ankit</forenames></author><author><keyname>Irsoy</keyname><forenames>Ozan</forenames></author><author><keyname>Ondruska</keyname><forenames>Peter</forenames></author><author><keyname>Iyyer</keyname><forenames>Mohit</forenames></author><author><keyname>Bradbury</keyname><forenames>James</forenames></author><author><keyname>Gulrajani</keyname><forenames>Ishaan</forenames></author><author><keyname>Zhong</keyname><forenames>Victor</forenames></author><author><keyname>Paulus</keyname><forenames>Romain</forenames></author><author><keyname>Socher</keyname><forenames>Richard</forenames></author></authors><title>Ask Me Anything: Dynamic Memory Networks for Natural Language Processing</title><categories>cs.CL cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most tasks in natural language processing can be cast into question answering
(QA) problems over language input. We introduce the dynamic memory network
(DMN), a neural network architecture which processes input sequences and
questions, forms episodic memories, and generates relevant answers. Questions
trigger an iterative attention process which allows the model to condition its
attention on the inputs and the result of previous iterations. These results
are then reasoned over in a hierarchical recurrent sequence model to generate
answers. The DMN can be trained end-to-end and obtains state-of-the-art results
on several types of tasks and datasets: question answering (Facebook's bAbI
dataset), text classification for sentiment analysis (Stanford Sentiment
Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The
training for these different tasks relies exclusively on trained word vector
representations and input-question-answer triplets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07290</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07290</id><created>2015-06-24</created><authors><author><keyname>Widemann</keyname><forenames>Baltasar Tranc&#xf3;n y</forenames></author><author><keyname>Hauhs</keyname><forenames>Michael</forenames></author></authors><title>Scientific Modelling with Coalgebra-Algebra Homomorphisms</title><categories>cs.LO</categories><acm-class>F.4.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Many recursive functions can be defined elegantly as the unique
homomorphisms, between two algebras, two coalgebras, or one each, that are
induced by some universal property of a distinguished structure. Besides the
well-known applications in recursive functional programming, several basic
modes of reasoning about scientific models have been demonstrated to admit such
an exact meta-theory. Here we explore the potential of coalgebra--algebra
homomorphism that are not a priori unique, for capturing more loosely
specifying patterns of scientific modelling. We investigate a pair of dual
techniques that leverage (co)monadic structure to obtain reasonable genericity
even when no universal properties are given. We show the general applicability
of the approach by discussing a surprisingly broad collection of instances from
real-world modelling practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07293</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07293</id><created>2015-06-24</created><authors><author><keyname>Qu</keyname><forenames>Bo</forenames></author><author><keyname>Wang</keyname><forenames>Huijuan</forenames></author></authors><title>SIS Epidemic Spreading with Heterogeneous Infection Rates</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 5 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we aim to understand the influence of the heterogeneity of
infection rates on the Susceptible-Infected-Susceptible (SIS) epidemic
spreading. Specifically, we keep the recovery rates the same for all nodes and
study the influence of the moments of the independently identically distributed
(i.i.d.) infection rates on the average fraction $y_\infty$ of infected nodes
in the meta-stable state, which indicates the severity of the overall
infection. Motivated by real-world datasets, we consider the log-normal and
gamma distributions for the infection rates and we design as well a symmetric
distribution so that we have a systematic view of the influence of various
distributions. By continuous-time simulations on several types of networks,
theoretical proofs and physical interpretations, we conclude that: 1) the
heterogeneity of infection rates on average retards the virus spread, and 2) a
larger even-order moment of the infection rates leads to a smaller average
fraction of infected nodes, but the odd-order moments contribute in the
opposite way. Our results suggest that, in reality, the epidemic spread may not
be so severe as the classic homogeneous SIS model indicates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07300</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07300</id><created>2015-06-24</created><authors><author><keyname>Magoarou</keyname><forenames>Luc Le</forenames></author><author><keyname>Gribonval</keyname><forenames>R&#xe9;mi</forenames></author></authors><title>Flexible Multi-layer Sparse Approximations of Matrices and Applications</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computational cost of many signal processing and machine learning
techniques is often dominated by the cost of applying certain linear operators
to high-dimensional vectors. This paper introduces an algorithm aimed at
reducing the complexity of applying linear operators in high dimension by
approximately factorizing the corresponding matrix into few sparse factors. The
approach relies on recent advances in non-convex optimization. It is first
explained and analyzed in details and then demonstrated experimentally on
various problems including dictionary learning for image denoising, and the
approximation of large matrices arising in inverse problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07310</identifier>
 <datestamp>2015-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07310</id><created>2015-06-24</created><updated>2015-07-22</updated><authors><author><keyname>Liu</keyname><forenames>Jingtuo</forenames></author><author><keyname>Deng</keyname><forenames>Yafeng</forenames></author><author><keyname>Bai</keyname><forenames>Tao</forenames></author><author><keyname>Wei</keyname><forenames>Zhengping</forenames></author><author><keyname>Huang</keyname><forenames>Chang</forenames></author></authors><title>Targeting Ultimate Accuracy: Face Recognition via Deep Embedding</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face Recognition has been studied for many decades. As opposed to traditional
hand-crafted features such as LBP and HOG, much more sophisticated features can
be learned automatically by deep learning methods in a data-driven way. In this
paper, we propose a two-stage approach that combines a multi-patch deep CNN and
deep metric learning, which extracts low dimensional but very discriminative
features for face verification and recognition. Experiments show that this
method outperforms other state-of-the-art methods on LFW dataset, achieving
99.77% pair-wise verification accuracy and significantly better accuracy under
other two more practical protocols. This paper also discusses the importance of
data size and the number of patches, showing a clear path to practical
high-performance face recognition systems in real world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07327</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07327</id><created>2015-06-24</created><authors><author><keyname>Santani</keyname><forenames>Darshan</forenames></author><author><keyname>Njuguna</keyname><forenames>Jidraph</forenames></author><author><keyname>Bills</keyname><forenames>Tierra</forenames></author><author><keyname>Bryant</keyname><forenames>Aisha W.</forenames></author><author><keyname>Bryant</keyname><forenames>Reginald</forenames></author><author><keyname>Ledgard</keyname><forenames>Jonathan</forenames></author><author><keyname>Gatica-Perez</keyname><forenames>Daniel</forenames></author></authors><title>CommuniSense: Crowdsourcing Road Hazards in Nairobi</title><categories>cs.CY</categories><comments>In Proceedings of 17th International Conference on Human-Computer
  Interaction with Mobile Devices and Services (MobileHCI 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nairobi is one of the fastest growing metropolitan cities and a major
business and technology powerhouse in Africa. However, Nairobi currently lacks
monitoring technologies to obtain reliable data on traffic and road
infrastructure conditions. In this paper, we investigate the use of mobile
crowdsourcing as means to gather and document Nairobi's road quality
information. We first present the key findings of a city-wide road quality
survey about the perception of existing road quality conditions in Nairobi.
Based on the survey's findings, we then developed a mobile crowdsourcing
application, called CommuniSense, to collect road quality data. The application
serves as a tool for users to locate, describe, and photograph road hazards. We
tested our application through a two-week field study amongst 30 participants
to document various forms of road hazards from different areas in Nairobi. To
verify the authenticity of user-contributed reports from our field study, we
proposed to use online crowdsourcing using Amazon's Mechanical Turk (MTurk) to
verify whether submitted reports indeed depict road hazards. We found 92% of
user-submitted reports to match the MTurkers judgements. While our prototype
was designed and tested on a specific city, our methodology is applicable to
other developing cities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07329</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07329</id><created>2015-06-24</created><updated>2015-09-08</updated><authors><author><keyname>Iyer</keyname><forenames>Rishabh</forenames></author><author><keyname>Bilmes</keyname><forenames>Jeff</forenames></author></authors><title>Polyhedral aspects of Submodularity, Convexity and Concavity</title><categories>cs.DM cs.DS</categories><comments>38 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Seminal work by Edmonds and Lovasz shows the strong connection between
submodularity and convexity. Submodular functions have tight modular lower
bounds, and subdifferentials in a manner akin to convex functions. They also
admit poly-time algorithms for minimization and satisfy the Fenchel duality
theorem and the Discrete Seperation Theorem, both of which are fundamental
characteristics of convex functions. Submodular functions also show signs
similar to concavity. Submodular maximization, though NP hard, admits constant
factor approximation guarantees. Concave functions composed with modular
functions are submodular, and they also satisfy diminishing returns property.
This manuscript provides a more complete picture on the relationship between
submodularity with convexity and concavity, by extending many of the results
connecting submodularity with convexity to the concave aspects of
submodularity. We first show the existence of superdifferentials, and
efficiently computable tight modular upper bounds of a submodular function.
While we show that it is hard to characterize this polyhedron, we obtain inner
and outer bounds on the superdifferential along with certain specific and
useful supergradients. We then investigate forms of concave extensions of
submodular functions and show interesting relationships to submodular
maximization. We next show connections between optimality conditions over the
superdifferentials and submodular maximization, and show how forms of
approximate optimality conditions translate into approximation factors for
maximization. We end this paper by studying versions of the discrete seperation
theorem and the Fenchel duality theorem when seen from the concave point of
view. In every case, we relate our results to the existing results from the
convex point of view, thereby improving the analysis of the relationship
between submodularity, convexity, and concavity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07330</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07330</id><created>2015-06-24</created><authors><author><keyname>Parsai</keyname><forenames>Ali</forenames></author><author><keyname>Murgia</keyname><forenames>Alessandro</forenames></author><author><keyname>Soetens</keyname><forenames>Quinten David</forenames></author><author><keyname>Demeyer</keyname><forenames>Serge</forenames></author></authors><title>Mutation Testing as a Safety Net for Test Code Refactoring</title><categories>cs.SE</categories><journal-ref>In Scientific Workshop Proceedings of the XP2015 (XP '15
  workshops). ACM, New York, NY, USA, , Article 8 , 7 pages</journal-ref><doi>10.1145/2764979.2764987</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Refactoring is an activity that improves the internal structure of the code
without altering its external behavior. When performed on the production code,
the tests can be used to verify that the external behavior of the production
code is preserved. However, when the refactoring is performed on test code,
there is no safety net that assures that the external behavior of the test code
is preserved. In this paper, we propose to adopt mutation testing as a means to
verify if the behavior of the test code is preserved after refactoring.
Moreover, we also show how this approach can be used to identify the part of
the test code which is improperly refactored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07331</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07331</id><created>2015-06-24</created><updated>2015-09-22</updated><authors><author><keyname>Hu</keyname><forenames>Sha</forenames></author><author><keyname>Rusek</keyname><forenames>Fredrik</forenames></author></authors><title>On the Design of Channel Shortening Demodulators for Iterative Receivers
  in MIMO and ISI Channels</title><categories>cs.IT math.IT</categories><comments>54 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of designing demodulators for linear vector channels
with memory that use reduced-size trellis descriptions for the received signal.
We assume an overall iterative receiver, and for the parts of the signal not
covered by the trellis description, we use interference cancelation based on
the soft information provided by the outer decoder. In order to reach a trellis
description, a linear filter is applied as front-end to compress the signal
structure into a small trellis. This process requires three parameters to be
designed: (i) the front-end filter, (ii) the feedback filter through which the
interference cancelation is done, and (iii) a target response which specifies
the trellis. Demodulators of this form have been studied before under then name
channel shortening (CS), but the interplay between CS and interference
cancelation has not been adequately addressed in the literature. In this paper,
we analyze two types of CS demodulators that are based on the Forney and
Ungerboeck detection models, respectively. The parameters are jointly optimized
based on a generalized mutual information (GMI) function. We also introduce a
third type of CS demodulator that is in general suboptimal but has closed form
solutions for all parameters. Moreover, signal to noise ratio (SNR) asymptotic
properties are analyzed and we show that the third CS demodulator
asymptotically converges to the optimal CS demodulator in the sense of
maximizing the GMI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07338</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07338</id><created>2015-06-24</created><authors><author><keyname>Bensmail</keyname><forenames>Julien</forenames><affiliation>LIP</affiliation></author><author><keyname>Brettell</keyname><forenames>Nick</forenames><affiliation>LIP</affiliation></author></authors><title>Orienting edges to fight fire in graphs</title><categories>cs.DM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate a new oriented variant of the Firefighter Problem. In the
traditional Firefighter Problem, a fire breaks out at a given vertex of a
graph, and at each time interval spreads to neighbouring vertices that have not
been protected, while a constant number of vertices are protected at each time
interval. In the version of the problem considered here, the firefighters are
able to orient the edges of the graph before the fire breaks out, but the fire
could start at any vertex. We consider this problem when played on a graph in
one of several graph classes, and give upper and lower bounds on the number of
vertices that can be saved. In particular, when one firefighter is available at
each time interval, and the given graph is a complete graph, or a complete
bipartite graph, we present firefighting strategies that are provably optimal.
We also provide lower bounds on the number of vertices that can be saved as a
function of the chromatic number, of the maximum degree, and of the treewidth
of a graph. For a subcubic graph, we show that the firefighters can save all
but two vertices, and this is best possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07347</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07347</id><created>2015-06-24</created><authors><author><keyname>Li</keyname><forenames>Yuanxin</forenames></author><author><keyname>Chi</keyname><forenames>Yuejie</forenames></author></authors><title>Stable Separation and Super-Resolution of Mixture Models</title><categories>cs.IT math.IT math.OC</categories><comments>conference version appeared at ISIT 2015 and SAMPTA 2015. arXiv admin
  note: text overlap with arXiv:1504.06015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider simultaneously identifying the membership and locations of point
sources that are convolved with different band-limited point spread functions,
from the observation of their superpositions. This problem arises in
three-dimensional super-resolution single-molecule imaging, neural spike
sorting, multi-user channel identification, among other applications. We
propose a novel algorithm, based on convex programming, and establish its
near-optimal performance guarantee for exact recovery in the noise-free setting
by exploiting the spectral sparsity of the point source model as well as the
incoherence between point spread functions. Furthermore, robustness of the
recovery algorithm in the presence of bounded noise is also established.
Numerical examples are provided to demonstrate the effectiveness of the
proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07359</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07359</id><created>2015-06-24</created><authors><author><keyname>Everitt</keyname><forenames>Tom</forenames></author><author><keyname>Leike</keyname><forenames>Jan</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Sequential Extensions of Causal and Evidential Decision Theory</title><categories>cs.AI</categories><comments>ADT 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Moving beyond the dualistic view in AI where agent and environment are
separated incurs new challenges for decision making, as calculation of expected
utility is no longer straightforward. The non-dualistic decision theory
literature is split between causal decision theory and evidential decision
theory. We extend these decision algorithms to the sequential setting where the
agent alternates between taking actions and observing their consequences. We
find that evidential decision theory has two natural extensions while causal
decision theory only has one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07362</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07362</id><created>2015-06-24</created><authors><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Breiling</keyname><forenames>Marco</forenames></author><author><keyname>Rohde</keyname><forenames>Christian</forenames></author><author><keyname>Burkhardt</keyname><forenames>Frank</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Energy-Efficient 5G Outdoor-to-Indoor Communication: SUDAS Over Licensed
  and Unlicensed Spectrum</title><categories>cs.IT math.IT</categories><comments>Submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the joint resource allocation algorithm design for
downlink and uplink multicarrier transmission assisted by a shared user
equipment (UE)-side distributed antenna system (SUDAS). The proposed SUDAS
simultaneously utilizes licensed frequency bands and unlicensed frequency
bands, (e.g. millimeter wave bands), to enable a spatial multiplexing gain for
single-antenna UEs to improve energy efficiency and system throughput of $5$-th
generation (5G) outdoor-to-indoor communication. The design of the UE
selection, the time allocation to uplink and downlink, and the transceiver
processing matrix is formulated as a non-convex optimization problem for the
maximization of the end-to-end system energy efficiency (bits/Joule). The
proposed problem formulation takes into account minimum data rate requirements
for delay sensitive UEs and the circuit power consumption of all transceivers.
In order to design a tractable resource allocation algorithm, we first show
that the optimal transmitter precoding and receiver post-processing matrices
jointly diagonalize the end-to-end communication channel for both downlink and
uplink communication via SUDAS. Subsequently, the matrix optimization problem
is converted to an equivalent scalar optimization problem for multiple parallel
channels, which is solved by an asymptotically globally optimal iterative
algorithm. Besides, we propose a suboptimal algorithm which finds a locally
optimal solution of the non-convex optimization problem. Simulation results
illustrate that the proposed resource allocation algorithms for SUDAS achieve a
significant performance gain in terms of system energy efficiency and spectral
efficiency compared to conventional baseline systems by offering multiple
parallel data streams for single-antenna UEs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07363</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07363</id><created>2015-06-24</created><authors><author><keyname>R</keyname><forenames>Sai Srivatsa</forenames></author><author><keyname>Babu</keyname><forenames>R. Venkatesh</forenames></author></authors><title>Salient Object Detection via Objectness Measure</title><categories>cs.CV</categories><comments>In IEEE International Conference on Image Processing (ICIP), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Salient object detection has become an important task in many image
processing applications. The existing approaches exploit background prior and
contrast prior to attain state of the art results. In this paper, instead of
using background cues, we estimate the foreground regions in an image using
objectness proposals and utilize it to obtain smooth and accurate saliency
maps. We propose a novel saliency measure called `foreground connectivity'
which determines how tightly a pixel or a region is connected to the estimated
foreground. We use the values assigned by this measure as foreground weights
and integrate these in an optimization framework to obtain the final saliency
maps. We extensively evaluate the proposed approach on two benchmark databases
and demonstrate that the results obtained are better than the existing state of
the art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07365</identifier>
 <datestamp>2015-11-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07365</id><created>2015-06-24</created><updated>2015-11-20</updated><authors><author><keyname>Watter</keyname><forenames>Manuel</forenames></author><author><keyname>Springenberg</keyname><forenames>Jost Tobias</forenames></author><author><keyname>Boedecker</keyname><forenames>Joschka</forenames></author><author><keyname>Riedmiller</keyname><forenames>Martin</forenames></author></authors><title>Embed to Control: A Locally Linear Latent Dynamics Model for Control
  from Raw Images</title><categories>cs.LG cs.CV stat.ML</categories><comments>Final NIPS version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Embed to Control (E2C), a method for model learning and control
of non-linear dynamical systems from raw pixel images. E2C consists of a deep
generative model, belonging to the family of variational autoencoders, that
learns to generate image trajectories from a latent space in which the dynamics
is constrained to be locally linear. Our model is derived directly from an
optimal control formulation in latent space, supports long-term prediction of
image sequences and exhibits strong performance on a variety of complex control
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07367</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07367</id><created>2015-06-24</created><updated>2015-06-30</updated><authors><author><keyname>McGillion</keyname><forenames>Brian</forenames></author><author><keyname>Dettenborn</keyname><forenames>Tanel</forenames></author><author><keyname>Nyman</keyname><forenames>Thomas</forenames></author><author><keyname>Asokan</keyname><forenames>N.</forenames></author></authors><title>Open-TEE - An Open Virtual Trusted Execution Environment</title><categories>cs.CR</categories><comments>Author's version of article to appear in 14th IEEE International
  Conference on Trust, Security and Privacy in Computing and Communications,
  TrustCom 2015, Helsinki, Finland, August 20-22, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hardware-based Trusted Execution Environments (TEEs) are widely deployed in
mobile devices. Yet their use has been limited primarily to applications
developed by the device vendors. Recent standardization of TEE interfaces by
GlobalPlatform (GP) promises to partially address this problem by enabling
GP-compliant trusted applications to run on TEEs from different vendors.
Nevertheless ordinary developers wishing to develop trusted applications face
significant challenges. Access to hardware TEE interfaces are difficult to
obtain without support from vendors. Tools and software needed to develop and
debug trusted applications may be expensive or non-existent.
  In this paper, we describe Open-TEE, a virtual, hardware-independent TEE
implemented in software. Open-TEE conforms to GP specifications. It allows
developers to develop and debug trusted applications with the same tools they
use for developing software in general. Once a trusted application is fully
debugged, it can be compiled for any actual hardware TEE. Through performance
measurements and a user study we demonstrate that Open-TEE is efficient and
easy to use. We have made Open- TEE freely available as open source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07372</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07372</id><created>2015-06-24</created><authors><author><keyname>Bao</keyname><forenames>Jingjun</forenames></author><author><keyname>Ji</keyname><forenames>Lijun</forenames></author></authors><title>New families of optimal frequency hopping sequence sets</title><categories>cs.IT math.IT</categories><comments>10 pages</comments><msc-class>94A55, 94B25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frequency hopping sequences (FHSs) are employed to mitigate the interferences
caused by the hits of frequencies in frequency hopping spread spectrum systems.
In this paper, we present some new algebraic and combinatorial constructions
for FHS sets, including an algebraic construction via the linear mapping, two
direct constructions by using cyclotomic classes and recursive constructions
based on cyclic difference matrices. By these constructions, a number of series
of new FHS sets are then produced. These FHS sets are optimal with respect to
the Peng-Fan bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07405</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07405</id><created>2015-06-24</created><authors><author><keyname>Zhang</keyname><forenames>Dejiao</forenames></author><author><keyname>Balzano</keyname><forenames>Laura</forenames></author></authors><title>Global Convergence of a Grassmannian Gradient Descent Algorithm for
  Subspace Estimation</title><categories>cs.NA math.NA stat.ML</categories><comments>17 pages, 8 figures</comments><msc-class>90C52, 65Y20</msc-class><acm-class>G.1.6; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been observed in a variety of contexts that gradient descent methods
have great success in solving low-rank matrix-factorization problems, despite
the relevant problem formulation being non-convex. We tackle a particular
instance of this scenario, where we seek the d-dimensional subspace spanned by
a streaming data matrix. We apply the natural first order incremental gradient
descent method, constraining the gradient method to the Grassmannian. We show
that this method converges from any random initialization to the global minimum
of the problem, which is also given by the span of the top d left singular
vectors. Further, we give bounds on the expected convergence rate and a high
probability convergence rate. In a local neighborhood of the global minimizer,
our results match the linear rate of convergence in [7].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07411</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07411</id><created>2015-06-24</created><authors><author><keyname>Tataro</keyname><forenames>Merly F.</forenames></author><author><keyname>Arada</keyname><forenames>Marian G.</forenames></author><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>Selecting the Best Traffic Scheme for the Bicutan Roundabout: A
  Microsimulation Approach with Multiple Driver-Agents</title><categories>cs.CE</categories><comments>9 pages, 5 figures</comments><journal-ref>Asia Pacific Journal of Multidisciplinary Research 3(3):127-135,
  2015</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We present the result of our microsimulation study on the effects of six
traffic schemes $T=\{t_0, t_1, \dots, t_5\}$ on the mean total delay time
($\Delta$) and mean speed ($\Sigma$) of vehicles at the non-signalized Bicutan
Roundabout (BR) in Upper Bicutan, Taguig City, Metro Manila, with $t_0$ as the
current traffic scheme being enforced and $t_{i&gt;0}$ as the proposed ones. We
present first that our simulation approach, a hybridized multi-agent system
(MAS) with the car-following and lane-changing models (CLM), can mimic the
current observed traffic scenario $C$ at a statistical significance of
$\alpha=0.05$. That is, the respective absolute differences of the $\Delta$ and
$\Sigma$ between $C$ and $t_0$ are not statistically different from zero. Next,
using our MAS-CLM, we simulated all proposed $t_{i&gt;0}$ and compared their
respective $\Delta$ and $\Sigma$. We found out using DMRT that the best traffic
scheme is $t_3$ (i.e., when we converted the bi-directional 4-lane PNR-PNCC
road into a bi-directional 1-lane PNR-to-PNCC and 3-lane PNCC-to-PNR routes
during rush hours). Then, we experimented on converting BR into a signalized
junction and re-implemented all $t_3$ with controlled stops of $S=\{15s,
45s\}$. We found out that $t_3$ with a 15-s stop has the best performance.
Finally, we simulated the effect of increased in vehicular volume $V$ due to
traffic improvement and we found out that $t_3$ with 15-s stop still
outperforms the others for all increased in $V=\{10\%, 50\%, 100\%\}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07413</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07413</id><created>2015-06-24</created><authors><author><keyname>Kulkarni</keyname><forenames>Devashish</forenames></author><author><keyname>Paldhe</keyname><forenames>Sagar</forenames></author><author><keyname>Kamat</keyname><forenames>Vinod</forenames></author></authors><title>Speech Controlled Quadruped</title><categories>cs.RO</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The project which we have performed is based on voice recognition and we
desire to create a four legged robot that can acknowledge the given
instructions which are given through vocal commands and perform the tasks. The
main processing unit of the robot will be Arduino Uno. We are using 8 servos
for the movement of its legs while two servos will be required for each leg.
The interface between a human and the robot is generated through Python
programming and Eclipse software and it is implemented by using Bluetooth
module HC 06.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07424</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07424</id><created>2015-06-24</created><authors><author><keyname>Arada</keyname><forenames>Marian G.</forenames></author><author><keyname>Tataro</keyname><forenames>Merly F.</forenames></author><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>Simulating the Effects of Various Road Infrastructure Improvements to
  Vehicular Traffic in a Busy Three-road Fork</title><categories>cs.CE</categories><comments>9 pages, 5 figures, Proceedings of the 11th National Conference on
  Information Technology Education (NCITE 2013), pp. 195-203</comments><journal-ref>Philipine Information Technology Journal 8(1):49-57, June 2015</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Using microsimulations of vehicular dynamics, we studied the effects of
several proposed infrastructure developments to the mean travel delay
time~$\Delta$ and mean speed~$\Sigma$ of vehicles passing a busy three-road
fork, particularly in the non-signalized roundabout junction of Lower Bicutan,
Taguig City, Metro Manila. We designed and implemented multi-agent-based
microsimulation models to mimic the autonomous driving behavior of
heterogeneous individuals and measured the effect of various proposed
infrastructure developments on~$\Delta$ and~$\Sigma$. Our aim is to find out
the best infrastructure development from among three choices being considered
by the local government for the purpose of solving the traffic problems in the
area. We created simulation models of the current vehicular traffic situation
in the area using the mean travel times~$\tau$ of statistically sampled
vehicles to show that our model can simulate the real-world at a significance
level of $\alpha=0.05$. Based on these models, we then simulated the effect of
the proposed infrastructure developments on~$\Delta$ and~$\Sigma$ and used
these metrics as our basis of comparison. We found out that the proposed
widening of one fork from two lanes to three lanes has the most improved
metrics at the same $\alpha=0.05$ compared to the metrics we observed in the
current situation. Under this infrastructure development, the~$\Delta$
increases linearly ($R^2=0.98$) at the rate of 1.03~$s$, while the~$\Sigma$
decreases linearly ($R^2&gt;0.99$) at the rate of 0.14~$km/h$ per percent increase
in the total vehicle volume~$\mathcal{V}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07429</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07429</id><created>2015-06-24</created><authors><author><keyname>Man&#x10d;inska</keyname><forenames>Laura</forenames></author><author><keyname>Roberson</keyname><forenames>David E.</forenames></author><author><keyname>Varvitsiotis</keyname><forenames>Antonios</forenames></author></authors><title>Deciding the existence of perfect entangled strategies for nonlocal
  games</title><categories>quant-ph cs.CC math.CO</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  First, we consider the problem of deciding whether a nonlocal game admits a
perfect entangled strategy that uses projective measurements on a maximally
entangled shared state. Via a polynomial-time Karp reduction, we show that
independent set games are the hardest instances of this problem. Secondly, we
show that if every independent set game whose entangled value is equal to one
admits a perfect entangled strategy, then the same holds for all symmetric
synchronous games. Finally, we identify combinatorial lower bounds on the
classical and entangled values of synchronous games in terms of variants of the
independence number of appropriate graphs. Our results suggest that independent
set games might be representative of all nonlocal games when dealing with
questions concerning perfect entangled strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07436</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07436</id><created>2015-06-24</created><authors><author><keyname>Kealy</keyname><forenames>Thomas</forenames></author><author><keyname>Johnson</keyname><forenames>Oliver</forenames></author><author><keyname>Piechocki</keyname><forenames>Robert</forenames></author></authors><title>Distributed Wideband Spectrum Sensing</title><categories>cs.IT math.IT</categories><comments>6 pages, 6 figures, submitted to ieee Globecom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of reconstructing wideband frequency spectra from
distributed, compressive measurements. The measurements are made by a network
of nodes, each independently mixing the ambient spectra with low frequency,
random signals. The reconstruction takes place via local transmissions between
nodes, each performing simple statistical operations such as ridge regression
and shrinkage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07437</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07437</id><created>2015-06-24</created><updated>2016-02-05</updated><authors><author><keyname>Hua</keyname><forenames>M.</forenames></author><author><keyname>Damelin</keyname><forenames>S. B.</forenames></author><author><keyname>Sun</keyname><forenames>J.</forenames></author><author><keyname>Yu</keyname><forenames>M.</forenames></author></authors><title>The Truncated &amp; Supplemented Pascal Matrix and Applications</title><categories>math.CO cs.IT math.IT</categories><msc-class>05B20, 05B35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce the $k\times n$ (with $k\leq n$) truncated,
supplemented Pascal matrix which has the property that any $k$ columns form a
linearly independent set. This property is also present in Reed-Solomon codes;
however, Reed-Solomon codes are completely dense, whereas the truncated,
supplemented Pascal matrix has multiple zeros. If the maximal-distance
separable code conjecture is correct, then our matrix has the maximal number of
columns (with the aformentioned property) that the conjecture allows. This
matrix has applications in coding, network coding, and matroid theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07439</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07439</id><created>2015-06-24</created><authors><author><keyname>Tang</keyname><forenames>Meng</forenames></author><author><keyname>Ayed</keyname><forenames>Ismail Ben</forenames></author><author><keyname>Marin</keyname><forenames>Dmitrii</forenames></author><author><keyname>Boykov</keyname><forenames>Yuri</forenames></author></authors><title>Secrets of GrabCut and Kernel K-means</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The log-likelihood energy term in popular model-fitting segmentation methods,
e.g. Zhu-Yuille, Chan-Vese, GrabCut, etc., is presented as a generalized
&quot;probabilistic&quot; K-means energy for color space clustering. This interpretation
reveals some limitations, e.g. over-fitting. We propose an alternative approach
to color clustering using kernel K-means energy with well-known properties such
as non-linear separation and scalability to higher-dimensional feature spaces.
Similarly to log-likelihoods, our kernel energy term for color space clustering
can be combined with image grid regularization, e.g. boundary smoothness, and
minimized using (pseudo-) bound optimization and max-flow algorithm. Unlike
histogram or GMM fitting and implicit entropy minimization, our approach is
closely related to general pairwise clustering such as average association and
normalized cut. But, in contrast to previous pairwise clustering algorithms,
our approach can incorporate any standard geometric regularization in the image
domain. We analyze extreme cases for kernel bandwidth (e.g. Gini bias) and
propose adaptive strategies. Our general kernel-based approach opens the door
for many extensions/applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07440</identifier>
 <datestamp>2015-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07440</id><created>2015-06-24</created><authors><author><keyname>Lactuan</keyname><forenames>Lei Kristoffer R.</forenames></author><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>Unshredding of Shredded Documents: Computational Framework and
  Implementation</title><categories>cs.CV</categories><comments>7 pages, 3 figures</comments><journal-ref>Asia Pacific Journal of Multidisciplinary Research 3(3):20-25,
  2015</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A shredded document $D$ is a document whose pages have been cut into strips
for the purpose of destroying private, confidential, or sensitive information
$I$ contained in $D$. Shredding has become a standard means of government
organizations, businesses, and private individuals to destroy archival records
that have been officially classified for disposal. It can also be used to
destroy documentary evidence of wrongdoings by entities who are trying to hide
$I$.
  In this paper, we present an optimal $O((n\times m)^2)$ algorithm $A$ that
reconstructs an $n$-page $D$, where each page $p$ is shredded into $m$ strips.
We also present the efficacy of $A$ in reconstructing three document types:
hand-written, machine typed-set, and images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07452</identifier>
 <datestamp>2015-07-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07452</id><created>2015-06-24</created><authors><author><keyname>Stollenga</keyname><forenames>Marijn F.</forenames></author><author><keyname>Byeon</keyname><forenames>Wonmin</forenames></author><author><keyname>Liwicki</keyname><forenames>Marcus</forenames></author><author><keyname>Schmidhuber</keyname><forenames>Juergen</forenames></author></authors><title>Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical
  Volumetric Image Segmentation</title><categories>cs.CV cs.LG</categories><comments>Marijn F. Stollenga and Wonmin Byeon are the shared first authors,
  both authors contributed equally to this work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional Neural Networks (CNNs) can be shifted across 2D images or 3D
videos to segment them. They have a fixed input size and typically perceive
only small local contexts of the pixels to be classified as foreground or
background. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceive
the entire spatio-temporal context of each pixel in a few sweeps through all
pixels, especially when the RNN is a Long Short-Term Memory (LSTM). Despite
these theoretical advantages, however, unlike CNNs, previous MD-LSTM variants
were hard to parallelize on GPUs. Here we re-arrange the traditional cuboid
order of computations in MD-LSTM in pyramidal fashion. The resulting
PyraMiD-LSTM is easy to parallelize, especially for 3D data such as stacks of
brain slice images. PyraMiD-LSTM achieved best known pixel-wise brain image
segmentation results on MRBrainS13 (and competitive results on EM-ISBI12).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07458</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07458</id><created>2015-06-24</created><authors><author><keyname>Allali</keyname><forenames>Julien</forenames></author><author><keyname>Bourgeade</keyname><forenames>Laetitia</forenames></author><author><keyname>Chauve</keyname><forenames>Cedric</forenames></author></authors><title>Chaining fragments in sequences: to sweep or not</title><categories>cs.DS</categories><comments>13 pages, 2 figures Longer version of an extended abstract to appear
  in the proceedings of SPIRE (String Processing and Information REtrieval)
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing an optimal chain of fragments is a classical problem in string
algorithms, with important applications in computational biology. There exist
two efficient dynamic programming algorithms solving this problem, based on
different principles. In the present note, we show how it is possible to
combine the principles of two of these algorithms in order to design a hybrid
dynamic programming algorithm that combines the advantages of both algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07459</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07459</id><created>2015-06-24</created><authors><author><keyname>Minvielle</keyname><forenames>Pierre</forenames></author><author><keyname>Massaloux</keyname><forenames>Pierre</forenames></author><author><keyname>Giovannelli</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>Fast 3D Synthetic Aperture Radar Imaging from Polarization-Diverse
  Measurements</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An innovative 3-D radar imaging technique is developed for fast and efficient
identification and characterization of radar backscattering components of
complex objects, when the collected scattered field is made of
polarization-diverse measurements. In this context, all the polarimetric
information seems irretrievably mixed. A direct model, derived from a simple
but original extension of the widespread &quot;multiple scattering model&quot; leads to a
high dimensional linear inverse problem. It is solved by a fast dedicated
imaging algorithm that performs to determine at a time three huge 3-D scatterer
maps which correspond to HH, VV and HV polarizations at emission and reception.
It is applied successfully to various mock-ups and data sets collected from an
accurate and dedicated 3D spherical experimental layout that provides
concentric polarization-diverse RCS measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07466</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07466</id><created>2015-06-24</created><authors><author><keyname>Ding</keyname><forenames>Jie</forenames></author><author><keyname>Bouabdallah</keyname><forenames>Abdelmadjid</forenames></author><author><keyname>Tarokh</keyname><forenames>Vahid</forenames></author></authors><title>Key Pre-Distributions From Graph-Based Block Designs</title><categories>cs.NI cs.CR</categories><comments>This manuscript was submitted to the IEEE Sensors Journal on March
  13, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the development of wireless communication technologies which
considerably contributed to the development of wireless sensor networks (WSN),
we have witnessed an ever-increasing WSN based applications which induced a
host of research activities in both academia and industry. Since most of the
target WSN applications are very sensitive, security issue is one of the major
challenges in the deployment of WSN. One of the important building blocks in
securing WSN is key management. Traditional key management solutions developed
for other networks are not suitable for WSN since WSN networks are resource
(e.g. memory, computation, energy) limited. Key pre-distribution algorithms
have recently evolved as efficient alternatives of key management in these
networks. In the key pre-distribution systems, secure communication is achieved
between a pair of nodes either by the existence of a key allowing for direct
communication or by a chain of keys forming a key-path between the pair. In
this paper, we propose methods which bring prior knowledge of network
characteristics and application constraints into the design of key
pre-distribution schemes, in order to provide better security and connectivity
while requiring less resources. Our methods are based on casting the prior
information as a graph. Motivated by this idea, we also propose a class of
quasi-symmetric designs referred here to as g-designs. These produce key
pre-distribution schemes that significantly improve upon the existing
constructions based on unital designs. We give some examples, and point out
open problems for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07468</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07468</id><created>2015-06-24</created><authors><author><keyname>Khawar</keyname><forenames>Awais</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>T. Charles</forenames></author></authors><title>Coexistence Analysis between Radar and Cellular System in LoS Channel</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sharing spectrum with incumbents such as radar systems is an attractive
solution for cellular operators in order to meet the ever growing bandwidth
requirements and ease the spectrum crunch problem. In order to realize
efficient spectrum sharing, interference mitigation techniques are required. In
this letter we address techniques to mitigate MIMO radar interference at MIMO
cellular base stations (BSs). We specifically look at the amount of power
received at BSs when radar uses null space projection (NSP)-based interference
mitigation method. NSP reduces the amount of projected power at targets that
are in-close vicinity to BSs. We study this issue and show that this can be
avoided if radar employs a larger transmit array. In addition, we compute the
coherence time of channel between radar and BSs and show that the coherence
time of channel is much larger than the pulse repetition interval of radars.
Therefore, NSP-based interference mitigation techniques which depends on
accurate channel state information (CSI) can be effective as the problem of CSI
being outdated does not occur for most practical scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07477</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07477</id><created>2015-06-24</created><authors><author><keyname>Gu</keyname><forenames>Jiatao</forenames></author><author><keyname>Li</keyname><forenames>Victor O. K.</forenames></author></authors><title>Efficient Learning for Undirected Topic Models</title><categories>cs.LG cs.CL cs.IR stat.ML</categories><comments>Accepted by ACL-IJCNLP 2015 short paper. 6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Replicated Softmax model, a well-known undirected topic model, is powerful in
extracting semantic representations of documents. Traditional learning
strategies such as Contrastive Divergence are very inefficient. This paper
provides a novel estimator to speed up the learning based on Noise Contrastive
Estimate, extended for documents of variant lengths and weighted inputs.
Experiments on two benchmarks show that the new estimator achieves great
learning efficiency and high accuracy on document retrieval and classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07490</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07490</id><created>2015-06-24</created><updated>2015-11-08</updated><authors><author><keyname>Stephens-Davidowitz</keyname><forenames>Noah</forenames></author></authors><title>Discrete Gaussian Sampling Reduces to CVP and SVP</title><categories>cs.CC cs.DS</categories><comments>SODA 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The discrete Gaussian $D_{L- t, s}$ is the distribution that assigns to each
vector $x$ in a shifted lattice $L - t$ probability proportional to $e^{-\pi
\|x\|^2/s^2}$. It has long been an important tool in the study of lattices.
More recently, algorithms for discrete Gaussian sampling (DGS) have found many
applications in computer science. In particular, polynomial-time algorithms for
DGS with very high parameters $s$ have found many uses in cryptography and in
reductions between lattice problems. And, in the past year, Aggarwal, Dadush,
Regev, and Stephens-Davidowitz showed $2^{n+o(n)}$-time algorithms for DGS with
a much wider range of parameters and used them to obtain the current fastest
known algorithms for the two most important lattice problems, the Shortest
Vector Problem (SVP) and the Closest Vector Problem (CVP).
  Motivated by its increasing importance, we investigate the complexity of DGS
itself and its relationship to CVP and SVP. Our first result is a
polynomial-time dimension-preserving reduction from DGS to CVP. There is a
simple reduction from CVP to DGS, so this shows that DGS is equivalent to CVP.
Our second result, which we find to be more surprising, is a polynomial-time
dimension-preserving reduction from centered DGS (the important special case
when $ t = 0$) to SVP. In the other direction, there is a simple reduction from
$\gamma$-approximate SVP for any $\gamma = \Omega(\sqrt{n/\log n})$, and we
present some (relatively weak) evidence to suggest that this might be the best
achievable approximation factor.
  We also show that our CVP result extends to a much wider class of
distributions and even to other norms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07491</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07491</id><created>2015-06-23</created><updated>2016-01-31</updated><authors><author><keyname>Liu</keyname><forenames>Ying</forenames></author><author><keyname>Li</keyname><forenames>Liang</forenames></author><author><keyname>Alexandropoulos</keyname><forenames>George C.</forenames></author><author><keyname>Pesavento</keyname><forenames>Marius</forenames></author></authors><title>Apply Artificial Noise To Secure Fading Relay Networks: A SER-Based
  Approach</title><categories>cs.IT math.IT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply the concept of artificial and controlled interference in a
triangular relay network with an untrusted relay aiming at enhancing the
wireless communication secrecy between the source and the destination node. In
order to shield the square quadrature amplitude modulated (QAM) signals
transmitted from the source node to the relay, the destination node designs and
transmits artificial noise (AN) symbols to jam the relay reception. The
objective of our considered AN design is to degrade the error probability
performance at the untrusted relay, for different types of channel state
information (CSI) at the destination. By considering perfect knowledge of the
instantaneous CSI of the source-to-relay and relay-to-destination links, we
first present an analytical expression for the symbol error rate (SER)
performance at the relay. Based on the assumption of an average power
constraint at the destination node, we then derive the optimal phase and power
distribution of the AN that maximizes the SER at the relay. Furthermore, we
obtain the optimal AN design for the case where only statistical CSI is
available at the destination node. For both cases, our study reveals that the
Gaussian distribution is generally not optimal to generate AN symbols. In
particular, the conducted numerical results show that the Gaussian distributed
AN is far from optimal. The presented AN design takes into account practical
parameters for the communication links, such as QAM signaling and maximum
likelihood decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07497</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07497</id><created>2015-06-24</created><authors><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>Two Power Series Models of Self-Similarity in Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>12 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:0812.2891 by other authors without attribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two power series models are proposed to represent self-similarity and they
are compared to the Zipf and Benford distributions. Since evolution of a social
network is associated with replicating self-similarity at many levels, the
nature of interconnections can serve as a measure of the optimality of its
organization. In contrast with the Zipf distribution where the middle term is
the harmonic mean of the adjoining terms, our distribution considers the middle
term to be the geometric mean. In one of the power series models, the scaling
factor at one level is shown to be the golden ratio. A model for evolution of
networks by oscillations between two different self-similarity measures is
described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07503</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07503</id><created>2015-06-24</created><authors><author><keyname>Chorowski</keyname><forenames>Jan</forenames></author><author><keyname>Bahdanau</keyname><forenames>Dzmitry</forenames></author><author><keyname>Serdyuk</keyname><forenames>Dmitriy</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Attention-Based Models for Speech Recognition</title><categories>cs.CL cs.LG cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent sequence generators conditioned on input data through an attention
mechanism have recently shown very good performance on a range of tasks in-
cluding machine translation, handwriting synthesis and image caption gen-
eration. We extend the attention-mechanism with features needed for speech
recognition. We show that while an adaptation of the model used for machine
translation in reaches a competitive 18.7% phoneme error rate (PER) on the
TIMIT phoneme recognition task, it can only be applied to utterances which are
roughly as long as the ones it was trained on. We offer a qualitative
explanation of this failure and propose a novel and generic method of adding
location-awareness to the attention mechanism to alleviate this issue. The new
method yields a model that is robust to long inputs and achieves 18% PER in
single utterances and 20% in 10-times longer (repeated) utterances. Finally, we
propose a change to the at- tention mechanism that prevents it from
concentrating too much on single frames, which further reduces PER to 17.6%
level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07504</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07504</id><created>2015-06-24</created><authors><author><keyname>Rudolph</keyname><forenames>Maja R.</forenames></author><author><keyname>Ellis</keyname><forenames>Joseph G.</forenames></author><author><keyname>Blei</keyname><forenames>David M.</forenames></author></authors><title>Objective Variables for Probabilistic Revenue Maximization in
  Second-Price Auctions with Reserve</title><categories>stat.ML cs.AI cs.GT cs.LG stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many online companies sell advertisement space in second-price auctions with
reserve. In this paper, we develop a probabilistic method to learn a profitable
strategy to set the reserve price. We use historical auction data with features
to fit a predictor of the best reserve price. This problem is delicate - the
structure of the auction is such that a reserve price set too high is much
worse than a reserve price set too low. To address this we develop objective
variables, a new framework for combining probabilistic modeling with optimal
decision-making. Objective variables are &quot;hallucinated observations&quot; that
transform the revenue maximization task into a regularized maximum likelihood
estimation problem, which we solve with an EM algorithm. This framework enables
a variety of prediction mechanisms to set the reserve price. As examples, we
study objective variable methods with regression, kernelized regression, and
neural networks on simulated and real data. Our methods outperform previous
approaches both in terms of scalability and profit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07512</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07512</id><created>2015-06-24</created><authors><author><keyname>Frostig</keyname><forenames>Roy</forenames></author><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author><author><keyname>Sidford</keyname><forenames>Aaron</forenames></author></authors><title>Un-regularizing: approximate proximal point and faster stochastic
  algorithms for empirical risk minimization</title><categories>stat.ML cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a family of accelerated stochastic algorithms that minimize sums
of convex functions. Our algorithms improve upon the fastest running time for
empirical risk minimization (ERM), and in particular linear least-squares
regression, across a wide range of problem settings. To achieve this, we
establish a framework based on the classical proximal point algorithm. Namely,
we provide several algorithms that reduce the minimization of a strongly convex
function to approximate minimizations of regularizations of the function. Using
these results, we accelerate recent fast stochastic algorithms in a black-box
fashion. Empirically, we demonstrate that the resulting algorithms exhibit
notions of stability that are advantageous in practice. Both in theory and in
practice, the provided algorithms reap the computational benefits of adding a
large strongly convex regularization term, without incurring a corresponding
bias to the original problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07515</identifier>
 <datestamp>2015-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07515</id><created>2015-06-24</created><authors><author><keyname>Huh</keyname><forenames>Dongsung</forenames></author></authors><title>The Vector Space of Convex Curves: How to Mix Shapes</title><categories>cs.GR cs.RO math.DG q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel, log-radius profile representation for convex curves and
define a new operation for combining the shape features of curves. Unlike the
standard, angle profile-based methods, this operation accurately combines the
shape features in a visually intuitive manner. This method have implications in
shape analysis as well as in investigating how the brain perceives and
generates curved shapes and motions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07540</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07540</id><created>2015-06-24</created><authors><author><keyname>Haeffele</keyname><forenames>Benjamin D.</forenames></author><author><keyname>Vidal</keyname><forenames>Rene</forenames></author></authors><title>Global Optimality in Tensor Factorization, Deep Learning, and Beyond</title><categories>cs.NA cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Techniques involving factorization are found in a wide range of applications
and have enjoyed significant empirical success in many fields. However, common
to a vast majority of these problems is the significant disadvantage that the
associated optimization problems are typically non-convex due to a multilinear
form or other convexity destroying transformation. Here we build on ideas from
convex relaxations of matrix factorizations and present a very general
framework which allows for the analysis of a wide range of non-convex
factorization problems - including matrix factorization, tensor factorization,
and deep neural network training formulations. We derive sufficient conditions
to guarantee that a local minimum of the non-convex optimization problem is a
global minimum and show that if the size of the factorized variables is large
enough then from any initialization it is possible to find a global minimizer
using a purely local descent algorithm. Our framework also provides a partial
theoretical justification for the increasingly common use of Rectified Linear
Units (ReLUs) in deep neural networks and offers guidance on deep network
architectures and regularization strategies to facilitate efficient
optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07545</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07545</id><created>2015-06-24</created><authors><author><keyname>Osegi</keyname><forenames>N. E.</forenames></author><author><keyname>Enyindah</keyname><forenames>P.</forenames></author></authors><title>Learning Representations from Deep Networks Using Mode Synthesizers</title><categories>cs.NE</categories><comments>10 pages, 5 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning Networks play a crucial role in the evolution of a vast number
of current machine learning models for solving a variety of real world
non-trivial tasks. Such networks use big data which is generally unlabeled
unsupervised and multi-layered requiring no form of supervision for training
and learning data and has been used to successfully build automatic supervisory
neural networks. However the question still remains how well the learned data
represents interestingness, and their effectiveness i.e. efficiency in deep
learning models or applications. If the output of a network of deep learning
models can be beamed unto a scene of observables, we could learn the
variational frequencies of these stacked networks in a parallel and
distributive way.This paper seeks to discover and represent interesting
patterns in an efficient and less complex way by incorporating the concept of
Mode synthesizers in the deep learning process models
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07552</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07552</id><created>2015-06-24</created><updated>2015-09-22</updated><authors><author><keyname>Zhang</keyname><forenames>Yuchen</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Splash: User-friendly Programming Interface for Parallelizing Stochastic
  Algorithms</title><categories>cs.LG</categories><comments>redo experiments to learn bigger models; compare Splash with
  state-of-the-art implementations on Spark</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic algorithms are efficient approaches to solving machine learning
and optimization problems. In this paper, we propose a general framework called
Splash for parallelizing stochastic algorithms on multi-node distributed
systems. Splash consists of a programming interface and an execution engine.
Using the programming interface, the user develops sequential stochastic
algorithms without concerning any detail about distributed computing. The
algorithm is then automatically parallelized by a communication-efficient
execution engine. We provide theoretical justifications on the optimal rate of
convergence for parallelizing stochastic gradient descent. Splash is built on
top of Apache Spark. The real-data experiments on logistic regression,
collaborative filtering and topic modeling verify that Splash yields
order-of-magnitude speedup over single-thread stochastic algorithms and over
state-of-the-art implementations on Spark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07563</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07563</id><created>2015-06-24</created><authors><author><keyname>Aleem</keyname><forenames>Saiqa</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author><author><keyname>Ahmed</keyname><forenames>Faheem</forenames></author></authors><title>Benchmarking Machine Learning Technologies for Software Defect Detection</title><categories>cs.SE</categories><journal-ref>International Journal of Software Engineering &amp; Applications
  (IJSEA), Volume 6, No.3, pp. 11-23, May 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine Learning approaches are good in solving problems that have less
information. In most cases, the software domain problems characterize as a
process of learning that depend on the various circumstances and changes
accordingly. A predictive model is constructed by using machine learning
approaches and classified them into defective and non-defective modules.
Machine learning techniques help developers to retrieve useful information
after the classification and enable them to analyse data from different
perspectives. Machine learning techniques are proven to be useful in terms of
software bug prediction. This study used public available data sets of software
modules and provides comparative performance analysis of different machine
learning techniques for software bug prediction. Results showed most of the
machine learning methods performed well on software bug datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07566</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07566</id><created>2015-06-24</created><authors><author><keyname>Zheng</keyname><forenames>Da</forenames></author><author><keyname>Burns</keyname><forenames>Randal</forenames></author><author><keyname>Szalay</keyname><forenames>Alexander S.</forenames></author></authors><title>Optimize Unsynchronized Garbage Collection in an SSD Array</title><categories>cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solid state disks (SSDs) have advanced to outperform traditional hard drives
significantly in both random reads and writes. However, heavy random writes
trigger fre- quent garbage collection and decrease the performance of SSDs. In
an SSD array, garbage collection of individ- ual SSDs is not synchronized,
leading to underutilization of some of the SSDs.
  We propose a software solution to tackle the unsyn- chronized garbage
collection in an SSD array installed in a host bus adaptor (HBA), where
individual SSDs are exposed to an operating system. We maintain a long I/O
queue for each SSD and flush dirty pages intelligently to fill the long I/O
queues so that we hide the performance imbalance among SSDs even when there are
few parallel application writes. We further define a policy of select- ing
dirty pages to flush and a policy of taking out stale flush requests to reduce
the amount of data written to SSDs. We evaluate our solution in a real system.
Experi- ments show that our solution fully utilizes all SSDs in an array under
random write-heavy workloads. It improves I/O throughput by up to 62% under
random workloads of mixed reads and writes when SSDs are under active garbage
collection. It causes little extra data writeback and increases the cache hit
rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07568</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07568</id><created>2015-06-24</created><authors><author><keyname>Dinitz</keyname><forenames>Michael</forenames></author><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author><author><keyname>Wagner</keyname><forenames>Tal</forenames></author></authors><title>Towards Resistance Sparsifiers</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study resistance sparsification of graphs, in which the goal is to find a
sparse subgraph (with reweighted edges) that approximately preserves the
effective resistances between every pair of nodes. We show that every dense
regular expander admits a $(1+\epsilon)$-resistance sparsifier of size $\tilde
O(n/\epsilon)$, and conjecture this bound holds for all graphs on $n$ nodes. In
comparison, spectral sparsification is a strictly stronger notion and requires
$\Omega(n/\epsilon^2)$ edges even on the complete graph.
  Our approach leads to the following structural question on graphs: Does every
dense regular expander contain a sparse regular expander as a subgraph? Our
main technical contribution, which may of independent interest, is a positive
answer to this question in a certain setting of parameters. Combining this with
a recent result of von Luxburg, Radl, and Hein~(JMLR, 2014) leads to the
aforementioned resistance sparsifiers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07571</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07571</id><created>2015-06-24</created><authors><author><keyname>Aminikashani</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Gu</keyname><forenames>Wenjun</forenames></author><author><keyname>Kavehrad</keyname><forenames>Mohsen</forenames></author></authors><title>Indoor Location Estimation with Optical-based OFDM Communications</title><categories>cs.IT math.IT</categories><comments>10 Pages, Journal paper. arXiv admin note: substantial text overlap
  with arXiv:1505.01811</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visible Light Communication (VLC) using light emitting diodes (LEDs) has been
gaining increasing attention in recent years as it is appealing for a wide
range of applications such as indoor positioning. Orthogonal frequency division
multiplexing (OFDM) has been applied to indoor wireless optical communications
in order to mitigate the effect of multipath distortion of the optical channel
as well as increasing data rate. In this paper, a novel OFDM VLC system is
proposed which can be utilized for both communications and indoor positioning.
A positioning algorithm based on power attenuation is used to estimate the
receiver coordinates. We further calculate the positioning errors in all the
locations of a room and compare them with those using single carrier modulation
scheme, i.e., on-off keying (OOK) modulation. We demonstrate that OFDM
positioning system outperforms its conventional counterpart. Finally, we
investigate the impact of different system parameters on the positioning
accuracy of the proposed OFDM VLC system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07577</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07577</id><created>2015-06-24</created><updated>2016-02-24</updated><authors><author><keyname>Bernstein</keyname><forenames>Gilbert Louis</forenames></author><author><keyname>Shah</keyname><forenames>Chinmayee</forenames></author><author><keyname>Lemire</keyname><forenames>Crystal</forenames></author><author><keyname>DeVito</keyname><forenames>Zachary</forenames></author><author><keyname>Fisher</keyname><forenames>Matthew</forenames></author><author><keyname>Levis</keyname><forenames>Philip</forenames></author><author><keyname>Hanrahan</keyname><forenames>Pat</forenames></author></authors><title>Ebb: A DSL for Physical Simulation on CPUs and GPUs</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing programming environments for physical simulation is challenging
because simulations rely on diverse algorithms and geometric domains. These
challenges are compounded when we try to run efficiently on heterogeneous
parallel architectures. We present Ebb, a domain-specific language (DSL) for
simulation, that runs efficiently on both CPUs and GPUs. Unlike previous DSLs,
Ebb uses a three-layer architecture to separate (1) simulation code, (2)
definition of data structures for geometric domains, and (3) runtimes
supporting parallel architectures. Different geometric domains are implemented
as libraries that use a common, unified, relational data model. By structuring
the simulation framework in this way, programmers implementing simulations can
focus on the physics and algorithms for each simulation without worrying about
their implementation on parallel computers. Because the geometric domain
libraries are all implemented using a common runtime based on relations, new
geometric domains can be added as needed, without specifying the details of
memory management, mapping to different parallel architectures, or having to
expand the runtime's interface.
  We evaluate Ebb by comparing it to several widely used simulations,
demonstrating comparable performance to hand-written GPU code where available,
and surpassing existing CPU performance optimizations by up to 9$\times$ when
no GPU code exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07584</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07584</id><created>2015-06-24</created><authors><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>Synchronization of ad hoc Clock Networks</title><categories>cs.DC</categories><comments>11 pages, 9 figures, appeared in H.N. Adorna and A.A. Sioson (eds.)
  Proceedings of the 7th National Symposium on Mathematical Aspects of Computer
  Science (SMACS 2014), Ateneo de Naga University, Naga City, Philippines,
  24-28 November 2014, pp. 33-43. Paper submitted to Philippine Computing
  Journal (ISSN 1908-1995)</comments><journal-ref>Philippine Computing Journal 10(1):22-32, August 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a graph-theoretic approach to synchronizing clocks in an {\em ad
hoc} network of $N$~timepieces. Clocks naturally drift away from being
synchronized because of many physical factors. The manual way of clock
synchronization suffers from an inherrent propagation of the so called &quot;clock
drift&quot; due to &quot;word-of-mouth effect.&quot; The current standard way of automated
clock synchronization is either via radio band transmission of the global clock
or via the software-based Network Time Protocol (NTP). Synchronization via
radio band transmission suffers from the wave transmission delay, while the
client-server-based NTP does not scale to increased number of clients as well
as to unforeseen server overload conditions (e.g., flash crowd and time-of-day
effects). Further, the trivial running time of NTP for synchronizing an
$N$-node network, where each node is a clock and the NTP server follows a
single-port communication model, is~$\bigO(N)$. We introduce in this paper a
$\bigO(\log N)$ time for synchronizing the clocks in exchange for an increase
of $\bigO(N)$ in space complexity, though through creative &quot;tweaking,&quot; we later
reduced the space requirement to~$\bigO(1)$. Our graph-theoretic protocol
assumes that the network is $\K_N$, while the subset of clocks are in an
embedded circulant graph $\C_{n&lt;N}^q$ with $q$~jumps and clock information is
communicated through circular shifts within the $\C_{n&lt;N}^q$. All $N$~nodes
communicate via a single-port duplex channel model. Theoretically, this
synchronization protocol allows for $N(\log N)^{-1} - 1$ more synchronizations
than the client-server-based one. Empirically through statistically replicated
multi-agent-based microsimulation runs, our protocol allows at most 80\% of the
clocks synchronized compared to the current protocol which only allows up to
30\% after some steady-state time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07589</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07589</id><created>2015-06-24</created><authors><author><keyname>Terra</keyname><forenames>Ricardo</forenames></author><author><keyname>Valente</keyname><forenames>Marco Tulio</forenames></author><author><keyname>Bigonha</keyname><forenames>Roberto</forenames></author><author><keyname>Czarnecki</keyname><forenames>Krzysztof</forenames></author></authors><title>DCLfix: A Recommendation System for Repairing Architectural Violations</title><categories>cs.SE</categories><journal-ref>3rd Brazilian Conference on Software: Theory and Practice (Tools
  Track), pages 63-68, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Architectural erosion is a recurrent problem in software evolution. Despite
this fact, the process is usually tackled in ad hoc ways, without adequate tool
support at the architecture level. To address this shortcoming, this paper
presents a recommendation system -- called DCLfix -- that provides refactoring
guidelines for maintainers when tackling architectural erosion. In short,
DCLfix suggests refactoring recommendations for violations detected after an
architecture conformance process using DCL, an architectural constraint
language
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07597</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07597</id><created>2015-06-24</created><authors><author><keyname>Tribou</keyname><forenames>Michael J.</forenames></author><author><keyname>Wang</keyname><forenames>David W. L.</forenames></author><author><keyname>Waslander</keyname><forenames>Steven L.</forenames></author></authors><title>Degenerate Motions in Multicamera Cluster SLAM with Non-overlapping
  Fields of View</title><categories>cs.CV cs.RO</categories><comments>18 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An analysis of the relative motion and point feature model configurations
leading to solution degeneracy is presented, for the case of a Simultaneous
Localization and Mapping system using multicamera clusters with non-overlapping
fields-of-view. The SLAM optimization system seeks to minimize image space
reprojection error and is formulated for a cluster containing any number of
component cameras, observing any number of point features over two keyframes.
The measurement Jacobian is transformed to expose a reduced-dimension
representation such that the degeneracy of the system can be determined by the
rank of a dense submatrix. A set of relative motions sufficient for degeneracy
are identified for certain cluster configurations, independent of target model
geometry. Furthermore, it is shown that increasing the number of cameras within
the cluster and observing features across different cameras over the two
keyframes reduces the size of the degenerate motion sets significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07603</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07603</id><created>2015-06-24</created><authors><author><keyname>Pishdad</keyname><forenames>Leila</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author></authors><title>Analytic MMSE Bounds in Linear Dynamic Systems with Gaussian Mixture
  Noise Statistics</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using state-space representation, mobile object positioning problems can be
described as dynamic systems, with the state representing the unknown location
and the observations being the information gathered from the location sensors.
For linear dynamic systems with Gaussian noise, the Kalman filter provides the
Minimum Mean-Square Error (MMSE) state estimation by tracking the posterior.
Hence, by approximating non-Gaussian noise distributions with Gaussian Mixtures
(GM), a bank of Kalman filters or Gaussian Sum Filter (GSF), can provide the
MMSE state estimation. However, the MMSE itself is not analytically tractable.
Moreover, the general analytic bounds proposed in the literature are not
tractable for GM noise statistics. Hence, in this work, we evaluate the MMSE of
linear dynamic systems with GM noise statistics and propose its analytic lower
and upper bounds. We provide two analytic upper bounds which are the
Mean-Square Errors (MSE) of implementable filters, and we show that based on
the shape of the GM noise distributions, the tighter upper bound can be
selected. We also show that for highly multimodal GM noise distributions, the
bounds and the MMSE converge. Simulation results support the validity of the
proposed bounds and their behavior in limits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07608</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07608</id><created>2015-06-25</created><authors><author><keyname>Teplitskiy</keyname><forenames>Misha</forenames></author><author><keyname>Lu</keyname><forenames>Grace</forenames></author><author><keyname>Duede</keyname><forenames>Eamon</forenames></author></authors><title>Amplifying the Impact of Open Access: Wikipedia and the Diffusion of
  Science</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>An earlier version of this paper was presented at the Wikipedia
  Workshop at 9th International Conference on Web and Social Media (ICWSM),
  Oxford, UK</comments><journal-ref>Forthcoming in: Journal of the Association for Information Science
  and Technology, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rise of Wikipedia as a first-stop source for scientific knowledge,
it is important to compare its representation of that knowledge to that of the
academic literature. This article approaches such a comparison through academic
references made within the worlds 50 largest Wikipedias. Previous studies have
raised concerns that Wikipedia editors may simply use the most easily
accessible academic sources rather than sources of the highest academic status.
We test this claim by identifying the 250 most heavily used journals in each of
26 research fields (4,721 journals, 19.4M articles in total) indexed by the
Scopus database, and modeling whether topic, academic status, and accessibility
make articles from these journals more or less likely to be referenced on
Wikipedia. We find that, controlling for field and impact factor, the odds that
an open access journal is referenced on the English Wikipedia are 47% higher
compared to closed access journals. Moreover, in most of the worlds Wikipedias
a journals high status (impact factor) and accessibility (open access policy)
both greatly increase the probability of referencing. Among the implications of
this study is that the chief effect of open access policies may be to
significantly amplify the diffusion of science, through an intermediary like
Wikipedia, to a broad public audience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07609</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07609</id><created>2015-06-25</created><authors><author><keyname>Garg</keyname><forenames>Vikas K.</forenames></author><author><keyname>Rudin</keyname><forenames>Cynthia</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi</forenames></author></authors><title>CRAFT: ClusteR-specific Assorted Feature selecTion</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for clustering with cluster-specific feature
selection. The framework, CRAFT, is derived from asymptotic log posterior
formulations of nonparametric MAP-based clustering models. CRAFT handles
assorted data, i.e., both numeric and categorical data, and the underlying
objective functions are intuitively appealing. The resulting algorithm is
simple to implement and scales nicely, requires minimal parameter tuning,
obviates the need to specify the number of clusters a priori, and compares
favorably with other methods on real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07611</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07611</id><created>2015-06-25</created><authors><author><keyname>Baingana</keyname><forenames>Brian</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>Joint community and anomaly tracking in dynamic networks</title><categories>stat.ML cs.SI physics.soc-ph</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most real-world networks exhibit community structure, a phenomenon
characterized by existence of node clusters whose intra-edge connectivity is
stronger than edge connectivities between nodes belonging to different
clusters. In addition to facilitating a better understanding of network
behavior, community detection finds many practical applications in diverse
settings. Communities in online social networks are indicative of shared
functional roles, or affiliation to a common socio-economic status, the
knowledge of which is vital for targeted advertisement. In buyer-seller
networks, community detection facilitates better product recommendations.
Unfortunately, reliability of community assignments is hindered by anomalous
user behavior often observed as unfair self-promotion, or &quot;fake&quot;
highly-connected accounts created to promote fraud. The present paper advocates
a novel approach for jointly tracking communities while detecting such
anomalous nodes in time-varying networks. By postulating edge creation as the
result of mutual community participation by node pairs, a dynamic factor model
with anomalous memberships captured through a sparse outlier matrix is put
forth. Efficient tracking algorithms suitable for both online and decentralized
operation are developed. Experiments conducted on both synthetic and real
network time series successfully unveil underlying communities and anomalous
nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07613</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07613</id><created>2015-06-25</created><authors><author><keyname>Parizi</keyname><forenames>Sobhan Naderi</forenames></author><author><keyname>He</keyname><forenames>Kun</forenames></author><author><keyname>Sclaroff</keyname><forenames>Stan</forenames></author><author><keyname>Felzenszwalb</keyname><forenames>Pedro</forenames></author></authors><title>Generalized Majorization-Minimization</title><categories>cs.CV cs.IT cs.LG math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-convex optimization is ubiquitous in machine learning. The
Majorization-Minimization (MM) procedure systematically optimizes non-convex
functions through an iterative construction and optimization of upper bounds on
the objective function. The bound at each iteration is required to \emph{touch}
the objective function at the optimizer of the previous bound. We show that
this touching constraint is unnecessary and overly restrictive. We generalize
MM by relaxing this constraint, and propose a new framework for designing
optimization algorithms, named Generalized Majorization-Minimization (G-MM).
Compared to MM, G-MM is much more flexible. For instance, it can incorporate
application-specific biases into the optimization procedure without changing
the objective function. We derive G-MM algorithms for several latent variable
models and show that they consistently outperform their MM counterparts in
optimizing non-convex objectives. In particular, G-MM algorithms appear to be
less sensitive to initialization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07615</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07615</id><created>2015-06-25</created><authors><author><keyname>Zhang</keyname><forenames>Hongyang</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author><author><keyname>Zhang</keyname><forenames>Chao</forenames></author></authors><title>Completing Low-Rank Matrices with Corrupted Samples from Few
  Coefficients in General Basis</title><categories>cs.IT cs.LG cs.NA math.IT math.NA stat.ML</categories><comments>27 pages</comments><msc-class>68T05</msc-class><acm-class>G.1.6; K.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace recovery from corrupted and missing data is crucial for various
applications in signal processing and information theory. To complete missing
values and detect column corruptions, existing robust Matrix Completion (MC)
methods mostly concentrate on recovering a low-rank matrix from few corrupted
coefficients w.r.t. the standard basis, which, however, does not apply to more
general basis, e.g., the Fourier basis. In this paper, we prove that the range
space of an $m\times n$ matrix with rank $r$ can be exactly recovered from few
coefficients w.r.t. general basis, though the rank $r$ and the number of
corrupted samples are both as high as $O(\min\{m,n\}/\log^3 (m+n))$. Thus our
results cover previous work as special cases, and robust MC can recover the
intrinsic matrix with a higher rank. Moreover, we suggest a universal choice of
the regularization parameter, which is $\lambda=1/\sqrt{\log n}$. By our
$\ell_{2,1}$ filtering algorithm, which has theoretical guarantees, we can
further reduce the computational cost of our model. As an application, we also
find that the solutions to extended robust Low-Rank Representation and to our
extended robust MC are mutually expressible, so both our theory and algorithm
can be immediately applied to the subspace clustering problem with missing
values. Experiments verify our theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07619</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07619</id><created>2015-06-25</created><authors><author><keyname>Li</keyname><forenames>Zhijun</forenames></author><author><keyname>Chen</keyname><forenames>Ziting</forenames></author><author><keyname>Fu</keyname><forenames>Jun</forenames></author><author><keyname>Sun</keyname><forenames>Changyin</forenames></author></authors><title>Direct Adaptive Controller for Uncertain MIMO Dynamic Systems with
  Time-varying Delay and Dead-zone Inputs</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an adaptive tracking control method for a class of
nonlinearly parameterized MIMO dynamic systems with time-varying delay and
unknown nonlinear dead-zone inputs. A new high dimensional integral
Lyapunov-Krasovskii functional is introduced for the adaptive controller to
guarantee global stability of the considered systems and also ensure
convergence of the tracking errors to the origin. The proposed method provides
an alternative to existing methods used for MIMO time-delay systems with
dead-zone nonlinearities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07629</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07629</id><created>2015-06-25</created><authors><author><keyname>Jalali</keyname><forenames>Bahram</forenames></author><author><keyname>Mahjoubfar</keyname><forenames>Ata</forenames></author></authors><title>Optical Hardware Accelerators using Nonlinear Dispersion Modes for
  Energy Efficient Computing</title><categories>physics.optics cs.ET</categories><comments>12 Figures</comments><journal-ref>Proceeding of the IEEE, Vol. 103, No. 7, pp. 1071-1086, 2015</journal-ref><doi>10.1109/JPROC.2015.2418538</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new class of hardware accelerators to alleviate
bottlenecks in the acquisition, analytics, storage and computation of
information carried by wideband streaming signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07631</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07631</id><created>2015-06-25</created><authors><author><keyname>Nath</keyname><forenames>Swaprava</forenames></author><author><keyname>Zoeter</keyname><forenames>Onno</forenames></author><author><keyname>Narahari</keyname><forenames>Y.</forenames></author><author><keyname>Dance</keyname><forenames>Christopher R.</forenames></author></authors><title>Dynamic Mechanism Design with Interdependent Valuations</title><categories>cs.GT</categories><comments>18 pages, 1 figure</comments><acm-class>J.4; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an infinite horizon dynamic mechanism design problem with
interdependent valuations. In this setting the type of each agent is assumed to
be evolving according to a first order Markov process and is independent of the
types of other agents. However, the valuation of an agent can depend on the
types of other agents, which makes the problem fall into an interdependent
valuation setting. Designing truthful mechanisms in this setting is non-trivial
in view of an impossibility result which says that for interdependent
valuations, any efficient and ex-post incentive compatible mechanism must be a
constant mechanism, even in a static setting. Mezzetti (2004) circumvents this
problem by splitting the decisions of allocation and payment into two stages.
However, Mezzetti's result is limited to a static setting and moreover in the
second stage of that mechanism, agents are weakly indifferent about reporting
their valuations truthfully. This paper provides a first attempt at designing a
dynamic mechanism which is efficient, `strict' ex-post incentive compatible and
ex-post individually rational in a setting with interdependent values and
Markovian type evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07632</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07632</id><created>2015-06-25</created><authors><author><keyname>Xu</keyname><forenames>Xiaoli</forenames></author><author><keyname>Kumar</keyname><forenames>Meenakshi Sundaram Gandhi Praveen</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author></authors><title>Reliable Broadcast to A User Group with Limited Source Transmissions</title><categories>cs.IT cs.NI math.IT</categories><comments>ICC 2015. arXiv admin note: substantial text overlap with
  arXiv:1504.04464</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to reduce the number of retransmissions and save power for the
source node, we propose a two-phase coded scheme to achieve reliable broadcast
from the source to a group of users with minimal source transmissions. In the
first phase, the information packets are encoded with batched sparse (BATS)
code, which are then broadcasted by the source node until the file can be
cooperatively decoded by the user group. In the second phase, each user
broadcasts the re-encoded packets to its peers based on their respective
received packets from the first phase, so that the file can be decoded by each
individual user. The performance of the proposed scheme is analyzed and the
rank distribution at the moment of decoding is derived, which is used as input
for designing the optimal BATS code. Simulation results show that the proposed
scheme can reduce the total number of retransmissions compared with the
traditional single-phase broadcast with optimal erasure codes. Furthermore,
since a large number of transmissions are shifted from the source node to the
users, power consumptions at the source node is significantly reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07635</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07635</id><created>2015-06-25</created><updated>2016-02-02</updated><authors><author><keyname>Narayan</keyname><forenames>Chinmay</forenames></author><author><keyname>Sharma</keyname><forenames>Subodh</forenames></author><author><keyname>Guha</keyname><forenames>Shibashis</forenames></author><author><keyname>Arun-Kumar</keyname><forenames>S.</forenames></author></authors><title>From Traces To Proofs: Proving Concurrent Program Safe</title><categories>cs.PL cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concurrent programs are very difficult to prove correct primarily because of
scheduling nondeterminism that results in a state space explosion leading to a
large number of executions to be explored. In order to prove a concurrent
program correct, it is essential to show that each of the feasible executions
is correct. Instead of showing every execution correct, one may first prove the
correctness of some execution and then generate a set of executions which
follow a similar correctness argument. This provably correct set can then
safely be removed from the set of all executions thus reducing the set of
executions to be checked further. This process is repeated until either all the
executions are covered or an execution is obtained that violates the given
property. For this approach to work, one needs to construct a formal structure
from a proof of correctness of an execution that captures the set of executions
having `similar' proof as of this execution. In this paper, we use an
alternating finite automaton as this formal structure. We give a novel
construction of an alternating finite automaton from an execution, a safety
property and its proof of correctness. We then use this automaton to give a
simple, sound and complete verification algorithm for concurrent finite state
programs. We also implemented our approach in a prototype tool and the
preliminary results on small benchmarks are encouraging. We assume the
underlying memory model to be sequentially consistent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07643</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07643</id><created>2015-06-25</created><updated>2015-09-21</updated><authors><author><keyname>Im</keyname><forenames>Daniel Jiwoong</forenames></author><author><keyname>Belghazi</keyname><forenames>Mohamed Ishmael Diwan</forenames></author><author><keyname>Memisevic</keyname><forenames>Roland</forenames></author></authors><title>Conservativeness of untied auto-encoders</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss necessary and sufficient conditions for an auto-encoder to define
a conservative vector field, in which case it is associated with an energy
function akin to the unnormalized log-probability of the data. We show that the
conditions for conservativeness are more general than for encoder and decoder
weights to be the same (&quot;tied weights&quot;), and that they also depend on the form
of the hidden unit activation function, but that contractive training criteria,
such as denoising, will enforce these conditions locally. Based on these
observations, we show how we can use auto-encoders to extract the conservative
component of a vector field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07645</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07645</id><created>2015-06-25</created><authors><author><keyname>Sohn</keyname><forenames>Ji Yong</forenames></author><author><keyname>Yoon</keyname><forenames>Sung Whan</forenames></author><author><keyname>Moon</keyname><forenames>Jaekyun</forenames></author></authors><title>When Pilots Should Not Be Reused Across Interfering Cells in Massive
  MIMO</title><categories>cs.IT math.IT</categories><comments>7 pages, accepted and presented at International Conference on
  Communications (ICC2015) Workshop on 5G &amp; Beyond</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pilot reuse issue in massive multi-input multi-output (MIMO) antenna
systems with interfering cells is closely examined. This paper considers
scenarios where the ratio of the channel coherence time to the number of users
in a cell may be sufficiently large. One such practical scenario arises when
the number of users per unit coverage area cannot grow freely while user
mobility is low, as in indoor networks. Another important scenario is when the
service provider is interested in maximizing the sum rate over a fixed,
selected number of users rather than the sum rate over all users in the cell. A
sum-rate comparison analysis shows that in such scenarios less aggressive reuse
of pilots involving allocation of additional pilots for interfering users
yields significant performance advantage relative to the case where all cells
reuse the same pilot set. For a given ratio of the normalized coherence time
interval to the number of users per cell, the optimal pilot assignment strategy
is revealed via a closed-form solution and the resulting net sum-rate is
compared with that of the full pilot reuse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07650</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07650</id><created>2015-06-25</created><authors><author><keyname>Xu</keyname><forenames>Kun</forenames></author><author><keyname>Feng</keyname><forenames>Yansong</forenames></author><author><keyname>Huang</keyname><forenames>Songfang</forenames></author><author><keyname>Zhao</keyname><forenames>Dongyan</forenames></author></authors><title>Semantic Relation Classification via Convolutional Neural Networks with
  Simple Negative Sampling</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Syntactic features play an essential role in identifying relationship in a
sentence. Previous neural network models often suffer from irrelevant
information introduced when subjects and objects are in a long distance. In
this paper, we propose to learn more robust relation representations from the
shortest dependency path through a convolution neural network. We further
propose a straightforward negative sampling strategy to improve the assignment
of subjects and objects. Experimental results show that our method outperforms
the state-of-the-art methods on the SemEval-2010 Task 8 dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07651</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07651</id><created>2015-06-25</created><authors><author><keyname>Alwadi</keyname><forenames>Mohammad</forenames></author><author><keyname>Chetty</keyname><forenames>Girija</forenames></author></authors><title>Sensor Selection Scheme in Temperature Wireless Sensor Network</title><categories>cs.NI</categories><comments>Keywords: Wireless sensor Networks, Physical environment Monitoring,
  machine learning, data mining, feature selection, adaptive routing</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we propose a novel energy efficient environment monitoring
scheme for wireless sensor networks, based on data mining formulation. The
proposed adapting routing scheme for sensors for achieving energy efficiency
from temperature wireless sensor network data set. The experimental validation
of the proposed approach using publicly available Intel Berkeley lab Wireless
Sensor Network dataset shows that it is possible to achieve energy efficient
environment monitoring for wireless sensor networks, with a trade-off between
accuracy and life time extension factor of sensors, using the proposed
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07653</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07653</id><created>2015-06-25</created><authors><author><keyname>Vladimirov</keyname><forenames>Igor G.</forenames></author></authors><title>Weyl variations and local sufficiency of linear observers in the mean
  square optimal coherent quantum filtering problem</title><categories>quant-ph cs.SY math-ph math.MP math.OC math.PR</categories><comments>10 pages, 1 figure. Submitted to the Australian Control Conference,
  Gold Coast, 05-06 November 2015. arXiv admin note: text overlap with
  arXiv:1506.04737</comments><msc-class>81Q93, 81P50, 81P16, 93E11, 93E20, 81S25, 49J40, 47J20, 49K30, 37K05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the coherent quantum filtering (CQF) problem,
where a quantum observer is cascaded in a measurement-free fashion with a
linear quantum plant so as to minimize a mean square error of estimating the
plant variables of interest. Both systems are governed by Markovian
Hudson-Parthasarathy quantum stochastic differential equations driven by
bosonic fields in vacuum state. These quantum dynamics are specified by the
Hamiltonians and system-field coupling operators. We apply a recently proposed
transverse Hamiltonian variational method to the development of first-order
necessary conditions of optimality for the CQF problem in a larger class of
observers. The latter is obtained by perturbing the Hamiltonian and
system-field coupling operators of a linear coherent quantum observer along
linear combinations of unitary Weyl operators, whose role here resembles that
of the needle variations in the Pontryagin minimum principle. We show that if
the observer is a stationary point of the performance functional in the class
of linear observers, then it is also a stationary point with respect to the
Weyl variations in the larger class of nonlinear observers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07656</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07656</id><created>2015-06-25</created><updated>2015-10-08</updated><authors><author><keyname>Revaud</keyname><forenames>Jerome</forenames><affiliation>LEAR</affiliation></author><author><keyname>Weinzaepfel</keyname><forenames>Philippe</forenames><affiliation>LEAR</affiliation></author><author><keyname>Harchaoui</keyname><forenames>Zaid</forenames><affiliation>LEAR</affiliation></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames><affiliation>LEAR</affiliation></author></authors><title>DeepMatching: Hierarchical Deformable Dense Matching</title><categories>cs.CV</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel matching algorithm, called DeepMatching, to compute
dense correspondences between images. DeepMatching relies on a hierarchical,
multi-layer, correlational architecture designed for matching images and was
inspired by deep convolutional approaches. The proposed matching algorithm can
handle non-rigid deformations and repetitive textures and efficiently
determines dense correspondences in the presence of significant changes between
images. We evaluate the performance of DeepMatching, in comparison with
state-of-the-art matching algorithms, on the Mikolajczyk (Mikolajczyk et al
2005), the MPI-Sintel (Butler et al 2012) and the Kitti (Geiger et al 2013)
datasets. DeepMatching outperforms the state-of-the-art algorithms and shows
excellent results in particular for repetitive textures.We also propose a
method for estimating optical flow, called DeepFlow, by integrating
DeepMatching in the large displacement optical flow (LDOF) approach of Brox and
Malik (2011). Compared to existing matching algorithms, additional robustness
to large displacements and complex motion is obtained thanks to our matching
approach. DeepFlow obtains competitive performance on public benchmarks for
optical flow estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07674</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07674</id><created>2015-06-25</created><updated>2015-09-07</updated><authors><author><keyname>Shagdar</keyname><forenames>Oyunchimeg</forenames><affiliation>RITS</affiliation></author></authors><title>Evaluation of Synchronous and Asynchronous Reactive Distributed
  Congestion Control Algorithms for the ITS G5 Vehicular Systems</title><categories>cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IEEE 802.11p is the technology dedicated to vehicular communications to
support road safety, efficiency, and comfort applications. A large number of
research activities have been carried out to study the characteristics of the
IEEE 802.11p. The key weakness of the IEEE 802.11p is the channel congestion
issue, where the wireless channel gets saturated when the road density
increases. The European Telecommunications Standardization Institute (ETSI) is
in the progress of studying the channel congestion problem and proposed
so-called Reactive Distributed Congestion Control (DCC) algorithm as a solution
to the congestion issue. In this report we investigate the impacts of the
Reactive DCC mechanism in comparison to the conventional IEEE 802.11p with no
congestion control. Our study shows that the Reactive DCC scheme creates
oscillation on channel load that consequently degrades communication
performance. The results reveal that the channel load oscillation is due to the
fact that in the Reactive DCC, the individual CAM (Cooperative Awareness
Message) controllers react to the channel congestion in a synchronized manner.
To reduce the oscillation, in this report we propose a simple extension to
Reactive DCC, Asynchronous Reactive DCC, in which the individual CAM
controllers adopt randomized rate setting, which can significantly reduce the
oscillation and improve the network performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07675</identifier>
 <datestamp>2015-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07675</id><created>2015-06-25</created><updated>2015-07-02</updated><authors><author><keyname>Hujdurovi&#x107;</keyname><forenames>Ademir</forenames></author><author><keyname>Ka&#x10d;ar</keyname><forenames>Ur&#x161;a</forenames></author><author><keyname>Milani&#x10d;</keyname><forenames>Martin</forenames></author><author><keyname>Ries</keyname><forenames>Bernard</forenames></author><author><keyname>Tomescu</keyname><forenames>Alexandru I.</forenames></author></authors><title>Finding a perfect phylogeny from mixed tumor samples</title><categories>q-bio.PE cs.CC cs.DM cs.DS</categories><comments>To be presented at WABI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Hajirasouliha and Raphael (WABI 2014) proposed a model for
deconvoluting mixed tumor samples measured from a collection of high-throughput
sequencing reads. This is related to understanding tumor evolution and critical
cancer mutations. In short, their formulation asks to split each row of a
binary matrix so that the resulting matrix corresponds to a perfect phylogeny
and has the minimum number of rows among all matrices with this property. In
this paper we disprove several claims about this problem, including an
NP-hardness proof of it. However, we show that the problem is indeed NP-hard,
by providing a different proof. We also prove NP-completeness of a variant of
this problem proposed in the same paper. On the positive side, we obtain a
polynomial time algorithm for matrix instances in which no column is contained
in both columns of a pair of conflicting columns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07677</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07677</id><created>2015-06-25</created><authors><author><keyname>Hosseini</keyname><forenames>Reshad</forenames></author><author><keyname>Sra</keyname><forenames>Suvrit</forenames></author></authors><title>Manifold Optimization for Gaussian Mixture Models</title><categories>stat.ML cs.LG math.OC</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We take a new look at parameter estimation for Gaussian Mixture Models
(GMMs). In particular, we propose using \emph{Riemannian manifold optimization}
as a powerful counterpart to Expectation Maximization (EM). An out-of-the-box
invocation of manifold optimization, however, fails spectacularly: it converges
to the same solution but vastly slower. Driven by intuition from manifold
convexity, we then propose a reparamerization that has remarkable empirical
consequences. It makes manifold optimization not only match EM---a highly
encouraging result in itself given the poor record nonlinear programming
methods have had against EM so far---but also outperform EM in many practical
settings, while displaying much less variability in running times. We further
highlight the strengths of manifold optimization by developing a somewhat tuned
manifold LBFGS method that proves even more competitive and reliable than
existing manifold optimization tools. We hope that our results encourage a
wider consideration of manifold optimization for parameter estimation problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07679</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07679</id><created>2015-06-25</created><authors><author><keyname>Donaire</keyname><forenames>Alejandro</forenames></author><author><keyname>Ortega</keyname><forenames>Romeo</forenames></author><author><keyname>Romero</keyname><forenames>Jose Guadalupe</forenames></author></authors><title>Simultaneous Interconnection and Damping Assignment Passivity-based
  Control of Mechanical Systems Using Generalized Forces</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To extend the realm of application of the well known controller design
technique of interconnection and damping assignment passivity-based control
(IDA-PBC) of mechanical systems two modifications to the standard method are
presented in this article. First, similarly to [1], it is proposed to avoid the
splitting of the control action into energy-shaping and damping injection
terms, but instead to carry them out simultaneously. Second, motivated by [2],
we propose to consider the inclusion of generalised forces, going beyond the
gyroscopic ones used in standard IDA-PBC. It is shown that several new
controllers for mechanical systems designed invoking other (less systematic
procedures) that do not satisfy the conditions of standard IDA-PBC, actually
belong to this new class of SIDA-PBC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07688</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07688</id><created>2015-06-25</created><authors><author><keyname>Haque</keyname><forenames>Snikdho Sworov</forenames></author><author><keyname>Mowla</keyname><forenames>Md. Munjure</forenames></author></authors><title>Performance Improvement for Papr Reduction in Lte Downlink System with
  Elliptic Filtering</title><categories>cs.IT math.IT</categories><comments>10 pages, 11 figures, Published in IJCNC. in International Journal of
  Computer Networks &amp; Communications (IJCNC) Vol.7, No.1, January 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the performance improvement of PAPR reduction of
orthogonal frequency division multiplexing (OFDM) signal using amplitude
clipping &amp; filtering based design. Note that OFDM is one of the well adept
multi-carrier multiplexing transmission scheme which has been implemented in
long term evolution (LTE) downlink. Nonetheless peak to average power ratio
(PAPR) is the more rattling problem with OFDM, consequently in this paper a
reduction procedure of the PAPR by using amplitude clipping and filtering is
proposed. Here we used IIR bandpass elliptic filter after amplitude clipping to
reduce the PAPR. The performance of the system in terms of bit error rate (BER)
is also canvased as a new filter based clipping method. Our results show that
the proposed methodology of clipping method with the IIR elliptic band pass
filter significantly reduces the PAPR value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07704</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07704</id><created>2015-06-25</created><updated>2015-09-26</updated><authors><author><keyname>Yoo</keyname><forenames>Donggeun</forenames></author><author><keyname>Park</keyname><forenames>Sunggyun</forenames></author><author><keyname>Lee</keyname><forenames>Joon-Young</forenames></author><author><keyname>Paek</keyname><forenames>Anthony S.</forenames></author><author><keyname>Kweon</keyname><forenames>In So</forenames></author></authors><title>AttentionNet: Aggregating Weak Directions for Accurate Object Detection</title><categories>cs.CV cs.LG</categories><comments>To appear in ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel detection method using a deep convolutional neural network
(CNN), named AttentionNet. We cast an object detection problem as an iterative
classification problem, which is the most suitable form of a CNN. AttentionNet
provides quantized weak directions pointing a target object and the ensemble of
iterative predictions from AttentionNet converges to an accurate object
boundary box. Since AttentionNet is a unified network for object detection, it
detects objects without any separated models from the object proposal to the
post bounding-box regression. We evaluate AttentionNet by a human detection
task and achieve the state-of-the-art performance of 65% (AP) on PASCAL VOC
2007/2012 with an 8-layered architecture only.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07719</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07719</id><created>2015-06-25</created><authors><author><keyname>Parise</keyname><forenames>Francesca</forenames></author><author><keyname>Grammatico</keyname><forenames>Sergio</forenames></author><author><keyname>Gentile</keyname><forenames>Basilio</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>Network Aggregative Games and Distributed Mean Field Control via
  Consensus Theory</title><categories>cs.SY cs.GT cs.MA cs.SI math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider network aggregative games to model and study multi-agent
populations in which each rational agent is influenced by the aggregate
behavior of its neighbors, as specified by an underlying network. Specifically,
we examine systems where each agent minimizes a quadratic cost function, that
depends on its own strategy and on a convex combination of the strategies of
its neighbors, and is subject to personalized convex constraints. We analyze
the best response dynamics and we propose alternative distributed algorithms to
steer the strategies of the rational agents to a Nash equilibrium
configuration. The convergence of these schemes is guaranteed under different
sufficient conditions, depending on the matrices defining the cost and on the
network. Additionally, we propose an extension to the network aggregative game
setting that allows for multiple rounds of communications among the agents, and
we illustrate how it can be combined with consensus theory to recover a
solution to the mean field control problem in a distributed fashion, that is,
without requiring the presence of a central coordinator. Finally, we apply our
theoretical findings to study a novel multi-dimensional, convex-constrained
model of opinion dynamics and a hierarchical demand-response scheme for energy
management in smart buildings, extending literature results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07721</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07721</id><created>2015-06-25</created><authors><author><keyname>Fukuchi</keyname><forenames>Kazuto</forenames></author><author><keyname>Sakuma</keyname><forenames>Jun</forenames></author></authors><title>Fairness-Aware Learning with Restriction of Universal Dependency using
  f-Divergences</title><categories>stat.ML cs.LG</categories><comments>15 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fairness-aware learning is a novel framework for classification tasks. Like
regular empirical risk minimization (ERM), it aims to learn a classifier with a
low error rate, and at the same time, for the predictions of the classifier to
be independent of sensitive features, such as gender, religion, race, and
ethnicity. Existing methods can achieve low dependencies on given samples, but
this is not guaranteed on unseen samples. The existing fairness-aware learning
algorithms employ different dependency measures, and each algorithm is
specifically designed for a particular one. Such diversity makes it difficult
to theoretically analyze and compare them. In this paper, we propose a general
framework for fairness-aware learning that uses f-divergences and that covers
most of the dependency measures employed in the existing methods. We introduce
a way to estimate the f-divergences that allows us to give a unified analysis
for the upper bound of the estimation error; this bound is tighter than that of
the existing convergence rate analysis of the divergence estimation. With our
divergence estimate, we propose a fairness-aware learning algorithm, and
perform a theoretical analysis of its generalization error. Our analysis
reveals that, under mild assumptions and even with enforcement of fairness, the
generalization error of our method is $O(\sqrt{1/n})$, which is the same as
that of the regular ERM. In addition, and more importantly, we show that, for
any f-divergence, the upper bound of the estimation error of the divergence is
$O(\sqrt{1/n})$. This indicates that our fairness-aware learning algorithm
guarantees low dependencies on unseen samples for any dependency measure
represented by an f-divergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07723</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07723</id><created>2015-06-25</created><updated>2015-11-14</updated><authors><author><keyname>Jia</keyname><forenames>Chengjin</forenames></author><author><keyname>Wang</keyname><forenames>Jin</forenames></author><author><keyname>Zhu</keyname><forenames>Yanqin</forenames></author><author><keyname>Wang</keyname><forenames>Xin</forenames></author><author><keyname>Lu</keyname><forenames>Kejie</forenames></author><author><keyname>Wang</keyname><forenames>Xiumin</forenames></author><author><keyname>Wen</keyname><forenames>Zhengqing</forenames></author></authors><title>On the Optimal Provider Selection for Repair in Distributed Storage
  System with Network Coding</title><categories>cs.DC</categories><comments>This paper has been withdrawn by the author due to several crucial
  errors in formulations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In large-scale distributed storage systems (DSS), reliability is provided by
redundancy spread over storage servers across the Internet. Network coding (NC)
has been widely studied in DSS because it can improve the reliability with low
repair time. To maintain reliability, an unavailable storage server should be
firstly replaced by a new server, named new comer. Then, multiple storage
servers, called providers, should be selected from surviving servers and send
their coded data through the Internet to the new comer for regenerating the
lost data. Therefore, in a large-scale DSS, provider selection and data routing
during the regeneration phase have great impact on the performance of
regeneration time. In this paper, we investigate a problem of optimal provider
selection and data routing for minimizing the regeneration time in the DSS with
NC. Specifically, we first define the problem in the DSS with NC. For the case
that the providers are given, we model the problem as a mathematical
programming. Based on the mathematical programming, we then formulate the
optimal provider selection and data routing problem as an integer linear
programming problem and develop an efficient near-optimal algorithm based on
linear programming relaxation (BLP). Finally, extensive simulation experiments
have been conducted, and the results show the effectiveness of the proposed
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07729</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07729</id><created>2015-06-25</created><authors><author><keyname>Jansen</keyname><forenames>Bart M. P.</forenames></author><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author></authors><title>A structural approach to kernels for ILPs: Treewidth and Total
  Unimodularity</title><categories>cs.CC cs.DS</categories><comments>Extended abstract in the Proceedings of the 23rd European Symposium
  on Algorithms (ESA 2015)</comments><msc-class>90C05, 68Q25</msc-class><acm-class>F.2.2; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernelization is a theoretical formalization of efficient preprocessing for
NP-hard problems. Empirically, preprocessing is highly successful in practice,
for example in state-of-the-art ILP-solvers like CPLEX. Motivated by this,
previous work studied the existence of kernelizations for ILP related problems,
e.g., for testing feasibility of Ax &lt;= b. In contrast to the observed success
of CPLEX, however, the results were largely negative. Intuitively, practical
instances have far more useful structure than the worst-case instances used to
prove these lower bounds.
  In the present paper, we study the effect that subsystems with (Gaifman graph
of) bounded treewidth or totally unimodularity have on the kernelizability of
the ILP feasibility problem. We show that, on the positive side, if these
subsystems have a small number of variables on which they interact with the
remaining instance, then we can efficiently replace them by smaller subsystems
of size polynomial in the domain without changing feasibility. Thus, if large
parts of an instance consist of such subsystems, then this yields a substantial
size reduction. We complement this by proving that relaxations to the
considered structures, e.g., larger boundaries of the subsystems, allow
worst-case lower bounds against kernelization. Thus, these relaxed structures
can be used to build instance families that cannot be efficiently reduced, by
any approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07731</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07731</id><created>2015-06-25</created><authors><author><keyname>Maksimovic</keyname><forenames>Zoran</forenames></author></authors><title>A multidimensional maximum bisection problem</title><categories>cs.DM</categories><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work introduces a multidimensional generalization of the maximum
bisection problem. A mixed integer linear programming formulation is proposed
with the proof of its correctness. The numerical tests, made on the randomly
generated graphs, indicates that the multidimensional generalization is more
difficult to solve than the original problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07732</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07732</id><created>2015-06-25</created><authors><author><keyname>Bourgeois</keyname><forenames>Nicolas</forenames><affiliation>SAMM</affiliation></author><author><keyname>Cottrell</keyname><forenames>Marie</forenames><affiliation>SAMM</affiliation></author><author><keyname>D&#xe9;ruelle</keyname><forenames>Benjamin</forenames><affiliation>LAMOP</affiliation></author><author><keyname>Lamass&#xe9;</keyname><forenames>St&#xe9;phane</forenames><affiliation>LAMOP</affiliation></author><author><keyname>Letr&#xe9;my</keyname><forenames>Patrick</forenames><affiliation>SAMM</affiliation></author></authors><title>How to improve robustness in Kohonen maps and display additional
  information in Factorial Analysis: application to text mining</title><categories>math.ST cs.CL stat.TH</categories><proxy>ccsd</proxy><journal-ref>Neurocomputing, Elsevier, 2014, 147, pp.120-135</journal-ref><doi>10.1016/j.neucom.2013.12.057</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is an extended version of a paper presented in the WSOM'2012
conference [1]. We display a combination of factorial projections, SOM
algorithm and graph techniques applied to a text mining problem. The corpus
contains 8 medieval manuscripts which were used to teach arithmetic techniques
to merchants. Among the techniques for Data Analysis, those used for
Lexicometry (such as Factorial Analysis) highlight the discrepancies between
manuscripts. The reason for this is that they focus on the deviation from the
independence between words and manuscripts. Still, we also want to discover and
characterize the common vocabulary among the whole corpus. Using the properties
of stochastic Kohonen maps, which define neighborhood between inputs in a
non-deterministic way, we highlight the words which seem to play a special role
in the vocabulary. We call them fickle and use them to improve both Kohonen map
robustness and significance of FCA visualization. Finally we use graph
algorithmic to exploit this fickleness for classification of words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07739</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07739</id><created>2015-06-25</created><updated>2015-06-30</updated><authors><author><keyname>Nyman</keyname><forenames>Thomas</forenames></author><author><keyname>McGillion</keyname><forenames>Brian</forenames></author><author><keyname>Asokan</keyname><forenames>N.</forenames></author></authors><title>On Making Emerging Trusted Execution Environments Accessible to
  Developers</title><categories>cs.CR</categories><comments>Author's version of article to appear in 8th Internation Conference
  of Trust &amp; Trustworthy Computing, TRUST 2015, Heraklion, Crete, Greece,
  August 24-26, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New types of Trusted Execution Environment (TEE) architectures like TrustLite
and Intel Software Guard Extensions (SGX) are emerging. They bring new features
that can lead to innovative security and privacy solutions. But each new TEE
environment comes with its own set of interfaces and programming paradigms,
thus raising the barrier for entry for developers who want to make use of these
TEEs. In this paper, we motivate the need for realizing standard TEE interfaces
on such emerging TEE architectures and show that this exercise is not
straightforward. We report on our on-going work in mapping GlobalPlatform
standard interfaces to TrustLite and SGX.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07742</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07742</id><created>2015-06-25</created><authors><author><keyname>Awan</keyname><forenames>Ahsan Javed</forenames></author><author><keyname>Brorsson</keyname><forenames>Mats</forenames></author><author><keyname>Vlassov</keyname><forenames>Vladimir</forenames></author><author><keyname>Ayguade</keyname><forenames>Eduard</forenames></author></authors><title>Performance Characterization of In-Memory Data Analytics on a Modern
  Cloud Server</title><categories>cs.DC cs.PF</categories><comments>Accepted to The 5th IEEE International Conference on Big Data and
  Cloud Computing (BDCloud 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In last decade, data analytics have rapidly progressed from traditional
disk-based processing to modern in-memory processing. However, little effort
has been devoted at enhancing performance at micro-architecture level. This
paper characterizes the performance of in-memory data analytics using Apache
Spark framework. We use a single node NUMA machine and identify the bottlenecks
hampering the scalability of workloads. We also quantify the inefficiencies at
micro-architecture level for various data analysis workloads. Through empirical
evaluation, we show that spark workloads do not scale linearly beyond twelve
threads, due to work time inflation and thread level load imbalance. Further,
at the micro-architecture level, we observe memory bound latency to be the
major cause of work time inflation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07749</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07749</id><created>2015-06-25</created><authors><author><keyname>Lange</keyname><forenames>Michael</forenames></author><author><keyname>Mitchell</keyname><forenames>Lawrence</forenames></author><author><keyname>Knepley</keyname><forenames>Matthew G.</forenames></author><author><keyname>Gorman</keyname><forenames>Gerard J.</forenames></author></authors><title>Efficient mesh management in Firedrake using PETSc-DMPlex</title><categories>cs.MS</categories><comments>12 pages, 6 figures, submitted to SISC CSE Special Issue</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of composable abstractions allows the application of new and
established algorithms to a wide range of problems while automatically
inheriting the benefits of well-known performance optimisations. This work
highlights the composition of the PETSc DMPlex domain topology abstraction with
the Firedrake automated finite element system to create a PDE solving
environment that combines expressiveness, flexibility and high performance. We
describe how Firedrake utilises DMPlex to provide the indirection maps required
for finite element assembly, while supporting various mesh input formats and
runtime domain decomposition. In particular, we describe how DMPlex and its
accompanying data structures allow the generic creation of user-defined
discretisations, while utilising data layout optimisations that improve cache
coherency and ensure overlapped communication during assembly computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07763</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07763</id><created>2015-06-25</created><authors><author><keyname>Bapierre</keyname><forenames>Halgurt</forenames></author><author><keyname>Jesdabodi</keyname><forenames>Chakajkla</forenames></author><author><keyname>Groh</keyname><forenames>Georg</forenames></author></authors><title>Mobile Homophily and Social Location Prediction</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mobility behavior of human beings is predictable to a varying degree e.g.
depending on the traits of their personality such as the trait extraversion -
introversion: the mobility of introvert users may be more dominated by routines
and habitual movement patterns, resulting in a more predictable mobility
behavior on the basis of their own location history while, in contrast,
extrovert users get about a lot and are explorative by nature, which may hamper
the prediction of their mobility. However, socially more active and extrovert
users meet more people and share information, experiences, believes, thoughts
etc. with others. which in turn leads to a high interdependency between their
mobility and social lives. Using a large LBSN dataset, his paper investigates
the interdependency between human mobility and social proximity, the influence
of social networks on enhancing location prediction of an individual and the
transmission of social trends/influences within social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07768</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07768</id><created>2015-06-23</created><authors><author><keyname>Pe&#xf1;a</keyname><forenames>Javier L&#xf3;pez</forenames></author><author><keyname>Navarro</keyname><forenames>Ra&#xfa;l S&#xe1;nchez</forenames></author></authors><title>Who can replace Xavi? A passing motif analysis of football players</title><categories>physics.soc-ph cs.SI</categories><comments>9 pages, 8 tables, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally, most of football statistical and media coverage has been
focused almost exclusively on goals and (ocassionally) shots. However, most of
the duration of a football game is spent away from the boxes, passing the ball
around. The way teams pass the ball around is the most characteristic
measurement of what a team's &quot;unique style&quot; is.
  In the present work we analyse passing sequences at the player level, using
the different passing frequencies as a &quot;digital fingerprint&quot; of a player's
style. The resulting numbers provide an adequate feature set which can be used
in order to construct a measure of similarity between players. Armed with such
a similarity tool, one can try to answer the question: Who might possibly
replace Xavi at FC Barcelona?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07773</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07773</id><created>2015-06-25</created><updated>2015-07-19</updated><authors><author><keyname>Kalra</keyname><forenames>Tushar</forenames></author><author><keyname>Mathew</keyname><forenames>Rogers</forenames></author><author><keyname>Pal</keyname><forenames>Sudebkumar Prasant</forenames></author><author><keyname>Pandey</keyname><forenames>Vijay</forenames></author></authors><title>Maximum weighted independent sets with a budget</title><categories>cs.CC cs.DS</categories><comments>12 pages</comments><msc-class>05C69, 05C85, 68W25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a graph $G$, a non-negative integer $k$, and a weight function that
maps each vertex in $G$ to a positive real number, the \emph{Maximum Weighted
Budgeted Independent Set (MWBIS) problem} is about finding a maximum weighted
independent set in $G$ of cardinality at most $k$. A special case of MWBIS,
when the weight assigned to each vertex is equal to its degree in $G$, is
called the \emph{Maximum Independent Vertex Coverage (MIVC)} problem. In other
words, the MIVC problem is about finding an independent set of cardinality at
most $k$ with maximum coverage.
  Since it is a generalization of the well-known Maximum Weighted Independent
Set (MWIS) problem, MWBIS too does not have any constant factor polynomial time
approximation algorithm assuming $P \neq NP$. In this paper, we study MWBIS in
the context of bipartite graphs. We show that, unlike MWIS, the MIVC (and
thereby the MWBIS) problem in bipartite graphs is NP-hard. Then, we show that
the MWBIS problem admits a $\frac{1}{2}$-factor approximation algorithm in the
class of bipartite graphs, which matches the integrality gap of a natural LP
relaxation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07774</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07774</id><created>2015-06-25</created><authors><author><keyname>Haase</keyname><forenames>Christoph</forenames></author><author><keyname>Hofman</keyname><forenames>Piotr</forenames></author></authors><title>Tightening the Complexity of Equivalence Problems for Commutative
  Grammars</title><categories>cs.FL cs.LO</categories><comments>21 pages</comments><acm-class>F.1.1; F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the language equivalence problem for regular and context-free
commutative grammars is coNEXP-complete. In addition, our lower bound
immediately yields further coNEXP-completeness results for equivalence problems
for communication-free Petri nets and reversal-bounded counter automata.
Moreover, we improve both lower and upper bounds for language equivalence for
exponent-sensitive commutative grammars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07776</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07776</id><created>2015-06-25</created><authors><author><keyname>Genova</keyname><forenames>Kyle</forenames></author><author><keyname>Williamson</keyname><forenames>David P.</forenames></author></authors><title>An Experimental Evaluation of the Best-of-Many Christofides' Algorithm
  for the Traveling Salesman Problem</title><categories>cs.DS math.OC</categories><comments>An extended abstract of this paper will appear in ESA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent papers on approximation algorithms for the traveling salesman problem
(TSP) have given a new variant on the well-known Christofides' algorithm for
the TSP, called the Best-of-Many Christofides' algorithm. The algorithm
involves sampling a spanning tree from the solution the standard LP relaxation
of the TSP, subject to the condition that each edge is sampled with probability
at most its value in the LP relaxation. One then runs Christofides' algorithm
on the tree by computing a minimum-cost matching on the odd-degree vertices in
the tree, and shortcutting the resulting Eulerian graph to a tour. In this
paper we perform an experimental evaluation of the Best-of-Many Christofides'
algorithm to see if there are empirical reasons to believe its performance is
better than that of Christofides' algorithm. Furthermore, several different
sampling schemes have been proposed; we implement several different schemes to
determine which ones might be the most promising for obtaining improved
performance guarantees over that of Christofides' algorithm. In our
experiments, all of the implemented methods perform significantly better than
the Christofides' algorithm; an algorithm that samples from a maximum entropy
distribution over spanning trees seems to be particularly good, though there
are others that perform almost as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07781</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07781</id><created>2015-06-25</created><authors><author><keyname>Castro</keyname><forenames>Francisco Enrique Vicente G.</forenames></author><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>Microsimulations of Arching, Clogging, and Bursty Exit Phenomena in
  Crowd Dynamics</title><categories>cs.MA physics.soc-ph</categories><comments>6 pages, 6 figures, original paper appeared in Proceedings of the
  10th National Conference on Information Technology Education (NCITE 2012),
  Laoag City, Ilocos Norte, Philippines, 18-20 October 2012. (ISSN 2012-0761)</comments><journal-ref>Philippine Information Technology Journal 6(1):11-16</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present in this paper the behavior of an artificial agent who is a member
of a crowd. The behavior is based on the social comparison theory, as well as
the trajectory mapping towards an agent's goal considering the agent's field of
vision. The crowd of artificial agents were able to exhibit arching, clogging,
and bursty exit rates. We were also able to observe a new phenomenon we called
double arching, which happens towards the end of the simulation, and whose
onset is exhibited by a &quot;calm&quot; density graph within the exit passage. The
density graph is usually bursty at this area. Because of these exhibited
phenomena, we can use these agents with high confidence to perform
microsimulation studies for modeling the behavior of humans and objects in very
realistic ways.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07810</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07810</id><created>2015-06-25</created><authors><author><keyname>Elberfeld</keyname><forenames>Michael</forenames></author><author><keyname>Schweitzer</keyname><forenames>Pascal</forenames></author></authors><title>Canonizing Graphs of Bounded Tree Width in Logspace</title><categories>cs.CC cs.DM cs.DS math.CO</categories><comments>26 pages</comments><msc-class>05C60, 05C85, 68R10</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph canonization is the problem of computing a unique representative, a
canon, from the isomorphism class of a given graph. This implies that two
graphs are isomorphic exactly if their canons are equal. We show that graphs of
bounded tree width can be canonized by logarithmic-space (logspace) algorithms.
This implies that the isomorphism problem for graphs of bounded tree width can
be decided in logspace. In the light of isomorphism for trees being hard for
the complexity class logspace, this makes the ubiquitous class of graphs of
bounded tree width one of the few classes of graphs for which the complexity of
the isomorphism problem has been exactly determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07813</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07813</id><created>2015-06-25</created><authors><author><keyname>Politz</keyname><forenames>Joe Gibbs</forenames></author><author><keyname>Eliopoulos</keyname><forenames>Spiridon</forenames></author><author><keyname>Guha</keyname><forenames>Arjun</forenames></author><author><keyname>Krishnamurthi</keyname><forenames>Shriram</forenames></author></authors><title>ADsafety: Type-Based Verification of JavaScript Sandboxing</title><categories>cs.PL</categories><comments>in Proceedings of the USENIX Security Symposium (2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web sites routinely incorporate JavaScript programs from several sources into
a single page. These sources must be protected from one another, which requires
robust sandboxing. The many entry-points of sandboxes and the subtleties of
JavaScript demand robust verification of the actual sandbox source. We use a
novel type system for JavaScript to encode and verify sandboxing properties.
The resulting verifier is lightweight and efficient, and operates on actual
source. We demonstrate the effectiveness of our technique by applying it to
ADsafe, which revealed several bugs and other weaknesses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07816</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07816</id><created>2015-06-25</created><authors><author><keyname>Gasparis</keyname><forenames>Ioannis</forenames></author><author><keyname>Kozat</keyname><forenames>Ulas C.</forenames></author><author><keyname>Sunay</keyname><forenames>M. Oguz</forenames></author></authors><title>Programming Flows in Dense Mobile Environments: A Multi-user Diversity
  Perspective</title><categories>cs.NI</categories><comments>Accepted and presented in IEEE ICC'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of OpenFlow and Software Defined Networks brings new
perspectives into how we design the next generation networks, where the number
of base stations/access points as well as the devices per subscriber will be
dramatically higher. In such dense environments, devices can communicate with
each other directly and can attach to multiple base stations (or access points)
for simultaneous data communication over multiple paths. This paper explores
how networks can maximally enable this multi-path diversity through network
programmability. In particular, we propose programmable flow clustering and set
policies for inter-group as well as intra-group wireless scheduling. Further,
we propose programmable demultiplexing of a single network flow onto multiple
paths before the congestion areas and multiplexing them together post
congestion areas. We show the benefits of such programmability first for legacy
applications that cannot take advantage of multi-homing without such
programmability. We then evaluate the benefits for smart applications that take
advantage of multi-homing by either opening multiple TCP connections over
multiple paths or utilizing a transport protocol such as MP-TCP designed for
supporting such environments. More specifically, we built an emulation
environment over Mininet for our experiments. Our evaluations using synthetic
and trace driven channel models indicate that the proposed programmability in
wireless scheduling and flow splitting can increase the throughput
substantially for both the legacy applications and the current state of the
art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07823</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07823</id><created>2015-06-25</created><authors><author><keyname>De Abreu</keyname><forenames>Ana</forenames></author><author><keyname>Toni</keyname><forenames>Laura</forenames></author><author><keyname>Thomos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Maugey</keyname><forenames>Thomas</forenames></author><author><keyname>Pereira</keyname><forenames>Fernando</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Optimal Layered Representation for Adaptive Interactive Multiview Video
  Streaming</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an interactive multiview video streaming (IMVS) system where
clients select their preferred viewpoint in a given navigation window. To
provide high quality IMVS, many high quality views should be transmitted to the
clients. However, this is not always possible due to the limited and
heterogeneous capabilities of the clients. In this paper, we propose a novel
adaptive IMVS solution based on a layered multiview representation where camera
views are organized into layered subsets to match the different clients
constraints. We formulate an optimization problem for the joint selection of
the views subsets and their encoding rates. Then, we propose an optimal and a
reduced computational complexity greedy algorithms, both based on
dynamic-programming. Simulation results show the good performance of our novel
algorithms compared to a baseline algorithm, proving that an effective IMVS
adaptive solution should consider the scene content and the client capabilities
and their preferences in navigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07840</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07840</id><created>2015-06-25</created><authors><author><keyname>Mishne</keyname><forenames>Gal</forenames></author><author><keyname>Shaham</keyname><forenames>Uri</forenames></author><author><keyname>Cloninger</keyname><forenames>Alexander</forenames></author><author><keyname>Cohen</keyname><forenames>Israel</forenames></author></authors><title>Diffusion Nets</title><categories>stat.ML cs.LG math.CA</categories><comments>24 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-linear manifold learning enables high-dimensional data analysis, but
requires out-of-sample-extension methods to process new data points. In this
paper, we propose a manifold learning algorithm based on deep learning to
create an encoder, which maps a high-dimensional dataset and its
low-dimensional embedding, and a decoder, which takes the embedded data back to
the high-dimensional space. Stacking the encoder and decoder together
constructs an autoencoder, which we term a diffusion net, that performs
out-of-sample-extension as well as outlier detection. We introduce new neural
net constraints for the encoder, which preserves the local geometry of the
points, and we prove rates of convergence for the encoder. Also, our approach
is efficient in both computational complexity and memory requirements, as
opposed to previous methods that require storage of all training points in both
the high-dimensional and the low-dimensional spaces to calculate the
out-of-sample-extension and the pre-image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07853</identifier>
 <datestamp>2015-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07853</id><created>2015-06-25</created><updated>2015-07-07</updated><authors><author><keyname>Kn&#xf6;ppel</keyname><forenames>Felix</forenames></author><author><keyname>Pinkall</keyname><forenames>Ulrich</forenames></author></authors><title>Complex Line Bundles over Simplicial Complexes and their Applications</title><categories>math.DG cs.DM</categories><msc-class>53C05, 55R15, 68R01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discrete vector bundles are important in Physics and recently found
remarkable applications in Computer Graphics. This article approaches discrete
bundles from the viewpoint of Discrete Differential Geometry, including a
complete classification of discrete vector bundles over finite simplicial
complexes. In particular, we obtain a discrete analogue of a theorem of Andr\'e
Weil on the classification of hermitian line bundles. Moreover, we associate to
each discrete hermitian line bundle with curvature a unique piecewise-smooth
hermitian line bundle of piecewise constant curvature. This is then used to
define a discrete Dirichlet energy which generalizes the well-known cotangent
Laplace operator to discrete hermitian line bundles over Euclidean simplicial
manifolds of arbitrary dimension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07861</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07861</id><created>2015-06-24</created><updated>2015-09-10</updated><authors><author><keyname>Laurenti</keyname><forenames>Luca</forenames></author><author><keyname>Cardelli</keyname><forenames>Luca</forenames></author><author><keyname>Kwiatkowska</keyname><forenames>Marta</forenames></author></authors><title>Stochastic Analysis of Chemical Reaction Networks Using Linear Noise
  Approximation</title><categories>cs.LO q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic evolution of Chemical Reactions Networks (CRNs) over time is
usually analysed through solving the Chemical Master Equation (CME) or
performing extensive simulations. Analysing stochasticity is often needed,
particularly when some molecules occur in low numbers. Unfortunately, both
approaches become infeasible if the system is complex and/or it cannot be
ensured that initial populations are small. We develop a probabilistic logic
for CRNs that enables stochastic analysis of the evolution of populations of
molecular species. We present an approximate model checking algorithm based on
the Linear Noise Approximation (LNA) of the CME, whose computational complexity
is independent of the population size of each species and polynomial in the
number of different species. The algorithm requires the solution of first order
polynomial differential equations. We prove that our approach is valid for any
CRN close enough to the thermodynamical limit. However, we show on four case
studies that it can still provide good approximation even for low molecule
counts. Our approach enables rigorous analysis of CRNs that are not analyzable
by solving the CME, but are far from the deterministic limit. Moreover, it can
be used for a fast approximate stochastic characterization of a CRN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07866</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07866</id><created>2015-06-25</created><updated>2015-11-24</updated><authors><author><keyname>Ben-Artzi</keyname><forenames>Gil</forenames></author><author><keyname>Kasten</keyname><forenames>Yoni</forenames></author><author><keyname>Peleg</keyname><forenames>Shmuel</forenames></author><author><keyname>Werman</keyname><forenames>Michael</forenames></author></authors><title>Camera Calibration from Dynamic Silhouettes Using Motion Barcodes</title><categories>cs.CV</categories><comments>Update metadata</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing the epipolar geometry between cameras with very different
viewpoints is often problematic as matching points are hard to find. In these
cases, it has been proposed to use information from dynamic objects in the
scene for suggesting point and line correspondences.
  We propose a speed up of about two orders of magnitude, as well as an
increase in robustness and accuracy, to methods computing epipolar geometry
from dynamic silhouettes. This improvement is based on a new temporal
signature: motion barcode for lines. Motion barcode is a binary temporal
sequence for lines, indicating for each frame the existence of at least one
foreground pixel on that line. The motion barcodes of two corresponding
epipolar lines are very similar, so the search for corresponding epipolar lines
can be limited only to lines having similar barcodes. The use of motion
barcodes leads to increased speed, accuracy, and robustness in computing the
epipolar geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07867</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07867</id><created>2015-06-24</created><authors><author><keyname>Becker</keyname><forenames>Martin</forenames></author><author><keyname>Neumair</keyname><forenames>Markus</forenames></author><author><keyname>S&#xf6;hn</keyname><forenames>Alexander</forenames></author><author><keyname>Chakraborty</keyname><forenames>Samarjit</forenames></author></authors><title>Approaches for Software Verification of an Emergency Recovery System for
  Micro Air Vehicles</title><categories>cs.SE</categories><comments>12 pages, 34th International Conference on Computer Safety,
  Reliability and Security (SAFECOMP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the development and verification of a competitive
parachute system for Micro Air Vehicles, in particular focusing on verification
of the embedded software. We first introduce the overall solution including a
system level failure analysis, and then show how we minimized the influence of
faulty software. This paper demonstrates that with careful abstraction and
little overapproximation, the entire code running on a microprocessor can be
verified using bounded model checking, and that this is a useful approach for
resource-constrained embedded systems. he resulting Emergency Recovery System
is to our best knowledge the first of its kind that passed formal verification,
and furthermore is superior to all other existing solutions (including
commercially available ones) from an operational point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07895</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07895</id><created>2015-06-25</created><authors><author><keyname>Adamek</keyname><forenames>Jordan</forenames><affiliation>NPA, LINCS, IUF, LIP6</affiliation></author><author><keyname>Nesterenko</keyname><forenames>Mikhail</forenames><affiliation>NPA, LINCS, IUF, LIP6</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>NPA, LINCS, IUF, LIP6</affiliation></author></authors><title>Stateless Geocasting</title><categories>cs.DC cs.DS cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present two stateless algorithms that guarantee to deliver the message to
every device in a designated geographic area: flooding and planar geocasting.
Due to the algorithms' statelessness, intermediate devices do not have to keep
message data between message transmissions. We formally prove the algorithms
correct, estimate their message complexity and evaluate their performance
through simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07898</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07898</id><created>2015-06-25</created><updated>2016-01-30</updated><authors><author><keyname>M&#xfc;tze</keyname><forenames>Torsten</forenames></author><author><keyname>Nummenpalo</keyname><forenames>Jerri</forenames></author></authors><title>Efficient computation of middle levels Gray codes</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For any integer $n\geq 1$ a middle levels Gray code is a cyclic listing of
all bitstrings of length $2n+1$ that have either $n$ or $n+1$ entries equal to
1 such that any two consecutive bitstrings in the list differ in exactly one
bit. The question whether such a Gray code exists for every $n\geq 1$ has been
the subject of intensive research during the last 30 years, and has been
answered affirmatively only recently [T. M\&quot;utze. Proof of the middle levels
conjecture. arXiv:1404.4442, 2014]. In this work we provide the first efficient
algorithm to compute a middle levels Gray code. For a given bitstring, our
algorithm computes the next $\ell$ bitstrings in the Gray code in time
$\mathcal{O}(n\ell(1+\frac{n}{\ell}))$, which is $\mathcal{O}(n)$ on average
per bitstring provided that $\ell=\Omega(n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07902</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07902</id><created>2015-06-25</created><updated>2016-01-25</updated><authors><author><keyname>Krishnamurthy</keyname><forenames>Akshay</forenames></author></authors><title>Minimax Structured Normal Means Inference</title><categories>stat.ML cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a unified treatment of a broad class of noisy structure recovery
problems, known as structured normal means problems. In this setting, the goal
is to identify, from a finite collection of Gaussian distributions with
different means, the distribution that produced some observed data. Recent work
has studied several special cases including sparse vectors, biclusters, and
graph-based structures. We establish nearly matching upper and lower bounds on
the minimax probability of error for any structured normal means problem, and
we derive an optimality certificate for the maximum likelihood estimator, which
can be applied to many instantiations. We also consider an experimental design
setting, where we generalize our minimax bounds and derive an algorithm for
computing a design strategy with a certain optimality property. We show that
our results give tight minimax bounds for many structure recovery problems and
consider some consequences for interactive sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07903</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07903</id><created>2015-06-25</created><updated>2015-10-16</updated><authors><author><keyname>Dumitrescu</keyname><forenames>Adrian</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>Csaba D.</forenames></author></authors><title>Constant-Factor Approximation for TSP with Disks</title><categories>cs.CG</categories><comments>14 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the traveling salesman problem with neighborhoods (TSPN) and
present the first constant-ratio approximation for disks in the plane: Given a
set of $n$ disks in the plane, a TSP tour whose length is at most $O(1)$ times
the optimal with high probability can be computed in time that is polynomial in
$n$. Our result is the first constant-ratio approximation for a class of planar
convex bodies of arbitrary size and arbitrary intersections. In order to
achieve a $O(1)$-approximation, we reduce the traveling salesman problem with
disks, up to constant factors, to a minimum weight hitting set problem in a
geometric hypergraph. The connection between TSPN and hitting sets in geometric
hypergraphs, established here, is likely to have future applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07905</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07905</id><created>2015-06-25</created><authors><author><keyname>Folwarczn&#xfd;</keyname><forenames>Luk&#xe1;&#x161;</forenames></author><author><keyname>Sgall</keyname><forenames>Ji&#x159;&#xed;</forenames></author></authors><title>General Caching Is Hard: Even with Small Pages</title><categories>cs.CC cs.DS</categories><comments>19 pages, 8 figures, an extended abstract appeared in the proceedings
  of MAPSP 2015 (www.mapsp2015.com), a conference version has been submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching (also known as paging) is a classical problem concerning page
replacement policies in two-level memory systems. General caching is the
variant with pages of different sizes and fault costs. We give the first
NP-hardness result for general caching with small pages: General caching is
(strongly) NP-hard even when page sizes are limited to {1, 2, 3}. It holds
already in the fault model (each page has unit fault cost) as well as in the
bit model (each page has the same fault cost as size). We also give a very
short proof of the strong NP-hardness of general caching with page sizes
restricted to {1, 2, 3} and arbitrary costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07911</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07911</id><created>2015-06-25</created><authors><author><keyname>Ford</keyname><forenames>Russell</forenames></author><author><keyname>Gomez-Cuba</keyname><forenames>Felipe</forenames></author><author><keyname>Mezzavilla</keyname><forenames>Marco</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author></authors><title>Dynamic Time-domain Duplexing for Self-backhauled Millimeter Wave
  Cellular Networks</title><categories>cs.NI</categories><comments>IEEE Workshop on Next Generation Backhaul/Fronthaul Networks -
  BackNets 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave (mmW) bands between 30 and 300 GHz have attracted
considerable attention for next-generation cellular networks due to vast
quantities of available spectrum and the possibility of very high-dimensional
antenna ar-rays. However, a key issue in these systems is range: mmW signals
are extremely vulnerable to shadowing and poor high-frequency propagation.
Multi-hop relaying is therefore a natural technology for such systems to
improve cell range and cell edge rates without the addition of wired access
points. This paper studies the problem of scheduling for a simple
infrastructure cellular relay system where communication between wired base
stations and User Equipment follow a hierarchical tree structure through fixed
relay nodes. Such a systems builds naturally on existing cellular mmW backhaul
by adding mmW in the access links. A key feature of the proposed system is that
TDD duplexing selections can be made on a link-by-link basis due to directional
isolation from other links. We devise an efficient, greedy algorithm for
centralized scheduling that maximizes network utility by jointly optimizing the
duplexing schedule and resources allocation for dense, relay-enhanced OFDMA/TDD
mmW networks. The proposed algorithm can dynamically adapt to loading, channel
conditions and traffic demands. Significant throughput gains and improved
resource utilization offered by our algorithm over the static,
globally-synchronized TDD patterns are demonstrated through simulations based
on empirically-derived channel models at 28 GHz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07915</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07915</id><created>2015-06-25</created><authors><author><keyname>Rodrigues</keyname><forenames>Jose</forenames></author><author><keyname>Romani</keyname><forenames>Luciana</forenames></author><author><keyname>Traina</keyname><forenames>Agma</forenames></author><author><keyname>Traina</keyname><forenames>Caetano</forenames></author></authors><title>Combining Visual Analytics and Content Based Data Retrieval Technology
  for Efficient Data Analysis</title><categories>cs.GR</categories><comments>Published as Jose Rodrigues, Luciana A. S. Romani, Agma Juci Machado
  Traina, Caetano Traina Jr (2010), Combining Visual Analytics and Content
  Based Data Retrieval Technology for Efficient Data Analysis, 14th Int Conf on
  Inf Visualisation, 61-67</comments><journal-ref>14th Int Conf on Inf Visualisation, 61-67 IEEE Press (2010)</journal-ref><doi>10.1109/IV.2010.101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most useful techniques to help visual data analysis systems is
interactive filtering (brushing). However, visualization techniques often
suffer from overlap of graphical items and multiple attributes complexity,
making visual selection inefficient. In these situations, the benefits of data
visualization are not fully observable because the graphical items do not pop
up as comprehensive patterns. In this work we propose the use of content-based
data retrieval technology combined with visual analytics. The idea is to use
the similarity query functionalities provided by metric space systems in order
to select regions of the data domain according to user-guidance and interests.
After that, the data found in such regions feed multiple visualization
workspaces so that the user can inspect the correspondent datasets. Our
experiments showed that the methodology can break the visual analysis process
into smaller problems (views) and that the views hold the expectations of the
analyst according to his/her similarity query selection, improving data
perception and analytical possibilities. Our contribution introduces a
principle that can be used in all sorts of visualization techniques and
systems, this principle can be extended with different kinds of integration
visualization-metric-space, and with different metrics, expanding the
possibilities of visual data analysis in aspects such as semantics and
scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07924</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07924</id><created>2015-06-25</created><authors><author><keyname>Arslan</keyname><forenames>G&#xfc;rdal</forenames></author><author><keyname>Y&#xfc;ksel</keyname><forenames>Serdar</forenames></author></authors><title>Decentralized Q-Learning for Stochastic Dynamic Games</title><categories>math.OC cs.GT cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are only a few learning algorithms applicable to stochastic dynamic
games. Learning in games is generally difficult because of the non-stationary
environment in which each decision maker aims to learn its optimal decisions
with minimal information in the presence of the other decision makers who are
also learning. In the case of dynamic games, learning is more challenging
because, while learning, the decision makers alter the state of the system and
hence the future cost. In this paper, we present decentralized Q-learning
algorithms for stochastic dynamic games, and study their convergence for the
weakly acyclic case. We show that the decision makers employing these
algorithms would eventually be using equilibrium policies almost surely in
large classes of stochastic dynamic games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07933</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07933</id><created>2015-06-25</created><updated>2015-09-22</updated><authors><author><keyname>Gholami</keyname><forenames>Amir</forenames></author><author><keyname>Hill</keyname><forenames>Judith</forenames></author><author><keyname>Malhotra</keyname><forenames>Dhairya</forenames></author><author><keyname>Biros</keyname><forenames>George</forenames></author></authors><title>AccFFT: A library for distributed-memory FFT on CPU and GPU
  architectures</title><categories>cs.DC</categories><comments>Parallel FFT Library</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new library for parallel distributed Fast Fourier Transforms
(FFT). Despite the large amount of work on FFTs, we show that significant
speedups can be achieved for distributed transforms. The importance of FFT in
science and engineering and the advances in high performance computing
necessitate further improvements. AccFFT extends existing FFT libraries for x86
architectures (CPUs) and CUDA-enabled Graphics Processing Units (GPUs) to
distributed memory clusters using the Message Passing Interface (MPI). Our
library uses specifically optimized all-to-all communication algorithms, to
efficiently perform the communication phase of the distributed FFT algorithm.
The GPU based algorithm, effectively hides the overhead of PCIe transfers. We
present numerical results on the Maverick and Stampede platforms at the Texas
Advanced Computing Center (TACC) and on the Titan system at the Oak Ridge
National Laboratory (ORNL). We compare the CPU version of AccFFT with P3DFFT
and PFFT libraries and we show a consistent $2-3\times$ speedup across a range
of processor counts and problem sizes. The comparison of the GPU code with FFTE
library shows a similar trend with a $2\times$ speedup. The library is tested
up to 131K cores and 4,096 GPUs of Titan, and up to 16K cores of Stampede.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07942</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07942</id><created>2015-06-25</created><authors><author><keyname>Yamamoto</keyname><forenames>Koji</forenames></author></authors><title>A Comprehensive Survey of Potential Game Approaches to Wireless Networks</title><categories>cs.GT cs.NI</categories><comments>44 pages, 6 figures, to appear in IEICE Transactions on
  Communications, vol. E98-B, no. 9, Sept. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Potential games form a class of non-cooperative games where unilateral
improvement dynamics are guaranteed to converge in many practical cases. The
potential game approach has been applied to a wide range of wireless network
problems, particularly to a variety of channel assignment problems. In this
paper, the properties of potential games are introduced, and games in wireless
networks that have been proven to be potential games are comprehensively
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07943</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07943</id><created>2015-06-25</created><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Zhan</keyname><forenames>Jianfeng</forenames></author><author><keyname>Jia</keyname><forenames>Zhen</forenames></author><author><keyname>Han</keyname><forenames>Rui</forenames></author></authors><title>Characterization and Architectural Implications of Big Data Workloads</title><categories>cs.DC cs.DB cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big data areas are expanding in a fast way in terms of increasing workloads
and runtime systems, and this situation imposes a serious challenge to workload
characterization, which is the foundation of innovative system and architecture
design. The previous major efforts on big data benchmarking either propose a
comprehensive but a large amount of workloads, or only select a few workloads
according to so-called popularity, which may lead to partial or even biased
observations. In this paper, on the basis of a comprehensive big data benchmark
suite---BigDataBench, we reduced 77 workloads to 17 representative workloads
from a micro-architectural perspective. On a typical state-of-practice
platform---Intel Xeon E5645, we compare the representative big data workloads
with SPECINT, SPECCFP, PARSEC, CloudSuite and HPCC. After a comprehensive
workload characterization, we have the following observations. First, the big
data workloads are data movement dominated computing with more branch
operations, taking up to 92% percentage in terms of instruction mix, which
places them in a different class from Desktop (SPEC CPU2006), CMP (PARSEC), HPC
(HPCC) workloads. Second, corroborating the previous work, Hadoop and Spark
based big data workloads have higher front-end stalls. Comparing with the
traditional workloads i. e. PARSEC, the big data workloads have larger
instructions footprint. But we also note that, in addition to varied
instruction-level parallelism, there are significant disparities of front-end
efficiencies among different big data workloads. Third, we found complex
software stacks that fail to use state-of-practise processors efficiently are
one of the main factors leading to high front-end stalls. For the same
workloads, the L1I cache miss rates have one order of magnitude differences
among diverse implementations with different software stacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07947</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07947</id><created>2015-06-25</created><authors><author><keyname>Oh</keyname><forenames>Sewoong</forenames></author><author><keyname>Thekumparampil</keyname><forenames>Kiran K.</forenames></author><author><keyname>Xu</keyname><forenames>Jiaming</forenames></author></authors><title>Collaboratively Learning Preferences from Ordinal Data</title><categories>cs.LG cs.IT math.IT stat.ML</categories><comments>38 pages 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In applications such as recommendation systems and revenue management, it is
important to predict preferences on items that have not been seen by a user or
predict outcomes of comparisons among those that have never been compared. A
popular discrete choice model of multinomial logit model captures the structure
of the hidden preferences with a low-rank matrix. In order to predict the
preferences, we want to learn the underlying model from noisy observations of
the low-rank matrix, collected as revealed preferences in various forms of
ordinal data. A natural approach to learn such a model is to solve a convex
relaxation of nuclear norm minimization. We present the convex relaxation
approach in two contexts of interest: collaborative ranking and bundled choice
modeling. In both cases, we show that the convex relaxation is minimax optimal.
We prove an upper bound on the resulting error with finite samples, and provide
a matching information-theoretic lower bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07950</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07950</id><created>2015-06-25</created><authors><author><keyname>Korytkowski</keyname><forenames>Marcin</forenames></author><author><keyname>Scherer</keyname><forenames>Rafal</forenames></author><author><keyname>Staszewski</keyname><forenames>Pawel</forenames></author><author><keyname>Woldan</keyname><forenames>Piotr</forenames></author></authors><title>Bag-of-Features Image Indexing and Classification in Microsoft SQL
  Server Relational Database</title><categories>cs.DB cs.CV</categories><comments>2015 IEEE 2nd International Conference on Cybernetics (CYBCONF),
  Gdynia, Poland, 24-26 June 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel relational database architecture aimed to visual
objects classification and retrieval. The framework is based on the
bag-of-features image representation model combined with the Support Vector
Machine classification and is integrated in a Microsoft SQL Server database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07952</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07952</id><created>2015-06-26</created><authors><author><keyname>Miller</keyname><forenames>Avery</forenames></author><author><keyname>Pelc</keyname><forenames>Andrzej</forenames></author></authors><title>Tradeoffs Between Cost and Information for Rendezvous and Treasure Hunt</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In rendezvous, two agents traverse network edges in synchronous rounds and
have to meet at some node. In treasure hunt, a single agent has to find a
stationary target situated at an unknown node of the network. We study
tradeoffs between the amount of information ($\mathit{advice}$) available
$\mathit{a\ priori}$ to the agents and the cost (number of edge traversals) of
rendezvous and treasure hunt. Our goal is to find the smallest size of advice
which enables the agents to solve these tasks at some cost $C$ in a network
with $e$ edges. This size turns out to depend on the initial distance $D$ and
on the ratio $\frac{e}{C}$, which is the $\mathit{relative\ cost\ gain}$ due to
advice. For arbitrary graphs, we give upper and lower bounds of $O(D\log(D\cdot
\frac{e}{C}) +\log\log e)$ and $\Omega(D\log \frac{e}{C})$, respectively, on
the optimal size of advice. For the class of trees, we give nearly tight upper
and lower bounds of $O(D\log \frac{e}{C} + \log\log e)$ and $\Omega (D\log
\frac{e}{C})$, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07955</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07955</id><created>2015-06-26</created><authors><author><keyname>Li</keyname><forenames>Yuzhe</forenames></author><author><keyname>Quevedo</keyname><forenames>Daniel E.</forenames></author><author><keyname>Dey</keyname><forenames>Subhrakanti</forenames></author><author><keyname>Shi</keyname><forenames>Ling</forenames></author></authors><title>Fake-Acknowledgment Attack on ACK-based Sensor Power Schedule for Remote
  State Estimation</title><categories>cs.SY math.OC</categories><comments>submitted to IEEE CDC 2015</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We consider a class of malicious attacks against remote state estimation. A
sensor with limited resources adopts an acknowledgement (ACK)-based online
power schedule to improve the remote state estimation performance. A malicious
attacker can modify the ACKs from the remote estimator and convey fake
information to the sensor. When the capability of the attacker is limited, we
propose an attack strategy for the attacker and analyze the corresponding
effect on the estimation performance. The possible responses of the sensor are
studied and a condition for the sensor to discard ACKs and switch from online
schedule to offline schedule is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07957</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07957</id><created>2015-06-26</created><authors><author><keyname>Hajisheykhi</keyname><forenames>Reza</forenames></author><author><keyname>Roohitavaf</keyname><forenames>Mohammad</forenames></author><author><keyname>Kulkarni</keyname><forenames>Sandeep</forenames></author></authors><title>Auditable Restoration of Distributed Programs</title><categories>cs.DC</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on a protocol for auditable restoration of distributed systems. The
need for such protocol arises due to conflicting requirements (e.g., access to
the system should be restricted but emergency access should be provided). One
can design such systems with a tamper detection approach (based on the
intuition of &quot;break the glass door&quot;). However, in a distributed system, such
tampering, which are denoted as auditable events, is visible only for a single
node. This is unacceptable since the actions they take in these situations can
be different than those in the normal mode. Moreover, eventually, the auditable
event needs to be cleared so that system resumes the normal operation.
  With this motivation, in this paper, we present a protocol for auditable
restoration, where any process can potentially identify an auditable event.
Whenever a new auditable event occurs, the system must reach an &quot;auditable
state&quot; where every process is aware of the auditable event. Only after the
system reaches an auditable state, it can begin the operation of restoration.
Although any process can observe an auditable event, we require that only
&quot;authorized&quot; processes can begin the task of restoration. Moreover, these
processes can begin the restoration only when the system is in an auditable
state. Our protocol is self-stabilizing and has bounded state space. It can
effectively handle the case where faults or auditable events occur during the
restoration protocol. Moreover, it can be used to provide auditable restoration
to other distributed protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07964</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07964</id><created>2015-06-26</created><authors><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>A Framework for a Multiagent-based Scheduling of Parallel Jobs</title><categories>cs.DC</categories><comments>8 pages, 8 figures, in R.P. Salda\~na (ed.) Proceedings of the 6th
  Philippine Computing Science Congress (PCSC 2006), Ateneo De Manila
  University, Loyola Heights, Quezon City, Philippines, 28-29 March 2006, pp.
  81-88 (CDROM ISSN 1908-1146)</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper presents a multiagent approach as a paradigm for scheduling
parallel jobs in a parallel system. Scheduling parallel jobs is performed as a
means to balance the load of a system in order to improve the performance of a
parallel application. Parallel job scheduling is presented as a mapping between
two graphs: one represents the dependency of jobs and the other represents the
interconnection among processors. The usual implementation of parallel job
scheduling algorithms is via the master-slave paradigm. The master-slave
paradigm has inherent communication bottleneck that reduces the performance of
the system when more processors are needed to process the jobs. The multiagent
approach attempts to distribute the communication latency among the processors
which improves the performance of the system as the number of participating
processors increases. Presented in this paper is a framework for the behavior
of an autonomous agent that cooperates with other agents to achieve a community
goal of minimizing the processing time. Achieving this goal means an agent must
truthfully share information with other agents via {\em normalization}, {\em
task sharing}, and {\em result sharing} procedures. The agents consider a
parallel scientific application as a finite-horizon game where truthful
information sharing results into performance improvement for the parallel
application. The performance of the multiagent-based algorithm is compared to
that of an existing one via a simulation of the wavepacket dynamics using the
quantum trajectory method (QTM) as a test application. The average parallel
cost of running the QTM using the multiagent-based system is lower at higher
number of processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07980</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07980</id><created>2015-06-26</created><authors><author><keyname>Pereira</keyname><forenames>Jos&#xe9; C.</forenames></author><author><keyname>Lobo</keyname><forenames>Fernando G.</forenames></author></authors><title>A Java Implementation of the SGA, UMDA, ECGA, and HBOA</title><categories>cs.NE cs.MS</categories><comments>6 pages</comments><acm-class>I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Simple Genetic Algorithm, the Univariate Marginal Distribution Algorithm,
the Extended Compact Genetic Algorithm, and the Hierarchical Bayesian
Optimization Algorithm are all well known Evolutionary Algorithms.
  In this report we present a Java implementation of these four algorithms with
detailed instructions on how to use each of them to solve a given set of
optimization problems. Additionally, it is explained how to implement and
integrate new problems within the provided set. The source and binary files of
the Java implementations are available for free download at
https://github.com/JoseCPereira/2015EvolutionaryAlgorithmsJava.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.07990</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.07990</id><created>2015-06-26</created><updated>2016-02-25</updated><authors><author><keyname>Andersen</keyname><forenames>Mikkel Birkegaard</forenames></author><author><keyname>Bolander</keyname><forenames>Thomas</forenames></author><author><keyname>van Ditmarsch</keyname><forenames>Hans</forenames></author><author><keyname>Jensen</keyname><forenames>Martin Holm</forenames></author></authors><title>Bisimulation and expressivity for conditional belief, degrees of belief,
  and safe belief</title><categories>cs.AI cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Plausibility models are Kripke models that agents use to reason about
knowledge and belief, both of themselves and of each other. Such models are
used to interpret the notions of conditional belief, degrees of belief, and
safe belief. The logic of conditional belief contains that modality and also
the knowledge modality, and similarly for the logic of degrees of belief and
the logic of safe belief. With respect to these logics, plausibility models may
contain too much information. A proper notion of bisimulation is required that
characterises them. We define that notion of bisimulation and prove the
required characterisations: on the class of image-finite and preimage-finite
models (with respect to the plausibility relation), two pointed Kripke models
are modally equivalent in either of the three logics, if and only if they are
bisimilar. As a result, the information content of such a model can be
similarly expressed in the logic of conditional belief, or the logic of degrees
of belief, or that of safe belief. This, we found a surprising result. Still,
that does not mean that the logics are equally expressive: the logics of
conditional and degrees of belief are incomparable, the logics of degrees of
belief and safe belief are incomparable, while the logic of safe belief is more
expressive than the logic of conditional belief. In view of the result on
bisimulation characterisation, this is an equally surprising result. We hope
our insights may contribute to the growing community of formal epistemology and
on the relation between qualitative and quantitative modelling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08004</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08004</id><created>2015-06-26</created><authors><author><keyname>Basak</keyname><forenames>Jayanta</forenames></author></authors><title>ASOC: An Adaptive Parameter-free Stochastic Optimization Techinique for
  Continuous Variables</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic optimization is an important task in many optimization problems
where the tasks are not expressible as convex optimization problems. In the
case of non-convex optimization problems, various different stochastic
algorithms like simulated annealing, evolutionary algorithms, and tabu search
are available. Most of these algorithms require user-defined parameters
specific to the problem in order to find out the optimal solution. Moreover, in
many situations, iterative fine-tunings are required for the user-defined
parameters, and therefore these algorithms cannot adapt if the search space and
the optima changes over time. In this paper we propose an \underline{a}daptive
parameter-free \underline{s}tochastic \underline{o}ptimization technique for
\underline{c}ontinuous random variables called ASOC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08006</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08006</id><created>2015-06-26</created><authors><author><keyname>Boyali</keyname><forenames>Ali</forenames></author></authors><title>Spectral Collaborative Representation based Classification for Hand
  Gestures recognition on Electromyography Signals</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we introduce a novel variant and application of the
Collaborative Representation based Classification in spectral domain for
recognition of the hand gestures using the raw surface Electromyography
signals. The intuitive use of spectral features are explained via circulant
matrices. The proposed Spectral Collaborative Representation based
Classification (SCRC) is able to recognize gestures with higher levels of
accuracy for a fairly rich gesture set. The worst recognition result which is
the best in the literature is obtained as 97.3\% among the four sets of the
experiments for each hand gestures. The recognition results are reported with a
substantial number of experiments and labeling computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08009</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08009</id><created>2015-06-26</created><updated>2016-01-05</updated><authors><author><keyname>Petitjean</keyname><forenames>Francois</forenames></author><author><keyname>Li</keyname><forenames>Tao</forenames></author><author><keyname>Tatti</keyname><forenames>Nikolaj</forenames></author><author><keyname>Webb</keyname><forenames>Geoffrey I.</forenames></author></authors><title>Skopus: Exact discovery of the most interesting sequential patterns
  under Leverage</title><categories>cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a framework for exact discovery of the most interesting
sequential patterns. It combines (1) a novel definition of the expected support
for a sequential pattern - a concept on which most interestingness measures
directly rely - with (2) SkOPUS: a new branch-and-bound algorithm for the exact
discovery of top-k sequential patterns under a given measure of interest. Our
interestingness measure is based on comparing the pattern support with the
average support of its sister patterns, obtained by permuting (to certain
extent) the items of the pattern. The larger the support compared to the
expectation, the more interesting is the pattern. We build on these two
elements to exactly extract the k sequential patterns with highest leverage,
consistent with our definition of expected support. We conduct experiments on
both synthetic data with known patterns and real-world datasets; both
experiments confirm the consistency and relevance of our approach with regard
to the state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08018</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08018</id><created>2015-06-26</created><authors><author><keyname>V&#xe1;zquez</keyname><forenames>Miguel &#xc1;ngel</forenames></author><author><keyname>P&#xe9;rez-Neira</keyname><forenames>Ana</forenames></author><author><keyname>Christopoulos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Arapoglou</keyname><forenames>Pantelis-Daniel</forenames></author><author><keyname>Ginesi</keyname><forenames>Alberto</forenames></author><author><keyname>Taricco</keyname><forenames>Giorgio</forenames></author></authors><title>Precoding in Multibeam Satellite Communications: Present and Future
  Challenges</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whenever multibeam satellite systems target very aggressive frequency reuse
in their coverage area, inter-beam interference becomes the major obstacle for
increasing the overall system throughput. As a matter of fact, users located at
the beam edges suffer from a very large interference for even a moderately
aggressive planning of reuse-2. Although solutions for inter-beam interference
management have been investigated at the satellite terminal, it turns out that
the performance improvement does not justify the increased terminal complexity
and cost. In this article, we pay attention to interference mitigation
techniques that take place at the transmitter (i.e. the gateway). Based on this
understanding, we provide our vision on advanced precoding techniques and user
clustering methods for multibeam broadband fixed satellite communications. We
also discuss practical challenges to deploy precoding schemes and the support
introduced in the recently published DVB-S2X standard. Future challenges for
novel configurations employing precoding are also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08030</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08030</id><created>2015-06-26</created><authors><author><keyname>Ceylan</keyname><forenames>&#x130;smail &#x130;lkan</forenames></author><author><keyname>Pe&#xf1;aloza</keyname><forenames>Rafael</forenames></author></authors><title>Dynamic Bayesian Ontology Languages</title><categories>cs.AI cs.LO</categories><comments>Fifth International Workshop on Statistical Relational AI
  (StarAI'2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many formalisms combining ontology languages with uncertainty, usually in the
form of probabilities, have been studied over the years. Most of these
formalisms, however, assume that the probabilistic structure of the knowledge
remains static over time. We present a general approach for extending ontology
languages to handle time-evolving uncertainty represented by a dynamic Bayesian
network. We show how reasoning in the original language and dynamic Bayesian
inferences can be exploited for effective reasoning in our framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08052</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08052</id><created>2015-06-26</created><updated>2015-10-27</updated><authors><author><keyname>Combi</keyname><forenames>Carlo</forenames></author><author><keyname>Lora</keyname><forenames>Riccardo</forenames></author><author><keyname>Moretti</keyname><forenames>Ugo</forenames></author><author><keyname>Pagliarini</keyname><forenames>Marco</forenames></author><author><keyname>Zorzi</keyname><forenames>Margherita</forenames></author></authors><title>Automagically encoding Adverse Drug Reactions in MedDRA</title><categories>cs.CL</categories><comments>Submitted, 22 pages, 3 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pharmacovigilance is the field of science devoted to the collection, analysis
and prevention of Adverse Drug Reactions (ADRs). Efficient strategies for the
extraction of information about ADRs from free text resources are essential to
support the work of experts, employed in the crucial task of detecting and
classifying unexpected pathologies possibly related to drug assumptions.
Narrative ADR descriptions may be collected in several way, e.g. by monitoring
social networks or through the so called spontaneous reporting, the main method
pharmacovigilance adopts in order to identify ADRs. The encoding of free-text
ADR descriptions according to MedDRA standard terminology is central for report
analysis. It is a complex work, which has to be manually implemented by the
pharmacovigilance experts. The manual encoding is expensive (in terms of time).
Moreover, a problem about the accuracy of the encoding may occur, since the
number of reports is growing up day by day. In this paper, we propose
MagiCoder, an efficient Natural Language Processing algorithm able to
automatically derive MedDRA terminologies from free-text ADR descriptions.
MagiCoder is part of VigiWork, a web application for online ADR reporting and
analysis. From a practical view-point, MagiCoder radically reduces the revision
time of ADR reports: the pharmacologist has simply to revise and validate the
automatic solution versus the hard task of choosing solutions in the 70k terms
of MedDRA. This improvement of the expert work efficiency has a meaningful
impact on the quality of data analysis. Moreover, our procedure is general
purpose. We developed MagiCoder for the Italian pharmacovigilance language, but
preliminarily analyses show that it is robust to language and dictionary
changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08105</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08105</id><created>2015-06-26</created><authors><author><keyname>Kasarapu</keyname><forenames>Parthan</forenames></author></authors><title>Modelling of directional data using Kent distributions</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modelling of data on a spherical surface requires the consideration of
directional probability distributions. To model asymmetrically distributed data
on a three-dimensional sphere, Kent distributions are often used. The moment
estimates of the parameters are typically used in modelling tasks involving
Kent distributions. However, these lack a rigorous statistical treatment. The
focus of the paper is to introduce a Bayesian estimation of the parameters of
the Kent distribution which has not been carried out in the literature, partly
because of its complex mathematical form. We employ the Bayesian
information-theoretic paradigm of Minimum Message Length (MML) to bridge this
gap and derive reliable estimators. The inferred parameters are subsequently
used in mixture modelling of Kent distributions. The problem of inferring the
suitable number of mixture components is also addressed using the MML
criterion. We demonstrate the superior performance of the derived MML-based
parameter estimates against the traditional estimators. We apply the MML
principle to infer mixtures of Kent distributions to model empirical data
corresponding to protein conformations. We demonstrate the effectiveness of
Kent models to act as improved descriptors of protein structural data as
compared to commonly used von Mises-Fisher distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08106</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08106</id><created>2015-06-26</created><authors><author><keyname>Ben-David</keyname><forenames>Shalev</forenames></author></authors><title>A Super-Grover Separation Between Randomized and Quantum Query
  Complexities</title><categories>cs.CC quant-ph</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a total Boolean function $f$ satisfying
$R(f)=\tilde{\Omega}(Q(f)^{5/2})$, refuting the long-standing conjecture that
$R(f)=O(Q(f)^2)$ for all total Boolean functions. Assuming a conjecture of
Aaronson and Ambainis about optimal quantum speedups for partial functions, we
improve this to $R(f)=\tilde{\Omega}(Q(f)^3)$. Our construction is motivated by
the G\&quot;o\&quot;os-Pitassi-Watson function but does not use it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08110</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08110</id><created>2015-06-24</created><authors><author><keyname>Charles</keyname><forenames>Richard M.</forenames></author><author><keyname>Taylor</keyname><forenames>Kye M.</forenames></author><author><keyname>Curry</keyname><forenames>James H.</forenames></author></authors><title>Nonnegative Matrix Factorization applied to reordered pixels of single
  images based on patches to achieve structured nonnegative dictionaries</title><categories>cs.CV math.NA</categories><comments>34 pages, 15 figures, 2 tables</comments><msc-class>65K02</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent improvements in computing allow for the processing and analysis of
very large datasets in a variety of fields. Often the analysis requires the
creation of low-rank approximations to the datasets leading to efficient
storage. This article presents and analyzes a novel approach for creating
nonnegative, structured dictionaries using NMF applied to reordered pixels of
single, natural images. We reorder the pixels based on patches and present our
approach in general. We investigate our approach when using the Singular Value
Decomposition (SVD) and Nonnegative Matrix Factorizations (NMF) as low-rank
approximations. Peak Signal-to-Noise Ratio (PSNR) and Mean Structural
Similarity Index (MSSIM) are used to evaluate the algorithm. We report that
while the SVD provides the best reconstructions, its dictionary of vectors lose
both the sign structure of the original image and details of localized image
content. In contrast, the dictionaries produced using NMF preserves the sign
structure of the original image matrix and offer a nonnegative, parts-based
dictionary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08125</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08125</id><created>2015-06-26</created><authors><author><keyname>Wang</keyname><forenames>Zhi</forenames></author></authors><title>Data-driven Approaches for Social Video Distribution</title><categories>cs.MM cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet has recently witnessed the convergence of online social network
services and online video services: users import videos from content sharing
sites, and propagate them along the social connections by re-sharing them. Such
social behaviors have dramatically reshaped how videos are disseminated, and
the users are now actively engaged to be part of the social ecosystem, rather
than being passively consumers. Despite the increasingly abundant bandwidth and
computation resources, the ever increasing data volume of user generated video
content and the boundless coverage of socialized sharing have presented
unprecedented challenges. In this paper, we first presents the challenges in
social-aware video delivery. Then, we present a principal framework for
data-driven social video delivery approaches. Moreover, we identify the unique
characteristics of social-aware video access and the social content
propagation, and closely reveal the design of individual modules and their
integration towards enhancing users' experience in the social network context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08126</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08126</id><created>2015-06-26</created><authors><author><keyname>Radev</keyname><forenames>Dragomir</forenames></author><author><keyname>Stent</keyname><forenames>Amanda</forenames></author><author><keyname>Tetreault</keyname><forenames>Joel</forenames></author><author><keyname>Pappu</keyname><forenames>Aasish</forenames></author><author><keyname>Iliakopoulou</keyname><forenames>Aikaterini</forenames></author><author><keyname>Chanfreau</keyname><forenames>Agustin</forenames></author><author><keyname>de Juan</keyname><forenames>Paloma</forenames></author><author><keyname>Vallmitjana</keyname><forenames>Jordi</forenames></author><author><keyname>Jaimes</keyname><forenames>Alejandro</forenames></author><author><keyname>Jha</keyname><forenames>Rahul</forenames></author><author><keyname>Mankoff</keyname><forenames>Bob</forenames></author></authors><title>Humor in Collective Discourse: Unsupervised Funniness Detection in the
  New Yorker Cartoon Caption Contest</title><categories>cs.CL cs.AI cs.MM stat.ML</categories><comments>10 pages, in submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The New Yorker publishes a weekly captionless cartoon. More than 5,000
readers submit captions for it. The editors select three of them and ask the
readers to pick the funniest one. We describe an experiment that compares a
dozen automatic methods for selecting the funniest caption. We show that
negative sentiment, human-centeredness, and lexical centrality most strongly
match the funniest captions, followed by positive sentiment. These results are
useful for understanding humor and also in the design of more engaging
conversational agents in text and multimodal (vision+text) systems. As part of
this work, a large set of cartoons and captions is being made available to the
community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08133</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08133</id><created>2015-06-25</created><authors><author><keyname>Castro</keyname><forenames>Francisco Enrique Vicente G.</forenames></author><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>A Study on the Effect of Exit Widths and Crowd Sizes in the Formation of
  Arch in Clogged Crowds</title><categories>cs.MA physics.soc-ph</categories><comments>9 pages, 5 figures, originally appeared in H.N. Adorna and A.L.
  Sioson (eds.) Proceedings of the 6th National Symposium on Mathematical
  Aspects of Computer Science (SMACS 2012), La Carmela de Boracay Convention
  Center, Boracay Island, Malay, Aklan, 04-08 December 2012, pp. 66-74. arXiv
  admin note: text overlap with arXiv:1506.07781</comments><journal-ref>Philippine Computing Journal 8(1):21-29</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The arching phenomenon is an emergent pattern formed by a $c$-sized crowd of
intelligent, goal-oriented, autonomous, heterogeneous individuals moving
towards a $w$-wide exit along a long $W$-wide corridor, where $W&gt;w$. We
collected empirical data from microsimulations to identify the combination
effects of~$c$ and~$w$ to the time~$T$ of the onset of and the size~$S$ of the
formation of the arch. The arch takes on the form of the perimeter of a half
ellipse halved along the minor axis. We measured the~$S$ with respect to the
lengths of the major~$M$ and minor~$m$ axes of the ellipse, respectively. The
mathematical description of the formation of this phenomenon will be an
important information in the design of walkways to control and easily direct
the flow of large crowds, especially during panic egress conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08134</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08134</id><created>2015-06-26</created><updated>2015-07-17</updated><authors><author><keyname>Plonka</keyname><forenames>David</forenames></author><author><keyname>Berger</keyname><forenames>Arthur</forenames></author></authors><title>Temporal and Spatial Classification of Active IPv6 Addresses</title><categories>cs.NI</categories><doi>10.1145/2815675.2815678</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is striking volume of World-Wide Web activity on IPv6 today. In early
2015, one large Content Distribution Network handles 50 billion IPv6 requests
per day from hundreds of millions of IPv6 client addresses; billions of unique
client addresses are observed per month. Address counts, however, obscure the
number of hosts with IPv6 connectivity to the global Internet. There are
numerous address assignment and subnetting options in use; privacy addresses
and dynamic subnet pools significantly inflate the number of active IPv6
addresses. As the IPv6 address space is vast, it is infeasible to
comprehensively probe every possible unicast IPv6 address. Thus, to survey the
characteristics of IPv6 addressing, we perform a year-long passive measurement
study, analyzing the IPv6 addresses gleaned from activity logs for all clients
accessing a global CDN.
  The goal of our work is to develop flexible classification and measurement
methods for IPv6, motivated by the fact that its addresses are not merely more
numerous; they are different in kind. We introduce the notion of classifying
addresses and prefixes in two ways: (1) temporally, according to their
instances of activity to discern which addresses can be considered stable; (2)
spatially, according to the density or sparsity of aggregates in which active
addresses reside. We present measurement and classification results numerically
and visually that: provide details on IPv6 address use and structure in global
operation across the past year; establish the efficacy of our classification
methods; and demonstrate that such classification can clarify dimensions of the
Internet that otherwise appear quite blurred by current IPv6 addressing
practices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08159</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08159</id><created>2015-06-26</created><authors><author><keyname>Bahmani</keyname><forenames>Sohail</forenames></author><author><keyname>Romberg</keyname><forenames>Justin</forenames></author></authors><title>Near-Optimal Estimation of Simultaneously Sparse and Low-Rank Matrices
  from Nested Linear Measurements</title><categories>math.ST cs.IT math.IT math.OC stat.AP stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of estimating simultaneously low-rank
and row-wise sparse matrices from nested linear measurements where the linear
operator consists of the product of a linear operator $\mathcal{W}$ and a
matrix $\mathbf{\varPsi}$. Leveraging the nested structure of the measurement
operator, we propose a computationally efficient two-stage algorithm for
estimating the simultaneously structured target matrix. Assuming that
$\mathcal{W}$ is a restricted isometry for low-rank matrices and
$\mathbf{\varPsi}$ is a restricted isometry for row-wise sparse matrices, we
establish an accuracy guarantee that holds uniformly for all sufficiently
low-rank and row-wise sparse matrices with high probability. Furthermore, using
standard tools from information theory, we establish a minimax lower bound for
estimation of simultaneously low-rank and row-wise sparse matrices from linear
measurements that need not be nested. The accuracy bounds established for the
algorithm, that also serve as a minimax upper bound, differ from the derived
minimax lower bound merely by a polylogarithmic factor of the dimensions.
Therefore, the proposed algorithm is nearly minimax optimal. We also discuss
some applications of the proposed observation model and evaluate our algorithm
through numerical simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08180</identifier>
 <datestamp>2015-06-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08180</id><created>2015-06-26</created><authors><author><keyname>Shah</keyname><forenames>Amar</forenames></author><author><keyname>Knowles</keyname><forenames>David A.</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author></authors><title>An Empirical Study of Stochastic Variational Algorithms for the Beta
  Bernoulli Process</title><categories>stat.ML cs.LG stat.AP stat.CO stat.ME</categories><comments>ICML, 12 pages. Volume 37: Proceedings of The 32nd International
  Conference on Machine Learning, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic variational inference (SVI) is emerging as the most promising
candidate for scaling inference in Bayesian probabilistic models to large
datasets. However, the performance of these methods has been assessed primarily
in the context of Bayesian topic models, particularly latent Dirichlet
allocation (LDA). Deriving several new algorithms, and using synthetic, image
and genomic datasets, we investigate whether the understanding gleaned from LDA
applies in the setting of sparse latent factor models, specifically beta
process factor analysis (BPFA). We demonstrate that the big picture is
consistent: using Gibbs sampling within SVI to maintain certain posterior
dependencies is extremely effective. However, we find that different posterior
dependencies are important in BPFA relative to LDA. Particularly,
approximations able to model intra-local variable dependence perform best.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08187</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08187</id><created>2015-06-26</created><authors><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Lee</keyname><forenames>Yin Tat</forenames></author><author><keyname>Singh</keyname><forenames>Mohit</forenames></author></authors><title>A geometric alternative to Nesterov's accelerated gradient descent</title><categories>math.OC cs.DS cs.LG cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method for unconstrained optimization of a smooth and
strongly convex function, which attains the optimal rate of convergence of
Nesterov's accelerated gradient descent. The new algorithm has a simple
geometric interpretation, loosely inspired by the ellipsoid method. We provide
some numerical evidence that the new method can be superior to Nesterov's
accelerated gradient descent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08189</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08189</id><created>2015-06-26</created><updated>2015-12-09</updated><authors><author><keyname>Puleo</keyname><forenames>Gregory J.</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>Minimax Correlation Clustering and Biclustering: Bounding Errors Locally</title><categories>cs.DS cs.LG</categories><comments>17 pages, corrected several errors in the previous version of the
  paper and included discussion of more general objective functions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new agnostic clustering model, \emph{minimax correlation
clustering}, and a rounding algorithm tailored to the needs of this model.
Given a graph whose edges are labeled with $+$ or $-$, we wish to partition the
graph into clusters while trying to avoid errors: $+$ edges between clusters or
$-$ edges within clusters. Unlike classical correlation clustering, which seeks
to minimize the total number of errors, minimax clustering instead seeks to
minimize the number of errors at the \emph{worst vertex}, that is, at the
vertex with the greatest number of incident errors. This minimax objective
function may be seen as a way to enforce individual-level quality of partition
constraints for vertices in a graph. We study this problem on complete graphs
and complete bipartite graphs, proving that the problem is NP-hard on these
graph classes and giving polynomial-time constant-factor approximation
algorithms. The approximation algorithms rely on LP relaxation and rounding
procedures. We also discuss the broader applicability of our rounding algorithm
to other (nonlinear) objective functions for correlation clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08204</identifier>
 <datestamp>2015-08-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08204</id><created>2015-06-26</created><updated>2015-08-13</updated><authors><author><keyname>Lee</keyname><forenames>Yin Tat</forenames></author><author><keyname>Peng</keyname><forenames>Richard</forenames></author><author><keyname>Spielman</keyname><forenames>Daniel A.</forenames></author></authors><title>Sparsified Cholesky Solvers for SDD linear systems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that Laplacian and symmetric diagonally dominant (SDD) matrices can
be well approximated by linear-sized sparse Cholesky factorizations. We show
that these matrices have constant-factor approximations of the form $L L^{T}$,
where $L$ is a lower-triangular matrix with a number of nonzero entries linear
in its dimension. Furthermore linear systems in $L$ and $L^{T}$ can be solved
in $O (n)$ work and $O(\log{n}\log^2\log{n})$ depth, where $n$ is the dimension
of the matrix.
  We present nearly linear time algorithms that construct solvers that are
almost this efficient. In doing so, we give the first nearly-linear work
routine for constructing spectral vertex sparsifiers---that is, spectral
approximations of Schur complements of Laplacian matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08230</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08230</id><created>2015-06-26</created><updated>2016-02-16</updated><authors><author><keyname>Tygert</keyname><forenames>Mark</forenames></author><author><keyname>Szlam</keyname><forenames>Arthur</forenames></author><author><keyname>Chintala</keyname><forenames>Soumith</forenames></author><author><keyname>Ranzato</keyname><forenames>Marc'Aurelio</forenames></author><author><keyname>Tian</keyname><forenames>Yuandong</forenames></author><author><keyname>Zaremba</keyname><forenames>Wojciech</forenames></author></authors><title>Convolutional networks and learning invariant to homogeneous
  multiplicative scalings</title><categories>cs.LG cs.NE</categories><comments>12 pages, 6 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conventional classification schemes -- notably multinomial logistic
regression -- used in conjunction with convolutional networks (convnets) are
classical in statistics, designed without consideration for the usual coupling
with convnets, stochastic gradient descent, and backpropagation. In the
specific application to supervised learning for convnets, a simple
scale-invariant classification stage turns out to be more robust than
multinomial logistic regression, appears to result in slightly lower errors on
several standard test sets, has similar computational costs, and features
precise control over the actual rate of learning. &quot;Scale-invariant&quot; means that
multiplying the input values by any nonzero scalar leaves the output unchanged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08231</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08231</id><created>2015-06-26</created><authors><author><keyname>Hanley</keyname><forenames>Brian P.</forenames></author></authors><title>A zero-sum monetary system, interest rates, and implications</title><categories>cs.CE</categories><comments>12 pages, 4 figures, 2 tables, 3 equations. This is a breakout/rework
  of the unwieldy section 2.6 of arXiv:1312.2048 &quot;The False Premises and
  Promises of Bitcoin.&quot; The rest of arXiv:1312.2048 is scholarly &amp; not
  original, although educational for cryptocurrency buffs. Section 2.6 of
  arXiv:1312.2048 is also replaced by a short section referencing this new
  paper</comments><acm-class>J.4.1</acm-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  To the knowledge of the author, this is the first time it has been shown that
interest rates that are extremely high by modern standards are necessary within
a zero-sum monetary system. Extreme interest rates persisted for long periods
of time in many places. Prior to the invention of banking, most money was
hard-money in the form of some type of coin. Here a model is presented that
examines the interest rate required to succeed as an investor in a zero-sum
hard-money system. Even when the playing field is significantly tilted toward
the investor, interest rates need to be much higher than expected. In a
completely fair zero-sum system, an investor cannot break even without charging
100% interest. Even with a 5% advantage, an investor will not break even at 15%
interest. From this it is concluded that what we consider usurious rates today
are, within a hard-money system, driven by necessity.
  Cryptocurrency is a novel form of hard-currency. The inability to virtualize
the money creates a system close to zero-sum. Therefore, within the bounds of a
cryptocurrency system that limits money creation, interest rates must rise to
levels that the modern world considers usury. It is impossible, therefore, that
a cryptocurrency that is not expandable could take over a modern economy and
replace modern fiat currency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08234</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08234</id><created>2015-06-26</created><authors><author><keyname>Deshmukh</keyname><forenames>Jyotirmoy V.</forenames></author><author><keyname>Donz&#xe9;</keyname><forenames>Alexandre</forenames></author><author><keyname>Ghosh</keyname><forenames>Shromona</forenames></author><author><keyname>Jin</keyname><forenames>Xiaoqing</forenames></author><author><keyname>Juniwal</keyname><forenames>Garvit</forenames></author><author><keyname>Seshia</keyname><forenames>Sanjit A.</forenames></author></authors><title>Robust Online Monitoring of Signal Temporal Logic</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signal Temporal Logic (STL) is a formalism used to rigorously specify
requirements of cyberphysical systems (CPS), i.e., systems mixing digital or
discrete components in interaction with a continuous environment or analog com-
ponents. STL is naturally equipped with a quantitative semantics which can be
used for various purposes: from assessing the robustness of a specification to
guiding searches over the input and parameter space with the goal of falsifying
the given property over system behaviors. Algorithms have been proposed and
implemented for offline computation of such quantitative semantics, but only
few methods exist for an online setting, where one would want to monitor the
satisfaction of a formula during simulation. In this paper, we formalize a
semantics for robust online monitoring of partial traces, i.e., traces for
which there might not be enough data to decide the Boolean satisfaction (and to
compute its quantitative counterpart). We propose an efficient algorithm to
compute it and demonstrate its usage on two large scale real-world case studies
coming from the automotive domain and from CPS education in a Massively Open
Online Course (MOOC) setting. We show that savings in computationally expensive
simulations far outweigh any overheads incurred by an online approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08235</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08235</id><created>2015-06-26</created><authors><author><keyname>Xin</keyname><forenames>Hongyi</forenames></author><author><keyname>Zhu</keyname><forenames>Richard</forenames></author><author><keyname>Nahar</keyname><forenames>Sunny</forenames></author><author><keyname>Emmons</keyname><forenames>John</forenames></author><author><keyname>Pekhimenko</keyname><forenames>Gennady</forenames></author><author><keyname>Kingsford</keyname><forenames>Carl</forenames></author><author><keyname>Alkan</keyname><forenames>Can</forenames></author><author><keyname>Mutlu</keyname><forenames>Onur</forenames></author></authors><title>Optimal Seed Solver: Optimizing Seed Selection in Read Mapping</title><categories>cs.CE q-bio.GN</categories><comments>10 pages of main text. 6 pages of supplementary materials. Under
  review by Oxford Bioinformatics</comments><doi>10.1093/bioinformatics/btv670</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: Optimizing seed selection is an important problem in read
mapping. The number of non-overlapping seeds a mapper selects determines the
sensitivity of the mapper while the total frequency of all selected seeds
determines the speed of the mapper. Modern seed-and-extend mappers usually
select seeds with either an equal and fixed-length scheme or with an inflexible
placement scheme, both of which limit the potential of the mapper to select
less frequent seeds to speed up the mapping process. Therefore, it is crucial
to develop a new algorithm that can adjust both the individual seed length and
the seed placement, as well as derive less frequent seeds.
  Results: We present the Optimal Seed Solver (OSS), a dynamic programming
algorithm that discovers the least frequently-occurring set of x seeds in an
L-bp read in $O(x \times L)$ operations on average and in $O(x \times L^{2})$
operations in the worst case. We compared OSS against four state-of-the-art
seed selection schemes and observed that OSS provides a 3-fold reduction of
average seed frequency over the best previous seed selection optimizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08238</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08238</id><created>2015-06-26</created><authors><author><keyname>Li</keyname><forenames>Wenda</forenames></author><author><keyname>Passmore</keyname><forenames>Grant Olney</forenames></author><author><keyname>Paulson</keyname><forenames>Lawrence C.</forenames></author></authors><title>A Complete Decision Procedure for Univariate Polynomial Problems in
  Isabelle/HOL</title><categories>cs.LO</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a complete, certificate-based decision procedure for first-order
univariate polynomial problems in Isabelle. It is built around an executable
function to decide the sign of a univariate polynomial at a real algebraic
point. The procedure relies on no trusted code except for Isabelle's kernel and
code generation. This work is the first step towards integrating the MetiTarski
theorem prover into Isabelle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08244</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08244</id><created>2015-06-26</created><authors><author><keyname>Srinivasan</keyname><forenames>Seshadhri</forenames></author><author><keyname>Ayyagari</keyname><forenames>R.</forenames></author></authors><title>Formation Control in Multi-Agent Systems Over Packet Dropping Links</title><categories>cs.RO</categories><comments>12 pages</comments><journal-ref>Mobile Intelligent Autonomous Systems, 2013, CRC Press, pp.113-124</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One major challenge in implementation of formation control problems stems
from the packet loss that occur in these shared communication channel. In the
presence of packet loss the coordination information among agents is lost.
Moreover, there is a move to use wireless channels in formation control
applications. It has been found in practice that packet losses are more
pronounced in wireless channels, than their wired counterparts. In our
analysis, we first show that packet loss may result in loss of rigidity. In
turn this causes the entire formation to fail. Later, we present an estimation
based formation control algorithm that is robust to packet loss among agents.
The proposed estimation algorithm employs minimal spanning tree algorithm to
compute the estimate of the node variables (coordination variables).
Consequently, this reduces the communication overhead required for information
exchange. Later, using simulation, we verify the data that is to be transmitted
for optimal estimation of these variables in the event of a packet loss.
Finally, the effectiveness of the proposed algorithm is illustrated using
suitable simulation example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08248</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08248</id><created>2015-06-26</created><updated>2015-09-27</updated><authors><author><keyname>Merwaday</keyname><forenames>Arvind</forenames></author><author><keyname>Guvenc</keyname><forenames>Ismail</forenames></author></authors><title>Handover Count Based Velocity Estimation and Mobility State Detection in
  Dense HetNets</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless cellular networks with densely deployed base stations, knowing
the velocities of mobile devices is a key to avoid call drops and improve the
quality of service to the user equipments (UEs). A simple and efficient way to
estimate a UE's velocity is by counting the number of handovers made by the UE
during a predefined time window. Indeed, handover-count based mobility state
detection has been standardized since Long Term Evolution (LTE) Release-8
specifications. The increasing density of small cells in wireless networks can
help in accurate estimation of velocity and mobility state of a UE. In this
paper, we model densely deployed small cells using stochastic geometry, and
then analyze the statistics of the number of handovers as a function of UE
velocity, small-cell density, and handover count measurement time window. Using
these statistics, we derive approximations to the Cramer-Rao lower bound (CRLB)
for the velocity estimate of a UE. Also, we determine a minimum variance
unbiased (MVU) velocity estimator whose variance tightly matches with the CRLB.
Using this velocity estimator, we formulate the problem of detecting the
mobility state of a UE as low, medium, or high-mobility, as in LTE
specifications. Subsequently, we derive the probability of correctly detecting
the mobility state of a UE. Finally, we evaluate the accuracy of the velocity
estimator under more realistic scenarios such as clustered deployments of small
cells, random waypoint (RWP) mobility model for UEs, and variable UE velocity.
Our analysis shows that the accuracy of velocity estimation and mobility state
detection increases with increasing small cell density and with increasing
handover count measurement time window.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08250</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08250</id><created>2015-06-26</created><authors><author><keyname>Chatzivasileiadis</keyname><forenames>Spyros</forenames></author><author><keyname>Andersson</keyname><forenames>G&#xf6;ran</forenames></author></authors><title>A Fully Controllable Power System - Concept for FACTS and HVDC Placement</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper puts forward the vision of fully decoupling market operations from
security considerations through controllable power flows. In &quot;A Fully
Controllable Power System&quot;, power system security is no longer dependent on the
location of the power injection points. In the ideal case, this leads to the
elimination of redispatching costs, which amount to several million dollars per
year in large systems. This paper determines the upper and lower bounds for the
number of controllable lines and number of controllers to achieve this
decoupling in any system. It further introduces the notion of the
controllability vector CV, which expresses the effect of any controller on the
AC line flows. Based on two alternative definitions for controllability, two
controller placement algorithms to maximize controllability are presented and
their results are compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08251</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08251</id><created>2015-06-26</created><authors><author><keyname>Raiman</keyname><forenames>Jonathan</forenames></author><author><keyname>Sidor</keyname><forenames>Szymon</forenames></author></authors><title>Occam's Gates</title><categories>cs.LG</categories><comments>In review at NIPS</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a complimentary objective for training recurrent neural networks
(RNN) with gating units that helps with regularization and interpretability of
the trained model. Attention-based RNN models have shown success in many
difficult sequence to sequence classification problems with long and short term
dependencies, however these models are prone to overfitting. In this paper, we
describe how to regularize these models through an L1 penalty on the activation
of the gating units, and show that this technique reduces overfitting on a
variety of tasks while also providing to us a human-interpretable visualization
of the inputs used by the network. These tasks include sentiment analysis,
paraphrase recognition, and question answering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08258</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08258</id><created>2015-06-27</created><authors><author><keyname>Bennett</keyname><forenames>Janine C.</forenames></author><author><keyname>Bhagatwala</keyname><forenames>Ankit</forenames></author><author><keyname>Chen</keyname><forenames>Jacqueline H.</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author><author><keyname>Salloum</keyname><forenames>Maher</forenames></author></authors><title>Trigger detection for adaptive scientific workflows using percentile
  sampling</title><categories>cs.CE cs.DC cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing complexity of scientific simulations and HPC architectures are
driving the need for adaptive workflows, where the composition and execution of
computational and data manipulation steps dynamically depend on the
evolutionary state of the simulation itself. Consider for example, the
frequency of data storage. Critical phases of the simulation should be captured
with high frequency and with high fidelity for post-analysis, however we cannot
afford to retain the same frequency for the full simulation due to the high
cost of data movement. We can instead look for triggers, indicators that the
simulation will be entering a critical phase and adapt the workflow
accordingly.
  We present a method for detecting triggers and demonstrate its use in direct
numerical simulations of turbulent combustion using S3D. We show that chemical
explosive mode analysis (CEMA) can be used to devise a noise-tolerant indicator
for rapid increase in heat release. However, exhaustive computation of CEMA
values dominates the total simulation, thus is prohibitively expensive. To
overcome this bottleneck, we propose a quantile-sampling approach. Our
algorithm comes with provable error/confidence bounds, as a function of the
number of samples. Most importantly, the number of samples is independent of
the problem size, thus our proposed algorithm offers perfect scalability. Our
experiments on homogeneous charge compression ignition (HCCI) and reactivity
controlled compression ignition (RCCI) simulations show that the proposed
method can detect rapid increases in heat release, and its computational
overhead is negligible. Our results will be used for dynamic workflow decisions
about data storage and mesh resolution in future combustion simulations.
Proposed framework is generalizable and we detail how it could be applied to a
broad class of scientific simulation workflows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08259</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08259</id><created>2015-06-27</created><updated>2015-09-21</updated><authors><author><keyname>Rahimi</keyname><forenames>Afshin</forenames></author><author><keyname>Cohn</keyname><forenames>Trevor</forenames></author><author><keyname>Baldwin</keyname><forenames>Timothy</forenames></author></authors><title>Twitter User Geolocation Using a Unified Text and Network Prediction
  Model</title><categories>cs.CL cs.SI</categories><comments>To appear in ACL 2015, Proceedings of the 53rd Annual Meeting of the
  Association for Computational Linguistics (ACL 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a label propagation approach to geolocation prediction based on
Modified Adsorption, with two enhancements:(1) the removal of &quot;celebrity&quot; nodes
to increase location homophily and boost tractability, and (2) he incorporation
of text-based geolocation priors for test users. Experiments over three Twitter
benchmark datasets achieve state-of-the-art results, and demonstrate the
effectiveness of the enhancements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08264</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08264</id><created>2015-06-27</created><authors><author><keyname>Denoyelle</keyname><forenames>Quentin</forenames></author><author><keyname>Duval</keyname><forenames>Vincent</forenames></author><author><keyname>Peyr&#xe9;</keyname><forenames>Gabriel</forenames></author></authors><title>Support Recovery for Sparse Deconvolution of Positive Measures</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study sparse spikes deconvolution over the space of Radon measures when
the input measure is a finite sum of positive Dirac masses using the BLASSO
convex program. We focus on the recovery properties of the support and the
amplitudes of the initial measure in the presence of noise as a function of the
minimum separation $t$ of the input measure (the minimum distance between two
spikes). We show that when $w/\lambda$, $w/t^{2N-1}$ and $\lambda/t^{2N-1}$ are
small enough (where $\lambda$ is the regularization parameter, $w$ the noise
and $N$ the number of spikes), which corresponds roughly to a sufficient
signal-to-noise ratio and a noise level small enough with respect to the
minimum separation, there exists a unique solution to the BLASSO program with
exactly the same number of spikes as the original measure. We show that the
amplitudes and positions of the spikes of the solution both converge toward
those of the input measure when the noise and the regularization parameter
drops to zero faster than $t^{2N-1}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08268</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08268</id><created>2015-06-27</created><authors><author><keyname>Iaconesi</keyname><forenames>Salvatore</forenames></author><author><keyname>Persico</keyname><forenames>Oriana</forenames></author></authors><title>The Third Infoscape: Data, Information and Knowledge in the city. New
  paradigms for urban interaction</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Third Infoscape refers to the information and knowledge generated through
the myriads of micro-histories, through the progressive, emergent and
polyphonic sedimentation of the expressions of the daily lives of city
dwellers. To all effects, with the development of wireless sensors, of smart
dust1, and with the possibility to engage human beings in urban sensing
processes, the dimension of virtuality collapses. Heading towards a state which
is basically comparable to the one of telepathy (among human beings, human
beings and machines, machines and machines...), reconfiguring urban ecologies
so that mapping virtuality or physicality would not be needed anymore, and
replacing this need with the possibility to create recombinant inventories of
the telepathic migration of dusts, of the myriads of pulverized sensors which
are disseminated, diffused. We can imagine information mutating into landscape,
delineating an urban space which is not determined by distance and time, but
from the transformation of densities and presences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08269</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08269</id><created>2015-06-27</created><authors><author><keyname>Huang</keyname><forenames>Yu-Chih</forenames></author><author><keyname>Narayanan</keyname><forenames>Krishna R.</forenames></author></authors><title>Construction $\pi_A$ and $\pi_D$ Lattices: Construction, Goodness, and
  Decoding Algorithms</title><categories>cs.IT math.IT</categories><comments>27 pages, 11 figures. arXiv admin note: substantial text overlap with
  arXiv:1401.2228</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel construction of lattices is proposed. This construction can be
thought of as Construction A with codes that can be represented as the
Cartesian product of $L$ linear codes over
$\mathbb{F}_{p_1},\ldots,\mathbb{F}_{p_L}$, respectively; hence, it is referred
to as Construction $\pi_A$. The existence of a sequence of such lattices that
is good for quantization and good for channel coding (i.e., Poltyrev-limit
achieving) under multistage decoding is shown. This family of lattices is then
used to generate a sequence of nested lattice codes that can achieve the AWGN
capacity under multistage decoding, which substantially reduces decoding
complexity as compared to codes from Construction A lattices. A generalization
named Construction $\pi_D$ is also proposed which subsumes Construction A,
Construction D, and Construction $\pi_A$ as special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08272</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08272</id><created>2015-06-27</created><updated>2015-10-29</updated><authors><author><keyname>Lian</keyname><forenames>Xiangru</forenames></author><author><keyname>Huang</keyname><forenames>Yijun</forenames></author><author><keyname>Li</keyname><forenames>Yuncheng</forenames></author><author><keyname>Liu</keyname><forenames>Ji</forenames></author></authors><title>Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization</title><categories>math.OC cs.NA stat.ML</categories><comments>31 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asynchronous parallel implementations of stochastic gradient (SG) have been
broadly used in solving deep neural network and received many successes in
practice recently. However, existing theories cannot explain their convergence
and speedup properties, mainly due to the nonconvexity of most deep learning
formulations and the asynchronous parallel mechanism. To fill the gaps in
theory and provide theoretical supports, this paper studies two asynchronous
parallel implementations of SG: one is on the computer network and the other is
on the shared memory system. We establish an ergodic convergence rate
$O(1/\sqrt{K})$ for both algorithms and prove that the linear speedup is
achievable if the number of workers is bounded by $\sqrt{K}$ ($K$ is the total
number of iterations). Our results generalize and improve existing analysis for
convex minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08291</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08291</id><created>2015-06-27</created><authors><author><keyname>Datta</keyname><forenames>T.</forenames></author><author><keyname>Eshwaraiah</keyname><forenames>H. S.</forenames></author><author><keyname>Chockalingam</keyname><forenames>A.</forenames></author></authors><title>Generalized Space and Frequency Index Modulation</title><categories>cs.IT math.IT</categories><comments>IEEE Trans. on Vehicular Technology, accepted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike in conventional modulation where information bits are conveyed only
through symbols from modulation alphabets defined in the complex plane (e.g.,
quadrature amplitude modulation (QAM), phase shift keying (PSK)), in index
modulation (IM), additional information bits are conveyed through indices of
certain transmit entities that get involved in the transmission. Transmit
antennas in multi-antenna systems and subcarriers in multi-carrier systems are
examples of such transmit entities that can be used to convey additional
information bits through indexing. In this paper, we introduce {\em generalized
space and frequency index modulation}, where the indices of active transmit
antennas and subcarriers convey information bits. We first introduce index
modulation in the spatial domain, referred to as generalized spatial index
modulation (GSIM). For GSIM, where bits are indexed only in the spatial domain,
we derive the expression for achievable rate as well as easy-to-compute upper
and lower bounds on this rate. We show that the achievable rate in GSIM can be
more than that in spatial multiplexing, and analytically establish the
condition under which this can happen. It is noted that GSIM achieves this
higher rate using fewer transmit radio frequency (RF) chains compared to
spatial multiplexing. We also propose a Gibbs sampling based detection
algorithm for GSIM and show that GSIM can achieve better bit error rate (BER)
performance than spatial multiplexing. For generalized space-frequency index
modulation (GSFIM), where bits are encoded through indexing in both active
antennas as well as subcarriers, we derive the achievable rate expression.
Numerical results show that GSFIM can achieve higher rates compared to
conventional MIMO-OFDM. Also, BER results show the potential for GSFIM
performing better than MIMO-OFDM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08301</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08301</id><created>2015-06-27</created><authors><author><keyname>Li</keyname><forenames>Zhiqiang</forenames></author><author><keyname>Wang</keyname><forenames>Yilun</forenames></author><author><keyname>Wang</keyname><forenames>Yifeng</forenames></author><author><keyname>Wang</keyname><forenames>Xiaona</forenames></author><author><keyname>Zheng</keyname><forenames>Junjie</forenames></author><author><keyname>Chen</keyname><forenames>Huafu</forenames></author></authors><title>A Novel Feature Selection Approach for Analyzing High dimensional
  Functional MRI Data</title><categories>cs.CV cs.LG stat.ML</categories><acm-class>I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection based on traditional multivariate methods is likely to
obtain unstable and unreliable results in case of an extremely high dimensional
space and very limited training samples. In order to overcome this difficulty,
we introduced a novel feature selection method which combines the idea of
stability selection approach and the elastic net approach to detect
discriminative features in a stable and robust way. This new method is applied
to functional magnetic resonance imaging (fMRI) data, whose discriminative
features are often correlated or redundant. Compared with the original
stability selection approach with the pure l_1 -norm regularized model serving
as the baseline model, the proposed method achieves a better sensitivity
empirically, because elastic net encourages a grouping effect besides sparsity.
Compared with the feature selection method based on the plain Elastic Net, our
method achieves the finite sample control for certain error rates of false
discoveries, transparent principle for choosing a proper amount of
regularization and the robustness of the feature selection results, due to the
incorporation of the stability selection idea. A simulation study showed that
our approach are less influenced than other methods by label noise. In
addition, the advantage in terms of better control of false discoveries and
missed discoveries of our approach was verified in a real fMRI experiment.
Finally, a multi-center resting-state fMRI data about Attention-deficit/
hyperactivity disorder (ADHD) suggested that the resulted classifier based on
our feature selection method achieves the best and most robust prediction
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08307</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08307</id><created>2015-06-27</created><authors><author><keyname>Argyriou</keyname><forenames>Antonios</forenames></author><author><keyname>Breva</keyname><forenames>Alberto Caballero</forenames></author><author><keyname>Aoun</keyname><forenames>Marc</forenames></author></authors><title>Optimizing Data Forwarding from Body Area Networks in the Presence of
  Body Shadowing with Dual Wireless Technology Nodes</title><categories>cs.NI</categories><journal-ref>IEEE Transactions on Mobile Computing, Volume 14, No. 3, Pages
  632-645, March 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we are concerned with the problem of data forwarding from a
wireless body area network (WBAN) to a gateway when body shadowing affects the
ability of WBAN nodes to communicate with the gateway. To solve this problem we
present a new WBAN architecture that uses two communication technologies. One
network is formed between on-body nodes, and is realized with capacitive
body-coupled communication (BCC), while an IEEE 802.15.4 radio frequency (RF)
network is used for forwarding data to the gateway. WBAN nodes that have
blocked RF links due to body shadowing forward their data through the BCC link
to a node that acts as a relay and has an active RF connection. For this
architecture we design a network layer protocol that manages the two
communication technologies and is responsible for relay selection and data
forwarding. Next, we develop analytical performance models of the medium access
control (MAC) protocols of the two independent communication links in order to
be used for driving the decisions of the previous algorithms. Finally, the
analytical models are used for further optimizing energy and delay efficiency.
We test our system under different configurations first by performing
simulations and next by using real RF traces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08308</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08308</id><created>2015-06-27</created><authors><author><keyname>Argyriou</keyname><forenames>Antonios</forenames></author><author><keyname>Kosmanos</keyname><forenames>Dimitrios</forenames></author><author><keyname>Tassiulas</keyname><forenames>Leandros</forenames></author></authors><title>Joint Time-Domain Resource Partitioning, Rate Allocation, and Video
  Quality Adaptation in Heterogeneous Cellular Networks</title><categories>cs.NI</categories><comments>IEEE Transactions on Multimedia, Vol. 17, Issue 5, March 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogenous cellular networks (HCN) introduce small cells within the
transmission range of a macrocell. For the efficient operation of HCNs it is
essential that the high power macrocell shuts off its transmissions for an
appropriate amount of time in order for the low power small cells to transmit.
This is a mechanism that allows time-domain resource partitioning (TDRP) and is
critical to be optimized for maximizing the throughput of the complete HCN. In
this paper, we investigate video communication in HCNs when TDRP is employed.
After defining a detailed system model for video streaming in such a HCN, we
consider the problem of maximizing the experienced video quality at all the
users, by jointly optimizing the TDRP for the HCN, the rate allocated to each
specific user, and the selected video quality transmitted to a user. The
NP-hard problem is solved with a primal-dual approximation algorithm that
decomposes the problem into simpler subproblems, making them amenable to fast
well-known solution algorithms. Consequently, the calculated solution can be
enforced in the time scale of real-life video streaming sessions. This last
observation motivates the enhancement of the proposed framework to support
video delivery with dynamic adaptive streaming over HTTP (DASH). Our extensive
simulation results demonstrate clearly the need for our holistic approach for
improving the video quality and playback performance of the video streaming
users in HCNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08311</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08311</id><created>2015-06-27</created><updated>2015-11-14</updated><authors><author><keyname>Avis</keyname><forenames>David</forenames></author><author><keyname>Tiwary</keyname><forenames>Hans Raj</forenames></author></authors><title>On the H-Free Extension Complexity of the TSP</title><categories>cs.CC</categories><comments>Minor revisions; 9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that the extension complexity of the TSP polytope for the
complete graph $K_n$ is exponential in $n$ even if the subtour inequalities are
excluded. In this article we study the polytopes formed by removing other
subsets $\mathcal{H}$ of facet-defining inequalities of the TSP polytope. In
particular, we consider the case when $\mathcal{H}$ is either the set of
blossom inequalities or the simple comb inequalities. These inequalities are
routinely used in cutting plane algorithms for the TSP. We show that the
extension complexity remains exponential even if we exclude these inequalities.
In addition we show that the extension complexity of polytope formed by all
comb inequalities is exponential. For our proofs, we introduce a subclass of
comb inequalities, called $(h,t)$-uniform inequalities, which may be of
independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08316</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08316</id><created>2015-06-27</created><updated>2016-03-04</updated><authors><author><keyname>Chao</keyname><forenames>Jianshu</forenames></author><author><keyname>Steinbach</keyname><forenames>Eckehard</forenames></author></authors><title>Keypoint Encoding for Improved Feature Extraction from Compressed Video
  at Low Bitrates</title><categories>cs.MM cs.CV cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many mobile visual analysis applications, compressed video is transmitted
over a communication network and analyzed by a server. Typical processing steps
performed at the server include keypoint detection, descriptor calculation, and
feature matching. Video compression has been shown to have an adverse effect on
feature-matching performance. The negative impact of compression can be reduced
by using the keypoints extracted from the uncompressed video to calculate
descriptors from the compressed video. Based on this observation, we propose to
provide these keypoints to the server as side information and to extract only
the descriptors from the compressed video. First, we introduce four different
frame types for keypoint encoding to address different types of changes in
video content. These frame types represent a new scene, the same scene, a
slowly changing scene, or a rapidly moving scene and are determined by
comparing features between successive video frames. Then, we propose Intra,
Skip and Inter modes of encoding the keypoints for different frame types. For
example, keypoints for new scenes are encoded using the Intra mode, and
keypoints for unchanged scenes are skipped. As a result, the bitrate of the
side information related to keypoint encoding is significantly reduced.
Finally, we present pairwise matching and image retrieval experiments conducted
to evaluate the performance of the proposed approach using the Stanford mobile
augmented reality dataset and 720p format videos. The results show that the
proposed approach offers significantly improved feature matching and image
retrieval performance at a given bitrate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08318</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08318</id><created>2015-06-27</created><authors><author><keyname>Tan</keyname><forenames>Le Thanh</forenames></author><author><keyname>Le</keyname><forenames>Long Bao</forenames></author></authors><title>Compressed Sensing Based Data Processing and MAC Protocol Design for
  Smartgrids</title><categories>cs.IT cs.NI math.IT</categories><comments>2015 IEEE Wireless Communication and Networking Conference (IEEE WCNC
  2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the joint design of data compression and
802.15.4-based medium access control (MAC) protocol for smartgrids with
renewable energy. We study the setting where a number of nodes, each of which
comprises electricity load and/or renewable sources, report periodically their
injected powers to a data concentrator. Our design exploits the correlation of
the reported data in both time and space to perform efficient data compression
using the compressed sensing (CS) technique and efficiently engineer the MAC
protocol so that the reported data can be recovered reliably within minimum
reporting time. Specifically, we perform the following design tasks: i) we
employ the two-dimensional (2D) CS technique to compress the reported data in
the distributed manner, ii) we propose to adapt the 802.15.4 MAC protocol frame
structure to enable efficient data transmission and reliable data
reconstruction, and iii) we develop an analytical model based on which we can
obtain the optimal parameter configuration to minimize the reporting delay.
Finally, numerical results are presented to demonstrate the effectiveness of
our design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08319</identifier>
 <datestamp>2015-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08319</id><created>2015-06-27</created><authors><author><keyname>Chalermsook</keyname><forenames>Parinya</forenames></author><author><keyname>Goswami</keyname><forenames>Mayank</forenames></author><author><keyname>Kozma</keyname><forenames>Laszlo</forenames></author><author><keyname>Mehlhorn</keyname><forenames>Kurt</forenames></author><author><keyname>Saranurak</keyname><forenames>Thatchaphol</forenames></author></authors><title>Greedy Is an Almost Optimal Deque</title><categories>cs.DS math.CO</categories><comments>to be presented at WADS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we extend the geometric binary search tree (BST) model of
Demaine, Harmon, Iacono, Kane, and Patrascu (DHIKP) to accommodate for
insertions and deletions. Within this extended model, we study the online
Greedy BST algorithm introduced by DHIKP. Greedy BST is known to be equivalent
to a maximally greedy (but inherently offline) algorithm introduced
independently by Lucas in 1988 and Munro in 2000, conjectured to be dynamically
optimal.
  With the application of forbidden-submatrix theory, we prove a quasilinear
upper bound on the performance of Greedy BST on deque sequences. It has been
conjectured (Tarjan, 1985) that splay trees (Sleator and Tarjan, 1983) can
serve such sequences in linear time. Currently neither splay trees, nor other
general-purpose BST algorithms are known to fulfill this requirement. As a
special case, we show that Greedy BST can serve output-restricted deque
sequences in linear time. A similar result is known for splay trees (Tarjan,
1985; Elmasry, 2004).
  As a further application of the insert-delete model, we give a simple proof
that, given a set U of permutations of [n], the access cost of any BST
algorithm is Omega(log |U| + n) on &quot;most&quot; of the permutations from U. In
particular, this implies that the access cost for a random permutation of [n]
is Omega(n log n) with high probability.
  Besides the splay tree noted before, Greedy BST has recently emerged as a
plausible candidate for dynamic optimality. Compared to splay trees, much less
effort has gone into analyzing Greedy BST. Our work is intended as a step
towards a full understanding of Greedy BST, and we remark that
forbidden-submatrix arguments seem particularly well suited for carrying out
this program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08326</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08326</id><created>2015-06-27</created><authors><author><keyname>Morone</keyname><forenames>Flaviano</forenames></author><author><keyname>Makse</keyname><forenames>Hernan A.</forenames></author></authors><title>Influence maximization in complex networks through optimal percolation</title><categories>physics.soc-ph cond-mat.dis-nn cs.SI</categories><comments>7 pages, 3 figures, Supplementary Information: 52 pages, 10 extended
  data figs</comments><journal-ref>Nature 524, 65-68 (2015)</journal-ref><doi>10.1038/nature14604</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The whole frame of interconnections in complex networks hinges on a specific
set of structural nodes, much smaller than the total size, which, if activated,
would cause the spread of information to the whole network [1]; or, if
immunized, would prevent the diffusion of a large scale epidemic [2,3].
Localizing this optimal, i.e. minimal, set of structural nodes, called
influencers, is one of the most important problems in network science [4,5].
Despite the vast use of heuristic strategies to identify influential spreaders
[6-14], the problem remains unsolved. Here, we map the problem onto optimal
percolation in random networks to identify the minimal set of influencers,
which arises by minimizing the energy of a many-body system, where the form of
the interactions is fixed by the non-backtracking matrix [15] of the network.
Big data analyses reveal that the set of optimal influencers is much smaller
than the one predicted by previous heuristic centralities. Remarkably, a large
number of previously neglected weakly-connected nodes emerges among the optimal
influencers. These are topologically tagged as low-degree nodes surrounded by
hierarchical coronas of hubs, and are uncovered only through the optimal
collective interplay of all the influencers in the network. Eventually, the
present theoretical framework may hold a larger degree of universality, being
applicable to other hard optimization problems exhibiting a continuous
transition from a known phase [16].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08328</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08328</id><created>2015-06-27</created><authors><author><keyname>Tan</keyname><forenames>Le Thanh</forenames></author><author><keyname>Le</keyname><forenames>Long Bao</forenames></author></authors><title>Distributed MAC Protocol Design for Full-Duplex Cognitive Radio Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>2015 IEEE Global Communications Conference (IEEE GLOBECOM 2015)
  (accepted)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the Medium Access Control (MAC) protocol design
for full-duplex cognitive radio networks (FDCRNs). Our design exploits the fact
that full-duplex (FD) secondary users (SUs) can perform spectrum sensing and
access simultaneously, which enable them to detect the primary users' (PUs)
activity during transmission. The developed FD MAC protocol employs the
standard backoff mechanism as in the 802.11 MAC protocol. However, we propose
to adopt the frame fragmentation during the data transmission phase for timely
detection of active PUs where each data packet is divided into multiple
fragments and the active SU makes sensing detection at the end of each data
fragment. Then, we develop a mathematical model to analyze the throughput
performance of the proposed FD MAC protocol. Furthermore, we propose an
algorithm to configure the MAC protocol so that efficient self-interference
management and sensing overhead control can be achieved. Finally, numerical
results are presented to evaluate the performance of our design and demonstrate
the throughput enhancement compared to the existing half-duplex (HD) cognitive
MAC protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08347</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08347</id><created>2015-06-27</created><authors><author><keyname>Ghiasi</keyname><forenames>Golnaz</forenames></author><author><keyname>Fowlkes</keyname><forenames>Charless C.</forenames></author></authors><title>Occlusion Coherence: Detecting and Localizing Occluded Faces</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The presence of occluders significantly impacts object recognition accuracy.
However, occlusion is typically treated as an unstructured source of noise and
explicit models for occluders have lagged behind those for object appearance
and shape. In this paper we describe a hierarchical deformable part model for
face detection and keypoint localization that explicitly models part occlusion.
The proposed model structure makes it possible to augment positive training
data with large numbers of synthetically occluded instances. This allows us to
easily incorporate the statistics of occlusion patterns in a discriminatively
trained model. We test the model on several benchmarks for keypoint
localization and detection including challenging data sets featuring
significant occlusion. We find that the addition of an explicit model of
occlusion yields a system that outperforms existing approaches in keypoint
localization accuracy and detection performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08348</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08348</id><created>2015-06-27</created><updated>2015-09-10</updated><authors><author><keyname>Salahuddin</keyname><forenames>Mohammad A.</forenames></author><author><keyname>Elbiaze</keyname><forenames>Halima</forenames></author><author><keyname>Ajib</keyname><forenames>Wessam</forenames></author><author><keyname>Glitho</keyname><forenames>Roch</forenames></author></authors><title>Social Network Analysis Inspired Content Placement with QoS in
  Cloud-based Content Delivery Networks</title><categories>cs.NI cs.MM</categories><comments>6 pages, 4 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content Placement (CP) problem in Cloud-based Content Delivery Networks
(CCDNs) leverage resource elasticity to build cost effective CDNs that
guarantee QoS. In this paper, we present our novel CP model, which optimally
places content on surrogates in the cloud, to achieve (a) minimum cost of
leasing storage and bandwidth resources for data coming into and going out of
the cloud zones and regions, (b) guarantee Service Level Agreement (SLA), and
(c) minimize degree of QoS violations. The CP problem is NP-Hard, hence we
design a unique push-based heuristic, called Weighted Social Network Analysis
(W-SNA) for CCDN providers. W-SNA is based on Betweeness Centrality (BC) from
SNA and prioritizes surrogates based on their relationship to the other
vertices in the network graph. To achieve our unique objectives, we further
prioritize surrogates based on weights derived from storage cost and content
requests. We compare our heuristic to current state of the art Greedy Site (GS)
and purely Social Network Analysis (SNA) heuristics, which are relevant to our
work. We show that W-SNA outperforms GS and SNA in minimizing cost and QoS.
Moreover, W-SNA guarantees SLA but also minimizes the degree of QoS violations.
To the best of our knowledge, this is the first model and heuristic of its
kind, which is timely and gives a fundamental pre-allocation scheme for future
online and dynamic resource provision for CCDNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08349</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08349</id><created>2015-06-27</created><authors><author><keyname>Li</keyname><forenames>Lantian</forenames></author><author><keyname>Lin</keyname><forenames>Yiye</forenames></author><author><keyname>Zhang</keyname><forenames>Zhiyong</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author></authors><title>Improved Deep Speaker Feature Learning for Text-Dependent Speaker
  Recognition</title><categories>cs.CL cs.LG cs.NE</categories><comments>arXiv admin note: substantial text overlap with arXiv:1505.06427</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A deep learning approach has been proposed recently to derive speaker
identifies (d-vector) by a deep neural network (DNN). This approach has been
applied to text-dependent speaker recognition tasks and shows reasonable
performance gains when combined with the conventional i-vector approach.
Although promising, the existing d-vector implementation still can not compete
with the i-vector baseline. This paper presents two improvements for the deep
learning approach: a phonedependent DNN structure to normalize phone variation,
and a new scoring approach based on dynamic time warping (DTW). Experiments on
a text-dependent speaker recognition task demonstrated that the proposed
methods can provide considerable performance improvement over the existing
d-vector implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08350</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08350</id><created>2015-06-27</created><updated>2016-01-12</updated><authors><author><keyname>Mu</keyname><forenames>Yadong</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Fan</keyname><forenames>Wei</forenames></author></authors><title>Stochastic Gradient Made Stable: A Manifold Propagation Approach for
  Large-Scale Optimization</title><categories>cs.LG cs.NA</categories><comments>14 pages, 9 figures</comments><msc-class>90C06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic gradient descent (SGD) holds as a classical method to build large
scale machine learning models over big data. A stochastic gradient is typically
calculated from a limited number of samples (known as mini-batch), so it
potentially incurs a high variance and causes the estimated parameters bounce
around the optimal solution. To improve the stability of stochastic gradient,
recent years have witnessed the proposal of several semi-stochastic gradient
descent algorithms, which distinguish themselves from standard SGD by
incorporating global information into gradient computation. In this paper we
contribute a novel stratified semi-stochastic gradient descent (S3GD) algorithm
to this nascent research area, accelerating the optimization of a large family
of composite convex functions. Though theoretically converging faster, prior
semi-stochastic algorithms are found to suffer from high iteration complexity,
which makes them even slower than SGD in practice on many datasets. In our
proposed S3GD, the semi-stochastic gradient is calculated based on efficient
manifold propagation, which can be numerically accomplished by sparse matrix
multiplications. This way S3GD is able to generate a highly-accurate estimate
of the exact gradient from each mini-batch with largely-reduced computational
complexity. Theoretic analysis reveals that the proposed S3GD elegantly
balances the geometric algorithmic convergence rate against the space and time
complexities during the optimization. The efficacy of S3GD is also
experimentally corroborated on several large-scale benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08352</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08352</id><created>2015-06-27</created><authors><author><keyname>Liu</keyname><forenames>Junbiao</forenames></author><author><keyname>Jin</keyname><forenames>Xinyu</forenames></author><author><keyname>Jiang</keyname><forenames>Lurong</forenames></author><author><keyname>Xia</keyname><forenames>Yongxiang</forenames></author><author><keyname>Ouyang</keyname><forenames>Bo</forenames></author><author><keyname>Dong</keyname><forenames>Fang</forenames></author><author><keyname>Lang</keyname><forenames>Yicong</forenames></author><author><keyname>Zhang</keyname><forenames>Wenping</forenames></author></authors><title>Threshold for the Outbreak of Cascading Failures in Degree-degree
  Uncorrelated Networks</title><categories>cs.SI cs.NI physics.soc-ph</categories><comments>11 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In complex networks, the failure of one or very few nodes may cause cascading
failures. When this dynamical process stops in steady state, the size of the
giant component formed by remaining un-failed nodes can be used to measure the
severity of cascading failures, which is critically important for estimating
the robustness of networks. In this paper, we provide a cascade of overload
failure model with local load sharing mechanism, and then explore the threshold
of node capacity when the large-scale cascading failures happen and un-failed
nodes in steady state cannot connect to each other to form a large connected
sub-network. We get the theoretical derivation of this threshold in
degree-degree uncorrelated networks, and validate the effectiveness of this
method in simulation. This threshold provide us a guidance to improve the
network robustness under the premise of limited capacity resource when creating
a network and assigning load. Therefore, this threshold is useful and important
to analyze the robustness of networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08353</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08353</id><created>2015-06-27</created><authors><author><keyname>Hu</keyname><forenames>Haijuan</forenames></author><author><keyname>Froment</keyname><forenames>Jacques</forenames></author><author><keyname>Liu</keyname><forenames>Quansheng</forenames></author></authors><title>Patch-Based Low-Rank Minimization for Image Denoising</title><categories>cs.CV</categories><comments>4pages (two columns)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Patch-based sparse representation and low-rank approximation for image
processing attract much attention in recent years. The minimization of the
matrix rank coupled with the Frobenius norm data fidelity can be solved by the
hard thresholding filter with principle component analysis (PCA) or singular
value decomposition (SVD). Based on this idea, we propose a patch-based
low-rank minimization method for image denoising, which learns compact
dictionaries from similar patches with PCA or SVD, and applies simple hard
thresholding filters to shrink the representation coefficients. Compared to
recent patch-based sparse representation methods, experiments demonstrate that
the proposed method is not only rather rapid, but also effective for a variety
of natural images, especially for texture parts in images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08354</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08354</id><created>2015-06-28</created><updated>2015-07-13</updated><authors><author><keyname>Cheng</keyname><forenames>Jianjun</forenames></author><author><keyname>Li</keyname><forenames>Longjie</forenames></author><author><keyname>Leng</keyname><forenames>Mingwei</forenames></author><author><keyname>Lu</keyname><forenames>Weiguo</forenames></author><author><keyname>Yao</keyname><forenames>Yukai</forenames></author><author><keyname>Chen</keyname><forenames>Xiaoyun</forenames></author></authors><title>A divisive spectral method for network community detection</title><categories>cs.SI physics.soc-ph</categories><comments>23pages, 10 figures, and 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection is a fundamental problem in the domain of complex-network
analysis. It has received great attention, and many community detection methods
have been proposed in the last decade. In this paper, we propose a divisive
spectral method for identifying community structures from networks, which
utilizes a sparsification operation to pre-process the networks first, and then
uses a repeated bisection spectral algorithm to partition the networks into
communities. The sparsification operation makes the community boundaries more
clearer and more sharper, so that the repeated spectral bisection algorithm
extract high-quality community structures accurately from the sparsified
networks. Experiments show that the combination of network sparsification and
spectral bisection algorithm is highly successful, the proposed method is more
effective in detecting community structures from networks than the others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08361</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08361</id><created>2015-06-28</created><authors><author><keyname>Pabico</keyname><forenames>Jaderick P.</forenames></author></authors><title>Simultaneously Solving Computational Problems Using an Artificial
  Chemical Reactor</title><categories>cs.ET cs.NE</categories><comments>6 pages, appeared in R.P. Salda\~na (ed.) Proceedings (CDROM) of the
  6th Philippine Computing Science Congress (PCSC 2006), Ateneo De Manila
  University, Loyola Heights, Quezon City, Philippines, 28-29 March 2006, pp
  48-53 (ISSN 1908-1146)</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper is centered on using chemical reaction as a computational metaphor
for simultaneously solving problems. An artificial chemical reactor that can
simultaneously solve instances of three unrelated problems was created. The
reactor is a distributed stochastic algorithm that simulates a chemical
universe wherein the molecular species are being represented either by a human
genomic contig panel, a Hamiltonian cycle, or an aircraft landing schedule. The
chemical universe is governed by reactions that can alter genomic sequences,
re-order Hamiltonian cycles, or reschedule an aircraft landing program.
Molecular masses were considered as measures of goodness of solutions, and
represented radiation hybrid (RH) vector similarities, costs of Hamiltonian
cycles, and penalty costs for landing an aircraft before and after target
landing times. This method, tested by solving in tandem with deterministic
algorithms, has been shown to find quality solutions in finding the minima RH
vector similarities of genomic data, minima costs in Hamiltonian cycles of the
traveling salesman, and minima costs for landing aircrafts before or after
target landing times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08370</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08370</id><created>2015-06-28</created><authors><author><keyname>Tal</keyname><forenames>Ido</forenames></author></authors><title>On the Construction of Polar Codes for Channels with Moderate Input
  Alphabet Sizes</title><categories>cs.IT math.IT</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current deterministic algorithms for the construction of polar codes can only
be argued to be practical for channels with small input alphabet sizes. In this
paper, we show that any construction algorithm for channels with moderate input
alphabet size which follows the paradigm of &quot;degrading after each polarization
step&quot; will inherently be impractical with respect to a certain &quot;hard&quot;
underlying channel. This result also sheds light on why the construction of
LDPC codes using density evolution is impractical for channels with moderate
sized input alphabets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08375</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08375</id><created>2015-06-28</created><authors><author><keyname>Gamard</keyname><forenames>Guilhem</forenames></author><author><keyname>Richomme</keyname><forenames>Gwena&#xeb;l</forenames></author></authors><title>Comparison of Coverability and Multi-Scale Coverability in One and Two
  Dimensions</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A word is quasiperiodic (or coverable) if it can be covered with occurrences
of another finite word, called its quasiperiod. A word is multi-scale
quasiperiodic (or multi-scale coverable) if it has infinitely many different
quasiperiods. These notions were previously studied in the domains of text
algorithms and combinatorics of right infinite words.
  We extend them to infinite pictures (two-dimensional words). Then we compare
the regularity properties (uniform recurrence, uniform frequencies, topological
entropy) of quasiperiodicity with multi-scale quasiperiodicity, and we also
compare each of them with its one-dimensional counterpart.
  We also study which properties of quasiperiods enforce properties on the
quasiperiodic words.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08378</identifier>
 <datestamp>2015-08-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08378</id><created>2015-06-28</created><authors><author><keyname>Ausloos</keyname><forenames>Marcel</forenames></author></authors><title>Slow-down or speed-up of inter- and intra-cluster diffusion of
  controversial knowledge in stubborn communities based on a small world
  network</title><categories>physics.soc-ph cond-mat.dis-nn cond-mat.stat-mech cs.SI</categories><comments>16 pages, 28 references, 6 Tables, prepared for a Frontiers Research
  Topic &quot;Opinions, Choices and Actions: Applications of Sociophysics to the
  diffusion of ideas&quot; issue, A. Martins &amp; S.Galam, Eds</comments><journal-ref>Front. Phys. 3:43 (2015)</journal-ref><doi>10.3389/fphy.2015.00043</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion of knowledge is expected to be huge when agents are open minded.
The report concerns a more difficult diffusion case when communities are made
of stubborn agents. Communities having markedly different opinions are for
example the Neocreationist and Intelligent Design Proponents (IDP), on one
hand, and the Darwinian Evolution Defenders (DED), on the other hand. The case
of knowledge diffusion within such communities is studied here on a network
based on an adjacency matrix built from time ordered selected quotations of
agents, whence for inter- and intra-communities. The network is intrinsically
directed and not necessarily reciprocal. Thus, the adjacency matrices have
complex eigenvalues, the eigenvectors present complex components. A
quantification of the slow-down or speed-up effects of information diffusion in
such temporal networks, with non-Markovian contact sequences, can be made by
comparing the real time dependent (directed) network to its counterpart, the
time aggregated (undirected) network, - which has real eigenvalues. In order to
do so, small world networks which both contain an $odd$ number of nodes are
studied and compared to similar networks with an $even$ number of nodes.
  It is found that (i) the diffusion of knowledge is more difficult on the
largest networks, (ii) the network size influences the slowing-down or
speeding-up diffusion process. Interestingly, it is observed that (iii) the
diffusion of knowledge is slower in IDP and faster in DED communities. It is
suggested that the finding can be &quot;rationalized&quot;, if some &quot;scientific quality&quot;
and &quot;publication habit&quot; is attributed to the agents, as common sense would
guess. This finding offers some opening discussion toward tying scientific
knowledge to belief.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08388</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08388</id><created>2015-06-28</created><authors><author><keyname>Folwarczn&#xfd;</keyname><forenames>Luk&#xe1;&#x161;</forenames></author><author><keyname>Knop</keyname><forenames>Du&#x161;an</forenames></author></authors><title>IV-matching is strongly NP-hard</title><categories>cs.DM cs.CC</categories><comments>5 pages, 2 figures</comments><msc-class>05C70</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IV-matching is a generalization of perfect bipartite matching. The complexity
of finding IV-matching in a graph was posted as an open problem at the ICALP
2014 conference.
  In this note, we resolve the question and prove that, contrary to the
expectations of the authors, the given problem is strongly NP-hard (already in
the simplest non-trivial case of four layers). Hence it is unlikely that there
would be an efficient (polynomial or pseudo-polynomial) algorithm solving the
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08392</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08392</id><created>2015-06-28</created><authors><author><keyname>Elkin</keyname><forenames>Michael</forenames></author><author><keyname>Pettie</keyname><forenames>Seth</forenames></author></authors><title>A Linear-Size Logarithmic Stretch Path-Reporting Distance Oracle for
  General Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2001 Thorup and Zwick devised a distance oracle, which given an $n$-vertex
undirected graph and a parameter $k$, has size $O(k n^{1+1/k})$. Upon a query
$(u,v)$ their oracle constructs a $(2k-1)$-approximate path $\Pi$ between $u$
and $v$. The query time of the Thorup-Zwick's oracle is $O(k)$, and it was
subsequently improved to $O(1)$ by Chechik. A major drawback of the oracle of
Thorup and Zwick is that its space is $\Omega(n \cdot \log n)$. Mendel and Naor
devised an oracle with space $O(n^{1+1/k})$ and stretch $O(k)$, but their
oracle can only report distance estimates and not actual paths. In this paper
we devise a path-reporting distance oracle with size $O(n^{1+1/k})$, stretch
$O(k)$ and query time $O(n^\epsilon)$, for an arbitrarily small $\epsilon &gt; 0$.
In particular, our oracle can provide logarithmic stretch using linear size.
Another variant of our oracle has size $O(n \log\log n)$, polylogarithmic
stretch, and query time $O(\log\log n)$.
  For unweighted graphs we devise a distance oracle with multiplicative stretch
$O(1)$, additive stretch $O(\beta(k))$, for a function $\beta(\cdot)$, space
$O(n^{1+1/k} \cdot \beta)$, and query time $O(n^\epsilon)$, for an arbitrarily
small constant $\epsilon &gt;0$. The tradeoff between multiplicative stretch and
size in these oracles is far below girth conjecture threshold (which is stretch
$2k-1$ and size $O(n^{1+1/k})$). Breaking the girth conjecture tradeoff is
achieved by exhibiting a tradeoff of different nature between additive stretch
$\beta(k)$ and size $O(n^{1+1/k})$. A similar type of tradeoff was exhibited by
a construction of $(1+\epsilon,\beta)$-spanners due to Elkin and Peleg.
However, so far $(1+\epsilon,\beta)$-spanners had no counterpart in the
distance oracles' world.
  An important novel tool that we develop on the way to these results is a
{distance-preserving path-reporting oracle}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08399</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08399</id><created>2015-06-28</created><authors><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Binary full adder, made of fusion gates, in sub-excitable
  Belousov-Zhabotinsky system</title><categories>cs.ET</categories><doi>10.1103/PhysRevE.92.032811</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sub-excitable BZ medium responds to asymmetric local perturbation by
producing travelling localised excitation wave-fragments, distant relatives of
dissipative solitons. The size and life span of an excitation wave-fragment
depend on the illumination level of the medium. Under the right conditions the
wave-fragments conserve their shape and velocity vectors for extended time
periods. We interpret the wave-fragments as values of Boolean variables. When
two or more wave-fragments collide they annihilate or merge into a new
wave-fragment. States of the logic variables, represented by the
wave-fragments, are changed in the result of the collision between the
wave-fragments. Thus, a logical gate is implemented. Several theoretical
designs and experimental laboratory implementations of Boolean logic gates have
been proposed in the past but little has been done cascading the gates into
binary arithmetical circuits. We propose a unique design of a binary one-bit
full adder based on a fusion gate. A fusion gate is a two-input three-output
logical device which calculates conjunction of the input variables and
conjunction of one input variable with negation of another input variable. The
gate is made of three channels: two channels cross each other at an angle,
third channel starts at the junction. The channels contain BZ medium. When two
excitation wave-fragments, travelling towards each other along input channels,
collide at the junction they merge into a single wave-front travelling along
the third channel. If there is just one wave-front in the input channel, the
front continues its propagation undisturbed. We make a one-bit full adder by
cascading two fusion gates. We show how to cascade the adder blocks into a
many-bit full adder. We evaluate feasibility of our designs by simulating
evolution of excitation in the gates and adders using numerical integration of
Oregonator equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08400</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08400</id><created>2015-06-28</created><authors><author><keyname>Rook</keyname><forenames>Christopher J.</forenames></author></authors><title>Optimal Equity Glidepaths in Retirement</title><categories>q-fin.GN cs.DM math.PR</categories><comments>Fully documented source code from a C++ implementation is included in
  the attached proofs appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic retirement glidepaths evolve over time based on some measure such as
the retiree's funded status or current market valuations. Conversely, static
glidepaths are fixed at a starting point and selected under the assumption that
they will not change. In practice, new static glidepaths may be derived
periodically making them more flexible. The optimal static retirement glidepath
would be the one that performs better than all others with respect to some
metric. When systematic withdrawals are made from a retirement portfolio,
glidepaths are often assessed via the probability of ruin (or success). Our
goal here is to derive the optimal static glidepath with respect to this
metric. It is a result new to the literature and the shape will be of special
interest to retirees, financial advisors, retirement researchers, and
target-date fund providers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08409</identifier>
 <datestamp>2015-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08409</id><created>2015-06-28</created><updated>2015-07-09</updated><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Langerman</keyname><forenames>Stefan</forenames></author></authors><title>Bust-a-Move/Puzzle Bobble is NP-Complete</title><categories>cs.CC cs.CG</categories><comments>9 pages, 9 figures. Corrected mistakes in gadgets</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the classic 1994 Taito video game, known as Puzzle Bobble or
Bust-a-Move, is NP-complete. Our proof applies to the perfect-information
version where the bubble sequence is known in advance, and it uses just three
bubble colors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08415</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08415</id><created>2015-06-28</created><authors><author><keyname>Burattin</keyname><forenames>Andrea</forenames></author></authors><title>PLG2: Multiperspective Processes Randomization and Simulation for Online
  and Offline Settings</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Process mining represents as an important field in BPM and data mining
research. Recently, it has gained importance also for practitioners: more and
more companies are creating business process intelligence solutions. The
evaluation of process mining algorithms requires, as any other data mining
task, the availability of large amount of real-world data. Despite the
increasing availability of such datasets, they are affected by many
limitations, in primis the absence of a &quot;gold standard&quot; (i.e., the reference
model).
  This paper extends an approach, already available in the literature, for the
generation of random processes. Novelties have been introduces throughout the
work and, in particular, they involve the complete support for multiperspective
models and logs (i.e., the control-flow perspective is enriched with time and
data information) and for online settings (i.e., generation of multiperspective
event streams and concept drifts). The proposed new framework is able to almost
entirely cover the spectrum of possible scenarios that can be observed in the
real-world.
  The proposed approach is implemented as a publicly available Java
application, with a set of APIs for the programmatic execution of experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08417</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08417</id><created>2015-06-28</created><authors><author><keyname>Ouyang</keyname><forenames>Yi</forenames></author><author><keyname>Teneketzis</keyname><forenames>Demosthenis</forenames></author></authors><title>A Common Information-Based Multiple Access Protocol Achieving Full
  Throughput and Linear Delay</title><categories>cs.NI cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multiple access communication system where multiple users share
a common collision channel. Each user observes its local traffic and the
feedback from the channel. At each time instant the feedback from the channel
is one of three messages: no transmission, successful transmission, collision.
The objective is to design a transmission protocol that coordinates the users'
transmissions and achieves high throughput and low delay.
  We present a decentralized Common Information-Based Multiple Access (CIMA)
protocol that has the following features: (i) it achieves the full throughput
region of the collision channel; (ii) it results in a delay that is linear in
the number of users, and is significantly lower than that of CSMA protocols;
(iii) it avoids collisions without channel sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08422</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08422</id><created>2015-06-28</created><authors><author><keyname>Niu</keyname><forenames>Li-Qiang</forenames></author><author><keyname>Dai</keyname><forenames>Xin-Yu</forenames></author></authors><title>Topic2Vec: Learning Distributed Representations of Topics</title><categories>cs.CL cs.LG</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latent Dirichlet Allocation (LDA) mining thematic structure of documents
plays an important role in nature language processing and machine learning
areas. However, the probability distribution from LDA only describes the
statistical relationship of occurrences in the corpus and usually in practice,
probability is not the best choice for feature representations. Recently,
embedding methods have been proposed to represent words and documents by
learning essential concepts and representations, such as Word2Vec and Doc2Vec.
The embedded representations have shown more effectiveness than LDA-style
representations in many tasks. In this paper, we propose the Topic2Vec approach
which can learn topic representations in the same semantic vector space with
words, as an alternative to probability. The experimental results show that
Topic2Vec achieves interesting and meaningful results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08425</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08425</id><created>2015-06-28</created><authors><author><keyname>Lee</keyname><forenames>Sue Han</forenames></author><author><keyname>Chan</keyname><forenames>Chee Seng</forenames></author><author><keyname>Wilkin</keyname><forenames>Paul</forenames></author><author><keyname>Remagnino</keyname><forenames>Paolo</forenames></author></authors><title>Deep-Plant: Plant Identification with convolutional neural networks</title><categories>cs.CV cs.AI cs.NE</categories><comments>6 pages, 8 figures, accepted as oral presentation in ICIP2015,
  Qu\'ebec City, Canada</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies convolutional neural networks (CNN) to learn unsupervised
feature representations for 44 different plant species, collected at the Royal
Botanic Gardens, Kew, England. To gain intuition on the chosen features from
the CNN model (opposed to a 'black box' solution), a visualisation technique
based on the deconvolutional networks (DN) is utilized. It is found that
venations of different order have been chosen to uniquely represent each of the
plant species. Experimental results using these CNN features with different
classifiers show consistency and superiority compared to the state-of-the art
solutions which rely on hand-crafted features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08435</identifier>
 <datestamp>2015-07-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08435</id><created>2015-06-28</created><updated>2015-07-18</updated><authors><author><keyname>Chang</keyname><forenames>J.</forenames></author><author><keyname>Karra</keyname><forenames>S.</forenames></author><author><keyname>Nakshatrala</keyname><forenames>K. B.</forenames></author></authors><title>Large-scale Optimization-based Non-negative Computational Framework for
  Diffusion Equations: Parallel Implementation and Performance Studies</title><categories>cs.NA cs.CE cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well-known that the standard Galerkin formulation, which is often the
formulation of choice under the finite element method for solving self-adjoint
diffusion equations, does not meet maximum principles and the non-negative
constraint for anisotropic diffusion equations. Recently, optimization-based
methodologies that satisfy maximum principles and the non-negative constraint
for steady-state and transient diffusion-type equations have been proposed.
Till date, these methodologies have been tested only on small-scale academic
problems. The purpose of this paper is to systematically study the performance
of the non-negative methodology in the context of high performance computing
(HPC). PETSc and TAO libraries are, respectively, used for the parallel
environment and optimization solvers. For large-scale problems, it is important
for computational scientists to understand the computational cost, scalability
and efficiency; which all will be addressed in this paper. The numerical
experiments are conducted on the state-of-the-art HPC systems, and relevant
parallel performance metrics are provided to illustrate the efficiency of our
methodology. Our studies indicate that the proposed non-negative computational
framework for diffusion-type equations exhibits excellent strong scaling for
large and complicated 3D problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08438</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08438</id><created>2015-06-28</created><updated>2016-01-27</updated><authors><author><keyname>Sener</keyname><forenames>Ozan</forenames></author><author><keyname>Zamir</keyname><forenames>Amir</forenames></author><author><keyname>Savarese</keyname><forenames>Silvio</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>Unsupervised Semantic Parsing of Video Collections</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human communication typically has an underlying structure. This is reflected
in the fact that in many user generated videos, a starting point, ending, and
certain objective steps between these two can be identified. In this paper, we
propose a method for parsing a video into such semantic steps in an
unsupervised way. The proposed method is capable of providing a semantic
&quot;storyline&quot; of the video composed of its objective steps. We accomplish this
using both visual and language cues in a joint generative model. The proposed
method can also provide a textual description for each of the identified
semantic steps and video segments. We evaluate this method on a large number of
complex YouTube videos and show results of unprecedented quality for this
intricate and impactful problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08447</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08447</id><created>2015-06-28</created><authors><author><keyname>Geneson</keyname><forenames>Jesse</forenames></author></authors><title>Improved lower bounds on extremal functions of multidimensional
  permutation matrices</title><categories>math.CO cs.DM</categories><comments>8 pages</comments><msc-class>05D99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $d$-dimensional zero-one matrix $A$ avoids another $d$-dimensional zero-one
matrix $P$ if no submatrix of $A$ can be transformed to $P$ by changing some
ones to zeroes. Let $f(n,P,d)$ denote the maximum number of ones in a
$d$-dimensional $n \times \cdots \times n$ zero-one matrix that avoids $P$.
  Fox proved for $n$ sufficiently large that $f(n, P, 2) = 2^{k^{\Theta(1)}}n$
for almost all $k \times k$ permutation matrices $P$. We extend this result by
proving for $d \geq 2$ and $n$ sufficiently large that $f(n, P, d) =
2^{k^{\Theta(1)}}n^{d-1}$ for almost all $d$-dimensional permutation matrices
$P$ of dimensions $k \times \cdots \times k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08448</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08448</id><created>2015-06-28</created><updated>2015-10-23</updated><authors><author><keyname>Forster</keyname><forenames>Dennis</forenames></author><author><keyname>Sheikh</keyname><forenames>Abdul-Saboor</forenames></author><author><keyname>L&#xfc;cke</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Neural Simpletrons - Minimalistic Directed Generative Networks for
  Learning with Few Labels</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning is intensively studied using the perspectives of unsupervised
and supervised learning. Comparisons of deep directed generative models and
deep discriminative networks is difficult, however, because of: (A) the
different semantics of their graphical descriptions; (B) different parameter
optimization methods; (C) different benchmarking objectives, and (D) different
scalability. Here, we investigate a deep directed model in a form and setting
as similar to standard deep neural networks as possible. Based on normalized
Poisson mixtures, we derive a minimalistic deep neural network with local
activation and learning rules. The network can learn in a semi-supervised
setting and can be scaled using standard deep learning tools. Benchmarks with
partly labeled data provide the canonical domain for comparison with deep
discriminative networks. Empirical evaluations show, that: (A) Performance of
the network is competitive with recent deep networks (and other systems). (B)
The network can be applied down to the limit of very few labeled data points.
(C) The network is the best performing monolithic (i.e., non-hybrid) system for
few labels. Our results provide a baseline for more expressive deep directed
models, they highlight the performance vs. complexity tradeoff for deep
learning, and show that already minimalistic deep directed models are
competitive if they can be scaled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08454</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08454</id><created>2015-06-28</created><authors><author><keyname>Chenthamarakshan</keyname><forenames>Vijil</forenames></author><author><keyname>Desphande</keyname><forenames>Prasad M</forenames></author><author><keyname>Krishnapuram</keyname><forenames>Raghu</forenames></author><author><keyname>Varadarajan</keyname><forenames>Ramakrishna</forenames></author><author><keyname>Stolze</keyname><forenames>Knut</forenames></author></authors><title>WYSIWYE: An Algebra for Expressing Spatial and Textual Rules for
  Information Extraction</title><categories>cs.CL cs.DB cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The visual layout of a webpage can provide valuable clues for certain types
of Information Extraction (IE) tasks. In traditional rule based IE frameworks,
these layout cues are mapped to rules that operate on the HTML source of the
webpages. In contrast, we have developed a framework in which the rules can be
specified directly at the layout level. This has many advantages, since the
higher level of abstraction leads to simpler extraction rules that are largely
independent of the source code of the page, and, therefore, more robust. It can
also enable specification of new types of rules that are not otherwise
possible. To the best of our knowledge, there is no general framework that
allows declarative specification of information extraction rules based on
spatial layout. Our framework is complementary to traditional text based rules
framework and allows a seamless combination of spatial layout based rules with
traditional text based rules. We describe the algebra that enables such a
system and its efficient implementation using standard relational and text
indexing features of a relational database. We demonstrate the simplicity and
efficiency of this system for a task involving the extraction of software
system requirements from software product pages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08459</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08459</id><created>2015-06-28</created><authors><author><keyname>Ye</keyname><forenames>Jianbo</forenames></author><author><keyname>Yan</keyname><forenames>Zhixin</forenames></author></authors><title>On the Approximation Theory of Linear Variational Subspace Design</title><categories>cs.GR</categories><comments>10 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving large-scale optimization on-the-fly is often a difficult task for
real-time computer graphics applications. To tackle this challenge, model
reduction is a well-adopted technique. Despite its usefulness, model reduction
often requires a handcrafted subspace that spans a domain that hypothetically
embodies desirable solutions. For many applications, obtaining such subspaces
case-by-case either is impossible or requires extensive human labors, hence
does not readily have a scalable solution for growing number of tasks. We
propose linear variational subspace design for large-scale constrained
quadratic programming, which can be computed automatically without any human
interventions. We provide meaningful approximation error bound that
substantiates the quality of calculated subspace, and demonstrate its empirical
success in interactive deformable modeling for triangular and tetrahedral
meshes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08471</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08471</id><created>2015-06-28</created><authors><author><keyname>Paulson</keyname><forenames>Joel A.</forenames></author><author><keyname>Buehler</keyname><forenames>Edward A.</forenames></author><author><keyname>Braatz</keyname><forenames>Richard D.</forenames></author><author><keyname>Mesbah</keyname><forenames>Ali</forenames></author></authors><title>Receding-horizon Stochastic Model Predictive Control with Hard Input
  Constraints and Joint State Chance Constraints</title><categories>math.OC cs.SY</categories><comments>Submitted to Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article considers the stochastic optimal control of discrete-time linear
systems subject to (possibly) unbounded stochastic disturbances, hard
constraints on the manipulated variables, and joint chance constraints on the
states. A tractable convex second-order cone program (SOCP) is derived for
calculating the receding-horizon control law at each time step. Feedback is
incorporated during prediction by parametrizing the control law as an affine
function of the disturbances. Hard input constraints are guaranteed by
saturating the disturbances that appear in the control law parametrization. The
joint state chance constraints are conservatively approximated as a collection
of individual chance constraints that are subsequently relaxed via the
Cantelli-Chebyshev inequality. Feasibility of the SOCP is guaranteed by
softening the approximated chance constraints using the exact penalty function
method. Closed-loop stability in a stochastic sense is established by
establishing that the states satisfy a geometric drift condition outside of a
compact set such that their variance is bounded at all times. The SMPC approach
is demonstrated using a continuous acetone-butanol-ethanol fermentation
process, which is used for production of high-value-added drop-in biofuels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08472</identifier>
 <datestamp>2015-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08472</id><created>2015-06-28</created><updated>2015-07-13</updated><authors><author><keyname>Dvijotham</keyname><forenames>Krishnamurthy</forenames></author><author><keyname>Low</keyname><forenames>Steven</forenames></author><author><keyname>Chertkov</keyname><forenames>Michael</forenames></author></authors><title>Solving the power flow equations: a monotone operator approach</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The AC power flow equations underlie all operational aspects of power
systems. They are solved routinely in operational practice using the
Newton-Raphson method and its variants. These methods work well given a good
initial &quot;guess&quot; for the solution, which is always available in normal system
operations. However, with the increase in levels of intermittent generation,
the assumption of a good initial guess always being available is no longer
valid. In this paper, we solve this problem using the theory of monotone
operators. We show that it is possible to compute (using an offline
optimization) a &quot;monotonicity domain&quot; in the space of voltage phasors. Given
this domain, there is a simple efficient algorithm that will either find a
solution in the domain, or provably certify that no solutions exist in it. We
validate the approach on several IEEE test cases and demonstrate that the
offline optimization can be performed tractably and the computed &quot;monotonicity
domain&quot; includes all practically relevant power flow solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08473</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08473</id><created>2015-06-28</created><updated>2016-01-11</updated><authors><author><keyname>Janzamin</keyname><forenames>Majid</forenames></author><author><keyname>Sedghi</keyname><forenames>Hanie</forenames></author><author><keyname>Anandkumar</keyname><forenames>Anima</forenames></author></authors><title>Beating the Perils of Non-Convexity: Guaranteed Training of Neural
  Networks using Tensor Methods</title><categories>cs.LG cs.NE stat.ML</categories><comments>The tensor decomposition analysis is expanded, and the analysis of
  ridge regression is added for recovering the parameters of last layer of
  neural network</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training neural networks is a challenging non-convex optimization problem,
and backpropagation or gradient descent can get stuck in spurious local optima.
We propose a novel algorithm based on tensor decomposition for guaranteed
training of two-layer neural networks. We provide risk bounds for our proposed
method, with a polynomial sample complexity in the relevant parameters, such as
input dimension and number of neurons. While learning arbitrary target
functions is NP-hard, we provide transparent conditions on the function and the
input for learnability. Our training method is based on tensor decomposition,
which provably converges to the global optimum, under a set of mild
non-degeneracy conditions. It consists of simple embarrassingly parallel linear
and multi-linear operations, and is competitive with standard stochastic
gradient descent (SGD), in terms of computational complexity. Thus, we propose
a computationally efficient method with guaranteed risk bounds for training
neural networks with one hidden layer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1506.08477</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1506.08477</id><created>2015-06-28</created><updated>2016-01-14</updated><authors><author><keyname>Kim</keyname><forenames>Eun Jung</forenames></author><author><keyname>Kwon</keyname><forenames>O-joung</forenames></author></authors><title>A polynomial kernel for Block Graph Deletion</title><categories>cs.DS cs.DM</categories><comments>22 pages, 2 figures, An extended abstract appeared in IPEC2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Block Graph Deletion problem, we are given a graph $G$ on $n$ vertices
and a positive integer $k$, and the objective is to check whether it is
possible to delete at most $k$ vertices from $G$ to make it a block graph,
i.e., a graph in which each block is a clique. In this paper, we obtain a
kernel with $\mathcal{O}(k^{6})$ vertices for the Block Graph Deletion problem.
This is a first step to investigate polynomial kernels for deletion problems
into non-trivial classes of graphs of bounded rank-width, but unbounded
tree-width. Our result also implies that Chordal Vertex Deletion admits a
polynomial-size kernel on diamond-free graphs. For the kernelization and its
analysis, we introduce the notion of `complete degree' of a vertex. We believe
that the underlying idea can be potentially applied to other problems. We also
prove that the Block Graph Deletion problem can be solved in time $10^{k}\cdot
n^{\mathcal{O}(1)}$.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="79000" completeListSize="102538">1122234|80001</resumptionToken>
</ListRecords>
</OAI-PMH>
